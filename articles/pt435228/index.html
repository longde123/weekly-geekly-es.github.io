<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨‍🏫 🆑 🏝️ Cluster Kubernetes por US $ 20 por mês 🏫 💄 👎🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="TL DR 


 Nós aumentamos o cluster para atender a aplicativos da web sem estado com entrada , permite criptografar , sem usar ferramentas de automação...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster Kubernetes por US $ 20 por mês</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/iponweb/blog/435228/"><h1 id="tl-dr">  TL  DR </h1><br><p>  Nós aumentamos o cluster para atender a aplicativos da web <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sem estado</a> com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entrada</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">permite criptografar</a> , sem usar ferramentas de automação como kubespray, kubeadm e outras. <br>  Tempo de leitura: ~ 45-60 minutos, tempo de reprodução: a partir de 3 horas. </p><br><h1 id="preambula">  Preâmbulo </h1><br><p>  Fui solicitado a escrever um artigo pela necessidade de meu próprio cluster Kubernetes para experimentação.  As soluções de instalação e configuração automatizadas de código aberto não funcionaram no meu caso, pois usei distribuições Linux não convencionais.  O trabalho intensivo com kubernetes no IPONWEB incentiva você a ter essa plataforma, resolvendo suas tarefas de maneira confortável, inclusive para projetos domésticos. </p><br><h1 id="komponenty">  Componentes </h1><br><p>  Os seguintes componentes aparecerão no artigo: </p><br><p>  - <em>Seu</em> Linux <em>favorito</em> - usei o Gentoo (nó-1: systemd / nó-2: openrc), Ubuntu 18.04.1. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Servidor Kubernetes</a> - kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy. <br>  - <a href="">Plugins</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Containerd</a> + <a href="">CNI (0.7.4)</a> - para organizar a conteinerização, usaremos containererd + CNI em vez de janela de encaixe (embora inicialmente toda a configuração tenha sido carregada na janela de encaixe, nada impede que ela seja usada, se necessário). <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CoreDNS</a> - para organizar a descoberta de serviços de componentes que trabalham dentro do cluster kubernetes.  Recomenda-se uma versão não inferior a 1.2.5, pois com esta versão há suporte sensato para que os coredns funcionem como um processo em execução fora do cluster. <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Flanela</a> - para organizar uma pilha de rede, comunicar lares e contêineres entre si. <br>  - <em>Seu</em> db <em>favorito</em> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="Para todos"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Limitações e premissas </h1><br><ul><li>  O artigo não examina o custo das soluções vps / vds no mercado, bem como a possibilidade de implantar máquinas nesses serviços.  Supõe-se que você já tenha algo expandido ou poderá fazê-lo você mesmo.  Além disso, a instalação / configuração do seu banco de dados favorito e repositório do docker privado, se você precisar de um, não é coberta. </li><li>  Podemos usar os plugins e a docker containserd + cni.  Este artigo não considera o uso do Docker como uma ferramenta de contêiner.  Se você quiser usar o docker, poderá configurar a <a href="">flanela de acordo</a> . Além disso, você precisará configurar o kubelet, ou seja, remover todas as opções relacionadas ao containererd.  Como minhas experiências mostraram, a janela de encaixe e o container em diferentes nós como contêineres funcionarão corretamente. </li><li> Não podemos usar <code>host-gw</code> back <code>host-gw</code> end <code>host-gw</code> para flanela, leia a seção <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configuração de flanela</a> para obter mais detalhes </li><li>  Não usaremos nada para monitorar, fazer backups, salvar arquivos do usuário (status), armazenar arquivos de configuração e código do aplicativo (git / hg / svn / etc) </li></ul><br><h1 id="vvedenie">  1. Introdução </h1><br><p>  No decorrer do trabalho, usei um grande número de fontes, mas quero mencionar separadamente um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes</a> bastante detalhado, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">o</a> guia da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">maneira mais difícil</a> , que cobre cerca de 90% da configuração básica de seu próprio cluster.  Se você já leu este manual, pode prosseguir com segurança diretamente para a seção <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Configuração</a> da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">flanela</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Designações</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  Lista de Termos / Glossário </h2><br><ul><li>  api-server - uma máquina física ou virtual na qual está localizado um conjunto de aplicativos para execução e funcionamento correto do kubernetes kube-apiserver.  Para os fins deste artigo, é etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - uma estação de trabalho dedicada ou instalação do VPS, sinônimo de api-server. </li><li>  nó X - uma estação de trabalho dedicada ou instalação VPS, <code>X</code> indica o número de série da estação.  Neste artigo, todos os números são únicos e são essenciais para a compreensão: <br><ul><li>  nó-1 - número da máquina 1 </li><li>  nó-2 - número da máquina 2 </li></ul></li><li>  vCPU - CPU virtual, núcleo do processador.  O número corresponde ao número de núcleos: 1vCPU - um núcleo, 2vCPU - dois e assim por diante. </li><li>  usuário - usuário ou espaço do usuário.  Ao usar o <code>user$</code> nas instruções da linha de comando, o termo refere-se a qualquer máquina cliente. </li><li>  worker - o nó de trabalho no qual os cálculos diretos serão executados, como sinônimo de <code>node-X</code> </li><li>  resource é a entidade na qual o cluster Kubernetes opera.  Os recursos do Kubernetes incluem um grande número de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">entidades relacionadas</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Soluções de arquitetura de rede </h1><br><p>  No processo de aumento do cluster, não defini a tarefa de otimizar os recursos de ferro de forma a caber no orçamento de US $ 20 por mês.  Era apenas necessário montar um cluster de trabalho com pelo menos dois nós de trabalho (nós).  Portanto, inicialmente o cluster ficou assim: </p><br><ul><li>  máquina com 2 vCPU / 4G RAM: api-server + node-1 [20 $] </li><li>  máquina com 2 RAM vCPU / 4G: nó-2 [$ 20] </li></ul><br><p>  Depois que a primeira versão do cluster funcionou, decidi reconstruí-lo para distinguir entre os nós responsáveis ​​pela execução de aplicativos no cluster (nós de trabalho, eles também são trabalhadores) e a API do servidor mestre. </p><br><p>  Como resultado, recebi a resposta para a pergunta: "Como obter um cluster mais ou menos barato, mas funcional, se eu quiser colocar não os aplicativos mais espessos lá". </p><br><div class="spoiler">  <b class="spoiler_title">Decisão de US $ 20</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Desenho"><br>  (Planejado para ser assim) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">Informações gerais da arquitetura Kubernetes</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Desenho"><br>  (Roubado da Internet se alguém de repente ainda não sabe ou não viu) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Componentes e seu desempenho </h2><br><p>  A primeira etapa foi entender quantos recursos eu preciso para executar pacotes de software diretamente relacionados ao cluster.  A busca por "requisitos de hardware" não deu resultados específicos, então tive que abordar a tarefa de um ponto de vista prático.  Como medida de MEM e CPU, tirei estatísticas do systemd - podemos assumir que as medições foram realizadas de maneira muito amadora, mas não tive a tarefa de obter valores precisos, pois ainda não consegui encontrar opções mais baratas do que US $ 5 por instância. </p><br><div class="spoiler">  <b class="spoiler_title">Por que exatamente $ 5?</b> <div class="spoiler_text"><p>  Foi possível encontrar o VPS / VDS mais barato ao hospedar servidores na Rússia ou na CEI, mas as tristes histórias associadas ao ILV e suas ações criam certos riscos e dão origem a um desejo natural de evitá-los. </p></div></div><br><p>  Então: </p><br><ul><li>  Servidor principal / Configuração do servidor (nós principais): <br><ul><li>  etcd (3.2.17): 80 - 100M, as métricas foram obtidas no tempo selecionado aleatoriamente.  O consumo médio de memória Etcd não excedeu 300M; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237.6M ~ 300M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): aproximadamente 90M, não subiu acima de 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): aproximadamente 20M, o consumo acima de 30-50M não é fixo. </li></ul></li><li>  Configuração do servidor de trabalho (nós de trabalho): <br><ul><li>  kubelet (1.12.3 - 1.13.1): aproximadamente 35 Mb, o consumo acima de 50M não é fixo; </li><li>  kube-proxy (1.12.3 - 1.13.1): aproximadamente 7.5 - 10M; </li><li>  flanela (0.10.0): aproximadamente 15-20M; </li><li>  coredns (1.3.0): aproximadamente 25M; </li><li>  Containerd (1.2.1): O consumo de Containerd é baixo, mas as estatísticas também mostram os processos de contêiner iniciados pelo daemon. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">É necessário oerderd / docker nos nós principais?</b> <div class="spoiler_text"><p>  <strong>Não, não é necessário</strong> .  O nó mestre não requer docker ou container em si, embora exista um grande número de manuais na Internet que, para uma finalidade ou outra, incluem o uso do ambiente para contêiner.  Na configuração em questão, o containerd foi desativado intencionalmente da lista de dependências, no entanto, não realço vantagens óbvias dessa abordagem. </p><br><p>  A configuração fornecida acima é mínima e suficiente para iniciar o cluster.  Nenhuma ação / componente adicional é necessária, a menos que você queira adicionar algo como desejar. </p></div></div><br><p>  Para criar um cluster de teste ou cluster para projetos domésticos, 1vCPU / 1G RAM será suficiente para o nó mestre funcionar.  Obviamente, a carga no nó mestre variará dependendo do número de trabalhadores envolvidos, bem como da disponibilidade e do volume de solicitações de terceiros ao servidor da API. </p><br><p>  Explodi as configurações de mestre e trabalhador da seguinte maneira: </p><br><ul><li>  1x Mestre com componentes instalados: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Trabalhadores com componentes instalados: container, coredns, flanela, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  Configuração </h1><br><p>  Para configurar o assistente, os seguintes componentes são necessários: </p><br><ul><li><p>  etcd - para armazenar dados para api-server, bem como para flanela; </p><br></li><li><p>  kube-apiserver - na verdade, api-server; </p><br></li><li><p>  kube-controller-manager - para gerar e processar eventos; </p><br></li><li><p>  kube-scheduler - para distribuição de recursos registrados através do servidor de API - por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lareira</a> . <br>  Para a configuração dos cavalos de trabalho, são necessários os seguintes componentes: </p><br></li><li><p>  kubelet - para executar as lareiras, definir configurações de rede; </p><br></li><li><p>  kube-proxy - para organizar roteamento / balanceamento de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">serviços</a> kubernetes; </p><br></li><li><p>  coredns - para descoberta de serviços dentro de contêineres em execução; </p><br></li><li><p>  flanela - para organizar o acesso à rede de contêineres operando em diferentes nós, bem como para a distribuição dinâmica de redes entre os nós do cluster (nó kubernetes). </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">Coredns</b> <div class="spoiler_text"><p>  Uma pequena digressão deve ser feita aqui: os coredns também podem ser iniciados no servidor mestre.  Não há restrições que forçam os coredns a serem executados nos nós de trabalho, exceto pela nuance de configuração do coredns.service, que simplesmente não inicia em um servidor Ubuntu padrão / não modificado devido a um conflito com o serviço resolvido pelo sistema.  Não tentei resolver esse problema, pois os servidores de 2 ns localizados nos nós de trabalho estavam muito felizes comigo. </p></div></div><br><p>  Para não perder tempo agora se familiarizando com todos os detalhes do processo de configuração de componentes, sugiro que você se familiarize com eles no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">guia da maneira mais difícil do Kubernetes</a> .  Vou me concentrar nos recursos distintivos da minha opção de configuração. </p><br><h2 id="fayly">  Arquivos </h2><br><p>  Todos os arquivos para o funcionamento dos componentes do cluster para o assistente e os nós de trabalho são colocados em <strong>/ var / lib / kubernetes /</strong> por conveniência.  Se necessário, você pode colocá-los de outra maneira. </p><br><h2 id="sertifikaty">  Certificações </h2><br><p>  A base para a geração de certificados ainda é a mesma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais difícil</a> , praticamente não há diferenças significativas.  Para regenerar certificados subordinados, scripts simples de bash foram escritos em torno de aplicativos <a href="">cfssl</a> - isso foi muito útil no processo de depuração. </p><br><p>  Você pode gerar certificados para suas necessidades usando os scripts abaixo, receitas do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais difícil</a> ou outras ferramentas adequadas. </p><br><div class="spoiler">  <b class="spoiler_title">Geração de certificado usando scripts bash</b> <div class="spoiler_text"><p>  Você pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">obter</a> scripts aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bootstrap do kubernetes</a> .  Antes de iniciar, edite o arquivo <a href="">certs / env.sh</a> , especificando suas configurações.  Um exemplo: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  Se você usou o <code>env.sh</code> e especificou corretamente todos os parâmetros, não há necessidade de tocar nos certificados gerados.  Se você cometeu algum erro em algum momento, os certificados podem ser regenerados em partes.  Os scripts bash acima são triviais, não é difícil classificá-los. </p><br><p>  Uma observação importante - você não deve recriar frequentemente os <code>ca.pem</code> e <code>ca-key.pem</code> , pois são os certificados raiz de todos os certificados subsequentes; em outras palavras, será necessário recriar todos os certificados anexos e entregá-los a todas as máquinas e todos os diretórios necessários. </p></div></div><br><h3 id="master">  O mestre </h3><br><p>  Os certificados necessários para iniciar os serviços no nó principal devem ser colocados em <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - esse certificado é usado em qualquer lugar, pode ser gerado apenas uma vez e depois usado sem alterações, portanto, tenha cuidado.  Ao regenerá-lo, você precisará copiá-lo para todos os nós, bem como atualizar os arquivos kubeconfig usando-o (também em todas as máquinas). </li><li>  ca-key.pem é o mesmo que copiar sobre nós. </li><li>  kube-controller-manager.pem - necessário apenas para o kube-controller-manager. </li><li>  kube-controller-manager-key.pem - necessário apenas para o kube-controller-manager. </li><li><p>  kubernetes.pem - necessário para flanela, coredns ao conectar-se ao etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Retiro teórico</b> <div class="spoiler_text"><p>  Esse recurso é baseado na lógica de configuração do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais difícil</a> . <br>  Com base nisso, esse arquivo será necessário em qualquer lugar - no assistente e nos nós de trabalho.  Não mudei a abordagem fornecida pelo manual original, pois com sua ajuda é possível organizar a operação do cluster de maneira mais rápida e clara e entender todo o conjunto de dependências. </p><br><p>  Minha opinião pessoal é que, para o etcd, você precisa de certificados separados que não se sobreponham aos certificados usados ​​pelo kubernetes. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - permanece nos servidores principais. </li><li>  service-account.pem - necessário apenas para daemons do kube-controller-manager. </li><li>  service-account-key.pem - da mesma forma. </li></ul><br><h3 id="rabochie-uzly">  Unidades de Trabalho </h3><br><ul><li>  ca.pem - necessário para todos os serviços envolvidos nos nós de trabalho (kubelet, kube-proxy), bem como para flanela, coredns.  Entre outras coisas, seu conteúdo é incluído nos arquivos kubeconfig quando são gerados usando o kubectl. </li><li>  kubernetes-key.pem - necessário apenas para flanela e coredns se conectarem ao etcd, que está localizado no nó principal da api. </li><li>  kubernetes.pem - semelhante ao anterior, necessário apenas para flanela e coredns. </li><li>  kubelet / node-1.pem - chave para a autorização node-1. </li><li>  kubelet / node-1-key.pem - chave para autorização node-1. </li></ul><br><p>  <strong>Importante!</strong>  Se você tiver mais de um nó, cada nó incluirá os arquivos <code>node-X-key.pem</code> , <code>node-X.pem</code> e <code>node-X.kubeconfig</code> dentro do kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Depuração de certificado</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Depuração de certificado </h4><br><p>  Às vezes, pode ser necessário verificar como o certificado está configurado para descobrir quais hosts IP / DNS foram usados ​​para gerá-lo.  O <code>cfssl-certinfo -cert &lt;cert&gt;</code> nos ajudará com isso.  Por exemplo, aprendemos essas informações para o <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  Todos os outros certificados para kubelet e kube-proxy são incorporados diretamente no kubeconfig correspondente. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  Todo o kubeconfig necessário pode ser feito usando o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes da maneira mais difícil</a> , no entanto, aqui algumas diferenças começam.  O manual usa configurações de <code>cni bridge</code> <code>kubedns</code> e <code>kubedns</code> , também abrange <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">corns</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">flanela</a> .  Esses dois serviços, por sua vez, usam o <code>kubeconfig</code> para <code>kubeconfig</code> no cluster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  O mestre </h3><br><p>  Para o assistente, os seguintes arquivos kubeconfig são necessários (como mencionado acima, após a geração eles podem ser obtidos em <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  Esses arquivos serão necessários para executar cada um dos componentes de serviço. </p><br><h3 id="rabochie-uzly-1">  Unidades de Trabalho </h3><br><p>  Para nós de trabalho, os seguintes arquivos kubeconfig são necessários: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¦  L-- coredns.kubeconfig +-- flanneld ¦  L-- flanneld.kubeconfig +-- kubelet ¦  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Lançamento do serviço </h2><br><div class="spoiler">  <b class="spoiler_title">Serviços</b> <div class="spoiler_text"><p>  Apesar do fato de meus nós de trabalho usarem sistemas de inicialização diferentes, os exemplos e o repositório oferecem opções usando systemd.  Com a ajuda deles, é mais fácil entender qual processo e com quais parâmetros você precisa iniciar; além disso, eles não devem causar grandes problemas ao estudar serviços com sinalizadores de destino. </p></div></div><br><p>  Para iniciar os serviços, você precisa copiar <code>service-name.service</code> para <code>/lib/systemd/system/</code> ou qualquer outro diretório em que os serviços do systemd estejam localizados e, em seguida, ligue e inicie o serviço.  Exemplo para kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Obviamente, todos os serviços devem ser <em>ecológicos</em> (ou seja, em execução e funcionando).  Se você encontrar um erro, os <code>journalct -xe</code> ou <code>journal -f -t kube-apiserver</code> ajudarão a entender o que exatamente deu errado. </p><br><p>  Não se apresse em iniciar todos os servidores de uma só vez; para começar, será suficiente ativar o etcd e o kube-apiserver.  Se tudo correu bem e você ganhou imediatamente todos os quatro serviços no assistente, o lançamento do assistente pode ser considerado bem-sucedido. </p><br><h3 id="master-2">  O mestre </h3><br><p>  Você pode usar as configurações do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd</a> ou gerar scripts init para a configuração que está usando.  Como já mencionado, para o mestre você precisa: </p><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / etcd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-apiserver</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-controller-manager</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Unidades de Trabalho </h3><br><p>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / containerd</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kubelet</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / kube-proxy</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / coredns</a> <br>  - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / flanela</a> </p><br><h3 id="klient">  Cliente </h3><br><p>  Para que o cliente funcione, basta copiar <code>certs/kubeconfig/admin.kubeconfig</code> (depois de gerá-lo ou gravá-lo você mesmo) em <code>${HOME}/.kube/config</code> </p><br><p>  Faça o download do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubectl</a> e verifique o funcionamento do kube-apiserver.  Deixe-me lembrá-lo mais uma vez de que, neste estágio, para que o kube-apiserver funcione, apenas o etcd deve funcionar.  Os componentes restantes serão necessários para a operação completa do cluster um pouco mais tarde. </p><br><p>  Verifique se o kube-apiserver e o kubectl funcionam: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Configuração de flanela </h1><br><p>  Como uma configuração de flanela, decidi pelo back-end do <code>vxlan</code> .  Leia mais sobre back-end <a href="">aqui</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw e por que não vai funcionar</b> <div class="spoiler_text"><p>  Devo dizer imediatamente que a execução de um cluster kubernetes em um VPS provavelmente limitará você a usar o back <code>host-gw</code> end <code>host-gw</code> .  Não sendo um engenheiro de rede experiente, passei cerca de dois dias depurando para entender qual era o problema com seu uso em provedores populares de VDS / VPS. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Linode.com</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">digitalocean</a> foram testados.  A essência do problema é que os provedores não fornecem L2 honesto para uma rede privada.  Isso, por sua vez, torna impossível mover o tráfego de rede entre os nós nesta configuração: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Tráfego"></p><br><p>  Para que o tráfego de rede funcione entre os nós, o roteamento normal será suficiente.  Não esqueça que net.ipv4.ip_forward deve ser definido como 1 e a cadeia FORWARD na tabela de filtros não deve conter regras de proibição para nós. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  É exatamente isso que não funciona no VPS / VDS indicado (e, provavelmente, geralmente em todos). </p><br><p>  Portanto, se a configuração de uma solução com alto desempenho de rede entre os nós <strong>for importante para</strong> você, você ainda precisará gastar mais de US $ 20 para organizar o cluster. </p></div></div><br><p>  Você pode usar o <code>set-flannel-config.sh</code> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">etc / flannel</a> para definir a configuração de flanela desejada.  <strong>É importante lembrar</strong> : se você decidir alterar o back-end, será necessário excluir a configuração no etcd e reiniciar todos os daemons de flanela em todos os nós; portanto, escolha-o com sabedoria.  O padrão é vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  Depois de registrar a configuração desejada no etcd, você precisa configurar o serviço para executá-lo em cada um dos nós em funcionamento. </p><br><h2 id="flannelservice">  flannel.service </h2><br><p>  Um exemplo para o serviço pode ser obtido aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">flannel.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Personalização </h2><br><p>  Como descrito anteriormente, precisamos dos arquivos ca.pem, kubernetes.pem e kubernetes-key.pem para autorização no etcd.  Todos os outros parâmetros não possuem nenhum significado sagrado.  A única coisa que é realmente importante é configurar o endereço IP global através do qual os pacotes de rede irão entre redes: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Rede de flanela"><br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sobreposição de rede de vários hosts com flanela</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  Depois que a flanela iniciar com êxito, você deverá encontrar a interface de rede flannel.N em seu sistema: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Verificar se suas interfaces estão funcionando corretamente em todos os nós é bastante simples.  No meu caso, o nó 1 e o nó 2 têm redes 10.200.8.0/24 e 10.200.12.0/24, respectivamente, portanto, usando uma solicitação icmp normal, verifique sua disponibilidade: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  Em caso de problemas, é recomendável verificar se existem regras de corte nas tabelas de ip sobre UDP entre hosts. </p><br><h1 id="konfiguraciya-containerd">  Configuração Containerd </h1><br><p>  Coloque o arquivo <a href="">etc / containserd / config.toml</a> em <code>/etc/containerd/config.toml</code> ou sempre que for conveniente para você, o principal é lembrar de alterar o caminho para o arquivo de configuração no serviço (containerd.service, descrito abaixo). </p><br><p>  Configuração com algumas modificações do padrão.  <strong>É importante não definir</strong> <code>enable_tls_streaming = true</code> se você não entender por que está fazendo isso.    <code>kubectl exec</code>        ,      . </p><br><h2 id="containerdservice"> containerd.service </h2><br><div class="spoiler"> <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Personalização </h2><br><p>  ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Personalização </h1><br><p>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Red Hat  Docker  Podman</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) —    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               —     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Personalização </h2><br><p>      ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Personalização </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Personalização </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> — <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=http://no-">http://no-https.kubernetes-example.w40k.net/</a> —  ssl;  ,  -   ,     . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://kubernetes-example.w40k.net/</a> —   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,        . </p><br><h1 id="ssylki">  Referências </h1><br><p> ,     ,   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes the hard way</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Multi-Host Networking Overlay with Flannel</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Intro to Podman</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Stateless Applications</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">What is ingress</a> </p><br><p>   : </p><br><p> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kubernetes Networking: Behind the scenes</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A Guide to the Kubernetes Networking Model</a> <br> — <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Understanding kubernetes networking: services</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            —    . </p><br><p>              . , ,   , —      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> (  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> </p></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt435228/">https://habr.com/ru/post/pt435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt435214/index.html">Lançamento do Linux 4.20 - o que mudou na nova versão do kernel</a></li>
<li><a href="../pt435216/index.html">Como criar 200 a partir de duas linhas de código e por que você precisa fazer isso</a></li>
<li><a href="../pt435220/index.html">Kotlin Native: acompanhe os arquivos</a></li>
<li><a href="../pt435224/index.html">Como se comunicar em um escritório em inglês: 14 expressões úteis</a></li>
<li><a href="../pt435226/index.html">Restaurar dados do zero</a></li>
<li><a href="../pt435234/index.html">Mais inteligente, além disso, mais precisamente: como a IA transforma os voos no espaço</a></li>
<li><a href="../pt435236/index.html">Byte-machine para o forte (e não apenas) no nativo americano (parte 3)</a></li>
<li><a href="../pt435240/index.html">Unreal Engine4 - efeito de verificação pós-processo</a></li>
<li><a href="../pt435242/index.html">Por que tenho medo de me tornar um "homem bombeado"</a></li>
<li><a href="../pt435244/index.html">Projeto ITER em 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>