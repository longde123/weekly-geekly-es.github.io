<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïç üîñ „äôÔ∏è Agentes de aprendizaje autom√°tico en Unity üëãüèø ‚òëÔ∏è üë≤üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este art√≠culo sobre agentes de aprendizaje autom√°tico en Unity fue escrito por Michael Lanham, un innovador t√©cnico, desarrollador activo de Unity, co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Agentes de aprendizaje autom√°tico en Unity</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="imagen"></div><br>  <em>Este art√≠culo sobre agentes de aprendizaje autom√°tico en Unity fue escrito por Michael Lanham, un innovador t√©cnico, desarrollador activo de Unity, consultor, gerente y autor de muchos juegos, proyectos gr√°ficos y libros de Unity.</em> <br><br>  Los desarrolladores de Unity han implementado el soporte para el aprendizaje autom√°tico y, en particular, el aprendizaje por refuerzo en aras de crear SDK de aprendizaje por refuerzo profundo (DRL) para desarrolladores de juegos y simulaci√≥n.  Afortunadamente, el equipo de Unity, dirigido por Danny Lange, ha implementado con √©xito un motor DRL confiable y moderno capaz de ofrecer resultados impresionantes.  Unity utiliza el modelo de optimizaci√≥n de pol√≠tica proximal (PPO) como base del motor DRL;  Este modelo es mucho m√°s complejo y puede diferir en algunos aspectos. <br><br>  En este art√≠culo, te presentar√© las herramientas y los SDK para crear agentes DRL en juegos y simulaciones.  A pesar de la novedad y el poder de esta herramienta, es f√°cil de usar y tiene herramientas auxiliares que le permiten aprender conceptos de aprendizaje autom√°tico sobre la marcha.  Para trabajar con el tutorial, necesita instalar el motor de Unity. <br><a name="habracut"></a><br><h2>  Instalar agentes ML </h2><br>  En esta secci√≥n, hablar√© brevemente sobre los pasos que se deben seguir para instalar el SDK de ML-Agents.  Este material a√∫n est√° en versi√≥n beta y puede variar de una versi√≥n a otra.  Sigue estos pasos: <br><br><ol><li>  Instale Git en la computadora;  Funciona desde la l√≠nea de comando.  Git es un sistema de gesti√≥n de c√≥digo fuente muy popular, y hay muchos recursos en Internet sobre la instalaci√≥n y el uso de Git en todas las plataformas.  Despu√©s de instalar Git, aseg√∫rese de que funcione creando un clon de cualquier repositorio. </li><li>  Abra un s√≠mbolo del sistema o un shell normal.  Los usuarios de Windows pueden abrir la ventana de Anaconda. </li><li>  Vaya a la carpeta de trabajo donde desea colocar su nuevo c√≥digo e ingrese el siguiente comando (los usuarios de Windows pueden seleccionar C: \ ML-Agents): <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents </pre></li><li>  As√≠ que clonas el repositorio de ml-agents en tu computadora y creas una nueva carpeta con el mismo nombre.  Tambi√©n puede agregar un n√∫mero de versi√≥n al nombre de la carpeta.  La unidad, como casi todo el mundo de la inteligencia artificial, cambia constantemente, al menos por ahora.  Esto significa que constantemente aparecen nuevos cambios.  Al momento de escribir, estamos clonando el repositorio en la carpeta ml-agents.6: <br><br><pre>  git clone https://github.com/Unity-Technologies/ml-agents ml-agents.6 </pre></li><li>  Cree un nuevo entorno virtual para ml-agents y especifique la versi√≥n 3.6, como esta: <br><br><pre>  #Windows 
 conda create -n ml-agents python = 3.6
 
 #Mac
 Use la documentaci√≥n para su entorno preferido </pre></li><li>  Activa tu entorno nuevamente con Anaconda: <br><br><pre>  activar ml-agentes </pre></li><li>  Instale TensorFlow.  En Anaconda, esto se puede hacer con el siguiente comando: <br><br><pre>  pip install tensorflow == 1.7.1 </pre></li><li>  Instalar paquetes de Python.  En Anaconda, ingrese lo siguiente: <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li>  Por lo tanto, instala todos los paquetes necesarios del SDK de Agentes;  Esto puede tomar varios minutos.  No cierre la ventana, pronto ser√° √∫til. </li></ol><br>  As√≠ que instalamos y configuramos Unity Python SDK para ML-Agents.  En la siguiente secci√≥n, aprenderemos c√≥mo configurar y capacitar uno de los muchos entornos proporcionados por Unity. <br><br><h2>  Entrenamiento de agente </h2><br>  Ahora podemos ponernos manos a la obra de inmediato y explorar ejemplos que utilizan el aprendizaje de refuerzo profundo (DRL).  Afortunadamente, hay varios ejemplos en el conjunto de herramientas del nuevo agente para demostrar la potencia del motor.  Abra Unity o Unity Hub y siga estos pasos: <br><br><ol><li>  Haga clic en el bot√≥n Abrir proyecto en la parte superior del cuadro de di√°logo Proyecto. </li><li>  Ubique y abra la carpeta del proyecto UnitySDK, como se muestra en la captura de pantalla: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>Abra el proyecto Unity SDK</i> </li><li>  Espere a que se cargue el proyecto y luego abra la ventana Proyecto en la parte inferior del editor.  Si se abre una ventana pidi√©ndole que actualice el proyecto, seleccione s√≠ o contin√∫e.  Actualmente, todo el c√≥digo de agente es compatible con versiones anteriores. </li><li>  Localice y abra la escena GridWorld como se muestra en la captura de pantalla: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>Abrir un ejemplo de una escena de GridWorld</em> </li><li>  Seleccione el objeto GridAcademy en la ventana Jerarqu√≠a. </li><li>  Vaya a la ventana del Inspector y al lado del campo Cerebros, haga clic en el icono para abrir el cuadro de di√°logo Selecci√≥n de cerebro: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li>  Seleccione el cerebro del GridWorldPlayer.  Este cerebro pertenece al jugador, es decir, el jugador (usted) puede controlar el juego. </li><li>  Haga clic en el bot√≥n Reproducir en la parte superior del editor y observe el entorno.  Como el juego ahora est√° configurado para controlar al jugador, puedes usar las teclas WASD para mover el cubo.  La tarea es mover el cubo azul al s√≠mbolo verde +, evitando la X roja. </li></ol><br>  Ponte c√≥modo en el juego.  Tenga en cuenta que el juego solo funciona durante un cierto per√≠odo de tiempo y no est√° basado en turnos.  En la siguiente secci√≥n, aprenderemos c√≥mo ejecutar este ejemplo con el agente DRL. <br><br><h2>  ¬øQu√© hay en el cerebro? </h2><br>  Uno de los aspectos sorprendentes de la plataforma ML-Agents es la capacidad de cambiar r√°pida y f√°cilmente de la gesti√≥n de jugadores a la gesti√≥n de AI / agente.  Para esto, Unity utiliza el concepto de "cerebro".  El cerebro puede ser controlado por el jugador o por el agente (cerebro de aprendizaje).  Lo m√°s sorprendente es que puedes armar el juego y probarlo como jugador, y luego darlo bajo el control de un agente de RL.  Gracias a esto, cualquier juego escrito con un poco de esfuerzo puede hacerse para ser controlado usando IA. <br><br>  El proceso de configurar e iniciar la capacitaci√≥n de agentes RL en Unity es bastante simple.  Unity usa Python externo para construir un modelo del cerebro de aprendizaje.  Usar Python tiene mucho sentido porque ya hay varias bibliotecas de aprendizaje profundo (DL) construidas a su alrededor.  Para capacitar al agente en GridWorld, complete los siguientes pasos: <br><br><ol><li>  Seleccione GridAcademy nuevamente y seleccione el cerebro GridWorldLearning en el campo Brains en lugar de GridWorldPlayer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>Cambio al uso de GridWorldLearning Brain</em> </li><li>  Marque la casilla Control a la derecha.  Este par√°metro simple informa que el cerebro puede controlarse externamente.  Esta opci√≥n debe estar habilitada. </li><li>  Seleccione el objeto trueAgent en la ventana Jerarqu√≠a y luego, en la ventana Inspector, cambie la propiedad Cerebro del componente Agente de cuadr√≠cula al cerebro Aprendizaje mundial de Grid: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>Trabajo de GridWorldLearning brain para agente</em> </li><li>  En este ejemplo, necesitamos que tanto Academy como Agent utilicen el mismo cerebro de GridWorldLearning.  Cambie a la ventana Anaconda o Python y seleccione la carpeta ML-Agents / ml-agents. </li><li>  Ejecute el siguiente comando en una ventana de Anaconda o Python utilizando el entorno virtual ml-agents: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = firstRun --train </pre></li><li>  Esto lanzar√° el modelo de capacitaci√≥n Unity PPO y un agente de ejemplo con la configuraci√≥n especificada.  En cierto punto, la ventana del s√≠mbolo del sistema le pedir√° que inicie el editor de Unity con el entorno cargado. </li><li>  Haga clic en Reproducir en el editor de Unity para iniciar el entorno GridWorld.  Poco despu√©s, deber√≠a ver el entrenamiento del agente y la salida a la ventana del script Python: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>Ejecutar GridWorld en modo de aprendizaje</em> </li><li>  Tenga en cuenta que el script mlagents-learn es un c√≥digo de Python que crea un modelo RL para ejecutar un agente.  Como puede ver en el resultado de la secuencia de comandos, hay varios par√°metros (hiperpar√°metros) que deben configurarse. </li><li>  Deje que el agente aprenda algunos miles de iteraciones y observe qu√© tan r√°pido aprende.  El modelo interno utilizado aqu√≠ llamado PPO ha demostrado ser un modelo de aprendizaje muy efectivo para muchas tareas diferentes, y es muy adecuado para el desarrollo de juegos.  Con un equipo suficientemente potente, un agente puede aprender idealmente en menos de una hora. </li></ol><br>  Permita que el agente aprenda m√°s y explore otras formas de rastrear el proceso de aprendizaje del agente, como se presenta en la siguiente secci√≥n. <br><br><h2>  Monitoreo del aprendizaje con TensorBoard </h2><br>  Capacitar a un agente utilizando el modelo RL o cualquier modelo DL a menudo es una tarea desalentadora y requiere atenci√≥n al detalle.  Afortunadamente, TensorFlow tiene un conjunto de herramientas de gr√°ficos llamado TensorBoard que puede usar para monitorear su proceso de aprendizaje.  Siga estos pasos para iniciar TensorBoard: <br><br><ol><li>  Abra una ventana de Anaconda o Python.  Active el entorno virtual de ml-agents.  No cierre la ventana en la que se ejecuta el modelo de entrenamiento;  Lo necesitamos para continuar. </li><li>  Vaya a la carpeta ML-Agents / ml-agents y ejecute el siguiente comando: <br><br><pre>  tensorboard --logdir = res√∫menes </pre></li><li>  Entonces lanzamos TensorBoard en nuestro propio servidor web incorporado.  Puede cargar la p√°gina utilizando la URL que se muestra despu√©s del comando anterior. </li><li>  Ingrese la URL para el TensorBoard como se muestra en la ventana, o escriba localhost: 6006 o machinename: 6006 en el navegador.  Despu√©s de aproximadamente una hora, deber√≠a ver algo como esto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>Ventana de tabla de TensorBoard</em> </li><li>  La captura de pantalla anterior muestra gr√°ficos, cada uno de los cuales muestra un aspecto separado del entrenamiento.  Para comprender c√≥mo se capacita a nuestro agente, debe lidiar con cada uno de estos gr√°ficos, por lo que analizaremos el resultado de cada secci√≥n: </li></ol><br><ul><li>  Entorno: esta secci√≥n muestra c√≥mo el agente se manifiesta en el entorno en su conjunto.  A continuaci√≥n se muestra una vista m√°s detallada de los gr√°ficos con la tendencia preferida: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>Una imagen detallada de los gr√°ficos de la secci√≥n Medio ambiente.</em> <br><br><ul><li>  Recompensa acumulativa: esta es la recompensa total que maximiza al agente.  Por lo general, es necesario que aumente, pero por alguna raz√≥n puede disminuir.  Siempre es mejor maximizar las recompensas entre 1 y -1.  Si las recompensas de horario van m√°s all√° de este rango, entonces esto tambi√©n necesita ser arreglado. </li><li>  Duraci√≥n del episodio: si este valor disminuye, generalmente es una buena se√±al.  En √∫ltima instancia, cuanto m√°s cortos son los episodios, m√°s entrenamiento.  Sin embargo, tenga en cuenta que si es necesario, la duraci√≥n de los episodios puede aumentar, por lo que la imagen puede ser diferente. </li><li>  Lecci√≥n: este cuadro deja en claro en qu√© lecci√≥n se encuentra el agente;  Est√° destinado al aprendizaje curricular. </li><li>  P√©rdidas: esta secci√≥n muestra gr√°ficos que representan las p√©rdidas o los costos calculados para la p√≥liza y el valor.  A continuaci√≥n se muestra una captura de pantalla de esta secci√≥n con flechas que apuntan a la configuraci√≥n √≥ptima: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>P√©rdidas y entrenamiento preferido</em> </li></ul><br><ul><li>  P√©rdida de p√≥liza: este cuadro determina la cantidad de cambio de p√≥liza a lo largo del tiempo.  La pol√≠tica es un elemento que define acciones, y en el caso general, este cronograma debe tender a la baja, lo que demuestra que la pol√≠tica est√° tomando mejores decisiones. </li><li>  P√©rdida de valor: esta es la p√©rdida promedio de la funci√≥n de valor.  En esencia, modela qu√© tan bien el agente predice el valor de su pr√≥ximo estado.  Inicialmente, este valor deber√≠a aumentar, y despu√©s de la estabilizaci√≥n de la remuneraci√≥n, deber√≠a disminuir. </li><li>  Pol√≠tica: para evaluar la calidad de las acciones en PPO, se utiliza el concepto de una pol√≠tica, no un modelo.  La siguiente captura de pantalla muestra los cuadros de pol√≠ticas y la tendencia preferida: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>Gr√°ficos de pol√≠ticas y tendencias preferidas</i> </li><li>  Entrop√≠a: este gr√°fico muestra la magnitud del agente de investigaci√≥n.  Es necesario reducir este valor, porque el agente aprende m√°s sobre el medio ambiente y necesita menos investigaci√≥n. </li><li>  Tasa de aprendizaje: en este caso, este valor deber√≠a disminuir gradualmente linealmente. </li><li>  Estimaci√≥n del valor: este es el valor promedio visitado por todos los estados del agente.  Para reflejar el mayor conocimiento de un agente, este valor debe crecer y luego estabilizarse. </li></ul><br>  6. Deje el agente en funcionamiento hasta que se complete y no cierre el TensorBoard. <br>  7. Regrese a la ventana Anaconda / Python que entren√≥ al cerebro y ejecute este comando: <br><br><pre>  mlagents-learn config / trainer_config.yaml --run-id = secondRun --train </pre><br>  8. Nuevamente se le pedir√° que haga clic en Reproducir en el editor;  entonces hazlo.  Deje que el agente comience a entrenar y realice varias sesiones.  En el proceso, observe la ventana de TensorBoard y observe c√≥mo se muestra secondRun en los gr√°ficos.  Puede dejar que este agente se ejecute hasta su finalizaci√≥n, pero puede detenerlo si lo desea. <br><br>  En versiones anteriores de ML-Agents, primero ten√≠a que construir el ejecutable de Unity como entorno de aprendizaje para el juego y luego ejecutarlo.  El cerebro externo de Python deber√≠a haber funcionado de la misma manera.  Este m√©todo hizo muy dif√≠cil depurar problemas en el c√≥digo o en el juego.  En la nueva t√©cnica, se eliminaron todas estas dificultades. <br><br>  Ahora que hemos visto lo f√°cil que es configurar y entrenar al agente, pasaremos a la siguiente secci√≥n, en la que aprenderemos c√≥mo ejecutar el agente sin el cerebro externo de Python y ejecutarlo directamente en Unity. <br><br><h2>  Lanzamiento de agente </h2><br>  El entrenamiento de Python es genial, pero no puedes usarlo en un juego real.  Idealmente, nos gustar√≠a construir un gr√°fico TensorFlow y usarlo en Unity.  Afortunadamente, se cre√≥ la biblioteca TensorFlowSharp que permite a .NET usar gr√°ficos TensorFlow.  Esto nos permite construir modelos TFModels fuera de l√≠nea y luego inyectarlos en el juego.  Desafortunadamente, solo podemos usar modelos entrenados, pero no entrenarlos de esa manera, al menos no todav√≠a. <br><br>  Veamos c√≥mo funciona esto, usando el ejemplo del gr√°fico que acabamos de entrenar para el entorno GridWorld;  √öselo como un cerebro interno en la Unidad.  Siga los pasos en la siguiente secci√≥n para configurar y usar su cerebro interno: <br><br><ol><li>  Descargue el complemento TFSharp <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" rel="external nofollow">desde aqu√≠</a> </li><li>  En el men√∫ del editor, seleccione Activos |  Paquete de importaci√≥n |  Paquete personalizado ... </li><li>  Encuentre el paquete de activos que acaba de descargar y use los cuadros de di√°logo de importaci√≥n para cargar el complemento en el proyecto. </li><li>  Desde el men√∫, seleccione Editar |  Configuraci√≥n del proyecto.  Se abre la ventana Configuraci√≥n (apareci√≥ en la versi√≥n 2018.3) </li><li>  Encuentre los caracteres de Scripting Define Symbols en las opciones del reproductor y cambie el texto a ENABLE_TENSORFLOW, y tambi√©n habilite Permitir c√≥digo inseguro, como se muestra en la captura de pantalla: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>Establecer la bandera ENABLE_TENSORFLOW</em> </li><li>  Busque el objeto GridWorldAcademy en la ventana Jerarqu√≠a y aseg√∫rese de que utiliza Brains |  GridWorldLearning.  Deshabilite la opci√≥n Control en la secci√≥n Cerebros del script de Grid Academy. </li><li>  Encuentre el cerebro de GridWorldLearning en la carpeta Activos / Ejemplos / GridWorld / Brains y aseg√∫rese de que el par√°metro Modelo en la ventana del Inspector est√© configurado, como se muestra en la captura de pantalla: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>Tarea modelo para el cerebro</em> </li><li>  GridWorldLearning ya deber√≠a estar configurado como modelo.  En este ejemplo, usamos el TFModel que viene con el ejemplo de GridWorld. </li><li>  Haga clic en Reproducir para iniciar el editor y ver c√≥mo el agente administra el cubo. </li></ol><br>  Ahora estamos lanzando el entorno pre-entrenado de Unity.  En la siguiente secci√≥n, aprenderemos c√≥mo usar el cerebro que entrenamos en la secci√≥n anterior. <br><br><h2>  Carga cerebral entrenada </h2><br>  Todos los ejemplos de Unity tienen cerebros pre-entrenados que pueden usarse para estudiar ejemplos.  Por supuesto, queremos poder cargar nuestros propios gr√°ficos TF en Unity y ejecutarlos.  Para cargar un gr√°fico entrenado, siga estos pasos: <br><br><ol><li>  Vaya a la carpeta ML-Agents / ml-agents / models / firstRun-0.  Dentro de esta carpeta est√° el archivo GridWorldLearning.bytes.  Arrastre este archivo a la carpeta Proyecto / Activos / Agentes ML / Ejemplos / GridWorld / TFModels dentro del editor de Unity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>Arrastrando un gr√°fico de bytes a Unity</em> </li><li>  Por lo tanto, importamos el gr√°fico en el proyecto de Unity como un recurso y le cambiamos el nombre a GridWorldLearning 1. El motor hace esto porque el modelo predeterminado ya tiene el mismo nombre. </li><li>  Busque GridWorldLearning en la carpeta del cerebro, selecci√≥nelo en la ventana del Inspector y arrastre el nuevo modelo GridWorldLearning 1 al campo Modelo de los par√°metros de Par√°metros cerebrales: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>Cargando el cerebro en el campo Modelo de gr√°fico</em> </li><li>  En esta etapa, no necesitamos cambiar ning√∫n otro par√°metro, sino prestar especial atenci√≥n a c√≥mo est√° configurado el cerebro.  Por ahora, la configuraci√≥n est√°ndar servir√°. </li><li>  Haga clic en Jugar en el editor de Unity y vea c√≥mo el agente se mueve con √©xito en el juego. </li><li>  El √©xito del agente en el juego depende del tiempo de su entrenamiento.  Si le permites completar el entrenamiento, el agente ser√° similar a un agente de Unity completamente entrenado. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/454612/">https://habr.com/ru/post/454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454600/index.html">C√≥mo hicimos un trato seguro en Freelansim: elija, reduzca funciones, compare comisiones</a></li>
<li><a href="../454604/index.html">Generando una aplicaci√≥n React con un backend GraphQL en minutos</a></li>
<li><a href="../454606/index.html">Caracter√≠sticas del atributo inputmode para SO y navegadores m√≥viles</a></li>
<li><a href="../454608/index.html">Acuerdo de nivel de servicio: escribimos SLA para ... otros, o la conclusi√≥n de un SLA con un operador de telecomunicaciones</a></li>
<li><a href="../454610/index.html">Marketing de contenidos, SEO, pruebas y encuestas: 9 herramientas para promover una startup en el extranjero</a></li>
<li><a href="../454614/index.html">XXE: entidad externa XML</a></li>
<li><a href="../454616/index.html">El uso de IA para aumentar la eficiencia de los trabajadores mentales</a></li>
<li><a href="../454618/index.html">Foso de productividad: c√≥mo da√±a Slack nuestro flujo de trabajo</a></li>
<li><a href="../454620/index.html">#NoDeployFriday: ¬øayuda o perjudica?</a></li>
<li><a href="../454622/index.html">Kreisel EVEX 910e: modelo hist√≥rico - nueva vida</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>