<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•† ‚ö∞Ô∏è üòæ Nous testons SharxBase, une plate-forme de virtualisation logicielle et mat√©rielle du fournisseur russe SharxDC üí™üèº üéΩ ü§≤üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Aujourd'hui, je vais parler de la plateforme hyperconverg√©e SharxBase. Il n'y a pas eu d'examen de ce complexe sur Habr√©, et il a √©t√© d√©cid√© de mettre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nous testons SharxBase, une plate-forme de virtualisation logicielle et mat√©rielle du fournisseur russe SharxDC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/429042/">  Aujourd'hui, je vais parler de la plateforme hyperconverg√©e SharxBase.  Il n'y a pas eu d'examen de ce complexe sur Habr√©, et il a √©t√© d√©cid√© de mettre fin √† cette injustice.  Notre √©quipe a r√©ussi √† tester la solution "en combat", les r√©sultats sont ci-dessous. <br><br><img src="https://habrastorage.org/webt/my/e0/-f/mye0-flo3itmlmf2o1zry6tu-eg.png" alt="image"><br><br>  PS Il y a beaucoup de tables, de vrais chiffres et d'autres ¬´viandes¬ª sous la coupe.  Pour ceux qui sont plong√©s dans l'essence - bienvenue! <br><a name="habracut"></a><br><h2>  √Ä propos du produit </h2><br>  La plateforme SharxBase est bas√©e sur des serveurs Intel et des logiciels open source OpenNebula et StorPool.  Il se pr√©sente sous la forme d'une solution en bo√Æte, qui comprend du mat√©riel serveur avec une virtualisation pr√©install√©e et un logiciel de stockage distribu√©. <br><br>  L'une des quatre configurations standard de base - Petite, Moyenne, Grande, Stockage - est disponible sur commande, qui diff√®rent par la quantit√© de ressources informatiques disponibles (processeurs, RAM) et l'espace disque.  Les serveurs sont con√ßus comme des modules: un ch√¢ssis 2RU typique, dans lequel jusqu'√† quatre serveurs peuvent √™tre install√©s pour une installation dans un rack de serveur 19 "standard. La plate-forme prend en charge √† la fois une mise √† l'√©chelle horizontale en augmentant le nombre de n≈ìuds et verticale en augmentant la quantit√© de RAM dans les n≈ìuds , installation de lecteurs suppl√©mentaires et de cartes d'extension.Nous prenons actuellement en charge l'installation d'adaptateurs r√©seau, de modules de contr√¥le de d√©marrage, de lecteurs NVMe. <br><br><h2>  Architecture de stockage </h2><br>  Pour l'organisation des disques flash de stockage √† tol√©rance de panne distribu√©s (SSD et / ou NVMe) sont utilis√©s.  Le support utilis√© est Ethernet.  Pour transf√©rer le stockage de stockage, il est n√©cessaire d'utiliser des interfaces r√©seau d√©di√©es - au moins deux interfaces 25 GbE.  Services qui fournissent un travail de stockage distribu√© sur chaque serveur du cluster et utilisent une partie de ses ressources informatiques.  La quantit√© de ressources d√©pend du nombre et du volume de disques install√©s, en moyenne, la surcharge est de 34 Go de RAM par h√¥te.  La connexion au stockage distribu√© se fait via le protocole d'acc√®s au bloc iSCSI.  Pour garantir la tol√©rance aux pannes, la sauvegarde des donn√©es deux ou trois fois est prise en charge.  Pour les installations productives, le fabricant recommande d'utiliser une triple redondance.  Actuellement, √† partir des technologies d'optimisation du stockage, seul le provisionnement fin est pris en charge.  La d√©duplication et la compression des donn√©es √† l'aide du stockage distribu√© ne sont pas prises en charge.  Les futures versions prendront en charge le codage d'effacement. <br><br><h2>  Virtualisation </h2><br>  Pour d√©marrer une machine virtuelle (VM), l'hyperviseur KVM est utilis√©.  Toutes les fonctionnalit√©s de base pour leur cr√©ation et leur gestion sont prises en charge: <br><br><ul><li>  cr√©ation d'une VM √† partir de z√©ro avec indication de la configuration mat√©rielle requise (c≈ìurs de processeur, taille de la RAM, nombre et taille des disques virtuels, nombre d'adaptateurs r√©seau, etc.); </li><li>  Clonage de VM √† partir d'un mod√®le existant ou; </li><li>  cr√©er un instantan√© (instantan√©), supprimer un instantan√©, annuler les modifications apport√©es √† la machine virtuelle √† partir du moment o√π l'instantan√© a √©t√© pris; </li><li>  Modification de la configuration mat√©rielle d'une machine virtuelle cr√©√©e pr√©c√©demment, y compris la connexion ou la d√©connexion d'un disque virtuel ou d'un adaptateur r√©seau pour une machine virtuelle incluse (hotplug / hot unplug); </li><li>  Migration de VM entre serveurs de virtualisation </li><li>  surveillance de l'√©tat de la machine virtuelle, y compris la surveillance de la charge des ressources informatiques et des disques virtuels (taille actuelle, volume d'E / S en Mo / s ou en IOPS); </li><li>  planifier des op√©rations avec des VMs selon un calendrier (allumer, √©teindre, cr√©er un instantan√©, etc.); </li><li>  connexion et gestion de machines virtuelles via les protocoles VNC ou SPICE √† partir d'une console Web. </li></ul><br><img src="https://habrastorage.org/webt/s3/3z/cn/s33zcnamce-04s6gj2jf5oc31iy.png" alt="image"><br>  <i>Sch√©ma fonctionnel typique (4 n≈ìuds)</i> <br><br>  La gestion de la plateforme est effectu√©e √† partir de l'interface graphique ou de la ligne de commande (localement ou √† distance lors de la connexion via SSH), ainsi que via l'API publique. <br><br>  Parmi les limites de la plate-forme de virtualisation, on peut noter l'absence de m√©canismes pour √©quilibrer automatiquement les VM entre les h√¥tes du cluster. <br><br>  En plus de prendre en charge la virtualisation des serveurs, SharxBase a la capacit√© de cr√©er des centres de donn√©es configurables par logiciel et des infrastructures de cloud priv√©.  A titre d'exemple de telles fonctions, on peut noter: <br><br><ul><li>  gestion des droits d'acc√®s en fonction de l'appartenance des utilisateurs aux groupes et des listes de contr√¥le d'acc√®s (ACL): des droits peuvent √™tre attribu√©s √† diff√©rents groupes d'utilisateurs qui restreignent l'acc√®s aux composants de l'infrastructure virtuelle; </li><li>  comptabilit√© de la consommation des ressources (comptabilit√©): processeurs, RAM, ressources disque; </li><li>  Estimation du co√ªt de consommation des ressources informatiques (showback) en unit√©s arbitraires en fonction des ressources consomm√©es et de leurs prix; </li><li>  fonctionnalit√©s de base d'IPAM (IP Address Management): attribution automatique des adresses IP pour les interfaces r√©seau VM √† partir d'une plage pr√©d√©termin√©e; </li><li>  capacit√©s de base de SDN: cr√©ation d'un routeur virtuel pour transf√©rer le trafic entre les r√©seaux virtuels. </li></ul><br>  √Ä l'aide du module de s√©curit√© des informations d√©velopp√©, SharxBase met en ≈ìuvre des mesures suppl√©mentaires pour assurer la s√©curit√© des informations du syst√®me de gestion de la plateforme: exigences personnalisables pour les mots de passe des comptes d'utilisateurs (complexit√©, longueur, dur√©e d'utilisation, r√©p√©tabilit√©, etc.), blocage des utilisateurs, gestion des sessions d'acc√®s actuelles √† la console de gestion, enregistrement √©v√©nements et autres Le logiciel est inscrit au registre des logiciels russes (num√©ro 4445).  Une conclusion positive a √©t√© re√ßue du laboratoire d'essais sur les tests de certification r√©ussis du logiciel SharxBase dans le syst√®me de certification RF FSTEC pour le niveau 4 de surveillance de l'absence de NDV, ainsi que pour la conformit√© aux sp√©cifications techniques (r√©pondant aux exigences de protection des environnements de virtualisation) jusqu'au niveau de s√©curit√© GIS classe 1 / ISPD inclus.  L'obtention d'un certificat de conformit√© aux exigences du syst√®me de certification des moyens de s√©curit√© de l'information n ¬∞ ROSS RU.0001.01BI00 (FSTEC de la F√©d√©ration de Russie) est attendue en d√©cembre 2018. <br><br>  Une description d√©taill√©e de la fonctionnalit√© est donn√©e dans le tableau ci-dessous. <br><br><h2>  Suivi </h2><br>  SharxBase Monitoring donne acc√®s √† des informations avanc√©es sur l'√©tat de la plateforme, aux param√®tres d'alerte et √† l'analyse de l'√©tat de la plateforme. <br>  Le sous-syst√®me de surveillance est un syst√®me distribu√© install√© sur chacun des n≈ìuds de cluster et fournissant des donn√©es sur l'√©tat de la plate-forme au syst√®me de gestion de la virtualisation. <br><br>  Le sous-syst√®me de surveillance en temps r√©el collecte des informations sur les ressources de la plateforme, telles que: <br><br><table><tbody><tr><th>  N≈ìuds de serveur </th><th>  Alimentations </th><th>  Commutateurs </th><th>  Machines virtuelles </th><th>  Entrep√¥t de donn√©es distribu√© </th></tr><tr><td>  - Num√©ro de s√©rie de l'unit√© <br>  - Num√©ro de s√©rie du n≈ìud et de la carte m√®re <br>  - Unit√© et temp√©rature de l'unit√© <br>  - Mod√®le CPU et charge <br>  - Num√©ros d'emplacement, fr√©quence, taille et disponibilit√© de la RAM <br>  - Noeud et adresse de stockage <br>  - La vitesse de rotation des ventilateurs de refroidissement <br>  - √âtat de l'adaptateur r√©seau <br>  - Num√©ro de s√©rie de l'adaptateur r√©seau <br>  - L'√©tat du disque et ses informations syst√®me <br></td><td>  - Num√©ro de s√©rie de l'alimentation <br>  - L'√©tat de l'alimentation et sa charge <br></td><td>  - Mod√®le de commutateur <br>  - √âtat du commutateur et de ses ports <br>  - La vitesse de rotation des ventilateurs de refroidissement <br>  - Statut des ventilateurs de refroidissement <br>  - Afficher la liste des VLAN <br></td><td>  - Charge CPU <br>  - Charge RAM <br>  - charge r√©seau <br>  - √âtat de la machine virtuelle <br>  - Vitesse d'√©criture / lecture du disque <br>  - Vitesse de connexion entrante / sortante <br></td><td>  - Affichage de l'espace libre / occup√© <br>  - √âtat du disque <br>  - Espace disque utilis√© <br>  - Erreurs de conduite <br></td></tr></tbody></table><br><h2>  Sous-totaux </h2><br>  Les avantages de la solution incluent: <br><br><ul><li>  la possibilit√© de remise aux organisations sur les listes de sanctions; </li><li>  La solution est bas√©e sur le projet OpenNebula, qui se d√©veloppe activement depuis longtemps; </li><li>  prise en charge de toutes les fonctions n√©cessaires concernant la virtualisation des serveurs, suffisante pour les petites et moyennes installations (jusqu'√† 128 h√¥tes); </li><li>  la pr√©sence d'un module de s√©curit√© de l'information qui assure la mise en ≈ìuvre des exigences r√©glementaires dans le domaine de la s√©curit√© de l'information. </li></ul><br>  Les inconv√©nients de la solution incluent: <br><br><ul><li>  fonctionnalit√©s inf√©rieures par rapport aux autres solutions HCI sur le march√© (par exemple, Dell VxRail, Nutanix); </li><li>  prise en charge limit√©e des syst√®mes de sauvegarde (actuellement, la prise en charge de Veritas NetBackup a √©t√© annonc√©e); </li><li>  certaines t√¢ches administratives sont effectu√©es √† partir de la console et ne sont pas accessibles via le Web. </li></ul><br><h2>  Fonctionnalit√© </h2><br><img src="https://habrastorage.org/webt/o6/lt/el/o6ltelcxnsjd_c96lx8knqgqxna.png" alt="image"><br><img src="https://habrastorage.org/webt/r9/qa/r-/r9qar-ldggojsce4ual_tty2lku.png" alt="image"><br><img src="https://habrastorage.org/webt/k4/bg/_y/k4bg_ybgnbidmtqxvxyby2hgubu.png" alt="image"><br><img src="https://habrastorage.org/webt/gc/km/28/gckm28xyjnpyrti65uxzlzos4cu.png" alt="image"><br><br>  Lors de l'√©largissement du portefeuille de solutions hyperconverg√©es, nous avons effectu√© des tests de performances et de tol√©rance aux pannes en collaboration avec le fournisseur. <br><br><h2>  Test de performance </h2><br>  Le banc d'essai √©tait un cluster √† 4 n≈ìuds de serveurs Intel HNS2600TP.  La configuration de tous les serveurs √©tait identique.  Les serveurs avaient les caract√©ristiques mat√©rielles suivantes: <br><br><ul><li>  mod√®le de serveur - Intel HNS2600TP; </li><li>  deux processeurs Intel Xeon E5-2650 v4 (12 c≈ìurs avec une fr√©quence d'horloge de 2,2 GHz et la prise en charge d'Hyper Threading); </li><li>  256 Go de RAM (224 Go de m√©moire sont disponibles pour ex√©cuter la machine virtuelle); </li><li>  adaptateur r√©seau avec 2 ports QSFP + avec un taux de transfert de donn√©es de 40 Gb / s; </li><li>  un contr√¥leur RAID LSI SAS3008; </li><li>  6 disques SSD SATA Intel DC S3700 d'une capacit√© de 800 Go chacun; </li><li>  deux alimentations d'une puissance nominale de 1600 W chacune. </li><li>  Le logiciel de virtualisation SharxBase v1.5 est install√© sur les serveurs. </li></ul><br>  Tous les serveurs connect√©s au commutateur r√©seau Mellanox.  Le sch√©ma de connexion est illustr√© dans la figure. <br><br><img src="https://habrastorage.org/webt/mn/-c/ve/mn-cveotdolbvmhfd0ugghmpaos.png" alt="image"><br>  <i>Sch√©ma de connexion des serveurs sur banc d'essai</i> <br><br>  Toutes les fonctionnalit√©s d√©crites pr√©c√©demment ont √©t√© confirm√©es √† la suite des tests fonctionnels. <br><br>  Le test du sous-syst√®me de disque a √©t√© effectu√© √† l'aide de la version 5.04.06 du logiciel Vdbench.  Sur chaque serveur physique, une machine virtuelle a √©t√© cr√©√©e avec Linux OS avec 8 vCPU, 16 Go de RAM.  Pour les tests sur chaque machine virtuelle, 8 disques virtuels de 100 Go chacun ont √©t√© cr√©√©s. <br><br>  Lors des tests, les types de charges suivants ont √©t√© v√©rifi√©s: <br><br><ul><li>  (Sauvegarde) 0% al√©atoire, 100% en lecture, taille de bloc de 64 Ko, 1 E / S exceptionnelle; </li><li>  (Restauration) 0% al√©atoire, 100% en √©criture, taille de bloc de 64 Ko, 1 E / S exceptionnelle; </li><li>  (Typique) 100% al√©atoire, 70% en lecture, taille de bloc de 4 Ko, 4 E / S exceptionnelles; </li><li>  (VDI) 100% al√©atoire, 20% en lecture, taille de bloc de 4 Ko, 8 E / S exceptionnelles; </li><li>  (OLTP) 100% al√©atoire, 70% en lecture, taille de bloc de 8 Ko, 4 E / S exceptionnelles. </li></ul><br>  Les r√©sultats des tests de ces types sont pr√©sent√©s dans le tableau: <br><br><img src="https://habrastorage.org/webt/_w/pf/it/_wpfitc9wrh6hgycysn3ywpne2g.png" alt="image"><br><img src="https://habrastorage.org/webt/1j/0q/sy/1j0qsyi-8tbdj8usufyacft_1fg.png" alt="image"><br><img src="https://habrastorage.org/webt/98/y6/uq/98y6uqj2sre-qyh_ocldf8sagrs.png" alt="image"><br>  Le stockage fournit des indicateurs de performances particuli√®rement √©lev√©s sur les op√©rations de lecture et d'√©criture s√©quentielles de 8295,71 Mo et 2966,16 Mo, respectivement.  Les performances de stockage √† une charge typique (E / S al√©atoires avec des blocs de 4 Ko avec 70% de lecture) atteignent 133977,94 IOPS avec un d√©lai d'E / S moyen de 1,91 ms et diminuent avec une augmentation du rapport des op√©rations d'√©criture aux op√©rations de lecture. <br><br><h2>  Test de tol√©rance aux pannes </h2><br>  Ces tests ont permis de v√©rifier qu'une d√©faillance d'un des composants du syst√®me n'entra√Æne pas l'arr√™t de l'ensemble du syst√®me. <br><table><tbody><tr><th>  Test </th><th>  D√©tails du test </th><th>  Commentaires </th></tr><tr><td>  √âchec de disque dans le pool de stockage </td><td>  14h00 - le syst√®me fonctionne normalement; <br>  14:11 - d√©sactivation du premier SSD du serveur 1; <br>  14:12 - L'√©chec du SSD est affich√© dans la console de gestion de la plateforme; <br>  14:21 - d√©sactivez le premier SSD du serveur 2; <br>  14:35 - la panne de deux SSD est affich√©e dans la console de gestion de plateforme; <br>  14:38 - renvoyez les disques aux serveurs 1 et 2. Les voyants LED du SSD ne sont pas affich√©s; <br>  14:40 - l'ing√©nieur via la CLI a effectu√© l'ajout de SSD au r√©f√©rentiel; <br>  14:50 - dans la console de gestion de plate-forme sont affich√©s comme fonctionnant; <br>  15h00 - La synchronisation des composants VM est termin√©e; <br></td><td>  Le syst√®me fonctionnait normalement.  L'indicateur de tol√©rance aux pannes est comme indiqu√©. </td></tr><tr><td>  Panne de r√©seau </td><td>  15:02 - le syst√®me fonctionne normalement; <br>  15:17 - d√©sactivez l'un des deux ports du serveur 1; <br>  15:17 - perte d'une requ√™te Echo √† l'adresse IP de la console Web (le serveur isol√© a servi de leader), la VM ex√©cut√©e sur le serveur est accessible sur le r√©seau; <br>  15:18 - d√©sactivation du deuxi√®me port sur le serveur 1, la machine virtuelle et la console de gestion du serveur sont devenues indisponibles; <br>  15h20 - La machine virtuelle a red√©marr√© sur le n≈ìud du serveur 3; <br>  15:26 - Les interfaces r√©seau du serveur 1 sont connect√©es, le serveur est revenu au cluster; <br>  15:35 - la synchronisation des composants des disques VM est termin√©e; <br></td><td>  Le syst√®me fonctionnait normalement. </td></tr><tr><td>  Panne d'un serveur physique </td><td>  15h35 - le syst√®me fonctionne normalement; <br>  15:36 - arr√™t du serveur 3 via la commande poweroff dans l'interface IPMI; <br>  15:38 - la VM de test a red√©marr√© sur le serveur 1; <br>  15h40 - inclusion du serveur 3; <br>  15:43 - fonctionnement du serveur restaur√©; <br>  15:47 - la synchronisation est termin√©e. <br></td><td>  Le syst√®me fonctionnait normalement. </td></tr></tbody></table><br><h2>  R√©sultats des tests </h2><br>  La plate-forme SharxBase offre un haut niveau de disponibilit√© et de tol√©rance aux pannes en cas de d√©faillance d'un composant mat√©riel principal.  En raison de la triple redondance du sous-syst√®me de disques, la plateforme garantit la disponibilit√© et la s√©curit√© des donn√©es en cas de double panne. <br><br>  Les inconv√©nients de la plate-forme incluent des exigences √©lev√©es en espace disque dues √† la n√©cessit√© de stocker et de synchroniser trois copies compl√®tes de donn√©es, et le manque de m√©canismes pour une utilisation plus efficace de l'espace disque, tels que la d√©duplication, la compression ou le codage d'effacement. <br><br>  Sur la base des r√©sultats de tous les tests effectu√©s, nous pouvons conclure que la plateforme hyperconverg√©e SharxBase est en mesure de fournir un niveau √©lev√© de disponibilit√© et de performances pour diff√©rents types de charges, y compris les syst√®mes OLTP, VDI et les services d'infrastructure. <br><br>  <i>Ilya Kuykin,</i> <i><br></i>  <i>Ing√©nieur de conception leader des syst√®mes informatiques,</i> <i><br></i>  <i>Jet Infosystems</i> <i><br></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr429042/">https://habr.com/ru/post/fr429042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr429030/index.html">Logomachine cr√©e des logos gratuits par commentaire</a></li>
<li><a href="../fr429032/index.html">Splunk Essentials pour l'application Financial Services Industry, ou comment Splunk entre sur le march√© de l'analyse financi√®re</a></li>
<li><a href="../fr429034/index.html">Quelques histoires sur les programmeurs underground</a></li>
<li><a href="../fr429038/index.html">Rust News # 2 (octobre 2018)</a></li>
<li><a href="../fr429040/index.html">Patch de code Java sur la production sans anesth√©sie</a></li>
<li><a href="../fr429044/index.html">Les actions Apple ont connu la pire chute depuis 2014. Les grands investisseurs ont perdu des milliards</a></li>
<li><a href="../fr429046/index.html">Quatre ans de d√©veloppement de SObjectizer-5.5. Comment SObjectizer a-t-il chang√© pendant cette p√©riode?</a></li>
<li><a href="../fr429048/index.html">Conseils pour un h√©bergeur novice</a></li>
<li><a href="../fr429050/index.html">Attaque d'√©change de crypto-monnaie Gate.io enregistr√©e</a></li>
<li><a href="../fr429052/index.html">Pourquoi sur les ordinateurs portables tactiles, certains SPA ont cess√© de prendre en charge les √©v√©nements tactiles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>