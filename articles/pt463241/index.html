<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüéì üê® üéº Notas da Confer√™ncia ACL 2019 üßòüèø üï• üåÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A Reuni√£o Anual da Association for Computational Linguistics (ACL) √© a principal confer√™ncia de processamento de linguagem natural. Est√° organizado de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Notas da Confer√™ncia ACL 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463241/"><img src="https://habrastorage.org/webt/to/6d/jy/to6djymeashzcthmzckiaiwlnsg.jpeg"><br><br>  A Reuni√£o Anual da Association for Computational Linguistics (ACL) √© a principal confer√™ncia de processamento de linguagem natural.  Est√° organizado desde 1962.  Depois do Canad√° e da Austr√°lia, ela retornou √† Europa e marchou em Floren√ßa.  Assim, este ano foi mais popular entre os pesquisadores europeus do que a EMNLP. <br><br>  Este ano foram publicados 660 artigos de 2900 enviados.  Uma quantidade enorme.  Dificilmente √© poss√≠vel fazer algum tipo de revis√£o objetiva do que estava na confer√™ncia.  Portanto, contarei meus sentimentos subjetivos a partir deste evento. <br><a name="habracut"></a><br>  Eu vim √† confer√™ncia para mostrar em uma sess√£o de p√¥steres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nossa decis√£o</a> da competi√ß√£o Kaggle sobre a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resolu√ß√£o de pronome de g√™nero</a> do Google.  Nossa solu√ß√£o se baseou fortemente no uso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelos BERT</a> pr√©- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinados</a> .  E, como se viu, n√£o est√°vamos sozinhos nisso. <br><br><h2>  Bertology </h2><br><img src="https://habrastorage.org/webt/ol/zh/ba/olzhbat3al984zylni9zvcrizqo.jpeg"><br>  Havia tantos trabalhos baseados no BERT, descrevendo suas propriedades e usando-os como um por√£o, que at√© o termo Bertologia apareceu.  De fato, os modelos BERT foram t√£o bem-sucedidos que at√© grandes grupos de pesquisa comparam seus modelos com o BERT. <br><br>  Ent√£o, no in√≠cio de junho, surgiram trabalhos sobre a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">XLNet</a> .  E logo antes da confer√™ncia - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ERNIE 2.0</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RoBERTa</a> <br><br><h4>  Facebook RoBERTa </h4><br>  Quando o modelo XLNet foi introduzido pela primeira vez, alguns pesquisadores sugeriram que ele obteve melhores resultados, n√£o apenas por causa de seus princ√≠pios de arquitetura e treinamento.  Ela tamb√©m estudou em um corpo maior (quase 10 vezes) que o BERT e por mais tempo (4 vezes mais itera√ß√µes). <br><br>  Pesquisadores do Facebook mostraram que o BERT ainda n√£o atingiu seu m√°ximo.  Eles apresentaram uma abordagem otimizada para o ensino do modelo BERT - RoBERTa (abordagem BERT otimizada com robustez). <br><br>  N√£o mudando nada na arquitetura do modelo, eles mudaram o procedimento de treinamento: <br><br><ol><li>  Aumentamos o corpo para treinamento, o tamanho do lote, a dura√ß√£o da sequ√™ncia e a dura√ß√£o do treinamento. </li><li>  A tarefa de prever a pr√≥xima frase foi removida do treinamento. </li><li>  Eles come√ßaram a gerar dinamicamente tokens MASK (tokens que o modelo tenta prever durante o pr√©-treinamento). </li></ol><br><h4>  ERNIE 2.0 do Baidu </h4><br>  Como todos os modelos recentes populares (BERT, GPT, XLM, RoBERTa, XLNet), o ERNIE √© baseado no conceito de um transformador com um mecanismo de auto-aten√ß√£o.  O que o distingue de outros modelos s√£o os conceitos de aprendizado multitarefa e aprendizado cont√≠nuo. <br><br>  O ERNIE aprende sobre diferentes tarefas, atualizando constantemente a representa√ß√£o interna do seu modelo de linguagem.  Essas tarefas t√™m, como outros modelos, objetivos de auto-aprendizado (auto-supervisionado e fraco-supervisionado).  Exemplos de tais tarefas: <br><br><ul><li>  Recupere a ordem correta das palavras em uma frase. </li><li>  Capitaliza√ß√£o de palavras. </li><li>  Defini√ß√£o de palavras mascaradas. </li></ul><br>  Nessas tarefas, o modelo aprende sequencialmente, retornando √†s tarefas nas quais foi treinado anteriormente. <br><br><h4>  RoBERTa vs ERNIE </h4><br>  Nas publica√ß√µes, RoBERTa e ERNIE n√£o s√£o comparados entre si, pois apareceram quase simultaneamente.  Eles s√£o comparados ao BERT e ao XLNet.  Mas aqui n√£o √© t√£o f√°cil fazer uma compara√ß√£o.  Por exemplo, no popular <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">benchmark, o GLUE</a> XLNet √© representado por um conjunto de modelos.  E os pesquisadores do Baidu est√£o mais interessados ‚Äã‚Äãem comparar modelos √∫nicos.  Al√©m disso, como o Baidu √© uma empresa chinesa, eles tamb√©m est√£o interessados ‚Äã‚Äãem comparar os resultados do trabalho com o idioma chin√™s.  Mais recentemente, uma nova refer√™ncia apareceu: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SuperGLUE</a> .  Ainda n√£o existem muitas solu√ß√µes, mas o RoBERTa est√° em primeiro lugar aqui. <br><br>  No geral, por√©m, tanto o RoBERTa quanto o ERNIE apresentam desempenho melhor que o XLNet e significativamente melhor que o BERT.  O RoBERTa, por sua vez, funciona um pouco melhor que o ERNIE. <br><br><h2>  Gr√°ficos de conhecimento </h2><br>  Muito trabalho foi dedicado √† combina√ß√£o de duas abordagens: redes pr√©-treinadas e o uso de regras na forma de gr√°ficos de conhecimento (Knowledge Graphs, KG). <br><br>  Por exemplo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ERNIE: representa√ß√£o aprimorada de idiomas com entidades informativas</a> .  Este artigo destaca o uso de gr√°ficos de conhecimento sobre o modelo de linguagem BERT.  Isso permite que voc√™ obtenha melhores resultados em tarefas como determinar o tipo de entidade ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Digita√ß√£o de entidade) e Classifica√ß√£o de rela√ß√£o</a> . <br><br>  Em geral, a moda de escolher nomes de modelos pelos nomes de personagens da Vila S√©samo leva a consequ√™ncias engra√ßadas.  Por exemplo, esse ERNIE n√£o tem nada a ver com o ERNIE 2.0 do Baidu, sobre o qual escrevi acima. <br><br><img src="https://habrastorage.org/webt/um/6y/qp/um6yqpe7esqpdixrlodndhuf_pi.jpeg"><br><br>  Outro trabalho interessante sobre a gera√ß√£o de novos conhecimentos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">COMET: Transformadores Commonsense para constru√ß√£o autom√°tica de gr√°ficos de conhecimento</a> .  O artigo considera a possibilidade de usar novas arquiteturas baseadas em transformadores para o treinamento de redes baseadas no conhecimento.  As bases de conhecimento de forma simplificada s√£o muitos triplos: sujeito, atitude, objeto.  Eles usaram dois conjuntos de dados da base de conhecimento: ATOMIC e ConceptNet.  E eles treinaram uma rede baseada no modelo GPT (Generative Pr√©-Treinado Transformer).  O sujeito e a atitude foram introduzidos e tentaram prever o objeto.  Assim, eles obtiveram um modelo que gera objetos por sujeitos e relacionamentos de entrada. <br><br><h2>  M√©tricas </h2><br>  Outro t√≥pico interessante na confer√™ncia foi a quest√£o da escolha de m√©tricas.  Muitas vezes, √© dif√≠cil avaliar a qualidade de um modelo em tarefas de processamento de linguagem natural, o que atrasa o progresso nessa √°rea de aprendizado de m√°quina. <br><br>  Em um artigo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Estudo de m√©tricas de avalia√ß√£o de resumo no intervalo de pontua√ß√£o apropriada</a> , Maxim Peyar discute o uso de v√°rias m√©tricas em um problema de resumo de texto.  Essas m√©tricas nem sempre se correlacionam bem entre si, o que interfere na compara√ß√£o objetiva de v√°rios algoritmos. <br><br>  Ou aqui est√° um trabalho interessante: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Avalia√ß√£o autom√°tica de textos com v√°rias frases</a> .  Nele, os autores apresentam uma m√©trica que pode substituir BLEU e ROUGE em tarefas nas quais voc√™ precisa avaliar textos de v√°rias frases. <br><br>  A m√©trica BLEU pode ser representada como Precis√£o - quantas palavras (ou n gramas) da resposta do modelo est√£o contidas no destino.  ROUGE is Recall - quantas palavras (ou n gramas) do alvo est√£o contidas na resposta do modelo. <br><br>  A m√©trica proposta no artigo √© baseada na m√©trica WMD (Dist√¢ncia do motor do Word) - a dist√¢ncia entre dois documentos.  √â igual √† dist√¢ncia m√≠nima entre palavras em duas frases no espa√ßo da representa√ß√£o vetorial dessas palavras.  Mais informa√ß√µes sobre o WMD podem ser encontradas no tutorial, que usa o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">WMD do Word2Vec</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do GloVe</a> . <br><br>  Em seu artigo, eles oferecem uma nova m√©trica: WMS (Similarity do Word Mover). <br><br><pre><code class="plaintext hljs">WMS(A, B) = exp(‚àíWMD(A, B))</code> </pre> <br>  Eles ent√£o definem o SMS (semelhan√ßa do movedor de senten√ßas).  Ele usa uma abordagem semelhante √† do WMS.  Como representa√ß√£o vetorial da senten√ßa, eles recebem o vetor m√©dio das palavras da senten√ßa. <br><br>  Ao calcular o WMS, as palavras s√£o normalizadas pela frequ√™ncia no documento.  Ao calcular senten√ßas por SMS, √© normalizado pelo n√∫mero de palavras na senten√ßa. <br><br>  Finalmente, a m√©trica S + WMS √© uma combina√ß√£o de WMS e SMS.  Em seu artigo, eles apontam que suas m√©tricas se correlacionam melhor com a avalia√ß√£o manual de uma pessoa. <br><br><h2>  Chatbots </h2><br>  A parte mais √∫til da confer√™ncia, na minha opini√£o, foram sess√µes de p√¥steres.  Nem todos os relat√≥rios foram interessantes, mas se voc√™ come√ßar a ouvir alguns, n√£o sair√° para outro no meio do relat√≥rio.  Cartazes s√£o outra quest√£o.  Existem v√°rias dezenas deles na sess√£o de p√¥steres.  Voc√™ escolhe os que mais gosta e, em regra, pode conversar diretamente com o desenvolvedor sobre detalhes t√©cnicos.  A prop√≥sito, existe um site interessante com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">p√¥steres de confer√™ncias</a> .  √â verdade que existem p√¥steres de duas confer√™ncias por l√° e n√£o se sabe se o site ser√° atualizado. <br><br><img src="https://habrastorage.org/webt/tm/y9/z4/tmy9z4i7y1tzu2gxpk5kfheopqy.jpeg"><br><br>  Nas sess√µes de p√¥steres, grandes empresas frequentemente apresentavam trabalhos interessantes.  Por exemplo, aqui est√° um artigo do Facebook <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendendo com o Di√°logo ap√≥s a Implanta√ß√£o: Alimente-se, Chatbot!</a>  . <br><br>  A peculiaridade de seu sistema √© o uso expandido das respostas do usu√°rio.  Eles possuem um classificador que avalia o grau de satisfa√ß√£o do usu√°rio com o di√°logo.  Eles usam essas informa√ß√µes para diferentes tarefas: <br><br><ul><li>  Use uma medida de satisfa√ß√£o como uma m√©trica de qualidade. </li><li>  Eles treinam o modelo, aplicando a abordagem da aprendizagem cont√≠nua (Aprendizagem Cont√≠nua). </li><li>  Use diretamente no di√°logo.  Expresse alguma rea√ß√£o humana se o usu√°rio estiver satisfeito.  Ou eles perguntam o que est√° errado se o usu√°rio est√° insatisfeito. </li></ul><br>  A partir dos relat√≥rios, havia uma hist√≥ria interessante sobre o chatbot chin√™s da Microsoft.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O design e a implementa√ß√£o do XiaoIce, um chatbot social emp√°tico</a> <br><br>  A China j√° √© um dos l√≠deres na introdu√ß√£o de tecnologias de intelig√™ncia artificial.  Mas muitas vezes o que est√° acontecendo na China n√£o √© bem conhecido na Europa.  E o XiaoIce √© um projeto incr√≠vel.  J√° existe h√° cinco anos.  Atualmente, n√£o h√° muitos chatbots dessa idade trabalhando.  Em 2018, j√° tinha 660 milh√µes de usu√°rios. <br><br>  O sistema possui um bot de bate-papo e um sistema de habilidades.  O bot j√° possui 230 habilidades, ou seja, adiciona aproximadamente uma habilidade por semana. <br><br>  Para avaliar a qualidade do bot de bate-papo, eles usam a dura√ß√£o do di√°logo.  E n√£o em minutos, como geralmente √© feito, mas no n√∫mero de r√©plicas em uma conversa.  Eles chamam essa m√©trica de Convers√£o por turnos por sess√£o (CPS) e escrevem que no momento seu valor m√©dio √© 23, que √© o melhor indicador entre sistemas similares. <br><br>  Em geral, o projeto √© muito popular na China.  Al√©m do bot em si, o sistema escreve poesia, desenha imagens, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lan√ßa uma cole√ß√£o de roupas</a> , canta m√∫sicas. <br><br><h2>  Tradu√ß√£o autom√°tica </h2><br>  De todos os discursos que participei, o mais animado foi o relat√≥rio de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">interpreta√ß√£o simult√¢nea</a> de Liang Huang, representando a Baidu Research. <br><br>  Ele falou sobre essas dificuldades na tradu√ß√£o simult√¢nea moderna: <br><br><ul><li>  Existem apenas 3.000 int√©rpretes simult√¢neos certificados no mundo. </li><li>  Os tradutores podem trabalhar apenas 15 a 20 minutos continuamente. </li><li>  Apenas cerca de 60% do texto original √© traduzido. </li></ul><br>  A tradu√ß√£o em frases inteiras j√° atingiu um bom n√≠vel, mas ainda h√° espa√ßo para melhorias na tradu√ß√£o simult√¢nea.  Como exemplo, ele citou o sistema de interpreta√ß√£o simult√¢nea, que funcionou na Confer√™ncia Mundial do Baidu.  O atraso na tradu√ß√£o em 2018 em compara√ß√£o com 2017 foi reduzido de 10 para 3 segundos. <br><br>  Poucas equipes fazem isso e existem poucos sistemas de trabalho.  Por exemplo, quando o Google traduz a frase que voc√™ escreve online, ele refaz constantemente a frase final.  E isso n√£o √© tradu√ß√£o simult√¢nea, porque com tradu√ß√£o simult√¢nea n√£o podemos mudar as palavras j√° ditas. <br><br><img src="https://habrastorage.org/webt/ko/z9/xg/koz9xgvrqfclne8wwzw02fxwrdy.png"><br><br>  No sistema deles, eles usam a tradu√ß√£o de prefixos - parte de uma frase.  Ou seja, eles esperam algumas palavras e come√ßam a traduzir, tentando adivinhar o que aparecer√° na fonte.  O tamanho dessa mudan√ßa √© medido em palavras e √© adapt√°vel.  Ap√≥s cada etapa, o sistema decide se vale a pena esperar ou se j√° pode ser traduzido.  Para avaliar esse atraso, eles introduzem a seguinte m√©trica: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">m√©trica de</a> atraso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">m√©dio (AL)</a> . <br><br>  A principal dificuldade com a tradu√ß√£o simult√¢nea √© a ordem das palavras diferentes nos idiomas.  E o contexto ajuda a combater isso.  Por exemplo, voc√™ muitas vezes precisa traduzir os discursos dos pol√≠ticos, e eles s√£o bastante estereotipados.  Mas tamb√©m h√° problemas.  Ent√£o o orador brincou sobre Trump.  Ent√£o, ele diz, se Bush voou para Moscou, √© altamente prov√°vel que, para se encontrar com Putin.  E se Trump voou para, ent√£o ele pode conhecer e jogar golfe.  Em geral, ao traduzir, as pessoas geralmente criam algo que adicionam.  E digamos que, se voc√™ precisar traduzir algum tipo de piada, e eles n√£o puderem fazer isso imediatamente, eles podem dizer: "Uma piada foi dita aqui, apenas ria". <br><br>  Havia tamb√©m um artigo sobre tradu√ß√£o autom√°tica que recebeu o pr√™mio ‚ÄúO Melhor Artigo Longo‚Äù: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fazendo a ponte entre treinamento e infer√™ncia para</a> tradu√ß√£o autom√°tica <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neural</a> . <br><br>  Ele descreve um problema de tradu√ß√£o autom√°tica.  No processo de aprendizado, geramos tradu√ß√£o palavra por palavra com base no contexto de palavras conhecidas.  No processo de uso do modelo, contamos com o contexto das palavras rec√©m-geradas.  H√° uma discrep√¢ncia entre treinar o modelo e us√°-lo. <br><br>  Para reduzir essa discrep√¢ncia, os autores prop√µem, na fase de treinamento no contexto, misturar as palavras previstas pelo modelo no processo de treinamento.  O artigo discute a escolha ideal de tais palavras geradas. <br><br><h2>  Conclus√£o </h2><br>  Obviamente, uma confer√™ncia n√£o √© apenas artigos e relat√≥rios.  √â tamb√©m comunica√ß√£o, namoro e outras redes.  Al√©m disso, os organizadores da confer√™ncia est√£o tentando entreter os participantes.  No ACL, na festa principal, houve uma apresenta√ß√£o de tenores, na It√°lia, afinal.  E para resumir, houve an√∫ncios dos organizadores de outras confer√™ncias.  E a rea√ß√£o mais violenta entre os participantes foi causada por mensagens dos organizadores da EMNLP de que este ano a festa principal ser√° na Disneyl√¢ndia de Hong Kong e, em 2020, a confer√™ncia ser√° realizada na Rep√∫blica Dominicana. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt463241/">https://habr.com/ru/post/pt463241/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt463229/index.html">Jetpacks em Cultura: Cinema</a></li>
<li><a href="../pt463231/index.html">Treinamento Cisco 200-125 CCNA v3.0. Dia 14. VTP, Poda e VLAN Nativa</a></li>
<li><a href="../pt463233/index.html">Treinamento Cisco 200-125 CCNA v3.0. Dia 15. Comunica√ß√£o lenta e seguran√ßa portu√°ria</a></li>
<li><a href="../pt463237/index.html">Como tocamos m√∫sica com redes neurais v 2.0</a></li>
<li><a href="../pt463239/index.html">22 de agosto - Alfa JS MeetUP SPb</a></li>
<li><a href="../pt463243/index.html">Manipula√ß√£o da consci√™ncia. Por que isso √© t√£o simples?</a></li>
<li><a href="../pt463245/index.html">Como o reposit√≥rio DWH foi organizado no TELE2</a></li>
<li><a href="../pt463247/index.html">Ferramentas de informa√ß√£o ou como falamos sobre nossos servi√ßos e processos</a></li>
<li><a href="../pt463249/index.html">Game Dev Sim: jogo de tabuleiro sobre desenvolvimento de jogos</a></li>
<li><a href="../pt463251/index.html">Como cortar o subconjunto da cidade (qualquer rela√ß√£o) dos dados do OSM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>