<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òïÔ∏è üòî üë®üèº‚Äçüöí Arquitectura o caracter√≠sticas de AERODISK vAIR del edificio de cl√∫ster nacional ‚èπÔ∏è ü•Ä üì®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬°Hola Khabrovchans! Continuamos familiariz√°ndolo con el sistema ruso hiperconvergente AERODISK vAIR. Este art√≠culo se centrar√° en la arquitectura de e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Arquitectura o caracter√≠sticas de AERODISK vAIR del edificio de cl√∫ster nacional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/475254/"><p><img src="https://habrastorage.org/webt/4c/_3/or/4c_3or6x9ro2_f_g8-waac1oen0.jpeg"></p><br><p>  ¬°Hola Khabrovchans!  Continuamos familiariz√°ndolo con el sistema ruso hiperconvergente AERODISK vAIR.  Este art√≠culo se centrar√° en la arquitectura de este sistema.  En el √∫ltimo art√≠culo, analizamos nuestro sistema de archivos ARDFS, y en este art√≠culo revisaremos todos los componentes principales de software que componen vAIR y sus tareas. </p><a name="habracut"></a><br><p>  Comenzamos la descripci√≥n de la arquitectura de abajo hacia arriba, desde el almacenamiento hasta la administraci√≥n. </p><br><h2 id="faylovaya-sistema-ardfs--raft-cluster-driver">  Sistema de archivos ARDFS + Raft Cluster Driver </h2><br><p>  La base de vAIR es el sistema de archivos distribuido ARDFS, que combina los discos locales de todos los nodos del cl√∫ster en un solo grupo l√≥gico, sobre la base de los cuales los discos virtuales con uno u otro esquema de tolerancia a fallas (factor de replicaci√≥n o codificaci√≥n de borrado) se forman a partir de bloques virtuales de 4 MB.  Una descripci√≥n m√°s detallada del trabajo de ARDFS se da en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo anterior.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><br></a> <br>  Raft Cluster Driver es un servicio ARDFS interno que resuelve el problema del almacenamiento distribuido y confiable de los metadatos del sistema de archivos. </p><br><p>  Los metadatos ARDFS se dividen convencionalmente en dos clases. </p><br><ul><li>  notificaciones: informaci√≥n sobre operaciones con objetos de almacenamiento e informaci√≥n sobre los objetos mismos; </li><li>  informaci√≥n de servicio: configuraci√≥n de bloqueos e informaci√≥n de configuraci√≥n para nodos de almacenamiento. </li></ul><br><p>  El servicio RCD se utiliza para distribuir estos datos.  Asigna autom√°ticamente un nodo con el papel de un l√≠der cuya tarea es obtener y difundir metadatos entre los nodos.  Un l√≠der es la √∫nica fuente verdadera de esta informaci√≥n.  Adem√°s, el l√≠der organiza un latido del coraz√≥n, es decir  comprueba la disponibilidad de todos los nodos de almacenamiento (esto no tiene relaci√≥n con la disponibilidad de m√°quinas virtuales, RCD es solo un servicio de almacenamiento). </p><br><p>  Si por alguna raz√≥n el l√≠der no est√° disponible para uno de los nodos ordinarios durante m√°s de un segundo, este nodo ordinario organiza una reelecci√≥n del l√≠der, solicitando la disponibilidad del l√≠der de otros nodos ordinarios.  Si hay qu√≥rum, el l√≠der es reelegido.  Despu√©s de que el antiguo l√≠der "despert√≥", se convierte autom√°ticamente en un nodo ordinario, porque  El nuevo l√≠der le env√≠a el equipo apropiado. </p><br><p> La l√≥gica del RCD en s√≠ no es nueva.  Muchas soluciones de terceros y comerciales y gratuitas tambi√©n se gu√≠an por esta l√≥gica, pero estas soluciones no nos conven√≠an (como el FS de c√≥digo abierto existente), porque son bastante pesadas y es muy dif√≠cil optimizarlas para nuestras tareas simples, as√≠ que simplemente escribimos las nuestras. Servicio de RCD. <br>  Puede parecer que el l√≠der es un "cuello estrecho" que puede ralentizar el trabajo en grandes grupos de cientos de nodos, pero esto no es as√≠.  El proceso descrito ocurre casi instant√°neamente y "pesa" muy poco ya que lo escribimos nosotros mismos e incluimos solo las funciones m√°s necesarias.  Adem√°s, ocurre de forma completamente autom√°tica, dejando solo mensajes en los registros. </p><br><h2 id="masterio--sluzhba-upravleniya-mnogopotochnym-vvodom-vyvodom">  MasterIO - Servicio de gesti√≥n de E / S multiproceso </h2><br><p>  Una vez que se organiza un grupo ARDFS con discos virtuales, se puede usar para E / S.  En este punto, la pregunta surge espec√≠ficamente para los sistemas hiperconvergentes, a saber: ¬øcu√°ntos recursos del sistema (CPU / RAM) podemos donar para IO? </p><br><p>  En los sistemas de almacenamiento cl√°sicos, esta pregunta no es tan aguda, porque la tarea de almacenamiento es solo almacenar datos (y la mayor√≠a de los recursos de almacenamiento del sistema se pueden proporcionar de manera segura bajo IO), y las tareas de hiperconvergencia, adem√°s del almacenamiento, tambi√©n incluyen la ejecuci√≥n de m√°quinas virtuales.  En consecuencia, el GCS requiere el uso de recursos de CPU y RAM principalmente para m√°quinas virtuales.  Bueno, ¬øqu√© hay de E / S? </p><br><p>  Para resolver este problema, vAIR utiliza el servicio de administraci√≥n de E / S: MasterIO.  La tarea del servicio es simple: <del>  "Toma todo y comparte" </del>  se garantiza que recoge el en√©simo n√∫mero de recursos del sistema para entrada y salida y, a partir de ellos, comienza el en√©simo n√∫mero de flujos de entrada / salida. </p><br><p>  Inicialmente, quer√≠amos proporcionar un mecanismo "muy inteligente" para asignar recursos para IO.  Por ejemplo, si no hay carga en el almacenamiento, los recursos del sistema pueden usarse para m√°quinas virtuales, y si aparece la carga, estos recursos se eliminan "suavemente" de las m√°quinas virtuales dentro de l√≠mites predeterminados.  Pero este intento termin√≥ en un fracaso parcial.  Las pruebas mostraron que si la carga se incrementa gradualmente, entonces todo est√° bien, los recursos (marcados para una posible eliminaci√≥n) se retiran gradualmente de la VM a favor de E / S.  Pero estallidos bruscos de cargas de almacenamiento provocan una retirada no tan "suave" de los recursos de las m√°quinas virtuales y, como resultado, se acumulan colas en los procesadores y, como resultado, <del>  y los lobos tienen hambre y las ovejas est√°n muertas </del>  y virtualka cuelgan, y no hay IOPS. </p><br><p>  Quiz√°s en el futuro volvamos a este problema, pero por ahora hemos implementado la emisi√≥n de recursos para IO de la manera del viejo abuelo: las manos. </p><br><p>  Seg√∫n los datos de dimensionamiento, el administrador asigna previamente el en√©simo n√∫mero de n√∫cleos de CPU y RAM para el servicio MasterIO.  A estos recursos se les asigna el monopolio, es decir  no se pueden usar de ninguna manera para las necesidades de VM hasta que el administrador lo permita.  Los recursos se asignan de manera uniforme, es decir  Se toma la misma cantidad de recursos del sistema de cada nodo del cl√∫ster.  En primer lugar, los recursos del procesador son de inter√©s para MasterIO (la RAM es menos importante), especialmente si utilizamos la codificaci√≥n Erasure. </p><br><p>  Si se produjo un error con el tama√±o y le dimos demasiados recursos a MasterIO, entonces la situaci√≥n se resuelve f√°cilmente eliminando estos recursos al grupo de recursos de VM.  Si los recursos est√°n inactivos, regresar√°n casi de inmediato al grupo de recursos de VM, pero si se eliminan estos recursos, tendr√° que esperar un tiempo para que MasterIO los libere suavemente. </p><br><p>  La situaci√≥n inversa es m√°s complicada.  Si necesit√°ramos aumentar el n√∫mero de n√∫cleos para MasterIO, y est√°n ocupados con los virtuales, entonces tenemos que "negociar" con los virtuales, es decir, seleccionarlos con asas, porque en modo autom√°tico en una situaci√≥n de fuerte explosi√≥n de carga, esta operaci√≥n est√° cargada de congelaciones de VM y otros comportamientos caprichosos. </p><br><p>  En consecuencia, se debe prestar mucha atenci√≥n al dimensionamiento del rendimiento de los sistemas hiperconvergentes de E / S (no solo los nuestros).  Un poco m√°s adelante, en uno de los art√≠culos, prometemos considerar este tema con m√°s detalle. </p><br><h2 id="gipervizor">  Hipervisor </h2><br><p>  Hypervisor Aist es responsable de ejecutar m√°quinas virtuales en vAIR.  Este hipervisor se basa en el hipervisor KVM probado con el tiempo.  En principio, se ha escrito mucho sobre el trabajo de KVM, por lo que no hay necesidad particular de pintarlo, solo indique que todas las funciones est√°ndar de KVM est√°n almacenadas en Stork y funcionan bien. </p><br><p>  Por lo tanto, aqu√≠ describiremos las principales diferencias con el KVM est√°ndar, que implementamos en Stork.  La cig√ºe√±a forma parte del sistema (hipervisor preinstalado) y se controla desde la consola vAIR com√∫n a trav√©s de la GUI web (versiones en ruso e ingl√©s) y SSH (obviamente, solo ingl√©s). </p><br><p><img src="https://habrastorage.org/webt/vi/ju/9e/viju9ee4sxkeq-ezvoobttyjvck.png"></p><br><p>  Adem√°s, las configuraciones del hipervisor se almacenan en la base de datos distribuida de ConfigDB (sobre esto un poco m√°s tarde), que tambi√©n es un √∫nico punto de control.  Es decir, puede conectarse a cualquier nodo en el cl√∫ster y administrar todo sin la necesidad de un servidor de administraci√≥n separado. </p><br><p>  Una adici√≥n importante a la funcionalidad est√°ndar de KVM es el m√≥dulo HA que desarrollamos.  Esta es la implementaci√≥n m√°s simple de un cl√∫ster de m√°quinas virtuales de alta disponibilidad, que le permite reiniciar autom√°ticamente la m√°quina virtual en otro nodo del cl√∫ster en caso de falla del nodo. </p><br><p>  Otra caracter√≠stica √∫til es el despliegue masivo de m√°quinas virtuales (relevante para entornos VDI), que automatizar√° el despliegue de m√°quinas virtuales con distribuci√≥n autom√°tica entre nodos dependiendo de la carga en ellos. </p><br><p>  La distribuci√≥n de VM entre nodos es la base para el equilibrio de carga autom√°tico (ala DRS).  Esta funci√≥n a√∫n no est√° disponible en la versi√≥n actual, pero estamos trabajando activamente en ella y definitivamente aparecer√° en una de las pr√≥ximas actualizaciones. </p><br><p>  El hipervisor VMware ESXi tiene soporte opcional, actualmente se implementa utilizando el protocolo iSCSI, y el soporte NFS tambi√©n est√° planeado en el futuro. </p><br><h2 id="virtualnye-kommutatory">  Interruptores virtuales </h2><br><p>  Para la implementaci√≥n del software de los conmutadores, se proporciona un componente separado: Fractal.  Al igual que en nuestros otros componentes, pasamos de simple a complejo, por lo que en la primera versi√≥n se implementa un cambio simple, mientras que el enrutamiento y el firewall se otorgan a dispositivos de terceros.  El principio de funcionamiento es est√°ndar.  La interfaz f√≠sica del servidor est√° conectada por un puente al objeto Fractal, un grupo de puertos.  Un grupo de puertos, a su vez, con las m√°quinas virtuales deseadas en el cl√∫ster.  Se admite la organizaci√≥n de VLAN, y en una de las pr√≥ximas versiones se agregar√° soporte de VxLAN.  Todos los conmutadores creados se distribuyen por defecto, es decir  distribuidos en todos los nodos del cl√∫ster, por lo que las m√°quinas virtuales a las que los conmutadores se conectan a la VM no dependen del nodo de ubicaci√≥n, esto es solo una decisi√≥n del administrador. </p><br><h2 id="monitoring-i-statistika">  Monitoreo y estad√≠sticas </h2><br><p>  El componente responsable del monitoreo y las estad√≠sticas (t√≠tulo de trabajo M√≥nica) es, de hecho, un clon redise√±ado del sistema de almacenamiento ENGINE.  En un momento, se recomend√≥ bien y decidimos usarlo con vAIR con un ajuste f√°cil.  Al igual que todos los dem√°s componentes, M√≥nica se ejecuta y se almacena en todos los nodos del cl√∫ster al mismo tiempo. </p><br><p>  Las dif√≠ciles responsabilidades de M√≥nica pueden resumirse de la siguiente manera: </p><br><p>  Recolecci√≥n de datos: </p><br><ul><li>  de sensores de hardware (que pueden dar hierro sobre IPMI); </li><li>  de objetos l√≥gicos vAIR (ARDFS, Stork, Fractal, MasterIO y otros objetos). </li></ul><br><p><img src="https://habrastorage.org/webt/1c/4k/u8/1c4ku82o_bkanp-348z-jbeioma.png"></p><br><p>  Recopilaci√≥n de datos en una base de datos distribuida; </p><br><p>  Interpretaci√≥n de datos en forma de: </p><br><ul><li>  troncos </li><li>  Alertas </li><li>  horarios. </li></ul><br><p>  Interacci√≥n externa con sistemas de terceros a trav√©s de los protocolos SMTP (env√≠o de alertas por correo electr√≥nico) y SNMP (interacci√≥n con sistemas de monitoreo de terceros). </p><br><p><img src="https://habrastorage.org/webt/2k/-p/9o/2k-p9oah-yrta3n5ti0m5yfm22e.png"></p><br><h2 id="raspredelennaya-baza-konfiguraciy">  Base de configuraci√≥n distribuida </h2><br><p>  En los p√°rrafos anteriores, se mencion√≥ que muchos datos se almacenan en todos los nodos del cl√∫ster al mismo tiempo.  Para organizar este m√©todo de almacenamiento, se proporciona una base de datos distribuida especial de ConfigDB.  Como su nombre lo indica, la base de datos almacena las configuraciones de todos los objetos del cl√∫ster: hipervisor, m√°quinas virtuales, m√≥dulo HA, conmutadores, sistema de archivos (que no debe confundirse con la base de datos de metadatos FS, esta es otra base de datos), as√≠ como estad√≠sticas.  Estos datos se almacenan sincr√≥nicamente en todos los nodos y la coherencia de estos datos es un requisito previo para el funcionamiento estable de vAIR. </p><br><p>  Un punto importante: aunque el funcionamiento de ConfigDB es vital para la operaci√≥n de vAIR, su falla, aunque detendr√° el cl√∫ster, no afecta la consistencia de los datos almacenados en ARDFS, lo que en nuestra opini√≥n es una ventaja para la confiabilidad de la soluci√≥n en su conjunto. </p><br><p>  ConfigDB tambi√©n es un √∫nico punto de administraci√≥n, por lo que puede ir a cualquier nodo del cl√∫ster por direcci√≥n IP y administrar completamente todos los nodos del cl√∫ster, lo cual es bastante conveniente. </p><br><p>  Adem√°s, para acceder a sistemas externos, ConfigDB proporciona una API Restful a trav√©s de la cual puede configurar la integraci√≥n con sistemas de terceros.  Por ejemplo, recientemente realizamos una integraci√≥n piloto con varias soluciones rusas en los campos de VDI y seguridad de la informaci√≥n.  Cuando se completen los proyectos, estaremos encantados de escribir detalles t√©cnicos aqu√≠. </p><br><h2 id="kartina-v-celom">  Toda la imagen </h2><br><p>  Como resultado, tenemos dos versiones de la arquitectura del sistema. </p><br><p>  En el primer caso principal, se utilizan nuestro hipervisor Aist basado en KVM y los conmutadores de software Fractal. </p><br><p>  <strong>Escenario 1. Verdadero</strong> </p><br><p><img src="https://habrastorage.org/webt/0-/pb/7j/0-pb7j1-zdrpqone3dlvw5un4py.png"></p><br><p>  En la segunda opci√≥n opcional, cuando desea utilizar el hipervisor ESXi, el esquema es algo complicado.  Para utilizar ESXi, debe instalarse de forma est√°ndar en las unidades locales del cl√∫ster.  A continuaci√≥n, en cada nodo ESXi, se instala la m√°quina virtual vAIR MasterVM, que contiene una distribuci√≥n especial vAIR para ejecutarse como una m√°quina virtual VMware. </p><br><p>  ESXi ofrece todos los discos locales gratuitos mediante reenv√≠o directo a MasterVM.  Dentro de MasterVM, estos discos ya est√°n formateados de manera est√°ndar en ARDFS y entregados al exterior (o m√°s bien, de vuelta a ESXi) utilizando el protocolo iSCSI (y en el futuro tambi√©n habr√° NFS) a trav√©s de las interfaces dedicadas en ESXi.  En consecuencia, las m√°quinas virtuales y la red de software en este caso son proporcionadas por ESXi. </p><br><p>  <strong>Escenario 2. ESXi</strong> </p><br><p><img src="https://habrastorage.org/webt/no/jf/cv/nojfcvippesbznyxdmzpvnmq2se.png"></p><br><p>  Por lo tanto, hemos desmontado todos los componentes principales de la arquitectura vAIR y sus tareas.  En el pr√≥ximo art√≠culo hablaremos sobre la funcionalidad y los planes ya implementados para el futuro cercano. </p><br><p>  Estamos a la espera de comentarios y sugerencias. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/475254/">https://habr.com/ru/post/475254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../475244/index.html">Nuevas caracter√≠sticas de JavaScript esperadas que debe conocer</a></li>
<li><a href="../475246/index.html">Programaci√≥n asincr√≥nica de Python: una breve descripci√≥n</a></li>
<li><a href="../475248/index.html">El uso de polyfills al escribir aplicaciones de navegador cruzado</a></li>
<li><a href="../475250/index.html">Como Redash not√≥ y solucion√≥ un problema que caus√≥ la degradaci√≥n del rendimiento del c√≥digo Python</a></li>
<li><a href="../475252/index.html">C√≥mo criticar a Microsoft</a></li>
<li><a href="../475258/index.html">Una representaci√≥n visual de las elecciones en San Petersburgo: la magia del ajuste de voz</a></li>
<li><a href="../475260/index.html">La diferencia entre una funci√≥n asincr√≥nica y una funci√≥n que devuelve una promesa</a></li>
<li><a href="../475262/index.html">El resumen de materiales frescos del mundo del front-end para la √∫ltima semana No. 388 (4 al 10 de noviembre de 2019)</a></li>
<li><a href="../475264/index.html">Sniffers que podr√≠an: c√≥mo la familia FakeSecurity infect√≥ las tiendas en l√≠nea</a></li>
<li><a href="../475266/index.html">Revertimos los dispositivos m√≥viles 1 bajo Android. C√≥mo agregar un poco de funcionalidad y deshacerse de algunas noches</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>