<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé≤ üíáüèæ üßòüèΩ Vis√£o de m√°quina de alta velocidade no vers√°til dispositivo de classifica√ß√£o de pe√ßas LEGO üíÉüèø üõ¢Ô∏è ü§≥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nos √∫ltimos anos, tenho projetado e fabricado uma m√°quina que pode reconhecer e classificar pe√ßas LEGO. A parte mais importante da m√°quina √© a unidade...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Vis√£o de m√°quina de alta velocidade no vers√°til dispositivo de classifica√ß√£o de pe√ßas LEGO</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480294/"> Nos √∫ltimos anos, tenho projetado e fabricado uma m√°quina que pode reconhecer e classificar pe√ßas LEGO.  A parte mais importante da m√°quina √© a <strong>unidade de captura</strong> , um pequeno compartimento quase completamente fechado no qual h√° uma correia transportadora, ilumina√ß√£o e uma c√¢mera. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/6c7/5d8/6416c75d85738aceecf1162f1d48718d.jpg"></div><br>  <i>Iluminando voc√™ ver√° um pouco mais baixo.</i> <br><br>  A c√¢mera tira fotos das pe√ßas LEGO que chegam pelo transportador e depois transfere as imagens sem fio para um servidor que executa um algoritmo de intelig√™ncia artificial para reconhecer a pe√ßa entre milhares de poss√≠veis elementos LEGO.  Falarei mais sobre o algoritmo de IA em artigos futuros, e este artigo se concentrar√° no processamento que √© realizado entre a sa√≠da bruta da c√¢mera de v√≠deo e a entrada na rede neural. <br><br>  O principal problema que eu precisava resolver era converter o fluxo de v√≠deo do transportador em imagens separadas de partes que uma rede neural poderia usar. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca8/feb/2ea/ca8feb2eafab64d7d835d50e090e7bc5.gif"></div><br>  <i>O objetivo final: mudar de um v√≠deo bruto (√† esquerda) para um conjunto de imagens do mesmo tamanho (√† direita) para transferi-las para uma rede neural.</i>  <i>(em compara√ß√£o com o trabalho real, o gif √© aproximadamente metade da velocidade)</i> <br><br>  Este √© um √≥timo exemplo de tarefa que, aparentemente, parece simples, mas na verdade apresenta muitos obst√°culos √∫nicos e interessantes, muitos dos quais exclusivos das plataformas de vis√£o de m√°quina. <br><br>  A recupera√ß√£o das partes corretas de uma imagem dessa maneira geralmente √© chamada de detec√ß√£o de objeto.  √â exatamente isso que preciso fazer: reconhecer a presen√ßa de objetos, sua localiza√ß√£o e tamanho, para que voc√™ possa gerar <strong>ret√¢ngulos delimitadores</strong> para cada parte de cada quadro. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfc/265/c31/cfc265c31250940a6170ad404f89de08.gif"></div><br>  <i>O mais importante √© encontrar boas caixas delimitadoras (mostradas acima em verde)</i> <br><br>  Vou considerar tr√™s aspectos da solu√ß√£o do problema: <br><br><ul><li>  Preparando-se para eliminar vari√°veis ‚Äã‚Äãdesnecess√°rias </li><li>  Criando um processo a partir de opera√ß√µes simples de vis√£o de m√°quina </li><li>  Manuten√ß√£o de desempenho suficiente em uma plataforma Raspberry Pi com recursos limitados </li></ul><br><h2>  Elimina√ß√£o de vari√°veis ‚Äã‚Äãdesnecess√°rias </h2><br>  No caso de tais tarefas, √© melhor eliminar o maior n√∫mero poss√≠vel de vari√°veis ‚Äã‚Äãantes de usar t√©cnicas de vis√£o de m√°quina.  Por exemplo, n√£o devo me preocupar com condi√ß√µes ambientais, posi√ß√µes diferentes da c√¢mera, perda de informa√ß√µes devido √† sobreposi√ß√£o de algumas partes por outras.  Obviamente, √© poss√≠vel (embora muito dif√≠cil) resolver todas essas vari√°veis ‚Äã‚Äãprogramaticamente, mas felizmente para mim, essa m√°quina √© criada do zero.  Eu mesmo posso me preparar para uma solu√ß√£o bem-sucedida, eliminando toda a interfer√™ncia antes mesmo de come√ßar a escrever o c√≥digo. <br><br>  O primeiro passo √© fixar firmemente a posi√ß√£o, √¢ngulo e foco da c√¢mera.  Com isso, tudo √© simples - no sistema, a c√¢mera √© montada acima do transportador.  N√£o preciso me preocupar com interfer√™ncias de outras partes;  objetos indesejados quase n√£o t√™m chance de entrar na unidade de captura.  Um pouco mais complicado, mas √© muito importante garantir <strong>condi√ß√µes de ilumina√ß√£o constantes</strong> .  N√£o preciso que o reconhecedor de objetos interprete erroneamente a sombra de uma parte m√≥vel ao longo da fita como um objeto f√≠sico.  Felizmente, a unidade de captura √© muito pequena (todo o campo de vis√£o da c√¢mera √© menor que um peda√ßo de p√£o), ent√£o eu tinha controle mais do que suficiente sobre as condi√ß√µes do ambiente. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8ec/9da/652/8ec9da6525ba0a6f906d3e1ef6309647.jpg"></div><br>  <i>Unidade de captura, vista interna.</i>  <i>A c√¢mera est√° no ter√ßo superior do quadro.</i> <br><br>  Uma solu√ß√£o √© tornar o compartimento totalmente fechado para que n√£o aconte√ßa nenhuma luz externa.  Eu tentei essa abordagem usando tiras de LED como fonte de ilumina√ß√£o.  Infelizmente, o sistema acabou sendo muito sombrio - basta apenas um pequeno orif√≠cio na caixa e a luz penetra no compartimento, impossibilitando o reconhecimento de objetos. <br><br>  No final, a melhor solu√ß√£o foi "entupir" todas as outras fontes de luz enchendo o compartimento pequeno com luz forte.  Verificou-se que as fontes de luz que podem ser usadas para iluminar instala√ß√µes residenciais s√£o muito baratas e f√°ceis de usar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/16a/031/025/16a031025c08531ff3ef2f0ef20a2394.jpg"></div><br>  <i>Pegue as sombras!</i> <br><br>  Quando a fonte √© direcionada para o pequeno compartimento, ela obstrui completamente todas as poss√≠veis interfer√™ncias externas da luz.  Esse sistema tamb√©m tem um efeito colateral conveniente: devido √† grande quantidade de luz na c√¢mera, voc√™ pode usar uma velocidade do obturador muito alta, obtendo imagens perfeitamente n√≠tidas das pe√ßas, mesmo quando se move rapidamente ao longo do transportador. <br><br><h2>  Reconhecimento de Objetos </h2><br>  Como consegui transformar esse lindo v√≠deo com ilumina√ß√£o uniforme nas caixas delimitadoras de que eu precisava?  Se voc√™ trabalha com IA, pode sugerir que eu implemente uma rede neural para reconhecimento de objetos como <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow">YOLO</a> ou <a href="https://arxiv.org/abs/1506.01497" rel="nofollow">Faster R-CNN</a> .  Essas redes neurais podem lidar facilmente com a tarefa.  Infelizmente, estou executando o c√≥digo de reconhecimento de objeto no <a href="https://www.raspberrypi.org/" rel="nofollow">Raspberry pi</a> .  Mesmo um computador poderoso teria problemas para executar essas redes neurais convolucionais na frequ√™ncia que eu precisava em cerca de 90FPS.  E o Raspberry pi, que n√£o possui uma GPU compat√≠vel com IA, n√£o conseguiu lidar com uma vers√£o muito simplificada de um desses algoritmos de IA.  Posso transmitir v√≠deo do Pi para outro computador, mas a transmiss√£o de v√≠deo em tempo real √© um processo muito sombrio, e os atrasos e as limita√ß√µes de largura de banda causam problemas s√©rios, especialmente quando voc√™ precisa de uma alta velocidade de transfer√™ncia de dados. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/K9a6mGNmhbc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>YOLO √© muito legal!</i>  <i>Mas eu n√£o preciso de todas as suas fun√ß√µes.</i> <br><br>  Felizmente, pude evitar uma solu√ß√£o dif√≠cil baseada em IA usando as t√©cnicas de vis√£o de m√°quina da ‚Äúvelha escola‚Äù.  A primeira t√©cnica √© a <strong>subtra√ß√£o em segundo plano</strong> , que tenta isolar todas as partes alteradas da imagem.  No meu caso, a √∫nica coisa que se move no campo de vis√£o da c√¢mera s√£o os detalhes da LEGO.  (Obviamente, a fita tamb√©m se move, mas, como possui uma cor uniforme, parece estacion√°ria para a c√¢mera).  Separe esses detalhes da LEGO do plano de fundo e metade do problema est√° resolvido. <br><br>  Para que a subtra√ß√£o em segundo plano funcione, os objetos em primeiro plano devem ser significativamente diferentes do segundo plano.  Os detalhes da LEGO t√™m uma ampla variedade de cores, ent√£o eu tive que escolher a cor de fundo com muito cuidado para que ela estivesse o mais longe poss√≠vel das cores da LEGO.  √â por isso que a fita embaixo da c√¢mera √© feita de papel - ela n√£o s√≥ precisa ser muito uniforme, mas tamb√©m n√£o pode consistir em LEGO; caso contr√°rio, ser√° da cor de uma das partes que eu preciso reconhecer!  Eu escolhi o rosa p√°lido, mas qualquer outra cor pastel, ao contr√°rio das cores comuns da LEGO, servir√°. <br><br>  A maravilhosa biblioteca OpenCV j√° possui v√°rios algoritmos para subtra√ß√£o em segundo plano.  O Subtrator em segundo plano do MOG2 √© o mais complexo deles e, ao mesmo tempo, funciona incrivelmente r√°pido, mesmo no raspberry pi.  No entanto, alimentar quadros de v√≠deo diretamente no MOG2 n√£o funciona muito bem.  Figuras cinza e branco claras est√£o muito pr√≥ximas do brilho de um fundo p√°lido e se perdem nele.  Eu precisava encontrar uma maneira de separar mais claramente a fita das partes, ordenando que o subtractor de fundo olhasse mais de perto a <i>cor</i> , e n√£o o <i>brilho</i> .  Para fazer isso, bastava aumentar a satura√ß√£o das imagens antes de transferi-las para um subtractor de fundo.  Os resultados melhoraram significativamente. <br><br>  Depois de subtrair o plano de fundo, eu precisava usar opera√ß√µes morfol√≥gicas para me livrar do m√°ximo de ru√≠do poss√≠vel.  Para encontrar os contornos das √°reas em branco, voc√™ pode usar a fun√ß√£o findContours () da biblioteca OpenCV.  Ao aplicar v√°rias heur√≠sticas para desviar os loops que cont√™m ru√≠do, voc√™ pode facilmente converter esses loops em caixas delimitadoras predefinidas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/214/785/f54/214785f5468b2b84ae29f212a66d5abd.gif"></div><br><h2>  Desempenho </h2><br>  Uma rede neural √© uma criatura voraz.  Para obter melhores resultados na classifica√ß√£o, ela precisa de imagens de resolu√ß√£o m√°xima e em quantidades t√£o grandes quanto poss√≠vel.  Isso significa que eu preciso grav√°-las em uma taxa de quadros muito alta, mantendo a qualidade e a resolu√ß√£o da imagem.  Eu tenho que espremer o m√°ximo poss√≠vel da c√¢mera e GPU Raspberry PI. <br><br>  Uma <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">documenta√ß√£o</a> muito detalhada <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">para a picamera</a> diz que o chip da c√¢mera V2 pode produzir imagens de 1280x720 pixels em tamanho com uma frequ√™ncia m√°xima de 90 quadros por segundo.  √â uma quantidade incr√≠vel de dados e, embora a c√¢mera possa ger√°-los, isso n√£o significa que um computador possa lidar com isso.  Se eu fosse processar imagens RGB brutas de 24 bits, teria que transferir dados a uma velocidade de aproximadamente 237 MB / s, o que √© demais para a GPU ruim do computador Pi e a SDRAM.  Mesmo ao usar a compacta√ß√£o acelerada por GPU em JPEG, 90fps n√£o podem ser alcan√ßados. <br><br>  O Raspberry Pi √© capaz de exibir imagens YUV brutas e n√£o filtradas.  Embora seja mais dif√≠cil trabalhar do que com RGB, o YUV realmente tem muitas propriedades convenientes.  O mais importante deles √© que ele armazena apenas 12 bits por pixel (para RGB √© de 24 bits). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d50/28a/686/d5028a6862f79b8caf4d6f895dd46d84.png"></div><br>  <i>A cada quatro bytes de Y, h√° um byte U e um byte V, ou seja, 1,5 bytes por pixel.</i> <br><br>  Isso significa que, comparado aos quadros RGB, posso processar o <strong>dobro de</strong> quadros YUV, e isso n√£o est√° contando o tempo adicional que a GPU economiza na convers√£o para imagem RGB. <br><br>  No entanto, essa abordagem imp√µe restri√ß√µes exclusivas ao processo de processamento.  A maioria das opera√ß√µes com um quadro de v√≠deo de tamanho completo consumir√° uma quantidade extremamente grande de mem√≥ria e recursos da CPU.  Dentro dos meus r√≠gidos prazos, nem √© poss√≠vel decodificar um quadro YUV em tela cheia. <br><br>  Felizmente, n√£o preciso processar todo o quadro!  Para o reconhecimento de objetos, os ret√¢ngulos delimitadores n√£o precisam ser precisos; a precis√£o aproximada √© suficiente; portanto, todo o processo de reconhecimento de objetos pode ser realizado com um quadro muito menor.  A opera√ß√£o de redu√ß√£o de zoom n√£o √© necess√°ria para levar em considera√ß√£o todos os pixels de um quadro de tamanho normal, para que os quadros possam ser reduzidos muito rapidamente e sem nenhum custo.  Em seguida, a escala dos ret√¢ngulos delimitadores resultantes aumenta novamente e √© usada para cortar objetos de um quadro YUV de tamanho completo.  Gra√ßas a isso, n√£o preciso decodificar ou processar todo o quadro de alta resolu√ß√£o. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fcd/d6a/9e4/fcdd6a9e466f0b87be4dbb349f19e402.png"></div><br>  Felizmente, gra√ßas ao m√©todo de armazenamento deste formato YUV (veja acima), √© muito f√°cil implementar opera√ß√µes de corte e zoom r√°pidas que funcionam diretamente com o formato YUV.  Al√©m disso, todo o processo pode ser paralelo a quatro n√∫cleos Pi sem problemas.  No entanto, descobri que nem todos os n√∫cleos est√£o acostumados a todo o seu potencial, e isso nos diz que a largura de banda da mem√≥ria ainda √© o gargalo.  Mas, mesmo assim, consegui atingir 70-80FPS na pr√°tica.  Uma an√°lise mais profunda do uso da mem√≥ria pode ajudar a acelerar ainda mais as coisas. <br><br><hr><br>  Se voc√™ quiser saber mais sobre o projeto, leia meu artigo anterior, <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">‚ÄúComo criei mais de 100 mil imagens LEGO rotuladas para aprendizado‚Äù</a> . <br><br>  V√≠deo da opera√ß√£o de toda a m√°quina de triagem: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/04JkdHEX3Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt480294/">https://habr.com/ru/post/pt480294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt480280/index.html">Estudo de seguran√ßa do TurboConf</a></li>
<li><a href="../pt480282/index.html">A hist√≥ria da startup de fitness Peloton: de uma avalia√ß√£o de US $ 8 bilh√µes a publicidade e previs√µes malsucedidas para uma queda de 85% nas a√ß√µes</a></li>
<li><a href="../pt480284/index.html">Minha (nano) experi√™ncia com a API Yandex.Maps ou por que preciso de instru√ß√µes</a></li>
<li><a href="../pt480288/index.html">√â poss√≠vel transmitir e receber informa√ß√µes sem restri√ß√µes quanto √† dist√¢ncia e velocidade da luz?</a></li>
<li><a href="../pt480290/index.html">ZedRipper laptop caseiro em dezesseis Z80</a></li>
<li><a href="../pt480296/index.html">Desenvolvimento reativo do Telegram bot</a></li>
<li><a href="../pt480300/index.html">Em 2011, a quest√£o de saber se o Nginx pertence a Igor Sysoev ou Rambler</a></li>
<li><a href="../pt480302/index.html">Pioneiros e pioneiros. Impressora 3D de constru√ß√£o circular - como tudo come√ßou?</a></li>
<li><a href="../pt480304/index.html">Infer√™ncia de tipo em jscodeshift e TypeScript</a></li>
<li><a href="../pt480306/index.html">Por que bater a porta fechada?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>