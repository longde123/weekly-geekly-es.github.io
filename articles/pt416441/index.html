<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåø üèÑ ü¶ñ Rastreamento de afina√ß√£o ou determina√ß√£o da frequ√™ncia de afina√ß√£o na fala, usando Praat, YAAPT e YIN como exemplos üèéÔ∏è üë©‚Äçüëß‚Äçüë¶ üë©üèø‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No campo do reconhecimento emocional, a voz √© a segunda fonte mais importante de dados emocionais ap√≥s o rosto. A voz pode ser caracterizada por v√°rio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Rastreamento de afina√ß√£o ou determina√ß√£o da frequ√™ncia de afina√ß√£o na fala, usando Praat, YAAPT e YIN como exemplos</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/neurodatalab/blog/416441/"><img src="https://habrastorage.org/getpro/habr/post_images/37d/3f1/975/37d3f19758eb7d646ccff079d37772f8.png" alt="imagem"><br><br>  No campo do reconhecimento emocional, a voz √© a segunda fonte mais importante de dados emocionais ap√≥s o rosto.  A voz pode ser caracterizada por v√°rios par√¢metros.  O tom da voz √© uma das principais caracter√≠sticas, no entanto, no campo da tecnologia ac√∫stica, √© mais correto chamar esse par√¢metro de frequ√™ncia fundamental. <br><br>  A frequ√™ncia do tom fundamental est√° diretamente relacionada ao que chamamos de entona√ß√£o.  E a entona√ß√£o, por exemplo, est√° associada √†s caracter√≠sticas emocionalmente expressivas da voz. <br><br>  N√£o obstante, determinar a frequ√™ncia do tom fundamental n√£o √© uma tarefa completamente trivial, com nuances interessantes.  Neste artigo, discutiremos os recursos dos algoritmos para sua determina√ß√£o e comparamos as solu√ß√µes existentes com exemplos de grava√ß√µes de √°udio espec√≠ficas. <br><a name="habracut"></a><br>  <b>1. Introdu√ß√£o</b> <br><br>  Para come√ßar, lembremos qual √©, em ess√™ncia, a frequ√™ncia do tom fundamental e em quais tarefas ele pode ser necess√°rio.  <i>A frequ√™ncia fundamental</i> , tamb√©m chamada de CHOT, Frequ√™ncia fundamental ou F0, √© a frequ√™ncia das cordas vocais quando elas pronunciam sons sonoros.  Ao pronunciar sons sem tom (n√£o sonoros), por exemplo, falando em um sussurro ou emitindo assobios e assobios, os ligamentos n√£o hesitam, o que significa que essa caracter√≠stica n√£o √© relevante para eles. <br><br>  * Observe que a divis√£o em sons tonais e n√£o tonais n√£o √© equivalente √† divis√£o em vogais e consoantes. <br><br>  A varia√ß√£o de frequ√™ncia do tom fundamental √© bastante grande e pode variar bastante n√£o apenas entre as pessoas (para vozes masculinas m√©dias mais baixas, a frequ√™ncia √© de 70 a 200 Hz e, para vozes femininas, pode chegar a 400 Hz), mas tamb√©m para uma pessoa, especialmente na fala emocional. . <br><br>  A determina√ß√£o da frequ√™ncia do tom fundamental √© usada para resolver uma ampla gama de problemas: <br><br><ul><li>  Reconhecimento de emo√ß√µes, como dissemos acima; </li><li>  Determina√ß√£o de sexo; </li><li>  Ao resolver o problema de segmentar √°udio com v√°rias vozes ou dividir o discurso em frases; </li><li>  Na medicina, para determinar as caracter√≠sticas patol√≥gicas da voz (por exemplo, usando os par√¢metros ac√∫sticos Jitter e Shimmer).  Por exemplo, a identifica√ß√£o de sinais da doen√ßa de Parkinson [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> ].  Jitter e Shimmer tamb√©m podem ser usados ‚Äã‚Äãpara reconhecer emo√ß√µes [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> ]. </li></ul><br>  No entanto, existem v√°rias dificuldades na determina√ß√£o de F0.  Por exemplo, muitas vezes √© poss√≠vel confundir F0 com harm√¥nicos, o que pode levar aos chamados efeitos de duplica√ß√£o de pitch / metade do pitch [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> ].  E em grava√ß√µes de √°udio de baixa qualidade, F0 √© bastante dif√≠cil de calcular, pois o pico desejado em baixas frequ√™ncias quase desaparece. <br><br>  A prop√≥sito, lembra-se da hist√≥ria de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Laurel e Yanny</a> ?  As diferen√ßas nas palavras que as pessoas ouvem ao ouvir a mesma grava√ß√£o de √°udio surgiram precisamente devido √† diferen√ßa na percep√ß√£o F0, que √© influenciada por muitos fatores: idade do ouvinte, grau de fadiga e dispositivo de reprodu√ß√£o.  Portanto, ao ouvir grava√ß√µes em alto-falantes com reprodu√ß√£o de alta qualidade de baixas frequ√™ncias, voc√™ ouvir√° Laurel e em sistemas de √°udio onde as baixas frequ√™ncias s√£o mal reproduzidas, Yanny.  O efeito de transi√ß√£o pode ser visto em um dispositivo, por exemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  E neste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> , a rede neural atua como um ouvinte.  Em outro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo,</a> voc√™ pode ler como o fen√¥meno Yanny / Laurel √© explicado em termos de forma√ß√£o da fala. <br><br>  Como uma an√°lise detalhada de todos os m√©todos para determinar F0 seria muito volumosa, o artigo √© de natureza geral e pode ajudar a navegar no t√≥pico. <br><br>  <b>M√©todos para determinar F0</b> <br><br>  Os m√©todos para determinar F0 podem ser divididos em tr√™s categorias: com base na din√¢mica do tempo do sinal ou no dom√≠nio do tempo;  com base na estrutura de frequ√™ncia ou no dom√≠nio da frequ√™ncia, bem como em m√©todos combinados.  Sugerimos que voc√™ se familiarize com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo de</a> revis√£o sobre o t√≥pico, onde os m√©todos indicados para extrair F0 s√£o analisados ‚Äã‚Äãem detalhes. <br><br>  Observe que qualquer um dos algoritmos discutidos consiste em 3 etapas principais: <br><br>  Pr√©-processamento (filtrando o sinal, dividindo-o em quadros) <br>  Procure por poss√≠veis valores de F0 (candidatos) <br>  O rastreamento √© a escolha da trajet√≥ria mais prov√°vel F0 (j√° que para cada momento em que temos v√°rios candidatos concorrentes, precisamos encontrar o caminho mais prov√°vel entre eles) <br><br>  <b>Dom√≠nio do tempo</b> <br><br>  Delineamos alguns pontos gerais.  Antes de aplicar os m√©todos no dom√≠nio do tempo, o sinal √© pr√©-filtrado, deixando apenas frequ√™ncias baixas.  Os limites s√£o definidos - as frequ√™ncias m√≠nima e m√°xima, por exemplo, de 75 a 500 Hz.  A determina√ß√£o de F0 √© feita apenas para √°reas com fala harm√¥nica, pois para pausas ou sons de ru√≠do isso n√£o √© apenas sem sentido, mas tamb√©m pode introduzir erros em quadros adjacentes quando a interpola√ß√£o e / ou suaviza√ß√£o √© aplicada.  O comprimento do quadro √© selecionado para conter pelo menos tr√™s per√≠odos. <br><br>  O m√©todo principal, com base no qual toda uma fam√≠lia de algoritmos apareceu posteriormente, √© a autocorrela√ß√£o.  A abordagem √© bastante simples - √© necess√°rio calcular a fun√ß√£o de autocorrela√ß√£o e obter o seu primeiro m√°ximo.  Ele exibir√° o componente de frequ√™ncia mais pronunciado no sinal.  Qual poderia ser a dificuldade no uso de autocorrela√ß√£o e por que nem sempre o primeiro m√°ximo corresponde √† frequ√™ncia desejada?  Mesmo em condi√ß√µes pr√≥ximas √†s ideais em grava√ß√µes de alta qualidade, o m√©todo pode estar errado devido √† estrutura complexa do sinal.  Em condi√ß√µes pr√≥ximas do real, onde, entre outras coisas, podemos encontrar o desaparecimento do pico desejado em grava√ß√µes ruidosas ou grava√ß√µes de baixa qualidade inicialmente, o n√∫mero de erros aumenta acentuadamente. <br><br>  Apesar dos erros, o m√©todo de autocorrela√ß√£o √© bastante conveniente e atraente devido √† sua simplicidade e l√≥gica b√°sicas, raz√£o pela qual √© tomado como base em muitos algoritmos, incluindo o YIN.  At√© o nome do algoritmo nos remete a um equil√≠brio entre a conveni√™ncia e a imprecis√£o do m√©todo de autocorrela√ß√£o: ‚ÄúO nome YIN de '' yin '' e '' yang '' da filosofia oriental alude √† intera√ß√£o entre autocorrela√ß√£o e cancelamento que envolve.‚Äù  [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">4</a> ] <br><br>  Os criadores do YIN tentaram corrigir os pontos fracos da abordagem de autocorrela√ß√£o.  A primeira mudan√ßa √© o uso da fun√ß√£o Diferen√ßa Normalizada M√©dia Cumulativa, que deve reduzir a sensibilidade √†s modula√ß√µes de amplitude, tornando os picos mais pronunciados: <br><br>  \ begin {equation} <br>  d'_t (\ tau) = <br>  \ begin {cases} <br>  1, &amp; \ tau = 0 \\ <br>  d_t (\ tau) \ bigg / \ bigg [\ frac {1} {\ tau} \ sum \ limits_ {j = 1} ^ {\ tau} d_t (j) \ bigg] e \ text {caso contr√°rio} <br>  \ end {cases} <br>  \ end {equa√ß√£o} <br>  O YIN tamb√©m tenta evitar erros que ocorrem nos casos em que o comprimento da fun√ß√£o da janela n√£o √© completamente dividido pelo per√≠odo de oscila√ß√£o.  Para isso, √© utilizada interpola√ß√£o m√≠nima parab√≥lica.  Na √∫ltima etapa do processamento do sinal de √°udio, a fun√ß√£o Melhor estimativa local √© executada para evitar saltos acentuados nos valores (bons ou ruins - este √© um ponto discut√≠vel). <br><br>  <b>Dom√≠nio de frequ√™ncia</b> <br><br>  Se falamos sobre o dom√≠nio da frequ√™ncia, a estrutura harm√¥nica do sinal vem √† tona, ou seja, a presen√ßa de picos espectrais em frequ√™ncias que s√£o m√∫ltiplos de F0.  Voc√™ pode "colapsar" esse padr√£o peri√≥dico em um pico claro usando a an√°lise cepstral.  Cepstrum - transformada de Fourier do logaritmo do espectro de pot√™ncia;  o pico cepstral corresponde ao componente mais peri√≥dico do espectro (pode-se ler <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ). <br><br>  <b>M√©todos h√≠bridos para determinar F0</b> <br><br>  O pr√≥ximo algoritmo, que vale a pena explorar com mais detalhes, tem o nome falante YAAPT - mais um algoritmo de rastreamento de afina√ß√£o - e, na verdade, √© h√≠brido, porque usa informa√ß√µes de frequ√™ncia e tempo.  Uma descri√ß√£o completa est√° no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo</a> , aqui descrevemos apenas os est√°gios principais. <br><br><img src="https://habrastorage.org/webt/r2/mu/uj/r2muujzlcxgdgp5a0bqem3t_iuu.png"><br>  <i>Figura 1. Diagrama do algoritmo YAAPTalgo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link</a> )</i> . <br><br>  O YAAPT consiste em v√°rias etapas principais, a primeira das quais √© o pr√©-processamento.  Nesse est√°gio, os valores do sinal original s√£o elevados ao quadrado e uma segunda vers√£o do sinal √© obtida.  Esta etapa persegue o mesmo objetivo que a Fun√ß√£o de Diferen√ßa Normalizada M√©dia Acumulada no YIN - amplifica√ß√£o e restaura√ß√£o de picos "congestionados" de autocorrela√ß√£o.  Ambas as vers√µes do sinal s√£o filtradas - geralmente elas variam de 50 a 1500 Hz, √†s vezes de 50 a 900 Hz. <br><br>  Ent√£o, a trajet√≥ria de base F0 √© calculada a partir do espectro do sinal convertido.  Os candidatos a F0 s√£o determinados usando o recurso Correla√ß√£o de Harm√¥nicas Espectrais (SHC). <br><br>  \ begin {equation} <br>  SHC (t, f) = \ sum \ limits_ {f '= - WL / 2} ^ {WL / 2} \ prod \ limits_ {r = 1} ^ {NH + 1} S (t, rf + f') <br>  \ end {equa√ß√£o} <br>  onde S (t, f) √© o espectro de magnitude para o quadro te a frequ√™ncia f, WL √© o comprimento da janela em Hz, NH √© o n√∫mero de harm√¥nicos (os autores recomendam o uso dos tr√™s primeiros harm√¥nicos).  A pot√™ncia espectral tamb√©m √© usada para determinar os quadros sonoros e n√£o sonoros, ap√≥s os quais a trajet√≥ria mais ideal √© pesquisada e a possibilidade de duplica√ß√£o / metade do tom √© levada em considera√ß√£o [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> , Se√ß√£o II, C]. <br><br>  Al√©m disso, os candidatos a F0 s√£o determinados para o sinal inicial e o convertido e, em vez da fun√ß√£o de autocorrela√ß√£o, a Correla√ß√£o Cruzada Normalizada (NCCF) √© usada aqui. <br><br>  \ begin {equation} <br>  NCCF (m) = \ frac {\ sum \ limits_ {n = 0} ^ {Nm-1} x (n) * x (n + m)} {\ sqrt {\ sum \ limits_ {n = 0} ^ { Nm-1} x ^ 2 (n) * \ sum \ limits_ {n = 0} ^ {Nm-1} x ^ 2 (n + m)}} \ text {,} \ espa√ßo {0,3cm} 0 &lt;m &lt;M_ {0} <br>  \ end {equa√ß√£o} <br>  O pr√≥ximo passo √© avaliar todos os poss√≠veis candidatos e calcular sua signific√¢ncia ou peso (m√©rito).  O peso dos candidatos obtidos a partir do sinal de √°udio depende n√£o apenas da amplitude do pico da NCCF, mas tamb√©m da proximidade da trajet√≥ria F0 determinada a partir do espectro.  Ou seja, o dom√≠nio da frequ√™ncia √© considerado grosseiro em termos de precis√£o, mas est√°vel [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> , Se√ß√£o II, D]. <br><br>  Ent√£o, para todos os pares de candidatos restantes, √© calculada a matriz de Custo de Transi√ß√£o - o pre√ßo de transi√ß√£o, no qual eles finalmente encontram a trajet√≥ria ideal [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> , Se√ß√£o II, E]. <br><br>  <b>Exemplos</b> <br><br>  Agora, aplicamos todos os algoritmos acima a grava√ß√µes de √°udio espec√≠ficas.  Como ponto de partida, usaremos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Praat</a> , uma ferramenta fundamental para muitos estudiosos da fala.  E ent√£o, em Python, examinaremos a implementa√ß√£o do YIN e YAAPT e compararemos os resultados recebidos. <br><br>  Como material de √°udio, voc√™ pode usar qualquer √°udio dispon√≠vel.  Tiramos v√°rios trechos de nosso banco de dados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RAMAS</a> - um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">conjunto de dados</a> multimodal criado com a participa√ß√£o de atores do VGIK.  Voc√™ tamb√©m pode usar material de outros bancos de dados abertos, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LibriSpeech</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RAVDESS</a> . <br><br>  Para um exemplo ilustrativo, extra√≠mos trechos de v√°rias grava√ß√µes com vozes masculinas e femininas, tanto neutras quanto emocionalmente coloridas, e para maior clareza, as combinamos em uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">grava√ß√£o</a> .  Vejamos nosso sinal, seu espectrograma, intensidade (cor laranja) e F0 (cor azul).  No Praat, isso pode ser feito usando Ctrl + O (Abrir - Ler do arquivo) e, em seguida, o bot√£o Exibir e Editar. <br><br><img src="https://habrastorage.org/webt/ir/ay/kh/iraykhpzwfetpahhiymdic6pdwi.png"><br>  <i>Figura 2. Espectrograma, intensidade (cor laranja), F0 (cor azul) em Praat.</i> <br><br>  O √°udio mostra claramente que, na fala emocional, o tom aumenta em homens e mulheres.  Ao mesmo tempo, F0 para a fala emocional do homem pode muito bem ser comparado com o F0 de uma voz feminina. <br><br>  <b>Rastreamento</b> <br><br>  Selecione a guia Analisar periodicidade - para Inclinar (ac) no menu Praat, ou seja, a defini√ß√£o de F0 usando a correla√ß√£o autom√°tica.  Aparecer√° uma janela para definir par√¢metros, na qual √© poss√≠vel definir 3 par√¢metros para determinar candidatos para F0 e mais 6 par√¢metros para o algoritmo de busca de caminhos, que cria o caminho F0 mais prov√°vel entre todos os candidatos. <br><br><div class="spoiler">  <b class="spoiler_title">Muitos par√¢metros (no Praat, sua descri√ß√£o tamb√©m est√° no bot√£o Ajuda)</b> <div class="spoiler_text"><ul><li>  Limiar de sil√™ncio - o limiar da amplitude relativa do sinal para determinar o sil√™ncio, o valor padr√£o √© 0,03. </li><li>  Limiar de sonoridade - o peso do candidato n√£o sonoro, o valor m√°ximo √© 1. Quanto maior esse par√¢metro, mais quadros ser√£o definidos como sonoros, ou seja, sem sons de tom.  Nesses quadros, F0 n√£o ser√° determinado.  O valor deste par√¢metro √© o limite para picos da fun√ß√£o de autocorrela√ß√£o.  O valor padr√£o √© 0,45. </li><li>  Custo de oitava - determina quanto mais peso os candidatos de alta frequ√™ncia t√™m em rela√ß√£o aos de baixa frequ√™ncia.  Quanto maior o valor, mais prefer√™ncia √© dada ao candidato de alta frequ√™ncia.  O valor padr√£o √© 0,01 por oitava. </li><li>  Custo de salto de oitava - com um aumento nesse coeficiente, o n√∫mero de transi√ß√µes n√≠tidas de salto entre valores sucessivos de F0 diminui.  O valor padr√£o √© 0,35. </li><li>  Custo expresso / n√£o expresso - aumentar esse coeficiente diminui o n√∫mero de transi√ß√µes sonoras / n√£o expressas.  O valor padr√£o √© 0,14. </li><li>  Teto de inclina√ß√£o (Hz) - candidatos acima dessa frequ√™ncia n√£o s√£o considerados.  O valor padr√£o √© 600 Hz. </li></ul><br></div></div><br>  Uma descri√ß√£o detalhada do algoritmo pode ser encontrada em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um artigo de</a> 1993. <br><br>  A apar√™ncia do resultado do rastreador (localizador de caminho) pode ser vista clicando em OK e depois visualizando (Exibir e editar) o arquivo Pitch resultante.  Pode-se observar que, al√©m da trajet√≥ria selecionada, ainda havia candidatos bastante significativos com frequ√™ncia mais baixa. <br><br><img src="https://habrastorage.org/webt/wq/rq/vf/wqrqvf_cbnbn8orcajij6sfrasu.png"><br>  <i>Figura 3. PitchPath pelos primeiros 1,3 segundos de grava√ß√£o de √°udio.</i> <br><br>  <b>Mas e o Python?</b> <br><br>  Vamos pegar duas bibliotecas que oferecem rastreamento de pitch - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aubio</a> , no qual o algoritmo padr√£o √© YIN, e a biblioteca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AMFM_decompsition</a> , que possui uma implementa√ß√£o do algoritmo YAAPT.  No arquivo separado (arquivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PraatPitch.txt</a> ), <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">insira os</a> valores F0 do Praat (isso pode ser feito manualmente: selecione o arquivo de som, clique em Exibir e editar, selecione o arquivo inteiro e selecione a listagem de afina√ß√£o no menu superior). <br><br>  Agora compare os resultados para todos os tr√™s algoritmos (YIN, YAAPT, Praat). <br><br><div class="spoiler">  <b class="spoiler_title">Muito c√≥digo</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.basic_tools <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> basic <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> amfm_decompy.pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pYAAPT <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> aubio <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> source, pitch <span class="hljs-comment"><span class="hljs-comment"># load audio signal = basic.SignalObj('/home/eva/Documents/papers/habr/media/audio.wav') filename = '/home/eva/Documents/papers/habr/media/audio.wav' # YAAPT pitches pitchY = pYAAPT.yaapt(signal, frame_length=40, tda_frame_length=40, f0_min=75, f0_max=600) # YIN pitches downsample = 1 samplerate = 0 win_s = 1764 // downsample # fft size hop_s = 441 // downsample # hop size s = source(filename, samplerate, hop_s) samplerate = s.samplerate tolerance = 0.8 pitch_o = pitch("yin", win_s, hop_s, samplerate) pitch_o.set_unit("midi") pitch_o.set_tolerance(tolerance) pitchesYIN = [] confidences = [] total_frames = 0 while True: samples, read = s() pitch = pitch_o(samples)[0] pitch = int(round(pitch)) confidence = pitch_o.get_confidence() pitchesYIN += [pitch] confidences += [confidence] total_frames += read if read &lt; hop_s: break # load PRAAT pitches praat = np.genfromtxt('/home/eva/Documents/papers/habr/PraatPitch.txt', filling_values=0) praat = praat[:,1] # plot fig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(12, 8)) ax1.plot(np.asarray(pitchesYIN), label='YIN', color='green') ax1.legend(loc="upper right") ax2.plot(pitchY.samp_values, label='YAAPT', color='blue') ax2.legend(loc="upper right") ax3.plot(praat, label='Praat', color='red') ax3.legend(loc="upper right") plt.show()</span></span></code> </pre> <br></div></div><br><br><img src="https://habrastorage.org/webt/tv/k7/uq/tvk7uqi50ctcnd3j3rjq0sjzl8s.png"><br>  <i>Figura 4. Compara√ß√£o da opera√ß√£o dos algoritmos YIN, YAAPT e Praat.</i> <br><br>  Vemos que, com os par√¢metros padr√£o, o YIN √© bastante nocauteado, obtendo uma trajet√≥ria bastante plana com valores inferiores a Praat e perdendo completamente as transi√ß√µes entre as vozes masculina e feminina, bem como entre a fala emocional e n√£o emocional. <br><br>  YAAPT cortou um tom muito alto no discurso emocional feminino, mas no geral conseguiu claramente melhor.  Devido √†s suas caracter√≠sticas espec√≠ficas, o YAAPT funciona melhor - √© imposs√≠vel responder imediatamente, √© claro, mas pode-se supor que o papel seja desempenhado pela obten√ß√£o de candidatos de tr√™s fontes e um c√°lculo mais meticuloso do seu peso do que no YIN. <br><br>  <b>Conclus√£o</b> <br><br>  Como a quest√£o de determinar a frequ√™ncia do tom fundamental (F0), de uma forma ou de outra, surge antes de quase todo mundo que trabalha com som, h√° muitas maneiras de resolv√™-lo.  A quest√£o da precis√£o e dos recursos necess√°rios do material de √°udio em cada caso determina com que cuidado √© necess√°rio selecionar par√¢metros ou, em outro caso, voc√™ pode restringir-se a uma solu√ß√£o b√°sica como o YAAPT.  Tomando o Praat como o padr√£o do algoritmo para processamento de fala (no entanto, um grande n√∫mero de pesquisadores o utiliza), podemos concluir que o YAAPT √©, para uma primeira aproxima√ß√£o, mais confi√°vel e preciso que o YIN, embora nosso exemplo tenha sido complicado para ele. <br><br>  Postado por <b>Eva</b> Kazimirova, pesquisadora do Laborat√≥rio Neurodata, especialista em processamento de fala. <br><br>  <font color="green"><b>Offtop</b></font> : Voc√™ gostou do artigo?  De fato, temos v√°rias tarefas interessantes em ML, matem√°tica e programa√ß√£o e precisamos de c√©rebros.  Voc√™ est√° curioso?  Venha para n√≥s!  E-mail: hr@neurodatalab.com <br><br><div class="spoiler">  <b class="spoiler_title">Refer√™ncias</b> <div class="spoiler_text"><ol><li>  Rusz, J., Cmejla, R., Ruzickova, H., Ruzicka, E. Medidas ac√∫sticas quantitativas para caracteriza√ß√£o de dist√∫rbios da fala e da voz na doen√ßa de Parkinson n√£o tratada precoce.  O Jornal da Sociedade Ac√∫stica da Am√©rica, vol.  129, edi√ß√£o 1 (2011), pp.  350-367.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Acesso</a> </li><li>  Farr√∫s, M., Hernando, J., Ejarque, P. Jitter e Shimmer Measurement para reconhecimento de alto-falante.  Anais da Confer√™ncia Anual da International Speech Communication Association, INTERSPEECH, vol.  2 (2007), pp.  1153-1156.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Acesso</a> </li><li>  Zahorian, S., Hu, HA.  M√©todo espectral / temporal para rastreamento de frequ√™ncia fundamental robusto.  O Jornal da Sociedade Ac√∫stica da Am√©rica, vol.  123, edi√ß√£o 6 (2008), pp.  4559-4571.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Acesso</a> </li><li>  De Cheveign√©, A., Kawahara, H. YIN, um estimador de frequ√™ncia fundamental para fala e m√∫sica.  O Jornal da Sociedade Ac√∫stica da Am√©rica, vol.  111, edi√ß√£o 4 (2002), pp.  1917-1930.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Acesso</a> </li></ol></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt416441/">https://habr.com/ru/post/pt416441/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt416431/index.html">Os melhores projetos de blockchain. OIC julho 2018 (vota√ß√£o)</a></li>
<li><a href="../pt416433/index.html">Pergunte a Ethan: As perdas de radia√ß√£o estelar podem explicar a energia escura?</a></li>
<li><a href="../pt416435/index.html">Por que o c√©rebro humano √© t√£o eficaz?</a></li>
<li><a href="../pt416437/index.html">Existem produtos qu√≠micos suficientes nos mundos gelados para sustentar a vida l√°?</a></li>
<li><a href="../pt416439/index.html">iOS 12: agrupamento de notifica√ß√µes</a></li>
<li><a href="../pt416443/index.html">9 segredos do n√∫cleo do ASP.NET</a></li>
<li><a href="../pt416445/index.html">Webinars do Skillbox: os mais interessantes - de gra√ßa</a></li>
<li><a href="../pt416449/index.html">.NET Core + Docker no Raspberry Pi. Isso √© legal?</a></li>
<li><a href="../pt416451/index.html">Bancos de dados de pesquisa da Microsoft agora dispon√≠veis para todos</a></li>
<li><a href="../pt416453/index.html">Esquemas de roubo em sistemas RBS e cinco n√≠veis de contra-a√ß√£o para eles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>