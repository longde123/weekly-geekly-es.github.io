<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì∂ ‚ôéÔ∏è üë©‚Äçüé® Reconocimiento de productos en estanter√≠as que utilizan redes neuronales que utilizan las tecnolog√≠as API Keras y Tensorflow Object Detection ‚õëÔ∏è üëâ üë®üèæ‚Äç‚öñÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el art√≠culo hablaremos sobre el uso de redes neuronales convolucionales para resolver una tarea empresarial pr√°ctica de restaurar un realograma a p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconocimiento de productos en estanter√≠as que utilizan redes neuronales que utilizan las tecnolog√≠as API Keras y Tensorflow Object Detection</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/416123/">  En el art√≠culo hablaremos sobre el uso de redes neuronales convolucionales para resolver una tarea empresarial pr√°ctica de restaurar un realograma a partir de fotograf√≠as de estantes con productos.  Usando la API de detecci√≥n de objetos de Tensorflow, entrenaremos el modelo de b√∫squeda / localizaci√≥n.  Mejoraremos la calidad de la b√∫squeda de productos peque√±os en fotograf√≠as de alta resoluci√≥n utilizando una ventana flotante y un algoritmo de supresi√≥n no m√°xima.  En Keras, estamos implementando un clasificador de productos por marca.  Paralelamente, compararemos enfoques y resultados con decisiones de hace 4 a√±os.  Todos los datos utilizados en el art√≠culo est√°n disponibles para su descarga, y el c√≥digo que funciona completamente est√° en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GitHub</a> y est√° dise√±ado como un tutorial. <br><br><img src="https://habrastorage.org/webt/aw/27/ar/aw27argaeseqsgos5cbpwdwt89s.jpeg"><br><a name="habracut"></a><br><h3>  Introduccion </h3><br>  ¬øQu√© es un planograma?  El diagrama de dise√±o de la exhibici√≥n de productos en el equipo comercial de concreto de la tienda. <br><br>  ¬øQu√© es un realograma?  El dise√±o de los productos en un equipo comercial espec√≠fico existente en la tienda aqu√≠ y ahora. <br><br>  Planograma, como debe ser, realograma, lo que tenemos. <br><br><img src="https://habrastorage.org/webt/ym/oo/ew/ymooewlirqpaosatarrenno8caw.jpeg"><br><br>  Hasta ahora, en muchas tiendas, administrar el resto de los productos en estantes, estantes, mostradores, estanter√≠as es exclusivamente trabajo manual.  Miles de empleados verifican la disponibilidad de productos manualmente, calculan el saldo, verifican la ubicaci√≥n con los requisitos.  Es costoso y los errores son muy probables.  La exhibici√≥n incorrecta o la falta de productos conduce a menores ventas. <br><br>  Adem√°s, muchos fabricantes celebran acuerdos con minoristas para exhibir sus productos.  Y como hay muchos fabricantes, entre ellos comienza la lucha por el mejor lugar en el estante.  Todos quieren que su producto se encuentre en el centro frente a los ojos del comprador y que ocupe el √°rea m√°s grande posible.  Hay una necesidad de auditor√≠a continua. <br><br>  Miles de comerciantes se mueven de una tienda a otra para asegurarse de que los productos de su empresa est√©n en el estante y se presenten de acuerdo con el contrato.  A veces son flojos: es mucho m√°s agradable compilar un informe sin salir de su casa que ir a un punto de venta.  Existe la necesidad de una auditor√≠a permanente de los auditores. <br><br>  Naturalmente, la tarea de automatizaci√≥n y simplificaci√≥n de este proceso se ha resuelto durante mucho tiempo.  Una de las partes m√°s dif√≠ciles fue el procesamiento de im√°genes: encontrar y reconocer productos.  Y solo relativamente recientemente, esta tarea se ha simplificado tanto que para un caso particular en una forma simplificada, su soluci√≥n completa se puede describir en un art√≠culo.  Esto es lo que haremos. <br><br>  El art√≠culo contiene un m√≠nimo de c√≥digo (solo para los casos en que el c√≥digo es m√°s claro que el texto).  La soluci√≥n completa est√° disponible como un tutorial ilustrado en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cuadernos jupyter</a> .  El art√≠culo no contiene una descripci√≥n de la arquitectura de las redes neuronales, los principios de las neuronas, las f√≥rmulas matem√°ticas.  En el art√≠culo, los usamos como una herramienta de ingenier√≠a, sin entrar demasiado en los detalles de su dispositivo. <br><br><h3>  Datos y enfoque </h3><br>  Al igual que con cualquier enfoque basado en datos, las soluciones de redes neuronales requieren datos.  Tambi√©n puede ensamblarlos manualmente: para capturar varios cientos de contadores y marcarlos usando, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LabelImg</a> .  Puede ordenar el marcado, por ejemplo, en Yandex.Tolok. <br><br><img src="https://habrastorage.org/webt/ji/he/lv/jihelvxh9vkmixjzsxink1nknya.jpeg"><br><br>  No podemos revelar los detalles de un proyecto real, por lo tanto, explicaremos la tecnolog√≠a en datos abiertos.  Ir de compras y tomar fotograf√≠as era demasiado vago (y no nos hubieran entendido all√≠), y el deseo de hacer el marcado de las fotos encontradas en Internet por mi cuenta termin√≥ despu√©s del cent√©simo objeto clasificado.  Afortunadamente, por casualidad me encontr√© con el archivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Grocery Dataset</a> . <br><br>  En 2014, los empleados de Idea Teknoloji, Estambul, Turqu√≠a, subieron 354 fotos de 40 tiendas hechas con 4 c√°maras.  En cada una de estas fotograf√≠as, resaltaron con rect√°ngulos un total de varios miles de objetos, algunos de los cuales se clasificaron en 10 categor√≠as. <br><br>  Estas son fotos de cajetillas de cigarrillos.  No promovemos ni promocionamos fumar.  Simplemente no hab√≠a nada m√°s neutral.  Prometemos que en todas partes del art√≠culo, donde la situaci√≥n lo permita, usaremos fotograf√≠as de gatos. <br><br><img src="https://habrastorage.org/webt/04/8x/ek/048xekrylrnspiwd7vefbux_kgu.jpeg"><br><br>  Adem√°s de las fotos etiquetadas de los estantes, escribieron un art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hacia el reconocimiento de productos minoristas en los estantes de comestibles</a> con una soluci√≥n al problema de localizaci√≥n y clasificaci√≥n.  Esto establece un tipo de punto de referencia: nuestra soluci√≥n con nuevos enfoques deber√≠a resultar m√°s simple y precisa, de lo contrario no es interesante.  Su enfoque consiste en una combinaci√≥n de algoritmos: <br><br><img src="https://habrastorage.org/webt/j5/p4/t_/j5p4t_ul9ungv_luvlzxa5aa1gi.jpeg"><br><br>  Recientemente, las redes neuronales convolucionales (CNN) han revolucionado el campo de la visi√≥n por computadora y han cambiado por completo el enfoque para resolver tales problemas.  En los √∫ltimos a√±os, estas tecnolog√≠as han estado disponibles para una amplia gama de desarrolladores, y las API de alto nivel como Keras han reducido significativamente su umbral de entrada.  Ahora, casi cualquier desarrollador puede usar todo el poder de las redes neuronales convolucionales despu√©s de unos pocos d√≠as de citas.  El art√≠culo describe el uso de estas tecnolog√≠as utilizando un ejemplo, que muestra c√≥mo una cascada completa de algoritmos se puede reemplazar f√°cilmente con solo dos redes neuronales sin p√©rdida de precisi√≥n. <br><br>  Resolveremos el problema en pasos: <br><br><ul><li>  Preparaci√≥n de datos.  Bombeamos los archivos y los transformamos en una vista conveniente para el trabajo. </li><li>  Clasificaci√≥n de marca.  Resolvemos el problema de clasificaci√≥n usando una red neuronal. </li><li>  Busca productos en la foto.  Entrenamos la red neuronal para buscar bienes. </li><li>  Implementaci√≥n de b√∫squeda.  Mejoraremos la calidad de detecci√≥n utilizando una ventana flotante y un algoritmo para suprimir los no m√°ximos. </li><li>  Conclusi√≥n  Explique brevemente por qu√© la vida real es mucho m√°s complicada que este ejemplo. </li></ul><br><h3>  Tecnolog√≠a </h3><br>  Las principales tecnolog√≠as que utilizaremos: Tensorflow, Keras, Tensorflow Object Detection API, OpenCV.  Aunque tanto Windows como Mac OS son adecuados para trabajar con Tensorflow, todav√≠a recomendamos usar Ubuntu.  Incluso si nunca antes ha trabajado con este sistema operativo, usarlo le ahorrar√° un mont√≥n de tiempo.  La instalaci√≥n de Tensorflow para trabajar con la GPU es un tema que merece un art√≠culo separado.  Afortunadamente, tales art√≠culos ya existen.  Por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Instalar TensorFlow en Ubuntu 16.04 con una GPU Nvidia</a> .  Algunas instrucciones pueden estar desactualizadas. <br><br>  <b>Paso 1. Preparar los datos ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace github</a> )</b> <br><br>  Este paso, como regla, lleva mucho m√°s tiempo que la simulaci√≥n misma.  Afortunadamente, utilizamos datos listos para usar, que convertimos al formulario que necesitamos. <br><br>  Puede descargar y descomprimir de esta manera: <br><br><pre><code class="bash hljs">wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz tar -xvzf GroceryDataset_part1.tar.gz tar -xvzf GroceryDataset_part2.tar.gz</code> </pre> <br>  Obtenemos la siguiente estructura de carpetas: <br><br><img src="https://habrastorage.org/webt/n1/yi/8n/n1yi8n3faxzmxia70bxsee-b69u.jpeg"><br><br>  Utilizaremos informaci√≥n de los directorios ShelfImages y ProductImagesFromShelves. <br>  ShelfImages contiene im√°genes de los estantes mismos.  En el nombre, se codifica el identificador del bastidor con el identificador de la imagen.  Puede haber varias im√°genes de un estante.  Por ejemplo, una fotograf√≠a en su totalidad y 5 fotograf√≠as en partes con intersecciones. <br><br>  Archivo C1_P01_N1_S2_2.JPG (bastidor C1_P01, instant√°nea N1_S2_2): <br><br><img src="https://habrastorage.org/webt/nv/jd/or/nvjdorlqwj1qc7asuzktk7dqccs.jpeg"><br><br>  Revisamos todos los archivos y recopilamos informaci√≥n en el marco de datos de pandas photos_df: <br><br><img src="https://habrastorage.org/webt/us/zq/zq/uszqzqw3haortnmdvscxp0sq1cq.png"><br>  ProductImagesFromShelves contiene fotos recortadas de productos de estanter√≠as en 11 subdirectorios: 0 - no clasificado, 1 - Marlboro, 2 - Kent, etc.  Para no publicitarlos, utilizaremos solo n√∫meros de categor√≠a sin especificar nombres.  Los archivos en los nombres contienen informaci√≥n sobre el bastidor, la posici√≥n y el tama√±o del paquete en √©l. <br><br>  Archivo C1_P01_N1_S3_1.JPG_1276_1828_276_448.png del directorio 1 (categor√≠a 1, bastidor C1_P01, imagen N1_S3_1, coordenadas de la esquina superior izquierda (1276, 1828), ancho 276, altura 448): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/-x/pc/f0-xpc4nqkymbj-ycs1my_sldhi.jpeg"></div><br>  No necesitamos las fotograf√≠as de los paquetes individuales (las recortaremos de las im√°genes de los estantes), y recopilamos informaci√≥n sobre su categor√≠a y posici√≥n en el marco de datos de pandas products_df: <br><br><img src="https://habrastorage.org/webt/0x/nh/rv/0xnhrvv5nibludwh7svn54hj_ba.png"><br>  En el mismo paso, dividimos toda nuestra informaci√≥n en dos secciones: entrenamiento para entrenamiento y validaci√≥n para monitoreo de entrenamiento.  Por supuesto, esto no vale la pena hacerlo en proyectos reales.  Y tampoco conf√≠es en los que hacen esto.  Al menos debe asignar otra prueba para la prueba final.  Pero incluso con este enfoque no muy honesto, es importante que no nos enga√±emos mucho. <br><br>  Como ya hemos se√±alado, puede haber varias fotos de un estante.  En consecuencia, el mismo paquete puede caer en varias im√°genes.  Por lo tanto, le recomendamos que desglose no por im√°genes, y m√°s a√∫n no por paquetes, sino por bastidores.  Esto es necesario para que no ocurra que el mismo objeto, tomado desde diferentes √°ngulos, termine tanto en el tren como en la validaci√≥n. <br><br>  Hacemos una divisi√≥n de 70/30 (el 30% de los bastidores van para validaci√≥n, el resto para entrenamiento): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get distinct shelves shelves = list(set(photos_df['shelf_id'].values)) # use train_test_split from sklearn shelves_train, shelves_validation, _, _ = train_test_split(   shelves, shelves, test_size=0.3, random_state=6) # mark all records in data frames with is_train flag def is_train(shelf_id): return shelf_id in shelves_train photos_df['is_train'] = photos_df.shelf_id.apply(is_train) products_df['is_train'] = products_df.shelf_id.apply(is_train)</span></span></code> </pre> <br>  Nos aseguraremos de que cuando nos separemos, haya suficientes representantes de cada clase tanto para la capacitaci√≥n como para la validaci√≥n: <br><img src="https://habrastorage.org/webt/9n/w_/xj/9nw_xjw1qiqc21sbi5q3s0pv3_q.jpeg"><br>  El color azul muestra la cantidad de productos en la categor√≠a para validaci√≥n y el naranja para capacitaci√≥n.  La situaci√≥n no es muy buena con la categor√≠a 3 para validaci√≥n, pero en principio hay pocos de sus representantes. <br><br>  En la etapa de preparaci√≥n de datos, es importante no cometer errores, ya que todo el trabajo adicional se basa en sus resultados.  Todav√≠a cometimos un error y pasamos muchas horas felices tratando de entender por qu√© la calidad de los modelos es muy mediocre.  Ya me sent√≠a como un perdedor de las tecnolog√≠as de la "vieja escuela", hasta que not√≥ accidentalmente que algunas de las fotos originales se rotaron 90 grados, y algunas se hicieron boca abajo. <br><br>  Al mismo tiempo, el marcado se realiza como si las fotos estuvieran orientadas correctamente.  Despu√©s de una soluci√≥n r√°pida, las cosas fueron mucho m√°s divertidas. <br><br>  Guardaremos nuestros datos en archivos pkl para usar en los siguientes pasos.  Total, tenemos: <br><br><ul><li>  Un directorio de fotograf√≠as de bastidores y sus partes con paquetes, </li><li>  Un marco de datos con una descripci√≥n de cada bastidor con una nota sobre si est√° destinado a capacitaci√≥n, </li><li>  Un marco de datos con informaci√≥n sobre todos los productos en los estantes, indicando su posici√≥n, tama√±o, categor√≠a y marcando si est√°n destinados a capacitaci√≥n. </li></ul><br>  Para la verificaci√≥n, mostramos un rack de acuerdo con nuestros datos: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function to display shelf photo with rectangled products def draw_shelf_photo(file):   file_products_df = products_df[products_df.file == file]   coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values   im = cv2.imread(f'{shelf_images}{file}')   im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)      for xmin, ymin, xmax, ymax in coordinates:       cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)   plt.imshow(im) # draw one photo to check our data fig = plt.gcf() fig.set_size_inches(18.5, 10.5) draw_shelf_photo('C3_P07_N1_S6_1.JPG')</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/1m/7a/xr/1m7axrc3gcdvgt1sg0sc9-dbs0y.png"><br><br>  <b>Paso 2. Clasificaci√≥n por marca ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace en github</a> )</b> <br><br>  La clasificaci√≥n de im√°genes es la tarea principal en el campo de la visi√≥n por computadora.  El problema es la "brecha sem√°ntica": la fotograf√≠a es solo una gran matriz de n√∫meros [0, 255].  Por ejemplo, 800x600x3 (3 canales RGB). <br><br><img src="https://habrastorage.org/webt/0w/mi/rx/0wmirxot0tx0_dl_b-m_gpdunse.jpeg"><br><br>  Por qu√© esta tarea es dif√≠cil: <br><br><img src="https://habrastorage.org/webt/gx/il/fh/gxilfhn6woijdjngbgrbgfzgjmo.png"><br><br>  Como ya hemos dicho, los autores de los datos que utilizamos identificaron 10 marcas.  Esta es una tarea extremadamente simplificada, ya que hay muchas m√°s marcas de cigarrillos en los estantes.  Pero todo lo que no entraba en estas 10 categor√≠as se envi√≥ a 0, no clasificado: <br><br><img src="https://habrastorage.org/webt/wv/-j/hm/wv-jhmn18kt8ta1zhsai4fzdxbq.png">  " <br><br>  Su art√≠culo ofrece un algoritmo de clasificaci√≥n con una precisi√≥n total del 92%: <br><img src="https://habrastorage.org/webt/vv/d3/ub/vvd3ubvr_tvjxasrwvs7jap6h7o.jpeg"><br>  ¬øQu√© haremos? <br><br><ul><li>  Prepararemos los datos para el entrenamiento, </li><li>  Entrenamos una red neuronal convolucional con la arquitectura ResNet v1, </li><li>  Verifique las fotos para la validaci√≥n. </li></ul><br>  Suena "voluminoso", pero acabamos de usar el ejemplo de Keras " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Entrena una ResNet en el conjunto de datos CIFAR10</a> " tomando de √©l la funci√≥n de crear ResNet v1. <br><br>  Para comenzar el proceso de capacitaci√≥n, debe preparar dos matrices: x: fotograf√≠as de paquetes con dimensi√≥n (n√∫mero de paquetes, altura, ancho, 3) ey: sus categor√≠as con dimensi√≥n (cantidad de paquetes, 10).  La matriz y contiene los llamados vectores 1-hot.  Si la categor√≠a de un paquete para entrenamiento tiene el n√∫mero 2 (de 0 a 9), entonces corresponde al vector [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. <br><br>  Una pregunta importante es qu√© hacer con el ancho y la altura, porque todas las fotos fueron tomadas con diferentes resoluciones desde diferentes distancias.  Necesitamos elegir un tama√±o fijo, al que podamos traer todas nuestras fotos de los paquetes.  Este tama√±o fijo es un metapar√°metro que determina c√≥mo se entrenar√° y funcionar√° nuestra red neuronal. <br><br>  Por un lado, quiero que este tama√±o sea lo m√°s grande posible para que ning√∫n detalle de la imagen pase desapercibido.  Por otro lado, con nuestra escasa cantidad de datos de entrenamiento, esto puede conducir a una r√°pida capacitaci√≥n: el modelo funcionar√° perfectamente en los datos de entrenamiento, pero mal en los datos de validaci√≥n.  Elegimos el tama√±o 120x80, tal vez en un tama√±o diferente obtendr√≠amos un mejor resultado.  Funci√≥n de zoom: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT def resize_pack(pack):   fx_ratio = SHAPE_WIDTH / pack.shape[1]   fy_ratio = SHAPE_HEIGHT / pack.shape[0]      pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)   return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]</span></span></code> </pre> <br>  Escale y muestre un paquete para verificaci√≥n.  El nombre de la marca es dif√≠cil de leer por una persona, veamos c√≥mo la red neuronal har√° frente a la tarea de clasificaci√≥n: <br><br><img src="https://habrastorage.org/webt/z_/8p/0f/z_8p0f5kuxx27mneryilqv81ols.png"><br><br>  Despu√©s de preparar de acuerdo con el indicador obtenido en el paso anterior, dividimos las matrices x e y en x_train / x_validation y y_train / y_validation, obtenemos: <br><br><pre> <code class="bash hljs">x_train shape: (1969, 120, 80, 3) y_train shape: (1969, 10) 1969 train samples 775 validation samples</code> </pre><br>  Los datos est√°n preparados, copiamos la funci√≥n del constructor de la red neuronal de la arquitectura ResNet v1 del ejemplo de Keras: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">resnet_v1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape, depth, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   ‚Ä¶</code> </pre> <br>  Construimos un modelo: <br><br><pre> <code class="python hljs">model = resnet_v1(input_shape=x_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:], depth=depth, num_classes=num_classes) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>,             optimizer=Adam(lr=lr_schedule(<span class="hljs-number"><span class="hljs-number">0</span></span>)), metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Tenemos un conjunto de datos bastante limitado.  Por lo tanto, para evitar que el modelo vea la misma foto cada vez durante el entrenamiento, utilizamos el aumento: cambia aleatoriamente la imagen y g√≠rala un poco.  Keras proporciona este conjunto de opciones para esto: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator(   featurewise_center=False,  # set input mean to 0 over the dataset   samplewise_center=False,  # set each sample mean to 0   featurewise_std_normalization=False,  # divide inputs by std of the dataset   samplewise_std_normalization=False,  # divide each input by its std   zca_whitening=False,  # apply ZCA whitening   rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)   width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)   height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)   horizontal_flip=False,  # randomly flip images   vertical_flip=False)  # randomly flip images datagen.fit(x_train)</span></span></code> </pre> <br>  Comenzamos el proceso de entrenamiento. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's run training process, 20 epochs is enough batch_size = 50 epochs = 15 model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),                   validation_data=(x_validation, y_validation),                   epochs=epochs, verbose=1, workers=4,                   callbacks=[LearningRateScheduler(lr_schedule)])</span></span></code> </pre> <br>  Despu√©s del entrenamiento y la evaluaci√≥n, obtenemos precisi√≥n en la regi√≥n del 92%.  Puede obtener una precisi√≥n diferente: hay muy pocos datos, por lo que la precisi√≥n depende en gran medida del √©xito de la partici√≥n.  En esta partici√≥n, no obtuvimos una precisi√≥n significativamente mayor que la indicada en el art√≠culo, pero pr√°cticamente no hicimos nada nosotros mismos y escribimos poco c√≥digo.  Adem√°s, podemos agregar f√°cilmente una nueva categor√≠a, y la precisi√≥n deber√≠a (en teor√≠a) aumentar significativamente si preparamos m√°s datos. <br><br>  Por inter√©s, compare las matrices de confusi√≥n: <br><img src="https://habrastorage.org/webt/ee/ns/tm/eenstmjcconudxyjjkmtpzcdmow.jpeg"><br>  Casi todas las categor√≠as nuestra red neuronal define mejor, excepto las categor√≠as 4 y 7. Tambi√©n es √∫til observar a los representantes m√°s brillantes de cada celda de matriz de confusi√≥n: <br><img src="https://habrastorage.org/webt/qv/tm/lw/qvtmlwxgqvbdhut73zgsbbtfqqo.jpeg"><br>  Tambi√©n puedes entender por qu√© el Parlamento se confundi√≥ con Camel, pero por qu√© Winston se confundi√≥ con Lucky Strike es completamente incomprensible, pero no tienen nada en com√∫n.  Este es el principal problema de las redes neuronales: la opacidad completa de lo que sucede en su interior.  Por supuesto, puede visualizar algunas capas, pero para nosotros esta visualizaci√≥n se ve as√≠: <br><br><img src="https://habrastorage.org/webt/w1/tn/et/w1tnetor61yz-uwvmjlserwvh3m.jpeg"><br><br>  Una oportunidad obvia para mejorar la calidad del reconocimiento en nuestras condiciones es agregar m√°s fotos. <br><br>  Entonces, el clasificador est√° listo.  Ve al detector. <br><br>  <b>Paso 3. Busque productos en la foto ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace en github</a> )</b> <br><br>  Las siguientes tareas importantes en el campo de la visi√≥n por computadora son la segmentaci√≥n sem√°ntica, la localizaci√≥n, la b√∫squeda de objetos y la segmentaci√≥n de instancias. <br><br><img src="https://habrastorage.org/webt/z0/y9/bf/z0y9bfe7f6qp143c3wubj8rvnd8.jpeg"><br><br>  Nuestra tarea necesita detecci√≥n de objetos.  El art√≠culo de 2014 ofrece un enfoque basado en el m√©todo Viola-Jones y HOG con precisi√≥n visual: <br><br><img src="https://habrastorage.org/webt/11/jt/8j/11jt8jcggyutkrxyelswr8psm8s.jpeg"><br><br>  Gracias al uso de restricciones estad√≠sticas adicionales, su precisi√≥n es muy buena: <br><br><img src="https://habrastorage.org/webt/od/nb/yl/odnbylwn0yo-k92q_nu6mazz8fq.jpeg"><br><br>  Ahora la tarea de reconocimiento de objetos se resuelve con √©xito con la ayuda de redes neuronales.  Utilizaremos el sistema API de detecci√≥n de objetos de Tensorflow y entrenaremos una red neuronal con la arquitectura Mobilenet V1 SSD.  La capacitaci√≥n de un modelo de este tipo desde cero requiere una gran cantidad de datos y puede llevar d√≠as, por lo que utilizamos un modelo capacitado en datos COCO de acuerdo con el principio del aprendizaje por transferencia. <br><br>  El concepto clave de este enfoque es este.  ¬øPor qu√© un ni√±o no necesita mostrar millones de objetos para que aprenda a encontrar y distinguir una bola de un cubo?  Porque el ni√±o tiene 500 millones de a√±os de desarrollo de la corteza visual.  La evoluci√≥n ha hecho de la visi√≥n el sistema sensorial m√°s grande.  Casi el 50% (pero esto no es exacto) de las neuronas del cerebro humano son responsables del procesamiento de im√°genes.  Los padres solo pueden mostrar la pelota y el cubo, y luego corregir al ni√±o varias veces para que encuentre y distinga perfectamente uno del otro. <br><br>  Desde un punto de vista filos√≥fico (con diferencias t√©cnicas m√°s que generales), el aprendizaje de transferencia en redes neuronales funciona de manera similar.  Las redes neuronales convolucionales consisten en niveles, cada uno de los cuales define formas cada vez m√°s complejas: identifica puntos clave, los combina en l√≠neas, que a su vez se combinan en figuras.  Y solo en el √∫ltimo nivel de la totalidad de los signos encontrados determina el objeto. <br><br>  Los objetos del mundo real tienen mucho en com√∫n.  Al transferir el aprendizaje, utilizamos los niveles de definici√≥n de caracter√≠sticas b√°sicas ya entrenados y entrenamos solo a las capas responsables de identificar objetos.  Para hacer esto, un par de cientos de fotos y un par de horas de funcionamiento de una GPU normal son suficientes para nosotros.  La red se form√≥ originalmente en el conjunto de datos COCO (Microsoft Common Objects in Context), que tiene 91 categor√≠as y 2,500,000 im√°genes.  Muchos, aunque no 500 millones de a√±os de evoluci√≥n. <br><br>  Mirando un poco hacia adelante, esta animaci√≥n gif (un poco lenta, no se desplaza inmediatamente) del tablero de tensor visualiza el proceso de aprendizaje.  Como puede ver, el modelo comienza a producir un resultado de alta calidad casi de inmediato, y luego viene la molienda: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2d/356/aa4/d2d356aa49bedf31aa918930d8bf1545.gif" alt="imagen"><br><br>  El "entrenador" del sistema API de detecci√≥n de objetos de Tensorflow puede realizar aumentos de forma independiente, recortar partes aleatorias de im√°genes para el entrenamiento y seleccionar ejemplos "negativos" (secciones de fotos que no contienen ning√∫n objeto).  En teor√≠a, no se necesita preprocesamiento de fotos.  Sin embargo, en una computadora hogare√±a con un disco duro y una peque√±a cantidad de RAM, se neg√≥ a trabajar con im√°genes de alta resoluci√≥n: al principio colg√≥ durante mucho tiempo, cruji√≥ con un disco y luego sali√≥ volando. <br><br>  Como resultado, comprimimos las fotos a un tama√±o de 1000x1000 p√≠xeles manteniendo la relaci√≥n de aspecto.  Pero dado que al comprimir una foto grande, se pierden muchos signos, primero se cortaron varios cuadrados de un tama√±o aleatorio de cada foto del estante y se comprimieron en 1000x1000.  Como resultado, los paquetes en alta resoluci√≥n (pero no suficiente) y en peque√±o (pero muchos) cayeron en los datos de entrenamiento.  Repetimos: este paso es forzado y, muy probablemente, completamente innecesario y posiblemente da√±ino. <br><br>  Las fotos preparadas y comprimidas se guardan en directorios separados (eval y train), y su descripci√≥n (con los paquetes contenidos en ellas) se forma en forma de dos marcos de datos de pandas (train_df y eval_df): <br><br><img src="https://habrastorage.org/webt/dl/lx/pn/dllxpn03xu5us7wr6h7y3rgzayg.png"><br>  El sistema API de detecci√≥n de objetos de Tensorflow requiere que la entrada se presente como archivos tfrecord.  Puede formarlos utilizando la utilidad, pero lo convertiremos en un c√≥digo: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">class_text_to_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row_label)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> row_label == <span class="hljs-string"><span class="hljs-string">'pack'</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, group)</span></span></span><span class="hljs-function">:</span></span>   data = namedtuple(<span class="hljs-string"><span class="hljs-string">'data'</span></span>, [<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, <span class="hljs-string"><span class="hljs-string">'object'</span></span>])   gb = df.groupby(group)   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [data(filename, gb.get_group(x))           <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(gb.groups.keys(), gb.groups)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_tf_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(group, path)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.gfile.GFile(os.path.join(path, <span class="hljs-string"><span class="hljs-string">'{}'</span></span>.format(group.filename)), <span class="hljs-string"><span class="hljs-string">'rb'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> fid:       encoded_jpg = fid.read()   encoded_jpg_io = io.BytesIO(encoded_jpg)   image = Image.open(encoded_jpg_io)   width, height = image.size   filename = group.filename.encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>)   image_format = <span class="hljs-string"><span class="hljs-string">b'jpg'</span></span>   xmins = []   xmaxs = []   ymins = []   ymaxs = []   classes_text = []   classes = []   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index, row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> group.object.iterrows():       xmins.append(row[<span class="hljs-string"><span class="hljs-string">'xmin'</span></span>] / width)       xmaxs.append(row[<span class="hljs-string"><span class="hljs-string">'xmax'</span></span>] / width)       ymins.append(row[<span class="hljs-string"><span class="hljs-string">'ymin'</span></span>] / height)       ymaxs.append(row[<span class="hljs-string"><span class="hljs-string">'ymax'</span></span>] / height)       classes_text.append(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>].encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>))       classes.append(class_text_to_int(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]))   tf_example = tf.train.Example(features=tf.train.Features(feature={       <span class="hljs-string"><span class="hljs-string">'image/height'</span></span>: dataset_util.int64_feature(height),       <span class="hljs-string"><span class="hljs-string">'image/width'</span></span>: dataset_util.int64_feature(width),       <span class="hljs-string"><span class="hljs-string">'image/filename'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/source_id'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/encoded'</span></span>: dataset_util.bytes_feature(encoded_jpg),       <span class="hljs-string"><span class="hljs-string">'image/format'</span></span>: dataset_util.bytes_feature(image_format),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmin'</span></span>: dataset_util.float_list_feature(xmins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmax'</span></span>: dataset_util.float_list_feature(xmaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymin'</span></span>: dataset_util.float_list_feature(ymins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymax'</span></span>: dataset_util.float_list_feature(ymaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/class/text'</span></span>: dataset_util.bytes_list_feature(classes_text),       <span class="hljs-string"><span class="hljs-string">'image/object/class/label'</span></span>: dataset_util.int64_list_feature(classes),   }))   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf_example <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert_to_tf_records</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_path, examples, dst_file)</span></span></span><span class="hljs-function">:</span></span>   writer = tf.python_io.TFRecordWriter(dst_file)   grouped = split(examples, <span class="hljs-string"><span class="hljs-string">'filename'</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grouped:       tf_example = create_tf_example(group, images_path)       writer.write(tf_example.SerializeToString())   writer.close() convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">train/'</span></span>, train_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">train.record'</span></span>) convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">eval/'</span></span>, eval_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">eval.record'</span></span>)</code> </pre><br>  Nos queda preparar un directorio especial e iniciar los procesos: <br><br><img src="https://habrastorage.org/webt/ld/yd/rb/ldydrbk3ivkixsx9kep69b5eyoy.jpeg"><br><br>  La estructura puede ser diferente, pero nos parece muy conveniente. <br><br>  El directorio de datos contiene los archivos que hemos creado con tfrecords (train.record y eval.record), as√≠ como pack.pbtxt con los tipos de objetos para los que entrenaremos la red neuronal.  Solo tenemos que definir un tipo de objeto, por lo que el archivo es muy corto: <br><br><img src="https://habrastorage.org/webt/pc/jw/x0/pcjwx0l3fc-wuzhiyt7lfo9xtcq.png"><br><br>  El directorio de modelos (puede haber muchos modelos para resolver un problema) en el directorio secundario ssd_mobilenet_v1 contiene la configuraci√≥n para el entrenamiento en el archivo .config, as√≠ como dos directorios vac√≠os: train y eval.  En el tren, el "entrenador" guardar√° los puntos de control del modelo, el "evaluador" los recoger√°, los ejecutar√° en los datos para evaluaci√≥n y los colocar√° en el directorio eval.  Tensorboard har√° un seguimiento de estos dos directorios y mostrar√° informaci√≥n del proceso. <br><br>  Descripci√≥n detallada de la estructura de los archivos de configuraci√≥n, etc.  Se puede encontrar <a href="">aqu√≠</a> y <a href="">aqu√≠</a> .  Las instrucciones de instalaci√≥n de la API de detecci√≥n de objetos de Tensorflow se pueden encontrar <a href="">aqu√≠</a> . <br><br>  Entramos en el directorio models / research / object_detection y desinflamos el modelo pre-entrenado: <br><br><pre> <code class="bash hljs">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</code> </pre><br>  Copiamos el directorio pack_detector preparado por nosotros all√≠. <br><br>  Primero, comience el proceso de capacitaci√≥n: <br><br><pre> <code class="bash hljs">python3 train.py --logtostderr \   --train_dir=pack_detector/models/ssd_mobilenet_v1/train/ \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config</code> </pre><br>  Iniciamos el proceso de evaluaci√≥n.  No tenemos una segunda tarjeta de video, as√≠ que la lanzamos en el procesador (usando la instrucci√≥n CUDA_VISIBLE_DEVICES = "").  Debido a esto, llegar√° muy tarde con respecto al proceso de capacitaci√≥n, pero esto no es tan malo: <br><br><pre> <code class="bash hljs">CUDA_VISIBLE_DEVICES=<span class="hljs-string"><span class="hljs-string">""</span></span> python3 eval.py \   --logtostderr \   --checkpoint_dir=pack_detector/models/ssd_mobilenet_v1/train \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --eval_dir=pack_detector/models/ssd_mobilenet_v1/<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span></code> </pre> <br>  Comenzamos el proceso de tensorboard: <br><br><pre> <code class="bash hljs">tensorboard --logdir=pack_detector/models/ssd_mobilenet_v1</code> </pre> <br>  Despu√©s de eso, podemos ver gr√°ficos hermosos, as√≠ como el trabajo real del modelo en los datos estimados (gif al principio): <br><br><img src="https://habrastorage.org/webt/qc/wt/rl/qcwtrlmdugb4zgyhy6gnnaoia9w.jpeg"><br><br>  El proceso de capacitaci√≥n se puede detener y reanudar en cualquier momento.  Cuando creemos que el modelo es lo suficientemente bueno, guardamos el punto de control en forma de un gr√°fico de inferencia: <br><br><pre> <code class="bash hljs">python3 export_inference_graph.py \   --input_type image_tensor \   --pipeline_config_path pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --trained_checkpoint_prefix pack_detector/models/ssd_mobilenet_v1/train/model.ckpt-13756 \   --output_directory pack_detector/models/ssd_mobilenet_v1/pack_detector_2018_06_03</code> </pre> <br>  Entonces, en este paso tenemos un gr√°fico de inferencia, que podemos usar para buscar objetos de paquete.  Pasamos a su uso. <br><br>  <b>Paso 4. Implementar la b√∫squeda ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace github</a> )</b> <br><br>  El c√≥digo de carga e inicializaci√≥n del gr√°fico de inferencia se encuentra en el enlace de arriba.  Funciones clave de b√∫squeda: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's write function that executes detection def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):   # Run inference   expanded_dims = np.expand_dims(image, 0)   output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})   # all outputs are float32 numpy arrays, so convert types as appropriate   output_dict['num_detections'] = int(output_dict['num_detections'][0])   output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)   output_dict['detection_boxes'] = output_dict['detection_boxes'][0]   output_dict['detection_scores'] = output_dict['detection_scores'][0]   return output_dict # it is useful to be able to run inference not only on the whole image, # but also on its parts # cutoff - minimum detection score needed to take box def run_inference_for_image_part(image_tensor, sess, tensor_dict,                                image, cutoff, ax0, ay0, ax1, ay1):   boxes = []   im = image[ay0:ay1, ax0:ax1]   h, w, c = im.shape   output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)   for i in range(100):       if output_dict['detection_scores'][i] &lt; cutoff:           break       y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \                               output_dict['detection_scores'][i]       x0, y0, x1, y1, score = int(x0*w), int(y0*h), \                               int(x1*w), int(y1*h), \                               int(score * 100)       boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))   return boxes</span></span></code> </pre> <br>  La funci√≥n busca cuadros delimitados para paquetes no en toda la foto, sino en su parte.  La funci√≥n tambi√©n filtra los rect√°ngulos encontrados con una puntuaci√≥n de detecci√≥n baja especificada en el par√°metro de corte. <br><br>  Resulta ser un dilema.  Por un lado, con un corte alto, perdemos muchos objetos, por otro lado, con un corte bajo, comenzamos a encontrar muchos objetos que no son paquetes.  Al mismo tiempo, todav√≠a no encontramos todo y no idealmente: <br><img src="https://habrastorage.org/webt/-u/ao/7y/-uao7ylwycrzrn3xqd1kfh2q0bs.jpeg"><br>  Sin embargo, tenga en cuenta que si ejecutamos la funci√≥n para una peque√±a parte de la foto, el reconocimiento es casi perfecto con el corte = 0.9: <br><br><img src="https://habrastorage.org/webt/gg/qn/ia/ggqniasnrkggb6bjnarwum_6i48.jpeg"><br><br>  Esto se debe al hecho de que el modelo SSD MobileNet V1 acepta fotos de 300x300 como entrada.  Naturalmente, con tal compresi√≥n se pierden muchos signos. <br><br>  Pero estos signos persisten si cortamos un peque√±o cuadrado que contiene varios paquetes.  Esto sugiere la idea de usar una ventana flotante: corremos a trav√©s de un peque√±o rect√°ngulo en una fotograf√≠a y recordamos todo lo que encontramos. <br><br><img src="https://habrastorage.org/webt/zn/7s/dz/zn7sdzl4wcb9dwugzkxilg2d2hk.jpeg"><br><br>  :          ,     .         .   :          (detection score),  ,    ,        overlapTresh (       ): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function for non-maximum suppression def non_max_suppression(boxes, overlapThresh):   if len(boxes) == 0:       return np.array([]).astype("int")   if boxes.dtype.kind == "i":       boxes = boxes.astype("float")   pick = []   x1 = boxes[:,0]   y1 = boxes[:,1]   x2 = boxes[:,2]   y2 = boxes[:,3]   sc = boxes[:,4]   area = (x2 - x1 + 1) * (y2 - y1 + 1)   idxs = np.argsort(sc)   while len(idxs) &gt; 0:       last = len(idxs) - 1       i = idxs[last]       pick.append(i)       xx1 = np.maximum(x1[i], x1[idxs[:last]])       yy1 = np.maximum(y1[i], y1[idxs[:last]])       xx2 = np.minimum(x2[i], x2[idxs[:last]])       yy2 = np.minimum(y2[i], y2[idxs[:last]])       w = np.maximum(0, xx2 - xx1 + 1)       h = np.maximum(0, yy2 - yy1 + 1)       #todo fix overlap-contains...       overlap = (w * h) / area[idxs[:last]]              idxs = np.delete(idxs, np.concatenate(([last],           np.where(overlap &gt; overlapThresh)[0])))     return boxes[pick].astype("int")</span></span></code> </pre> <br>     : <br><br><img src="https://habrastorage.org/webt/go/hd/yz/gohdyzvwuyu8ja4w893big9c1yy.jpeg"><br><br>          : <br><br><img src="https://habrastorage.org/webt/jm/oi/da/jmoida7irvm4u3drey8nu6g00kw.jpeg"><br><br>   ,           ,    . <br><br><h3>  Conclusi√≥n </h3><br>       ¬´¬ª:         ,       . ,    ,         ..    . <br><br>       ,    ,    : <br><br><ol><li>  150  ,     ,   , </li><li>        3-7  , </li><li>   100    , </li><li>        , </li><li>        (), </li><li>    (,  ), </li><li>    ,        ¬´¬ª, </li><li>  ,   ,     (SSD  ), </li><li>      ,  , </li><li>  . </li></ol><br>         ,      ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es416123/">https://habr.com/ru/post/es416123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es416111/index.html">Conversi√≥n de datos GraphQL para el componente CustomTreeData de DevExtreme-Reactive</a></li>
<li><a href="../es416113/index.html">Steven Wolfram: Memorias de Steve Jobs</a></li>
<li><a href="../es416115/index.html">10 peque√±os errores de dise√±o que a√∫n cometemos</a></li>
<li><a href="../es416119/index.html">Publicaci√≥n del viernes el mi√©rcoles: los mejores paquetes de NPM m√°s "esenciales"</a></li>
<li><a href="../es416121/index.html">Fujitsu Artificial Intelligence calcula la geometr√≠a de los materiales magn√©ticos.</a></li>
<li><a href="../es416125/index.html">Instalaci√≥n, configuraci√≥n del sistema y control de c√°maras.</a></li>
<li><a href="../es416127/index.html">CUDA y GPU remota</a></li>
<li><a href="../es416129/index.html">C√≥mo AI aprende a generar im√°genes de gatos</a></li>
<li><a href="../es416131/index.html">C√≥mo manejar la EP en la Federaci√≥n de Rusia y no violar la ley</a></li>
<li><a href="../es416135/index.html">Aplicaci√≥n GUI menor a 1 kb</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>