<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèø üë∏üèæ üêà Regroupement hi√©rarchique des donn√©es cat√©gorielles dans R üë©üèª‚Äçü§ù‚Äçüë®üèæ üôèüèø üöë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La traduction a √©t√© pr√©par√©e pour les √©tudiants du cours ¬´Applied Analytics on R¬ª . 




 C'√©tait ma premi√®re tentative de regrouper des clients sur l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Regroupement hi√©rarchique des donn√©es cat√©gorielles dans R</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/461741/">  <i>La traduction a √©t√© pr√©par√©e pour les √©tudiants du cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´Applied Analytics on R¬ª</a> .</i> <br><br><img src="https://habrastorage.org/webt/wq/q0/sp/wqq0sphqihtnsg1f8eor15ffkgi.png"><br><hr><br><br>  C'√©tait ma premi√®re tentative de regrouper des clients sur la base de donn√©es r√©elles, et cela m'a donn√© une exp√©rience pr√©cieuse.  Il existe de nombreux articles sur Internet sur le regroupement √† l'aide de variables num√©riques, mais trouver des solutions pour les donn√©es cat√©gorielles, qui est un peu plus difficile, n'√©tait pas si simple.  Les m√©thodes de regroupement des donn√©es cat√©gorielles sont toujours en cours de d√©veloppement, et dans un autre article, je vais en essayer un autre. <br><a name="habracut"></a><br>  D'un autre c√¥t√©, de nombreuses personnes pensent que le regroupement de donn√©es cat√©gorielles peut ne pas produire de r√©sultats significatifs - et cela est en partie vrai (voir l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">excellente discussion sur CrossValidated</a> ).  √Ä un moment donn√©, j'ai pens√©: ¬´Qu'est-ce que je fais?  Ils peuvent simplement √™tre divis√©s en cohortes. ¬ª  Cependant, l'analyse de cohorte n'est √©galement pas toujours recommand√©e, en particulier avec un nombre significatif de variables cat√©gorielles avec un grand nombre de niveaux: vous pouvez facilement traiter 5-7 cohortes, mais si vous avez 22 variables et chacune a 5 niveaux (par exemple, une enqu√™te client avec des estimations discr√®tes 1 , 2, 3, 4 et 5), et vous devez comprendre √† quels groupes de clients caract√©ristiques vous avez affaire - vous obtiendrez des cohortes de 22x5.  Personne ne veut s'emb√™ter avec une telle t√¢che.  Et ici, le clustering pourrait aider.  Donc, dans cet article, je parlerai de ce que j'aimerais savoir moi-m√™me d√®s que j'ai commenc√© √† regrouper. <br><br>  Le processus de clustering lui-m√™me comprend trois √©tapes: <br><br><ol><li>  Construire une matrice de dissimilarit√© est sans aucun doute la d√©cision la plus importante dans le clustering.  Toutes les √©tapes suivantes seront bas√©es sur la matrice de dissimilarit√© que vous avez cr√©√©e. </li><li>  Le choix de la m√©thode de clustering. </li><li>  √âvaluation des grappes. </li></ol><br>  Ce billet sera une sorte d'introduction qui d√©crit les principes de base du clustering et sa mise en ≈ìuvre dans l'environnement R. <br><br><h2>  Matrice de dissimilarit√© </h2><br>  La base du clustering sera la matrice de dissimilarit√© qui, en termes math√©matiques, d√©crit la diff√©rence (supprim√©e) entre les points de l'ensemble de donn√©es.  Il vous permet de combiner davantage dans les groupes les points les plus proches les uns des autres, ou de s√©parer les plus √©loign√©s les uns des autres - c'est l'id√©e principale du clustering. <br><br>  √Ä ce stade, les diff√©rences entre les types de donn√©es sont importantes, car la matrice de dissimilarit√© est bas√©e sur les distances entre les points de donn√©es individuels.  Il est facile d'imaginer les distances entre les points de donn√©es num√©riques (un exemple bien connu est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les distances euclidiennes</a> ), mais dans le cas de donn√©es cat√©gorielles (facteurs en R), tout n'est pas si √©vident. <br><br>  Afin de construire une matrice de dissimilarit√© dans ce cas, la distance dite de Gover doit √™tre utilis√©e.  Je ne m'attarderai pas sur la partie math√©matique de ce concept, je fournirai simplement des liens: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l√†</a> .  Pour cela, je pr√©f√®re utiliser <code>daisy()</code> avec la <code>metric = c("gower")</code> du package de <code>cluster</code> . <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#-----   -----# #    ,       ,     ,   ,    library(dplyr) #     set.seed(40) #     #    ;   data.frame()     #    ,   200   1  200 id.s &lt;- c(1:200) %&gt;% factor() budget.s &lt;- sample(c("small", "med", "large"), 200, replace = T) %&gt;% factor(levels=c("small", "med", "large"), ordered = TRUE) origins.s &lt;- sample(c("x", "y", "z"), 200, replace = T, prob = c(0.7, 0.15, 0.15)) area.s &lt;- sample(c("area1", "area2", "area3", "area4"), 200, replace = T, prob = c(0.3, 0.1, 0.5, 0.2)) source.s &lt;- sample(c("facebook", "email", "link", "app"), 200, replace = T, prob = c(0.1,0.2, 0.3, 0.4)) ##   ‚Äî      dow.s &lt;- sample(c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), 200, replace = T, prob = c(0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2)) %&gt;% factor(levels=c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), ordered = TRUE) #  dish.s &lt;- sample(c("delicious", "the one you don't like", "pizza"), 200, replace = T) #   data.frame()      synthetic.customers &lt;- data.frame(id.s, budget.s, origins.s, area.s, source.s, dow.s, dish.s) #-----   -----# library(cluster) #       #   : daisy(), diana(), clusplot() gower.dist &lt;- daisy(synthetic.customers[ ,2:7], metric = c("gower")) # class(gower.dist) ## , </span></span></code> </pre> <br>  La matrice de dissimilarit√© est pr√™te.  Pour 200 observations, il est construit rapidement, mais peut n√©cessiter une tr√®s grande quantit√© de calcul si vous avez affaire √† un grand ensemble de donn√©es. <br><br>  Dans la pratique, il est tr√®s probable que vous deviez d'abord nettoyer l'ensemble de donn√©es, effectuer les transformations n√©cessaires des lignes en facteurs et suivre les valeurs manquantes.  Dans mon cas, l'ensemble de donn√©es contenait √©galement des lignes de valeurs manquantes qui √©taient joliment regroup√©es √† chaque fois, il semblait donc que c'√©tait un tr√©sor - jusqu'√† ce que je regarde les valeurs (h√©las!). <br><br><h2>  Algorithmes de clustering </h2><br>  Vous savez peut-√™tre d√©j√† que le clustering est <i>k-means et hi√©rarchique</i> .  Dans cet article, je me concentre sur la deuxi√®me m√©thode, car elle est plus flexible et permet diff√©rentes approches: vous pouvez choisir un algorithme de clustering <i>agglom√©ratif</i> (de bas en haut) ou <i>divisionnaire</i> (de haut en bas). <br><br><img src="https://habrastorage.org/webt/nl/vp/u4/nlvpu4e8ykoh_nd_el_4i6plh8q.png"><br>  <i>Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Guide de programmation UC Business Analytics R</a></i> <br><br>  Le clustering agglom√©ratif commence par <code>n</code> clusters, o√π <code>n</code> est le nombre d'observations: on suppose que chacun d'eux est un cluster s√©par√©.  Ensuite, l'algorithme essaie de trouver et de regrouper les points de donn√©es les plus similaires entre eux - c'est ainsi que la formation des clusters commence. <br><br>  Le regroupement divisionnaire est effectu√© de la mani√®re oppos√©e - il est initialement suppos√© que tous les n points de donn√©es que nous avons sont un grand cluster, puis les moins similaires sont divis√©s en groupes s√©par√©s. <br><br>  Lorsque vous d√©cidez laquelle de ces m√©thodes choisir, il est toujours judicieux d'essayer toutes les options.Cependant, en g√©n√©ral, le <i>clustering agglom√©r√© est meilleur pour identifier les petits clusters et est utilis√© par la plupart des programmes informatiques, et le clustering divisionnaire est plus appropri√© pour identifier les grands clusters</i> . <br><br>  Personnellement, avant de d√©cider quelle m√©thode utiliser, je pr√©f√®re regarder les dendrogrammes - une repr√©sentation graphique du clustering.  Comme vous le verrez plus tard, certains dendrogrammes sont bien √©quilibr√©s, tandis que d'autres sont tr√®s chaotiques. <br><br>  # L'entr√©e principale pour le code ci-dessous est la dissimilarit√© (matrice de distance) <br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#             #            ‚Äî         ‚Äî    #------------  ------------# divisive.clust &lt;- diana(as.matrix(gower.dist), diss = TRUE, keep.diss = TRUE) plot(divisive.clust, main = "Divisive")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/54/mp/m1/54mpm19v8jkkpmj6usehxlgr5qk.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#------------   ------------# #      #         ‚Äî     ,      #    (complete linkages) aggl.clust.c &lt;- hclust(gower.dist, method = "complete") plot(aggl.clust.c, main = "Agglomerative, complete linkages")</span></span></code> </pre> <br><h2>  √âvaluation de la qualit√© du clustering </h2><br>  A ce stade, il est n√©cessaire de choisir entre diff√©rents algorithmes de clustering et un nombre diff√©rent de clusters.  Vous pouvez utiliser diff√©rentes m√©thodes d'√©valuation, sans oublier de vous laisser guider par le <b>bon sens</b> .  J'ai soulign√© ces mots en gras et en italique, car la signification du choix est <b>tr√®s importante</b> - le nombre de grappes et la m√©thode de division des donn√©es en groupes doivent √™tre pratiques d'un point de vue pratique.  Le nombre de combinaisons de valeurs de variables cat√©gorielles est fini (car elles sont discr√®tes), mais aucune ventilation bas√©e sur elles ne sera significative.  Vous pouvez √©galement ne pas vouloir avoir tr√®s peu de clusters - dans ce cas, ils seront trop g√©n√©ralis√©s.  En fin de compte, tout d√©pend de votre objectif et des t√¢ches de l'analyse. <br><br>  En g√©n√©ral, lors de la cr√©ation de clusters, vous souhaitez obtenir des groupes de points de donn√©es clairement d√©finis, de sorte que la distance entre ces points au sein du cluster ( <i>ou compacit√©</i> ) soit minimale, et la distance entre les groupes ( <i>s√©parabilit√©</i> ) soit le maximum possible.  Ceci est facile √† comprendre intuitivement: la distance entre les points est une mesure de leur dissimilarit√©, obtenue √† partir de la matrice de dissimilarit√©.  Ainsi, l'√©valuation de la qualit√© du clustering est bas√©e sur l'√©valuation de la compacit√© et de la s√©parabilit√©. <br><br>  Ensuite, je vais d√©montrer deux approches et montrer que l'une d'elles peut donner des r√©sultats d√©nu√©s de sens. <br><br><ul><li>  <i>M√©thode du coude</i> : commencez par celle-ci si le facteur le plus important pour votre analyse est la compacit√© des grappes, c'est-√†-dire la similitude au sein des groupes. </li><li>  <i>M√©thode d'√©valuation des silhouettes</i> : Le graphique de <i>silhouette</i> utilis√© comme mesure de la coh√©rence des donn√©es montre √† quel point chacun des points d'un cluster est proche des points des clusters voisins. </li></ul><br>  Dans la pratique, ces deux m√©thodes donnent souvent des r√©sultats diff√©rents, ce qui peut entra√Æner une certaine confusion - la compacit√© maximale et la s√©paration la plus claire seront obtenues avec un nombre diff√©rent de clusters, de sorte que le bon sens et la compr√©hension de ce que vos donn√©es signifient r√©ellement joueront un r√¥le important lors de la prise de d√©cision finale. <br><br>  Il existe √©galement un certain nombre de mesures que vous pouvez analyser.  Je vais les ajouter directement au code. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,        #      ,     ,   ‚Äî   #     ,      ,         ,   ,     library(fpc) cstats.table &lt;- function(dist, tree, k) { clust.assess &lt;- c("cluster.number","n","within.cluster.ss","average.within","average.between", "wb.ratio","dunn2","avg.silwidth") clust.size &lt;- c("cluster.size") stats.names &lt;- c() row.clust &lt;- c() output.stats &lt;- matrix(ncol = k, nrow = length(clust.assess)) cluster.sizes &lt;- matrix(ncol = k, nrow = k) for(i in c(1:k)){ row.clust[i] &lt;- paste("Cluster-", i, " size") } for(i in c(2:k)){ stats.names[i] &lt;- paste("Test", i-1) for(j in seq_along(clust.assess)){ output.stats[j, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.assess])[j] } for(d in 1:k) { cluster.sizes[d, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.size])[d] dim(cluster.sizes[d, i]) &lt;- c(length(cluster.sizes[i]), 1) cluster.sizes[d, i] } } output.stats.df &lt;- data.frame(output.stats) cluster.sizes &lt;- data.frame(cluster.sizes) cluster.sizes[is.na(cluster.sizes)] &lt;- 0 rows.all &lt;- c(clust.assess, row.clust) # rownames(output.stats.df) &lt;- clust.assess output &lt;- rbind(output.stats.df, cluster.sizes)[ ,-1] colnames(output) &lt;- stats.names[2:k] rownames(output) &lt;- rows.all is.num &lt;- sapply(output, is.numeric) output[is.num] &lt;- lapply(output[is.num], round, 2) output } #     :      7 #     ,            stats.df.divisive &lt;- cstats.table(gower.dist, divisive.clust, 7) stats.df.divisive</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/r-/g_/ou/r-g_oukwyorhqnsls_cbg4c8spw.png"><br><br>  Ainsi, l'indicateur average .within, qui repr√©sente la distance moyenne entre les observations au sein des grappes, diminue, de m√™me que within.cluster.ss (la somme des carr√©s des distances entre les observations dans une grappe).  La largeur moyenne de la silhouette (largeur moyenne) ne change pas sans ambigu√Øt√©, cependant, une relation inverse peut encore √™tre remarqu√©e. <br>  Remarquez √† quel point les tailles de cluster sont disproportionn√©es.  Je ne me pr√©cipiterais pas pour travailler avec un nombre incomparable d'observations au sein des clusters.  L'une des raisons est que l'ensemble de donn√©es peut √™tre d√©s√©quilibr√©, et certains groupes d'observations l'emporteront sur tous les autres dans l'analyse - ce n'est pas bon et conduira tr√®s probablement √† des erreurs. <br><br> <code>stats.df.aggl &lt;-cstats.table(gower.dist, aggl.clust.c, 7) #      </code> <br> <br> <code>stats.df.aggl</code> <br> <br><img src="https://habrastorage.org/webt/a_/-u/aa/a_-uaa_nff99nuyobulroyk_hka.png"><br><br>  Remarquez √† quel point le nombre d'observations par groupe est mieux √©quilibr√© par le regroupement hi√©rarchique agglom√©r√© bas√© sur la m√©thode de communication compl√®te. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># ---------    ---------# #   ¬´¬ª       #    ,     7  library(ggplot2) #  #   ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/kw/kz/xy/kwkzxyuuzwhe0yst9kteg9inias.png"><br><br>  Nous avons donc cr√©√© un graphique du ¬´coude¬ª.  Il montre comment la somme des distances au carr√© entre les observations (nous l'utilisons comme mesure de la proximit√© des observations - plus elle est petite, plus les mesures √† l'int√©rieur du cluster sont proches les unes des autres) varie pour un nombre diff√©rent de clusters.  Id√©alement, nous devrions voir un ¬´coude de coude¬ª distinct au point o√π un regroupement suppl√©mentaire ne donne qu'une l√©g√®re diminution de la somme des carr√©s (SS).  Pour le graphique ci-dessous, je m'arr√™terais √† environ 7. Bien que dans ce cas l'un des clusters ne comprendra que deux observations.  Voyons ce qui se passe lors du clustering agglom√©ratif. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#       ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/y0/ck/q-/y0ckq-zxtzg0fbjr9gcq1jgorvq.png"><br><br>  Le ¬´coude¬ª agglom√©r√© est similaire √† la division, mais le graphique semble plus lisse - les courbes ne sont pas si prononc√©es.  Comme pour le clustering divisionnaire, je me concentrerais sur 7 clusters, cependant, lorsque je choisis entre ces deux m√©thodes, j'aime davantage les tailles de cluster obtenues par la m√©thode agglom√©rative - il vaut mieux qu'elles soient comparables. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/u9/nj/nf/u9njnfcjqbxbzlfgpl5sxqailra.png"><br><br>  Lorsque vous utilisez la m√©thode d'estimation de la silhouette, vous devez choisir la quantit√© qui donne le coefficient de silhouette maximal, car vous avez besoin de grappes suffisamment √©loign√©es pour √™tre consid√©r√©es comme distinctes. <br><br>  Le coefficient de silhouette peut aller de ‚Äì1 √† 1, 1 correspondant √† une bonne coh√©rence au sein des grappes et ‚Äì1 pas tr√®s bon. <br>  Dans le cas du graphique ci-dessus, vous choisiriez 9 plut√¥t que 5 grappes. <br><br>  A titre de comparaison: dans le cas ¬´simple¬ª, le graphique de silhouette est similaire √† celui ci-dessous.  Pas tout √† fait comme le n√¥tre, mais presque. <br><br><img src="https://habrastorage.org/webt/18/yw/uj/18ywujz8uh4q5hhnhtxlzrgs1nm.png"><br>  <i>Source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Data Sailors</a></i> <br><br><pre> <code class="sql hljs">ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</code> </pre> <br><img src="https://habrastorage.org/webt/vk/f1/fl/vkf1fln-v-nedwuh6rzbjkxz2pg.png"><br><br>  Le graphique de largeur de silhouette nous dit: plus vous divisez l'ensemble de donn√©es, plus les clusters deviennent clairs.  Cependant, √† la fin, vous atteindrez des points individuels et vous n'en aurez pas besoin.  Cependant, c'est exactement ce que vous verrez si vous commencez √† augmenter le nombre de clusters <i>k</i> .  Par exemple, pour <code>k=30</code> j'ai obtenu le graphique suivant: <br><br><img src="https://habrastorage.org/webt/sz/nq/sy/sznqsykdros9uf8clfabfg8yb94.png"><br><br>  Pour r√©sumer: plus vous divisez l'ensemble de donn√©es, meilleurs sont les clusters, mais nous ne pouvons pas atteindre des points individuels (par exemple, dans le graphique ci-dessus, nous avons s√©lectionn√© 30 clusters et nous n'avons que 200 points de donn√©es). <br><br>  Ainsi, le clustering agglom√©r√© dans notre cas me semble beaucoup plus √©quilibr√©: les tailles de cluster sont plus ou moins comparables (il suffit de regarder un cluster de seulement deux observations lors de la division par la m√©thode divisionnaire!), Et je m'arr√™terais √† 7 clusters obtenus par cette m√©thode.  Voyons √† quoi ils ressemblent et de quoi ils sont faits. <br><br>  L'ensemble de donn√©es se compose de 6 variables qui doivent √™tre visualis√©es en 2D ou 3D, vous devez donc travailler dur!  La nature des donn√©es cat√©gorielles impose √©galement certaines limites, de sorte que les solutions toutes faites peuvent ne pas fonctionner.  Je dois: a) voir comment les observations sont divis√©es en grappes, b) comprendre comment les observations sont class√©es.  Par cons√©quent, j'ai cr√©√© a) un dendrogramme en couleurs, b) une carte thermique du nombre d'observations par variable √† l'int√©rieur de chaque groupe. <br><br><pre> <code class="sql hljs">library("ggplot2") library("reshape2") library("purrr") library("dplyr") <span class="hljs-comment"><span class="hljs-comment">#    library("dendextend") dendro &lt;- as.dendrogram(aggl.clust.c) dendro.col &lt;- dendro %&gt;% set("branches_k_color", k = 7, value = c("darkslategray", "darkslategray4", "darkslategray3", "gold3", "darkcyan", "cyan3", "gold3")) %&gt;% set("branches_lwd", 0.6) %&gt;% set("labels_colors", value = c("darkslategray")) %&gt;% set("labels_cex", 0.5) ggd1 &lt;- as.ggdend(dendro.col) ggplot(ggd1, theme = theme_minimal()) + labs(x = "Num. observations", y = "Height", title = "Dendrogram, k = 7")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/iy/hf/jx/iyhfjxt9q7vztvwbaazmqlzzno0.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#     ( ) ggplot(ggd1, labels = T) + scale_y_reverse(expand = c(0.2, 0)) + coord_polar(theta="x")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/if/4g/yv/if4gyv42vtgecjd9n-b_0bb91rs.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  ‚Äî   #    ‚Äî       #    ,      clust.num &lt;- cutree(aggl.clust.c, k = 7) synthetic.customers.cl &lt;- cbind(synthetic.customers, clust.num) cust.long &lt;- melt(data.frame(lapply(synthetic.customers.cl, as.character), stringsAsFactors=FALSE), id = c("id.s", "clust.num"), factorsAsStrings=T) cust.long.q &lt;- cust.long %&gt;% group_by(clust.num, variable, value) %&gt;% mutate(count = n_distinct(id.s)) %&gt;% distinct(clust.num, variable, value, count) # heatmap.c ,      ‚Äî ,   ,     heatmap.c &lt;- ggplot(cust.long.q, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat","sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = count))+ scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") #            cust.long.p &lt;- cust.long.q %&gt;% group_by(clust.num, variable) %&gt;% mutate(perc = count / sum(count)) %&gt;% arrange(clust.num) heatmap.p &lt;- ggplot(cust.long.p, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat", "sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = perc), alpha = 0.85)+ labs(title = "Distribution of characteristics across clusters", x = "Cluster number", y = NULL) + geom_hline(yintercept = 3.5) + geom_hline(yintercept = 10.5) + geom_hline(yintercept = 13.5) + geom_hline(yintercept = 17.5) + geom_hline(yintercept = 21.5) + scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") heatmap.p</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/c5/gg/y5/c5ggy5vih07qcfi4h26mgcvkfgy.png"><br><br>  La carte thermique montre graphiquement combien d'observations sont faites pour chaque niveau de facteur pour les facteurs initiaux (les variables avec lesquelles nous avons commenc√©).  La couleur bleu fonc√© correspond √† un nombre relativement important d'observations au sein de l'amas.  Cette carte de chaleur montre √©galement que pour le jour de la semaine (soleil, sam ... lun) et la taille du panier (grand, moyen, petit), le nombre de clients dans chaque cellule est presque le m√™me - cela peut signifier que ces cat√©gories ne sont pas d√©terminantes pour l'analyse, et Il n'est peut-√™tre pas n√©cessaire d'en tenir compte. <br><br><h2>  Conclusion </h2><br>  Dans cet article, nous avons calcul√© la matrice de dissimilarit√©, test√© les m√©thodes d'agglom√©ration et de division du clustering hi√©rarchique et nous sommes familiaris√©s avec les m√©thodes du coude et de la silhouette pour √©valuer la qualit√© des clusters. <br><br>  Le clustering hi√©rarchique divisionnaire et agglom√©r√© est un bon d√©but pour √©tudier le sujet, mais ne vous arr√™tez pas l√† si vous voulez vraiment ma√Ætriser l'analyse de cluster.  Il existe de nombreuses autres m√©thodes et techniques.  La principale diff√©rence avec le regroupement des donn√©es num√©riques est le calcul de la matrice de dissimilarit√©.  Lors de l'√©valuation de la qualit√© du regroupement, toutes les m√©thodes standard ne donneront pas des r√©sultats fiables et significatifs - la m√©thode de la silhouette n'est probablement pas appropri√©e. <br><br>  Et enfin, comme un certain temps s'est d√©j√† √©coul√© depuis que j'ai fait cet exemple, je vois maintenant un certain nombre de lacunes dans mon approche et je serai heureux de tout commentaire.  L'un des probl√®mes importants de mon analyse n'√©tait pas li√© au clustering en tant que tel - <i>mon ensemble de donn√©es √©tait d√©s√©quilibr√©</i> √† bien des √©gards, et ce moment restait inexpliqu√©.  Cela a eu un effet notable sur le regroupement: 70% des clients appartenaient √† un niveau du facteur ¬´citoyennet√©¬ª et ce groupe dominait la plupart des grappes obtenues, il √©tait donc difficile de calculer les diff√©rences entre les autres niveaux du facteur.  La prochaine fois, j'essaierai d'√©quilibrer l'ensemble de donn√©es et de comparer les r√©sultats du clustering.  Mais plus √† ce sujet dans un autre post. <br><br>  Enfin, si vous souhaitez cloner mon code, voici le lien vers github: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/khunreus/cluster-categorical</a> <br>  J'esp√®re que cet article vous a plu! <br><br><h3>  <i>Sources qui m'ont aid√©:</i> </h3><br>  Guide de clustering hi√©rarchique (pr√©paration des donn√©es, clustering, visualisation) - ce blog sera int√©ressant pour ceux qui sont int√©ress√©s par l'analyse commerciale dans l'environnement R: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://uc-r.github.io/hc_clustering</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https: // uc-r. github.io/kmeans_clustering</a> <br><br>  Clustering: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/</a> <br><br>    (   k-): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/</a> <br><br>    denextend,        : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#the-set-function</a> <br><br>    ,   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/</a> <br><br>     : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://jcoliver.github.io/learn-r/008-ggplot-dendrograms-and-heatmaps.html</a> <br><br>       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025633/</a> (  GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/khunreus/EnsCat</a> ). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr461741/">https://habr.com/ru/post/fr461741/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr461731/index.html">DPKI: rem√©dier aux inconv√©nients de l'ICP centralis√©e au moyen de la cha√Æne de blocs</a></li>
<li><a href="../fr461733/index.html">Apprendre l'anglais: 9 idiomes √† l'am√©ricaine</a></li>
<li><a href="../fr461735/index.html">Pratique de d√©codage mat√©riel FFmpeg DXVA2</a></li>
<li><a href="../fr461737/index.html">Nous collectons l'environnement pour le TDD moderne sur le code JavaScript + VS</a></li>
<li><a href="../fr461739/index.html">Backend United 4: Okroshka. Incidents</a></li>
<li><a href="../fr461743/index.html">Security Week 31: vuln√©rabilit√© VLC et t√©l√©phone cass√©</a></li>
<li><a href="../fr461745/index.html">DeviceLock DLP: Prix du march√© noir russe pour percer les donn√©es personnelles (plus une r√©ponse √† la r√©ponse de Tinkoff Bank)</a></li>
<li><a href="../fr461747/index.html">Comment nous avons impl√©ment√© ML dans une application avec pr√®s de 50 millions d'utilisateurs. Exp√©rience Sberbank</a></li>
<li><a href="../fr461749/index.html">La beaut√© dans l'≈ìil du spectateur</a></li>
<li><a href="../fr461751/index.html">Contribution du concepteur au d√©veloppement d'applications mobiles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>