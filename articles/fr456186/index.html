<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕵🏼 📚 👩🏾‍🤝‍👨🏿 CS231n: Réseaux de neurones convolutifs pour la reconnaissance de formes 💪🏾 🙈 🤽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bienvenue à l'une des conférences de CS231n: Réseaux de neurones convolutifs pour la reconnaissance visuelle . 



 Table des matières 


- Présentati...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n: Réseaux de neurones convolutifs pour la reconnaissance de formes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p>  Bienvenue à l'une des conférences de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CS231n: Réseaux de neurones convolutifs pour la reconnaissance visuelle</a> . </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1>  Table des matières </h1><br><ul><li>  Présentation de l'architecture </li><li>  Couches dans un réseau neuronal convolutif <br>  - couche convolutionnelle <br>  - Sous-échantillonnage de couche <br>  - Couche de normalisation <br>  - couche entièrement connectée <br>  - Convertissez des couches entièrement connectées en couches convolutives </li><li>  Architecture de réseau neuronal convolutif <br>  - Modèles de calque <br>  - Modèles de taille de couche <br>  - Étude de cas (LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet) <br>  - Aspects informatiques </li><li>  Lectures complémentaires </li></ul><br><h1>  Réseaux de neurones convolutifs (CNN / ConvNets) </h1><br><p>  Les réseaux de neurones convolutifs sont très similaires aux réseaux de neurones habituels que nous avons étudiés dans le dernier chapitre (se référant au dernier chapitre du cours CS231n): ils sont constitués de neurones, qui, à leur tour, contiennent des poids et des déplacements variables.  Chaque neurone reçoit des données d'entrée, calcule le produit scalaire et, éventuellement, utilise une fonction d'activation non linéaire.  Le réseau entier, comme précédemment, est la seule fonction d'évaluation différenciable: de l'ensemble initial de pixels (image) à une extrémité à la distribution de probabilité d'appartenir à une classe particulière à l'autre extrémité.  Ces réseaux ont toujours une fonction de perte (par exemple, SVM / Softmax) sur la dernière couche (entièrement connectée), et tous les conseils et recommandations qui ont été donnés dans le chapitre précédent concernant les réseaux de neurones ordinaires sont également pertinents pour les réseaux de neurones convolutifs. </p><br><p>  Alors qu'est-ce qui a changé?  L'architecture des réseaux de neurones convolutifs implique explicitement l'obtention d'images en entrée, ce qui nous permet de prendre en compte certaines propriétés des données d'entrée dans l'architecture de réseau elle-même.  Ces propriétés vous permettent de mettre en œuvre la fonction de distribution directe plus efficacement et de réduire considérablement le nombre total de paramètres dans le réseau. </p><br><h1>  Présentation de l'architecture </h1><br><p>  Nous rappelons les réseaux neuronaux ordinaires.  Comme nous l'avons vu dans le chapitre précédent, les réseaux de neurones reçoivent des données d'entrée (un seul vecteur) et les transforment en «poussant» à travers une série de <em>couches cachées</em> .  Chaque couche cachée est constituée d'un certain nombre de neurones, chacun étant connecté à tous les neurones de la couche précédente et où les neurones de chaque couche sont complètement indépendants des autres neurones au même niveau.  La dernière couche entièrement connectée est appelée la «couche de sortie» et dans les problèmes de classification est la distribution des notes par classe. </p><br><p>  <em>Les réseaux de neurones conventionnels ne s'adaptent pas bien aux images plus grandes</em> .  Dans l'ensemble de données CIFAR-10, les images sont de taille 32x32x3 (32 pixels de haut, 32 pixels de large, 3 canaux de couleur).  Pour traiter une telle image, un neurone entièrement connecté dans la première couche cachée d'un réseau neuronal normal aura 32x32x3 = 3072 poids.  Cette quantité est toujours acceptable, mais il devient évident qu'une telle structure ne fonctionnera pas avec des images plus grandes.  Par exemple, une image plus grande - 200x200x3, fera que le nombre de poids deviendra 200x200x3 = 120 000. De plus, nous aurons besoin de plus d'un tel neurone, donc le nombre total de poids commencera rapidement à augmenter.  Il devient évident que la connectivité est excessive et qu'un grand nombre de paramètres conduira rapidement le réseau à une reconversion. </p><br><p>  <em>Représentations 3D des neurones</em> .  Les réseaux de neurones convolutifs utilisent le fait que les données d'entrée sont des images, ils forment donc une architecture plus sensible pour ce type de données.  En particulier, contrairement aux réseaux de neurones conventionnels, les couches du réseau de neurones convolutifs organisent les neurones en 3 dimensions - largeur, hauteur, profondeur ( <em>Remarque</em> : le mot «profondeur» fait référence à la 3e dimension des neurones d'activation, et non la profondeur du réseau de neurones lui-même mesurée nombre de couches).  Par exemple, les images d'entrée du jeu de données CIFAR-10 sont des données d'entrée dans une représentation 3D, dont la dimension est 32x32x3 (largeur, hauteur, profondeur).  Comme nous le verrons plus loin, les neurones d'une couche seront associés à un petit nombre de neurones de la couche précédente, au lieu d'être connectés à tous les neurones précédents de la couche.  De plus, la couche de sortie de l'image de l'ensemble de données CIFAR-10 aura une dimension de 1 × 1 × 10, car à l'approche de la fin du réseau neuronal, nous réduirons la taille de l'image à un vecteur d'estimations de classe situé le long de la profondeur (3e dimension). </p><br><p>  Visualisation: </p><br><div class="scrollable-table"><table><thead><tr><th>  Réseau de neurones standard </th><th>  Réseau de neurones convolutifs </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>Côté gauche:</em> réseau neuronal standard à 3 couches. <br>  <em>A droite: le</em> réseau neuronal convolutionnel a ses neurones en 3 dimensions (largeur, hauteur, profondeur), comme indiqué sur l'une des couches.  Chaque couche de réseau neuronal convolutif convertit une représentation 3D de l'entrée en une représentation 3D de la sortie sous forme de neurones d'activation.  Dans cet exemple, le calque d'entrée rouge contient l'image, donc sa taille sera égale à la taille de l'image, et la profondeur sera de 3 (trois canaux - rouge, vert, bleu). </p><br><blockquote>  Le réseau neuronal convolutionnel est constitué de couches.  Chaque couche est une API simple: convertit la représentation 3D d'entrée en représentation 3D de sortie d'une fonction différenciable, qui peut ou non contenir des paramètres. </blockquote><br><h1>  Couches utilisées pour construire des réseaux de neurones convolutifs </h1><br><p>  Comme nous l'avons déjà décrit ci-dessus, un simple réseau neuronal convolutionnel est un ensemble de couches, où chaque couche convertit une représentation en une autre en utilisant une certaine fonction différenciable.  Nous utilisons trois types principaux de couches pour construire des réseaux de neurones convolutionnels: une <em>couche convolutionnelle</em> , <em>une</em> <em>couche de</em> <em>sous-échantillonnage</em> et une <em>couche entièrement connectée</em> (la même que celle que nous utilisons dans un réseau neuronal normal).  Nous organisons ces couches séquentiellement pour obtenir l'architecture SNA. </p><br><p> <em>Exemple d'architecture: vue d'ensemble.</em>  Ci-dessous, nous allons plonger dans les détails, mais pour l'instant, pour l'ensemble de données CIFAR-10, l'architecture de notre réseau neuronal convolutionnel peut être <code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> .  Maintenant plus en détail: </p><br><ul><li>  <code>INPUT</code> [32x32x3] contiendra les valeurs originales des pixels de l'image, dans notre cas, l'image est large de 32 pixels, haute de 32 pixels et 3 canaux de couleur R, G, B. </li><li>  <code>CONV</code> couche <code>CONV</code> produira un ensemble de neurones de sortie qui seront associés à la zone locale de l'image source d'entrée;  chacun de ces neurones calculera le produit scalaire entre ses poids et la petite partie de l'image originale à laquelle il est associé.  La valeur de sortie peut être une représentation 3D de <code>323212</code> , si, par exemple, nous décidons d'utiliser 12 filtres. </li><li>  <code>RELU</code> couche <code>RELU</code> appliquera la fonction d'activation d'élément <code>max(0, x)</code> .  Cette conversion ne changera pas la dimension des données - <code>[32x32x12]</code> . </li><li>  <code>POOL</code> couche <code>POOL</code> effectuera l'opération d'échantillonnage de l'image en deux dimensions - hauteur et largeur, ce qui nous donnera par conséquent une nouvelle représentation 3D <code>[161612]</code> . </li><li>  <code>FC</code> couche <code>FC</code> (couche entièrement connectée) calculera les notes par classes, la dimension résultante sera <code>[1x1x10]</code> , où chacune des 10 valeurs correspondra aux notes d'une classe particulière parmi 10 catégories d'images de CIFAR-10.  Comme dans les réseaux de neurones conventionnels, chaque neurone de cette couche sera associé à tous les neurones de la couche précédente (représentation 3D). </li></ul><br><p>  C'est ainsi que le réseau de neurones convolutionnels transforme l'image originale, couche par couche, de la valeur de pixel initiale à l'estimation de classe finale.  Notez que certains calques contiennent des options et d'autres non.  En particulier, les couches <code>CONV/FC</code> effectuent une transformation, qui est non seulement une fonction qui dépend des données d'entrée, mais dépend également des valeurs internes des poids et des déplacements dans les neurones eux-mêmes.  <code>RELU/POOL</code> couches <code>RELU/POOL</code> , <code>RELU/POOL</code> revanche, utilisent des fonctions non paramétrées.  Les paramètres des couches <code>CONV/FC</code> seront entraînés par descente de gradient afin que l'entrée reçoive les étiquettes de sortie correctes correspondantes. </p><br><p>  Pour résumer: </p><br><ul><li>  L'architecture du réseau neuronal convolutionnel, dans sa représentation la plus simple, est un ensemble ordonné de couches qui transforme la représentation d'une image en une autre représentation, par exemple, des estimations d'appartenance à une classe. </li><li>  Il existe plusieurs types de couches (CONV - couche convolutionnelle, FC - entièrement connecté, RELU - fonction d'activation, POOL - couche de sous-échantillon - la plus populaire). </li><li>  Chaque couche d'entrée reçoit une représentation 3D, la convertit en une représentation 3D de sortie à l'aide d'une fonction différenciable. </li><li>  Chaque couche peut et peut ne pas avoir de paramètres (CONV / FC - avoir des paramètres, RELU / POOL - non). </li><li>  Chaque couche peut et peut ne pas avoir d'hyper paramètres (CONV / FC / POOL - avoir, RELU - non) </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>La représentation initiale contient les valeurs en pixels de l'image (à gauche) et les estimations pour les classes auxquelles appartient l'objet dans l'image (à droite).</em>  <em>Chaque transformation de vue est marquée comme une colonne.</em> </p><br><h1>  Couche convolutionnelle </h1><br><p>  <em>La couche convolutionnelle</em> est la couche principale de la construction de réseaux de neurones convolutionnels. </p><br><p>  <em>Aperçu sans plonger dans les caractéristiques du cerveau.</em>  Essayons d'abord de comprendre ce que la couche CONV calcule toujours sans immerger et toucher le sujet du cerveau et des neurones.  Les paramètres de la couche convolutionnelle consistent en un ensemble de filtres formés.  Chaque filtre est une petite grille sur la largeur et la hauteur, mais s'étendant sur toute la profondeur de la représentation d'entrée. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p>  Par exemple, un filtre standard sur la première couche d'un réseau neuronal convolutionnel peut avoir des dimensions 5x5x3 (5px - largeur et hauteur, 3 - le nombre de canaux de couleur).  Lors d'un passage direct, nous déplaçons (pour être exact - nous réduisons) le filtre le long de la largeur et de la hauteur de la représentation d'entrée et calculons le produit scalaire entre les valeurs du filtre et les valeurs correspondantes de la représentation d'entrée en tout point.  Dans le processus de déplacement du filtre le long de la largeur et de la hauteur de la représentation d'entrée, nous formons une carte d'activation bidimensionnelle qui contient les valeurs d'application de ce filtre à chacune des zones de la représentation d'entrée.  Intuitivement, il devient clair que le réseau apprendra aux filtres à s'activer lorsqu'ils voient un certain signe visuel, par exemple, une ligne droite à un certain angle ou des représentations en forme de roue à des niveaux supérieurs.  Maintenant que nous avons appliqué tous nos filtres à l'image d'origine, par exemple, il y en avait 12. À la suite de l'application de 12 filtres, nous avons reçu 12 cartes d'activation de dimension 2. Pour produire une représentation de sortie, nous combinons ces cartes (séquentiellement dans la 3ème dimension) et obtenons une représentation dimension [LxHx12]. </p><br><p>  <em>Un aperçu auquel nous connectons le cerveau et les neurones.</em>  Si vous êtes un fan du cerveau et des neurones, vous pouvez imaginer que chaque neurone "regarde" une grande partie de la représentation d'entrée et transfère des informations sur cette section aux neurones voisins.  Ci-dessous, nous discuterons des détails de la connectivité des neurones, de leur emplacement dans l'espace et du mécanisme de partage des paramètres. </p><br><p>  <em>Connectivité locale.</em>  Lorsque nous traitons des données d'entrée avec un grand nombre de dimensions, par exemple, comme dans le cas des images, alors, comme nous l'avons déjà vu, il n'est absolument pas nécessaire de connecter les neurones avec tous les neurones de la couche précédente.  Au lieu de cela, nous ne connecterons les neurones qu'aux zones locales de la représentation d'entrée.  Le degré de connectivité spatiale est l'un des hyper-paramètres et est appelé le <em>champ récepteur</em> (le champ récepteur d'un neurone est la taille du même noyau de filtre / convolution).  Le degré de connectivité le long de la 3ème dimension (profondeur) est toujours égal à la profondeur de la représentation originale.  Il est très important de se concentrer à nouveau sur ce point, attention à la façon dont nous définissons les dimensions spatiales (largeur et hauteur) et la profondeur: les connexions neuronales sont locales en largeur et en hauteur, mais s'étendent <em>toujours</em> sur toute la profondeur de la représentation d'entrée. </p><br><p>  <em>Exemple 1.</em> Imaginez que la représentation d'entrée ait une taille de 32x32x3 (RGB, CIFAR-10).  Si la taille du filtre (champ récepteur du neurone) est de 5 × 5, alors chaque neurone de la couche convolutionnelle aura des poids dans la région 5 × 5 × 3 de la représentation originale, ce qui conduira finalement à l'établissement de 5 × 5 × 3 = 75 liaisons (poids) + 1 paramètre de décalage.  Veuillez noter que le degré de connectivité en profondeur doit être égal à 3, car il s'agit de la dimension de la représentation d'origine. </p><br><p>  <em>Exemple 2.</em> Imaginez que la représentation d'entrée ait une taille de 16x16x20.  En utilisant comme exemple le champ récepteur d'un neurone de taille 3x3, chaque neurone de couche convolutionnelle aura 3x3x320 = 180 connexions (poids) + 1 paramètre de déplacement.  Notez que la connectivité est locale en largeur et en hauteur, mais complète en profondeur (20). </p><br><div class="scrollable-table"><table><thead><tr><th>  # 1 </th><th>  # 2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>Du côté gauche: la</em> représentation d'entrée est affichée en rouge (par exemple, une image de taille 32x332 CIFAR-10) et un exemple de la représentation des neurones dans la première couche convolutionnelle.  Chaque neurone de la couche convolutionnelle n'est associé qu'à la zone locale de la représentation d'entrée, mais complètement en profondeur (dans l'exemple, le long de tous les canaux de couleur).  Veuillez noter qu'il y a beaucoup de neurones dans l'image (dans l'exemple - 5) et qu'ils sont situés le long de la 3ème dimension (profondeur) - des explications concernant cet arrangement seront données ci-dessous. <br>  <em>Du côté droit: les</em> neurones du réseau neuronal restent inchangés: ils calculent toujours le produit scalaire entre leurs poids et les données d'entrée, appliquent la fonction d'activation, mais leur connectivité est maintenant limitée par la zone locale spatiale. </p><br><p>  <em>L'emplacement spatial.</em>  Nous avons déjà compris la connectivité de chaque neurone dans la couche convolutionnelle avec la représentation d'entrée, mais nous n'avons pas encore discuté du nombre de ces neurones ni de leur localisation.  Trois hyper paramètres affectent la taille de la vue de sortie: la <em>profondeur</em> , le <em>pas</em> et l' <em>alignement</em> . </p><br><ol><li>  <em>La profondeur de la</em> représentation de sortie est un hyper paramètre: elle correspond au nombre de filtres que nous voulons appliquer, chacun apprenant autre chose dans la représentation originale.  Par exemple, si la première couche convolutionnelle reçoit une image en entrée, différents neurones le long de la 3ème dimension (profondeur) peuvent être activés en présence de différentes orientations de lignes dans une certaine zone ou de grappes d'une certaine couleur.  L'ensemble des neurones qui "regardent" la même zone de la représentation d'entrée, nous l'appellerons la <em>colonne profonde</em> (ou "fibre" - fibre). </li><li>  Nous devons déterminer le <em>pas</em> (taille de décalage en pixels) avec lequel le filtre se déplacera.  Si le pas est 1, alors nous décalons le filtre de 1 pixel en une seule itération.  Si l'étape est 2 (ou, ce qui est encore moins utilisé, 3 ou plus), le décalage se produit pour tous les deux pixels en une seule itération.  Une étape plus grande se traduit par une représentation de sortie plus petite. </li><li>  Comme nous le verrons bientôt, il sera parfois nécessaire de compléter la représentation d'entrée le long des bords par des zéros.  La taille d'alignement (le nombre de colonnes / lignes remplies par zéro) est également un hyper paramètre.  Une caractéristique intéressante de l'utilisation de l'alignement est le fait que l'alignement nous permettra de contrôler la dimension de la représentation de sortie (le plus souvent, nous conserverons les dimensions d'origine de la vue - en préservant la largeur et la hauteur de la représentation d'entrée avec la largeur et la hauteur de la représentation de sortie). </li></ol><br><p>  Nous pouvons calculer la dimension finale de la représentation de sortie en la présentant en fonction de la taille de la représentation d'entrée ( <strong>W</strong> ), de la taille du champ récepteur des neurones de la couche convolutionnelle ( <strong>F</strong> ), du pas ( <strong>S</strong> ) et de la taille de l'alignement ( <strong>P</strong> ) aux frontières.  Vous pouvez constater par vous-même que la formule correcte pour calculer le nombre de neurones dans la représentation de sortie est la suivante <strong>(W - F + 2P) / S + 1</strong> .  Par exemple, pour une représentation d'entrée de taille 7x7 et une taille de filtre de 3x3, étape 1 et alignement 0, nous obtenons une représentation de sortie de taille 5x5.  À l'étape 2, nous obtiendrions une représentation de sortie de 3x3.  Regardons un autre exemple, illustré cette fois graphiquement: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>Illustration d'une disposition spatiale.</em>  <em>Dans cet exemple, une seule dimension spatiale (axe x), un neurone avec un champ récepteur <strong>F = 3</strong> , une taille de représentation d'entrée <strong>W = 5</strong> et un alignement <strong>P = 1</strong> .</em>  <em><strong>A gauche</strong> : le champ récepteur du neurone se déplace d'un pas <strong>S = 1</strong> , ce qui donne en conséquence la taille de la représentation de sortie (5 - 3 + 2) / 1 + 1 = 5. <strong>A droite</strong> : le neurone utilise le champ récepteur de taille <strong>S = 2</strong> , qui en le résultat est la taille de la représentation de sortie (5 - 3 + 2) / 2 + 1 = 3. Notez que la taille de pas <strong>S = 3</strong> ne peut pas être utilisée, car avec cette taille de pas, le champ récepteur ne capturera pas une partie de l'image.</em>  <em>Si nous utilisons notre formule, alors (5 - 3 + 2) = 4 n'est pas un multiple de 3. Les poids des neurones dans cet exemple sont [1, 0, -1] (comme indiqué dans l'image la plus à droite), et le décalage est nul.</em>  <em>Ces poids sont partagés par tous les neurones jaunes.</em> </p><br><p>  <em>Utilisation de l'alignement</em> .  Faites attention à l'exemple sur le côté gauche, qui contient 5 éléments à la sortie et 5 éléments à la sortie.  Cela a fonctionné car la taille du champ récepteur (filtre) était de 3 et nous avons utilisé l'alignement <strong>P = 1</strong> .  S'il n'y avait pas d'alignement, la taille de la représentation de sortie serait égale à 3, car il y avait précisément autant de neurones qui pouvaient s'y adapter.  En général, le réglage de la taille d'alignement <strong>P = (F - 1) / 2</strong> avec un pas égal à <strong>S = 1</strong> vous permet d'obtenir la taille de la représentation de sortie similaire à la représentation d'entrée.  Une approche similaire utilisant l'alignement est souvent appliquée dans la pratique, et nous discuterons des raisons ci-dessous lorsque nous parlerons de l'architecture des réseaux de neurones convolutifs. </p><br><p>  <em>Limites de taille de pas</em> .  Veuillez noter que les hyper-paramètres responsables de l'agencement spatial sont également liés par des limitations.  Par exemple, si la représentation d'entrée a une taille de <strong>W = 10</strong> , <strong>P = 0</strong> et la taille du champ récepteur <strong>F = 3</strong> , alors il devient impossible d'utiliser une taille de pas égale à <strong>S = 2</strong> , car <strong>(W - F + 2P) / S + 1 = (10 - 3 + 0) / 2 + 1 = 4,5</strong> , ce qui donne une valeur entière du nombre de neurones.  Ainsi, une telle configuration d'hyper paramètres est considérée comme invalide et les bibliothèques pour travailler avec des réseaux de neurones convolutifs lèveront une exception, forceront l'alignement ou même couperont la représentation d'entrée.  Comme nous le verrons dans les sections suivantes de ce chapitre, la définition des hyper-paramètres de la couche convolutionnelle est toujours un casse-tête qui peut être réduit en utilisant certaines recommandations et «bonnes règles de tonalité» lors de la conception de l'architecture des réseaux de neurones convolutionnels. </p><br><p>  <em>Exemple concret</em> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Architecture de</a> réseau de neurones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolutifs Krizhevsky et al.</a>  , qui a remporté le concours ImageNet en 2012, a reçu 227x227x3 images.  Sur la première couche convolutive, elle a utilisé un champ récepteur de taille <strong>F = 11</strong> , étape <strong>S = 4</strong> et d'alignement de taille <strong>P = 0</strong> .  Puisque (227 - 11) / 4 + 1 = 55, et que la couche convolutionnelle avait une profondeur de <strong>K = 96</strong> , la dimension de sortie de la présentation était de 55x55x96.  Chacun des neurones 55x55x96 dans cette représentation était associé à une région de taille 11x11x3 dans la représentation d'entrée.  De plus, les 96 neurones de la colonne profonde sont associés à la même région 11x11x3, mais avec des poids différents.  Et maintenant, un peu d'humour - si vous décidez de vous familiariser avec le document d'origine (étude), notez que le document prétend que l'entrée reçoit des images 224x224, ce qui ne peut pas être vrai, car (224-11) / 4 + 1 ne donne en aucun cas une valeur entière.  Ce genre de situation est souvent confondu pour les personnes dans des histoires avec des réseaux de neurones convolutionnels.  Je suppose qu'Alex a utilisé la taille d'alignement <strong>P = 3</strong> , mais a oublié de le mentionner dans le document. </p><br><p>  <em>Options de partage.</em>  Le mécanisme de partage des paramètres dans les couches convolutives est utilisé pour contrôler le nombre de paramètres.  Faites attention à l'exemple ci-dessus, car vous pouvez voir qu'il y a 55x55x96 = 290,400 neurones sur la première couche convolutionnelle et chacun des neurones a 11x11x3 = 363 poids + 1 valeur de décalage.  Au total, si l'on multiplie ces deux valeurs, on obtient 290400x364 = 105 705 600 paramètres <em>uniquement</em> sur la première couche du réseau neuronal convolutif.  De toute évidence, cela est d'une grande importance! </p><br><p>  Il s'avère qu'il est possible de réduire considérablement le nombre de paramètres en faisant une hypothèse: si une propriété calculée en position (x, y) nous importe, alors cette propriété calculée en position (x2, y2) nous importera également.  En d'autres termes, désignant une «couche» bidimensionnelle en profondeur comme une «couche profonde» (par exemple, la vue [55x55x96] contient 96 couches profondes, chacune de 55x55), nous construirons des neurones en profondeur avec les mêmes poids et déplacements.  Avec ce schéma de partage des paramètres, la première couche convolutionnelle dans notre exemple contiendra désormais 96 ensembles de poids uniques (chaque ensemble pour chaque couche de profondeur), au total, il y aura 96x11x11x3 = 34848 poids uniques ou 34944 paramètres (+96 décalages).  De plus, tous les neurones 55x55 de chaque couche profonde utiliseront désormais les mêmes paramètres.  En pratique, pendant la rétropropagation, chaque neurone dans cette représentation calculera le gradient pour ses propres poids, mais ces gradients seront additionnés sur chaque couche de profondeur et ne mettront à jour qu'un seul ensemble de poids à chaque niveau. </p><br><p>  Notez que si tous les neurones de la même couche profonde utilisaient les mêmes poids, alors pour une propagation directe à travers la couche convolutionnelle, la convolution entre les valeurs des poids des neurones et les données d'entrée serait calculée.  C'est pourquoi il est habituel d'appeler un seul ensemble de poids - un <strong>filtre (noyau)</strong> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>Des exemples de filtres obtenus en entraînant le modèle Krizhevsky et al.</em>  <em>Chacun des 96 filtres montrés ici est de taille 11x11x3 et chacun d'eux est partagé par tous les neurones 55x55 d'une couche profonde.</em>  <em>Veuillez noter que l'hypothèse de partager les mêmes poids est logique: si la détection d'une ligne horizontale est importante dans une partie de l'image, il est intuitivement clair qu'une telle détection est importante dans une autre partie de cette image.</em>  <em>Par conséquent, cela n'a aucun sens de se recycler à chaque fois pour trouver des lignes horizontales dans chacun des 55x55 endroits différents de l'image dans la couche convolutionnelle.</em> </p><br><p>  Il ne faut pas oublier que l'hypothèse d'un partage des paramètres n'est pas toujours logique.  Par exemple, si une image avec une structure centrée est alimentée à l'entrée d'un réseau neuronal convolutif, où nous aimerions pouvoir apprendre une propriété dans une partie de l'image et une autre propriété dans l'autre partie de l'image.  Un exemple pratique est les images à face centrée.  On peut supposer que différents signes oculaires ou capillaires peuvent être identifiés dans différentes zones de l'image.Par conséquent, dans ce cas, la relaxation des poids est utilisée et la couche est appelée <strong>connectée localement</strong> . </p><br><p>  <strong>Quelques exemples</strong> .  Les discussions précédentes devraient être transférées au plan des spécificités et sur des exemples avec du code.  Imaginez que la représentation d'entrée est un tableau <code>numpy</code> de <code>X</code>  Ensuite: </p><br><ul><li>  <em>La colonne profonde</em> ( <em>thread</em> ) à la position <code>(x,y)</code> sera représentée comme suit <code>X[x,y,:]</code> . </li><li>  <em>La couche profonde</em> , ou comme nous l'appelions précédemment une telle couche - <em>la carte d'activation</em> à la profondeur <code>d</code> sera représentée comme suit <code>X[:,:,d]</code> . </li></ul><br><p>  <em>Un exemple de couche convolutionnelle</em> . ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     — (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         —    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 — F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 — F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( —  ,  — ,  —  ),        —   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 — 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 — 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               — <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     —              ,    . ,    ,      —        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B —  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   —           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 — F)/S + 1</strong> </li><li> <strong>H2 = (H1 — F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    — <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     —    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    —            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 — 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   —    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 — 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     —     55.        ""  33     ,    —   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           —          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       — "  ":  ,       ,        ,         ImageNet —   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    — Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet —        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU — 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       —  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> …   call-to-action — ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Télégramme</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456186/">https://habr.com/ru/post/fr456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456174/index.html">Comment organiser un hackathon en tant qu'étudiant 101. Deuxième partie</a></li>
<li><a href="../fr456178/index.html">Thèmes et styles Android sans magie. Et comment les cuisiner avec SwitchCompat</a></li>
<li><a href="../fr456180/index.html">Comment ai-je trouvé ma première vulnérabilité?</a></li>
<li><a href="../fr456182/index.html">Audio via Bluetooth: informations les plus détaillées sur les profils, les codecs et les appareils</a></li>
<li><a href="../fr456184/index.html">Radio définie par logiciel - comment ça marche? Partie 8</a></li>
<li><a href="../fr456188/index.html">Jeton, jeton d'actualisation et création d'un wrapper asynchrone pour une demande REST</a></li>
<li><a href="../fr456192/index.html">Des monolithes aux microservices: l'expérience de M.Video-Eldorado et MegaFon</a></li>
<li><a href="../fr456194/index.html">Aide-mémoire sur les structures de données Go</a></li>
<li><a href="../fr456196/index.html">Idées fausses sur SCRUM</a></li>
<li><a href="../fr456200/index.html">Histoire d'Internet: ARPANET - L'Origine</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>