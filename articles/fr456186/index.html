<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïµüèº üìö üë©üèæ‚Äçü§ù‚Äçüë®üèø CS231n: R√©seaux de neurones convolutifs pour la reconnaissance de formes üí™üèæ üôà ü§Ω</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bienvenue √† l'une des conf√©rences de CS231n: R√©seaux de neurones convolutifs pour la reconnaissance visuelle . 



 Table des mati√®res 


- Pr√©sentati...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n: R√©seaux de neurones convolutifs pour la reconnaissance de formes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p>  Bienvenue √† l'une des conf√©rences de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CS231n: R√©seaux de neurones convolutifs pour la reconnaissance visuelle</a> . </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1>  Table des mati√®res </h1><br><ul><li>  Pr√©sentation de l'architecture </li><li>  Couches dans un r√©seau neuronal convolutif <br>  - couche convolutionnelle <br>  - Sous-√©chantillonnage de couche <br>  - Couche de normalisation <br>  - couche enti√®rement connect√©e <br>  - Convertissez des couches enti√®rement connect√©es en couches convolutives </li><li>  Architecture de r√©seau neuronal convolutif <br>  - Mod√®les de calque <br>  - Mod√®les de taille de couche <br>  - √âtude de cas (LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet) <br>  - Aspects informatiques </li><li>  Lectures compl√©mentaires </li></ul><br><h1>  R√©seaux de neurones convolutifs (CNN / ConvNets) </h1><br><p>  Les r√©seaux de neurones convolutifs sont tr√®s similaires aux r√©seaux de neurones habituels que nous avons √©tudi√©s dans le dernier chapitre (se r√©f√©rant au dernier chapitre du cours CS231n): ils sont constitu√©s de neurones, qui, √† leur tour, contiennent des poids et des d√©placements variables.  Chaque neurone re√ßoit des donn√©es d'entr√©e, calcule le produit scalaire et, √©ventuellement, utilise une fonction d'activation non lin√©aire.  Le r√©seau entier, comme pr√©c√©demment, est la seule fonction d'√©valuation diff√©renciable: de l'ensemble initial de pixels (image) √† une extr√©mit√© √† la distribution de probabilit√© d'appartenir √† une classe particuli√®re √† l'autre extr√©mit√©.  Ces r√©seaux ont toujours une fonction de perte (par exemple, SVM / Softmax) sur la derni√®re couche (enti√®rement connect√©e), et tous les conseils et recommandations qui ont √©t√© donn√©s dans le chapitre pr√©c√©dent concernant les r√©seaux de neurones ordinaires sont √©galement pertinents pour les r√©seaux de neurones convolutifs. </p><br><p>  Alors qu'est-ce qui a chang√©?  L'architecture des r√©seaux de neurones convolutifs implique explicitement l'obtention d'images en entr√©e, ce qui nous permet de prendre en compte certaines propri√©t√©s des donn√©es d'entr√©e dans l'architecture de r√©seau elle-m√™me.  Ces propri√©t√©s vous permettent de mettre en ≈ìuvre la fonction de distribution directe plus efficacement et de r√©duire consid√©rablement le nombre total de param√®tres dans le r√©seau. </p><br><h1>  Pr√©sentation de l'architecture </h1><br><p>  Nous rappelons les r√©seaux neuronaux ordinaires.  Comme nous l'avons vu dans le chapitre pr√©c√©dent, les r√©seaux de neurones re√ßoivent des donn√©es d'entr√©e (un seul vecteur) et les transforment en ¬´poussant¬ª √† travers une s√©rie de <em>couches cach√©es</em> .  Chaque couche cach√©e est constitu√©e d'un certain nombre de neurones, chacun √©tant connect√© √† tous les neurones de la couche pr√©c√©dente et o√π les neurones de chaque couche sont compl√®tement ind√©pendants des autres neurones au m√™me niveau.  La derni√®re couche enti√®rement connect√©e est appel√©e la ¬´couche de sortie¬ª et dans les probl√®mes de classification est la distribution des notes par classe. </p><br><p>  <em>Les r√©seaux de neurones conventionnels ne s'adaptent pas bien aux images plus grandes</em> .  Dans l'ensemble de donn√©es CIFAR-10, les images sont de taille 32x32x3 (32 pixels de haut, 32 pixels de large, 3 canaux de couleur).  Pour traiter une telle image, un neurone enti√®rement connect√© dans la premi√®re couche cach√©e d'un r√©seau neuronal normal aura 32x32x3 = 3072 poids.  Cette quantit√© est toujours acceptable, mais il devient √©vident qu'une telle structure ne fonctionnera pas avec des images plus grandes.  Par exemple, une image plus grande - 200x200x3, fera que le nombre de poids deviendra 200x200x3 = 120 000. De plus, nous aurons besoin de plus d'un tel neurone, donc le nombre total de poids commencera rapidement √† augmenter.  Il devient √©vident que la connectivit√© est excessive et qu'un grand nombre de param√®tres conduira rapidement le r√©seau √† une reconversion. </p><br><p>  <em>Repr√©sentations 3D des neurones</em> .  Les r√©seaux de neurones convolutifs utilisent le fait que les donn√©es d'entr√©e sont des images, ils forment donc une architecture plus sensible pour ce type de donn√©es.  En particulier, contrairement aux r√©seaux de neurones conventionnels, les couches du r√©seau de neurones convolutifs organisent les neurones en 3 dimensions - largeur, hauteur, profondeur ( <em>Remarque</em> : le mot ¬´profondeur¬ª fait r√©f√©rence √† la 3e dimension des neurones d'activation, et non la profondeur du r√©seau de neurones lui-m√™me mesur√©e nombre de couches).  Par exemple, les images d'entr√©e du jeu de donn√©es CIFAR-10 sont des donn√©es d'entr√©e dans une repr√©sentation 3D, dont la dimension est 32x32x3 (largeur, hauteur, profondeur).  Comme nous le verrons plus loin, les neurones d'une couche seront associ√©s √† un petit nombre de neurones de la couche pr√©c√©dente, au lieu d'√™tre connect√©s √† tous les neurones pr√©c√©dents de la couche.  De plus, la couche de sortie de l'image de l'ensemble de donn√©es CIFAR-10 aura une dimension de 1 √ó 1 √ó 10, car √† l'approche de la fin du r√©seau neuronal, nous r√©duirons la taille de l'image √† un vecteur d'estimations de classe situ√© le long de la profondeur (3e dimension). </p><br><p>  Visualisation: </p><br><div class="scrollable-table"><table><thead><tr><th>  R√©seau de neurones standard </th><th>  R√©seau de neurones convolutifs </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>C√¥t√© gauche:</em> r√©seau neuronal standard √† 3 couches. <br>  <em>A droite: le</em> r√©seau neuronal convolutionnel a ses neurones en 3 dimensions (largeur, hauteur, profondeur), comme indiqu√© sur l'une des couches.  Chaque couche de r√©seau neuronal convolutif convertit une repr√©sentation 3D de l'entr√©e en une repr√©sentation 3D de la sortie sous forme de neurones d'activation.  Dans cet exemple, le calque d'entr√©e rouge contient l'image, donc sa taille sera √©gale √† la taille de l'image, et la profondeur sera de 3 (trois canaux - rouge, vert, bleu). </p><br><blockquote>  Le r√©seau neuronal convolutionnel est constitu√© de couches.  Chaque couche est une API simple: convertit la repr√©sentation 3D d'entr√©e en repr√©sentation 3D de sortie d'une fonction diff√©renciable, qui peut ou non contenir des param√®tres. </blockquote><br><h1>  Couches utilis√©es pour construire des r√©seaux de neurones convolutifs </h1><br><p>  Comme nous l'avons d√©j√† d√©crit ci-dessus, un simple r√©seau neuronal convolutionnel est un ensemble de couches, o√π chaque couche convertit une repr√©sentation en une autre en utilisant une certaine fonction diff√©renciable.  Nous utilisons trois types principaux de couches pour construire des r√©seaux de neurones convolutionnels: une <em>couche convolutionnelle</em> , <em>une</em> <em>couche de</em> <em>sous-√©chantillonnage</em> et une <em>couche enti√®rement connect√©e</em> (la m√™me que celle que nous utilisons dans un r√©seau neuronal normal).  Nous organisons ces couches s√©quentiellement pour obtenir l'architecture SNA. </p><br><p> <em>Exemple d'architecture: vue d'ensemble.</em>  Ci-dessous, nous allons plonger dans les d√©tails, mais pour l'instant, pour l'ensemble de donn√©es CIFAR-10, l'architecture de notre r√©seau neuronal convolutionnel peut √™tre <code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> .  Maintenant plus en d√©tail: </p><br><ul><li>  <code>INPUT</code> [32x32x3] contiendra les valeurs originales des pixels de l'image, dans notre cas, l'image est large de 32 pixels, haute de 32 pixels et 3 canaux de couleur R, G, B. </li><li>  <code>CONV</code> couche <code>CONV</code> produira un ensemble de neurones de sortie qui seront associ√©s √† la zone locale de l'image source d'entr√©e;  chacun de ces neurones calculera le produit scalaire entre ses poids et la petite partie de l'image originale √† laquelle il est associ√©.  La valeur de sortie peut √™tre une repr√©sentation 3D de <code>323212</code> , si, par exemple, nous d√©cidons d'utiliser 12 filtres. </li><li>  <code>RELU</code> couche <code>RELU</code> appliquera la fonction d'activation d'√©l√©ment <code>max(0, x)</code> .  Cette conversion ne changera pas la dimension des donn√©es - <code>[32x32x12]</code> . </li><li>  <code>POOL</code> couche <code>POOL</code> effectuera l'op√©ration d'√©chantillonnage de l'image en deux dimensions - hauteur et largeur, ce qui nous donnera par cons√©quent une nouvelle repr√©sentation 3D <code>[161612]</code> . </li><li>  <code>FC</code> couche <code>FC</code> (couche enti√®rement connect√©e) calculera les notes par classes, la dimension r√©sultante sera <code>[1x1x10]</code> , o√π chacune des 10 valeurs correspondra aux notes d'une classe particuli√®re parmi 10 cat√©gories d'images de CIFAR-10.  Comme dans les r√©seaux de neurones conventionnels, chaque neurone de cette couche sera associ√© √† tous les neurones de la couche pr√©c√©dente (repr√©sentation 3D). </li></ul><br><p>  C'est ainsi que le r√©seau de neurones convolutionnels transforme l'image originale, couche par couche, de la valeur de pixel initiale √† l'estimation de classe finale.  Notez que certains calques contiennent des options et d'autres non.  En particulier, les couches <code>CONV/FC</code> effectuent une transformation, qui est non seulement une fonction qui d√©pend des donn√©es d'entr√©e, mais d√©pend √©galement des valeurs internes des poids et des d√©placements dans les neurones eux-m√™mes.  <code>RELU/POOL</code> couches <code>RELU/POOL</code> , <code>RELU/POOL</code> revanche, utilisent des fonctions non param√©tr√©es.  Les param√®tres des couches <code>CONV/FC</code> seront entra√Æn√©s par descente de gradient afin que l'entr√©e re√ßoive les √©tiquettes de sortie correctes correspondantes. </p><br><p>  Pour r√©sumer: </p><br><ul><li>  L'architecture du r√©seau neuronal convolutionnel, dans sa repr√©sentation la plus simple, est un ensemble ordonn√© de couches qui transforme la repr√©sentation d'une image en une autre repr√©sentation, par exemple, des estimations d'appartenance √† une classe. </li><li>  Il existe plusieurs types de couches (CONV - couche convolutionnelle, FC - enti√®rement connect√©, RELU - fonction d'activation, POOL - couche de sous-√©chantillon - la plus populaire). </li><li>  Chaque couche d'entr√©e re√ßoit une repr√©sentation 3D, la convertit en une repr√©sentation 3D de sortie √† l'aide d'une fonction diff√©renciable. </li><li>  Chaque couche peut et peut ne pas avoir de param√®tres (CONV / FC - avoir des param√®tres, RELU / POOL - non). </li><li>  Chaque couche peut et peut ne pas avoir d'hyper param√®tres (CONV / FC / POOL - avoir, RELU - non) </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>La repr√©sentation initiale contient les valeurs en pixels de l'image (√† gauche) et les estimations pour les classes auxquelles appartient l'objet dans l'image (√† droite).</em>  <em>Chaque transformation de vue est marqu√©e comme une colonne.</em> </p><br><h1>  Couche convolutionnelle </h1><br><p>  <em>La couche convolutionnelle</em> est la couche principale de la construction de r√©seaux de neurones convolutionnels. </p><br><p>  <em>Aper√ßu sans plonger dans les caract√©ristiques du cerveau.</em>  Essayons d'abord de comprendre ce que la couche CONV calcule toujours sans immerger et toucher le sujet du cerveau et des neurones.  Les param√®tres de la couche convolutionnelle consistent en un ensemble de filtres form√©s.  Chaque filtre est une petite grille sur la largeur et la hauteur, mais s'√©tendant sur toute la profondeur de la repr√©sentation d'entr√©e. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p>  Par exemple, un filtre standard sur la premi√®re couche d'un r√©seau neuronal convolutionnel peut avoir des dimensions 5x5x3 (5px - largeur et hauteur, 3 - le nombre de canaux de couleur).  Lors d'un passage direct, nous d√©pla√ßons (pour √™tre exact - nous r√©duisons) le filtre le long de la largeur et de la hauteur de la repr√©sentation d'entr√©e et calculons le produit scalaire entre les valeurs du filtre et les valeurs correspondantes de la repr√©sentation d'entr√©e en tout point.  Dans le processus de d√©placement du filtre le long de la largeur et de la hauteur de la repr√©sentation d'entr√©e, nous formons une carte d'activation bidimensionnelle qui contient les valeurs d'application de ce filtre √† chacune des zones de la repr√©sentation d'entr√©e.  Intuitivement, il devient clair que le r√©seau apprendra aux filtres √† s'activer lorsqu'ils voient un certain signe visuel, par exemple, une ligne droite √† un certain angle ou des repr√©sentations en forme de roue √† des niveaux sup√©rieurs.  Maintenant que nous avons appliqu√© tous nos filtres √† l'image d'origine, par exemple, il y en avait 12. √Ä la suite de l'application de 12 filtres, nous avons re√ßu 12 cartes d'activation de dimension 2. Pour produire une repr√©sentation de sortie, nous combinons ces cartes (s√©quentiellement dans la 3√®me dimension) et obtenons une repr√©sentation dimension [LxHx12]. </p><br><p>  <em>Un aper√ßu auquel nous connectons le cerveau et les neurones.</em>  Si vous √™tes un fan du cerveau et des neurones, vous pouvez imaginer que chaque neurone "regarde" une grande partie de la repr√©sentation d'entr√©e et transf√®re des informations sur cette section aux neurones voisins.  Ci-dessous, nous discuterons des d√©tails de la connectivit√© des neurones, de leur emplacement dans l'espace et du m√©canisme de partage des param√®tres. </p><br><p>  <em>Connectivit√© locale.</em>  Lorsque nous traitons des donn√©es d'entr√©e avec un grand nombre de dimensions, par exemple, comme dans le cas des images, alors, comme nous l'avons d√©j√† vu, il n'est absolument pas n√©cessaire de connecter les neurones avec tous les neurones de la couche pr√©c√©dente.  Au lieu de cela, nous ne connecterons les neurones qu'aux zones locales de la repr√©sentation d'entr√©e.  Le degr√© de connectivit√© spatiale est l'un des hyper-param√®tres et est appel√© le <em>champ r√©cepteur</em> (le champ r√©cepteur d'un neurone est la taille du m√™me noyau de filtre / convolution).  Le degr√© de connectivit√© le long de la 3√®me dimension (profondeur) est toujours √©gal √† la profondeur de la repr√©sentation originale.  Il est tr√®s important de se concentrer √† nouveau sur ce point, attention √† la fa√ßon dont nous d√©finissons les dimensions spatiales (largeur et hauteur) et la profondeur: les connexions neuronales sont locales en largeur et en hauteur, mais s'√©tendent <em>toujours</em> sur toute la profondeur de la repr√©sentation d'entr√©e. </p><br><p>  <em>Exemple 1.</em> Imaginez que la repr√©sentation d'entr√©e ait une taille de 32x32x3 (RGB, CIFAR-10).  Si la taille du filtre (champ r√©cepteur du neurone) est de 5 √ó 5, alors chaque neurone de la couche convolutionnelle aura des poids dans la r√©gion 5 √ó 5 √ó 3 de la repr√©sentation originale, ce qui conduira finalement √† l'√©tablissement de 5 √ó 5 √ó 3 = 75 liaisons (poids) + 1 param√®tre de d√©calage.  Veuillez noter que le degr√© de connectivit√© en profondeur doit √™tre √©gal √† 3, car il s'agit de la dimension de la repr√©sentation d'origine. </p><br><p>  <em>Exemple 2.</em> Imaginez que la repr√©sentation d'entr√©e ait une taille de 16x16x20.  En utilisant comme exemple le champ r√©cepteur d'un neurone de taille 3x3, chaque neurone de couche convolutionnelle aura 3x3x320 = 180 connexions (poids) + 1 param√®tre de d√©placement.  Notez que la connectivit√© est locale en largeur et en hauteur, mais compl√®te en profondeur (20). </p><br><div class="scrollable-table"><table><thead><tr><th>  # 1 </th><th>  # 2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>Du c√¥t√© gauche: la</em> repr√©sentation d'entr√©e est affich√©e en rouge (par exemple, une image de taille 32x332 CIFAR-10) et un exemple de la repr√©sentation des neurones dans la premi√®re couche convolutionnelle.  Chaque neurone de la couche convolutionnelle n'est associ√© qu'√† la zone locale de la repr√©sentation d'entr√©e, mais compl√®tement en profondeur (dans l'exemple, le long de tous les canaux de couleur).  Veuillez noter qu'il y a beaucoup de neurones dans l'image (dans l'exemple - 5) et qu'ils sont situ√©s le long de la 3√®me dimension (profondeur) - des explications concernant cet arrangement seront donn√©es ci-dessous. <br>  <em>Du c√¥t√© droit: les</em> neurones du r√©seau neuronal restent inchang√©s: ils calculent toujours le produit scalaire entre leurs poids et les donn√©es d'entr√©e, appliquent la fonction d'activation, mais leur connectivit√© est maintenant limit√©e par la zone locale spatiale. </p><br><p>  <em>L'emplacement spatial.</em>  Nous avons d√©j√† compris la connectivit√© de chaque neurone dans la couche convolutionnelle avec la repr√©sentation d'entr√©e, mais nous n'avons pas encore discut√© du nombre de ces neurones ni de leur localisation.  Trois hyper param√®tres affectent la taille de la vue de sortie: la <em>profondeur</em> , le <em>pas</em> et l' <em>alignement</em> . </p><br><ol><li>  <em>La profondeur de la</em> repr√©sentation de sortie est un hyper param√®tre: elle correspond au nombre de filtres que nous voulons appliquer, chacun apprenant autre chose dans la repr√©sentation originale.  Par exemple, si la premi√®re couche convolutionnelle re√ßoit une image en entr√©e, diff√©rents neurones le long de la 3√®me dimension (profondeur) peuvent √™tre activ√©s en pr√©sence de diff√©rentes orientations de lignes dans une certaine zone ou de grappes d'une certaine couleur.  L'ensemble des neurones qui "regardent" la m√™me zone de la repr√©sentation d'entr√©e, nous l'appellerons la <em>colonne profonde</em> (ou "fibre" - fibre). </li><li>  Nous devons d√©terminer le <em>pas</em> (taille de d√©calage en pixels) avec lequel le filtre se d√©placera.  Si le pas est 1, alors nous d√©calons le filtre de 1 pixel en une seule it√©ration.  Si l'√©tape est 2 (ou, ce qui est encore moins utilis√©, 3 ou plus), le d√©calage se produit pour tous les deux pixels en une seule it√©ration.  Une √©tape plus grande se traduit par une repr√©sentation de sortie plus petite. </li><li>  Comme nous le verrons bient√¥t, il sera parfois n√©cessaire de compl√©ter la repr√©sentation d'entr√©e le long des bords par des z√©ros.  La taille d'alignement (le nombre de colonnes / lignes remplies par z√©ro) est √©galement un hyper param√®tre.  Une caract√©ristique int√©ressante de l'utilisation de l'alignement est le fait que l'alignement nous permettra de contr√¥ler la dimension de la repr√©sentation de sortie (le plus souvent, nous conserverons les dimensions d'origine de la vue - en pr√©servant la largeur et la hauteur de la repr√©sentation d'entr√©e avec la largeur et la hauteur de la repr√©sentation de sortie). </li></ol><br><p>  Nous pouvons calculer la dimension finale de la repr√©sentation de sortie en la pr√©sentant en fonction de la taille de la repr√©sentation d'entr√©e ( <strong>W</strong> ), de la taille du champ r√©cepteur des neurones de la couche convolutionnelle ( <strong>F</strong> ), du pas ( <strong>S</strong> ) et de la taille de l'alignement ( <strong>P</strong> ) aux fronti√®res.  Vous pouvez constater par vous-m√™me que la formule correcte pour calculer le nombre de neurones dans la repr√©sentation de sortie est la suivante <strong>(W - F + 2P) / S + 1</strong> .  Par exemple, pour une repr√©sentation d'entr√©e de taille 7x7 et une taille de filtre de 3x3, √©tape 1 et alignement 0, nous obtenons une repr√©sentation de sortie de taille 5x5.  √Ä l'√©tape 2, nous obtiendrions une repr√©sentation de sortie de 3x3.  Regardons un autre exemple, illustr√© cette fois graphiquement: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>Illustration d'une disposition spatiale.</em>  <em>Dans cet exemple, une seule dimension spatiale (axe x), un neurone avec un champ r√©cepteur <strong>F = 3</strong> , une taille de repr√©sentation d'entr√©e <strong>W = 5</strong> et un alignement <strong>P = 1</strong> .</em>  <em><strong>A gauche</strong> : le champ r√©cepteur du neurone se d√©place d'un pas <strong>S = 1</strong> , ce qui donne en cons√©quence la taille de la repr√©sentation de sortie (5 - 3 + 2) / 1 + 1 = 5. <strong>A droite</strong> : le neurone utilise le champ r√©cepteur de taille <strong>S = 2</strong> , qui en le r√©sultat est la taille de la repr√©sentation de sortie (5 - 3 + 2) / 2 + 1 = 3. Notez que la taille de pas <strong>S = 3</strong> ne peut pas √™tre utilis√©e, car avec cette taille de pas, le champ r√©cepteur ne capturera pas une partie de l'image.</em>  <em>Si nous utilisons notre formule, alors (5 - 3 + 2) = 4 n'est pas un multiple de 3. Les poids des neurones dans cet exemple sont [1, 0, -1] (comme indiqu√© dans l'image la plus √† droite), et le d√©calage est nul.</em>  <em>Ces poids sont partag√©s par tous les neurones jaunes.</em> </p><br><p>  <em>Utilisation de l'alignement</em> .  Faites attention √† l'exemple sur le c√¥t√© gauche, qui contient 5 √©l√©ments √† la sortie et 5 √©l√©ments √† la sortie.  Cela a fonctionn√© car la taille du champ r√©cepteur (filtre) √©tait de 3 et nous avons utilis√© l'alignement <strong>P = 1</strong> .  S'il n'y avait pas d'alignement, la taille de la repr√©sentation de sortie serait √©gale √† 3, car il y avait pr√©cis√©ment autant de neurones qui pouvaient s'y adapter.  En g√©n√©ral, le r√©glage de la taille d'alignement <strong>P = (F - 1) / 2</strong> avec un pas √©gal √† <strong>S = 1</strong> vous permet d'obtenir la taille de la repr√©sentation de sortie similaire √† la repr√©sentation d'entr√©e.  Une approche similaire utilisant l'alignement est souvent appliqu√©e dans la pratique, et nous discuterons des raisons ci-dessous lorsque nous parlerons de l'architecture des r√©seaux de neurones convolutifs. </p><br><p>  <em>Limites de taille de pas</em> .  Veuillez noter que les hyper-param√®tres responsables de l'agencement spatial sont √©galement li√©s par des limitations.  Par exemple, si la repr√©sentation d'entr√©e a une taille de <strong>W = 10</strong> , <strong>P = 0</strong> et la taille du champ r√©cepteur <strong>F = 3</strong> , alors il devient impossible d'utiliser une taille de pas √©gale √† <strong>S = 2</strong> , car <strong>(W - F + 2P) / S + 1 = (10 - 3 + 0) / 2 + 1 = 4,5</strong> , ce qui donne une valeur enti√®re du nombre de neurones.  Ainsi, une telle configuration d'hyper param√®tres est consid√©r√©e comme invalide et les biblioth√®ques pour travailler avec des r√©seaux de neurones convolutifs l√®veront une exception, forceront l'alignement ou m√™me couperont la repr√©sentation d'entr√©e.  Comme nous le verrons dans les sections suivantes de ce chapitre, la d√©finition des hyper-param√®tres de la couche convolutionnelle est toujours un casse-t√™te qui peut √™tre r√©duit en utilisant certaines recommandations et ¬´bonnes r√®gles de tonalit√©¬ª lors de la conception de l'architecture des r√©seaux de neurones convolutionnels. </p><br><p>  <em>Exemple concret</em> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Architecture de</a> r√©seau de neurones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolutifs Krizhevsky et al.</a>  , qui a remport√© le concours ImageNet en 2012, a re√ßu 227x227x3 images.  Sur la premi√®re couche convolutive, elle a utilis√© un champ r√©cepteur de taille <strong>F = 11</strong> , √©tape <strong>S = 4</strong> et d'alignement de taille <strong>P = 0</strong> .  Puisque (227 - 11) / 4 + 1 = 55, et que la couche convolutionnelle avait une profondeur de <strong>K = 96</strong> , la dimension de sortie de la pr√©sentation √©tait de 55x55x96.  Chacun des neurones 55x55x96 dans cette repr√©sentation √©tait associ√© √† une r√©gion de taille 11x11x3 dans la repr√©sentation d'entr√©e.  De plus, les 96 neurones de la colonne profonde sont associ√©s √† la m√™me r√©gion 11x11x3, mais avec des poids diff√©rents.  Et maintenant, un peu d'humour - si vous d√©cidez de vous familiariser avec le document d'origine (√©tude), notez que le document pr√©tend que l'entr√©e re√ßoit des images 224x224, ce qui ne peut pas √™tre vrai, car (224-11) / 4 + 1 ne donne en aucun cas une valeur enti√®re.  Ce genre de situation est souvent confondu pour les personnes dans des histoires avec des r√©seaux de neurones convolutionnels.  Je suppose qu'Alex a utilis√© la taille d'alignement <strong>P = 3</strong> , mais a oubli√© de le mentionner dans le document. </p><br><p>  <em>Options de partage.</em>  Le m√©canisme de partage des param√®tres dans les couches convolutives est utilis√© pour contr√¥ler le nombre de param√®tres.  Faites attention √† l'exemple ci-dessus, car vous pouvez voir qu'il y a 55x55x96 = 290,400 neurones sur la premi√®re couche convolutionnelle et chacun des neurones a 11x11x3 = 363 poids + 1 valeur de d√©calage.  Au total, si l'on multiplie ces deux valeurs, on obtient 290400x364 = 105 705 600 param√®tres <em>uniquement</em> sur la premi√®re couche du r√©seau neuronal convolutif.  De toute √©vidence, cela est d'une grande importance! </p><br><p>  Il s'av√®re qu'il est possible de r√©duire consid√©rablement le nombre de param√®tres en faisant une hypoth√®se: si une propri√©t√© calcul√©e en position (x, y) nous importe, alors cette propri√©t√© calcul√©e en position (x2, y2) nous importera √©galement.  En d'autres termes, d√©signant une ¬´couche¬ª bidimensionnelle en profondeur comme une ¬´couche profonde¬ª (par exemple, la vue [55x55x96] contient 96 couches profondes, chacune de 55x55), nous construirons des neurones en profondeur avec les m√™mes poids et d√©placements.  Avec ce sch√©ma de partage des param√®tres, la premi√®re couche convolutionnelle dans notre exemple contiendra d√©sormais 96 ensembles de poids uniques (chaque ensemble pour chaque couche de profondeur), au total, il y aura 96x11x11x3 = 34848 poids uniques ou 34944 param√®tres (+96 d√©calages).  De plus, tous les neurones 55x55 de chaque couche profonde utiliseront d√©sormais les m√™mes param√®tres.  En pratique, pendant la r√©tropropagation, chaque neurone dans cette repr√©sentation calculera le gradient pour ses propres poids, mais ces gradients seront additionn√©s sur chaque couche de profondeur et ne mettront √† jour qu'un seul ensemble de poids √† chaque niveau. </p><br><p>  Notez que si tous les neurones de la m√™me couche profonde utilisaient les m√™mes poids, alors pour une propagation directe √† travers la couche convolutionnelle, la convolution entre les valeurs des poids des neurones et les donn√©es d'entr√©e serait calcul√©e.  C'est pourquoi il est habituel d'appeler un seul ensemble de poids - un <strong>filtre (noyau)</strong> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>Des exemples de filtres obtenus en entra√Ænant le mod√®le Krizhevsky et al.</em>  <em>Chacun des 96 filtres montr√©s ici est de taille 11x11x3 et chacun d'eux est partag√© par tous les neurones 55x55 d'une couche profonde.</em>  <em>Veuillez noter que l'hypoth√®se de partager les m√™mes poids est logique: si la d√©tection d'une ligne horizontale est importante dans une partie de l'image, il est intuitivement clair qu'une telle d√©tection est importante dans une autre partie de cette image.</em>  <em>Par cons√©quent, cela n'a aucun sens de se recycler √† chaque fois pour trouver des lignes horizontales dans chacun des 55x55 endroits diff√©rents de l'image dans la couche convolutionnelle.</em> </p><br><p>  Il ne faut pas oublier que l'hypoth√®se d'un partage des param√®tres n'est pas toujours logique.  Par exemple, si une image avec une structure centr√©e est aliment√©e √† l'entr√©e d'un r√©seau neuronal convolutif, o√π nous aimerions pouvoir apprendre une propri√©t√© dans une partie de l'image et une autre propri√©t√© dans l'autre partie de l'image.  Un exemple pratique est les images √† face centr√©e.  On peut supposer que diff√©rents signes oculaires ou capillaires peuvent √™tre identifi√©s dans diff√©rentes zones de l'image.Par cons√©quent, dans ce cas, la relaxation des poids est utilis√©e et la couche est appel√©e <strong>connect√©e localement</strong> . </p><br><p>  <strong>Quelques exemples</strong> .  Les discussions pr√©c√©dentes devraient √™tre transf√©r√©es au plan des sp√©cificit√©s et sur des exemples avec du code.  Imaginez que la repr√©sentation d'entr√©e est un tableau <code>numpy</code> de <code>X</code>  Ensuite: </p><br><ul><li>  <em>La colonne profonde</em> ( <em>thread</em> ) √† la position <code>(x,y)</code> sera repr√©sent√©e comme suit <code>X[x,y,:]</code> . </li><li>  <em>La couche profonde</em> , ou comme nous l'appelions pr√©c√©demment une telle couche - <em>la carte d'activation</em> √† la profondeur <code>d</code> sera repr√©sent√©e comme suit <code>X[:,:,d]</code> . </li></ul><br><p>  <em>Un exemple de couche convolutionnelle</em> . ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     ‚Äî (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         ‚Äî    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( ‚Äî  ,  ‚Äî ,  ‚Äî  ),        ‚Äî   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 ‚Äî 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 ‚Äî 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               ‚Äî <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     ‚Äî              ,    . ,    ,      ‚Äî        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B ‚Äî  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   ‚Äî           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 ‚Äî F)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    ‚Äî <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     ‚Äî    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    ‚Äî            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 ‚Äî 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   ‚Äî    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 ‚Äî 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     ‚Äî     55.        ""  33     ,    ‚Äî   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           ‚Äî          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       ‚Äî "  ":  ,       ,        ,         ImageNet ‚Äî   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    ‚Äî Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet ‚Äî        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU ‚Äî 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       ‚Äî  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">T√©l√©gramme</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr456186/">https://habr.com/ru/post/fr456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr456174/index.html">Comment organiser un hackathon en tant qu'√©tudiant 101. Deuxi√®me partie</a></li>
<li><a href="../fr456178/index.html">Th√®mes et styles Android sans magie. Et comment les cuisiner avec SwitchCompat</a></li>
<li><a href="../fr456180/index.html">Comment ai-je trouv√© ma premi√®re vuln√©rabilit√©?</a></li>
<li><a href="../fr456182/index.html">Audio via Bluetooth: informations les plus d√©taill√©es sur les profils, les codecs et les appareils</a></li>
<li><a href="../fr456184/index.html">Radio d√©finie par logiciel - comment √ßa marche? Partie 8</a></li>
<li><a href="../fr456188/index.html">Jeton, jeton d'actualisation et cr√©ation d'un wrapper asynchrone pour une demande REST</a></li>
<li><a href="../fr456192/index.html">Des monolithes aux microservices: l'exp√©rience de M.Video-Eldorado et MegaFon</a></li>
<li><a href="../fr456194/index.html">Aide-m√©moire sur les structures de donn√©es Go</a></li>
<li><a href="../fr456196/index.html">Id√©es fausses sur SCRUM</a></li>
<li><a href="../fr456200/index.html">Histoire d'Internet: ARPANET - L'Origine</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>