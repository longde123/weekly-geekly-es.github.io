<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∂üèº üåî üéé Alles, was Sie √ºber die Abfrageverarbeitung wissen wollten, aber sch√ºchtern zu fragen waren üåº üî∑ üë©‚Äçüîß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Was ist ein Netzwerkdienst? Dies ist ein Programm, das eingehende Anforderungen √ºber das Netzwerk akzeptiert und verarbeitet und m√∂glicherweise Antwor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Alles, was Sie √ºber die Abfrageverarbeitung wissen wollten, aber sch√ºchtern zu fragen waren</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/432630/"><p>  Was ist ein Netzwerkdienst?  Dies ist ein Programm, das eingehende Anforderungen √ºber das Netzwerk akzeptiert und verarbeitet und m√∂glicherweise Antworten zur√ºckgibt. </p><br><p>  Es gibt viele Aspekte, in denen sich Netzwerkdienste voneinander unterscheiden.  In diesem Artikel konzentriere ich mich auf den Umgang mit eingehenden Anfragen. </p><br><p>  Die Auswahl einer Anforderungsverarbeitungsmethode hat weitreichende Konsequenzen.  Wie kann ein Chat-Dienst 100.000 gleichzeitigen Verbindungen standhalten?  Welchen Ansatz w√§hlen Sie, um Daten aus einem Strom schlecht strukturierter Dateien zu extrahieren?  Eine falsche Wahl f√ºhrt zu Zeit- und Energieverschwendung. </p><br><p>  Der Artikel beschreibt Ans√§tze wie einen Pool von Prozessen / Threads, ereignisorientierte Verarbeitung, halb synchrones / halb asynchrones Muster und viele andere.  Es werden zahlreiche Beispiele gegeben, die Vor- und Nachteile von Ans√§tzen, ihre Merkmale und Anwendungen werden ber√ºcksichtigt. </p><a name="habracut"></a><br><h2 id="vvedenie">  Einf√ºhrung </h2><br><p>  Das Thema Abfrageverarbeitungsmethoden ist nicht neu, siehe zum Beispiel: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">eins</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zwei</a> .  Die meisten Artikel betrachten dies jedoch nur teilweise.  Dieser Artikel soll die L√ºcken f√ºllen und eine konsistente Darstellung des Problems bieten. </p><br><p>  Die folgenden Ans√§tze werden ber√ºcksichtigt: </p><br><ul><li>  sequentielle Verarbeitung </li><li>  Anforderungsprozess </li><li>  Stream anfordern </li><li>  Prozess- / Thread-Pool </li><li>  ereignisorientierte Verarbeitung (Reaktormuster) </li><li>  halb synchron / halb asynchron </li><li>  F√∂rdererbearbeitung </li></ul><br><p>  Es ist zu beachten, dass ein Dienst, der Anforderungen verarbeitet, nicht unbedingt ein Netzwerkdienst ist.  Dies kann ein Dienst sein, der neue Aufgaben aus der Datenbank oder der Aufgabenwarteschlange empf√§ngt.  In diesem Artikel sind Netzwerkdienste gemeint, aber Sie m√ºssen verstehen, dass die betrachteten Ans√§tze einen gr√∂√üeren Umfang haben. </p><br><h3 id="tldr">  TL; DR </h3><br><p>  Am Ende des Artikels befindet sich eine Liste mit einer kurzen Beschreibung der einzelnen Ans√§tze. </p><br><h2 id="posledovatelnaya-obrabotka">  Sequentielle Verarbeitung </h2><br><p>  Eine Anwendung besteht aus einem einzelnen Thread in einem einzelnen Prozess.  Alle Anfragen werden nur nacheinander bearbeitet.  Es gibt keine Parallelit√§t.  Wenn mehrere Anforderungen gleichzeitig an den Dienst kommen, wird eine davon verarbeitet, der Rest wird in die Warteschlange gestellt. </p><br><p>  Au√üerdem ist dieser Ansatz einfach zu implementieren.  Es gibt keine Sperren und keinen Wettbewerb um Ressourcen.  Das offensichtliche Minus ist die Unf√§higkeit, mit einer gro√üen Anzahl von Kunden zu skalieren. </p><br><h2 id="process-na-zapros">  Anforderungsprozess </h2><br><p>  Eine Anwendung besteht aus einem Kernprozess, der eingehende Anforderungen und Workflows akzeptiert.  F√ºr jede neue Anforderung erstellt der Hauptprozess einen Workflow, der die Anforderung verarbeitet.  Die Skalierung nach Anzahl der Anforderungen ist einfach: Jede Anforderung erh√§lt ihren eigenen Prozess. </p><br><p>  Es gibt nichts Kompliziertes in dieser Architektur, aber es hat <del>  die Probleme </del>  <strong>Einschr√§nkungen</strong> : </p><br><ul><li>  Der Prozess verbraucht viele Ressourcen. <br>  Versuchen Sie, 10.000 gleichzeitige Verbindungen zu PostgreSQL RDBMS herzustellen, und sehen Sie sich das Ergebnis an. </li><li>  Prozesse haben keinen gemeinsamen Speicher (Standard).  Wenn Sie Zugriff auf gemeinsam genutzte Daten oder einen gemeinsam genutzten Cache ben√∂tigen, m√ºssen Sie den gemeinsam genutzten Speicher zuordnen (Linux mmap, munmap aufrufen) oder externen Speicher verwenden (memcahed, redis). </li></ul><br><p>  Diese Probleme h√∂ren keineswegs auf.  Im Folgenden wird gezeigt, wie sie in PostgeSQL RDBMS verwaltet werden. </p><br><p>  <strong>Vorteile</strong> dieser Architektur: </p><br><ul><li>  Der Fall eines der Prozesse hat keine Auswirkungen auf die anderen.  Beispielsweise wird durch einen Verarbeitungsfehler in seltenen F√§llen nicht die gesamte Anwendung gel√∂scht, sondern nur die verarbeitete Anforderung </li><li>  Differenzierung von Zugriffsrechten auf Betriebssystemebene.  Da der Prozess die Essenz des Betriebssystems darstellt, k√∂nnen Sie seine Standardmechanismen zum Abgrenzen von Zugriffsrechten auf Betriebssystemressourcen verwenden </li><li>  Sie k√∂nnen den laufenden Prozess im laufenden Betrieb √§ndern.  Wenn beispielsweise ein separates Skript zum Verarbeiten einer Anforderung verwendet wird und der Verarbeitungsalgorithmus ersetzt wird, reicht es aus, das Skript zu √§ndern.  Ein Beispiel wird unten betrachtet. </li><li>  Multicore-Maschinen effizient eingesetzt </li></ul><br><p>  <strong>Beispiele:</strong> </p><br><ul><li>  PostgreSQL RDBMS erstellt f√ºr jede neue Verbindung einen neuen Prozess.  Shared Memory wird verwendet, um mit allgemeinen Daten zu arbeiten.  PostgreSQL kann den hohen Ressourcenverbrauch von Prozessen auf viele verschiedene Arten bew√§ltigen.  Wenn es nur wenige Kunden gibt (ein dedizierter Stand f√ºr Analysten), gibt es kein solches Problem.  Wenn eine einzelne Anwendung auf die Datenbank zugreift, k√∂nnen Sie auf Anwendungsebene einen Datenbankverbindungspool erstellen.  Wenn es viele Anwendungen gibt, k√∂nnen Sie pgbouncer verwenden </li><li>  sshd wartet bei jeder Verbindung auf eingehende Anfragen an Port 22 und Fork.  Jede SSH-Verbindung ist eine Abzweigung des SSH-D√§mons, der Benutzerbefehle nacheinander empf√§ngt und ausf√ºhrt.  Dank dieser Architektur werden Ressourcen des Betriebssystems selbst verwendet, um Zugriffsrechte zu unterscheiden </li><li>  Ein Beispiel aus unserer eigenen Praxis.  Es gibt einen Strom unstrukturierter Dateien, aus denen Sie Metadaten abrufen m√ºssen.  Der Hauptdienstprozess verteilt Dateien auf die Handlerprozesse.  Jeder Handlerprozess ist ein Skript, das einen Dateipfad als Parameter verwendet.  Die Dateiverarbeitung erfolgt in einem separaten Prozess. Aufgrund eines Verarbeitungsfehlers st√ºrzt der gesamte Dienst nicht ab.  Um den Verarbeitungsalgorithmus zu aktualisieren, reicht es aus, die Verarbeitungsskripte zu √§ndern, ohne den Dienst zu beenden. </li></ul><br><p>  Im Allgemeinen muss ich sagen, dass dieser Ansatz seine Vorteile hat, die seinen Umfang bestimmen, aber die Skalierbarkeit ist sehr begrenzt. </p><br><h2 id="potok-na-zapros">  Stream anfordern </h2><br><p>  Dieser Ansatz √§hnelt dem vorherigen.  Der Unterschied besteht darin, dass Threads anstelle von Prozessen verwendet werden.  Auf diese Weise k√∂nnen Sie den gemeinsam genutzten Speicher sofort verwenden.  Die anderen Vorteile des vorherigen Ansatzes k√∂nnen jedoch nicht mehr genutzt werden, w√§hrend der Ressourcenverbrauch ebenfalls hoch sein wird. </p><br><p>  <strong>Vorteile:</strong> </p><br><ul><li>  Standardm√§√üig gemeinsam genutzter Speicher </li><li>  Einfache Implementierung </li><li>  Effiziente Nutzung von Multi-Core-CPUs </li></ul><br><p>  <strong>Nachteile:</strong> </p><br><ul><li>  Ein Stream verbraucht viele Ressourcen.  Unter Unix-√§hnlichen Betriebssystemen verbraucht ein Thread fast so viele Ressourcen wie ein Prozess </li></ul><br><p>  Ein Anwendungsbeispiel ist MySQL.  Es sollte jedoch beachtet werden, dass MySQL einen gemischten Ansatz verwendet, sodass dieses Beispiel im n√§chsten Abschnitt erl√§utert wird. </p><br><h2 id="pul-processovpotokov">  Prozess- / Thread-Pool </h2><br><p>  Streams (Prozesse) erzeugen teuer und lang.  Um keine Ressourcen zu verschwenden, k√∂nnen Sie denselben Thread wiederholt verwenden.  Nachdem wir die maximale Anzahl von Threads zus√§tzlich begrenzt haben, erhalten wir einen Pool von Threads (Prozessen).  Jetzt akzeptiert der Hauptthread eingehende Anforderungen und stellt sie in eine Warteschlange.  Workflows nehmen Anforderungen aus der Warteschlange und verarbeiten sie.  Dieser Ansatz kann als nat√ºrliche Skalierung der sequentiellen Verarbeitung von Anforderungen angesehen werden: Jeder Worker-Thread kann Flows nur sequentiell verarbeiten. Wenn Sie sie b√ºndeln, k√∂nnen Sie Anforderungen parallel verarbeiten.  Wenn jeder Stream 1000 U / min verarbeiten kann, bew√§ltigen 5 Streams die Last nahe 5000 U / s (vorbehaltlich eines minimalen Wettbewerbs um gemeinsam genutzte Ressourcen). </p><br><p>  Der Pool kann zu Beginn des Dienstes im Voraus erstellt oder schrittweise gebildet werden.  Die Verwendung eines Thread-Pools ist h√§ufiger als  erm√∂glicht es Ihnen, gemeinsam genutzten Speicher anzuwenden. </p><br><p>  Die Gr√∂√üe des Thread-Pools muss nicht begrenzt sein.  Ein Dienst kann freie Threads aus dem Pool verwenden. Wenn keine vorhanden sind, erstellen Sie einen neuen Thread.  Nach der Verarbeitung der Anforderung tritt der Thread dem Pool bei und wartet auf die n√§chste Anforderung.  Diese Option ist eine Kombination aus einem Thread-on-Request-Ansatz und einem Thread-Pool.  Ein Beispiel wird unten gegeben. </p><br><p>  <strong>Vorteile:</strong> </p><br><ul><li>  die Verwendung vieler CPU-Kerne </li><li>  Kostenreduzierung f√ºr die Erstellung eines Threads / Prozesses </li></ul><br><p>  <strong>Nachteile:</strong> </p><br><ul><li>  Begrenzte Skalierbarkeit bei der Anzahl gleichzeitiger Clients.  Die Verwendung des Pools erm√∂glicht es uns, denselben Thread ohne zus√§tzliche Ressourcenkosten mehrmals wiederzuverwenden, l√∂st jedoch nicht das grundlegende Problem einer gro√üen Anzahl von Ressourcen, die vom Thread / Prozess ausgegeben werden.  Das Erstellen eines Chat-Dienstes, der mit diesem Ansatz 100.000 gleichzeitigen Verbindungen standh√§lt, schl√§gt fehl. </li><li>  Die Skalierbarkeit wird durch gemeinsam genutzte Ressourcen eingeschr√§nkt, z. B. wenn Threads gemeinsam genutzten Speicher verwenden, indem der Zugriff mithilfe von Semaphoren / Mutexen angepasst wird.  Dies ist eine Einschr√§nkung aller Ans√§tze, die gemeinsam genutzte Ressourcen verwenden. </li></ul><br><p>  <strong>Beispiele:</strong> </p><br><ol><li>  Python-Anwendung, die mit uWSGI und nginx ausgef√ºhrt wird.  Der uWSGI-Hauptprozess empf√§ngt eingehende Anforderungen von nginx und verteilt sie auf die Python-Prozesse des Interpreters, der die Anforderungen verarbeitet.  Die Anwendung kann auf jedem uWSGI-kompatiblen Framework geschrieben werden - Django, Flask usw. </li><li>  MySQL verwendet einen Thread-Pool: Jede neue Verbindung wird von einem der freien Threads aus dem Pool verarbeitet.  Wenn keine freien Threads vorhanden sind, erstellt MySQL einen neuen Thread.  Die Gr√∂√üe des Pools freier Threads und die maximale Anzahl von Threads (Verbindungen) sind durch die Einstellungen begrenzt. </li></ol><br><p>  Vielleicht ist dies einer der h√§ufigsten Ans√§tze zum Aufbau von Netzwerkdiensten, wenn nicht der h√§ufigste.  Sie k√∂nnen gut skalieren und gro√üe U / min erreichen.  Die Hauptbeschr√§nkung des Ansatzes ist die Anzahl der gleichzeitig verarbeiteten Netzwerkverbindungen.  Tats√§chlich funktioniert dieser Ansatz nur dann gut, wenn die Anfragen kurz sind oder nur wenige Kunden. </p><br><h2 id="sobytiyno-orientirovannaya-obrabotka-reactor-pattern">  Ereignisorientierte Verarbeitung (Reaktormuster) </h2><br><p>  Zwei Paradigmen - synchron und asynchron - sind ewige Konkurrenten voneinander.  Bisher wurden nur synchrone Ans√§tze diskutiert, aber es w√§re falsch, den asynchronen Ansatz zu ignorieren.  Die ereignisorientierte oder reaktive Anforderungsverarbeitung ist ein Ansatz, bei dem jede E / A-Operation asynchron ausgef√ºhrt wird und am Ende der Operation ein Handler aufgerufen wird.  In der Regel besteht die Verarbeitung jeder Anforderung aus vielen asynchronen Aufrufen, gefolgt von der Ausf√ºhrung von Handlern.  Zu jedem Zeitpunkt f√ºhrt eine Single-Threaded-Anwendung den Code nur eines Handlers aus, aber die Ausf√ºhrung der Handler verschiedener Anforderungen wechselt sich ab, sodass Sie viele parallele Anforderungen gleichzeitig (pseudo-parallel) verarbeiten k√∂nnen. </p><br><p>  Eine vollst√§ndige Er√∂rterung dieses Ansatzes w√ºrde den Rahmen dieses Artikels sprengen.  F√ºr einen tieferen Blick k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reactor (Reactor)</a> empfehlen. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was ist das Geheimnis der NodeJS-Geschwindigkeit?</a>  , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Innerhalb von NGINX</a> .  Hier beschr√§nken wir uns darauf, die Vor- und Nachteile dieses Ansatzes zu betrachten. </p><br><p>  <strong>Vorteile:</strong> </p><br><ul><li>  Effektive Skalierung durch rps und die Anzahl der gleichzeitigen Verbindungen.  Ein reaktiver Dienst kann gleichzeitig eine gro√üe Anzahl von Verbindungen (Zehntausende) verarbeiten, wenn die meisten Verbindungen auf den Abschluss der E / A warten </li></ul><br><p>  <strong>Nachteile:</strong> </p><br><ul><li>  Die Komplexit√§t der Entwicklung.  Das asynchrone Programmieren ist schwieriger als das synchrone Programmieren.  Die Logik der Anforderungsverarbeitung ist komplexer, das Debuggen ist auch schwieriger als bei synchronem Code. </li><li>  Fehler, die zum Blockieren des gesamten Dienstes f√ºhren.  Wenn die Sprache oder Laufzeit urspr√ºnglich nicht f√ºr die asynchrone Verarbeitung ausgelegt ist, kann eine einzelne synchrone Operation den gesamten Dienst blockieren und die M√∂glichkeit einer Skalierung zunichte machen. </li><li>  Schwierig √ºber CPU-Kerne zu skalieren.  Bei diesem Ansatz wird ein einzelner Thread in einem einzelnen Prozess vorausgesetzt, sodass Sie nicht mehrere CPU-Kerne gleichzeitig verwenden k√∂nnen.  Es ist zu beachten, dass es M√∂glichkeiten gibt, diese Einschr√§nkung zu umgehen. </li><li>  Folgerung aus dem vorherigen Absatz: Dieser Ansatz l√§sst sich f√ºr Anforderungen, die CPU erfordern, nicht gut skalieren.  Die Anzahl der RPS f√ºr diesen Ansatz ist umgekehrt proportional zur Anzahl der CPU-Operationen, die zur Verarbeitung jeder Anforderung erforderlich sind.  Das Anfordern von CPU-Anforderungen negiert die Vorteile dieses Ansatzes. </li></ul><br><p>  <strong>Beispiele:</strong> </p><br><ol><li>  Node.js verwendet das Out-of-Box-Reaktormuster.  Weitere Informationen finden Sie unter Was ist das Geheimnis der NodeJS-Geschwindigkeit? </li><li>  nginx: Die Arbeitsprozesse von nginx verwenden das Reaktormuster, um Anforderungen parallel zu verarbeiten.  Weitere Informationen finden Sie unter Inside NGINX. </li><li>  C / C ++ - Programm, das direkt Betriebssystemtools verwendet (epoll unter Linux, IOCP unter Windows, kqueue unter FreeBSD) oder das Framework (libev, libevent, libuv usw.) verwendet. </li></ol><br><h2 id="half-synchalf-async">  Halb synchron / halb asynchron </h2><br><p>  Der Name stammt von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">POSA: Patterns for Concurrent and Networked Objects</a> .  Im Original wird dieses Muster sehr weit ausgelegt, aber f√ºr die Zwecke dieses Artikels werde ich dieses Muster etwas enger verstehen.  Half Sync / Half Async ist ein Anforderungsverarbeitungsansatz, bei dem f√ºr jede Anforderung ein einfacher Kontrollfluss (gr√ºner Thread) verwendet wird.  Ein Programm besteht aus einem oder mehreren Threads auf Betriebssystemebene. Das Programmausf√ºhrungssystem unterst√ºtzt jedoch gr√ºne Threads, die das Betriebssystem nicht sieht und nicht steuern kann. </p><br><p>  Einige <strong>Beispiele</strong> , um die √úberlegung genauer zu machen: </p><br><ol><li>  Service in Go-Sprache.  Die Go-Sprache unterst√ºtzt viele einfache Ausf√ºhrungsthreads - Goroutine.  Das Programm verwendet einen oder mehrere Betriebssystem-Threads, aber der Programmierer arbeitet mit Goroutinen, die transparent zwischen Betriebssystem-Threads verteilt sind, um Mehrkern-CPUs zu verwenden </li><li>  Python-Dienst mit Gevent-Bibliothek.  Die Gevent-Bibliothek erm√∂glicht es dem Programmierer, gr√ºne Threads auf Bibliotheksebene zu verwenden.  Das gesamte Programm wird in einem einzigen Betriebssystem-Thread ausgef√ºhrt. </li></ol><br><p>  Im Wesentlichen soll dieser Ansatz die hohe Leistung des asynchronen Ansatzes mit der Einfachheit der Programmierung von synchronem Code kombinieren. </p><br><p>  Bei diesem Ansatz arbeitet das Programm trotz der Illusion der Synchronit√§t asynchron: Das Programmausf√ºhrungssystem steuert die Ereignisschleife, und jede "synchrone" Operation ist tats√§chlich asynchron.  Wenn eine solche Operation aufgerufen wird, ruft das Ausf√ºhrungssystem die asynchrone Operation unter Verwendung der Betriebssystemtools auf und registriert den Handler f√ºr den Abschluss der Operation.  Wenn die asynchrone Operation abgeschlossen ist, ruft das Ausf√ºhrungssystem den zuvor registrierten Handler auf, der das Programm zum Zeitpunkt des Aufrufs der "synchronen" Operation weiter ausf√ºhrt. </p><br><p>  Infolgedessen enth√§lt der halb synchrone / halb asynchrone Ansatz sowohl einige Vor- als auch einige Nachteile des asynchronen Ansatzes.  Der Umfang des Artikels erlaubt es uns nicht, diesen Ansatz im Detail zu betrachten.  F√ºr Interessierte empfehle ich Ihnen, das gleichnamige Kapitel im Buch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">POSA: Muster f√ºr gleichzeitige und vernetzte Objekte</a> zu lesen. </p><br><p>  Mit dem halb synchronen / halb asynchronen Ansatz selbst wird eine neue ‚ÄûGreen Stream‚Äú -Einheit eingef√ºhrt - ein einfacher Kontrollfluss auf der Ebene des Programm- oder Bibliotheksausf√ºhrungssystems.  Was mit gr√ºnen F√§den zu tun ist, entscheidet ein Programmierer.  Es kann einen Pool gr√ºner Threads verwenden und f√ºr jede neue Anforderung einen neuen gr√ºnen Thread erstellen.  Der Unterschied zu OS-Threads / -Prozessen besteht darin, dass gr√ºne Threads viel billiger sind: Sie verbrauchen viel weniger RAM und werden viel schneller erstellt.  Auf diese Weise k√∂nnen Sie eine gro√üe Anzahl gr√ºner Threads erstellen, z. B. Hunderttausende in der Sprache Go.  Eine solch gro√üe Menge rechtfertigt die Verwendung des Green-Flow-on-Request-Ansatzes. </p><br><p>  <strong>Vorteile:</strong> </p><br><ul><li>  Es skaliert gut in rps und der Anzahl gleichzeitiger Verbindungen </li><li>  Code ist einfacher zu schreiben und zu debuggen als der asynchrone Ansatz </li></ul><br><p>  <strong>Nachteile:</strong> </p><br><ul><li>  Da die Ausf√ºhrung von Operationen tats√§chlich asynchron ist, sind Programmierfehler m√∂glich, wenn eine einzelne synchrone Operation den gesamten Prozess blockiert.  Dies ist insbesondere in Sprachen zu sp√ºren, in denen dieser Ansatz mithilfe einer Bibliothek implementiert wird, beispielsweise Python. </li><li>  Die Deckkraft des Programms.  Bei Verwendung von Threads oder Betriebssystemprozessen ist der Programmausf√ºhrungsalgorithmus klar: Jeder Thread / Prozess f√ºhrt Operationen in der Reihenfolge aus, in der sie in den Code geschrieben sind.  Bei Verwendung des halb synchronen / halb asynchronen Ansatzes k√∂nnen sich Operationen, die nacheinander in den Code geschrieben werden, unvorhersehbar mit Operationen abwechseln, die gleichzeitige Anforderungen verarbeiten. </li><li>  Ungeeignet f√ºr Echtzeitsysteme.  Die asynchrone Verarbeitung von Anforderungen erschwert die Bereitstellung von Garantien f√ºr die Verarbeitungszeit jeder einzelnen Anforderung erheblich.  Dies ist eine Folge des vorherigen Absatzes. </li></ul><br><p>  Abh√§ngig von der Implementierung l√§sst sich dieser Ansatz gut √ºber CPU-Kerne hinweg skalieren (Golang) oder √ºberhaupt nicht skalieren (Python). <br>  Dieser Ansatz sowie die asynchrone Methode erm√∂glichen es Ihnen, eine gro√üe Anzahl gleichzeitiger Verbindungen zu verarbeiten.  Das Programmieren eines Dienstes mit diesem Ansatz ist jedoch einfacher, weil  Code wird synchron geschrieben. </p><br><h2 id="konveyernaya-obrabotka">  F√∂rderbearbeitung </h2><br><p>  Wie der Name schon sagt, werden bei diesem Ansatz Anforderungen per Pipeline verarbeitet.  Der Verarbeitungsprozess besteht aus mehreren OS-Threads, die in einer Kette angeordnet sind.  Jeder Thread ist ein Glied in der Kette und f√ºhrt eine bestimmte Teilmenge der Operationen aus, die zur Verarbeitung der Anforderung erforderlich sind.  Jede Anforderung durchl√§uft nacheinander alle Glieder in der Kette, und unterschiedliche Glieder verarbeiten zu jedem Zeitpunkt unterschiedliche Anforderungen. </p><br><p>  <strong>Vorteile:</strong> </p><br><ul><li>  Dieser Ansatz l√§sst sich gut in rps skalieren.  Je mehr Glieder in der Kette sind, desto mehr Anfragen werden pro Sekunde verarbeitet. </li><li>  Durch die Verwendung mehrerer Threads k√∂nnen Sie gut √ºber CPU-Kerne skalieren. </li></ul><br><p>  <strong>Nachteile:</strong> </p><br><ul><li>  Nicht alle Abfragekategorien sind f√ºr diesen Ansatz geeignet.  Zum Beispiel wird es schwierig und unpraktisch sein, lange Abstimmungen mit diesem Ansatz zu organisieren. </li><li>  Die Komplexit√§t der Implementierung und des Debuggens.  Die sequentielle Verarbeitung so zu schlagen, dass die Produktivit√§t hoch ist, kann schwierig sein.  Das Debuggen eines Programms, in dem jede Anforderung nacheinander in mehreren parallelen Threads verarbeitet wird, ist schwieriger als das sequentielle Verarbeiten. </li></ul><br><p>  <strong>Beispiele:</strong> </p><br><ol><li>  Ein interessantes Beispiel f√ºr die Verarbeitung von F√∂rderb√§ndern wurde im Bericht highload 2018 Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Entwicklung der Architektur des Handels- und Clearingsystems der Moskauer B√∂rse beschrieben</a> </li></ol><br><p>  Pipelining ist weit verbreitet, aber meistens sind die Links einzelne Komponenten in unabh√§ngigen Prozessen, die Nachrichten austauschen, beispielsweise √ºber eine Nachrichtenwarteschlange oder eine Datenbank. </p><br><h2 id="rezyume">  Zusammenfassung </h2><br><p>  Eine kurze Zusammenfassung der betrachteten Ans√§tze: </p><br><ul><li>  Synchrone Verarbeitung. <br>  Ein einfacher Ansatz, der jedoch in Bezug auf die Skalierbarkeit sowohl in Bezug auf die Geschwindigkeit als auch in Bezug auf die Anzahl der gleichzeitigen Verbindungen sehr eingeschr√§nkt ist.  Es ist nicht m√∂glich, mehrere CPU-Kerne gleichzeitig zu verwenden. </li><li>  Ein neuer Prozess f√ºr jede Anfrage. <br>  Hohe Kosten f√ºr die Erstellung von Prozessen.         ,      .             .       ( ,     ). </li><li>     . <br>   ,     ,      .       ,      . </li><li>  /. <br>            /.        .    rps    .        .      . </li><li> -  (reactor ). <br>    rps    .   -   ,     .      CPU    </li><li> Half sync/half async. <br>    rps    .         CPU (Golang)     (Python).      ,   ()  .        reactor ,      ,    reactor . </li><li>  . <br>    ,     .       (, long polling   ). </li></ul><br><p>     ,        . </p><br><p>   :    ?    ,        ? </p><br><h3 id="ssylki">  Referenzen </h3><br><ol><li>   : <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">     </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">     : </a> </li></ul></li><li> - : <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reactor ()</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    NodeJS?</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inside NGINX</a> </li></ul></li><li>       : <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache vs Nginx:  </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">       Node.js  PHP</a> </li></ul></li><li> Half sync/half async: <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Half-Sync/Half-Async (Java Design Patterns)</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">POSA: Patterns for Concurrent and Networked Objects</a> </li></ul></li><li>  : <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Green threads ()</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Green Vs Native Threads</a> </li></ul></li><li>  : <br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  -   </a> </li></ul></li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de432630/">https://habr.com/ru/post/de432630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de432620/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 20: Mobiltelefonsicherheit, Teil 3</a></li>
<li><a href="../de432622/index.html">Ben√∂tigen Sie mehr verschiedene Unsch√§rfen</a></li>
<li><a href="../de432624/index.html">Lernen Sie kontroverse Taktiken, Techniken und allgemeines Wissen (ATT @ CK). Unternehmenstaktik. Teil 5</a></li>
<li><a href="../de432626/index.html">M√∂glichkeiten zur Interaktion mit dem System: von Lochb√§ndern bis zu Neurointerfaces</a></li>
<li><a href="../de432628/index.html">@ Pythonetc November 2018</a></li>
<li><a href="../de432632/index.html">Die Geschichte von Lenny, dem beliebtesten Spam-Telefon-Troll des Internet-Trolls</a></li>
<li><a href="../de432634/index.html">√úbersicht √ºber f√ºnf HTTP-Webentwicklungsbibliotheken</a></li>
<li><a href="../de432636/index.html">React Tutorial Teil 1: Kurs√ºbersicht, React, ReactDOM und JSX Gr√ºnde f√ºr die Popularit√§t</a></li>
<li><a href="../de432638/index.html">Was ist neu in Upsource 2018.2?</a></li>
<li><a href="../de432640/index.html">Rust 1.31 und Rust 2018 Release</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>