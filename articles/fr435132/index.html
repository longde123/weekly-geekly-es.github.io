<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¥üèº üêâ üßöüèΩ Nomade: probl√®mes et solutions üÜî üë®üèª‚ÄçüöÄ ‚ô£Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le premier service dans Nomad I a √©t√© lanc√© en septembre 2016. √Ä l'heure actuelle, je l'utilise en tant que programmeur et support en tant qu'administ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Nomade: probl√®mes et solutions</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435132/"><p>  Le premier service dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nomad</a> I a √©t√© lanc√© en septembre 2016.  √Ä l'heure actuelle, je l'utilise en tant que programmeur et support en tant qu'administrateur de deux clusters Nomad - un "home" pour mes projets personnels (6 machines micro-virtuelles dans Hetzner Cloud et ArubaCloud dans 5 centres de donn√©es diff√©rents en Europe) et le second en fonctionnement (environ 40 serveurs virtuels et physiques priv√©s dans deux centres de donn√©es). </p><br><p>  Au cours des derni√®res ann√©es, beaucoup d'exp√©rience a √©t√© accumul√©e avec l'environnement Nomad, dans l'article, je d√©crirai les probl√®mes rencontr√©s par Nomad et comment y faire face. </p><br><p><img src="https://habrastorage.org/webt/k5/9m/pp/k59mpp5iyvtxtj2q9nrvthzpelo.jpeg"><br>  <em>Le nomade de Yamal cr√©e une instance de livraison continue de votre logiciel ¬© National Geographic Russia</em> </p><a name="habracut"></a><br><h2 id="1-kolichestvo-servernyh-nod-na-odin-datacentr">  1. Le nombre de n≈ìuds de serveur par centre de donn√©es </h2><br><p>  <strong>Solution: un n≈ìud de serveur suffit pour un centre de donn√©es.</strong> </p><br><p>  La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> n'indique pas explicitement le nombre de n≈ìuds de serveur requis dans un centre de donn√©es.  Il est seulement indiqu√© que 3 √† 5 n≈ìuds sont n√©cessaires par r√©gion, ce qui est logique pour le consensus du protocole de radeau. </p><br><p><img src="https://habrastorage.org/webt/go/yt/gu/goytgumjr0zxxqodicboxfgipze.png"></p><br><p>  Au d√©but, j'avais pr√©vu 2 √† 3 n≈ìuds de serveur dans chaque centre de donn√©es pour assurer la redondance. </p><br><p>  Lors de l'utilisation, il s'est av√©r√©: </p><br><ol><li>  Cela n'est tout simplement pas n√©cessaire, car en cas de d√©faillance d'un n≈ìud dans le centre de donn√©es, le r√¥le du n≈ìud de serveur pour les agents dans ce centre de donn√©es sera jou√© par d'autres n≈ìuds de serveur dans la r√©gion. </li><li>  Cela s'av√®re encore pire si le probl√®me 8 n'est pas r√©solu.  Lorsque l'assistant est r√©√©lu, des incoh√©rences peuvent se produire et Nomad red√©marrera une partie des services. </li></ol><br><h2 id="2-resursy-servera-dlya-servernoy-nody">  2. Ressources serveur pour le n≈ìud de serveur </h2><br><p>  <strong>Solution: une petite machine virtuelle suffit pour le n≈ìud de serveur.</strong>  <strong>Sur le m√™me serveur, il est autoris√© √† ex√©cuter d'autres services non gourmands en ressources.</strong> </p><br><p>  La consommation de m√©moire du d√©mon Nomad d√©pend du nombre de t√¢ches en cours d'ex√©cution.  Consommation CPU - bas√©e sur le nombre de t√¢ches et le nombre de serveurs / agents dans la r√©gion (non lin√©aire). </p><br><p>  Dans notre cas: pour 300 t√¢ches en cours d'ex√©cution, la consommation de m√©moire est d'environ 500 Mo pour le n≈ìud ma√Ætre actuel. </p><br><p>  Dans un cluster fonctionnel, une machine virtuelle pour un n≈ìud de serveur: 4 CPU, 6 Go de RAM. <br>  Lancement suppl√©mentaire: Consul, Etcd, Vault. </p><br><h2 id="3-konsensus-pri-nehvatke-datacentrov">  3. Consensus sur le manque de centres de donn√©es </h2><br><p>  <strong>Solution: nous fabriquons trois centres de donn√©es virtuels et trois n≈ìuds de serveur pour deux centres de donn√©es physiques.</strong> </p><br><p>  Le travail du nomade dans la r√©gion est bas√© sur le protocole du radeau.  Pour un fonctionnement correct, vous avez besoin d'au moins 3 n≈ìuds de serveur situ√©s dans diff√©rents centres de donn√©es.  Cela permettra un fonctionnement correct avec une perte compl√®te de connectivit√© r√©seau avec l'un des centres de donn√©es. </p><br><p>  Mais nous n'avons que deux centres de donn√©es.  Nous faisons un compromis: nous s√©lectionnons un centre de donn√©es, auquel nous faisons davantage confiance, et y faisons un n≈ìud de serveur suppl√©mentaire.  Pour ce faire, nous introduisons un centre de donn√©es virtuel suppl√©mentaire, qui sera physiquement situ√© dans le m√™me centre de donn√©es (voir le paragraphe 2 du probl√®me 1). </p><br><p>  <strong>Solution alternative: nous divisons les centres de donn√©es en r√©gions distinctes.</strong> </p><br><p>  Par cons√©quent, les centres de donn√©es fonctionnent ind√©pendamment et un consensus n'est n√©cessaire que dans un seul centre de donn√©es.  Dans un centre de donn√©es, dans ce cas, il est pr√©f√©rable de cr√©er 3 n≈ìuds de serveur en impl√©mentant trois centres de donn√©es virtuels en un seul. </p><br><p>  Cette option est moins pratique pour la r√©partition des t√¢ches, mais donne une garantie √† 100% de l'ind√©pendance des services en cas de probl√®mes de r√©seau entre les centres de donn√©es. </p><br><h2 id="4-server-i-agent-na-odnom-servere">  4. "Serveur" et "agent" sur le m√™me serveur </h2><br><p>  <strong>Solution: valide si vous avez un nombre limit√© de serveurs.</strong> </p><br><p>  La documentation des nomades indique que cela n'est pas souhaitable.  Mais si vous n'avez pas la possibilit√© d'allouer des machines virtuelles distinctes aux n≈ìuds de serveur, vous pouvez placer le serveur et les n≈ìuds d'agent sur le m√™me serveur. </p><br><p>  L'ex√©cution simultan√©e signifie le d√©marrage du d√©mon Nomad en mode client et en mode serveur. </p><br><p>  Qu'est-ce que cela menace?  Avec une lourde charge sur le CPU de ce serveur, le n≈ìud du serveur Nomad fonctionnera de mani√®re instable, la perte de consensus et les battements de c≈ìur, les rechargements de service sont possibles. <br>  Pour √©viter cela, nous augmentons les limites de la description du probl√®me n ¬∞ 8. </p><br><h2 id="5-realizaciya-prostranstv-imyon-namespaces">  5. Mise en place d'espaces de noms </h2><br><p>  <strong>Solution: peut-√™tre par l'organisation d'un data center virtuel.</strong> </p><br><p>  Parfois, vous devez ex√©cuter une partie des services sur des serveurs distincts. </p><br><p>  La solution est la premi√®re, simple, mais plus exigeante en ressources.  Nous divisons tous les services en groupes selon leur fonction: frontend, backend, ... Ajoutez des m√©ta-attributs aux serveurs, prescrivez les attributs √† ex√©cuter pour tous les services. </p><br><p>  La deuxi√®me solution est simple.  Nous ajoutons de nouveaux serveurs, leur prescrivons des m√©ta-attributs, prescrivons ces attributs de lancement aux services n√©cessaires, tous les autres services prescrivent une interdiction de lancement sur les serveurs avec cet attribut. </p><br><p>  La troisi√®me solution est compliqu√©e.  Nous cr√©ons un datacenter virtuel: lancez Consul pour un nouveau datacenter, lancez le n≈ìud serveur Nomad pour ce datacenter, sans oublier le nombre de n≈ìuds serveur pour cette r√©gion.  Vous pouvez d√©sormais ex√©cuter des services individuels dans ce centre de donn√©es virtuel d√©di√©. </p><br><h2 id="6-integraciya-s-vault">  6. Int√©gration avec Vault </h2><br><p>  <strong>Solution: √©vitez les d√©pendances circulaires Nomad &lt;-&gt; Vault.</strong> </p><br><p>  Launched Vault ne devrait pas avoir de d√©pendances sur Nomad.  L'adresse du coffre-fort enregistr√©e dans Nomad doit de pr√©f√©rence pointer directement vers le coffre-fort, sans couches d'√©quilibreurs (mais valide).  Dans ce cas, la r√©servation du coffre-fort peut √™tre effectu√©e via DNS - Consul DNS ou externe. </p><br><p>  Si les donn√©es Vault sont √©crites dans les fichiers de configuration de Nomad, Nomad essaie d'acc√©der √† Vault au d√©marrage.  Si l'acc√®s √©choue, alors Nomad refuse de commencer. </p><br><p>  J'ai fait une erreur avec une d√©pendance cyclique il y a longtemps, cela a une fois bri√®vement d√©truit presque compl√®tement le cluster Nomad.  Vault a √©t√© lanc√© correctement, ind√©pendamment de Nomad, mais Nomad a examin√© l'adresse de Vault via les √©quilibreurs qui s'ex√©cutaient dans Nomad lui-m√™me.  La reconfiguration et le red√©marrage des n≈ìuds du serveur Nomad ont provoqu√© un red√©marrage des services d'√©quilibrage, ce qui a entra√Æn√© l'√©chec du d√©marrage des n≈ìuds du serveur eux-m√™mes. </p><br><h2 id="7-zapusk-vazhnyh-statefull-servisov">  7. Lancement d'importants services publics </h2><br><p>  <strong>Solution: valide, mais pas moi.</strong> </p><br><p>  Est-il possible d'ex√©cuter PostgreSQL, ClickHouse, Redis Cluster, RabbitMQ, MongoDB via Nomad? </p><br><p>  Imaginez que vous disposez d'un ensemble de services importants, dont le travail est li√© √† la plupart des autres services.  Par exemple, une base de donn√©es dans PostgreSQL / ClickHouse.  Ou stockage g√©n√©ral √† court terme dans Redis Cluster / MongoDB.  Ou un bus de donn√©es dans Redis Cluster / RabbitMQ. </p><br><p>  Tous ces services impl√©mentent en quelque sorte un sch√©ma tol√©rant aux pannes: Stolon / Patroni pour PostgreSQL, sa propre impl√©mentation de radeau dans Redis Cluster, sa propre impl√©mentation de cluster dans RabbitMQ, MongoDB, ClickHouse. </p><br><p>  Oui, tous ces services peuvent √™tre lanc√©s via Nomad en r√©f√©rence √† des serveurs sp√©cifiques, mais pourquoi? </p><br><p>  Plus - facilit√© de lancement, un format de script unique, comme d'autres services.  Pas besoin de s'inqui√©ter avec les scripts ansibles / quoi que ce soit d'autre. </p><br><p>  Le moins est un point de d√©faillance suppl√©mentaire, qui ne pr√©sente aucun avantage.  Personnellement, j'ai compl√®tement abandonn√© le cluster Nomad √† deux reprises pour diverses raisons: une fois ¬´chez moi¬ª, une fois au travail.  C'√©tait au d√©but de l'introduction de Nomad et en raison de la n√©gligence. <br>  De plus, Nomad commence √† mal se comporter et red√©marre les services en raison du probl√®me num√©ro 8.  Mais m√™me si ce probl√®me est r√©solu, le danger demeure. </p><br><h2 id="8-stabilizaciya-raboty-i-restartov-servisov-v-nestabilnoy-seti">  8. Stabilisation du travail et des red√©marrages de service dans un r√©seau instable </h2><br><p>  <strong>Solution: utilisez les options de r√©glage du rythme cardiaque.</strong> </p><br><p>  Par d√©faut, Nomad est configur√© de sorte que tout probl√®me de r√©seau √† court terme ou charge de processeur entra√Æne une perte de consensus et une r√©√©lection de l'assistant ou le marquage du n≈ìud d'agent comme inaccessible.  Et cela conduit √† des red√©marrages spontan√©s des services et √† leur transfert vers d'autres n≈ìuds. </p><br><p>  Statistiques du cluster "home" avant de r√©soudre le probl√®me: la dur√©e de vie maximale du conteneur avant red√©marrage est d'environ 10 jours.  Ici, il est toujours surcharg√© d'ex√©cuter l'agent et le serveur sur un seul serveur et de le placer dans 5 centres de donn√©es diff√©rents en Europe, ce qui implique une charge importante sur le processeur et un r√©seau moins stable. </p><br><p>  Statistiques du cluster de travail avant de r√©soudre le probl√®me: la dur√©e de vie maximale du conteneur avant red√©marrage est sup√©rieure √† 2 mois.  Tout est relativement bon ici en raison des serveurs s√©par√©s pour les n≈ìuds de serveur Nomad et de l'excellent r√©seau entre les centres de donn√©es. </p><br><p>  Valeurs par d√©faut </p><br><pre><code class="plaintext hljs">heartbeat_grace = "10s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  A en juger par le code: dans cette configuration, les battements de c≈ìur sont effectu√©s toutes les 10 secondes.  Avec la perte de deux battements de c≈ìur, la r√©√©lection du ma√Ætre ou le transfert de services du n≈ìud d'agent commence.  Des param√®tres controvers√©s, √† mon avis.  Nous les modifions en fonction de l'application. </p><br><p>  Si vous avez tous les services ex√©cut√©s dans plusieurs instances et distribu√©s par des centres de donn√©es, alors tr√®s probablement, peu importe pour vous une longue p√©riode de d√©termination de l'inaccessibilit√© du serveur (environ 5 minutes, dans l'exemple ci-dessous) - nous rendons moins fr√©quents l'intervalle de pulsation et une plus longue p√©riode de d√©termination de l'inaccessibilit√©.  Voici un exemple de configuration de mon cluster d'origine: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "300s" min_heartbeat_ttl = "30s" max_heartbeats_per_second = 10.0</code> </pre> <br><p>  Si vous disposez d'une bonne connectivit√© r√©seau, de serveurs s√©par√©s pour les n≈ìuds de serveur et que la p√©riode de d√©termination de l'inaccessibilit√© du serveur est importante (il existe un service en cours d'ex√©cution dans une instance et il est important de le transf√©rer rapidement), augmentez la p√©riode de d√©termination de l'inaccessibilit√© (heartbeat_grace).  En option, vous pouvez faire plus de battements de c≈ìur (en diminuant min_heartbeat_ttl) - cela augmentera l√©g√®rement la charge sur le CPU.  Exemple de configuration de cluster de travail: </p><br><pre> <code class="plaintext hljs"> heartbeat_grace = "60s" min_heartbeat_ttl = "10s" max_heartbeats_per_second = 50.0</code> </pre> <br><p>  Ces param√®tres r√©solvent compl√®tement le probl√®me. </p><br><h2 id="9-zapusk-periodicheskih-zadach">  9. D√©marrage de t√¢ches p√©riodiques </h2><br><p>  <strong>Solution: les services p√©riodiques Nomade peuvent √™tre utilis√©s, mais cron est plus pratique pour le support.</strong> </p><br><p>  Nomad a la possibilit√© de lancer p√©riodiquement le service. </p><br><p>  Le seul avantage est la simplicit√© de cette configuration. </p><br><p>  Le premier inconv√©nient est que si le service d√©marre fr√©quemment, il jonchera la liste des t√¢ches.  Par exemple, au d√©marrage toutes les 5 minutes, 12 t√¢ches suppl√©mentaires seront ajout√©es √† la liste toutes les heures, jusqu'√† ce que le GC Nomad se d√©clenche, ce qui supprimera les anciennes t√¢ches. </p><br><p>  Le deuxi√®me inconv√©nient - on ne sait pas comment configurer correctement la surveillance d'un tel service.  Comment comprendre qu'un service commence, remplit et fait son travail jusqu'√† la fin? </p><br><p>  En cons√©quence, pour moi, je suis venu √† la mise en ≈ìuvre "cron" des t√¢ches p√©riodiques: </p><br><ol><li>  Il peut s'agir d'un cron r√©gulier dans un conteneur en fonctionnement constant.  Cron ex√©cute p√©riodiquement un certain script.  Un script-healthcheck est facilement ajout√© √† un tel conteneur, qui v√©rifie tout indicateur qui cr√©e un script en cours d'ex√©cution. </li><li>  Il peut s'agir d'un conteneur en cours d'ex√©cution, avec un service en cours d'ex√©cution.  Un lancement p√©riodique a d√©j√† √©t√© mis en place au sein du service.  Un script-healthcheck ou http-healthcheck similaire peut √™tre facilement ajout√© √† un tel service, qui v√©rifie imm√©diatement l'√©tat par ses "√©l√©ments internes". </li></ol><br><p>  Pour le moment, j'√©cris la plupart du temps en Go, respectivement, je pr√©f√®re la deuxi√®me option avec http healthcheck - on Go et le lancement p√©riodique, et http healthcheck'i sont ajout√©s avec quelques lignes de code. </p><br><h2 id="10-obespechenie-rezervirovaniya-servisov">  10. Fournir des services redondants </h2><br><p>  <strong>Solution: il n'y a pas de solution simple.</strong>  <strong>Il existe deux options plus difficiles.</strong> </p><br><p>  Le sch√©ma d'approvisionnement fourni par les d√©veloppeurs de Nomad doit prendre en charge le nombre de services en cours d'ex√©cution.  Vous dites que le nomade "lance-moi 5 instances du service" et il les d√©marre quelque part l√†-bas.  Il n'y a aucun contr√¥le sur la distribution.  Les instances peuvent s'ex√©cuter sur le m√™me serveur. </p><br><p>  Si le serveur tombe en panne, les instances sont transf√©r√©es vers d'autres serveurs.  Pendant le transfert des instances, le service ne fonctionne pas.  Il s'agit d'une mauvaise option de provision de r√©serve. </p><br><p>  Nous le faisons bien: </p><br><ol><li>  Nous distribuons des instances sur des serveurs via des h√¥tes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distincts</a> . </li><li>  Nous distribuons des instances dans les centres de donn√©es.  Malheureusement, seulement en cr√©ant une copie du script du formulaire service1, service2 avec le m√™me contenu, des noms diff√©rents et une indication du lancement dans diff√©rents centres de donn√©es. </li></ol><br><p>  Dans Nomad 0.9, une fonctionnalit√© appara√Ætra qui r√©soudra ce probl√®me: il sera possible de r√©partir les services dans un rapport de pourcentage entre les serveurs et les centres de donn√©es. </p><br><h2 id="11-web-ui-nomad">  11. Web UI Nomad </h2><br><p>  <strong>Solution: l'interface utilisateur int√©gr√©e est terrible, hashi-ui est magnifique.</strong> </p><br><p>  Le client de la console ex√©cute la plupart des fonctionnalit√©s requises, mais parfois vous voulez voir les graphiques, appuyez sur les boutons ... </p><br><p>  Nomad a une interface utilisateur int√©gr√©e.  Ce n'est pas tr√®s pratique (encore pire que la console). </p><br><p><img src="https://habrastorage.org/webt/0s/fv/6y/0sfv6yrspbj5easwnweyx8yzsme.png"></p><br><p>  La seule alternative que je connaisse est le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">hashi-ui</a> . </p><br><p><img src="https://habrastorage.org/webt/vd/4x/rv/vd4xrvrrnewmotnnio-yxbvriis.png"></p><br><p>  En fait, maintenant, j'ai personnellement besoin du client de console uniquement pour "run nomade".  Et m√™me cela pr√©voit de transf√©rer √† CI. </p><br><h2 id="12-podderzhka-oversubscription-po-pamyati">  12. Prise en charge de la sursouscription de la m√©moire </h2><br><p>  <strong>Solution: non.</strong> </p><br><p>  Dans la version actuelle de Nomad, vous devez sp√©cifier une limite de m√©moire stricte pour le service.  Si la limite est d√©pass√©e, le service sera tu√© par OOM Killer. </p><br><p>  La sursouscription est le moment o√π les limites d'un service peuvent √™tre sp√©cifi√©es "de et vers".  Certains services n√©cessitent plus de m√©moire au d√©marrage qu'en fonctionnement normal.  Certains services peuvent consommer plus de m√©moire que d'habitude pendant une courte p√©riode. </p><br><p>  Le choix d'une restriction stricte ou d'un logiciel est un sujet de discussion, mais, par exemple, Kubernetes permet au programmeur de faire un choix.  Malheureusement, dans les versions actuelles de Nomad, cette possibilit√© n'existe pas.  J'avoue que cela appara√Ætra dans les futures versions. </p><br><h2 id="13-ochistka-servera-ot-servisov-nomad">  13. Nettoyage du serveur des services Nomad </h2><br><p>  <strong>Solution:</strong> </p><br><pre> <code class="plaintext hljs">sudo systemctl stop nomad mount | fgrep alloc | awk '{print $3}' | xargs -I QQ sudo umount QQ sudo rm -rf /var/lib/nomad sudo docker ps | grep -v '(-1|-2|...)' | fgrep -v IMAGE | awk '{print $1}' | xargs -I QQ sudo docker stop QQ sudo systemctl start nomad</code> </pre> <br><p>  Parfois "quelque chose ne va pas".  Sur le serveur, il tue le n≈ìud d'agent et refuse de d√©marrer.  Ou le n≈ìud d'agent cesse de r√©pondre.  Ou le n≈ìud d'agent "perd" des services sur ce serveur. <br>  Cela arrivait parfois avec les anciennes versions de Nomad, maintenant cela ne se produit pas, ou tr√®s rarement. </p><br><p>  Dans ce cas, quelle est la plus simple √† faire, √©tant donn√© que le serveur de vidange ne produira pas le r√©sultat souhait√©?  Nous nettoyons le serveur manuellement: </p><br><ol><li>  Arr√™tez l'agent nomade. </li><li>  Faites un d√©montage sur le support qu'il cr√©e. </li><li>  Supprimez toutes les donn√©es de l'agent. </li><li>  Nous supprimons tous les conteneurs en filtrant les conteneurs de service (le cas √©ch√©ant). </li><li>  Nous d√©marrons l'agent. </li></ol><br><h2 id="14-kak-luchshe-razvorachivat-nomad">  14. Quelle est la meilleure fa√ßon de d√©ployer Nomad? </h2><br><p>  <strong>Solution: bien s√ªr, par le biais du Consul.</strong> </p><br><p>  Le consul dans ce cas n'est en aucun cas une couche suppl√©mentaire, mais un service qui s'int√®gre organiquement dans l'infrastructure, ce qui donne plus d'avantages que d'inconv√©nients: DNS, stockage KV, recherche de services, surveillance de la disponibilit√© du service, possibilit√© d'√©changer des informations en toute s√©curit√©. </p><br><p>  De plus, il se d√©roule aussi facilement que Nomad lui-m√™me. </p><br><h2 id="15-chto-luchshe---nomad-ili-kubernetes">  15. Quel est le meilleur - Nomad ou Kubernetes? </h2><br><p>  <strong>Solution: d√©pend de ...</strong> </p><br><p>  Auparavant, j'avais parfois l'id√©e de commencer une migration vers Kubernetes - j'√©tais tellement ennuy√© par le red√©marrage p√©riodique et spontan√© des services (voir probl√®me num√©ro 8).  Mais apr√®s une solution compl√®te au probl√®me, je peux dire: Nomad me convient en ce moment. </p><br><p>  D'autre part: Kubernetes a √©galement un rechargement semi-spontan√© de services - lorsque le planificateur Kubernetes redistribue les instances en fonction de la charge.  Ce n'est pas tr√®s cool, mais l√†, il est tr√®s probablement configur√©. </p><br><p>  Avantages de Nomad: l'infrastructure est tr√®s facile √† d√©ployer, des scripts simples, une bonne documentation, un support int√©gr√© pour Consul / Vault, ce qui donne √† son tour: une solution simple au probl√®me de stockage de mot de passe, DNS int√©gr√©, helchecks faciles √† configurer. </p><br><p>  Avantages de Kubernetes: Maintenant, c'est une ¬´norme de facto¬ª.  Bonne documentation, nombreuses solutions toutes faites, avec une bonne description et standardisation du lancement. </p><br><p>  Malheureusement, je n'ai pas la m√™me grande expertise dans Kubernetes pour r√©pondre sans √©quivoque √† la question - quoi utiliser pour le nouveau cluster.  D√©pend des besoins planifi√©s. <br>  Si vous avez pr√©vu beaucoup d'espaces de noms (probl√®me num√©ro 5) ou si vos services sp√©cifiques consomment beaucoup de m√©moire au d√©but, alors lib√©rez-les (probl√®me num√©ro 12) - certainement Kubernetes, car  ces deux probl√®mes dans Nomad ne sont pas enti√®rement r√©solus ou incommodes. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr435132/">https://habr.com/ru/post/fr435132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr435120/index.html">Fonctions lambda en SQL ... r√©fl√©chissons</a></li>
<li><a href="../fr435122/index.html">Comment la flamme a √©t√© impl√©ment√©e dans Doom sur la Playstation</a></li>
<li><a href="../fr435124/index.html">Chefs-d'≈ìuvre de la construction de colonnes du monde: un moniteur-transformateur de studio avec un nombre variable de bandes</a></li>
<li><a href="../fr435126/index.html">Exp√©rience dans l'organisation et la tenue de conf√©rences d'entreprise pour les analystes</a></li>
<li><a href="../fr435128/index.html">Pi-Sonos: un passe-temps incontr√¥lable</a></li>
<li><a href="../fr435134/index.html">Simplifiez l'utilisation des bases de donn√©es dans Qt avec QSqlRelationalTableModel</a></li>
<li><a href="../fr435136/index.html">Sergey et la m√©thode scientifique</a></li>
<li><a href="../fr435138/index.html">Comment prendre le contr√¥le de votre infrastructure r√©seau. Chapitre Trois S√©curit√© du r√©seau. Premi√®re partie</a></li>
<li><a href="../fr435142/index.html">Learning Trace Using eBPF: A Guide and Exemples</a></li>
<li><a href="../fr435144/index.html">Introduction √† Spring Boot: cr√©ation d'une API REST simple en Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>