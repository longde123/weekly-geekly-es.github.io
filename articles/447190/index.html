<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè¨ üôåüèæ üôÜüèæ Predicciones de matem√°ticos. Analizamos los principales m√©todos para detectar anomal√≠as. üë©‚Äçüè≠ üë®üèæ‚Äçüéì üõÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el extranjero, el uso de inteligencia artificial en la industria para el mantenimiento predictivo de varios sistemas est√° ganando popularidad. El p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Predicciones de matem√°ticos. Analizamos los principales m√©todos para detectar anomal√≠as.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lanit/blog/447190/">  En el extranjero, el uso de inteligencia artificial en la industria para el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mantenimiento predictivo de</a> varios sistemas est√° ganando popularidad.  El prop√≥sito de esta t√©cnica es identificar el mal funcionamiento de la operaci√≥n del sistema durante la fase de operaci√≥n antes de que se descomponga para una respuesta oportuna. <br><br>  ¬øCu√°n relevante es este enfoque en nuestro pa√≠s y en Occidente?  La conclusi√≥n puede hacerse, por ejemplo, en art√≠culos sobre Habr√© y en Medium.  Casi no hay art√≠culos sobre Habr√© sobre la resoluci√≥n de problemas de mantenimiento predictivo.  En Medium hay un conjunto completo.  Aqu√≠, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> se describe bien cu√°les son los objetivos y las ventajas de este enfoque. <br><br>  De este art√≠culo aprender√°s: <br><br><ul><li>  ¬øPor qu√© se necesita esta t√©cnica? </li><li>  qu√© enfoques de aprendizaje autom√°tico se usan m√°s com√∫nmente para el mantenimiento predictivo, </li><li>  c√≥mo prob√© uno de los trucos con un simple ejemplo. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/2a4/888/8452a4888db8d633dd14d426f6b80cbe.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><i>Fuente</i></a> <br><a name="habracut"></a><br>  ¬øQu√© caracter√≠sticas ofrece el servicio predictivo? <br><br><ul><li>  un proceso controlado de trabajos de reparaci√≥n, que se lleva a cabo seg√∫n sea necesario, ahorrando dinero y sin prisas, lo que mejora la calidad de estos trabajos; </li><li>  identificaci√≥n de un mal funcionamiento espec√≠fico en la operaci√≥n del equipo (la capacidad de comprar una pieza espec√≠fica para reemplazarla cuando el equipo est√° en funcionamiento brinda enormes ventajas); </li><li>  optimizaci√≥n del funcionamiento del equipo, cargas, etc. </li><li>  reducci√≥n de costos por apagado regular de equipos. </li></ul><br>  El <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">siguiente art√≠culo sobre Medium</a> describe bien las preguntas que deben responderse para comprender c√≥mo abordar este problema en un caso particular. <br><br>  Al recopilar datos o al elegir datos para construir un modelo, es importante responder tres grupos de preguntas: <br><br><ol><li>  ¬øSe pueden predecir todos los problemas del sistema?  ¬øQu√© predicci√≥n es especialmente importante? </li><li>  ¬øQu√© es un proceso de falla?  ¬øTodo el sistema deja de funcionar o solo est√° cambiando el modo operativo?  ¬øEs un proceso r√°pido, degradaci√≥n instant√°nea o gradual? </li><li>  ¬øEl rendimiento del sistema refleja adecuadamente su rendimiento?  ¬øSe relacionan con partes individuales del sistema o con el sistema en su conjunto? </li></ol><br>  Tambi√©n es importante comprender de antemano lo que desea predecir, lo que es posible predecir y lo que no. <br><br>  El art√≠culo en Medium tambi√©n enumera preguntas que ayudar√°n a determinar su objetivo espec√≠fico: <br><br><ul><li>  ¬øQu√© se debe predecir?  El tiempo de vida restante, comportamiento anormal o no, la probabilidad de falla en las pr√≥ximas N horas / d√≠as / semanas? </li><li>  ¬øHay suficientes datos hist√≥ricos? </li><li>  ¬øSe sabe cu√°ndo el sistema dio lecturas an√≥malas y cu√°ndo no?  ¬øEs posible marcar tales indicaciones? </li><li>  ¬øQu√© tan lejos debe ver el modelo?  ¬øQu√© tan independientes son las lecturas que reflejan el funcionamiento del sistema en el intervalo de una hora / d√≠a / semana? </li><li>  ¬øQu√© necesitas para optimizar?  Si el modelo detecta tantas violaciones como sea posible, mientras da una falsa alarma, o es suficiente para detectar varios eventos sin falsos positivos. </li></ul><br>  Se espera que la situaci√≥n mejore en el futuro.  Hasta ahora, existen dificultades en el campo del mantenimiento predictivo: hay pocos ejemplos de mal funcionamiento del sistema, o son suficientes momentos de mal funcionamiento del sistema, pero no est√°n marcados;  El proceso de falla es desconocido. <br><br>  La forma principal de superar las dificultades en el mantenimiento predictivo es utilizar <b>m√©todos de b√∫squeda de anomal√≠as</b> .  Dichos algoritmos no requieren marcado para el entrenamiento.  Para probar y depurar algoritmos, es necesario el marcado de una forma u otra.  Dichos m√©todos son limitados en el sentido de que no predecir√°n una falla espec√≠fica, sino que solo se√±alar√°n anormalidades de los indicadores. <br><br>  Pero esto ya no est√° mal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/847/46c/6d7/84746c6d7414722b0a1b7b322000f201.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><i>Fuente</i></a> <br><br><h2>  M√©todos </h2><br>  Ahora quiero hablar sobre algunas caracter√≠sticas de los enfoques de detecci√≥n de anomal√≠as, y luego juntos probaremos las capacidades de algunos algoritmos simples en la pr√°ctica. <br><br>  Aunque una situaci√≥n particular requerir√° probar varios algoritmos para buscar anomal√≠as y elegir la mejor, es posible determinar algunas ventajas y desventajas de las principales t√©cnicas utilizadas en esta √°rea. <br><br>  En primer lugar, es importante comprender de antemano cu√°l es el porcentaje de anomal√≠as en los datos. <br><br>  Si estamos hablando de una variaci√≥n del enfoque semi-supervisado (estudiamos solo con datos "normales" y trabajamos (probamos) y luego con datos con anomal√≠as), entonces la opci√≥n m√°s √≥ptima es <b>el m√©todo de vector de soporte con una clase ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SVM de una clase</a> )</b> .  Cuando se utilizan funciones de base radial como n√∫cleo, este algoritmo construye una superficie no lineal alrededor del origen.  Cuanto m√°s limpios sean los datos de entrenamiento, mejor funcionar√°. <br><br>  En otros casos, la necesidad de conocer la proporci√≥n de puntos anormales y "normales" tambi√©n permanece, para determinar el umbral de corte. <br><br>  Si el n√∫mero de anomal√≠as en los datos es superior al 5% y son bastante separables de la muestra principal, se pueden utilizar m√©todos de b√∫squeda de anomal√≠as est√°ndar. <br><br>  En este caso, el <b>m√©todo del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bosque de aislamiento</a></b> es el m√°s estable en t√©rminos de calidad: el <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bosque de aislamiento son</a></b> datos aleatorios.  Es m√°s probable que una indicaci√≥n m√°s caracter√≠stica sea m√°s profunda, mientras que los indicadores inusuales se separar√°n del resto de la muestra en las primeras iteraciones. <br><br>  Otros algoritmos funcionan mejor si se "ajustan" a los detalles de los datos. <br><br>  Cuando los datos tienen una distribuci√≥n normal, el <b>m√©todo de envoltura el√≠ptica</b> es adecuado, aproximando los datos con una distribuci√≥n normal multidimensional.  Cuanto menos probable sea que el punto pertenezca a la distribuci√≥n, mayor ser√° la probabilidad de que sea an√≥malo. <br><br>  Si los datos se presentan de tal manera que la posici√≥n relativa de diferentes puntos refleja bien sus diferencias, entonces los m√©todos m√©tricos parecen ser una buena opci√≥n: por ejemplo, <b>k vecinos m√°s cercanos, k-√©simo vecino m√°s cercano, ABOD (detecci√≥n de valores at√≠picos basados ‚Äã‚Äãen √°ngulos) o LOF (factor de valores at√≠picos locales) )</b> <br><br>  Todos estos m√©todos sugieren que los indicadores "correctos" se concentran en un √°rea del espacio multidimensional.  Si entre los vecinos m√°s cercanos k (o k-√©simo) todo est√° lejos del objetivo, entonces el punto es una anomal√≠a.  Para ABOD, el razonamiento es similar: si todos los k puntos m√°s cercanos est√°n en el mismo sector de espacio en relaci√≥n con el considerado, entonces el punto es una anomal√≠a.  Para LOF: si la densidad local (predeterminada para cada punto por k vecinos m√°s cercanos) es menor que la de k vecinos m√°s cercanos, entonces el punto es una anomal√≠a. <br><br>  Si los datos est√°n bien agrupados, los <b>m√©todos basados ‚Äã‚Äãen el an√°lisis de grupos</b> son una buena opci√≥n.  Si el punto es equidistante de los centros de varios grupos, entonces es an√≥malo. <br><br>  Si las direcciones de la mayor variaci√≥n en la varianza se distinguen bien en los datos, entonces parece ser una buena opci√≥n <b>buscar anomal√≠as basadas en el m√©todo del componente principal</b> .  En este caso, las desviaciones del valor promedio para n1 (la mayor√≠a de los componentes "principales") y n2 (la menor "principal") se consideran una medida de anomal√≠a. <br><br>  Por ejemplo, se sugiere mirar el conjunto de datos de <b>The Prognostics and Health Management Society (PHM Society)</b> .  Esta organizaci√≥n sin fines de lucro organiza la competencia todos los a√±os.  En 2018, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se requer√≠a predecir errores en la operaci√≥n y el tiempo antes de la falla de la planta de grabado de haz de iones</a> .  Tomaremos el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conjunto de datos para 2015</a> .  Contiene las lecturas de varios sensores para 30 instalaciones (muestra de entrenamiento), y se requiere para predecir cu√°ndo y qu√© error ocurrir√°. <br><br>  No encontr√© las respuestas para la muestra de prueba en la red, por lo que solo jugaremos con la capacitaci√≥n. <br><br>  En general, todos los ajustes son similares, pero difieren, por ejemplo, en el n√∫mero de componentes, en el n√∫mero de anomal√≠as, etc.  Por lo tanto, aprender en los primeros 20 y probar en otros no tiene mucho sentido. <br><br>  Por lo tanto, elegiremos una de las instalaciones, la cargaremos y echaremos un vistazo a estos datos.  El art√≠culo no se referir√° a la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ingenier√≠a de caracter√≠sticas</a> , por lo que no analizaremos mucho. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.covariance <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EllipticEnvelope <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LocalOutlierFactor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> IsolationForest <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OneClassSVM dfa=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12a.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Component number'</span></span>,<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4ref'</span></span>]) dfa.head(<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/301/77e/2bb/30177e2bb970e82495b18008b44d7ea4.jpg"></div><br>  Como puede ver, hay siete componentes para cada uno de los cuales hay lecturas de cuatro sensores que se toman cada 15 minutos.  S1ref-S4ref en la descripci√≥n de la competencia se enumeran como valores de referencia, pero los valores son muy diferentes de las lecturas de los sensores.  Para no perder el tiempo pensando en lo que significan, los eliminamos.  Si observa la distribuci√≥n de valores para cada caracter√≠stica (S1-S4), resulta que las distribuciones son continuas para S1, S2 y S4, y discretas para S3.  Adem√°s, si observa la distribuci√≥n conjunta de S2 y S4, resulta que son inversamente proporcionales. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/69b/82b/cde/69b82bcdecdfca69128d88673b0518b4.jpg"></div><br>  Aunque una desviaci√≥n de una dependencia directa puede indicar un error, no lo comprobaremos, sino que simplemente eliminaremos S4. <br><br>  Una vez m√°s, procesamos el conjunto de datos.  Deje S1, S2 y S3.  Escale S1 y S2 con StandardScaler (restamos el promedio y dividimos por la desviaci√≥n est√°ndar), traduzca S3 en OHE (One Hot Encoding).  Cosemos lecturas de todos los componentes de instalaci√≥n en una l√≠nea.  Total de 89 caracter√≠sticas.  2 * 7 = 14 - lecturas S1 y S2 para 7 componentes y 75 valores √∫nicos de R3.  Solo 56 mil de esas l√≠neas. <br><br>  Sube el archivo con errores. <br><br><pre> <code class="python hljs">dfc=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12c.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>, <span class="hljs-string"><span class="hljs-string">'End Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>]) dfc.head()</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/419/e50/3ad419e50ac870a71c153b523a22bfed.jpg"></div><br>  Antes de probar estos algoritmos en nuestro conjunto de datos, me permitir√© otra peque√±a digresi√≥n.  Necesitas ser probado.  Para esto, se propone tomar la hora de inicio del error y la hora de finalizaci√≥n.  Y todas las indicaciones dentro de este intervalo se consideran anormales y fuera de lo normal.  Este enfoque tiene muchas desventajas.  Pero especialmente uno: el comportamiento anormal probablemente ocurre antes de que se solucione el error.  Por fidelidad, cambiemos la ventana de anomal√≠as hace media hora a tiempo.  Evaluaremos la medida F1, la precisi√≥n y el recuerdo. <br><br>  El c√≥digo para distinguir caracter√≠sticas y determinar la calidad del modelo: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_and_preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(plant_num)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-comment"><span class="hljs-comment">#      ,       dfa=pd.read_csv('plant_{}a.csv'.format(plant_num),names=['Component number','Time','S1','S2','S3','S4','S1ref','S2ref','S3ref','S4ref'])   dfc=pd.read_csv('plant_{}c.csv'.format(plant_num),names=['Start Time','End Time','Type']).drop(0,axis=0)   N_comp=len(dfa['Component number'].unique())   #  15    dfa['Time']=pd.to_datetime(dfa['Time']).dt.round('15min')   #  6    (  ,    )   dfc=dfc[dfc['Type']!=6]   dfc['Start Time']=pd.to_datetime(dfc['Start Time'])   dfc['End Time']=pd.to_datetime(dfc['End Time'])   #      ,       OHE  3-    dfa=pd.concat([dfa.groupby('Time').nth(i)[['S1','S2','S3']].rename(columns={"S1":"S1_{}".format(i),"S2":"S2_{}".format(i),"S3":"S3_{}".format(i)}) for i in range(N_comp)],axis=1).dropna().reset_index()   for k in range(N_comp):       dfa=pd.concat([dfa.drop('S3_'+str(k),axis=1),pd.get_dummies(dfa['S3_'+str(k)],prefix='S3_'+str(k))],axis=1).reset_index(drop=True)   #          df_train,df_test=train_test_split(dfa,test_size=0.25,shuffle=False)   cols_to_scale=df_train.filter(regex='S[1,2]').columns   scaler=preprocessing.StandardScaler().fit(df_train[cols_to_scale])   df_train[cols_to_scale]=scaler.transform(df_train[cols_to_scale])   df_test[cols_to_scale]=scaler.transform(df_test[cols_to_scale])   return df_train,df_test,dfc #       def get_true_labels(measure_times,dfc,shift_delta):   idxSet=set()   dfc['Start Time']-=pd.Timedelta(minutes=shift_delta)   dfc['End Time']-=pd.Timedelta(minutes=shift_delta)   for idx,mes_time in tqdm_notebook(enumerate(measure_times),total=measure_times.shape[0]):       intersect=np.array(dfc['Start Time']&lt;mes_time).astype(int)*np.array(dfc['End Time']&gt;mes_time).astype(int)       idxs=np.where(intersect)[0]       if idxs.shape[0]:           idxSet.add(idx)   dfc['Start Time']+=pd.Timedelta(minutes=shift_delta)   dfc['End Time']+=pd.Timedelta(minutes=shift_delta)   true_labels=pd.Series(index=measure_times.index)   true_labels.iloc[list(idxSet)]=1   true_labels.fillna(0,inplace=True)   return true_labels #          def check_model(model,df_train,df_test,filt='S[123]'):   model.fit(df_train.drop('Time',axis=1).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))).map({-1:1,1:0})   print('F1 score: {:.3f}'.format(f1_score(df_test['Label'],y_preds)))   print('Precision score: {:.3f}'.format(precision_score(df_test['Label'],y_preds)))   print('Recall score: {:.3f}'.format(recall_score(df_test['Label'],y_preds)))   score = model.decision_function(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))   sns.distplot(score[df_test['Label']==0])   sns.distplot(score[df_test['Label']==1]) df_train,df_test,anomaly_times=load_and_preprocess(12) df_test['Label']=get_true_labels(df_test['Time'],dfc,30)</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/50b/6f2/be9/50b6f2be9c3220853e92461805030e0b.jpg"></div>  <i>Resultados de la prueba para algoritmos de b√∫squeda de anomal√≠as simples en el conjunto de datos PHM 2015 Data Challenge</i> <br><br>  De vuelta a los algoritmos.  Probemos One Class SVM (OCSVM), IsolationForest (IF), EllipticEnvelope (EE) y LocalOutlierFactor (LOF) en nuestros datos.  Para empezar, no estableceremos ning√∫n par√°metro.  Observo que LOF puede funcionar en dos modos.  Si novedad = Falso puede buscar anomal√≠as solo en el conjunto de entrenamiento (solo hay fit_predict), si es Verdadero, est√° dirigido a buscar anomal√≠as fuera del conjunto de entrenamiento (puede ajustarse y predecir por separado).  IF tiene un modo de comportamiento antiguo y nuevo.  Usamos nuevo.  √âl da mejores resultados. <br><br>  OCSVM detecta bien las anomal√≠as, pero hay demasiados falsos positivos.  Para otros m√©todos, el resultado es a√∫n peor. <br><br>  Pero supongamos que conocemos el porcentaje de anomal√≠as en los datos.  En nuestro caso, 27%.  OCSVM tiene nu: la estimaci√≥n superior para el porcentaje de errores y la inferior para el porcentaje de vectores de soporte.  Otros m√©todos de contaminaci√≥n tienen un porcentaje de errores de datos.  En los m√©todos IF y LOF, se determina autom√°ticamente, mientras que para OCSVM y EE se establece en 0.1 de forma predeterminada.  Intentemos establecer la contaminaci√≥n (nu) en 0.27.  Ahora el mejor resultado para EE. <br><br>  C√≥digo para verificar modelos: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">check_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model,df_train,df_test,filt=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'S[123]'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   model_type,model = model   model.fit(df_train.drop(<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))).map({<span class="hljs-number"><span class="hljs-number">-1</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>})   print(<span class="hljs-string"><span class="hljs-string">'F1 score for {}: {:.3f}'</span></span>.format(model_type,f1_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Precision score for {}: {:.3f}'</span></span>.format(model_type,precision_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Recall score for {}: {:.3f}'</span></span>.format(model_type,recall_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   score = model.decision_function(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>])   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>])   plt.title(<span class="hljs-string"><span class="hljs-string">'Decision score distribution for {}'</span></span>.format(model_type))   plt.show()</code> </pre> <br>  Es interesante observar la distribuci√≥n de los indicadores de anomal√≠as para diferentes m√©todos.  Se puede ver que LOF no funciona bien para estos datos.  EE tiene puntos que el algoritmo considera extremadamente anormales.  Sin embargo, los puntos normales caen all√≠.  IsoFor y OCSVM muestran que la elecci√≥n del umbral de corte (contaminaci√≥n / nu) es importante, lo que cambiar√° la compensaci√≥n entre precisi√≥n e integridad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9d/0c8/80f/a9d0c880f36fe37f1de6c9290b8cccc7.png"></div><br>  Es l√≥gico que las lecturas de los sensores tengan una distribuci√≥n cercana a la normal, cerca de valores estacionarios.  Si realmente tenemos una muestra de prueba etiquetada, y preferiblemente tambi√©n una de validaci√≥n, entonces el valor de contaminaci√≥n puede te√±irse.  La siguiente pregunta es, ¬øqu√© errores est√°n m√°s orientados: falso positivo o falso negativo? <br><br>  El resultado LOF es muy bajo.  No muy impresionante  Pero recuerde que las variables OHE van a la entrada junto con las variables transformadas por StandardScaler.  Y las distancias predeterminadas son euclidianas.  Pero si solo cuenta las variables de acuerdo con S1 y S2, la situaci√≥n se corrige y el resultado es comparable con otros m√©todos.  Sin embargo, es importante comprender que uno de los par√°metros clave de los clasificadores m√©tricos enumerados es el n√∫mero de vecinos.  Afecta significativamente la calidad y debe ajustarse.  La m√©trica de distancia en s√≠ tambi√©n ser√≠a agradable de recoger. <br><br>  Ahora intenta combinar los dos modelos.  Al comienzo de uno, eliminamos las anomal√≠as del conjunto de entrenamiento.  Y luego entrenaremos a OCSVM en un conjunto de entrenamiento "m√°s limpio".  Seg√∫n los resultados anteriores, observamos la mayor integridad en EE.  Borramos la muestra de entrenamiento a trav√©s de EE, entrenamos OCSVM en ella y obtenemos F1 = 0.50, Precisi√≥n = 0.34, integridad = 0.95.  No es impresionante  Pero acabamos de preguntar nu = 0.27.  Y los datos que tenemos son m√°s o menos "limpios".  Si suponemos que la plenitud de la EE en la muestra de entrenamiento es la misma, entonces permanecer√° el 5% de los errores.  Nos establecemos como nu y obtenemos F1 = 0.69, Precisi√≥n = 0.59, integridad = 0.82.  Genial  Es importante tener en cuenta que en otros m√©todos dicha combinaci√≥n no funcionar√°, ya que implican que el n√∫mero de anomal√≠as en el conjunto de entrenamiento y el n√∫mero de prueba es el mismo.  Al entrenar estos m√©todos en un conjunto de datos de entrenamiento puro, deber√° especificar menos contaminaci√≥n que en los datos reales y no cerca de cero, pero es mejor seleccionarlo para la validaci√≥n cruzada. <br><br>  Es interesante observar el resultado de la b√∫squeda en la secuencia de indicaciones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/0c5/26d/2a20c526d944b55116f3fcd6af064eab.png"></div><br>  La figura muestra un segmento de las lecturas del primer y segundo sensor para 7 componentes.  En la leyenda, el color de los errores correspondientes (el principio y el final se muestran mediante l√≠neas verticales del mismo color).  Los puntos indican las predicciones: verde - predicciones verdaderas, rojo - falso positivo, p√∫rpura - falso negativo.  De la figura se puede ver que es dif√≠cil determinar visualmente el tiempo de error, y el algoritmo hace frente a esta tarea bastante bien.  Aunque es importante entender que las lecturas del tercer sensor no se dan aqu√≠.  Adem√°s, hay lecturas de falsos positivos despu√©s del final del error.  Es decir  el algoritmo ve que tambi√©n hay valores err√≥neos y marcamos esta √°rea como libre de errores.  El lado derecho de la figura muestra el √°rea antes del error, que marcamos como err√≥nea (media hora antes del error), que se reconoci√≥ como libre de errores, lo que conduce a errores de modelo falso negativo.  En el centro de la figura, se reconoce una pieza coherente, reconocida como un error.  La conclusi√≥n se puede extraer de la siguiente manera: al resolver el problema de la b√∫squeda de anomal√≠as, debe interactuar estrechamente con los ingenieros que entienden la esencia de los sistemas cuya salida necesita predecir, ya que verificar los algoritmos utilizados en el marcado no refleja completamente la realidad y no simula las condiciones en las que dichos algoritmos podr√≠an ser utilizado <br><br>  C√≥digo para trazar el gr√°fico: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_time_course</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df_test,dfc,y_preds,start,end,vert_shift=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))   cols=df_train.filter(regex=(<span class="hljs-string"><span class="hljs-string">'S[12]'</span></span>)).columns   add=<span class="hljs-number"><span class="hljs-number">0</span></span>   preds_idx=y_preds.iloc[start:end][y_preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   true_idx=df_test.iloc[start:end,:][df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   tp_idx=set(true_idx.values).intersection(set(preds_idx.values))   fn_idx=set(true_idx.values).difference(set(preds_idx.values))   fp_idx=set(preds_idx.values).difference(set(true_idx.values))   xtime=df_test[<span class="hljs-string"><span class="hljs-string">'Time'</span></span>].iloc[start:end]   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> cols:       plt.plot(xtime,df_test[col].iloc[start:end]+add)       plt.scatter(xtime.loc[tp_idx].values,df_test.loc[tp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>)       plt.scatter(xtime.loc[fn_idx].values,df_test.loc[fn_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'violet'</span></span>)       plt.scatter(xtime.loc[fp_idx].values,df_test.loc[fp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>)       add+=vert_shift   failures=dfc[(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&gt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">0</span></span>])&amp;(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&lt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">-1</span></span>])]   unique_fails=np.sort(failures[<span class="hljs-string"><span class="hljs-string">'Type'</span></span>].unique())   colors=np.array([np.random.rand(<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> unique_fails])   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> failures.index:       c=colors[np.where(unique_fails==failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>]][<span class="hljs-number"><span class="hljs-number">0</span></span>]       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>],color=c)       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'End Time'</span></span>],color=c)   leg=plt.legend(unique_fails)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(unique_fails)):       leg.legendHandles[i].set_color(colors[i])</code> </pre> <br>  Si el porcentaje de anomal√≠as es inferior al 5% y / o est√°n mal separados de los indicadores "normales", los m√©todos anteriores funcionan mal y vale la pena usar algoritmos basados ‚Äã‚Äãen redes neuronales.  En el caso m√°s simple, estos ser√≠an: <br><br><ul><li>  codificadores autom√°ticos (un error alto de un codificador autom√°tico capacitado indicar√° una anomal√≠a en la lectura); </li><li>  redes recurrentes (aprender por secuencia para predecir la √∫ltima lectura. Si la diferencia es grande, el punto es anormal). </li></ul><br>  Por separado, vale la pena se√±alar los detalles de trabajar con series de tiempo.  Es importante comprender que la mayor√≠a de los algoritmos anteriores (excepto los autocodificadores y el aislamiento de bosques) probablemente dar√°n una peor calidad al agregar caracter√≠sticas de retraso (lecturas de puntos anteriores en el tiempo). <br><br>  Intentemos agregar caracter√≠sticas de retraso en nuestro ejemplo.  La descripci√≥n de la competencia dice que los valores 3 horas antes del error no est√°n relacionados con el error de ninguna manera.  Luego agregue las se√±ales en 3 horas.  Total 259 signos. <br><br>  Como resultado, los resultados para OCSVM y IsolationForest se mantuvieron casi sin cambios, mientras que los de Elliptic Envelope y LOF cayeron. <br><br>  Para usar informaci√≥n sobre la din√°mica del sistema, se deben usar codificadores autom√°ticos con redes neuronales recurrentes o convolucionales.  O, por ejemplo, una combinaci√≥n de codificadores autom√°ticos, informaci√≥n de compresi√≥n y enfoques convencionales para buscar anomal√≠as basadas en informaci√≥n comprimida.  El enfoque inverso tambi√©n parece prometedor.  Detecci√≥n primaria de los puntos m√°s poco caracter√≠sticos por algoritmos est√°ndar, y luego entrenando el codificador autom√°tico ya en datos m√°s limpios. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/340/9e8/dbc/3409e8dbcc5e9bcac2a1acb08bc7d146.jpg"></div>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><i>Fuente</i></a> <br><br>  Hay un conjunto de t√©cnicas para trabajar con series de tiempo unidimensionales.  Todos ellos est√°n destinados a predecir lecturas futuras, y los puntos que divergen de la predicci√≥n se consideran anomal√≠as. <br><br><h2>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Modelo Holt-Winters</a> </h2><br>  El suavizado exponencial triple divide la serie en 3 componentes: nivel, tendencia y estacionalidad.  En consecuencia, si la serie se presenta de esta forma, el m√©todo funciona bien.  Facebook Prophet opera con un principio similar, pero eval√∫a los componentes en s√≠ mismos de una manera diferente.  Se pueden leer m√°s detalles, por ejemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br><h2>  S (ARIMA) </h2><br>  En este m√©todo, el modelo predictivo se basa en la autorregresi√≥n y la media m√≥vil.  Si estamos hablando de la expansi√≥n de S (ARIMA), entonces nos permite evaluar la estacionalidad.  Lea m√°s sobre el enfoque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br><h2>  Otros enfoques de servicio predictivo </h2><br>  Cuando se trata de series de tiempo y hay informaci√≥n sobre los tiempos de ocurrencia de errores, puede aplicar m√©todos de ense√±anza con un maestro.  Adem√°s de la necesidad de datos etiquetados, en este caso es importante comprender que la predicci√≥n del error depender√° de la naturaleza del error.  Si hay muchos errores y de una naturaleza diferente, lo m√°s probable es que sea necesario predecir cada uno por separado, lo que requerir√° a√∫n m√°s datos etiquetados, pero las perspectivas ser√°n m√°s atractivas. <br><br>  Existen formas alternativas de utilizar el aprendizaje autom√°tico en el mantenimiento predictivo.  Por ejemplo, prediciendo una falla del sistema en los pr√≥ximos N d√≠as (tarea de clasificaci√≥n).  Es importante comprender que este enfoque requiere que la ocurrencia de un error en la operaci√≥n del sistema est√© precedida por un per√≠odo de degradaci√≥n (no necesariamente gradual).  En este caso, el enfoque m√°s exitoso parece ser el uso de redes neuronales con capas convolucionales y / o recurrentes.  Por separado, vale la pena se√±alar los m√©todos para aumentar las series de tiempo.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Dos enfoques</a> me parecen los m√°s interesantes y al mismo tiempo simples: <br><br><ul><li>  Se selecciona la parte continua de la fila (por ejemplo, 70% y se elimina el resto) y se estira al tama√±o original </li><li>  Se selecciona y estira o comprime una parte continua de la fila (por ejemplo, 20%).  Despu√©s de eso, toda la fila se comprime o estira en consecuencia a su tama√±o original. </li></ul><br>  Tambi√©n hay una opci√≥n para predecir la vida √∫til restante del sistema (tarea de regresi√≥n).  Aqu√≠ podemos distinguir un enfoque separado: la predicci√≥n no es de la vida √∫til, sino de los par√°metros de distribuci√≥n de Weibull. <br><br>  Puede leer sobre la distribuci√≥n en s√≠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> sobre su uso junto con mallas recurrentes.  Esta distribuci√≥n tiene dos par√°metros Œ± y Œ≤.  Œ± indica cu√°ndo ocurrir√° el evento, y Œ≤ indica qu√© tan seguro es el algoritmo.  Aunque la aplicaci√≥n de este enfoque es prometedora, en este caso surgen dificultades para entrenar la red neuronal, ya que al principio es m√°s f√°cil que el algoritmo sea inseguro que predecir una vida √∫til adecuada. <br><br>  Por separado, vale la pena se√±alar la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">regresi√≥n de Cox</a> .  Le permite predecir la tolerancia a fallas del sistema para cada punto en el tiempo despu√©s del diagn√≥stico, present√°ndolo como un producto de dos funciones.  Una funci√≥n es la degradaci√≥n del sistema, independientemente de sus par√°metros, es decir.  com√∫n a cualquiera de estos sistemas.  Y el segundo es una dependencia exponencial de los par√°metros de un sistema particular.  Entonces, para una persona, hay una funci√≥n com√∫n asociada con el envejecimiento, m√°s o menos la misma para todos.  Pero el deterioro de la salud tambi√©n se asocia con el estado de los √≥rganos internos, que es diferente para todos. <br><br>  Espero que ahora sepa un poco m√°s sobre el mantenimiento predictivo.  Estoy seguro de que tendr√° preguntas sobre los m√©todos de aprendizaje autom√°tico que se utilizan con mayor frecuencia para esta tecnolog√≠a.  Estar√© encantado de responder a cada uno de ellos en los comentarios.  Si est√° interesado en no solo preguntar sobre lo que est√° escrito, sino que quiere hacer algo similar, nuestro equipo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CleverDATA</a> siempre est√° encantado de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">contar</a> con profesionales talentosos y entusiastas. <br><br><div class="spoiler">  <b class="spoiler_title">¬øHay vacantes?</b>  <b class="spoiler_title">Por supuesto!</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Desarrollador Java (Big Data)</a> </li></ul></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/447190/">https://habr.com/ru/post/447190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447178/index.html">5 oportunidades efectivas para usar la tecnolog√≠a de miner√≠a de procesos</a></li>
<li><a href="../447180/index.html">Descripci√≥n general y comparaci√≥n de controladores de entrada para Kubernetes</a></li>
<li><a href="../447182/index.html">Sistemas operativos: tres piezas f√°ciles. Parte 3: API de proceso (traducci√≥n)</a></li>
<li><a href="../447184/index.html">¬øQu√© es la Oferta de intercambio inicial (IEO) y en qu√© se diferencia de ICO?</a></li>
<li><a href="../447186/index.html">C√≥mo lanzar un prototipo de ML en un d√≠a. Informe Yandex.Taxi</a></li>
<li><a href="../447192/index.html">¬øQu√© papel puede jugar la tecnolog√≠a en el antiguo arte de mezclar especias?</a></li>
<li><a href="../447194/index.html">Funciones de renderizado en Metro: trazado de rayos Exodus c</a></li>
<li><a href="../447196/index.html">7. Check Point Getting Started R80.20. Control de acceso</a></li>
<li><a href="../447198/index.html">Misi√≥n lunar "Bereshit": aterrizaje-accidente-ca√≠da en la luna</a></li>
<li><a href="../447204/index.html">17 de abril: Conferencia abierta "El camino del desarrollador del juego: desde la idea hasta el lanzamiento" y una biblioteca de juegos en la Escuela Superior de Derecho</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>