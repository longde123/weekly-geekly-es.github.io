<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👷🏼 👨🏼‍💼 🏵️ Penyimpanan Tepercaya dengan DRBD9 dan Proxmox (Bagian 1: NFS) 👾 🧖🏻 🧥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Mungkin setiap orang yang setidaknya pernah dibuat bingung oleh pencarian penyimpanan berdefinisi perangkat lunak berkinerja tinggi, cepat atau lambat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penyimpanan Tepercaya dengan DRBD9 dan Proxmox (Bagian 1: NFS)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417473/"><p><img src="https://habrastorage.org/getpro/habr/post_images/f6a/771/043/f6a7710433c8887d6bbbe4792cc178e1.jpg" alt="gambar"></p><br><p>  Mungkin setiap orang yang setidaknya pernah dibuat bingung oleh pencarian <strong>penyimpanan berdefinisi perangkat lunak berkinerja tinggi,</strong> cepat atau lambat, mendengar tentang <strong>DRBD</strong> , atau bahkan mungkin menanganinya. </p><br><p> Benar, di puncak popularitas <strong>Ceph</strong> dan <strong>GlusterFS</strong> , yang pada prinsipnya bekerja dengan sangat baik, dan yang paling penting langsung di luar kotak, semua orang hanya lupa sedikit tentang itu.  Selain itu, versi sebelumnya tidak mendukung replikasi ke lebih dari dua node, dan karena ini, masalah dengan <strong>split-brain</strong> sering dijumpai, yang jelas tidak menambah popularitasnya. </p><br><p>  Solusinya benar-benar bukan hal baru, tetapi cukup kompetitif.  Dengan biaya yang relatif rendah untuk CPU dan RAM, <strong>DRBD</strong> menyediakan sinkronisasi yang sangat cepat dan aman di tingkat <strong>perangkat blok</strong> .  Selama ini, pengembang LINBIT - DRBD tidak diam dan terus-menerus memperbaikinya.  Dimulai dengan versi <strong>DRBD9</strong> , <strong>tidak</strong> lagi menjadi cermin jaringan dan menjadi sesuatu yang lebih. </p><br><p>  Pertama, gagasan untuk membuat <strong>perangkat blok terdistribusi</strong> tunggal untuk beberapa server telah surut ke latar belakang, dan sekarang LINBIT sedang mencoba untuk menyediakan alat untuk mengatur dan mengelola banyak perangkat drbd dalam sebuah cluster yang dibuat di atas <strong>partisi</strong> <strong>LVM</strong> dan <strong>ZFS</strong> . </p><br><p>  Misalnya, DRBD9 mendukung hingga 32 replika, RDMA, node tanpa disk, dan alat orkestrasi baru memungkinkan Anda untuk menggunakan snapshot, migrasi online, dan banyak lagi. </p><br><p>  Terlepas dari kenyataan bahwa <strong>DRBD9</strong> memiliki alat integrasi dengan <strong>Proxmox</strong> , <strong>Kubernetes</strong> , <strong>OpenStack</strong> dan <strong>OpenNebula</strong> , saat ini mereka berada dalam beberapa mode transisi, ketika alat baru belum didukung di mana-mana, dan yang lama akan segera diumumkan sebagai <em>usang</em> .  Ini adalah <strong>DRBDmanage</strong> dan <strong>Linstor</strong> . </p><br><p>  Saya akan memanfaatkan momen ini untuk tidak terlalu banyak membahas masing-masing, tetapi untuk memeriksa lebih detail konfigurasi dan prinsip kerja dengan <strong>DRBD9</strong> itu sendiri. <a name="habracut"></a>  Anda masih harus mencari tahu, jika hanya karena konfigurasi toleran dari pengontrol Linstor menyiratkan menginstalnya pada salah satu perangkat ini. </p><br><p>  Pada artikel ini, saya ingin memberi tahu Anda tentang <strong>DRBD9</strong> dan kemungkinan penggunaannya di <strong>Proxmox</strong> tanpa plug-in pihak ketiga. </p><br><h2 id="drbdmanage-i-linstor">  DRBDmanage and Linstor </h2><br><p>  Pertama, perlu disebutkan sekali lagi tentang <strong>DRBDmanage</strong> , yang terintegrasi dengan sangat baik dalam <strong>Proxmox</strong> .  LINBIT menyediakan plugin DRBDmanage yang sudah jadi untuk Proxmox yang memungkinkan Anda untuk menggunakan semua fungsinya secara langsung dari antarmuka <strong>Proxmox</strong> . </p><br><p>  Ini terlihat sangat menakjubkan, tetapi sayangnya memiliki beberapa kelemahan. </p><br><ul><li> Pertama, nama volume yang ditandai, <strong>grup LVM,</strong> atau <strong>kumpulan ZFS</strong> harus bernama <code>drbdpool</code> . </li><li>  Ketidakmampuan untuk menggunakan lebih dari <strong>satu</strong> kumpulan per node </li><li>  Karena spesifik dari solusi, <strong>volume pengontrol</strong> hanya dapat pada LVM biasa dan tidak sebaliknya </li><li>  Gangguan <strong>dbus</strong> periodik, yang digunakan secara erat oleh <strong>DRBDmanage</strong> untuk berinteraksi dengan node. </li></ul><br><p>  Akibatnya, LINBIT memutuskan untuk mengganti semua logika DRBDmanage yang kompleks dengan aplikasi sederhana yang berkomunikasi dengan node menggunakan <strong>koneksi tcp</strong> biasa dan bekerja tanpa sihir di sana.  Jadi ada <strong>Linstor</strong> . </p><br><p>  <strong>Linstor</strong> benar-benar bekerja dengan sangat baik.  Sayangnya, pengembang memilih <strong>java</strong> sebagai bahasa utama untuk menulis Linstor-server, tetapi jangan biarkan ini membuat Anda takut, karena Linstor sendiri hanya peduli dengan <strong>mendistribusikan konfigurasi</strong> DRBD dan <strong>mengiris</strong> partisi LVM / ZFS pada node. </p><br><blockquote>  Kedua solusi ini gratis dan didistribusikan di bawah lisensi <strong>GPL3</strong> gratis <strong>.</strong> </blockquote><p>  Anda dapat membaca tentang masing-masing dari mereka dan tentang menyiapkan plug-in untuk <strong>Proxmox tersebut</strong> di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">wiki Proxmox resmi</a> </p><br><h2 id="otkazoustoychivyy-nfs-server">  Failover NFS Server </h2><br><p>  Sayangnya, pada saat penulisan, <strong>Linstor hanya</strong> memiliki integrasi dengan <strong>Kubernetes</strong> .  Tetapi pada akhir tahun, driver diharapkan untuk sisa sistem <strong>Proxmox</strong> , <strong>OpenNebula</strong> , <strong>OpenStack</strong> . </p><br><p>  Namun sejauh ini tidak ada solusi yang siap pakai, tetapi kami entah bagaimana tidak menyukai yang lama.  Mari kita coba gunakan DRBD9 cara kuno untuk mengatur <strong>akses NFS</strong> ke partisi bersama. </p><br><p>  Namun demikian, solusi ini juga akan terbukti bukan tanpa keuntungan, karena server NFS akan memungkinkan Anda untuk mengatur <strong>akses kompetitif</strong> ke sistem file repositori dari beberapa server tanpa perlu sistem file cluster kompleks dengan DLM, seperti OCFS dan GFS2. </p><br><p>  Dalam hal ini, Anda akan dapat mengganti peran simpul <strong>Primer</strong> / <strong>Sekunder</strong> hanya dengan memigrasikan wadah dengan server NFS di antarmuka Proxmox. </p><br><p>  Anda juga dapat menyimpan file apa pun di dalam sistem file ini, serta disk dan cadangan virtual. </p><br><p>  Jika Anda menggunakan <strong>Kubernetes,</strong> Anda dapat mengatur akses <strong>ReadWriteMany</strong> untuk <strong>PersistentVolumes</strong> Anda. </p><br><h2 id="proxmox-i-lxc-konteynery">  Wadah Proxmox dan LXC </h2><br><p>  Sekarang pertanyaannya adalah: mengapa Proxmox? </p><br><p>  Pada prinsipnya, untuk membangun skema seperti itu, kita bisa menggunakan Kubernetes serta skema biasa dengan manajer klaster.  Tetapi <strong>Proxmox</strong> menyediakan antarmuka yang siap pakai, sangat multi-fungsi dan pada saat yang sama sederhana dan intuitif untuk hampir semua yang Anda butuhkan.  Itu di luar kotak yang mampu <strong>mengelompokkan</strong> dan mendukung mekanisme <strong>pagar</strong> berdasarkan softdog.  Dan saat menggunakan <strong>wadah LXC,</strong> ini memungkinkan Anda untuk mencapai batas waktu minimal saat beralih. <br>  Solusi yang dihasilkan tidak akan memiliki satu <strong>titik kegagalan</strong> . </p><br><p>  Faktanya, kita akan menggunakan Proxmox terutama sebagai <strong>klaster-manajer</strong> , di mana kita dapat mempertimbangkan <strong>wadah LXC</strong> terpisah sebagai layanan yang berjalan di kluster HA klasik, hanya dengan perbedaan bahwa wadah tersebut juga dilengkapi dengan <strong>sistem akarnya</strong> .  Artinya, Anda tidak perlu menginstal beberapa instance layanan pada setiap server secara terpisah, Anda dapat melakukan ini hanya sekali di dalam wadah. <br>  Jika Anda pernah bekerja dengan <strong>perangkat lunak cluster-manager</strong> dan menyediakan <strong>HA</strong> untuk aplikasi, Anda akan mengerti apa yang saya maksud. </p><br><h2 id="obschaya-shema">  Skema umum </h2><br><p>  Solusi kami akan menyerupai skema replikasi standar dari suatu basis data. </p><br><ul><li>  Kami memiliki <strong>tiga node</strong> </li><li>  Setiap node memiliki <strong>perangkat drbd</strong> terdistribusi. </li><li>  Perangkat ini memiliki sistem file biasa ( <strong>ext4</strong> ) </li><li>  Hanya satu server yang bisa menjadi <strong>master</strong> </li><li>  <strong>Server NFS</strong> di <strong>wadah LXC</strong> diluncurkan pada wizard. </li><li>  Semua node mengakses perangkat secara ketat melalui <strong>NFS.</strong> </li><li>  Jika perlu, wizard dapat pindah ke node lain, bersama dengan <strong>server NFS</strong> </li></ul><br><p>  <strong>DRBD9</strong> memiliki satu fitur yang sangat keren yang sangat menyederhanakan segalanya: <br>  Perangkat drbd secara otomatis menjadi <strong>Utama</strong> pada saat dipasang pada beberapa node.  Jika perangkat ditandai sebagai <strong>Utama</strong> , setiap upaya untuk memasangnya pada simpul lain akan menghasilkan kesalahan akses.  Ini memastikan pemblokiran dan jaminan perlindungan terhadap akses simultan ke perangkat. </p><br><p>  Mengapa semua ini sangat disederhanakan?  Karena ketika wadah mulai, <strong>Proxmox</strong> secara otomatis <strong>me-</strong> mount perangkat ini dan itu menjadi <strong>Primer</strong> pada node ini, dan ketika kontainer berhenti, itu unmount itu sebaliknya dan perangkat menjadi <strong>Sekunder</strong> lagi. <br>  Dengan demikian, kita tidak perlu lagi khawatir beralih perangkat <strong>Primer</strong> / <strong>Sekunder</strong> , Proxmox akan melakukannya <strong>secara otomatis</strong> , Hore! </p><br><h2 id="nastroyka-drbd">  Pengaturan DRBD </h2><br><p>  Ya, kami sudah menemukan ide. Sekarang mari beralih ke implementasi. </p><br><p>  Secara default <strong>, versi kedelapan dari drbd</strong> disertakan <strong>dengan kernel Linux</strong> , sayangnya itu <strong>tidak cocok untuk</strong> kita dan kita perlu menginstal versi kesembilan dari modul. </p><br><p>  Hubungkan repositori LINBIT dan instal semua yang Anda butuhkan: </p><br><pre> <code class="bash hljs">wget -O- https://packages.linbit.com/package-signing-pubkey.asc | apt-key add - <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://packages.linbit.com/proxmox/ proxmox-5 drbd-9.0"</span></span> \ &gt; /etc/apt/sources.list.d/linbit.list apt-get update &amp;&amp; apt-get -y install pve-headers drbd-dkms drbd-utils drbdtop</code> </pre> <br><ul><li>  <code>pve-headers</code> - <code>pve-headers</code> kernel diperlukan untuk membangun modul </li><li>  <code>drbd-dkms</code> - modul kernel dalam format DKMS </li><li>  <code>drbd-utils</code> - utilitas manajemen DRBD dasar </li><li>  <code>drbdtop</code> adalah alat interaktif seperti top untuk DRBD saja </li></ul><br><p>  Setelah menginstal <strong>modul, kami akan</strong> memeriksa apakah semuanya sudah sesuai dengan itu: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe drbd # cat /proc/drbd version: 9.0.14-1 (api:2/proto:86-113)</span></span></code> </pre> <br><p>  Jika Anda melihat <strong>versi kedelapan</strong> di output perintah, maka ada yang tidak beres dan modul kernel <strong>in-tree</strong> dimuat.  Periksa <code>dkms status</code> mencari tahu apa alasannya. </p><br><p>  Setiap node yang kita miliki akan memiliki perangkat <strong>drbd yang</strong> sama berjalan di atas partisi reguler.  Pertama kita perlu menyiapkan bagian ini untuk drbd pada setiap node. </p><br><p>  Partisi seperti itu dapat berupa <strong>perangkat blok</strong> apa saja, dapat berupa lvm, zvol, partisi disk, atau keseluruhan disk.  Pada artikel ini saya akan menggunakan disk nvme terpisah dengan partisi di bawah drbd: <code>/dev/nvme1n1p1</code> </p><br><p>  Perlu dicatat bahwa nama perangkat cenderung berubah kadang-kadang, jadi lebih baik untuk segera menganggapnya sebagai kebiasaan untuk menggunakan symlink konstan ke perangkat. </p><br><p>  Anda dapat menemukan symlink untuk <code>/dev/nvme1n1p1</code> dengan cara ini: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># find /dev/disk/ -lname '*/nvme1n1p1' /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2 /dev/disk/by-path/pci-0000:0e:00.0-nvme-1-part1 /dev/disk/by-id/nvme-eui.0000000001000000e4d25c33da9f4d01-part1 /dev/disk/by-id/nvme-INTEL_SSDPEKKA010T7_BTPY703505FB1P0H-part1</span></span></code> </pre> <br><p>  Kami menggambarkan sumber daya kami pada ketiga simpul: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/drbd.d/nfs1.res resource nfs1 { meta-disk internal; device /dev/drbd100; protocol C; net { after-sb-0pri discard-zero-changes; after-sb-1pri discard-secondary; after-sb-2pri disconnect; } on pve1 { address 192.168.2.11:7000; disk /dev/disk/by-partuuid/95e7eabb-436e-4585-94ea-961ceac936f7; node-id 0; } on pve2 { address 192.168.2.12:7000; disk /dev/disk/by-partuuid/aa7490c0-fe1a-4b1f-ba3f-0ddee07dfee3; node-id 1; } on pve3 { address 192.168.2.13:7000; disk /dev/disk/by-partuuid/847b9713-8c00-48a1-8dff-f84c328b9da2; node-id 2; } connection-mesh { hosts pve1 pve2 pve3; } }</span></span></code> </pre> <br><p>  Dianjurkan untuk menggunakan <strong>jaringan terpisah</strong> untuk sinkronisasi drbd. </p><br><p>  Sekarang buat metadata untuk drbd dan jalankan: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md nfs1 initializing activity log initializing bitmap (320 KB) to all zero Writing meta data... New drbd meta data block successfully created. success # drbdadm up nfs1</span></span></code> </pre> <br><p>  Ulangi langkah-langkah ini pada ketiga node dan periksa status: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:Inconsistent pve2 role:Secondary peer-disk:Inconsistent pve3 role:Secondary peer-disk:Inconsistent</span></span></code> </pre> <br><p>  Sekarang disk <strong>tidak konsisten</strong> kami <strong>ada</strong> di ketiga node, ini karena drbd tidak tahu disk mana yang harus diambil seperti aslinya.  Kita harus menandai salah satunya sebagai <strong>Primer</strong> agar statusnya disinkronkan ke node lain: </p><br><pre> <code class="bash hljs">drbdadm primary --force nfs1 drbdadm secondary nfs1</code> </pre> <br><p>  Segera setelah ini, <strong>sinkronisasi</strong> akan dimulai: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm status nfs1 role:Secondary disk:UpToDate pve2 role:Secondary replication:SyncSource peer-disk:Inconsistent done:26.66 pve3 role:Secondary replication:SyncSource peer-disk:Inconsistent done:14.20</span></span></code> </pre><br><p>  Kita tidak harus menunggu sampai selesai dan kita bisa melakukan langkah lebih lanjut secara paralel.  Mereka dapat dieksekusi pada <strong>sembarang simpul</strong> , terlepas dari keadaan saat ini dari disk lokal di DRBD.  Semua permintaan akan secara otomatis dialihkan ke perangkat dengan status <strong>UpToDate</strong> . </p><br><p>  Jangan lupa untuk mengaktifkan <strong>autorun</strong> layanan drbd pada node: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">enable</span></span> drbd.service</code> </pre> <br><h2 id="nastroyka-lxc-konteynera">  Mengkonfigurasi wadah LXC </h2><br><p>  Kami <strong>akan</strong> menghilangkan bagian konfigurasi dari <strong>cluster Proxmox yang terdiri</strong> dari tiga simpul, bagian ini dijelaskan dengan baik di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">wiki resmi</a> </p><br><p>  Seperti yang saya katakan sebelumnya, <strong>server NFS</strong> kami akan bekerja dalam <strong>wadah LXC</strong> .  Kami akan menyimpan wadah di perangkat <code>/dev/drbd100</code> yang baru saja kami buat. </p><br><p>  Pertama kita perlu membuat <strong>sistem file</strong> di atasnya: </p><br><pre> <code class="hljs powershell">mkfs <span class="hljs-literal"><span class="hljs-literal">-t</span></span> ext4 <span class="hljs-literal"><span class="hljs-literal">-O</span></span> mmp <span class="hljs-literal"><span class="hljs-literal">-E</span></span> mmp_update_interval=<span class="hljs-number"><span class="hljs-number">5</span></span> /dev/drbd100</code> </pre> <br><p>  <strong>Proxmox</strong> secara default mencakup <strong>perlindungan multimount</strong> di tingkat sistem file, pada prinsipnya, kita dapat melakukannya tanpa itu, karena  DRBD memiliki perlindungan sendiri secara default, itu hanya melarang <strong>Primary</strong> kedua untuk perangkat, tetapi hati-hati tidak melukai kita. </p><br><p>  Sekarang unduh templat Ubuntu: </p><br><pre> <code class="hljs pgsql"># wget http://download.proxmox.com/images/<span class="hljs-keyword"><span class="hljs-keyword">system</span></span>/ubuntu<span class="hljs-number"><span class="hljs-number">-16.04</span></span>-standard_16<span class="hljs-number"><span class="hljs-number">.04</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>_amd64.tar.gz -P /var/lib/vz/<span class="hljs-keyword"><span class="hljs-keyword">template</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span>/</code> </pre> <br><p>  Dan buat wadah kami dari itu: </p><br><pre> <code class="hljs powershell">pct create <span class="hljs-number"><span class="hljs-number">101</span></span> local:vztmpl/ubuntu<span class="hljs-literal"><span class="hljs-literal">-16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-standard_16</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span><span class="hljs-literal"><span class="hljs-literal">-1_amd64</span></span>.tar.gz \ -<span class="hljs-literal"><span class="hljs-literal">-hostname</span></span>=nfs1 \ -<span class="hljs-literal"><span class="hljs-literal">-net0</span></span>=name=eth0,bridge=vmbr0,gw=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.1</span></span>,ip=<span class="hljs-number"><span class="hljs-number">192.168</span></span>.<span class="hljs-number"><span class="hljs-number">1.11</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> \ -<span class="hljs-literal"><span class="hljs-literal">-rootfs</span></span>=volume=/dev/drbd100,shared=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Dalam perintah ini, kami menunjukkan bahwa <strong>sistem root dari</strong> wadah kami akan ada di perangkat <code>/dev/drbd100</code> dan menambahkan parameter <code>shared=1</code> untuk memungkinkan <strong>migrasi</strong> kontainer di antara node. </p><br><p>  Jika terjadi kesalahan, Anda selalu dapat memperbaikinya melalui antarmuka <strong>Proxmox</strong> atau di <code>/etc/pve/lxc/101.conf</code> wadah </p><br><p>  Proxmox akan membongkar templat dan menyiapkan <strong>sistem root</strong> wadah untuk kami.  Setelah itu kita bisa meluncurkan wadah kita: </p><br><pre> <code class="hljs pgsql">pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><h2 id="nastroyka-nfs-servera">  Konfigurasikan server NFS. </h2><br><p>  Secara default, Proxmox <strong>tidak mengizinkan</strong> <strong>server NFS</strong> berjalan di dalam wadah, tetapi ada beberapa cara untuk mengaktifkannya. </p><br><p>  Salah satunya adalah dengan menambahkan <code>lxc.apparmor.profile: unconfined</code> pada <code>/etc/pve/lxc/100.conf</code> . </p><br><p>  Atau kita dapat <strong>mengaktifkan NFS</strong> untuk semua kontainer secara berkelanjutan, untuk ini kita perlu memperbarui template standar untuk LXC pada semua node, tambahkan baris berikut ke <code>/etc/apparmor.d/lxc/lxc-default-cgns</code> : </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">mount</span></span> fstype=nfs, mount fstype=nfs4, mount fstype=nfsd, mount fstype=rpc_pipefs,</code> </pre> <br><p>  Setelah perubahan, mulai ulang wadah: </p><br><pre> <code class="hljs pgsql">pct shutdown <span class="hljs-number"><span class="hljs-number">101</span></span> pct <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span></code> </pre> <br><p>  Sekarang mari kita masuk: </p><br><pre> <code class="hljs perl">pct <span class="hljs-keyword"><span class="hljs-keyword">exec</span></span> <span class="hljs-number"><span class="hljs-number">101</span></span> bash</code> </pre> <br><p>  Instal pembaruan dan <strong>server NFS</strong> : </p><br><pre> <code class="hljs powershell">apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> update apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> upgrade apt<span class="hljs-literal"><span class="hljs-literal">-get</span></span> <span class="hljs-literal"><span class="hljs-literal">-y</span></span> install nfs<span class="hljs-literal"><span class="hljs-literal">-kernel</span></span><span class="hljs-literal"><span class="hljs-literal">-server</span></span></code> </pre> <br><p>  Buat <strong>ekspor</strong> : </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">echo</span></span> '/<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> *(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">rw</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_root_squash</span></span></span><span class="hljs-class">,</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">no_subtree_check</span></span></span><span class="hljs-class">)' &gt;&gt; /etc/exports mkdir /</span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">data</span></span></span><span class="hljs-class"> exportfs -a</span></span></code> </pre> <br><h2 id="nastroyka-ha">  Pengaturan HA </h2><br><p>  Pada saat penulisan, proxmox <strong>HA-manager</strong> memiliki <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bug</a> yang tidak memungkinkan wadah HA berhasil menyelesaikan pekerjaannya, sebagai akibatnya proses <strong>ruang-kernel</strong> dari <strong>server nfs</strong> yang tidak sepenuhnya terbunuh mencegah perangkat drbd meninggalkan meninggalkan <strong>Secondary</strong> .  Jika Anda sudah mengalami situasi seperti itu, Anda tidak boleh panik dan hanya menjalankan <code>killall -9 nfsd</code> pada node di mana wadah diluncurkan dan kemudian perangkat drbd harus "melepaskan" dan itu akan pergi ke <strong>Sekunder</strong> . </p><br><p>  Untuk memperbaiki bug ini, jalankan perintah berikut di semua node: </p><br><pre> <code class="hljs powershell">sed <span class="hljs-literal"><span class="hljs-literal">-i</span></span> <span class="hljs-string"><span class="hljs-string">'s/forceStop =&gt; 1,/forceStop =&gt; 0,/'</span></span> /usr/share/perl5/PVE/HA/Resources/PVECT.pm systemctl restart pve<span class="hljs-literal"><span class="hljs-literal">-ha</span></span><span class="hljs-literal"><span class="hljs-literal">-lrm</span></span>.service</code> </pre> <br><p>  Sekarang kita dapat beralih ke konfigurasi <strong>HA-manager</strong> .  Mari kita membuat grup HA terpisah untuk perangkat kita: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> groupadd nfs1 -<span class="hljs-literal"><span class="hljs-literal">-nodes</span></span> pve1,pve2,pve3 -<span class="hljs-literal"><span class="hljs-literal">-nofailback</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> -<span class="hljs-literal"><span class="hljs-literal">-restricted</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  <strong>Sumber daya</strong> kami hanya akan berfungsi pada node yang ditentukan untuk grup ini.  Tambahkan wadah kami ke grup ini: </p><br><pre> <code class="hljs powershell">ha<span class="hljs-literal"><span class="hljs-literal">-manager</span></span> add ct:<span class="hljs-number"><span class="hljs-number">101</span></span> -<span class="hljs-literal"><span class="hljs-literal">-group</span></span>=nfs1 -<span class="hljs-literal"><span class="hljs-literal">-max_relocate</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span> -<span class="hljs-literal"><span class="hljs-literal">-max_restart</span></span>=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><p>  Itu saja.  Sederhana bukan? </p><br><p>  <strong>Bola nfs yang</strong> dihasilkan dapat langsung dihubungkan ke Proxmox untuk menyimpan dan menjalankan mesin dan wadah virtual lainnya. </p><br><h2 id="rekomendacii-i-tyuning">  Rekomendasi dan penyetelan </h2><br><h5 id="drbd">  DRBD </h5><br><p>  Seperti yang saya sebutkan di atas, selalu disarankan untuk menggunakan jaringan terpisah untuk replikasi.  Sangat disarankan untuk menggunakan <strong>adapter jaringan 10-gigabit</strong> , jika tidak, Anda akan mengalami kecepatan port. <br>  Jika replikasi tampaknya cukup lambat, cobalah beberapa opsi untuk <strong>DRBD</strong> .  Ini adalah konfigurasi, yang menurut saya optimal untuk <strong>jaringan 10G</strong> saya: </p><br><pre> <code class="hljs swift"># cat /etc/drbd.d/global_common.conf global { usage-<span class="hljs-built_in"><span class="hljs-built_in">count</span></span> yes; udev-always-use-vnr; } common { handlers { } startup { } options { } disk { <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-fill-target 10M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-rate 720M; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-plan-ahead <span class="hljs-number"><span class="hljs-number">10</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>-<span class="hljs-built_in"><span class="hljs-built_in">min</span></span>-rate 20M; } net { <span class="hljs-built_in"><span class="hljs-built_in">max</span></span>-buffers 36k; sndbuf-size 1024k; rcvbuf-size 2048k; } }</code> </pre> <br><p>  Anda dapat memperoleh informasi lebih lanjut tentang setiap parameter dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi DRBD resmi.</a> </p><br><h5 id="nfs-server">  Server NFS </h5><br><p>  Untuk mempercepat operasi <strong>server NFS,</strong> mungkin membantu untuk meningkatkan jumlah total menjalankan server NFS.  Secara default - <strong>8</strong> , secara pribadi, ini membantu saya meningkatkan angka ini menjadi <strong>64</strong> . </p><br><p>  Untuk mencapai ini, perbarui <code>RPCNFSDCOUNT=64</code> parameter di <code>/etc/default/nfs-kernel-server</code> . <br>  Dan restart daemon: </p><br><pre> <code class="hljs pgsql">systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-utils systemctl <span class="hljs-keyword"><span class="hljs-keyword">restart</span></span> nfs-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span></code> </pre> <br><h5 id="nfsv3-vs-nfsv4">  NFSv3 vs NFSv4 </h5><br><p>  Tahu perbedaan antara <strong>NFSv3</strong> dan <strong>NFSv4</strong> ? </p><br><ul><li>  <strong>NFSv3</strong> adalah <strong>protokol tanpa kewarganegaraan,</strong> sebagai aturan, ia lebih baik mentolerir kegagalan dan pulih lebih cepat. </li><li>  <strong>NFSv4</strong> adalah <strong>protokol stateful</strong> , ia bekerja lebih cepat dan dapat diikat ke port tcp tertentu, tetapi karena keberadaan negara itu lebih sensitif terhadap kegagalan.  Ini juga memiliki kemampuan untuk menggunakan otentikasi menggunakan Kerberos dan banyak fitur menarik lainnya. </li></ul><br><p>  Namun, ketika Anda menjalankan <code>showmount -e nfs_server</code> , protokol NFSv3 digunakan.  Proxmox juga menggunakan NFSv3.  NFSv3 juga biasa digunakan untuk mengatur mesin boot jaringan. </p><br><p>  Secara umum, jika Anda tidak memiliki alasan khusus untuk menggunakan NFSv4, coba gunakan NFSv3 karena tidak terlalu menyakitkan untuk kegagalan apa pun karena kurangnya status seperti itu. </p><br><p>  Anda bisa memasang bola menggunakan NFSv3 dengan menentukan parameter <code>-o vers=3</code> untuk perintah <strong>mount</strong> : </p><br><pre> <code class="bash hljs">mount -o vers=3 nfs_server:/share /mnt</code> </pre> <br><p>  Jika mau, Anda dapat menonaktifkan NFSv4 untuk server sama sekali, untuk melakukan ini, tambahkan opsi <code>--no-nfs-version 4</code> ke variabel <code>--no-nfs-version 4</code> dan restart server, misalnya: </p><br><pre> <code class="bash hljs">RPCNFSDCOUNT=<span class="hljs-string"><span class="hljs-string">"64 --no-nfs-version 4"</span></span></code> </pre> <br><h2 id="iscsi-i-lvm">  iSCSI dan LVM </h2><br><p>  Demikian pula, <strong>daemon tgt</strong> reguler dapat dikonfigurasikan di dalam container, iSCSI akan menghasilkan kinerja yang jauh lebih tinggi untuk operasi I / O, dan container akan bekerja lebih lancar karena server tgt bekerja sepenuhnya di ruang pengguna. </p><br><p>  Biasanya, <strong>LUN yang</strong> diekspor dipotong menjadi beberapa bagian menggunakan <strong>LVM</strong> .  Namun, ada beberapa nuansa yang perlu dipertimbangkan, misalnya: bagaimana <strong>kunci</strong> LVM disediakan untuk berbagi grup yang diekspor pada banyak host. </p><br><p>  Mungkin ini dan nuansa lain yang akan saya jelaskan di <strong><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel selanjutnya</a></strong> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id417473/">https://habr.com/ru/post/id417473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../hi486150/index.html">स्लोवाकिया में आईटी-क्षेत्र का विकास। युवा पेशेवरों के लिए काम का लाभ</a></li>
<li><a href="../hi486156/index.html">जैसा कि मैंने पढ़ाया, और फिर पायथन में एक प्रशिक्षण मैनुअल लिखा</a></li>
<li><a href="../hi486158/index.html">विज़ुअलाइज़िंग न्यूरल मशीन ट्रांसलेशन (seq2seq मॉडल विथ अटेंशन मैकेनिज़्म)</a></li>
<li><a href="../hi486164/index.html">कोरोनावायरस 2019-nCoV। श्वसन संरक्षण और कीटाणुशोधन पर अक्सर पूछे जाने वाले प्रश्न</a></li>
<li><a href="../hi486174/index.html">मेरा जीरो टर्नओवर है</a></li>
<li><a href="../id417475/index.html">Glusterfs + erasure coding: saat Anda membutuhkan banyak, murah dan dapat diandalkan</a></li>
<li><a href="../id417477/index.html">Meja panas</a></li>
<li><a href="../id417479/index.html">Penggabungan string do-it-yourself yang lebih cepat di Go</a></li>
<li><a href="../id417481/index.html">Tentang generator di JavaScript ES6, dan mengapa itu opsional untuk mempelajarinya</a></li>
<li><a href="../id417483/index.html">Perbandingan kerangka JS: React, Vue, dan Hyperapp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>