<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèæ ü§ù üíÜüèø Acc√©l√©ration mat√©rielle des r√©seaux de neurones profonds: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP et autres lettres üï§ üè≠ üë©üèø‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le 14 mai, alors que Trump s'appr√™tait √† lancer tous les chiens sur Huawei, je me suis assis paisiblement √† Shenzhen sur le Huawei STW 2019 - une gran...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Acc√©l√©ration mat√©rielle des r√©seaux de neurones profonds: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP et autres lettres</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455353/"><img src="https://habrastorage.org/getpro/habr/post_images/1d8/7a7/de6/1d87a7de6c72f1f712049ce550978d7c.png"><br><br>  Le 14 mai, alors que Trump s'appr√™tait √† lancer tous les chiens sur Huawei, je me suis assis paisiblement √† Shenzhen sur le Huawei STW 2019 - une grande conf√©rence pour 1000 participants - qui comprenait des rapports de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Philip Wong</a> , vice-pr√©sident de la recherche TSMC sur les perspectives de l'informatique non von Neumann architectures, et Heng Liao, boursier Huawei, scientifique en chef Huawei 2012 Lab, sur le d√©veloppement d'une nouvelle architecture de processeurs tenseurs et de neuroprocesseurs.  TSMC, si vous le savez, fabrique des acc√©l√©rateurs neuronaux pour Apple et Huawei en utilisant la technologie 7 nm (que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">peu de gens poss√®dent</a> ), et Huawei est pr√™t √† concurrencer Google et NVIDIA en termes de neuroprocesseurs. <br><br>  Google en Chine est interdit, je n'ai pas pris la peine de mettre un VPN sur la tablette, j'ai donc <s>patriquement</s> utilis√© Yandex afin de voir quelle est la situation avec d'autres fabricants de fer similaire, et ce qui se passe g√©n√©ralement.  En g√©n√©ral, j'ai observ√© la situation, mais ce n'est qu'apr√®s ces rapports que j'ai r√©alis√© √† quel point la r√©volution se pr√©parait √† grande √©chelle dans les entrailles des entreprises et le silence des salles scientifiques. <br><br>  L'an dernier seulement, plus de 3 milliards de dollars ont √©t√© investis dans le sujet.  Google a depuis longtemps d√©clar√© que les r√©seaux de neurones sont un domaine strat√©gique et construit activement leur support mat√©riel et logiciel.  NVIDIA, sentant que le tr√¥ne est stup√©fiant, fait des efforts fantastiques dans les biblioth√®ques d'acc√©l√©ration des r√©seaux neuronaux et le nouveau mat√©riel.  Intel a d√©pens√© 0,8 milliard d'euros en 2016 pour acheter deux soci√©t√©s impliqu√©es dans l'acc√©l√©ration mat√©rielle des r√©seaux de neurones.  Et cela malgr√© le fait que les principaux achats n'aient pas encore commenc√©, et que le nombre de joueurs a d√©pass√© la cinquantaine et cro√Æt rapidement. <br><br><div style="text-align:center;"><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/74c/308/a37/74c308a372700574cc0e29f13347ede5.png"></div><br>  TPU, VPU, IPU, DPU, NPU, RPU, NNP - qu'est-ce que tout cela signifie et qui va gagner?  Essayons de le comprendre.  Peu importe - Bienvenue au chat! <br><a name="habracut"></a><br><hr>  <b><font color="#ff0000">Avertissement: L'</font></b> auteur a d√ª r√©√©crire compl√®tement les algorithmes de traitement vid√©o pour une impl√©mentation efficace sur ASIC, et les clients ont fait du prototypage sur FPGA, donc il y a une id√©e de la profondeur de la diff√©rence dans les architectures.  Cependant, l'auteur n'a pas travaill√© directement avec le fer r√©cemment.  Mais il pr√©voit qu'il devra se plonger dans. <br><br><h2>  Contexte des probl√®mes </h2><br>  Le nombre de calculs requis augmente rapidement, les gens aimeraient prendre plus de couches, plus d'options d'architecture, jouer plus activement avec les hyperparam√®tres, mais ... cela d√©pend des performances.  Dans le m√™me temps, par exemple, avec la croissance de la productivit√© des bons vieux processeurs - gros probl√®mes.  Toutes les bonnes choses ont une fin: la loi de Moore, comme vous le savez, s'√©puise et le taux de croissance des performances du processeur chute: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/704/8e8/bab/7048e8bab326e1f4f62058e5f3a925f4.png"></div><br>  <i>Calculs des performances r√©elles des op√©rations enti√®res sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SPECint</a> par rapport au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VAX11-780</a> , ci-apr√®s souvent une √©chelle logarithmique</i> <br><br>  Si du milieu des ann√©es 80 au milieu des ann√©es 2000 - dans les ann√©es b√©nies de l'√¢ge d'or des ordinateurs - la croissance a √©t√© en moyenne de 52% par an, ces derni√®res ann√©es, elle est tomb√©e √† 3% par an.  Et c'est un probl√®me (une traduction d'un r√©cent article du patriarche John Hennessey sur les probl√®mes et les perspectives de l'architecture moderne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©tait sur Habr√©</a> ). <br><br>  Il existe de nombreuses raisons, par exemple, la fr√©quence des processeurs a cess√© de cro√Ætre: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/a49/5d6/be5/a495d6be5c00ce436bb4c2f1f7f356fc.png"></div><br>  Il est devenu plus difficile de r√©duire la taille des transistors.  Le dernier malheur qui r√©duit consid√©rablement les performances (y compris les performances des processeurs d√©j√† sortis) est (roulement de tambour) ... √† droite, la s√©curit√©.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La fusion</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Spectre</a> et d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">autres</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">vuln√©rabilit√©s</a> causent d'√©normes dommages au taux de croissance de la puissance de traitement du processeur ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un exemple de d√©sactivation de l'hyperthreading</a> (!)).  Le sujet est devenu populaire et de nouvelles vuln√©rabilit√©s de ce type sont trouv√©es <i>presque tous les mois</i> .  Et c'est une sorte de cauchemar, car √ßa fait mal en termes de performances. <br><br>  Dans le m√™me temps, le d√©veloppement de nombreux algorithmes est fermement li√© √† la croissance famili√®re de la puissance du processeur.  Par exemple, de nombreux chercheurs ne se pr√©occupent pas aujourd'hui de la vitesse des algorithmes - ils trouveront quelque chose.  Et ce serait bien lors de l'apprentissage - les r√©seaux deviennent grands et ¬´difficiles¬ª √† utiliser.  Cela est particuli√®rement √©vident dans la vid√©o, pour laquelle la plupart des approches, en principe, ne sont pas applicables √† grande vitesse.  Et ils n'ont souvent de sens qu'en temps r√©el.  C'est aussi un probl√®me. <br><br>  De m√™me, de nouvelles normes de compression sont en cours d'√©laboration qui sugg√®rent une augmentation de la puissance du d√©codeur.  Et si la puissance du processeur n'augmente pas?  L'ancienne g√©n√©ration se souvient des probl√®mes rencontr√©s dans les ann√©es 2000 lors de la lecture de vid√©os haute d√©finition dans le tout nouveau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">H.264</a> sur des ordinateurs plus anciens.  Oui, la qualit√© √©tait meilleure avec une taille plus petite, mais sur les sc√®nes rapides, l'image pendait ou le son √©tait d√©chir√©.  Je dois communiquer avec les d√©veloppeurs du nouveau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VVC / H.266</a> (une sortie est pr√©vue pour l'ann√©e prochaine).  Vous ne les envierez pas. <br><br>  Alors, que nous pr√©pare le si√®cle √† venir √† la lumi√®re de la diminution du taux de croissance des performances du processeur appliqu√© aux r√©seaux de neurones? <br><br><h2>  CPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/b44/44b/f1eb4444b3e5e320447c6f65fa4ef14c.png"><br><br>  Un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processeur</a> ordinaire est un grand concasseur perfectionn√© depuis des d√©cennies.  H√©las, pour d'autres t√¢ches. <br><br>  Lorsque nous travaillons avec des r√©seaux de neurones, en particulier les r√©seaux profonds, notre r√©seau lui-m√™me peut occuper des centaines de m√©gaoctets.  Par exemple, les besoins en m√©moire des r√©seaux de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©tection d'objets sont</a> les suivants: <br><div class="scrollable-table"><table><tbody><tr><td>  mod√®le <br></td><td>  taille d'entr√©e <br></td><td>  m√©moire param <br></td><td>  m√©moire de fonctions <br></td></tr><tr><td>  <a href="">rfcn-res50-pascal</a> <br></td><td>  600 x 850 <br></td><td>  122 Mo <br></td><td>  1 Go <br></td></tr><tr><td>  <a href="">rfcn-res101-pascal</a> <br></td><td>  600 x 850 <br></td><td>  194 Mo <br></td><td>  2 Go <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-300</a> <br></td><td>  300 x 300 <br></td><td>  100 Mo <br></td><td>  116 Mo <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-512</a> <br></td><td>  512 x 512 <br></td><td>  104 Mo <br></td><td>  337 Mo <br></td></tr><tr><td>  <a href="">ssd-pascal-mobilenet-ft</a> <br></td><td>  300 x 300 <br></td><td>  22 Mo <br></td><td>  37 Mo <br></td></tr><tr><td>  <a href="">plus rapide-rcnn-vggvd-pascal</a> <br></td><td>  600 x 850 <br></td><td>  523 Mo <br></td><td>  600 Mo <br></td></tr></tbody></table></div><br><br>  D'apr√®s notre exp√©rience, les coefficients d'un r√©seau neuronal profond pour le traitement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des bordures translucides</a> peuvent occuper 150-200 Mo.  Des coll√®gues du r√©seau neuronal d√©terminent l'√¢ge et le sexe de la taille des coefficients de l'ordre de 50 Mo.  Et pendant l'optimisation pour la version mobile de pr√©cision r√©duite - environ 25 Mo (float32‚áífloat16). <br><br>  Dans le m√™me temps, le graphique de retard lors de l'acc√®s √† la m√©moire en fonction de la taille des donn√©es est distribu√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">approximativement comme ceci</a> (l'√©chelle horizontale est logarithmique): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1f4/2aa/134/1f42aa13427972cd1762383b33cfe974.png"></div><br><br>  C'est-√†-dire  avec une augmentation du volume de donn√©es de plus de 16 Mo, le retard augmente de 50 fois ou plus, ce qui affecte fatalement les performances.  En fait, la plupart du temps, le CPU, lorsqu'il travaille avec des r√©seaux de neurones profonds, attend <s>b√™tement les</s> donn√©es.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Les donn√©es d'Intel</a> sur l'acc√©l√©ration de divers r√©seaux sont int√©ressantes, o√π, en fait, l'acc√©l√©ration ne se produit que lorsque le r√©seau devient petit (par exemple, √† la suite de la quantification des poids), afin de commencer au moins partiellement √† entrer dans le cache avec les donn√©es trait√©es.  Notez que le cache d'un CPU moderne consomme jusqu'√† la moiti√© de l'√©nergie du processeur.  Dans le cas des r√©seaux neuronaux lourds, il est inefficace et fonctionne comme un appareil de chauffage excessivement cher. <br><br><div class="spoiler">  <b class="spoiler_title">Pour les adh√©rents des r√©seaux de neurones sur le CPU</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Selon</a> nos tests internes, m√™me <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intel OpenVINO</a> perd l'impl√©mentation de la matrice de multiplication matricielle + NNPACK sur de nombreuses architectures de r√©seaux (en particulier sur les architectures simples o√π la bande passante est importante pour le traitement des donn√©es en temps r√©el en mode monothread).  Un tel sc√©nario est pertinent pour divers classificateurs d'objets dans l'image (o√π le r√©seau de neurones doit √™tre ex√©cut√© un grand nombre de fois - 50 √† 100 en termes de nombre d'objets dans l'image) et la surcharge de d√©marrage d'OpenVINO devient d√©raisonnablement √©lev√©e. <br></div></div><br>  <b>Avantages:</b> <br><br><ul><li>  ¬´Tout le monde l'a¬ª et est g√©n√©ralement inactif, c'est-√†-dire  prix d' <i>entr√©e</i> relativement bas pour la facturation et la mise en ≈ìuvre. <br></li><li>  Il existe des r√©seaux non CV distincts qui s'int√®grent bien sur le CPU, les coll√®gues appellent, par exemple, Wide &amp; Deep et GNMT. <br></li></ul><br>  <b>Moins:</b> <br><ul><li>  Le processeur est inefficace lorsque vous travaillez avec des r√©seaux de neurones profonds (lorsque le nombre de couches r√©seau et la taille des donn√©es d'entr√©e sont importants), tout fonctionne douloureusement lentement. <br></li></ul><br><h2>  GPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/aee/06a/d5b/aee06ad5beab0896005e75769b3ba910.png"><br><br>  Le sujet est bien connu, nous d√©crivons donc bri√®vement l'essentiel.  Dans le cas des r√©seaux de neurones, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GPU</a> a un avantage de performance significatif dans les t√¢ches massivement parall√®les: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/717/03d/d9b/71703dd9bce82918d1e0c7138a09adf0.png"></div><br>  Faites attention √† la fa√ßon dont le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Xeon Phi 7290 √†</a> 72 c≈ìurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est</a> recuit, tandis que le ¬´bleu¬ª est √©galement le serveur Xeon, c'est-√†-dire  Intel n'abandonne pas si facilement, ce qui sera discut√© ci-dessous.  Mais plus important encore, la m√©moire des cartes vid√©o a √©t√© initialement con√ßue pour des performances environ 5 fois sup√©rieures.  Dans les r√©seaux de neurones, le calcul avec des donn√©es est extr√™mement simple.  Quelques actions √©l√©mentaires, et nous avons besoin de nouvelles donn√©es.  Par cons√©quent, la vitesse d'acc√®s aux donn√©es est critique pour le fonctionnement efficace d'un r√©seau neuronal.  Une m√©moire haute vitesse ¬´√† bord¬ª du GPU et un syst√®me de gestion du cache plus flexible que sur le CPU peuvent r√©soudre ce probl√®me: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/30a/d1b/07e/30ad1b07e7b22d97dc3372e4351c2e3c.png"></div><br><br>  Tim Detmers soutient la revue int√©ressante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">¬´Quels GPU obtenir pour le Deep Learning: mon exp√©rience et mes conseils pour l'utilisation des GPU dans le Deep Learning¬ª</a> depuis plusieurs ann√©es maintenant.  Il est clair que Tesla et Titans gouvernent la formation, bien que la diff√©rence d'architectures puisse provoquer des explosions int√©ressantes, par exemple dans le cas de r√©seaux de neurones r√©currents (et le leader en g√©n√©ral est TPU, note pour l'avenir): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6e6/bb0/43a/6e6bb043aab85679d1ddf01028e19728.png"></div><br>  Cependant, il existe un graphique de performance extr√™mement utile pour le dollar, o√π sur le cheval <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RTX</a> (probablement en raison de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">leurs</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">c≈ìurs Tensor</a> ), si vous avez suffisamment de m√©moire pour cela, bien s√ªr: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f13/338/032/f13338032ca812397cbbe0228bda9ed5.png"></div><br>  Bien s√ªr, le co√ªt de l'informatique est important.  La deuxi√®me place de la premi√®re note et la derni√®re de la seconde - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tesla V100</a> est vendue pour 700 mille roubles, comme 10 ordinateurs ¬´ordinaires¬ª (+ le commutateur Infiniband cher, si vous voulez vous entra√Æner sur plusieurs n≈ìuds).  Vrai V100 et fonctionne pour dix.  Les gens sont pr√™ts √† payer trop cher pour une acc√©l√©ration tangible de l'apprentissage. <br><br>  Total, r√©sumez! <br><br>  <b>Avantages:</b> <br><ul><li>  Cardinal - 10-100 fois - acc√©l√©ration par rapport au CPU. <br></li><li>  Extr√™mement efficace pour la formation (et un peu moins efficace pour l'utilisation). <br></li></ul><br>  <b>Moins:</b> <br><ul><li>  Le co√ªt des cartes vid√©o haut de gamme (qui ont suffisamment de m√©moire pour former de grands r√©seaux) d√©passe le co√ªt du reste de l'ordinateur ... <br></li></ul><br><h2>  FPGA </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d5/554/90b/0d555490b015730051341cc857d14606.png"><br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le FPGA</a> est d√©j√† plus int√©ressant.  Il s'agit d'un r√©seau de plusieurs millions de blocs programmables, que nous pouvons √©galement interconnecter par programmation.  Le r√©seau et les blocs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ressemblent</a> √† ceci (le goulot d'√©tranglement est le goulot d'√©tranglement, faites attention, encore une fois devant la m√©moire de la puce, mais c'est plus facile, ce qui sera d√©crit ci-dessous): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d8/8f9/789/0d88f9789f60720da892b7acae7ab8ec.png"><br>  Naturellement, il est logique d'utiliser FPGA d√©j√† au stade de l'utilisation d'un r√©seau neuronal (dans la plupart des cas, il n'y a pas assez de m√©moire pour la formation).  De plus, le sujet de l'ex√©cution sur FPGA a maintenant commenc√© √† se d√©velopper activement.  Par exemple, voici le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cadre fpgaConvNet</a> , qui peut acc√©l√©rer consid√©rablement l'utilisation de CNN sur les FPGA et r√©duire la consommation d'√©nergie. <br><br>  Le principal avantage du FPGA est que nous pouvons stocker le r√©seau directement dans les cellules, c'est-√†-dire  une tache mince sous la forme de centaines de m√©gaoctets des m√™mes donn√©es transf√©r√©es 25 fois par seconde (pour la vid√©o) dans la m√™me direction dispara√Æt comme par magie.  Cela permet une vitesse d'horloge inf√©rieure et l'absence de caches au lieu de performances inf√©rieures pour obtenir une augmentation notable.  Oui, et r√©duit consid√©rablement la consommation d'√©nergie pour le <s>r√©chauffement climatique</s> par unit√© de calcul. <br><br>  Intel a rejoint activement le processus en lan√ßant l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">an dernier</a> l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenVINO Toolkit</a> dans l'open source, qui inclut le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning Deployment Toolkit</a> (qui fait partie d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenCV</a> ).  De plus, les performances des FPGA sur diff√©rentes grilles semblent assez int√©ressantes, et l'avantage des FPGA par rapport aux GPU (bien que les GPU int√©gr√©s Intel) est assez important: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/8ba/a66/37e/8baa6637e904732e4e883a196b8d803d.png"></div><br>  Ce qui r√©chauffe particuli√®rement l'√¢me de l'auteur - Les FPS sont compar√©s, c'est-√†-dire  images par seconde est la mesure la plus pratique pour la vid√©o.  √âtant donn√© qu'Intel a achet√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Altera</a> , le deuxi√®me acteur du march√© des FPGA, en 2015, le graphique donne mati√®re √† r√©flexion. <br><br>  Et, √©videmment, la barri√®re d'entr√©e √† de telles architectures est plus √©lev√©e, donc un certain temps doit s'√©couler avant que des outils pratiques apparaissent qui prennent efficacement en compte l'architecture FPGA fondamentalement diff√©rente.  Mais sous-estimer le potentiel de la technologie n'en vaut pas la peine.  Elle enduit douloureusement de nombreux endroits minces. <br><br>  Enfin, nous soulignons que la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">programmation des FPGA</a> est un art distinct.  En tant que tel, le programme n'y est pas ex√©cut√© et tous les calculs sont effectu√©s en termes de flux de donn√©es, de retards de flux (qui affectent les performances) et de portes utilis√©es (qui font toujours d√©faut).  Par cons√©quent, afin de commencer une programmation efficace, vous devez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">changer</a> compl√®tement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">votre propre firmware</a> (dans le r√©seau neuronal qui se trouve entre vos oreilles).  Avec une bonne efficacit√©, cela n'est pas du tout obtenu.  Cependant, les nouveaux cadres cacheront bient√¥t la diff√©rence externe aux chercheurs. <br><br>  <b>Avantages:</b> <br><br><ul><li>  Ex√©cution r√©seau potentiellement plus rapide. <br></li><li>  Consommation d'√©nergie nettement inf√©rieure √† celle du CPU et du GPU (ceci est particuli√®rement important pour les solutions mobiles). <br></li></ul><br>  <b>Inconv√©nients:</b> <br><br><ul><li>  G√©n√©ralement, ils aident √† acc√©l√©rer l'ex√©cution; la formation sur eux, contrairement au GPU, est nettement moins pratique. <br></li><li>  Programmation plus complexe par rapport aux options pr√©c√©dentes. <br></li><li>  Sensiblement moins de sp√©cialistes. <br></li></ul><br><h2>  ASIC </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/dee/5e4/059/dee5e40597454e07651718c325f2ac3f.png"><br><br>  Vient ensuite l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ASIC</a> , qui est l'abr√©viation de Application-Specific Integrated Circuit, c'est-√†-dire  circuit int√©gr√© pour notre t√¢che.  Par exemple, r√©aliser un r√©seau neuronal en fer.  Cependant, la plupart des n≈ìuds de calcul peuvent fonctionner en parall√®le.  En fait, seules les d√©pendances de donn√©es et l'informatique in√©gale √† diff√©rents niveaux du r√©seau peuvent nous emp√™cher d'utiliser constamment toutes les ALU qui fonctionnent. <br><br>  Peut-√™tre que l'extraction de crypto-monnaie a fait la plus grande publicit√© ASIC parmi le grand public ces derni√®res ann√©es.  Au tout d√©but, l'exploitation mini√®re sur le CPU √©tait assez rentable, plus tard j'ai d√ª acheter un GPU, puis un FPGA, puis des ASIC sp√©cialis√©s, car les gens (lire - le march√©) ont m√ªri pour des commandes dans lesquelles leur production est devenue rentable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de1/fda/062/de1fda062c797262a51047f57eac2f11.png"><br>  Dans notre r√©gion, des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">services</a> sont √©galement apparus (naturellement!) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Qui</a> aident √† mettre un r√©seau de neurones sur le fer avec les caract√©ristiques n√©cessaires pour la consommation d'√©nergie, le FPS et le prix.  Magiquement, d'accord! <br><br>  MAIS!  Nous perdons la personnalisation du r√©seau.  Et, bien s√ªr, les gens y pensent aussi.  Par exemple, voici un article avec le dicton: ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Une architecture reconfigurable peut-elle battre ASIC en tant qu'acc√©l√©rateur CNN?</a> ¬ª (¬´Une architecture configurable peut-elle battre ASIC comme un acc√©l√©rateur CNN?¬ª).  Il y a suffisamment de travail sur ce sujet, car la question n'est pas vide.  Le principal inconv√©nient de l'ASIC est qu'apr√®s avoir converti le r√©seau en mat√©riel, il devient difficile pour nous de le changer.  Ils sont plus avantageux dans les cas o√π nous avons d√©j√† besoin d'un r√©seau qui fonctionne bien avec des millions de puces avec une faible consommation d'√©nergie et des performances √©lev√©es.  Et cette situation se d√©veloppe progressivement sur le march√© des voitures √† pilote automatique, par exemple.  Ou dans des cam√©ras de surveillance.  Ou dans les chambres des aspirateurs robotiques.  Ou dans les chambres d'un r√©frig√©rateur domestique.  Ou dans une chambre de cafeti√®re.  <s>Ou dans la chambre en fer.</s>  Eh bien, vous comprenez l'id√©e, en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bref</a> ! <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51a/b5e/949/51ab5e9497fee71cf9f23606d3de4fb6.png"></div><br><br>  Il est important que dans la production de masse, la puce soit bon march√©, fonctionne rapidement et consomme un minimum d'√©nergie. <br><br>  <b>Avantages:</b> <br><br><ul><li>  Le co√ªt de puce le plus bas par rapport √† toutes les solutions pr√©c√©dentes. <br></li><li>  Consommation √©lectrique la plus faible par unit√© de fonctionnement. <br></li><li>  Assez grande vitesse (y compris, si vous le souhaitez, un record). <br></li></ul><br>  <b>Inconv√©nients:</b> <br><br><ul><li>  Capacit√© tr√®s limit√©e de mettre √† jour le r√©seau et la logique. <br></li><li>  Co√ªt de d√©veloppement le plus √©lev√© par rapport √† toutes les solutions pr√©c√©dentes. <br></li><li>  L'utilisation de l'ASIC est rentable principalement pour les grandes s√©ries. <br></li></ul><br><h2>  TPU </h2><br>  Rappelez-vous que lorsque vous travaillez avec des r√©seaux, il y a deux t√¢ches - la formation et l'ex√©cution (inf√©rence).  Si les FPGA / ASIC sont principalement ax√©s sur l'acc√©l√©ration de l'ex√©cution (y compris certains r√©seaux fixes), alors TPU (Tensor Processing Unit ou tensor processors) est soit une acc√©l√©ration d'apprentissage bas√©e sur le mat√©riel, soit une acc√©l√©ration relativement universelle d'un r√©seau arbitraire.  Le nom est beau, d'accord, bien qu'en fait, des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tenseurs de</a> rang 2 avec une unit√© de multiplication mixte (MXU) connect√©e √† la m√©moire √† large bande passante (HBM) soient toujours utilis√©s.  Ci-dessous, le sch√©ma d'architecture de TPU Google 2e et 3e version: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f5e/29c/696/f5e29c696e265435fadbc2e56f67e12e.png"></div><br><h2>  TPU Google </h2><br>  En g√©n√©ral, Google a fait une publicit√© pour le nom du TPU, r√©v√©lant des d√©veloppements internes en 2017: <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4af/ea1/4ca/4afea14cad5dc0a7d941ae6b7963c171.png"></div><br>  Ils ont commenc√© les travaux pr√©liminaires sur les processeurs sp√©cialis√©s pour les r√©seaux de neurones avec leurs mots en 2006, en 2013, ils ont cr√©√© un projet avec un bon financement, et en 2015, ils ont commenc√© √† travailler avec les premi√®res puces qui ont beaucoup aid√© les r√©seaux de neurones pour le service cloud de Google Translate et plus encore.  Et ce fut, nous le soulignons, l'acc√©l√©ration du r√©seau.  Un avantage important pour les centres de donn√©es est une efficacit√© √©nerg√©tique TPU sup√©rieure de deux ordres de grandeur par rapport aux processeurs (graphique pour TPU v1): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/877/a91/f61/877a91f61ce45f2d04c7c3fe9df55348.png"></div><br>  De plus, en r√®gle g√©n√©rale, par rapport au GPU, les <i>performances du</i> r√©seau sont 10 √† 30 fois meilleures pour le mieux: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/10a/302/83b/10a30283b9c0fe8f0c7c1993f0bc68aa.png"></div><br>  La diff√©rence est m√™me 10 fois significative.  Il est clair que la diff√©rence avec le GPU 20-30 fois d√©termine le d√©veloppement de cette zone. <br><br>  Et, heureusement, Google n'est pas le seul. <br><br><h2>  TPU Huawei </h2><br>  Aujourd'hui, Huawei, qui souffre depuis longtemps, a √©galement commenc√© √† d√©velopper le TPU il y a plusieurs ann√©es sous le nom de Huawei Ascend, et en deux versions √† la fois - pour les centres de donn√©es (comme Google) et pour les appareils mobiles (ce que Google a √©galement commenc√© √† faire r√©cemment).  Si vous croyez aux mat√©riaux de Huawei, ils ont d√©pass√© le nouveau Google TPU v3 par FP16 2,5 fois et NVIDIA V100 2 fois: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/692/66a/bb869266a2cfa5e28c2f086026309b01.png"><br><br>  Comme d'habitude, une bonne question: comment se comportera cette puce sur des t√¢ches r√©elles.  Pour sur le graphique, comme vous pouvez le voir, les performances de pointe.  De plus, Google TPU v3 est bon √† bien des √©gards car il peut fonctionner efficacement dans des clusters de 1024 processeurs.  Huawei a √©galement annonc√© des clusters de serveurs pour l'Ascend 910, mais il n'y a pas de d√©tails.  En g√©n√©ral, les ing√©nieurs de Huawei se sont montr√©s extr√™mement comp√©tents au cours des 10 derni√®res ann√©es, et il y a toutes les chances que des performances de pointe 2,8 fois sup√©rieures √† celles de Google TPU v3, associ√©es √† la derni√®re technologie de processus √† 7 nm, soient utilis√©es dans le cas. <br><br>  La m√©moire et le bus de donn√©es sont essentiels pour les performances, et la diapositive montre qu'une attention consid√©rable a √©t√© accord√©e √† ces composants (y compris la vitesse de communication avec la m√©moire beaucoup plus rapide que celle du GPU): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/98e/7bd/ab1/98e7bdab1c9e1a81e891bb72db3976c2.png"><br><br>  La puce utilise √©galement une approche l√©g√®rement diff√©rente - pas des √©chelles MXU 128x128 bidimensionnelles, mais des calculs dans un cube tridimensionnel d'une taille plus petite 16x16xN, o√π N = {16.8,4,2,1}.  Par cons√©quent, la question cl√© est de savoir dans quelle mesure elle reposera sur l'acc√©l√©ration r√©elle de r√©seaux sp√©cifiques (par exemple, les calculs dans un cube conviennent aux images).  En outre, une √©tude approfondie de la diapositive montre que, contrairement √† Google, la puce int√®gre imm√©diatement le travail avec la vid√©o FullHD compress√©e.  Pour l'auteur, cela semble <b>tr√®s</b> encourageant! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme mentionn√© ci-dessus, dans la m√™me ligne, les processeurs sont d√©velopp√©s pour les appareils mobiles pour lesquels l'efficacit√© √©nerg√©tique est critique et sur lesquels le r√©seau sera principalement ex√©cut√© (c'est-√†-dire s√©par√©ment - processeurs pour l'apprentissage cloud et s√©par√©ment - pour l'ex√©cution): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea0/a3c/7cb/ea0a3c7cb72def7195afa2011ba02907.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">et avec ce param√®tre, tout Cela semble bien en comparaison avec NVIDIA au moins (notez qu'ils n'ont pas apport√© de comparaison avec Google, cependant, Google ne donne pas de TPU cloud √† ses mains). Et leurs puces mobiles concurrenceront les processeurs d'Apple, de Google et d'autres soci√©t√©s, mais il est trop t√¥t pour faire le point ici. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On voit clairement que les nouvelles puces Nano, Tiny et Lite devraient √™tre encore meilleures. Il devient clair </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pourquoi Trump avait peur</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pourquoi de nombreux fabricants examinent attentivement les succ√®s de Huawei (qui a surpass√© toutes les soci√©t√©s sid√©rurgiques am√©ricaines en termes de revenus, y compris Intel en 2018). </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> R√©seaux profonds analogiques </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme vous le savez, la technologie √©volue souvent dans une spirale, lorsque les approches anciennes et oubli√©es deviennent pertinentes dans un nouveau cycle. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelque chose de similaire pourrait tr√®s bien arriver aux r√©seaux de neurones. Vous avez peut-√™tre entendu dire qu'une fois les op√©rations de multiplication et d'addition effectu√©es par des tubes √©lectroniques et des transistors (par exemple, la conversion des espaces colorim√©triques - une multiplication typique des matrices - se faisait dans chaque t√©l√©viseur couleur jusqu'au milieu des ann√©es 90)? Une bonne question s'est pos√©e: si notre r√©seau de neurones est relativement r√©sistant aux calculs inexacts √† l'int√©rieur, que faire si nous convertissons ces calculs sous forme analogique? Nous obtenons imm√©diatement une acc√©l√©ration notable des calculs et une r√©duction potentiellement spectaculaire de la consommation d'√©nergie pour une op√©ration:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/6b2/e08/aff/6b2e08afff1639668e02a548ed663fba.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Avec cette approche, DNN (Deep Neural Network) est calcul√© rapidement et √©conomes en √©nergie. Mais il y a un probl√®me - il s'agit des convertisseurs DAC / ADC (DAC / ADC) - des convertisseurs num√©riques en analogiques et vice versa, ce qui r√©duit √† la fois l'efficacit√© √©nerg√©tique et la pr√©cision des processus. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, en 2017, IBM Research a </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">propos√© des CMOS analogiques</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> pour les RPU ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resistive Processing Units</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), qui vous permettent de stocker des donn√©es trait√©es √©galement sous forme analogique et d'augmenter consid√©rablement l'efficacit√© globale de l'approche:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/0d3/720/3a8/0d37203a85a89791e8701098583e01ea.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En outre, en plus de la m√©moire analogique, la r√©duction de la pr√©cision d'un r√©seau neuronal peut grandement aider - c'est la cl√© de la miniaturisation des RPU, ce qui signifie augmenter le nombre de cellules de calcul sur une puce. </font><font style="vertical-align: inherit;">Et ici, IBM est √©galement un leader, et en particulier, r√©cemment cette ann√©e, ils ont r√©ussi √† rendre le r√©seau √† une pr√©cision de 2 bits et vont apporter une pr√©cision √† un bit (et deux bits pendant la formation), ce qui permettra potentiellement 100 fois (!) D'augmenter la productivit√© par rapport √† GPU modernes:</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1c4/1eb/c09/1c41ebc09f5e53d206534eefbb105eac.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Il est trop t√¥t pour parler en d√©tail des neurochips analogiques, car alors que tout cela est test√© au niveau des premiers prototypes: </font></font><br><br><div style="text-align:center;"><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/498/f6c/bdb/498f6cbdb53f20806cf41a78b7110e8b.png"></div><br>  Cependant, la direction potentielle du calcul analogique semble <b>extr√™mement</b> int√©ressante. <br><br>  La seule chose qui brouille, c'est que c'est IBM, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">qui a d√©j√† d√©pos√© des dizaines de brevets sur le sujet</a> .  Selon l'exp√©rience, en raison des particularit√©s de la culture d'entreprise, elles coop√®rent relativement faiblement avec d'autres entreprises et, poss√©dant certaines technologies, sont plus susceptibles de ralentir son d√©veloppement entre autres que de la partager efficacement.  Par exemple, IBM a refus√© √† un moment donn√© d'accorder une licence de compression arithm√©tique pour JPEG au comit√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ISO</a> , malgr√© le fait que dans le projet de norme, il y avait une option avec compression arithm√©tique.  En cons√©quence, JPEG a pris vie avec la compression Huffman et 10 √† 15% pire qu'il ne pouvait.  La m√™me situation √©tait avec les normes de compression vid√©o.  Et l'industrie n'est massivement pass√©e √† la compression arithm√©tique dans les codecs que lorsque 5 brevets IBM ont expir√© 12 ans plus tard ... Esp√©rons qu'IBM sera plus enclin √† coop√©rer cette fois, et, en cons√©quence, nous <b>souhaitons un maximum de succ√®s dans le domaine √† tous ceux qui ne sont pas associ√©s √† IBM</b> , l'avantage de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ces personnes et de ces entreprises est consid√©rable</a> . <br><br>  Si cela fonctionne, <b>ce sera une r√©volution dans l'utilisation des r√©seaux de neurones et une r√©volution dans de nombreux domaines de l'informatique.</b> <br><br><h2>  Divers autres lettres </h2><br>  En g√©n√©ral, le th√®me de l'acc√©l√©ration des r√©seaux de neurones est devenu √† la mode, toutes les grandes entreprises et des dizaines de startups y sont impliqu√©es, et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">au moins 5 d'entre elles ont attir√© plus de 100 millions de dollars d'</a> investissements au d√©but de 2018.  Au total, en 2017, 1,5 Mds $ ont √©t√© investis dans des startups li√©es au d√©veloppement de puces.  Malgr√© le fait que les investisseurs n'aient pas remarqu√© les fabricants de puces pendant 15 bonnes ann√©es (car il n'y avait rien √† y attraper dans le contexte des g√©ants).  En g√©n√©ral - il y a maintenant une r√©elle chance pour une petite r√©volution du fer.  De plus, il est extr√™mement difficile de pr√©dire quelle architecture gagnera, le besoin de r√©volution a m√ªri et les possibilit√©s d'augmenter la productivit√© sont grandes.  La situation r√©volutionnaire classique a m√ªri: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Moore</a> ne peut plus et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dean</a> n'est pas encore pr√™t. <br><br>  Eh bien, puisque la loi de march√© la plus importante - √™tre diff√©rente, il y a beaucoup de nouvelles lettres, par exemple: <br><br><ul><li>  <b>Neural Processing Unit ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NPU</a> )</b> - Un neuroprocesseur, parfois magnifiquement - une puce neuromorphique - de mani√®re g√©n√©rale, le nom g√©n√©ral d'un acc√©l√©rateur de r√©seaux de neurones, appel√©s puces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Samsung</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Huawei</a> et plus sur la liste ... <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/d28/900/72e/d2890072e95d7e63f70543b09ba759bb.png"></div>  <i>Ci-apr√®s dans cette section, principalement des diapositives de pr√©sentations d'entreprise seront donn√©es √† titre d'exemples d' <b>auto-noms</b> technologiques</i> <br><br>  Il est clair qu'une comparaison directe est probl√©matique, mais voici quelques donn√©es int√©ressantes comparant les puces aux neuroprocesseurs d'Apple et de Huawei, produites par le TSMC mentionn√© au d√©but.  On peut voir que la concurrence est rude, la nouvelle g√©n√©ration montre une augmentation de productivit√© de 2 √† 8 fois et la complexit√© des processus technologiques: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ee/ga/iu/eegaiu5u_rw5trv0-exwjngc7sw.png"></div><br></li><li>  <b>Processeur de r√©seau neuronal (NNP)</b> - Processeur de r√©seau neuronal. <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/917/1bb/ad2/9171bbad26941f97399ce80a56373e51.png"></div><br>  C'est le nom de sa famille de puces, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Intel</a> (√† l'origine c'√©tait la soci√©t√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Nervana Systems</a> , qu'Intel a achet√©e en 2016 pour 400 millions de dollars +).  Cependant, dans les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">articles</a> et les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livres, le</a> nom NNP est √©galement assez courant. <br></li><li>  <b>Intelligence Processing Unit (IPU)</b> - un processeur intelligent - le nom des puces promues par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Graphcore</a> (soit dit en passant, qui a d√©j√† re√ßu un investissement de 310 millions de dollars). <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e93/18f/8e7/e9318f8e7711ea5b3a669946484c72bf.png"></div><br>  Il produit des cartes sp√©ciales pour ordinateurs, mais destin√©es √† la formation de r√©seaux de neurones, avec des performances de formation RNN 180 √† 240 fois sup√©rieures √† celles du NVIDIA P100. <br></li><li>  <b>Dataflow Processing Unit (DPU)</b> - processeur de traitement de donn√©es - le nom est promu par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WAVE Computing</a> , qui a d√©j√† re√ßu un investissement de 203 millions de dollars.  Il produit environ les m√™mes acc√©l√©rateurs que Graphcore: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f53/e5e/25a/f53e5e25abb2fc2fee636d1949242be8.png"></div><br>  Puisqu'ils ont re√ßu 100 millions de moins, ils d√©clarent que la formation n'est que 25 fois plus rapide que sur le GPU (bien qu'ils promettent que ce sera 1000 fois bient√¥t).  Voyons voir ... <br></li><li>  <b>Unit√© de traitement de la vision ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VPU</a> )</b> - Processeur de vision par ordinateur: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/80e/756/110/80e7561100a794bb005635899b06ec40.png"></div><br>  Le terme est utilis√© dans les produits de plusieurs soci√©t√©s, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Myriad X VPU</a> de Movidius (√©galement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">achet√© par Intel</a> en 2016). <br></li><li>  L'un des concurrents d'IBM (qui, rappelons-le, utilise le terme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RPU</a> ) - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Mythic</a> - <b>propose Analog DNN</b> , qui stocke √©galement le r√©seau dans la puce et une ex√©cution relativement rapide.  Jusqu'√† pr√©sent, ils n'ont que des promesses, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bien que s√©rieuses</a> : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/01e/ae6/253/01eae6253a1d7ef3e2dec9401857a33a.png"></div><br></li></ul><br>  Et cela ne r√©pertorie que les zones les plus importantes dans le d√©veloppement desquelles des centaines de millions ont √©t√© investis (ce qui est important dans le d√©veloppement du fer). <br><br>  En g√©n√©ral, comme on le voit, toutes les fleurs fleurissent rapidement.  Progressivement, les entreprises dig√©reront des milliards de dollars d'investissements (il faut g√©n√©ralement 1,5 √† 3 ans pour produire des puces), la poussi√®re se d√©posera, le leader deviendra clair, les gagnants √©criront, comme d'habitude, une histoire, et le nom de la technologie la plus performante du march√© sera g√©n√©ralement accept√©.  Cela s'est d√©j√† produit plus d'une fois (¬´IBM PC¬ª, ¬´Smartphone¬ª, ¬´Xerox¬ª, etc.). <br><br><h2>  Quelques mots sur la comparaison correcte </h2><br>  Comme d√©j√† indiqu√© ci-dessus, comparer correctement les performances des r√©seaux de neurones n'est pas facile.  C'est exactement pourquoi Google publie un graphique dans lequel TPU v1 fabrique le NVIDIA V100.  NVIDIA, voyant une telle honte, publie un calendrier o√π Google TPU v1 perd le V100.  (Alors!) Google publie le tableau suivant, o√π le V100 perd sur Google TPU v2 &amp; v3.  Et enfin, Huawei est le calendrier o√π tout le monde perd sur le Huawei Ascend, mais le V100 est meilleur que TPU v3.  Cirque, en bref.  Ce qui est caract√©ristique - <i>chaque</i> graphique <i>a sa propre</i> v√©rit√©! <br><br>  Les causes profondes de la situation sont claires: <br><br><ul><li>  Vous pouvez mesurer la vitesse d'apprentissage ou la vitesse d'ex√©cution (selon ce qui est le plus pratique). <br></li><li>  Il est possible de mesurer diff√©rents r√©seaux de neurones, car la vitesse d'ex√©cution / d'apprentissage de diff√©rents r√©seaux de neurones sur des architectures sp√©cifiques peut diff√©rer consid√©rablement en raison de l'architecture du r√©seau et de la quantit√© de donn√©es requises. <br></li><li>  Et vous pouvez mesurer les performances maximales de l'acc√©l√©rateur (peut-√™tre le plus abstrait de tous les ci-dessus). <br></li></ul><br>  Afin de mettre les choses en ordre dans ce zoo, le test <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MLPerf est apparu</a> , qui a maintenant la version 0.5 disponible, c'est-√†-dire  il est en train de d√©velopper une m√©thodologie de comparaison, qui devrait √™tre pr√©sent√©e √† la premi√®re version au <a href="">3√®me trimestre de cette ann√©e</a> : <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/d14/e35/575/d14e3557543c05b83b99f12dc9e572ce.png"></div><br>  √âtant donn√© que les auteurs sont l'un des principaux contributeurs de TensorFlow, il y a toutes les chances de savoir quelle est la meilleure fa√ßon de s'entra√Æner et √©ventuellement de l'utiliser (car la version mobile de TF sera tr√®s probablement √©galement incluse dans ce test au fil du temps). <br><br>  R√©cemment, l‚Äôorganisation internationale <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IEEE</a> , qui publie la troisi√®me partie de la litt√©rature technique mondiale sur la radio√©lectronique, les ordinateurs et le g√©nie √©lectrique, a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">interdit √† Huawei</a> le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">visage d‚Äôun</a> enfant et a rapidement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">annul√© l‚Äô</a> interdiction.  Huawei n'est pas encore dans le classement MLPerf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">actuel</a> , tandis que Huawei TPU est un concurrent s√©rieux des cartes Google TPU et NVIDIA (c'est-√†-dire qu'en plus des politiques, il y a des raisons √©conomiques d'ignorer Huawei, franchement).  Avec un int√©r√™t non dissimul√©, nous suivrons l'√©volution des √©v√©nements! <br><br><h2>  Tout au paradis!  Plus pr√®s des nuages! </h2><br>  Et comme il s'agissait de formation, il vaut la peine de dire quelques mots sur ses sp√©cificit√©s: <br><br><ul><li>  Avec le d√©part g√©n√©ralis√© de la recherche sur les r√©seaux de neurones profonds (avec des dizaines et des centaines de couches qui d√©chirent vraiment tout le monde), il a fallu broyer des centaines de m√©gaoctets de coefficients, ce qui a imm√©diatement rendu inefficaces tous les caches de processeur des g√©n√©rations pr√©c√©dentes.  Dans le m√™me temps, l'ImageNet classique <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">discute d'</a> une stricte corr√©lation entre la taille du r√©seau et sa pr√©cision (plus la valeur est √©lev√©e, meilleure est la droite, plus le r√©seau est grand, l'axe horizontal est logarithmique): <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5fa/788/84f/5fa78884f561b6f93dfa126be53376f4.png"></div><br></li><li>  Le processus de calcul √† l'int√©rieur du r√©seau neuronal suit un sch√©ma fixe, c'est-√†-dire  o√π toutes les ¬´ramifications¬ª et ¬´transitions¬ª (en termes du si√®cle dernier) auront lieu dans la grande majorit√© des cas est pr√©cis√©ment connue √† l'avance, ce qui laisse l'ex√©cution sp√©culative des instructions sans travail, ce qui augmente auparavant de mani√®re significative la productivit√©: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ss/-6/9n/ss-69n-vr5c3rszuvmhkaomtct0.png"></div><br>  Cela rend les m√©canismes de pr√©diction <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">superscalaire</a> entass√©s pour la ramification et les pr√©calculs des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©cennies</a> pr√©c√©dentes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d'</a> am√©lioration <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">du</a> processeur inefficaces (cette partie de la puce, malheureusement, contribue √©galement au r√©chauffement climatique plut√¥t comme DNN sur le cache DNN). <br></li><li>  De plus, l'entra√Ænement des r√©seaux de neurones est relativement faiblement <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mis √† l'√©chelle horizontalement</a> .  C'est-√†-dire  nous ne pouvons pas prendre 1000 ordinateurs puissants et obtenir une acc√©l√©ration d'apprentissage 1000 fois.  Et m√™me √† 100, nous ne pouvons pas (du moins jusqu'√† ce que le probl√®me th√©orique de la d√©t√©rioration de la qualit√© de la formation sur une grande taille du lot soit r√©solu).  En g√©n√©ral, il est assez difficile pour nous de distribuer quelque chose sur plusieurs ordinateurs, car d√®s que la vitesse d'acc√®s √† la m√©moire unifi√©e dans laquelle se trouve le r√©seau diminue, la vitesse de son apprentissage chute de fa√ßon catastrophique.  Par cons√©quent, si un chercheur a acc√®s gratuitement √† 1000 ordinateurs puissants, il les prendra certainement tous bient√¥t, mais tr√®s probablement (s'il n'y a pas d'infinibande + RDMA), il y aura de nombreux r√©seaux de neurones avec diff√©rents param√®tres hyper.  C'est-√†-dire  la dur√©e totale de la formation ne sera que plusieurs fois inf√©rieure √† celle d'un seul ordinateur.  L√†, il est possible de jouer avec les tailles du lot, la formation continue et d'autres nouvelles technologies √† la mode, mais la principale conclusion est oui, avec une augmentation du nombre d'ordinateurs, l'efficacit√© du travail et la probabilit√© d'obtenir un r√©sultat augmenteront, mais pas de mani√®re lin√©aire.  Et aujourd'hui, le temps d'un chercheur en science des donn√©es est cher et souvent si vous pouvez d√©penser beaucoup de voitures (bien que d√©raisonnable), mais obtenez une acc√©l√©ration - cela est fait (voir l'exemple avec 1, 2 et 4 V100 chers dans les nuages ‚Äã‚Äãjuste en dessous). <br></li></ul><br>  Exactement ces points expliquent pourquoi tant de gens se sont pr√©cipit√©s vers le d√©veloppement de fer sp√©cialis√© pour les r√©seaux de neurones profonds.  Et pourquoi ont-ils obtenu leurs milliards.  Il y a vraiment de la lumi√®re visible au bout du tunnel et pas seulement Graphcore (qui, rappelons-le, 240 fois l'entra√Ænement RNN acc√©l√©r√©). <br><br>  Par exemple, les messieurs d'IBM Research <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sont optimistes quant au</a> d√©veloppement de puces sp√©ciales qui augmenteront l'efficacit√© des calculs d'un ordre de grandeur apr√®s 5 ans (et apr√®s 10 ans de 2 ordres de grandeur, atteignant une augmentation de 1000 fois par rapport au niveau de 2016 sur ce graphique, bien que , en efficacit√© par watt, mais la puissance de base augmentera √©galement): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51c/81f/226/51c81f2267b99ba00231532e606cf7fd.png"></div><br>  Tout cela signifie l'apparition de pi√®ces de fer, dont la formation sera relativement rapide, mais qui co√ªtera cher, ce qui conduit naturellement √† l'id√©e de partager le temps d'utilisation de cette pi√®ce de fer co√ªteuse entre les chercheurs.  Et cette id√©e nous conduit aujourd'hui non moins naturellement au cloud computing.  Et la transition de l'apprentissage vers les nuages ‚Äã‚Äãse poursuit depuis longtemps. <br><br>  Notez que maintenant la formation des m√™mes mod√®les peut diff√©rer dans le temps d'un ordre de grandeur de diff√©rents services cloud.  Amazon m√®ne en t√™te et Colab gratuit de Google vient en dernier.  Veuillez noter comment le r√©sultat du nombre de V100 change parmi les leaders - une augmentation du nombre de cartes de 4 fois (!) Augmente la productivit√© de moins d'un tiers (!!!) du bleu au violet, et Google a encore moins: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/78d/892/d83/78d892d833c0858572548aee7fca2697.png"></div><br>  Il semble que dans les ann√©es √† venir, la diff√©rence atteindra deux ordres de grandeur.  Seigneur!  Cuisiner de l'argent!  Nous rendrons √† l'amiable des investissements de plusieurs milliards aux investisseurs les plus performants ... <br><br><h2>  Bref </h2><br>  Essayons de r√©sumer les points cl√©s de la tablette: <br><div class="scrollable-table"><table><tbody><tr><td>  Tapez <br></td><td>  Ce qui acc√©l√®re <br></td><td>  Commentaire <br></td></tr><tr><td>  CPU <br></td><td>  Faire essentiellement <br></td><td>  Habituellement le pire en termes de vitesse et d'efficacit√© √©nerg√©tique, mais tout √† fait adapt√© √† la r√©alisation de r√©seaux neuronaux de petite taille <br></td></tr><tr><td>  GPU <br></td><td>  Ex√©cution + <br>  formation <br></td><td>  La solution la plus universelle, mais assez ch√®re, √† la fois en termes de co√ªt de calcul et d'efficacit√© √©nerg√©tique <br></td></tr><tr><td>  FPGA <br></td><td>  Accomplissement <br></td><td>  Une solution relativement universelle pour l'ex√©cution des r√©seaux, dans certains cas, peut acc√©l√©rer consid√©rablement la mise en ≈ìuvre <br></td></tr><tr><td>  ASIC <br></td><td>  Accomplissement <br></td><td>  La version du r√©seau la moins ch√®re, la plus rapide et la plus √©conome en √©nergie, mais de gros tirages sont n√©cessaires <br></td></tr><tr><td>  TPU <br></td><td>  Ex√©cution + <br>  formation <br></td><td>  Les premi√®res versions ont √©t√© utilis√©es pour acc√©l√©rer l'ex√©cution, maintenant elles sont utilis√©es pour acc√©l√©rer tr√®s rapidement l'ex√©cution et la formation <br></td></tr><tr><td>  IPU, DPU ... NNP <br></td><td>  Surtout la formation <br></td><td>  De nombreuses lettres marketing qui seront s√ªrement oubli√©es dans les ann√©es √† venir.  Le principal avantage de ce zoo est la v√©rification des diff√©rentes directions de l'acc√©l√©ration DNN <br></td></tr><tr><td>  DNN / RPU analogique <br></td><td>  Ex√©cution + <br>  formation <br></td><td>  Les acc√©l√©rateurs potentiellement analogiques peuvent r√©volutionner la vitesse et l'efficacit√© √©nerg√©tique des r√©seaux de neurones performants et d'entra√Ænement <br></td></tr></tbody></table></div><br><h2>  Quelques mots sur l'acc√©l√©ration logicielle </h2><br>  En toute justice, nous mentionnons qu'aujourd'hui le grand sujet est l'acc√©l√©ration logicielle de l'ex√©cution et de la formation des r√©seaux de neurones profonds.  L'ex√©cution peut √™tre consid√©rablement acc√©l√©r√©e principalement en raison de la soi-disant quantification du r√©seau.  Peut-√™tre est-ce d'abord parce que la plage de poids utilis√©e n'est pas si grande et qu'il est souvent possible de grossir les poids d'une valeur √† virgule flottante de 4 octets √† un entier de 1 octet (et, se souvenant des succ√®s d'IBM, encore plus fort).  Deuxi√®mement, le r√©seau form√© dans son ensemble est assez r√©sistant au bruit de calcul et la pr√©cision de la transition vers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">int8</a> diminue l√©g√®rement.  Dans le m√™me temps, malgr√© le fait que le nombre d'op√©rations peut m√™me augmenter (en raison de la mise √† l'√©chelle lors du calcul), le fait que le r√©seau est r√©duit de 4 fois et peut √™tre consid√©r√© comme des op√©rations vectorielles rapides augmente consid√©rablement la vitesse globale d'ex√©cution.  Ceci est particuli√®rement important pour les applications mobiles, mais cela fonctionne √©galement dans les nuages ‚Äã‚Äã(un exemple d'ex√©cution acc√©l√©r√©e dans les nuages ‚Äã‚ÄãAmazon): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e52/bbd/2cc/e52bbd2cc36e24eff9a788c2d8371c83.png"></div><br>  Il existe d'autres fa√ßons d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">acc√©l√©rer</a> algorithmiquement l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ex√©cution du r√©seau</a> et encore plus de fa√ßons d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">acc√©l√©rer l'apprentissage</a> .  Cependant, ce sont de grands sujets distincts sur lesquels pas cette fois. <br><br><h2>  Au lieu d'une conclusion </h2><br>  Dans ses conf√©rences, l'investisseur et auteur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tony Ceba en</a> donne un magnifique exemple: en 2000, le supercalculateur n ¬∞ 1 d'une capacit√© de 1 t√©raflops occupait 150 m√®tres carr√©s, co√ªtait 46 millions de dollars et consommait 850 kW: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/2v/6y/xe/2v6yxeo59aewcqngaybonbftsae.png"></div><br>  15 ans plus tard, le GPU NVIDIA avec une performance de 2,3 t√©raflops (2 fois plus) tient dans la main, co√ªte 59 $ (une am√©lioration d'environ un million de fois) et consomme 15 watts (une am√©lioration de 56 mille fois): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/gx/wz/qq/gxwzqqj1si3e3ho33nnypq_au3w.png"></div><br>  En mars de cette ann√©e, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Google a introduit les pods TPU</a> , qui sont en fait des superordinateurs refroidis par liquide bas√©s sur TPU v3, dont la principale caract√©ristique est qu'ils peuvent fonctionner ensemble dans des syst√®mes de 1024 TPU.  Ils ont l'air assez impressionnants: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/37c/849/2ec/37c8492ec7e380c5a26f8a420fc591d9.png"></div><br>  Les donn√©es exactes ne sont pas donn√©es, mais il est dit que le syst√®me est comparable aux Top-5 supercalculateurs dans le monde.  TPU Pod peut consid√©rablement augmenter la vitesse d'apprentissage des r√©seaux de neurones.  Pour augmenter la vitesse d'interaction, les TPU sont connect√©s par des lignes √† grande vitesse dans une structure toro√Ødale: <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/0f0/e26/532/0f0e2653272b633b8af1b6103540e95c.gif"><br>  Il semble qu'apr√®s 15 ans, ce neuroprocesseur deux fois plus puissant pourra √©galement tenir dans votre main, comme le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processeur Skynet</a> (vous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">devez</a> admettre, c'est quelque chose de similaire): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/cf2/5da/db6/cf25dadb60a350332cfef6bd147ec91b.png"></div>  <i>Tir√© de la version r√©alisatrice du film <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Terminator 2"</a></i> <br><br>  Compte tenu du taux actuel d'am√©lioration des acc√©l√©rateurs mat√©riels des r√©seaux de neurones profonds et de l'exemple ci-dessus, cela est tout √† fait r√©el.  Il y a toutes les chances en quelques ann√©es de prendre une puce avec une performance comme le TPU Pod d'aujourd'hui. <br><br>  Soit dit en passant, il est amusant que dans le film, les fabricants de puces (apparemment, imaginant o√π le r√©seau d'auto-formation pourrait mener) aient d√©sactiv√© le recyclage par d√©faut.  De mani√®re caract√©ristique, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">T-800</a> lui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">-</a> m√™me ne pouvait pas activer le mode de formation et fonctionnait en mode d'inf√©rence (voir la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">version de mise en sc√®ne</a> plus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">longue</a> ).  De plus, son <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processeur neural-net</a> √©tait avanc√© et lors de l'activation du recyclage, il pouvait utiliser les donn√©es pr√©c√©demment accumul√©es pour mettre √† jour le mod√®le.  Pas mal pour 1991. <br><br>  Ce texte a √©t√© commenc√© dans le chaud 13 millioni√®me Shenzhen.  Je me suis assis dans l'un des 27 000 taxis √©lectriques de la ville et j'ai regard√© avec grand int√©r√™t les 4 √©crans √† cristaux liquides de la voiture.  Un petit - parmi les appareils devant le conducteur, deux - au centre dans le tableau de bord et le dernier - translucide - dans le r√©troviseur, combin√© avec un DVR, une cam√©ra de vid√©osurveillance et un andro√Øde √† bord (√† en juger par la ligne sup√©rieure avec le niveau de charge et de communication avec le r√©seau).  Il affichait les donn√©es du conducteur (de qui se plaindre, le cas √©ch√©ant), de nouvelles pr√©visions m√©t√©orologiques et il semblait y avoir un lien avec la flotte de taxis.  Le conducteur ne connaissait pas l'anglais et n'a pas r√©ussi √† l'interroger sur ses impressions sur la machine √©lectrique.  Par cons√©quent, il a appuy√© paresseusement sur la p√©dale, d√©pla√ßant l√©g√®rement la voiture dans un embouteillage.  Et j'ai regard√© la fen√™tre avec un look futuriste avec int√©r√™t - les Chinois dans leurs vestes conduisaient du travail sur des scooters √©lectriques et des monowheels ... et je me demandais √† quoi tout cela ressemblerait dans 15 ans ... <br><br>  En fait, d√©j√† aujourd'hui, le r√©troviseur, utilisant les donn√©es de la cam√©ra du DVR et l' <i>acc√©l√©ration mat√©rielle des r√©seaux de neurones</i> , est tout √† fait capable de contr√¥ler la voiture dans la circulation et de tracer la route.  Dans l'apr√®s-midi, au moins).  Apr√®s 15 ans, le syst√®me sera clairement non seulement capable de conduire une voiture, mais sera √©galement ravi de me fournir les caract√©ristiques des v√©hicules √©lectriques chinois frais.  En russe, naturellement (en option: anglais, chinois ... albanais, enfin).  Le chauffeur ici est superflu, mal form√©, un maillon. <br><br>  Seigneur!  <b>EXTR√äMEMENT INT√âRESSANT</b> 15 ans nous attendent! <br><br>  Restez √† l'√©coute! <br><br>  Je reviens!  ))) <br><br><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/3e8/caf/2cd/3e8caf2cde12f7b9bce6cd64de106357.png"><br><br>  <b>UPD:</b> Les commentaires les plus int√©ressants: <br>  √Ä propos de la quantification et de l'acc√©l√©ration des calculs sur FPGA <br><div class="spoiler">  <b class="spoiler_title">Commentaires @Mirn</b> <div class="spoiler_text"><br>  Sur FPGA, non seulement l'arithm√©tique de la pr√©cision arbitraire est disponible, mais aussi l'enfer d'une capacit√© importante pour enregistrer et traiter des donn√©es de bits arbitraires.  Par exemple, il y a trop de coefficients dans les ennuyeux MobileNetV2 W et B et vous pouvez les quantifier sans trop de perte de pr√©cision √† seulement 16 bits, ou vous devrez vous recycler.  Mais si vous regardez √† l'int√©rieur et collectez des statistiques sur les canaux et les couches, vous pouvez voir que les 16 bits sont utilis√©s uniquement √† l'entr√©e des premiers coefficients de 1000 W, les autres ont 8-11 bits, dont seulement 2-3 bits et signes les plus significatifs sont vraiment importants, et des statistiques sur l'utilisation des canaux de sorte qu'il existe de nombreux canaux o√π g√©n√©ralement des z√©ros, ou de petites valeurs, ou des canaux o√π presque toutes les valeurs sont de 8 √† 11 bits, c'est-√†-dire  il est possible de clouer l'exposant dans les ongles au moment de la compilation et de ne pas le stocker, c'est-√†-dire  en fait, il est possible de stocker dans la m√©moire ROM non pas des valeurs 16 bits mais 4 bits, et vous pouvez m√™me stocker l'int√©gralit√© du r√©seau neuronal sur des FPGA bon march√© sans beaucoup de perte de pr√©cision (moins de 1%), et √©galement traiter √† des vitesses allant jusqu'√† des dizaines de milliers de FPS avec une latence de sorte que nous obtenons imm√©diatement une r√©ponse du r√©seau neuronal Comment se termine la r√©ception de la trame. <br><br>  √Ä propos de la quantification: mon id√©e est que si √† un certain nombre d'√©tapes de calcul de W, les coefficients pour le canal n ¬∞ 0 ne changent que de +50 √† -50, alors il est logique de compresser le bitness √† 7, et si de -123 √† +124 par exemple, puis √† 8 (y compris le signe )<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quoi qu'il en soit, √† l'int√©rieur du FPGA, tous les canaux sont calcul√©s en parall√®le par cycle, et ces blocs de m√©moire de 7, 8, etc. sont combin√©s en un grand morceau parall√®le de m√©moire ROM avec une taille de bits de l'ordre de plusieurs dizaines de milliers de bits. </font><font style="vertical-align: inherit;">Et bien plus encore, si leurs bits de poids faible n'affectent pas fortement le r√©sultat ou la pr√©cision, ils s'inclinent √©galement en place.</font></font><br><br>           (,  , ),  RTL            ,          ,      .        GCC  AVX256    bitperfect (    FPGA  )           FPS      (       W  B,         ). <br><br>            W  fc   , ..      -100  +100   +10000      255      9        ( ). <br><br>                  !  parce que dephwise    . <br><br>        u-law       (  !                ). <br><br>  ,          ,   6,      ,       . <br><br>                          (         ).  ‚Äî   ,    FixedPoint  dot product  ‚Äî     Fractional part,         ‚Äî       ,    ,      fc             . </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √Ä propos de la compilation automatique optimale des mod√®les sur GPU, FPGA, ASIC et autres mat√©riels </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Commentaire de @BigPack</font></font></b> <div class="spoiler_text"><br> -        TVM   ( tvm.ai/about),      (   Keras)    .   ,      ‚Äî  ¬´¬ª-  (bare metal,    ISA, FPGA  .)       edge computing.       TVM   HLS  TVM    FPGA.  HLS     FPGA ¬´¬ª    ,  ( )      FPGA    ,    GPU/TPU . <br><br> PS  FPGA    transparent hardware (  ‚Äî open-source hardware),        ,          (    ¬´¬ª )    .     -.  , FPGA        ‚Äî         </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A propos de l'annonce des innovations dans l'architecture FPGA, l'utilisation de Microsoft FPGA et l'optimisation automatique des r√©seaux de neurones </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grands commentaires @ Brak0del</font></font></b> <div class="spoiler_text"><br>    FPGA,   2019       ,      .       ‚Äî   .   /   dsp-  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Xilinx</a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Achronix</a> ,       DDR. <br><br>   , ,    , FPGA    ASIC-.   FPGA     :        ,       ASIC     ,  FPGA     -    .  C'est-√†-dire     -    .            , ASIC-, ,     .  ,        FPGA  ,   ASIC. <br><br> ,    CPU, FPGA           ,      ,        . <br> ,    GPU      ,   FPGA    ,     :  , - ,    GPU      ,       ,     , -   (     ,   ,  ,   ,  FPGA   ,   GPU     , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  </a> ).  , FPGA       ,   ,   ,    ASIC-. <br><br>       Microsoft ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> Catapult v.2</a> ),       FPGA-.  ,        FPGA.      ()    . <br><br>       FPGA         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ristretto</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deephi</a> ,      ,  Deephi       FPGA.   ,     ,         ,  . <br>   FPGA              . </div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √Ä propos de l'√©conomie du d√©veloppement FPGA par rapport √† ASIC </font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Commentaire de @Mirn</font></font></b> <div class="spoiler_text"><br>  ,  FPGA   : <br>     ,            ASIC. <br><br>  : <br> <b>FPGA</b> <br>     (  ),     (   , ,   IP      30-50   5     ). <br>   ,     10       (    ),    5*(N+1) <br>   , ,     ‚Äî    10     ,          ,   120*N <br>        (    ,     ‚Äî   ) <br>   : (120+50+5)*N,  5   880  <br>          <br><br> <b>ASIC</b> <br>              (    2 ) <br>             <br>           (3-4 ) <br>  ASIC      ¬´ ¬ª       ‚Äî    :     ,     <br>  ,      (             ), ,     ‚Äî         ,       . <br>  :          ‚Äî     ,    ,        . <br><br>                    (     MiT ‚Äî   ,          ,           ,    ) <br><br>     ,      ,    10              3-5 ,      (  ‚Äî   ,     ,   ‚Äî      ,   ‚Äî       )          ,    :      . <br>     ! !    .  NEC  SONY (c      ,        10-15        ,    ) <br><br> : FPGA             ASIC. </div></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Remerciements</font></font></b> <div class="spoiler_text">    : <br><br><ul><li>      . ..           , <br></li><li>  ,     ,    , <br></li><li>   ,      ,       , <br></li><li> , ,    ,  ,  ,  ,  ,  ,  ,  ,  ,            ,     ! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr455353/">https://habr.com/ru/post/fr455353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr455341/index.html">Que sait-on de la certification ITIL 4</a></li>
<li><a href="../fr455343/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 9. Le monde physique des interrupteurs. 2e partie</a></li>
<li><a href="../fr455345/index.html">Attention docteur</a></li>
<li><a href="../fr455347/index.html">Interfaces fonctionnelles ... en VBA</a></li>
<li><a href="../fr455351/index.html">VMware EMPOWER 2019 - les principales annonces et conclusions de la conf√©rence</a></li>
<li><a href="../fr455355/index.html">R√©seaux de t√©l√©vision par c√¢ble pour les plus petits. Partie 8: r√©seau dorsal optique</a></li>
<li><a href="../fr455359/index.html">Swift fonctionnel est facile</a></li>
<li><a href="../fr455361/index.html">Nous faisons une extension de navigateur qui v√©rifie les r√©sultats de l'examen</a></li>
<li><a href="../fr455369/index.html">Certification des administrateurs de bases de donn√©es et bien plus encore lors de l'anniversaire DevConfX (21-22 juin √† Moscou)</a></li>
<li><a href="../fr455371/index.html">Source de courant stable de 5 ŒºA √† 20 mA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>