<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìâ üèê üßëüèø‚Äçü§ù‚Äçüßëüèæ CS231n: Redes neurais convolucionais para reconhecimento de padr√µes üõê üñ≤Ô∏è üë©üèº‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bem-vindo a uma das palestras em CS231n: Redes neurais convolucionais para reconhecimento visual . 



 Conte√∫do 


- Vis√£o geral da arquitetura 
- Ca...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n: Redes neurais convolucionais para reconhecimento de padr√µes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p>  Bem-vindo a uma das palestras em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CS231n: Redes neurais convolucionais para reconhecimento visual</a> . </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1>  Conte√∫do </h1><br><ul><li>  Vis√£o geral da arquitetura </li><li>  Camadas em uma rede neural convolucional <br>  - camada convolucional <br>  - Subamostragem de camada <br>  - Camada de normaliza√ß√£o <br>  - camada totalmente conectada <br>  - Converta camadas totalmente conectadas em camadas convolucionais </li><li>  Arquitetura de rede neural convolucional <br>  - Modelos de camada <br>  - Padr√µes de tamanho de camada <br>  - Estudo de caso (LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet) <br>  - Aspectos computacionais </li><li>  Leitura adicional </li></ul><br><h1>  Redes neurais convolucionais (CNN / ConvNets) </h1><br><p>  As redes neurais convolucionais s√£o muito semelhantes √†s redes neurais usuais que estudamos no cap√≠tulo anterior (referindo-se ao cap√≠tulo anterior do curso CS231n): elas consistem em neur√¥nios que, por sua vez, cont√™m pesos e deslocamentos vari√°veis.  Cada neur√¥nio recebe alguns dados de entrada, calcula o produto escalar e, opcionalmente, usa uma fun√ß√£o de ativa√ß√£o n√£o linear.  Toda a rede, como antes, √© a √∫nica fun√ß√£o de avalia√ß√£o diferenci√°vel: do conjunto inicial de pixels (imagem) em uma extremidade √† distribui√ß√£o de probabilidade de pertencer a uma classe espec√≠fica na outra extremidade.  Essas redes ainda t√™m uma fun√ß√£o de perda (por exemplo, SVM / Softmax) na √∫ltima camada (totalmente conectada), e todas as dicas e recomenda√ß√µes fornecidas no cap√≠tulo anterior sobre redes neurais comuns tamb√©m s√£o relevantes para redes neurais convolucionais. </p><br><p>  Ent√£o o que mudou?  A arquitetura das redes neurais convolucionais envolve explicitamente a obten√ß√£o de imagens na entrada, o que nos permite levar em considera√ß√£o certas propriedades dos dados de entrada na pr√≥pria arquitetura de rede.  Essas propriedades permitem implementar a fun√ß√£o de distribui√ß√£o direta com mais efici√™ncia e reduzir bastante o n√∫mero total de par√¢metros na rede. </p><br><h1>  Vis√£o geral da arquitetura </h1><br><p>  Recordamos redes neurais comuns.  Como vimos no cap√≠tulo anterior, as redes neurais recebem dados de entrada (um √∫nico vetor) e os transformam "empurrando" atrav√©s de uma s√©rie de <em>camadas ocultas</em> .  Cada camada oculta consiste em um certo n√∫mero de neur√¥nios, cada um dos quais est√° conectado a todos os neur√¥nios da camada anterior e onde os neur√¥nios de cada camada s√£o completamente independentes de outros neur√¥nios no mesmo n√≠vel.  A √∫ltima camada totalmente conectada √© chamada de "camada de sa√≠da" e, nos problemas de classifica√ß√£o, √© a distribui√ß√£o de notas por classe. </p><br><p>  <em>As redes neurais convencionais n√£o s√£o bem dimensionadas para imagens maiores</em> .  No conjunto de dados CIFAR-10, as imagens t√™m tamanho 32x32x3 (32 pixels de altura, 32 pixels de largura, 3 canais de cores).  Para processar essa imagem, um neur√¥nio totalmente conectado na primeira camada oculta de uma rede neural normal ter√° 32x32x3 = 3072 pesos.  Essa quantidade ainda √© aceit√°vel, mas torna-se aparente que essa estrutura n√£o funcionar√° com imagens maiores.  Por exemplo, uma imagem maior - 200x200x3 far√° com que o n√∫mero de pesos se torne 200x200x3 = 120.000. Al√©m disso, precisaremos de mais de um desses neur√¥nios, para que o n√∫mero total de pesos comece a crescer rapidamente.  Torna-se √≥bvio que a conectividade √© excessiva e um grande n√∫mero de par√¢metros levar√° rapidamente a rede a reciclagem. </p><br><p>  <em>Representa√ß√µes em 3D de neur√¥nios</em> .  As redes neurais convolucionais usam o fato de que os dados de entrada s√£o imagens e, portanto, formam uma arquitetura mais sens√≠vel para esse tipo de dado.  Em particular, diferentemente das redes neurais comuns, as camadas da rede neural convolucional organizam os neur√¥nios em 3 dimens√µes - largura, altura, profundidade ( <em>Nota</em> : a palavra "profundidade" refere-se √† 3¬™ dimens√£o dos neur√¥nios de ativa√ß√£o, e n√£o √† profundidade da pr√≥pria rede neural medida em n√∫mero de camadas).  Por exemplo, as imagens de entrada do conjunto de dados CIFAR-10 s√£o dados de entrada em uma representa√ß√£o 3D, cuja dimens√£o √© 32x32x3 (largura, altura, profundidade).  Como veremos mais adiante, os neur√¥nios em uma camada ser√£o associados a um pequeno n√∫mero de neur√¥nios na camada anterior, em vez de serem conectados a todos os neur√¥nios anteriores na camada.  Al√©m disso, a camada de sa√≠da para a imagem do conjunto de dados CIFAR-10 ter√° uma dimens√£o de 1 √ó 1 √ó 10, porque ao se aproximar do final da rede neural reduziremos o tamanho da imagem a um vetor de estimativas de classe localizadas ao longo da profundidade (3¬™ dimens√£o). </p><br><p>  Visualiza√ß√£o: </p><br><div class="scrollable-table"><table><thead><tr><th>  Rede neural padr√£o </th><th>  Rede Neural Convolucional </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>Lado esquerdo:</em> rede neural padr√£o de 3 camadas. <br>  <em>No lado direito: a</em> rede neural convolucional possui seus neur√¥nios em 3 dimens√µes (largura, altura, profundidade), como mostrado em uma das camadas.  Cada camada de rede neural convolucional converte uma representa√ß√£o 3D da entrada em uma representa√ß√£o 3D da sa√≠da como neur√¥nios de ativa√ß√£o.  Neste exemplo, a camada de entrada vermelha cont√©m a imagem, portanto seu tamanho ser√° igual ao tamanho da imagem e a profundidade ser√° de 3 (tr√™s canais - vermelho, verde, azul). </p><br><blockquote>  A rede neural convolucional consiste em camadas.  Cada camada √© uma API simples: converte a representa√ß√£o 3D de entrada na representa√ß√£o 3D de sa√≠da de uma fun√ß√£o diferenci√°vel, que pode ou n√£o conter par√¢metros. </blockquote><br><h1>  Camadas usadas para construir redes neurais convolucionais </h1><br><p>  Como j√° descrevemos acima, uma rede neural convolucional simples √© um conjunto de camadas, em que cada camada converte uma representa√ß√£o em outra usando uma fun√ß√£o diferenci√°vel.  Utilizamos tr√™s tipos principais de camadas para construir redes neurais convolucionais: uma <em>camada convolucional</em> , <em>uma</em> <em>camada de</em> <em>subamostragem</em> e uma <em>camada totalmente conectada</em> (o mesmo que usamos em uma rede neural normal).  Organizamos essas camadas seq√ºencialmente para obter a arquitetura SNA. </p><br><p> <em>Exemplo de arquitetura: vis√£o geral.</em>  A seguir, nos aprofundaremos nos detalhes, mas, por enquanto, para o conjunto de dados CIFAR-10, a arquitetura de nossa rede neural convolucional pode ser <code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> .  Agora com mais detalhes: </p><br><ul><li>  <code>INPUT</code> [32x32x3] conter√° os valores originais dos pixels da imagem; no nosso caso, a imagem tem 32px de largura, 32px de altura e 3 canais de cor R, G, B. </li><li>  <code>CONV</code> camada <code>CONV</code> produzir√° um conjunto de neur√¥nios de sa√≠da que ser√£o associados √† √°rea local da imagem da fonte de entrada;  cada um desses neur√¥nios calcular√° o produto escalar entre seus pesos e a pequena parte da imagem original √† qual est√° associado.  O valor de sa√≠da pode ser uma representa√ß√£o 3D de <code>323212</code> , se, por exemplo, decidirmos usar 12 filtros. </li><li>  <code>RELU</code> camada <code>RELU</code> aplicar√° a fun√ß√£o de ativa√ß√£o do elemento <code>max(0, x)</code> .  Essa convers√£o n√£o altera a dimens√£o dos dados - <code>[32x32x12]</code> . </li><li>  <code>POOL</code> camada <code>POOL</code> realizar√° a opera√ß√£o de amostragem da imagem em duas dimens√µes - altura e largura, o que, como resultado, nos dar√° uma nova representa√ß√£o 3D <code>[161612]</code> . </li><li>  <code>FC</code> camada <code>FC</code> (camada totalmente conectada) calcular√° as notas por classes, a dimens√£o resultante ser√° <code>[1x1x10]</code> , onde cada um dos 10 valores corresponder√° √†s notas de uma classe espec√≠fica entre as 10 categorias de imagens do CIFAR-10.  Como nas redes neurais convencionais, cada neur√¥nio dessa camada ser√° associado a todos os neur√¥nios da camada anterior (representa√ß√£o 3D). </li></ul><br><p>  √â assim que a rede neural convolucional transforma a imagem original, camada por camada, do valor inicial do pixel √† estimativa final da classe.  Observe que algumas camadas cont√™m op√ß√µes e outras n√£o.  Em particular, as camadas <code>CONV/FC</code> realizam uma transforma√ß√£o, que n√£o √© apenas uma fun√ß√£o que depende dos dados de entrada, mas tamb√©m depende dos valores internos de pesos e deslocamentos nos pr√≥prios neur√¥nios.  <code>RELU/POOL</code> camadas <code>RELU/POOL</code> , <code>RELU/POOL</code> outro lado, usam fun√ß√µes n√£o parametrizadas.  Os par√¢metros nas camadas <code>CONV/FC</code> ser√£o treinados por descida gradiente para que a entrada receba os r√≥tulos de sa√≠da corretos correspondentes. </p><br><p>  Para resumir: </p><br><ul><li>  A arquitetura da rede neural convolucional, em sua representa√ß√£o mais simples, √© um conjunto ordenado de camadas que transforma a representa√ß√£o de uma imagem em outra representa√ß√£o, por exemplo, estimativas de associa√ß√£o de classe. </li><li>  Existem v√°rios tipos diferentes de camadas (CONV - camada convolucional, FC - totalmente conectado, fun√ß√£o de ativa√ß√£o RELU - fun√ß√£o, POOL - camada de subamostra - a mais popular). </li><li>  Cada camada de entrada recebe uma representa√ß√£o 3D e a converte em uma representa√ß√£o 3D de sa√≠da usando uma fun√ß√£o diferenci√°vel. </li><li>  Cada camada pode e pode n√£o ter par√¢metros (CONV / FC - possui par√¢metros, RELU / POOL - no). </li><li>  Cada camada pode e pode n√£o ter hiper par√¢metros (CONV / FC / POOL - have, RELU - no) </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>A representa√ß√£o inicial cont√©m os valores de pixel da imagem (√† esquerda) e estimativas para as classes √†s quais o objeto na imagem pertence (√† direita).</em>  <em>Cada transforma√ß√£o de visualiza√ß√£o √© marcada como uma coluna.</em> </p><br><h1>  Camada convolucional </h1><br><p>  <em>A camada convolucional</em> √© a camada principal na constru√ß√£o de redes neurais convolucionais. </p><br><p>  <em>Vis√£o geral sem mergulhar nas caracter√≠sticas do c√©rebro.</em>  Vamos primeiro tentar descobrir o que a camada CONV ainda est√° calculando sem imergir e tocar no assunto do c√©rebro e dos neur√¥nios.  Os par√¢metros da camada convolucional consistem em um conjunto de filtros treinados.  Cada filtro √© uma pequena grade ao longo da largura e altura, mas se estende por toda a profundidade da representa√ß√£o de entrada. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p>  Por exemplo, um filtro padr√£o na primeira camada de uma rede neural convolucional pode ter dimens√µes 5x5x3 (5px - largura e altura, 3 - o n√∫mero de canais de cores).  Durante uma passagem direta, movemos (para ser exato - reduzimos) o filtro ao longo da largura e altura da representa√ß√£o de entrada e calculamos o produto escalar entre os valores do filtro e os valores correspondentes da representa√ß√£o de entrada a qualquer momento.  No processo de mover o filtro ao longo da largura e altura da representa√ß√£o de entrada, formamos um mapa de ativa√ß√£o bidimensional que cont√©m os valores da aplica√ß√£o desse filtro a cada uma das √°reas da representa√ß√£o de entrada.  Intuitivamente, fica claro que a rede ensinar√° os filtros a serem ativados quando virem um determinado sinal visual, por exemplo, uma linha reta em um determinado √¢ngulo ou representa√ß√µes em forma de roda em n√≠veis mais altos.  Agora que aplicamos todos os nossos filtros √† imagem original, por exemplo, havia 12. Como resultado da aplica√ß√£o de 12 filtros, recebemos 12 cart√µes de ativa√ß√£o da dimens√£o 2. Para produzir uma representa√ß√£o de sa√≠da, combinamos esses cart√µes (sequencialmente na 3¬™ dimens√£o) e obtemos uma representa√ß√£o dimens√£o [LxAx12]. </p><br><p>  <em>Uma vis√£o geral √† qual conectamos o c√©rebro e os neur√¥nios.</em>  Se voc√™ √© um f√£ do c√©rebro e dos neur√¥nios, pode imaginar que cada neur√¥nio "observe" uma grande parte da representa√ß√£o de entrada e transfira informa√ß√µes sobre essa se√ß√£o para os neur√¥nios vizinhos.  A seguir, discutiremos os detalhes da conectividade dos neur√¥nios, sua localiza√ß√£o no espa√ßo e o mecanismo de compartilhamento de par√¢metros. </p><br><p>  <em>Conectividade local.</em>  Quando lidamos com dados de entrada com um grande n√∫mero de medi√ß√µes, por exemplo, como no caso de imagens, ent√£o, como j√° vimos, n√£o h√° absolutamente nenhuma necessidade de conectar neur√¥nios com todos os neur√¥nios na camada anterior.  Em vez disso, conectaremos apenas neur√¥nios a √°reas locais da representa√ß√£o de entrada.  O grau espacial de conectividade √© um dos hiperpar√¢metros e √© chamado de <em>campo receptivo</em> (o campo receptivo de um neur√¥nio √© do tamanho do mesmo n√∫cleo de filtro / convolu√ß√£o).  O grau de conectividade ao longo da 3¬™ dimens√£o (profundidade) √© sempre igual √† profundidade da representa√ß√£o original.  √â muito importante focar nisso novamente, aten√ß√£o √† forma como definimos as dimens√µes espaciais (largura e altura) e profundidade: as conex√µes dos neur√¥nios s√£o locais em largura e altura, mas <em>sempre se</em> estendem por toda a profundidade da representa√ß√£o de entrada. </p><br><p>  <em>Exemplo 1.</em> Imagine que a representa√ß√£o de entrada tenha um tamanho de 32x32x3 (RGB, CIFAR-10).  Se o tamanho do filtro (o campo receptivo do neur√¥nio) for 5 √ó 5, cada neur√¥nio na camada convolucional ter√° pesos na regi√£o 5 √ó 5 √ó 3 da representa√ß√£o original, o que levar√° ao estabelecimento de 5 √ó 5 √ó 3 = 75 liga√ß√µes (pesos) + 1 par√¢metro de deslocamento.  Observe que o grau de conex√£o em profundidade deve ser igual a 3, pois essa √© a dimens√£o da representa√ß√£o original. </p><br><p>  <em>Exemplo 2.</em> Imagine que a representa√ß√£o de entrada tenha um tamanho de 16x16x20.  Usando como exemplo o campo receptivo de um neur√¥nio do tamanho 3x3, cada neur√¥nio da camada convolucional ter√° 3x3x320 = 180 conex√µes (pesos) + 1 par√¢metro de deslocamento.  Observe que a conectividade √© local em largura e altura, mas em profundidade (20). </p><br><div class="scrollable-table"><table><thead><tr><th>  # 1 </th><th>  # 2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>Do lado esquerdo: a</em> representa√ß√£o de entrada √© exibida em vermelho (por exemplo, uma imagem do tamanho CIFAR-10 de 32x332) e um exemplo da representa√ß√£o de neur√¥nios na primeira camada convolucional.  Cada neur√¥nio na camada convolucional est√° associado apenas √† √°rea local da representa√ß√£o de entrada, mas completamente em profundidade (no exemplo, ao longo de todos os canais de cores).  Observe que h√° muitos neur√¥nios na imagem (no exemplo - 5) e eles est√£o localizados ao longo da 3¬™ dimens√£o (profundidade) - explica√ß√µes sobre esse arranjo ser√£o fornecidas abaixo. <br>  <em>No lado direito: os</em> neur√¥nios da rede neural ainda permanecem inalterados: eles ainda calculam o produto escalar entre seus pesos e dados de entrada, aplicam a fun√ß√£o de ativa√ß√£o, mas sua conectividade √© agora limitada pela √°rea espacial local. </p><br><p>  <em>A localiza√ß√£o espacial.</em>  J√° descobrimos a conectividade de cada neur√¥nio na camada convolucional com a representa√ß√£o de entrada, mas ainda n√£o discutimos quantos desses neur√¥nios ou como eles est√£o localizados.  Tr√™s hiper par√¢metros afetam o tamanho da visualiza√ß√£o de sa√≠da: <em>profundidade</em> , <em>passo</em> e <em>alinhamento</em> . </p><br><ol><li>  <em>A profundidade da</em> representa√ß√£o de sa√≠da √© um hiper par√¢metro: corresponde ao n√∫mero de filtros que queremos aplicar, cada um dos quais aprende algo mais na representa√ß√£o original.  Por exemplo, se a primeira camada convolucional recebe uma imagem como entrada, ent√£o diferentes neur√¥nios ao longo da 3¬™ dimens√£o (profundidade) podem ser ativados na presen√ßa de diferentes orienta√ß√µes de linhas em uma determinada √°rea ou aglomerados de uma determinada cor.  O conjunto de neur√¥nios que "olham" para a mesma √°rea da representa√ß√£o de entrada, chamaremos de <em>coluna profunda</em> (ou "fibra" - fibra). </li><li>  Precisamos determinar o <em>passo</em> (tamanho do deslocamento em pixels) com o qual o filtro se mover√°.  Se a etapa for 1, mudaremos o filtro em 1 pixel em uma itera√ß√£o.  Se a etapa for 2 (ou, ainda menos usada, 3 ou mais), o deslocamento ocorrer√° para cada dois pixels em uma itera√ß√£o.  Uma etapa maior resulta em uma representa√ß√£o de sa√≠da menor. </li><li>  Como veremos em breve, √†s vezes ser√° necess√°rio suplementar a representa√ß√£o de entrada ao longo das bordas com zeros.  O tamanho do alinhamento (o n√∫mero de zero colunas / linhas preenchidas) tamb√©m √© um hiper par√¢metro.  Uma boa caracter√≠stica do uso do alinhamento √© o fato de o alinhamento nos permitir controlar a dimens√£o da representa√ß√£o de sa√≠da (na maioria das vezes manteremos as dimens√µes originais da vista - preservando a largura e a altura da representa√ß√£o de entrada com a largura e a altura da representa√ß√£o de sa√≠da). </li></ol><br><p>  Podemos calcular a dimens√£o final da representa√ß√£o de sa√≠da, apresentando-a em fun√ß√£o do tamanho da representa√ß√£o de entrada ( <strong>W</strong> ), do tamanho do campo receptivo dos neur√¥nios da camada convolucional ( <strong>F</strong> ), do passo ( <strong>S</strong> ) e do tamanho do alinhamento ( <strong>P</strong> ) nas bordas.  Voc√™ pode ver por si mesmo que a f√≥rmula correta para calcular o n√∫mero de neur√¥nios na representa√ß√£o de sa√≠da √© a seguinte <strong>(W - F + 2P) / S + 1</strong> .  Por exemplo, para uma representa√ß√£o de entrada do tamanho 7x7 e tamanho do filtro de 3x3, etapa 1 e alinhamento 0, obtemos uma representa√ß√£o de sa√≠da do tamanho 5x5.  Na etapa 2, obter√≠amos uma representa√ß√£o de sa√≠da de 3x3.  Vejamos outro exemplo, desta vez ilustrado graficamente: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>Ilustra√ß√£o de um arranjo espacial.</em>  <em>Neste exemplo, apenas uma dimens√£o espacial (eixo x), um neur√¥nio com campo receptivo <strong>F = 3</strong> , tamanho da representa√ß√£o de entrada <strong>W = 5</strong> e alinhamento <strong>P = 1</strong> .</em>  <em><strong>No lado esquerdo</strong> : o campo receptivo do neur√¥nio se move com um passo <strong>S = 1</strong> , que, como resultado, fornece o tamanho da representa√ß√£o de sa√≠da (5 - 3 + 2) / 1 + 1 = 5. <strong>No lado direito</strong> : o neur√¥nio usa o campo receptivo de tamanho <strong>S = 2</strong> , que em o resultado √© o tamanho da representa√ß√£o de sa√≠da (5 - 3 + 2) / 2 + 1 = 3. Observe que o tamanho da etapa <strong>S = 3</strong> n√£o pode ser usado, pois com esse tamanho da etapa o campo receptivo n√£o captura parte da imagem.</em>  <em>Se usarmos nossa f√≥rmula, ent√£o (5 - 3 + 2) = 4 n√£o √© um m√∫ltiplo de 3. Os pesos dos neur√¥nios neste exemplo s√£o [1, 0, -1] (como mostrado na figura √† direita), e o deslocamento √© zero.</em>  <em>Esses pesos s√£o compartilhados por todos os neur√¥nios amarelos.</em> </p><br><p>  <em>Usando alinhamento</em> .  Preste aten√ß√£o ao exemplo no lado esquerdo, que cont√©m 5 elementos na sa√≠da e 5 elementos na sa√≠da.  Isso funcionou porque o tamanho do campo receptivo (filtro) era 3 e usamos o alinhamento <strong>P = 1</strong> .  Se n√£o houvesse alinhamento, o tamanho da representa√ß√£o de sa√≠da seria igual a 3, porque eram precisamente tantos neur√¥nios que caberiam l√°.  Em geral, definir o tamanho do alinhamento <strong>P = (F - 1) / 2</strong> com uma etapa igual a <strong>S = 1</strong> permite obter o tamanho da representa√ß√£o de sa√≠da semelhante √† representa√ß√£o de entrada.  Uma abordagem semelhante usando alinhamento √© frequentemente aplicada na pr√°tica, e discutiremos os motivos abaixo quando falarmos sobre a arquitetura de redes neurais convolucionais. </p><br><p>  <em>Limites de tamanho da etapa</em> .  Observe que os hiperpar√¢metros respons√°veis ‚Äã‚Äãpelo arranjo espacial tamb√©m est√£o relacionados por limita√ß√µes.  Por exemplo, se a representa√ß√£o de entrada tiver um tamanho de <strong>W = 10</strong> , <strong>P = 0</strong> e o tamanho do campo receptivo <strong>F = 3</strong> , torna-se imposs√≠vel usar um tamanho de etapa igual a <strong>S = 2</strong> , pois <strong>(W - F + 2P) / S + 1 = (10 - 3 + 0) / 2 + 1 = 4,5</strong> , que fornece um valor inteiro do n√∫mero de neur√¥nios.  Assim, essa configura√ß√£o de hiper par√¢metros √© considerada inv√°lida e as bibliotecas para trabalhar com redes neurais convolucionais lan√ßam uma exce√ß√£o, alinham a for√ßa ou at√© cortam a representa√ß√£o de entrada.  Como veremos nas pr√≥ximas se√ß√µes deste cap√≠tulo, a defini√ß√£o dos hiperpar√¢metros da camada convolucional ainda √© uma dor de cabe√ßa que pode ser reduzida usando certas recomenda√ß√µes e "regras de bom tom" ao projetar a arquitetura de redes neurais convolucionais. </p><br><p>  <em>Exemplo da vida real</em> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Arquitetura de</a> rede neural <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">convolucional Krizhevsky et al.</a>  , que venceu o concurso ImageNet em 2012, recebeu 227x227x3 imagens.  Na primeira camada convolucional, ela utilizou um campo receptivo de tamanho <strong>F = 11</strong> , passo <strong>S = 4</strong> e tamanho de alinhamento <strong>P = 0</strong> .  Como (227 - 11) / 4 + 1 = 55, e a camada convolucional tinha <strong>K = 96 de</strong> profundidade, a dimens√£o de sa√≠da da apresenta√ß√£o era 55x55x96.  Cada um dos neur√¥nios 55x55x96 nessa representa√ß√£o foi associado a uma regi√£o de tamanho 11x11x3 na representa√ß√£o de entrada.  Al√©m disso, todos os 96 neur√¥nios da coluna profunda est√£o associados √† mesma regi√£o 11x11x3, mas com pesos diferentes.  E agora um pouco de humor - se voc√™ se familiarizar com o documento original (estudo), observe que o documento afirma que a entrada recebe imagens de 224x224, o que n√£o pode ser verdade, porque (224 - 11) / 4 + 1 de forma alguma forne√ßa um valor inteiro.  Esse tipo de situa√ß√£o costuma ser confundido para pessoas em hist√≥rias com redes neurais convolucionais.  Meu palpite √© que Alex usou o tamanho de alinhamento <strong>P = 3</strong> , mas esqueceu de mencionar isso no documento. </p><br><p>  <em>Op√ß√µes de compartilhamento.</em>  O mecanismo para compartilhar par√¢metros em camadas convolucionais √© usado para controlar o n√∫mero de par√¢metros.  Preste aten√ß√£o ao exemplo acima, como voc√™ pode ver, existem 55x55x96 = 290.400 neur√¥nios na primeira camada convolucional e cada um dos neur√¥nios possui 11x11x3 = 363 pesos + 1 valor de deslocamento.  No total, se multiplicarmos esses dois valores, obteremos 290400x364 = 105 705 600 par√¢metros <em>apenas</em> na primeira camada da rede neural convolucional.  Obviamente, isso √© de grande import√¢ncia! </p><br><p>  Acontece que √© poss√≠vel reduzir significativamente o n√∫mero de par√¢metros fazendo uma suposi√ß√£o: se alguma propriedade calculada na posi√ß√£o (x, y) √© importante para n√≥s, ent√£o essa propriedade calculada na posi√ß√£o (x2, y2) tamb√©m ser√° importante para n√≥s.  Em outras palavras, denotando uma "camada" bidimensional em profundidade como uma "camada profunda" (por exemplo, a exibi√ß√£o [55x55x96] cont√©m 96 camadas profundas, cada uma com tamanho de 55x55), construiremos neur√¥nios em profundidade com os mesmos pesos e deslocamento.  Com esse esquema de compartilhamento de par√¢metros, a primeira camada convolucional em nosso exemplo agora conter√° 96 conjuntos de pesos exclusivos (cada conjunto para cada camada de profundidade); no total, haver√° 96x11x11x3 = 34.848 pesos exclusivos ou 34.944 par√¢metros (+96 deslocamentos).  Al√©m disso, todos os neur√¥nios 55x55 em cada camada profunda agora usar√£o os mesmos par√¢metros.  Na pr√°tica, durante a propaga√ß√£o traseira, cada neur√¥nio nesta representa√ß√£o calcular√° o gradiente para seus pr√≥prios pesos, mas esses gradientes ser√£o somados em cada camada de profundidade e atualizar√£o apenas um √∫nico conjunto de pesos em cada n√≠vel. </p><br><p>  Observe que, se todos os neur√¥nios da mesma camada profunda usassem os mesmos pesos, para propaga√ß√£o direta atrav√©s da camada convolucional, a convolu√ß√£o entre os valores dos pesos dos neur√¥nios e os dados de entrada seria calculada.  √â por isso que √© costume chamar um √∫nico conjunto de pesos - um <strong>filtro (n√∫cleo)</strong> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>Exemplos de filtros que foram obtidos treinando o modelo Krizhevsky et al.</em>  <em>Cada um dos 96 filtros mostrados aqui tem tamanho 11x11x3 e cada um deles √© compartilhado por todos os neur√¥nios 55x55 de uma camada profunda.</em>  <em>Observe que a suposi√ß√£o de compartilhar os mesmos pesos faz sentido: se a detec√ß√£o de uma linha horizontal √© importante em uma parte da imagem, √© intuitivamente claro que essa detec√ß√£o √© importante em outra parte da imagem.</em>  <em>Portanto, n√£o faz sentido treinar novamente para encontrar linhas horizontais em cada um dos 55x55 locais diferentes da imagem na camada convolucional.</em> </p><br><p>  Deve-se ter em mente que a suposi√ß√£o de compartilhar par√¢metros nem sempre faz sentido.  Por exemplo, se uma imagem com alguma estrutura centralizada for inserida na entrada de uma rede neural convolucional, onde gostar√≠amos de poder aprender uma propriedade em uma parte da imagem e outra propriedade na outra parte da imagem.  Um exemplo pr√°tico s√£o as imagens de face centralizada.  Pode-se assumir que diferentes sinais oculares ou capilares podem ser identificados em diferentes √°reas da imagem; portanto, nesse caso, o relaxamento dos pesos √© usado e a camada √© chamada <strong>conectada localmente</strong> . </p><br><p>  <strong>Exemplos numpy</strong> .  As discuss√µes anteriores devem ser transferidas para o plano de detalhes e em exemplos com c√≥digo.  Imagine que a representa√ß√£o de entrada seja uma matriz <code>numpy</code> de <code>X</code>  Ent√£o: </p><br><ul><li>  <em>A coluna profunda</em> ( <em>linha</em> ) na posi√ß√£o <code>(x,y)</code> ser√° representada da seguinte forma <code>X[x,y,:]</code> . </li><li>  <em>A camada profunda</em> , ou como anteriormente chamamos de camada - <em>o mapa de ativa√ß√£o</em> na profundidade <code>d</code> ser√° representado da seguinte forma <code>X[:,:,d]</code> . </li></ul><br><p>  <em>Um exemplo de uma camada convolucional</em> . ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     ‚Äî (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         ‚Äî    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( ‚Äî  ,  ‚Äî ,  ‚Äî  ),        ‚Äî   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 ‚Äî 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 ‚Äî 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               ‚Äî <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     ‚Äî              ,    . ,    ,      ‚Äî        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B ‚Äî  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   ‚Äî           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 ‚Äî F)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    ‚Äî <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     ‚Äî    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    ‚Äî            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 ‚Äî 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   ‚Äî    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 ‚Äî 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     ‚Äî     55.        ""  33     ,    ‚Äî   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           ‚Äî          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       ‚Äî "  ":  ,       ,        ,         ImageNet ‚Äî   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    ‚Äî Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet ‚Äî        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU ‚Äî 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       ‚Äî  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Telegram</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456186/">https://habr.com/ru/post/pt456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456174/index.html">Como organizar um hackathon como estudante 101. Parte II</a></li>
<li><a href="../pt456178/index.html">Temas e estilos para Android sem m√°gica. E como cozinh√°-los com o SwitchCompat</a></li>
<li><a href="../pt456180/index.html">Como eu encontrei minha primeira vulnerabilidade?</a></li>
<li><a href="../pt456182/index.html">√Åudio por Bluetooth: informa√ß√µes mais detalhadas sobre perfis, codecs e dispositivos</a></li>
<li><a href="../pt456184/index.html">R√°dio definido por software - como funciona? Parte 8</a></li>
<li><a href="../pt456192/index.html">Dos mon√≥litos aos microsservi√ßos: a experi√™ncia de M.Video-Eldorado e MegaFon</a></li>
<li><a href="../pt456194/index.html">Folha de dicas sobre estruturas de dados Go</a></li>
<li><a href="../pt456196/index.html">Equ√≠vocos b√°sicos sobre o SCRUM</a></li>
<li><a href="../pt456200/index.html">Hist√≥ria da Internet: ARPANET - A origem</a></li>
<li><a href="../pt456202/index.html">Agora o WSL 2 est√° dispon√≠vel no Windows Insiders</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>