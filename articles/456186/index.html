<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßîüèº üç¥ üòä CS231n: redes neuronales convolucionales para el reconocimiento de patrones üîº üåå üì°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bienvenido a una de las conferencias en CS231n: Redes neuronales convolucionales para el reconocimiento visual . 



 Contenido 


- Descripci√≥n de la...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n: redes neuronales convolucionales para el reconocimiento de patrones</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p>  Bienvenido a una de las conferencias en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CS231n: Redes neuronales convolucionales para el reconocimiento visual</a> . </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1>  Contenido </h1><br><ul><li>  Descripci√≥n de la arquitectura </li><li>  Capas en una red neuronal convolucional <br>  - capa convolucional <br>  - Submuestreo de capa <br>  - Capa de normalizaci√≥n <br>  - capa completamente conectada <br>  - Convierta capas completamente conectadas en capas convolucionales </li><li>  Arquitectura de red neuronal convolucional <br>  - Plantillas de capa <br>  - Patrones de tama√±o de capa <br>  - Estudio de caso (LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet) <br>  - Aspectos computacionales </li><li>  Lectura adicional </li></ul><br><h1>  Redes neuronales convolucionales (CNN / ConvNets) </h1><br><p>  Las redes neuronales convolucionales son muy similares a las redes neuronales habituales que estudiamos en el √∫ltimo cap√≠tulo (refiri√©ndose al √∫ltimo cap√≠tulo del curso CS231n): consisten en neuronas, que, a su vez, contienen pesos y desplazamientos variables.  Cada neurona recibe algunos datos de entrada, calcula el producto escalar y, opcionalmente, utiliza una funci√≥n de activaci√≥n no lineal.  La red completa, como antes, es la √∫nica funci√≥n de evaluaci√≥n diferenciable: desde el conjunto inicial de p√≠xeles (imagen) en un extremo hasta la distribuci√≥n de probabilidad de pertenecer a una clase particular en el otro extremo.  Estas redes todav√≠a tienen una funci√≥n de p√©rdida (por ejemplo, SVM / Softmax) en la √∫ltima capa (totalmente conectada), y todos los consejos y recomendaciones que se dieron en el cap√≠tulo anterior con respecto a las redes neuronales ordinarias tambi√©n son relevantes para las redes neuronales convolucionales. </p><br><p>  Entonces, ¬øqu√© ha cambiado?  La arquitectura de las redes neuronales convolucionales implica expl√≠citamente la obtenci√≥n de im√°genes en la entrada, lo que nos permite tener en cuenta ciertas propiedades de los datos de entrada en la propia arquitectura de red.  Estas propiedades le permiten implementar la funci√≥n de distribuci√≥n directa de manera m√°s eficiente y reducir en gran medida el n√∫mero total de par√°metros en la red. </p><br><h1>  Descripci√≥n de la arquitectura </h1><br><p>  Recordamos las redes neuronales ordinarias.  Como vimos en el cap√≠tulo anterior, las redes neuronales reciben datos de entrada (un solo vector) y los transforman "empujando" a trav√©s de una serie de <em>capas ocultas</em> .  Cada capa oculta consta de un cierto n√∫mero de neuronas, cada una de las cuales est√° conectada a todas las neuronas de la capa anterior y donde las neuronas de cada capa son completamente independientes de otras neuronas en el mismo nivel.  La √∫ltima capa completamente conectada se llama "capa de salida" y en problemas de clasificaci√≥n es la distribuci√≥n de calificaciones por clase. </p><br><p>  <em>Las redes neuronales convencionales no escalan bien para im√°genes m√°s grandes</em> .  En el conjunto de datos CIFAR-10, las im√°genes tienen un tama√±o de 32x32x3 (32 p√≠xeles de alto, 32 p√≠xeles de ancho, 3 canales de color).  Para procesar dicha imagen, una neurona completamente conectada en la primera capa oculta de una red neuronal normal tendr√° 32x32x3 = 3072 pesos.  Esta cantidad a√∫n es aceptable, pero se hace evidente que dicha estructura no funcionar√° con im√°genes m√°s grandes.  Por ejemplo, una imagen m√°s grande, 200x200x3, har√° que el n√∫mero de pesos se convierta en 200x200x3 = 120,000. Adem√°s, necesitaremos m√°s de una neurona, por lo que el n√∫mero total de pesos comenzar√° a crecer r√°pidamente.  Se hace obvio que la conectividad es excesiva y una gran cantidad de par√°metros conducir√°n r√°pidamente a la red a la reentrenamiento. </p><br><p>  <em>Representaciones 3D de neuronas</em> .  Las redes neuronales convolucionales utilizan el hecho de que los datos de entrada son im√°genes, por lo que forman una arquitectura m√°s sensible para este tipo de datos.  En particular, a diferencia de las redes neuronales convencionales, las capas en la red neuronal convolucional organizan las neuronas en 3 dimensiones: ancho, altura, profundidad ( <em>Nota</em> : la palabra "profundidad" se refiere a la tercera dimensi√≥n de las neuronas de activaci√≥n, y no a la profundidad de la red neuronal en s√≠ medida en n√∫mero de capas).  Por ejemplo, las im√°genes de entrada del conjunto de datos CIFAR-10 son datos de entrada en una representaci√≥n 3D, cuya dimensi√≥n es 32x32x3 (ancho, alto, profundidad).  Como veremos m√°s adelante, las neuronas en una capa se asociar√°n con un peque√±o n√∫mero de neuronas en la capa anterior, en lugar de estar conectadas a todas las neuronas anteriores en la capa.  Adem√°s, la capa de salida para la imagen del conjunto de datos CIFAR-10 tendr√° una dimensi√≥n de 1 √ó 1 √ó 10, porque al acercarnos al final de la red neuronal reduciremos el tama√±o de la imagen a un vector de estimaciones de clase ubicadas a lo largo de la profundidad (tercera dimensi√≥n). </p><br><p>  Visualizaci√≥n: </p><br><div class="scrollable-table"><table><thead><tr><th>  Red neuronal est√°ndar </th><th>  Red neuronal convolucional </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>Lado izquierdo:</em> red neuronal est√°ndar de 3 capas. <br>  <em>En el lado derecho: la</em> red neuronal convolucional tiene sus neuronas en 3 dimensiones (ancho, alto, profundidad), como se muestra en una de las capas.  Cada capa de red neuronal convolucional convierte una representaci√≥n 3D de la entrada en una representaci√≥n 3D de la salida como neuronas de activaci√≥n.  En este ejemplo, la capa de entrada roja contiene la imagen, por lo que su tama√±o ser√° igual al tama√±o de la imagen y la profundidad ser√° 3 (tres canales: rojo, verde, azul). </p><br><blockquote>  La red neuronal convolucional consiste en capas.  Cada capa es una API simple: convierte la representaci√≥n 3D de entrada en la representaci√≥n 3D de salida de una funci√≥n diferenciable, que puede contener o no par√°metros. </blockquote><br><h1>  Capas usadas para construir redes neuronales convolucionales </h1><br><p>  Como ya hemos descrito anteriormente, una red neuronal convolucional simple es un conjunto de capas, donde cada capa convierte una representaci√≥n en otra utilizando una funci√≥n diferenciable.  Utilizamos tres tipos principales de capas para construir redes neuronales convolucionales: una <em>capa convolucional</em> , <em>una</em> <em>capa de</em> <em>submuestreo</em> y una <em>capa totalmente conectada</em> (lo mismo que usamos en una red neuronal normal).  Organizamos estas capas secuencialmente para obtener la arquitectura SNA. </p><br><p> <em>Ejemplo de arquitectura: descripci√≥n general.</em>  A continuaci√≥n profundizaremos en los detalles, pero por ahora, para el conjunto de datos CIFAR-10, la arquitectura de nuestra red neuronal convolucional puede ser <code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> .  Ahora con m√°s detalle: </p><br><ul><li>  <code>INPUT</code> [32x32x3] contendr√° los valores originales de los p√≠xeles de la imagen, en nuestro caso la imagen tiene 32px de ancho, 32px de alto y 3 canales de color R, G, B. </li><li>  <code>CONV</code> capa <code>CONV</code> producir√° un conjunto de neuronas de salida que se asociar√°n con el √°rea local de la imagen fuente de entrada;  cada una de esas neuronas calcular√° el producto escalar entre sus pesos y esa peque√±a parte de la imagen original con la que est√° asociada.  El valor de salida puede ser una representaci√≥n 3D de <code>323212</code> , si, por ejemplo, decidimos usar 12 filtros. </li><li>  <code>RELU</code> capa <code>RELU</code> aplicar√° la funci√≥n de activaci√≥n del elemento <code>max(0, x)</code> .  Esta conversi√≥n no cambiar√° la dimensi√≥n de datos - <code>[32x32x12]</code> . </li><li>  <code>POOL</code> capa <code>POOL</code> realizar√° la operaci√≥n de muestreo de la imagen en dos dimensiones: altura y ancho, lo que como resultado nos dar√° una nueva representaci√≥n 3D <code>[161612]</code> . </li><li>  <code>FC</code> capa <code>FC</code> (capa totalmente conectada) calcular√° las calificaciones por clases, la dimensi√≥n resultante ser√° <code>[1x1x10]</code> , donde cada uno de los 10 valores corresponder√° a las calificaciones de una clase particular entre 10 categor√≠as de im√°genes de CIFAR-10.  Como en las redes neuronales convencionales, cada neurona de esta capa estar√° asociada con todas las neuronas de la capa anterior (representaci√≥n 3D). </li></ul><br><p>  As√≠ es como la red neuronal convolucional transforma la imagen original, capa por capa, desde el valor de p√≠xel inicial hasta la estimaci√≥n de clase final.  Tenga en cuenta que algunas capas contienen opciones y otras no.  En particular, las capas <code>CONV/FC</code> llevan a cabo una transformaci√≥n, que no solo es una funci√≥n que depende de los datos de entrada, sino que tambi√©n depende de los valores internos de pesos y desplazamientos en las neuronas mismas.  <code>RELU/POOL</code> capas <code>RELU/POOL</code> , <code>RELU/POOL</code> otro lado, usan funciones no parametrizadas.  Los par√°metros en las capas <code>CONV/FC</code> se entrenar√°n por gradiente descendente para que la entrada reciba las etiquetas de salida correctas correspondientes. </p><br><p>  Para resumir: </p><br><ul><li>  La arquitectura de la red neuronal convolucional, en su representaci√≥n m√°s simple, es un conjunto ordenado de capas que transforma la representaci√≥n de una imagen en otra representaci√≥n, por ejemplo, estimaciones de miembros de clase. </li><li>  Hay varios tipos diferentes de capas (CONV - capa convolucional, FC - completamente conectado, RELU - funci√≥n de activaci√≥n, POOL - capa de submuestra - el m√°s popular). </li><li>  Cada capa de entrada recibe una representaci√≥n 3D, la convierte en una representaci√≥n 3D de salida utilizando una funci√≥n diferenciable. </li><li>  Cada capa puede y no tener par√°metros (CONV / FC - tiene par√°metros, RELU / POOL - no). </li><li>  Cada capa puede y no tener hiperpar√°metros (CONV / FC / POOL - have, RELU - no) </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>La representaci√≥n inicial contiene los valores de p√≠xeles de la imagen (a la izquierda) y estimaciones de las clases a las que pertenece el objeto en la imagen (a la derecha).</em>  <em>Cada transformaci√≥n de vista se marca como una columna.</em> </p><br><h1>  Capa convolucional </h1><br><p>  <em>La capa convolucional</em> es la capa principal en la construcci√≥n de redes neuronales convolucionales. </p><br><p>  <em>Descripci√≥n general sin sumergirse en las caracter√≠sticas del cerebro.</em>  Primero intentemos averiguar qu√© sigue calculando la capa CONV sin sumergirnos y tocar el tema del cerebro y las neuronas.  Los par√°metros de la capa convolucional consisten en un conjunto de filtros entrenados.  Cada filtro es una cuadr√≠cula peque√±a a lo largo del ancho y la altura, pero se extiende por toda la profundidad de la representaci√≥n de entrada. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p>  Por ejemplo, un filtro est√°ndar en la primera capa de una red neuronal convolucional puede tener dimensiones de 5x5x3 (5px - ancho y alto, 3 - el n√∫mero de canales de color).  Durante un pase directo, movemos (para ser exactos: colapsamos) el filtro a lo largo del ancho y alto de la representaci√≥n de entrada y calculamos el producto escalar entre los valores del filtro y los valores correspondientes de la representaci√≥n de entrada en cualquier punto.  En el proceso de mover el filtro a lo largo del ancho y alto de la representaci√≥n de entrada, formamos un mapa de activaci√≥n bidimensional que contiene los valores de aplicar este filtro a cada una de las √°reas de la representaci√≥n de entrada.  Intuitivamente, queda claro que la red ense√±ar√° a los filtros a activarse cuando vean un cierto signo visual, por ejemplo, una l√≠nea recta en un cierto √°ngulo o representaciones en forma de rueda en niveles superiores.  Ahora que hemos aplicado todos nuestros filtros a la imagen original, por ejemplo, hab√≠a 12. Como resultado de la aplicaci√≥n de 12 filtros, recibimos 12 tarjetas de activaci√≥n de dimensi√≥n 2. Para producir una representaci√≥n de salida, combinamos estas tarjetas (secuencialmente en la tercera dimensi√≥n) y obtenemos una representaci√≥n dimensi√≥n [WxHx12]. </p><br><p>  <em>Una visi√≥n general a la que conectamos el cerebro y las neuronas.</em>  Si eres fan√°tico del cerebro y las neuronas, puedes imaginar que cada neurona "mira" una gran secci√≥n de la representaci√≥n de entrada y transfiere informaci√≥n sobre esta secci√≥n a las neuronas vecinas.  A continuaci√≥n discutiremos los detalles de la conectividad neuronal, su ubicaci√≥n en el espacio y el mecanismo para compartir par√°metros. </p><br><p>  <em>Conectividad local.</em>  Cuando se trata de datos de entrada con una gran cantidad de dimensiones, por ejemplo, como en el caso de las im√°genes, entonces, como ya hemos visto, no hay absolutamente ninguna necesidad de conectar las neuronas con todas las neuronas de la capa anterior.  En cambio, solo conectaremos neuronas a √°reas locales de la representaci√≥n de entrada.  El grado espacial de conectividad es uno de los hiperpar√°metros y se denomina <em>campo receptivo</em> (el campo receptivo de una neurona es del tama√±o del mismo n√∫cleo de filtro / convoluci√≥n).  El grado de conectividad a lo largo de la tercera dimensi√≥n (profundidad) siempre es igual a la profundidad de la representaci√≥n original.  Es muy importante enfocarse nuevamente en esto, prestar atenci√≥n a c√≥mo definimos las dimensiones espaciales (ancho y alto) y la profundidad: las conexiones neuronales son locales en ancho y alto, pero <em>siempre se</em> extienden a lo largo de toda la representaci√≥n de entrada. </p><br><p>  <em>Ejemplo 1.</em> Imagine que la representaci√≥n de entrada tiene un tama√±o de 32x32x3 (RGB, CIFAR-10).  Si el tama√±o del filtro (campo receptivo de la neurona) es 5 √ó 5, entonces cada neurona en la capa convolucional tendr√° pesos en la regi√≥n 5 √ó 5 √ó 3 de la representaci√≥n original, lo que finalmente conducir√° al establecimiento de 5 √ó 5 √ó 3 = 75 enlaces (pesos) + 1 par√°metro de compensaci√≥n.  Tenga en cuenta que el grado de conexi√≥n en profundidad debe ser igual a 3, ya que esta es la dimensi√≥n de la representaci√≥n original. </p><br><p>  <em>Ejemplo 2.</em> Imagine que la representaci√≥n de entrada tiene un tama√±o de 16x16x20.  Usando como ejemplo el campo receptivo de una neurona de tama√±o 3x3, cada neurona de capa convolucional tendr√° 3x3x320 = 180 conexiones (pesos) + 1 par√°metro de desplazamiento.  Tenga en cuenta que la conectividad es local en ancho y alto, pero completa en profundidad (20). </p><br><div class="scrollable-table"><table><thead><tr><th>  # 1 </th><th>  # 2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>Desde el lado izquierdo: la</em> representaci√≥n de entrada se muestra en rojo (por ejemplo, una imagen de tama√±o CIFAR-10 de 32x332) y un ejemplo de la representaci√≥n de neuronas en la primera capa convolucional.  Cada neurona en la capa convolucional est√° asociada solo con el √°rea local de la representaci√≥n de entrada, pero completamente en profundidad (en el ejemplo, a lo largo de todos los canales de color).  Tenga en cuenta que hay muchas neuronas en la imagen (en el ejemplo - 5) y est√°n ubicadas a lo largo de la 3¬™ dimensi√≥n (profundidad); a continuaci√≥n se explicar√°n estas disposiciones. <br>  <em>En el lado derecho: las</em> neuronas de la red neuronal a√∫n permanecen sin cambios: todav√≠a calculan el producto escalar entre sus pesos y datos de entrada, aplican la funci√≥n de activaci√≥n, pero su conectividad ahora est√° limitada por el √°rea local espacial. </p><br><p>  <em>La ubicaci√≥n espacial.</em>  Ya hemos descubierto la conectividad de cada neurona en la capa convolucional con la representaci√≥n de entrada, pero a√∫n no hemos discutido cu√°ntas de estas neuronas o c√≥mo est√°n ubicadas.  Tres hiperpar√°metros afectan el tama√±o de la vista de salida: <em>profundidad</em> , <em>paso</em> y <em>alineaci√≥n</em> . </p><br><ol><li>  <em>La profundidad de la</em> representaci√≥n de salida es un hiperpar√°metro: corresponde a la cantidad de filtros que queremos aplicar, cada uno de los cuales aprende algo m√°s en la representaci√≥n original.  Por ejemplo, si la primera capa convolucional recibe una imagen como entrada, entonces se pueden activar diferentes neuronas a lo largo de la tercera dimensi√≥n (profundidad) en presencia de diferentes orientaciones de l√≠neas en un √°rea determinada o grupos de un color determinado.  Al conjunto de neuronas que "miran" la misma √°rea de la representaci√≥n de entrada, llamaremos la <em>columna profunda</em> (o "fibra" - fibra). </li><li>  Necesitamos determinar el <em>paso</em> (tama√±o de desplazamiento en p√≠xeles) con el que se mover√° el filtro.  Si el paso es 1, entonces cambiamos el filtro por 1 p√≠xel en una iteraci√≥n.  Si el paso es 2 (o, que se usa con menos frecuencia, 3 o m√°s), el desplazamiento se produce por cada dos p√≠xeles en una iteraci√≥n.  Un paso m√°s grande da como resultado una representaci√≥n de salida m√°s peque√±a. </li><li>  Como veremos pronto, a veces ser√° necesario complementar la representaci√≥n de entrada a lo largo de los bordes con ceros.  El tama√±o de alineaci√≥n (el n√∫mero de columnas / filas rellenas con cero) tambi√©n es un hiperpar√°metro.  Una buena caracter√≠stica del uso de la alineaci√≥n es el hecho de que la alineaci√≥n nos permitir√° controlar la dimensi√≥n de la representaci√≥n de salida (a menudo mantendremos las dimensiones originales de la vista, preservando el ancho y la altura de la representaci√≥n de entrada con el ancho y la altura de la representaci√≥n de salida). </li></ol><br><p>  Podemos calcular la dimensi√≥n final de la representaci√≥n de salida present√°ndola en funci√≥n del tama√±o de la representaci√≥n de entrada ( <strong>W</strong> ), el tama√±o del campo receptivo de las neuronas de la capa convolucional ( <strong>F</strong> ), el paso ( <strong>S</strong> ) y el tama√±o de la alineaci√≥n ( <strong>P</strong> ) en los bordes.  Puede ver por s√≠ mismo que la f√≥rmula correcta para calcular el n√∫mero de neuronas en la representaci√≥n de salida es la siguiente <strong>(W - F + 2P) / S + 1</strong> .  Por ejemplo, para una representaci√≥n de entrada de tama√±o 7x7 y tama√±o de filtro de 3x3, paso 1 y alineaci√≥n 0, obtenemos una representaci√≥n de salida de tama√±o 5x5.  En el paso 2, obtendr√≠amos una representaci√≥n de salida de 3x3.  Veamos otro ejemplo, esta vez ilustrado gr√°ficamente: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>Ilustraci√≥n de una disposici√≥n espacial.</em>  <em>En este ejemplo, solo una dimensi√≥n espacial (eje x), una neurona con campo receptivo <strong>F = 3</strong> , tama√±o de representaci√≥n de entrada <strong>W = 5</strong> y alineaci√≥n <strong>P = 1</strong> .</em>  <em><strong>En el lado izquierdo</strong> : el campo receptivo de la neurona se mueve con un paso <strong>S = 1</strong> , que como resultado da el tama√±o de la representaci√≥n de salida (5 - 3 + 2) / 1 + 1 = 5. <strong>En el lado derecho</strong> : la neurona usa el campo receptivo de tama√±o <strong>S = 2</strong> , que en el resultado es el tama√±o de la representaci√≥n de salida (5 - 3 + 2) / 2 + 1 = 3. Tenga en cuenta que el tama√±o de paso <strong>S = 3</strong> no se puede utilizar, ya que con este tama√±o de paso el campo receptivo no capturar√° parte de la imagen.</em>  <em>Si usamos nuestra f√≥rmula, entonces (5 - 3 + 2) = 4 no es m√∫ltiplo de 3. Los pesos de las neuronas en este ejemplo son [1, 0, -1] (como se muestra en la imagen de la derecha), y el desplazamiento es cero.</em>  <em>Estos pesos son compartidos por todas las neuronas amarillas.</em> </p><br><p>  <em>Usando la alineaci√≥n</em> .  Preste atenci√≥n al ejemplo en el lado izquierdo, que contiene 5 elementos en la salida y 5 elementos en la salida.  Esto funcion√≥ porque el tama√±o del campo receptivo (filtro) era 3 y utilizamos la alineaci√≥n <strong>P = 1</strong> .  Si no hubiera alineaci√≥n, entonces el tama√±o de la representaci√≥n de salida ser√≠a igual a 3, porque precisamente precisamente tantas neuronas encajar√≠an all√≠.  En general, establecer el tama√±o de alineaci√≥n <strong>P = (F - 1) / 2</strong> con un paso igual a <strong>S = 1 le</strong> permite obtener el tama√±o de la representaci√≥n de salida similar a la representaci√≥n de entrada.  Un enfoque similar que usa la alineaci√≥n a menudo se aplica en la pr√°ctica, y discutiremos las razones a continuaci√≥n cuando hablemos sobre la arquitectura de las redes neuronales convolucionales. </p><br><p>  <em>L√≠mites de tama√±o de paso</em> .  Tenga en cuenta que los hiperpar√°metros responsables de la disposici√≥n espacial tambi√©n est√°n relacionados por limitaciones.  Por ejemplo, si la representaci√≥n de entrada tiene un tama√±o de <strong>W = 10</strong> , <strong>P = 0</strong> y el tama√±o del campo receptivo <strong>F = 3</strong> , entonces es imposible usar un tama√±o de paso igual a <strong>S = 2</strong> , ya que <strong>(W - F + 2P) / S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5</strong> , que proporciona un valor entero del n√∫mero de neuronas.  Por lo tanto, dicha configuraci√≥n de hiperpar√°metros se considera inv√°lida y las bibliotecas para trabajar con redes neuronales convolucionales generar√°n una excepci√≥n, forzar√°n la alineaci√≥n o incluso cortar√°n la representaci√≥n de entrada.  Como veremos en las siguientes secciones de este cap√≠tulo, la definici√≥n de los hiperpar√°metros de la capa convolucional sigue siendo un dolor de cabeza que puede reducirse mediante el uso de ciertas recomendaciones y "buenas reglas de tono" al dise√±ar la arquitectura de redes neuronales convolucionales. </p><br><p>  <em>Ejemplo de la vida real</em> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Arquitectura de</a> red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">convolucional Krizhevsky et al.</a>  , que gan√≥ el concurso ImageNet en 2012, recibi√≥ 227x227x3 im√°genes.  En la primera capa convolucional, utiliz√≥ un campo receptivo de tama√±o <strong>F = 11</strong> , paso <strong>S = 4</strong> y tama√±o de alineaci√≥n <strong>P = 0</strong> .  Como (227-11) / 4 + 1 = 55, y la capa convolucional ten√≠a una profundidad de <strong>K = 96</strong> , la dimensi√≥n de salida de la presentaci√≥n fue 55x55x96.  Cada una de las neuronas de 55x55x96 en esta representaci√≥n se asoci√≥ con una regi√≥n de tama√±o 11x11x3 en la representaci√≥n de entrada.  Adem√°s, todas las 96 neuronas en la columna profunda est√°n asociadas con la misma regi√≥n 11x11x3, pero con diferentes pesos.  Y ahora un poco de humor: si decide familiarizarse con el documento original (estudio), tenga en cuenta que el documento afirma que la entrada recibe im√°genes de 224x224, lo que no puede ser cierto, porque (224 - 11) / 4 + 1 de ninguna manera dar un valor entero.  Este tipo de situaci√≥n a menudo se confunde para las personas en historias con redes neuronales convolucionales.  Supongo que Alex us√≥ el tama√±o de alineaci√≥n <strong>P = 3</strong> , pero olvid√≥ mencionar esto en el documento. </p><br><p>  <em>Opciones para compartir</em>  El mecanismo para compartir par√°metros en capas convolucionales se usa para controlar el n√∫mero de par√°metros.  Preste atenci√≥n al ejemplo anterior, ya que puede ver que hay 55x55x96 = 290,400 neuronas en la primera capa convolucional y cada una de las neuronas tiene 11x11x3 = 363 pesos + 1 valor de compensaci√≥n.  En total, si multiplicamos estos dos valores, obtenemos 290400x364 = 105 705 600 par√°metros <em>solo</em> en la primera capa de la red neuronal convolucional.  ¬°Obviamente, esto es de gran importancia! </p><br><p>  Resulta que es posible reducir significativamente el n√∫mero de par√°metros haciendo una suposici√≥n: si alguna propiedad calculada en posici√≥n (x, y) nos importa, entonces esta propiedad calculada en posici√≥n (x2, y2) tambi√©n nos importar√°.  En otras palabras, denotando una "capa" bidimensional en profundidad como una "capa profunda" (por ejemplo, la vista [55x55x96] contiene 96 capas profundas, cada una de 55x55 de tama√±o), construiremos neuronas en profundidad con los mismos pesos y desplazamiento.  Con este esquema de compartir par√°metros, la primera capa convolucional en nuestro ejemplo ahora contendr√° 96 conjuntos √∫nicos de pesos (cada conjunto para cada capa de profundidad), en total habr√° 96x11x11x3 = 34,848 pesos √∫nicos o 34,944 par√°metros (+96 compensaciones).  Adem√°s, todas las neuronas de 55x55 en cada capa profunda ahora usar√°n los mismos par√°metros.  En la pr√°ctica, durante la propagaci√≥n hacia atr√°s, cada neurona en esta representaci√≥n calcular√° el gradiente para sus propios pesos, pero estos gradientes se sumar√°n sobre cada capa de profundidad y actualizar√°n solo un conjunto √∫nico de pesos en cada nivel. </p><br><p>  Tenga en cuenta que si todas las neuronas en la misma capa profunda usaran los mismos pesos, entonces para la propagaci√≥n directa a trav√©s de la capa convolucional, se calcular√≠a la convoluci√≥n entre los valores de los pesos de las neuronas y los datos de entrada.  Es por eso que es habitual llamar a un solo conjunto de pesos: un <strong>filtro (n√∫cleo)</strong> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>Ejemplos de filtros que se obtuvieron al entrenar el modelo Krizhevsky et al.</em>  <em>Cada uno de los 96 filtros que se muestran aqu√≠ tiene un tama√±o de 11x11x3 y cada uno de ellos es compartido por todas las neuronas de 55x55 de una capa profunda.</em>  <em>Tenga en cuenta que la suposici√≥n de compartir los mismos pesos tiene sentido: si la detecci√≥n de una l√≠nea horizontal es importante en una parte de la imagen, entonces es intuitivamente claro que dicha detecci√≥n es importante en otra parte de esta imagen.</em>  <em>Por lo tanto, no tiene sentido volver a entrenar cada vez para encontrar l√≠neas horizontales en cada uno de los 55x55 lugares diferentes de la imagen en la capa convolucional.</em> </p><br><p>  Debe tenerse en cuenta que la suposici√≥n de compartir par√°metros no siempre tiene sentido.  Por ejemplo, si una imagen con una estructura centrada se alimenta a la entrada de una red neuronal convolucional, donde nos gustar√≠a poder aprender una propiedad en una parte de la imagen y otra propiedad en la otra parte de la imagen.  Un ejemplo pr√°ctico son las im√°genes de caras centradas.  Se puede suponer que se pueden identificar diferentes signos oculares o capilares en diferentes √°reas de la imagen, por lo tanto, en este caso, se usa relajaci√≥n de pesas y la capa se denomina <strong>conectada localmente</strong> . </p><br><p>  <strong>Numpy ejemplos</strong> .  Las discusiones anteriores deben transferirse al plano de los detalles y a los ejemplos con c√≥digo.  Imagine que la representaci√≥n de entrada es una matriz <code>numpy</code> de <code>X</code>  Entonces: </p><br><ul><li>  <em>La columna profunda</em> ( <em>hilo</em> ) en la posici√≥n <code>(x,y)</code> se representar√° como sigue <code>X[x,y,:]</code> . </li><li>  <em>La capa profunda</em> , o como la llamamos anteriormente, <em>el mapa de activaci√≥n</em> en la profundidad <code>d</code> se representar√° de la siguiente manera <code>X[:,:,d]</code> . </li></ul><br><p>  <em>Un ejemplo de una capa convolucional</em> . ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     ‚Äî (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         ‚Äî    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( ‚Äî  ,  ‚Äî ,  ‚Äî  ),        ‚Äî   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 ‚Äî 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 ‚Äî 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               ‚Äî <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     ‚Äî              ,    . ,    ,      ‚Äî        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B ‚Äî  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   ‚Äî           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 ‚Äî F)/S + 1</strong> </li><li> <strong>H2 = (H1 ‚Äî F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    ‚Äî <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     ‚Äî    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    ‚Äî            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 ‚Äî 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   ‚Äî    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 ‚Äî 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     ‚Äî     55.        ""  33     ,    ‚Äî   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           ‚Äî          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       ‚Äî "  ":  ,       ,        ,         ImageNet ‚Äî   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    ‚Äî Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet ‚Äî        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU ‚Äî 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       ‚Äî  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegrama</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456186/">https://habr.com/ru/post/456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456174/index.html">C√≥mo organizar un hackathon como estudiante 101. Segunda parte</a></li>
<li><a href="../456178/index.html">Temas y estilos de Android sin magia. Y c√≥mo cocinarlos con SwitchCompat</a></li>
<li><a href="../456180/index.html">¬øC√≥mo encontr√© mi primera vulnerabilidad?</a></li>
<li><a href="../456182/index.html">Audio a trav√©s de Bluetooth: informaci√≥n m√°s detallada sobre perfiles, c√≥decs y dispositivos</a></li>
<li><a href="../456184/index.html">Radio definida por software: ¬øc√≥mo funciona? Parte 8</a></li>
<li><a href="../456188/index.html">Token, token de actualizaci√≥n y creaci√≥n de un contenedor as√≠ncrono para una solicitud REST</a></li>
<li><a href="../456192/index.html">De monolitos a microservicios: la experiencia de M.Video-Eldorado y MegaFon</a></li>
<li><a href="../456194/index.html">Ir a estructuras de datos hoja de trucos</a></li>
<li><a href="../456196/index.html">Conceptos err√≥neos b√°sicos sobre SCRUM</a></li>
<li><a href="../456200/index.html">Historia de Internet: ARPANET - El origen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>