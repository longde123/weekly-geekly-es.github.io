<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧔🏼 🍴 😊 CS231n: redes neuronales convolucionales para el reconocimiento de patrones 🔼 🌌 📡</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bienvenido a una de las conferencias en CS231n: Redes neuronales convolucionales para el reconocimiento visual . 



 Contenido 


- Descripción de la...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>CS231n: redes neuronales convolucionales para el reconocimiento de patrones</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456186/"><p>  Bienvenido a una de las conferencias en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CS231n: Redes neuronales convolucionales para el reconocimiento visual</a> . </p><br><p><img src="https://habrastorage.org/webt/b0/yc/wm/b0ycwm3fl6uveqvlr-usz5w9iqa.png"></p><a name="habracut"></a><br><h1>  Contenido </h1><br><ul><li>  Descripción de la arquitectura </li><li>  Capas en una red neuronal convolucional <br>  - capa convolucional <br>  - Submuestreo de capa <br>  - Capa de normalización <br>  - capa completamente conectada <br>  - Convierta capas completamente conectadas en capas convolucionales </li><li>  Arquitectura de red neuronal convolucional <br>  - Plantillas de capa <br>  - Patrones de tamaño de capa <br>  - Estudio de caso (LeNet, AlexNet, ZFNet, GoogLeNet, VGGNet) <br>  - Aspectos computacionales </li><li>  Lectura adicional </li></ul><br><h1>  Redes neuronales convolucionales (CNN / ConvNets) </h1><br><p>  Las redes neuronales convolucionales son muy similares a las redes neuronales habituales que estudiamos en el último capítulo (refiriéndose al último capítulo del curso CS231n): consisten en neuronas, que, a su vez, contienen pesos y desplazamientos variables.  Cada neurona recibe algunos datos de entrada, calcula el producto escalar y, opcionalmente, utiliza una función de activación no lineal.  La red completa, como antes, es la única función de evaluación diferenciable: desde el conjunto inicial de píxeles (imagen) en un extremo hasta la distribución de probabilidad de pertenecer a una clase particular en el otro extremo.  Estas redes todavía tienen una función de pérdida (por ejemplo, SVM / Softmax) en la última capa (totalmente conectada), y todos los consejos y recomendaciones que se dieron en el capítulo anterior con respecto a las redes neuronales ordinarias también son relevantes para las redes neuronales convolucionales. </p><br><p>  Entonces, ¿qué ha cambiado?  La arquitectura de las redes neuronales convolucionales implica explícitamente la obtención de imágenes en la entrada, lo que nos permite tener en cuenta ciertas propiedades de los datos de entrada en la propia arquitectura de red.  Estas propiedades le permiten implementar la función de distribución directa de manera más eficiente y reducir en gran medida el número total de parámetros en la red. </p><br><h1>  Descripción de la arquitectura </h1><br><p>  Recordamos las redes neuronales ordinarias.  Como vimos en el capítulo anterior, las redes neuronales reciben datos de entrada (un solo vector) y los transforman "empujando" a través de una serie de <em>capas ocultas</em> .  Cada capa oculta consta de un cierto número de neuronas, cada una de las cuales está conectada a todas las neuronas de la capa anterior y donde las neuronas de cada capa son completamente independientes de otras neuronas en el mismo nivel.  La última capa completamente conectada se llama "capa de salida" y en problemas de clasificación es la distribución de calificaciones por clase. </p><br><p>  <em>Las redes neuronales convencionales no escalan bien para imágenes más grandes</em> .  En el conjunto de datos CIFAR-10, las imágenes tienen un tamaño de 32x32x3 (32 píxeles de alto, 32 píxeles de ancho, 3 canales de color).  Para procesar dicha imagen, una neurona completamente conectada en la primera capa oculta de una red neuronal normal tendrá 32x32x3 = 3072 pesos.  Esta cantidad aún es aceptable, pero se hace evidente que dicha estructura no funcionará con imágenes más grandes.  Por ejemplo, una imagen más grande, 200x200x3, hará que el número de pesos se convierta en 200x200x3 = 120,000. Además, necesitaremos más de una neurona, por lo que el número total de pesos comenzará a crecer rápidamente.  Se hace obvio que la conectividad es excesiva y una gran cantidad de parámetros conducirán rápidamente a la red a la reentrenamiento. </p><br><p>  <em>Representaciones 3D de neuronas</em> .  Las redes neuronales convolucionales utilizan el hecho de que los datos de entrada son imágenes, por lo que forman una arquitectura más sensible para este tipo de datos.  En particular, a diferencia de las redes neuronales convencionales, las capas en la red neuronal convolucional organizan las neuronas en 3 dimensiones: ancho, altura, profundidad ( <em>Nota</em> : la palabra "profundidad" se refiere a la tercera dimensión de las neuronas de activación, y no a la profundidad de la red neuronal en sí medida en número de capas).  Por ejemplo, las imágenes de entrada del conjunto de datos CIFAR-10 son datos de entrada en una representación 3D, cuya dimensión es 32x32x3 (ancho, alto, profundidad).  Como veremos más adelante, las neuronas en una capa se asociarán con un pequeño número de neuronas en la capa anterior, en lugar de estar conectadas a todas las neuronas anteriores en la capa.  Además, la capa de salida para la imagen del conjunto de datos CIFAR-10 tendrá una dimensión de 1 × 1 × 10, porque al acercarnos al final de la red neuronal reduciremos el tamaño de la imagen a un vector de estimaciones de clase ubicadas a lo largo de la profundidad (tercera dimensión). </p><br><p>  Visualización: </p><br><div class="scrollable-table"><table><thead><tr><th>  Red neuronal estándar </th><th>  Red neuronal convolucional </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/2da/120/014/2da120014faf76c47fa4294c7206e291.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/d45/f30/a26/d45f30a26e57d437828f90567867c96f.jpg"></td></tr></tbody></table></div><br><hr><br><p>  <em>Lado izquierdo:</em> red neuronal estándar de 3 capas. <br>  <em>En el lado derecho: la</em> red neuronal convolucional tiene sus neuronas en 3 dimensiones (ancho, alto, profundidad), como se muestra en una de las capas.  Cada capa de red neuronal convolucional convierte una representación 3D de la entrada en una representación 3D de la salida como neuronas de activación.  En este ejemplo, la capa de entrada roja contiene la imagen, por lo que su tamaño será igual al tamaño de la imagen y la profundidad será 3 (tres canales: rojo, verde, azul). </p><br><blockquote>  La red neuronal convolucional consiste en capas.  Cada capa es una API simple: convierte la representación 3D de entrada en la representación 3D de salida de una función diferenciable, que puede contener o no parámetros. </blockquote><br><h1>  Capas usadas para construir redes neuronales convolucionales </h1><br><p>  Como ya hemos descrito anteriormente, una red neuronal convolucional simple es un conjunto de capas, donde cada capa convierte una representación en otra utilizando una función diferenciable.  Utilizamos tres tipos principales de capas para construir redes neuronales convolucionales: una <em>capa convolucional</em> , <em>una</em> <em>capa de</em> <em>submuestreo</em> y una <em>capa totalmente conectada</em> (lo mismo que usamos en una red neuronal normal).  Organizamos estas capas secuencialmente para obtener la arquitectura SNA. </p><br><p> <em>Ejemplo de arquitectura: descripción general.</em>  A continuación profundizaremos en los detalles, pero por ahora, para el conjunto de datos CIFAR-10, la arquitectura de nuestra red neuronal convolucional puede ser <code>[INPUT -&gt; CONV -&gt; RELU -&gt; POOL -&gt; FC]</code> .  Ahora con más detalle: </p><br><ul><li>  <code>INPUT</code> [32x32x3] contendrá los valores originales de los píxeles de la imagen, en nuestro caso la imagen tiene 32px de ancho, 32px de alto y 3 canales de color R, G, B. </li><li>  <code>CONV</code> capa <code>CONV</code> producirá un conjunto de neuronas de salida que se asociarán con el área local de la imagen fuente de entrada;  cada una de esas neuronas calculará el producto escalar entre sus pesos y esa pequeña parte de la imagen original con la que está asociada.  El valor de salida puede ser una representación 3D de <code>323212</code> , si, por ejemplo, decidimos usar 12 filtros. </li><li>  <code>RELU</code> capa <code>RELU</code> aplicará la función de activación del elemento <code>max(0, x)</code> .  Esta conversión no cambiará la dimensión de datos - <code>[32x32x12]</code> . </li><li>  <code>POOL</code> capa <code>POOL</code> realizará la operación de muestreo de la imagen en dos dimensiones: altura y ancho, lo que como resultado nos dará una nueva representación 3D <code>[161612]</code> . </li><li>  <code>FC</code> capa <code>FC</code> (capa totalmente conectada) calculará las calificaciones por clases, la dimensión resultante será <code>[1x1x10]</code> , donde cada uno de los 10 valores corresponderá a las calificaciones de una clase particular entre 10 categorías de imágenes de CIFAR-10.  Como en las redes neuronales convencionales, cada neurona de esta capa estará asociada con todas las neuronas de la capa anterior (representación 3D). </li></ul><br><p>  Así es como la red neuronal convolucional transforma la imagen original, capa por capa, desde el valor de píxel inicial hasta la estimación de clase final.  Tenga en cuenta que algunas capas contienen opciones y otras no.  En particular, las capas <code>CONV/FC</code> llevan a cabo una transformación, que no solo es una función que depende de los datos de entrada, sino que también depende de los valores internos de pesos y desplazamientos en las neuronas mismas.  <code>RELU/POOL</code> capas <code>RELU/POOL</code> , <code>RELU/POOL</code> otro lado, usan funciones no parametrizadas.  Los parámetros en las capas <code>CONV/FC</code> se entrenarán por gradiente descendente para que la entrada reciba las etiquetas de salida correctas correspondientes. </p><br><p>  Para resumir: </p><br><ul><li>  La arquitectura de la red neuronal convolucional, en su representación más simple, es un conjunto ordenado de capas que transforma la representación de una imagen en otra representación, por ejemplo, estimaciones de miembros de clase. </li><li>  Hay varios tipos diferentes de capas (CONV - capa convolucional, FC - completamente conectado, RELU - función de activación, POOL - capa de submuestra - el más popular). </li><li>  Cada capa de entrada recibe una representación 3D, la convierte en una representación 3D de salida utilizando una función diferenciable. </li><li>  Cada capa puede y no tener parámetros (CONV / FC - tiene parámetros, RELU / POOL - no). </li><li>  Cada capa puede y no tener hiperparámetros (CONV / FC / POOL - have, RELU - no) </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d92/59b/e82/d9259be829b1cdb3d98a399ebc56defa.jpg"><br>  <em>La representación inicial contiene los valores de píxeles de la imagen (a la izquierda) y estimaciones de las clases a las que pertenece el objeto en la imagen (a la derecha).</em>  <em>Cada transformación de vista se marca como una columna.</em> </p><br><h1>  Capa convolucional </h1><br><p>  <em>La capa convolucional</em> es la capa principal en la construcción de redes neuronales convolucionales. </p><br><p>  <em>Descripción general sin sumergirse en las características del cerebro.</em>  Primero intentemos averiguar qué sigue calculando la capa CONV sin sumergirnos y tocar el tema del cerebro y las neuronas.  Los parámetros de la capa convolucional consisten en un conjunto de filtros entrenados.  Cada filtro es una cuadrícula pequeña a lo largo del ancho y la altura, pero se extiende por toda la profundidad de la representación de entrada. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/497/c0f/e18/497c0fe1851288e36fad00c004d4f9cf.png"></p><br><p>  Por ejemplo, un filtro estándar en la primera capa de una red neuronal convolucional puede tener dimensiones de 5x5x3 (5px - ancho y alto, 3 - el número de canales de color).  Durante un pase directo, movemos (para ser exactos: colapsamos) el filtro a lo largo del ancho y alto de la representación de entrada y calculamos el producto escalar entre los valores del filtro y los valores correspondientes de la representación de entrada en cualquier punto.  En el proceso de mover el filtro a lo largo del ancho y alto de la representación de entrada, formamos un mapa de activación bidimensional que contiene los valores de aplicar este filtro a cada una de las áreas de la representación de entrada.  Intuitivamente, queda claro que la red enseñará a los filtros a activarse cuando vean un cierto signo visual, por ejemplo, una línea recta en un cierto ángulo o representaciones en forma de rueda en niveles superiores.  Ahora que hemos aplicado todos nuestros filtros a la imagen original, por ejemplo, había 12. Como resultado de la aplicación de 12 filtros, recibimos 12 tarjetas de activación de dimensión 2. Para producir una representación de salida, combinamos estas tarjetas (secuencialmente en la tercera dimensión) y obtenemos una representación dimensión [WxHx12]. </p><br><p>  <em>Una visión general a la que conectamos el cerebro y las neuronas.</em>  Si eres fanático del cerebro y las neuronas, puedes imaginar que cada neurona "mira" una gran sección de la representación de entrada y transfiere información sobre esta sección a las neuronas vecinas.  A continuación discutiremos los detalles de la conectividad neuronal, su ubicación en el espacio y el mecanismo para compartir parámetros. </p><br><p>  <em>Conectividad local.</em>  Cuando se trata de datos de entrada con una gran cantidad de dimensiones, por ejemplo, como en el caso de las imágenes, entonces, como ya hemos visto, no hay absolutamente ninguna necesidad de conectar las neuronas con todas las neuronas de la capa anterior.  En cambio, solo conectaremos neuronas a áreas locales de la representación de entrada.  El grado espacial de conectividad es uno de los hiperparámetros y se denomina <em>campo receptivo</em> (el campo receptivo de una neurona es del tamaño del mismo núcleo de filtro / convolución).  El grado de conectividad a lo largo de la tercera dimensión (profundidad) siempre es igual a la profundidad de la representación original.  Es muy importante enfocarse nuevamente en esto, prestar atención a cómo definimos las dimensiones espaciales (ancho y alto) y la profundidad: las conexiones neuronales son locales en ancho y alto, pero <em>siempre se</em> extienden a lo largo de toda la representación de entrada. </p><br><p>  <em>Ejemplo 1.</em> Imagine que la representación de entrada tiene un tamaño de 32x32x3 (RGB, CIFAR-10).  Si el tamaño del filtro (campo receptivo de la neurona) es 5 × 5, entonces cada neurona en la capa convolucional tendrá pesos en la región 5 × 5 × 3 de la representación original, lo que finalmente conducirá al establecimiento de 5 × 5 × 3 = 75 enlaces (pesos) + 1 parámetro de compensación.  Tenga en cuenta que el grado de conexión en profundidad debe ser igual a 3, ya que esta es la dimensión de la representación original. </p><br><p>  <em>Ejemplo 2.</em> Imagine que la representación de entrada tiene un tamaño de 16x16x20.  Usando como ejemplo el campo receptivo de una neurona de tamaño 3x3, cada neurona de capa convolucional tendrá 3x3x320 = 180 conexiones (pesos) + 1 parámetro de desplazamiento.  Tenga en cuenta que la conectividad es local en ancho y alto, pero completa en profundidad (20). </p><br><div class="scrollable-table"><table><thead><tr><th>  # 1 </th><th>  # 2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/490/db9/7a0/490db97a0f3fa98eb2f44e84764f8991.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/61f/e81/589/61fe81589ab491d1d3ba612b3bdf5b51.jpg"></td></tr></tbody></table></div><br><p>  <em>Desde el lado izquierdo: la</em> representación de entrada se muestra en rojo (por ejemplo, una imagen de tamaño CIFAR-10 de 32x332) y un ejemplo de la representación de neuronas en la primera capa convolucional.  Cada neurona en la capa convolucional está asociada solo con el área local de la representación de entrada, pero completamente en profundidad (en el ejemplo, a lo largo de todos los canales de color).  Tenga en cuenta que hay muchas neuronas en la imagen (en el ejemplo - 5) y están ubicadas a lo largo de la 3ª dimensión (profundidad); a continuación se explicarán estas disposiciones. <br>  <em>En el lado derecho: las</em> neuronas de la red neuronal aún permanecen sin cambios: todavía calculan el producto escalar entre sus pesos y datos de entrada, aplican la función de activación, pero su conectividad ahora está limitada por el área local espacial. </p><br><p>  <em>La ubicación espacial.</em>  Ya hemos descubierto la conectividad de cada neurona en la capa convolucional con la representación de entrada, pero aún no hemos discutido cuántas de estas neuronas o cómo están ubicadas.  Tres hiperparámetros afectan el tamaño de la vista de salida: <em>profundidad</em> , <em>paso</em> y <em>alineación</em> . </p><br><ol><li>  <em>La profundidad de la</em> representación de salida es un hiperparámetro: corresponde a la cantidad de filtros que queremos aplicar, cada uno de los cuales aprende algo más en la representación original.  Por ejemplo, si la primera capa convolucional recibe una imagen como entrada, entonces se pueden activar diferentes neuronas a lo largo de la tercera dimensión (profundidad) en presencia de diferentes orientaciones de líneas en un área determinada o grupos de un color determinado.  Al conjunto de neuronas que "miran" la misma área de la representación de entrada, llamaremos la <em>columna profunda</em> (o "fibra" - fibra). </li><li>  Necesitamos determinar el <em>paso</em> (tamaño de desplazamiento en píxeles) con el que se moverá el filtro.  Si el paso es 1, entonces cambiamos el filtro por 1 píxel en una iteración.  Si el paso es 2 (o, que se usa con menos frecuencia, 3 o más), el desplazamiento se produce por cada dos píxeles en una iteración.  Un paso más grande da como resultado una representación de salida más pequeña. </li><li>  Como veremos pronto, a veces será necesario complementar la representación de entrada a lo largo de los bordes con ceros.  El tamaño de alineación (el número de columnas / filas rellenas con cero) también es un hiperparámetro.  Una buena característica del uso de la alineación es el hecho de que la alineación nos permitirá controlar la dimensión de la representación de salida (a menudo mantendremos las dimensiones originales de la vista, preservando el ancho y la altura de la representación de entrada con el ancho y la altura de la representación de salida). </li></ol><br><p>  Podemos calcular la dimensión final de la representación de salida presentándola en función del tamaño de la representación de entrada ( <strong>W</strong> ), el tamaño del campo receptivo de las neuronas de la capa convolucional ( <strong>F</strong> ), el paso ( <strong>S</strong> ) y el tamaño de la alineación ( <strong>P</strong> ) en los bordes.  Puede ver por sí mismo que la fórmula correcta para calcular el número de neuronas en la representación de salida es la siguiente <strong>(W - F + 2P) / S + 1</strong> .  Por ejemplo, para una representación de entrada de tamaño 7x7 y tamaño de filtro de 3x3, paso 1 y alineación 0, obtenemos una representación de salida de tamaño 5x5.  En el paso 2, obtendríamos una representación de salida de 3x3.  Veamos otro ejemplo, esta vez ilustrado gráficamente: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/90a/f0b/d67/90af0bd67ba498239688c81fd61bbc66.jpg"><br>  <em>Ilustración de una disposición espacial.</em>  <em>En este ejemplo, solo una dimensión espacial (eje x), una neurona con campo receptivo <strong>F = 3</strong> , tamaño de representación de entrada <strong>W = 5</strong> y alineación <strong>P = 1</strong> .</em>  <em><strong>En el lado izquierdo</strong> : el campo receptivo de la neurona se mueve con un paso <strong>S = 1</strong> , que como resultado da el tamaño de la representación de salida (5 - 3 + 2) / 1 + 1 = 5. <strong>En el lado derecho</strong> : la neurona usa el campo receptivo de tamaño <strong>S = 2</strong> , que en el resultado es el tamaño de la representación de salida (5 - 3 + 2) / 2 + 1 = 3. Tenga en cuenta que el tamaño de paso <strong>S = 3</strong> no se puede utilizar, ya que con este tamaño de paso el campo receptivo no capturará parte de la imagen.</em>  <em>Si usamos nuestra fórmula, entonces (5 - 3 + 2) = 4 no es múltiplo de 3. Los pesos de las neuronas en este ejemplo son [1, 0, -1] (como se muestra en la imagen de la derecha), y el desplazamiento es cero.</em>  <em>Estos pesos son compartidos por todas las neuronas amarillas.</em> </p><br><p>  <em>Usando la alineación</em> .  Preste atención al ejemplo en el lado izquierdo, que contiene 5 elementos en la salida y 5 elementos en la salida.  Esto funcionó porque el tamaño del campo receptivo (filtro) era 3 y utilizamos la alineación <strong>P = 1</strong> .  Si no hubiera alineación, entonces el tamaño de la representación de salida sería igual a 3, porque precisamente precisamente tantas neuronas encajarían allí.  En general, establecer el tamaño de alineación <strong>P = (F - 1) / 2</strong> con un paso igual a <strong>S = 1 le</strong> permite obtener el tamaño de la representación de salida similar a la representación de entrada.  Un enfoque similar que usa la alineación a menudo se aplica en la práctica, y discutiremos las razones a continuación cuando hablemos sobre la arquitectura de las redes neuronales convolucionales. </p><br><p>  <em>Límites de tamaño de paso</em> .  Tenga en cuenta que los hiperparámetros responsables de la disposición espacial también están relacionados por limitaciones.  Por ejemplo, si la representación de entrada tiene un tamaño de <strong>W = 10</strong> , <strong>P = 0</strong> y el tamaño del campo receptivo <strong>F = 3</strong> , entonces es imposible usar un tamaño de paso igual a <strong>S = 2</strong> , ya que <strong>(W - F + 2P) / S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5</strong> , que proporciona un valor entero del número de neuronas.  Por lo tanto, dicha configuración de hiperparámetros se considera inválida y las bibliotecas para trabajar con redes neuronales convolucionales generarán una excepción, forzarán la alineación o incluso cortarán la representación de entrada.  Como veremos en las siguientes secciones de este capítulo, la definición de los hiperparámetros de la capa convolucional sigue siendo un dolor de cabeza que puede reducirse mediante el uso de ciertas recomendaciones y "buenas reglas de tono" al diseñar la arquitectura de redes neuronales convolucionales. </p><br><p>  <em>Ejemplo de la vida real</em> .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Arquitectura de</a> red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">convolucional Krizhevsky et al.</a>  , que ganó el concurso ImageNet en 2012, recibió 227x227x3 imágenes.  En la primera capa convolucional, utilizó un campo receptivo de tamaño <strong>F = 11</strong> , paso <strong>S = 4</strong> y tamaño de alineación <strong>P = 0</strong> .  Como (227-11) / 4 + 1 = 55, y la capa convolucional tenía una profundidad de <strong>K = 96</strong> , la dimensión de salida de la presentación fue 55x55x96.  Cada una de las neuronas de 55x55x96 en esta representación se asoció con una región de tamaño 11x11x3 en la representación de entrada.  Además, todas las 96 neuronas en la columna profunda están asociadas con la misma región 11x11x3, pero con diferentes pesos.  Y ahora un poco de humor: si decide familiarizarse con el documento original (estudio), tenga en cuenta que el documento afirma que la entrada recibe imágenes de 224x224, lo que no puede ser cierto, porque (224 - 11) / 4 + 1 de ninguna manera dar un valor entero.  Este tipo de situación a menudo se confunde para las personas en historias con redes neuronales convolucionales.  Supongo que Alex usó el tamaño de alineación <strong>P = 3</strong> , pero olvidó mencionar esto en el documento. </p><br><p>  <em>Opciones para compartir</em>  El mecanismo para compartir parámetros en capas convolucionales se usa para controlar el número de parámetros.  Preste atención al ejemplo anterior, ya que puede ver que hay 55x55x96 = 290,400 neuronas en la primera capa convolucional y cada una de las neuronas tiene 11x11x3 = 363 pesos + 1 valor de compensación.  En total, si multiplicamos estos dos valores, obtenemos 290400x364 = 105 705 600 parámetros <em>solo</em> en la primera capa de la red neuronal convolucional.  ¡Obviamente, esto es de gran importancia! </p><br><p>  Resulta que es posible reducir significativamente el número de parámetros haciendo una suposición: si alguna propiedad calculada en posición (x, y) nos importa, entonces esta propiedad calculada en posición (x2, y2) también nos importará.  En otras palabras, denotando una "capa" bidimensional en profundidad como una "capa profunda" (por ejemplo, la vista [55x55x96] contiene 96 capas profundas, cada una de 55x55 de tamaño), construiremos neuronas en profundidad con los mismos pesos y desplazamiento.  Con este esquema de compartir parámetros, la primera capa convolucional en nuestro ejemplo ahora contendrá 96 conjuntos únicos de pesos (cada conjunto para cada capa de profundidad), en total habrá 96x11x11x3 = 34,848 pesos únicos o 34,944 parámetros (+96 compensaciones).  Además, todas las neuronas de 55x55 en cada capa profunda ahora usarán los mismos parámetros.  En la práctica, durante la propagación hacia atrás, cada neurona en esta representación calculará el gradiente para sus propios pesos, pero estos gradientes se sumarán sobre cada capa de profundidad y actualizarán solo un conjunto único de pesos en cada nivel. </p><br><p>  Tenga en cuenta que si todas las neuronas en la misma capa profunda usaran los mismos pesos, entonces para la propagación directa a través de la capa convolucional, se calcularía la convolución entre los valores de los pesos de las neuronas y los datos de entrada.  Es por eso que es habitual llamar a un solo conjunto de pesos: un <strong>filtro (núcleo)</strong> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd6/2e1/d75/dd62e1d75bda9b592dabb91627d68aa6.jpg"><br>  <em>Ejemplos de filtros que se obtuvieron al entrenar el modelo Krizhevsky et al.</em>  <em>Cada uno de los 96 filtros que se muestran aquí tiene un tamaño de 11x11x3 y cada uno de ellos es compartido por todas las neuronas de 55x55 de una capa profunda.</em>  <em>Tenga en cuenta que la suposición de compartir los mismos pesos tiene sentido: si la detección de una línea horizontal es importante en una parte de la imagen, entonces es intuitivamente claro que dicha detección es importante en otra parte de esta imagen.</em>  <em>Por lo tanto, no tiene sentido volver a entrenar cada vez para encontrar líneas horizontales en cada uno de los 55x55 lugares diferentes de la imagen en la capa convolucional.</em> </p><br><p>  Debe tenerse en cuenta que la suposición de compartir parámetros no siempre tiene sentido.  Por ejemplo, si una imagen con una estructura centrada se alimenta a la entrada de una red neuronal convolucional, donde nos gustaría poder aprender una propiedad en una parte de la imagen y otra propiedad en la otra parte de la imagen.  Un ejemplo práctico son las imágenes de caras centradas.  Se puede suponer que se pueden identificar diferentes signos oculares o capilares en diferentes áreas de la imagen, por lo tanto, en este caso, se usa relajación de pesas y la capa se denomina <strong>conectada localmente</strong> . </p><br><p>  <strong>Numpy ejemplos</strong> .  Las discusiones anteriores deben transferirse al plano de los detalles y a los ejemplos con código.  Imagine que la representación de entrada es una matriz <code>numpy</code> de <code>X</code>  Entonces: </p><br><ul><li>  <em>La columna profunda</em> ( <em>hilo</em> ) en la posición <code>(x,y)</code> se representará como sigue <code>X[x,y,:]</code> . </li><li>  <em>La capa profunda</em> , o como la llamamos anteriormente, <em>el mapa de activación</em> en la profundidad <code>d</code> se representará de la siguiente manera <code>X[:,:,d]</code> . </li></ul><br><p>  <em>Un ejemplo de una capa convolucional</em> . ,    <code>X</code>   <code>X.shape: (11,11,4)</code> .   ,    <strong>P=1</strong> ,    () <strong>F=5</strong>   <strong>S=1</strong> .     44,     — (11-5)/2+1=4.     (  <code>V</code> ),     (      ): </p><br><ul><li> <code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code> </li> <li> <code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code> </li> <li> <code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code> </li> <li> <code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code> </li> </ul><br><p> ,   <code>numpy</code> ,  <code>*</code>      .    ,    <code>W0</code>      <code>b0</code>  .    <code>W0</code>   <code>W0.shape: (5,5,4)</code> ,      5,    4.                   .       ,           ,        2  ( ).             : </p><br><ul><li> <code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code> </li> <li> <code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code> </li> <li> <code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code> </li> <li> <code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code> </li> <li> <code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (,       <code>y</code> ) </li><li> <code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (,      ) </li></ul><br><p>         —    <code>W1</code>   <code>b1</code> .      ,               <code>V</code> .    ,         , , <code>ReLU</code> ,        .      . </p><br><p> <strong></strong> .     : </p><br><ul><li>      <strong>W1 x H1 x D1</strong> </li><li>  4 -: <br><ul><li>   <strong>K</strong> , </li><li>    <strong>F</strong> , </li><li>   <strong>S</strong> , </li><li>   <strong>P</strong> . </li></ul></li><li>     <strong>W2 x H2 x D2</strong> ,  <br><ul><li> <strong>W2 = (W1 — F + 2P)/S + 1</strong> </li><li> <strong>H2 = (H1 — F + 2P)/S + 1</strong> </li><li> <strong>D2 = K</strong> </li></ul></li><li>      <strong>F x F x D1</strong>    ,  <strong>(F x F x D1) x K</strong>   <strong>K</strong> . </li><li>   , <code>d</code> - ( <strong>W2 x H2</strong> )       <code>d</code> -      <strong>S</strong>      <code>d</code> -. </li></ul><br><p>    -  <strong>F = 3, S = 1, P = 1</strong> .        .      "   ". </p><br><p> <strong>.</strong>        .   3D-   ( —  ,  — ,  —  ),        —   .    <strong>W1 = 5, H1 = 5, D1 = 3</strong> ,     <strong>K = 2, F = 3, S = 2, P = 1</strong> . ,       33,     2.        (5 — 3 + 2)/2 + 1 = 3.  ,  ,   <strong>P = 1</strong>        .         ,         ()  ,   . </p><br><p> (   ,     html+css   ,       ) </p><br><p> <strong>    </strong> .               ().                   : </p><br><ol><li>        <strong>im2col</strong> . ,        227x227x3         11113   4,           11113 = 363 .   ,     4    ,   (227 — 11) / 4 + 1 = 55     ,          <strong>X_col</strong>  3633025,               3025.  ,   ,    ,  (),           . </li><li>          . ,    96   11113,       <strong>W_row</strong>  96363. </li><li>               — <strong>np.dot(W_row, X_col)</strong> ,           .         963025. </li><li>              555596. </li></ol><br><p>   , ,     —              ,    . ,    ,      —        (,    BLAS API).  ,    <strong>im2col</strong>        ,        . </p><br><p> <strong> </strong> .   (  )   (   ,    )     (  - ).     ,     . </p><br><p> <strong>11 </strong> .          11,       <a href="">Network in Network</a> .  ,      11,   ,       . ,    2-  ,   11    (     ).         ,       ,         3-  ,         . ,     32323,          11, ,  ,        3  (R, G, B —  , ). </p><br><p> <strong>   </strong> .      -      <em></em> .           .           ,   <em></em> .       <strong>w</strong>  3     <strong>x</strong> : <strong>w[0] <em>x[0] + w[1]</em> x[1] + w[2] <em>x[2] <strong>.      0.    1       :</strong> w[0]</em> x[0] + w[1] <em>x[2] + w[2]</em> x[4]</strong> .      ""  1    .        ,             . ,    2    33,     ,             55 (   55 <em>  </em> ).              . </p><br><h1>   </h1><br><p>   —           .         ,             ,        .          ,       MAX.     22   2,        2 ,    75% .   MAX            22.     .   ,  : </p><br><ul><li>    <strong>W1 x H1 x D1</strong> </li><li>  2 -: <br><ul><li>    <strong>F</strong> , </li><li>  <strong>S</strong> , </li></ul></li><li>    <strong>W2 x H2 x D2</strong> , : <br><ul><li> <strong>W2 = (W1 — F)/S + 1</strong> </li><li> <strong>H2 = (H1 — F)/S + 1</strong> </li><li> <strong>D2 = D1</strong> </li></ul></li><li>   ,         </li><li>       (zero-padding    ). </li></ul><br><p>    ,           :    <strong>F=3, S=2</strong> (   <em> </em> ),    — <strong>F=2, S=2</strong> .      -   . </p><br><p> <strong>   </strong> .      ,       , ,       L2-.         ,           ,      . </p><br><div class="scrollable-table"><table><thead><tr><th> #1 </th><th> #2 </th></tr></thead><tbody><tr><td><img src="https://habrastorage.org/getpro/habr/post_images/cd7/174/14d/cd717414dcf32dac4df73c00f1e7c6c3.jpg"></td><td><img src="https://habrastorage.org/getpro/habr/post_images/1a4/b2a/379/1a4b2a3795d8f073e921d766e70ce6ec.jpg"></td></tr></tbody></table></div><br><p> <em>              . <strong></strong> :       22422464        22   2,     11211264.  ,      . <strong></strong> :     —    (max-pooling),      2.      4  (   22)</em> </p><br><p> <strong> </strong> .      ,     max(a,b)    —            .  ,              (  <em></em> ),        . </p><br><p> <strong>  </strong> .        ,           . ,   <a href="">  :   </a> ,           .             .              ,     (VAEs)     (GANs). ,     -   ,    . </p><br><h1>   </h1><br><p>            ,      ,       . ,      ,            .         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> </a> . </p><br><h1>   </h1><br><p>           ,       .             . </p><br><h1>       </h1><br><p>  ,           ,                 (  ).      -   ,        .    ,         : </p><br><ul><li>       ,       .  ,   ,   ,   ,                  ,     . </li><li> ,         . ,    <strong>K=4096</strong> ( ),     7712          - <strong>F=7, P=0, S=1, K=4096</strong> .        ,               114096,      . </li></ul><br><p> <strong>    </strong> .    ,            .        ,       2242243                  77512 (     AlexNet,    ,     5  ,           7 — 224/2/2/2/2/2 = 7).   AlexNet      4096 , ,     1000 ,     .               : </p><br><ul><li>    ,  ""    77512,       <strong>F=7</strong> ,       114096. </li><li>           <strong>F=1</strong> ,      114096. </li><li>           <strong>F=1</strong> ,      111000. </li></ul><br><p>    ,  ,     (   )   <strong>W</strong>         . ,       "" ()             . </p><br><p> ,     224224  ,  77512   —    32 ,        384384       1212512,   384/32 = 12.                 ,    ,    ,   661000,   (12 — 7)/1 + 1 = 6.  ,        111000     66    384384 . </p><br><blockquote>        (  )     384384,   224244   32 ,    ,        . </blockquote><p>  ,             ,      36 ,    36    .        ,    ,         .                   . </p><br><p> ,           ,     32 ?          (   ). ,        16 ,             2 :               16     . </p><br><h1>     </h1><br><p>        ,  ,   3   :  ,   (    ,    )   .        ReLU   ,   -  .                . </p><br><p>            CONV-RELU-,    POOL-       ,        .  -      .      , ,   .  ,        : </p><br><pre> <code class="plaintext hljs">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?] * M -&gt; [FC -&gt; RELU]*K -&gt; FC</code> </pre> <br><p>   <code>*</code>  ,  <code>POOL?</code>    .  , <code>N &gt;= 0</code> ( <code>N &lt;= 3</code> ), <code>M &gt;= 0</code> , <code>K &gt;= 0</code> ( <code>K &lt; 3</code> ). ,       ,     : </p><br><ul><li> <code>INPUT -&gt; FC</code> ,   . <code>N = M = K = 0</code> . </li><li> <code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code> </li> <li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL] * 2 -&gt; FC -&gt; RELU -&gt; FC</code> ,           . </li><li> <code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL] * 3 -&gt; [FC -&gt; RELU] * 2 -&gt; FC</code> .     2      .  ,        ,                   . </li></ul><br><p> <em>               </em> .      3    33 ( RELU   ,  ).          ""   33  .      ""   33     ,     —     55.        ""  33     ,    —   77.  ,      33           77.         ""  77 (  )      ,    . -,        ,      3      ,       . -,        <strong>C</strong> ,   ,    77     <strong>(C(77)) = 49xxC</strong> ,        33    <strong>3((33)) = 27</strong> .   ,                 ,            .           —          ,      . </p><br><p> <strong></strong> .    ,       ,       Google,         Microsoft.               . </p><br><p> <strong> :  ,        ImageNet.</strong>                ,    ,   90%       .       — "  ":  ,       ,        ,         ImageNet —   ,       .               . </p><br><h1>      </h1><br><p>         -,        .    ,      : </p><br><p> <strong> </strong> ( )    2  .    32 (, CIFAR-10), 64, 96 (, STL-10),  224 (, ImageNet), 384  512. </p><br><p>  <strong> </strong>      (, 33 ,  55),    <strong>S=1</strong> ,    ,    ,         .  , <strong>F=3</strong>   <strong>P=1</strong>        .  <strong>F=5, P=2</strong> .    <strong>F</strong>   ,   <strong>P=(F-1)/2</strong>     .   -       (  77),                 . </p><br><p> <strong> </strong>      .            22 ( <strong>F=2</strong> )   2 ( <strong>S=2</strong> ). ,     75%    (- ,       ). ,   ,     33 ( )  2 ( ).       33   ,               .       . </p><br><p> <em>      .</em>     ,          ,        .   ,      1       ,                   ,             . </p><br><p> <em>     1   ?</em>       .     ,    1        (       ),        . </p><br><p> <em>  ?</em>                ,    .         ,     ,            ,          . </p><br><p> <em>    </em> .    (        ),       ,     . ,      64     33   1   2242243,       22422464. , ,  10  ,  72   ( ,       ).            GPU,     .  ,             77   2.  ,     AlexNet,    1111   4. </p><br><h1>    </h1><br><p>           .    : </p><br><ul><li> <strong>LeNet</strong> .         Yann LeCun  1990.       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LeNet</a> ,     ZIP-,   . </li><li> <strong>AlexNet</strong> .  ,        ,  Alex Krizhevsky, Ilya Sutskever  Geoff Hinton. AlexNet     ImageNet ILSVRC  2012         ( : 16%  26%).        LeNet,   ,       (               ). </li><li> <strong>ZFNet</strong> .  ILSVRC 2013       Matthew Zeiler  Rob Fergus.      ZFNet.     AlexNet,     -,                 . </li><li> <strong>GoogLeNet</strong> .  ILSVRC 2014       Szegedy et al.  Google.      Inception-,         (4   60   AlexNet).                ,      ,     .       ,    — Inveption-v4. </li><li> <strong>VGGNet</strong> .    2014 ILSVRC    Karen Simonyan  Andrew Zisserman,       VGGNet.               ,        .      16   +        (33    22   ).            .    VGGNet —        (140).          ,       ,          ,       . </li><li> <strong>ResNet</strong> . Residual-   Kaiming He et al.     ILSVRC 2015.        .          .            (  2016). </li></ul><br><p> <strong>VGGNet  </strong> .   VGGNet    .   VGGNet    ,         33,  1   1,       22   2.          (     )    : </p><br><pre> <code class="plaintext hljs">INPUT: [224x224x3] memory: 224*224*3=150K weights: 0 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*3)*64 = 1,728 CONV3-64: [224x224x64] memory: 224*224*64=3.2M weights: (3*3*64)*64 = 36,864 POOL2: [112x112x64] memory: 112*112*64=800K weights: 0 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*64)*128 = 73,728 CONV3-128: [112x112x128] memory: 112*112*128=1.6M weights: (3*3*128)*128 = 147,456 POOL2: [56x56x128] memory: 56*56*128=400K weights: 0 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*128)*256 = 294,912 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 CONV3-256: [56x56x256] memory: 56*56*256=800K weights: (3*3*256)*256 = 589,824 POOL2: [28x28x256] memory: 28*28*256=200K weights: 0 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*256)*512 = 1,179,648 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [28x28x512] memory: 28*28*512=400K weights: (3*3*512)*512 = 2,359,296 POOL2: [14x14x512] memory: 14*14*512=100K weights: 0 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 CONV3-512: [14x14x512] memory: 14*14*512=100K weights: (3*3*512)*512 = 2,359,296 POOL2: [7x7x512] memory: 7*7*512=25K weights: 0 FC: [1x1x4096] memory: 4096 weights: 7*7*512*4096 = 102,760,448 FC: [1x1x4096] memory: 4096 weights: 4096*4096 = 16,777,216 FC: [1x1x1000] memory: 1000 weights: 4096*1000 = 4,096,000 TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd) TOTAL params: 138M parameters</code> </pre> <br><p>          ,    ,     (  )     ,        .        100     140 . </p><br><h1>   </h1><br><p>           .   GPU  3/4/6  ,   GPU — 12  .      ,    : </p><br><ul><li>  :           ,      (  ). ,      .          ,                         . </li><li>  : ,    ,         .    ,      ,     3  . </li><li>            ,         ,      .. </li></ul><br><p>           (,   ),          .    ,    4     (      4 ,       —  8),       1024  ,    ,      .   " ",        ,        . </p><br><p> …   call-to-action — ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Telegrama</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">VKontakte</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456186/">https://habr.com/ru/post/456186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456174/index.html">Cómo organizar un hackathon como estudiante 101. Segunda parte</a></li>
<li><a href="../456178/index.html">Temas y estilos de Android sin magia. Y cómo cocinarlos con SwitchCompat</a></li>
<li><a href="../456180/index.html">¿Cómo encontré mi primera vulnerabilidad?</a></li>
<li><a href="../456182/index.html">Audio a través de Bluetooth: información más detallada sobre perfiles, códecs y dispositivos</a></li>
<li><a href="../456184/index.html">Radio definida por software: ¿cómo funciona? Parte 8</a></li>
<li><a href="../456188/index.html">Token, token de actualización y creación de un contenedor asíncrono para una solicitud REST</a></li>
<li><a href="../456192/index.html">De monolitos a microservicios: la experiencia de M.Video-Eldorado y MegaFon</a></li>
<li><a href="../456194/index.html">Ir a estructuras de datos hoja de trucos</a></li>
<li><a href="../456196/index.html">Conceptos erróneos básicos sobre SCRUM</a></li>
<li><a href="../456200/index.html">Historia de Internet: ARPANET - El origen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>