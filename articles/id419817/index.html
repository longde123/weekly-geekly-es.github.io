<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò† ü¶ã üë®‚Äç‚ù§Ô∏è‚Äçüë® Meluncurkan Cluster RabbitMQ di Kubernetes üßú üèïÔ∏è üñêüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dalam kasus organisasi layanan microser dari aplikasi, pekerjaan substansial bertumpu pada mekanisme koneksi integrasi layanan microser. Selain itu, i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Meluncurkan Cluster RabbitMQ di Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/419817/">  Dalam kasus organisasi layanan microser dari aplikasi, pekerjaan substansial bertumpu pada mekanisme koneksi integrasi layanan microser.  Selain itu, integrasi ini harus toleran terhadap kesalahan, dengan tingkat ketersediaan yang tinggi. <br><br>  Dalam solusi kami, kami menggunakan integrasi dengan Kafka, gRPC, dan RabbitMQ. <br><br>  Pada artikel ini, kami akan berbagi pengalaman kami tentang pengelompokan RabbitMQ, yang simpulnya di-host di Kubernetes. <br><br><img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="gambar"><br><br>  Sebelum RabbitMQ versi 3.7, mengelompokkannya dalam K8S bukanlah tugas yang sangat sepele, dengan banyak peretasan dan bukan solusi yang sangat indah.  Pada versi 3.6, plugin autocluster dari RabbitMQ Community digunakan.  Dan di 3,7 Kubernetes Peer Discovery Backend muncul.  Ini built-in oleh plug-in dalam pengiriman dasar RabbitMQ dan tidak memerlukan perakitan dan instalasi terpisah. <br><br>  Kami akan menggambarkan konfigurasi final secara keseluruhan, sambil mengomentari apa yang terjadi. <br><a name="habracut"></a><br><h2>  Secara teori </h2><br>  Plugin memiliki <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori di github</a> , di mana ada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">contoh penggunaan dasar</a> . <br>  Contoh ini tidak dimaksudkan untuk Produksi, yang secara jelas ditunjukkan dalam uraiannya, dan terlebih lagi, beberapa pengaturan di dalamnya diatur bertentangan dengan logika penggunaan dalam produk.  Juga, dalam contoh ini, kegigihan penyimpanan tidak disebutkan sama sekali, jadi dalam situasi darurat apa pun cluster kami akan berubah menjadi nihil. <br><br><h2>  Dalam praktek </h2><br>  Sekarang kami akan memberi tahu Anda apa yang Anda hadapi sendiri dan cara menginstal dan mengkonfigurasi RabbitMQ. <br><br>  Mari kita gambarkan konfigurasi semua bagian RabbitMQ sebagai layanan di K8s.  Kami akan segera mengklarifikasi bahwa kami menginstal RabbitMQ di K8 sebagai StatefulSet.  Pada setiap node dari cluster K8s, satu instance dari RabbitMQ akan selalu berfungsi (satu node dalam konfigurasi cluster klasik).  Kami juga akan menginstal panel kontrol RabbitMQ di K8 dan memberikan akses ke panel ini di luar cluster. <br><br><h3>  Hak dan peran: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Hak akses untuk RabbitMQ diambil sepenuhnya dari contoh, tidak diperlukan perubahan di dalamnya.  Kami membuat ServiceAccount untuk kluster kami dan memberikannya izin baca ke Endpoints K8s. <br><br><h3>  Penyimpanan Persisten: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Di sini kami mengambil kasus paling sederhana sebagai penyimpanan persisten - hostPath (folder reguler pada setiap simpul K8), tetapi Anda dapat menggunakan salah satu dari banyak jenis volume persisten yang didukung oleh K8. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Buat Klaim Volume pada volume yang dibuat pada langkah sebelumnya.  Klaim ini kemudian akan digunakan di StatefulSet sebagai penyimpan data yang persisten. <br><br><h3>  Layanan: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Kami membuat layanan tanpa kepala internal di mana plugin Peer Discovery akan berfungsi. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  Untuk aplikasi di K8 agar berfungsi dengan kluster kami, kami membuat layanan penyeimbang. <br><br>  Karena kita membutuhkan akses ke cluster RabbitMQ di luar K8s, kita menggulir melalui NodePort.  RabbitMQ akan tersedia ketika mengakses node cluster K8 pada port 31673 dan 30673. Dalam pekerjaan nyata, tidak ada kebutuhan besar untuk ini.  Pertanyaan tentang kenyamanan menggunakan panel admin RabbitMQ. <br><br>  Saat membuat layanan dengan tipe NodePort di K8s, layanan dengan tipe ClusterIP juga secara implisit dibuat untuk melayaninya.  Oleh karena itu, aplikasi dalam K8 yang perlu bekerja dengan RabbitMQ kami akan dapat mengakses cluster di <i>amqp: // rabbitmq: 5672</i> <br><br><h3>  Konfigurasi: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Kami membuat file konfigurasi RabbitMQ.  Keajaiban utama. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Tambahkan plugin yang diperlukan ke yang diizinkan untuk diunduh.  Sekarang kita dapat menggunakan Peer Discovery otomatis di K8S. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  Kami mengekspos plugin yang diperlukan sebagai backend untuk penemuan rekan. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Tentukan alamat dan port yang melaluinya Anda dapat mencapai apiserver kubernetes.  Di sini Anda dapat menentukan alamat-IP secara langsung, tetapi akan lebih indah untuk melakukannya. <br><br>  Dalam namespace default, sebuah layanan biasanya dibuat dengan nama kubernet yang mengarah ke apiserver k8.  Dalam opsi instalasi K8S yang berbeda, namespace, nama layanan, dan port mungkin berbeda.  Jika ada sesuatu dalam instalasi tertentu yang berbeda, Anda harus memperbaikinya. <br><br>  Sebagai contoh, kita dihadapkan dengan fakta bahwa di beberapa cluster layanan ada di port 443, dan di beberapa 6443. Akan mungkin untuk memahami bahwa ada sesuatu yang salah dalam log awal RabbitMQ, waktu koneksi ke alamat yang ditentukan di sini jelas disorot di sana. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  Secara default, contoh menentukan jenis alamat node cluster RabbitMQ berdasarkan alamat IP.  Tetapi ketika Anda me-restart pod, ia mendapat IP baru setiap saat.  Kejutan!  Cluster sedang sekarat dalam penderitaan. <br><br>  Ubah pengalamatan ke nama host.  StatefulSet menjamin kita tidak berubahnya nama host dalam siklus hidup seluruh StatefulSet, yang sepenuhnya cocok untuk kita. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Karena ketika kita kehilangan salah satu node, kita berasumsi bahwa itu akan pulih cepat atau lambat, kita menonaktifkan penghapusan-diri oleh sekelompok node yang tidak dapat diakses.  Dalam hal ini, segera setelah node kembali online, ia akan memasuki cluster tanpa kehilangan status sebelumnya. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  Parameter ini menentukan tindakan cluster jika kehilangan kuorum.  Di sini Anda hanya perlu membaca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi tentang topik ini</a> dan memahami sendiri apa yang lebih dekat dengan kasus penggunaan tertentu. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Tentukan pilihan panduan untuk antrian baru.  Dengan pengaturan ini, wizard akan memilih node dengan jumlah antrian paling sedikit, sehingga antrian akan didistribusikan secara merata di seluruh node cluster. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  Kami menamai layanan K8 tanpa kepala (dibuat oleh kami sebelumnya) di mana node RabbitMQ akan berkomunikasi satu sama lain. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  Satu hal penting untuk pengalamatan dalam sebuah cluster adalah nama host.  FQDN dari perapian K8s dibentuk sebagai nama pendek (rabbitmq-0, rabbitmq-1) + akhiran (bagian domain).  Di sini kami menunjukkan sufiks ini.  Di K8S, sepertinya <b>. &lt;Nama layanan&gt;. &lt;Namespace name&gt; .svc.cluster.local</b> <br><br>  kube-dns menyelesaikan nama bentuk rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local ke alamat IP pod tertentu tanpa konfigurasi tambahan, yang memungkinkan semua keajaiban pengelompokan berdasarkan nama host menjadi mungkin. <br><br><h3>  Konfigurasi StatefulSet RabbitMQ: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  Sebenarnya, StatefulSet sendiri.  Kami mencatat poin-poin menarik. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  Kami menulis nama layanan tanpa kepala yang digunakan pod berkomunikasi di StatefulSet. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  Tetapkan jumlah replika di cluster.  Di negara kita, itu sama dengan jumlah node yang bekerja K8. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  Ketika salah satu node K8 jatuh, statefulset berusaha untuk mempertahankan jumlah instance dalam set, oleh karena itu, ia menciptakan beberapa perapian pada node K8 yang sama.  Perilaku ini benar-benar tidak diinginkan dan, pada prinsipnya, tidak ada gunanya.  Oleh karena itu, kami meresepkan aturan anti-afinitas untuk set perapian dari statefulset.  Kami membuat aturan sulit (Diperlukan) sehingga kube-scheduler tidak dapat melanggarnya ketika merencanakan pod. <br><br>  Intinya sederhana: dilarang untuk penjadwal untuk menempatkan (dalam namespace) lebih dari satu pod dengan <i>aplikasi: tag rabbitmq</i> pada setiap node.  Kami membedakan <i>node</i> dengan nilai label <i>kubernetes.io/hostname</i> .  Sekarang, jika karena alasan tertentu jumlah node K8S yang bekerja kurang dari jumlah replika yang diperlukan di StatefulSet, replika baru tidak akan dibuat sampai simpul bebas muncul lagi. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  Kami mendaftarkan ServiceAccount, tempat pod kami bekerja. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  Gambar RabbitMQ sepenuhnya standar dan diambil dari hub docker, tidak memerlukan pembangunan kembali dan revisi file. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  Data persisten dari RabbitMQ disimpan di / var / lib / rabbitmq / mnesia.  Di sini kita memasang Klaim Volume Persisten kami di folder ini sehingga ketika memulai kembali tungku / node atau bahkan seluruh StatefulSet, data (kedua layanan, termasuk tentang kumpulan rakitan, dan data pengguna) aman dan sehat.  Ada beberapa contoh di mana seluruh folder / var / lib / rabbitmq / dibuat persisten.  Kami sampai pada kesimpulan bahwa ini bukan ide terbaik, karena pada saat yang sama semua informasi yang ditetapkan oleh konfigurasi Kelinci mulai diingat.  Artinya, untuk mengubah sesuatu dalam file konfigurasi, Anda perlu membersihkan penyimpanan persisten, yang sangat tidak nyaman dalam pengoperasian. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  Dengan set variabel lingkungan ini, pertama-tama kami memberi tahu RabbitMQ untuk menggunakan nama FQDN sebagai pengidentifikasi untuk anggota cluster, dan kedua, kami mengatur format nama ini.  Format telah dijelaskan sebelumnya ketika mem-parsing konfigurasi. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  Nama layanan tanpa kepala untuk komunikasi antara anggota cluster. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  Isi Cookie Erlang harus sama pada semua node cluster, Anda harus mendaftarkan nilai Anda sendiri.  Node dengan cookie berbeda tidak bisa masuk ke cluster. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  Tetapkan volume yang dipetakan dari Klaim Volume Persisten yang dibuat sebelumnya. <br><br>  Di sinilah kita selesai dengan pengaturan di K8s.  Hasilnya adalah cluster RabbitMQ, yang mendistribusikan antrian di antara node secara merata dan tahan terhadap masalah di lingkungan runtime. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="gambar"><br><br>  Jika salah satu node cluster tidak tersedia, antrian yang ada di dalamnya akan berhenti diakses, semuanya akan terus berfungsi.  Segera setelah node kembali ke operasi, ia akan kembali ke cluster, dan antrian yang menjadi Master akan operasional kembali, menjaga semua data yang terkandung di dalamnya (jika penyimpanan persisten tidak rusak, tentu saja).  Semua proses ini sepenuhnya otomatis dan tidak memerlukan intervensi. <br><br><h2>  Bonus: sesuaikan HA </h2><br>  Salah satu proyeknya adalah nuansa.  Persyaratan terdengar mirroring lengkap dari semua data yang terkandung dalam cluster.  Ini diperlukan agar dalam situasi di mana setidaknya satu node cluster operasional, semuanya terus bekerja dari sudut pandang aplikasi.  Momen ini tidak ada hubungannya dengan K8, kami menggambarkannya hanya sebagai mini how-to. <br><br>  Untuk mengaktifkan HA penuh, Anda perlu membuat Kebijakan di dasbor RabbitMQ pada <i>Admin -&gt;</i> tab <i>Kebijakan</i> .  Namanya arbitrer, Pola kosong (semua antrian), di Definisi menambahkan dua parameter: <i>mode-ha: semua</i> , <i>mode-ha-sinkronisasi: otomatis</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="gambar"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="gambar"><br><br>  Setelah itu, semua antrian yang dibuat dalam kluster akan berada dalam mode Ketersediaan Tinggi: jika simpul Master tidak tersedia, salah satu Budak akan secara otomatis dipilih oleh wizard baru.  Dan data yang masuk ke antrian akan dicerminkan ke semua node cluster.  Yang sebenarnya harus diterima. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="gambar"><br><br>  Baca lebih lanjut tentang HA di RabbitMQ di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> <br><br><h2>  Literatur yang berguna: </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Plugin RabbitMQ Peer Discovery Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Contoh Konfigurasi untuk Penggunaan RabbitMQ di K8S</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deskripsi prinsip pembentukan klaster, mekanisme Peer Discovery dan plugin untuknya</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Diskusi epik tentang pengaturan penemuan berbasis hostname yang tepat</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Panduan Pengelompokan RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deskripsi masalah dan solusi pengelompokan otak terpisah</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Antrian Ketersediaan Tinggi di RabbitMQ</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Konfigurasikan Kebijakan</a> </li></ul><br>  Semoga beruntung </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id419817/">https://habr.com/ru/post/id419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id419805/index.html">The Super Tiny Compiler - sekarang dalam bahasa Rusia</a></li>
<li><a href="../id419807/index.html">Glaukoma - bagaimana tidak menjadi buta: mari kita bicara tentang perawatan ...</a></li>
<li><a href="../id419811/index.html">Evolusi tampilan yang fleksibel</a></li>
<li><a href="../id419813/index.html">Webinar Skillbox: pilihan hari Jumat</a></li>
<li><a href="../id419815/index.html">Rahasia toleransi kesalahan kantor depan kami</a></li>
<li><a href="../id419819/index.html">Biomarker penuaan. Panel Frailty. Bagian 2</a></li>
<li><a href="../id419823/index.html">Duet yang tidak biasa - frasa sandi dan gambar mnemonik</a></li>
<li><a href="../id419825/index.html">Menguji kinerja beberapa jenis drive di lingkungan virtual</a></li>
<li><a href="../id419829/index.html">Enkripsi kunci default OpenSSH lebih buruk daripada tidak sama sekali</a></li>
<li><a href="../id419831/index.html">Cara Kerja JS: Elemen Kustom</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>