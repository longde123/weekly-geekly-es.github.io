<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è∫Ô∏è üî© üìã Tiefes Lernen. F√∂deriertes Lernen üåÑ üî∏ üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë©</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habrozhiteli! Wir haben das Buch k√ºrzlich Andrew W. Trask √ºbergeben und damit den Grundstein f√ºr die weitere Beherrschung der Deep-Learning-Tech...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tiefes Lernen. F√∂deriertes Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/458800/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/xs/t-/oc/xst-oc7auy1he8nhwbj7bbkhxwk.jpeg" align="left" alt="Bild"></a>  Hallo habrozhiteli!  Wir haben das Buch k√ºrzlich Andrew W. Trask √ºbergeben und damit den Grundstein f√ºr die weitere Beherrschung der Deep-Learning-Technologie gelegt.  Es beginnt mit einer Beschreibung der Grundlagen neuronaler Netze und untersucht anschlie√üend zus√§tzliche Schichten und Architekturen im Detail. <br><br>  Wir bieten einen R√ºckblick auf die Passage "Federated Learning" <br><br>  Die Idee des f√∂derierten Lernens entstand aus der Tatsache, dass viele Daten, die n√ºtzliche Informationen zur L√∂sung von Problemen enthalten (z. B. zur Diagnose onkologischer Erkrankungen mittels MRT), nur schwer in ausreichenden Mengen zu erhalten sind, um ein leistungsf√§higes Deep-Learning-Modell zu vermitteln.  Zus√§tzlich zu den n√ºtzlichen Informationen, die zum Trainieren des Modells erforderlich sind, enthalten die Datens√§tze auch andere Informationen, die f√ºr die jeweilige Aufgabe nicht relevant sind. Die Weitergabe an Dritte kann jedoch m√∂glicherweise sch√§dlich sein. <br><br>  Federated Learning ist eine Technik, mit der ein Modell in eine sichere Umgebung eingeschlossen und gelehrt werden kann, ohne dass Daten irgendwohin verschoben werden m√ºssen.  Betrachten Sie ein Beispiel. <br><a name="habracut"></a><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Counter <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecsnp.random.seed(<span class="hljs-number"><span class="hljs-number">12345</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> codecs.open(<span class="hljs-string"><span class="hljs-string">'spam.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">"r"</span></span>,encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>,errors=<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: ‚Üê     http:<span class="hljs-comment"><span class="hljs-comment">//www2.aueb.gr/users/ion/data/enron-spam/ raw = f.readlines() vocab, spam, ham = (set(["&lt;unk&gt;"]), list(), list()) for row in raw: spam.append(set(row[:-2].split(" "))) for word in spam[-1]: vocab.add(word) with codecs.open('ham.txt',"r",encoding='utf-8',errors='ignore') as f: raw = f.readlines() for row in raw: ham.append(set(row[:-2].split(" "))) for word in ham[-1]: vocab.add(word) vocab, w2i = (list(vocab), {}) for i,w in enumerate(vocab): w2i[w] = i def to_indices(input, l=500): indices = list() for line in input: if(len(line) &lt; l): line = list(line) + ["&lt;unk&gt;"] * (l - len(line)) idxs = list() for word in line: idxs.append(w2i[word]) indices.append(idxs) return indices</span></span></code> </pre> <br><h3>  Lernen, Spam zu erkennen. </h3><br>  <b>Angenommen, wir m√ºssen ein Modell trainieren, um Spam aus den E-Mails von Personen zu erkennen</b> <br><br>  In diesem Fall handelt es sich um eine E-Mail-Klassifizierung.  Wir werden unser erstes Modell auf einem √∂ffentlichen Datensatz namens Enron trainieren.  Dies ist eine gro√üe Anzahl von E-Mails, die w√§hrend der Enron-Anh√∂rungen ver√∂ffentlicht wurden (jetzt ein Standard-E-Mail-Analyse-Text).  Eine interessante Tatsache: Ich war mit Leuten vertraut, die aufgrund ihrer Aktivit√§ten diesen Datensatz lesen / kommentieren mussten, und sie stellten fest, dass sich die Leute in diesen Briefen eine Vielzahl von Informationen schickten (oft sehr pers√∂nlich).  Da dieses Korps jedoch w√§hrend des Prozesses ver√∂ffentlicht wurde, kann es jetzt ohne Einschr√§nkung verwendet werden. <br><br>  Der Code im vorherigen und in diesem Abschnitt implementiert nur vorbereitende Vorg√§nge.  Eingabedateien (ham.txt und spam.txt) sind auf der Webseite des Buches verf√ºgbar: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.manning.com/books/grokking-deep-learning</a> und im GitHub-Repository: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/iamtrask/Grokking-Deep-Learning</a> .  Wir m√ºssen es vorverarbeiten, um es f√ºr die √úbertragung in die Embedding-Klasse aus Kapitel 13 vorzubereiten, wo wir unser Deep-Learning-Framework erstellt haben.  Nach wie vor werden alle W√∂rter in diesem Korpus in Indexlisten konvertiert.  Au√üerdem bringen wir alle Buchstaben auf die gleiche L√§nge von 500 W√∂rtern, indem wir sie entweder zuschneiden oder Token hinzuf√ºgen.  Dank dessen erhalten wir einen rechteckigen Datensatz. <br><br><pre> <code class="javascript hljs">spam_idx = to_indices(spam) ham_idx = to_indices(ham) train_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] train_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] test_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] test_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] train_data = list() train_target = list() test_data = list() test_target = list() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(train_spam_idx),len(train_ham_idx))): train_data.append(train_spam_idx[i%len(train_spam_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) train_data.append(train_ham_idx[i%len(train_ham_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(test_spam_idx),len(test_ham_idx))): test_data.append(test_spam_idx[i%len(test_spam_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) test_data.append(test_ham_idx[i%len(test_ham_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) def train(model, input_data, target_data, batch_size=<span class="hljs-number"><span class="hljs-number">500</span></span>, iterations=<span class="hljs-number"><span class="hljs-number">5</span></span>): n_batches = int(len(input_data) / batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> iter <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(iterations): iter_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_batches): #         model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(input_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) target = Tensor(target_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() loss = criterion.forward(pred,target) loss.backward() optim.step() iter_loss += loss.data[<span class="hljs-number"><span class="hljs-number">0</span></span>] / bs sys.stdout.write(<span class="hljs-string"><span class="hljs-string">"\r\tLoss:"</span></span> + str(iter_loss / (b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>))) print() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model def test(model, test_input, test_output): model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(test_input, autograd=True) target = Tensor(test_output, autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ((pred.data &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>) == target.data).mean()</code> </pre> <br>  Nachdem wir die Hilfsfunktionen train () und test () definiert haben, k√∂nnen wir das neuronale Netzwerk initialisieren und trainieren, indem wir nur wenige Codezeilen schreiben.  Nach drei Iterationen kann das Netzwerk den Kontrolldatensatz mit einer Genauigkeit von 99,45% klassifizieren (der Kontrolldatensatz ist gut ausbalanciert, daher kann dieses Ergebnis als ausgezeichnet angesehen werden): <br><br><pre> <code class="javascript hljs">model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> criterion = MSELoss() optim = SGD(parameters=model.get_parameters(), alpha=<span class="hljs-number"><span class="hljs-number">0.01</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): model = train(model, train_data, train_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) ______________________________________________________________________________ Loss:<span class="hljs-number"><span class="hljs-number">0.037140416860871446</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">98.65</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.011258669226059114</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.15</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.008068268387986223</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.45</span></span></code> </pre> <br><h3>  Machen wir das Modell f√∂deral </h3><br>  <b>Oben wurde das h√§ufigste tiefe Lernen durchgef√ºhrt.</b>  <b>F√ºgen Sie jetzt Privatsph√§re hinzu</b> <br><br>  Im vorherigen Abschnitt haben wir ein Beispiel f√ºr eine E-Mail-Analyse implementiert.  Legen Sie nun alle E-Mails an einem Ort ab.  Dies ist eine gute alte Arbeitsmethode (die weltweit immer noch weit verbreitet ist).  Zun√§chst werden wir das Umfeld der Bundesbildung imitieren, in dem es verschiedene Briefsammlungen gibt: <br><br><pre> <code class="javascript hljs">bob = (train_data[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>]) alice = (train_data[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>]) sue = (train_data[<span class="hljs-number"><span class="hljs-number">2000</span></span>:], train_target[<span class="hljs-number"><span class="hljs-number">2000</span></span>:])</code> </pre> <br>  Noch nichts kompliziertes.  Jetzt k√∂nnen wir das gleiche Trainingsverfahren wie zuvor durchf√ºhren, jedoch bereits f√ºr drei separate Datens√§tze.  Nach jeder Iteration werden die Werte in den Modellen von Bob, Alice und Sue gemittelt und die Ergebnisse ausgewertet.  Bitte beachten Sie, dass einige Verbundlernmethoden das Kombinieren nach jedem Paket (oder jeder Paketsammlung) umfassen.  Ich habe beschlossen, den Code so einfach wie m√∂glich zu halten: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): print(<span class="hljs-string"><span class="hljs-string">"Starting Training Round..."</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\tStep 1: send the model to Bob"</span></span>) bob_model = train(copy.deepcopy(model), bob[<span class="hljs-number"><span class="hljs-number">0</span></span>], bob[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 2: send the model to Alice"</span></span>) alice_model = train(copy.deepcopy(model), alice[<span class="hljs-number"><span class="hljs-number">0</span></span>], alice[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 3: Send the model to Sue"</span></span>) sue_model = train(copy.deepcopy(model), sue[<span class="hljs-number"><span class="hljs-number">0</span></span>], sue[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tAverage Everyone's New Models"</span></span>) model.weight.data = (bob_model.weight.data + \ alice_model.weight.data + \ sue_model.weight.data)/<span class="hljs-number"><span class="hljs-number">3</span></span> print(<span class="hljs-string"><span class="hljs-string">"\t% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"\nRepeat!!\n"</span></span>)</code> </pre> <br><br>  Unten ist ein Ausschnitt mit den Ergebnissen.  Dieses Modell erreichte fast die gleiche Genauigkeit wie das vorherige, und theoretisch hatten wir keinen Zugriff auf Trainingsdaten - oder nicht?  Wie auch immer, aber jede Person √§ndert das Modell im Lernprozess, oder?  K√∂nnen wir nicht wirklich etwas aus ihren Datens√§tzen herausholen? <br><br><pre> <code class="javascript hljs">Starting Training Round... Step <span class="hljs-number"><span class="hljs-number">1</span></span>: send the model to Bob Loss:<span class="hljs-number"><span class="hljs-number">0.21908166249699718</span></span> ...... Step <span class="hljs-number"><span class="hljs-number">3</span></span>: Send the model to Sue Loss:<span class="hljs-number"><span class="hljs-number">0.015368461608470256</span></span> Average Everyone<span class="hljs-string"><span class="hljs-string">'s New Models % Correct on Test Set: 98.8</span></span></code> </pre> <br><h3>  Hacke ein Verbundmodell </h3><br>  <b>Schauen wir uns ein einfaches Beispiel an, wie Informationen aus einem Trainingsdatensatz extrahiert werden.</b> <br><br>  Das f√∂derierte Lernen leidet unter zwei gro√üen Problemen, die besonders schwer zu l√∂sen sind, wenn jede Person nur eine Handvoll Trainingsbeispiele hat - Geschwindigkeit und Vertraulichkeit.  Es stellt sich heraus, dass Sie immer noch viel √ºber die Quelldaten lernen k√∂nnen, wenn jemand nur wenige Trainingsbeispiele hat (oder das an Sie gesendete Modell mit nur wenigen Beispielen trainiert wurde: ein Trainingspaket).  Wenn Sie sich vorstellen, dass Sie 10.000 Personen haben (und jeder eine sehr kleine Datenmenge hat), verbringen Sie die meiste Zeit damit, das Modell hin und her zu senden und nicht so viel zu trainieren (insbesondere wenn das Modell sehr gro√ü ist). <br><br>  Aber lasst uns nicht weiterkommen.  Mal sehen, was Sie herausfinden k√∂nnen, nachdem der Benutzer die Gewichte f√ºr ein Paket aktualisiert hat: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy bobs_email = [<span class="hljs-string"><span class="hljs-string">"my"</span></span>, <span class="hljs-string"><span class="hljs-string">"computer"</span></span>, <span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"is"</span></span>, <span class="hljs-string"><span class="hljs-string">"pizza"</span></span>] bob_input = np.array([[w2i[x] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bobs_email]]) bob_target = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Bob erstellt und trainiert das Modell per E-Mail in seinem Posteingang.  Aber so kam es, dass er sein Passwort speicherte, indem er sich einen Brief mit dem Text schickte: "Mein Computer-Passwort ist Pizza."  Naiver Bob!  Nachdem wir gesehen haben, welche Gewichte sich ge√§ndert haben, k√∂nnen wir das W√∂rterbuch (und die Bedeutung) von Bobs Brief herausfinden: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(bobs_model.weight.data - model.weight.data): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(v != <span class="hljs-number"><span class="hljs-number">0</span></span>): print(vocab[i])</code> </pre> <br>  Auf so einfache Weise haben wir Bobs streng geheimes Passwort (und m√∂glicherweise seine kulinarischen Vorlieben) herausgefunden.  Und was tun?  Wie kann man dem f√∂derierten Lernen vertrauen, wenn es so einfach ist herauszufinden, welche Trainingsdaten die Gewichts√§nderung verursacht haben? <br><br><pre> <code class="javascript hljs">is pizza computer password my</code> </pre> <br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  30% Rabatt f√ºr Habrozhiteli Vorbestellungsb√ºcher auf einen Gutschein - <b>Grokking Deep Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458800/">https://habr.com/ru/post/de458800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458790/index.html">Einf√ºhrung in CatBoost. Yandex-Bericht</a></li>
<li><a href="../de458792/index.html">"Verbrannte" Mitarbeiter: Gibt es einen Ausweg?</a></li>
<li><a href="../de458794/index.html">Business Analysts Meeting bei Redmadrobot am 18. Juli</a></li>
<li><a href="../de458796/index.html">So bereiten Sie Ihre Site auf hohe Arbeitslasten vor: 5 praktische Tipps und n√ºtzliche Tools</a></li>
<li><a href="../de458798/index.html">N√§hrstoff-Bot oder wie ich Brot von Fitnesstrainern nehmen m√∂chte</a></li>
<li><a href="../de458804/index.html">Zusammenfassung der Artikel √ºber maschinelles Lernen und k√ºnstliche Intelligenz</a></li>
<li><a href="../de458808/index.html">Habr Postmortem-Bericht: Zeitung fiel</a></li>
<li><a href="../de458810/index.html">Corel und Parallels wurden aus den USA an die KKR Investment Group verkauft</a></li>
<li><a href="../de458812/index.html">JVM TI: Wie erstelle ich ein Plugin f√ºr eine virtuelle Maschine?</a></li>
<li><a href="../de458814/index.html">Starten einer Website f√ºr ein Produkt mit ungekl√§rter Nachfrage</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>