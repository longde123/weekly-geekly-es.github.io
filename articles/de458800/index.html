<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⏺️ 🔩 📋 Tiefes Lernen. Föderiertes Lernen 🌄 🔸 👩‍❤️‍💋‍👩</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habrozhiteli! Wir haben das Buch kürzlich Andrew W. Trask übergeben und damit den Grundstein für die weitere Beherrschung der Deep-Learning-Tech...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tiefes Lernen. Föderiertes Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/458800/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/xs/t-/oc/xst-oc7auy1he8nhwbj7bbkhxwk.jpeg" align="left" alt="Bild"></a>  Hallo habrozhiteli!  Wir haben das Buch kürzlich Andrew W. Trask übergeben und damit den Grundstein für die weitere Beherrschung der Deep-Learning-Technologie gelegt.  Es beginnt mit einer Beschreibung der Grundlagen neuronaler Netze und untersucht anschließend zusätzliche Schichten und Architekturen im Detail. <br><br>  Wir bieten einen Rückblick auf die Passage "Federated Learning" <br><br>  Die Idee des föderierten Lernens entstand aus der Tatsache, dass viele Daten, die nützliche Informationen zur Lösung von Problemen enthalten (z. B. zur Diagnose onkologischer Erkrankungen mittels MRT), nur schwer in ausreichenden Mengen zu erhalten sind, um ein leistungsfähiges Deep-Learning-Modell zu vermitteln.  Zusätzlich zu den nützlichen Informationen, die zum Trainieren des Modells erforderlich sind, enthalten die Datensätze auch andere Informationen, die für die jeweilige Aufgabe nicht relevant sind. Die Weitergabe an Dritte kann jedoch möglicherweise schädlich sein. <br><br>  Federated Learning ist eine Technik, mit der ein Modell in eine sichere Umgebung eingeschlossen und gelehrt werden kann, ohne dass Daten irgendwohin verschoben werden müssen.  Betrachten Sie ein Beispiel. <br><a name="habracut"></a><br><pre><code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Counter <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecsnp.random.seed(<span class="hljs-number"><span class="hljs-number">12345</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> codecs.open(<span class="hljs-string"><span class="hljs-string">'spam.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">"r"</span></span>,encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>,errors=<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: ←     http:<span class="hljs-comment"><span class="hljs-comment">//www2.aueb.gr/users/ion/data/enron-spam/ raw = f.readlines() vocab, spam, ham = (set(["&lt;unk&gt;"]), list(), list()) for row in raw: spam.append(set(row[:-2].split(" "))) for word in spam[-1]: vocab.add(word) with codecs.open('ham.txt',"r",encoding='utf-8',errors='ignore') as f: raw = f.readlines() for row in raw: ham.append(set(row[:-2].split(" "))) for word in ham[-1]: vocab.add(word) vocab, w2i = (list(vocab), {}) for i,w in enumerate(vocab): w2i[w] = i def to_indices(input, l=500): indices = list() for line in input: if(len(line) &lt; l): line = list(line) + ["&lt;unk&gt;"] * (l - len(line)) idxs = list() for word in line: idxs.append(w2i[word]) indices.append(idxs) return indices</span></span></code> </pre> <br><h3>  Lernen, Spam zu erkennen. </h3><br>  <b>Angenommen, wir müssen ein Modell trainieren, um Spam aus den E-Mails von Personen zu erkennen</b> <br><br>  In diesem Fall handelt es sich um eine E-Mail-Klassifizierung.  Wir werden unser erstes Modell auf einem öffentlichen Datensatz namens Enron trainieren.  Dies ist eine große Anzahl von E-Mails, die während der Enron-Anhörungen veröffentlicht wurden (jetzt ein Standard-E-Mail-Analyse-Text).  Eine interessante Tatsache: Ich war mit Leuten vertraut, die aufgrund ihrer Aktivitäten diesen Datensatz lesen / kommentieren mussten, und sie stellten fest, dass sich die Leute in diesen Briefen eine Vielzahl von Informationen schickten (oft sehr persönlich).  Da dieses Korps jedoch während des Prozesses veröffentlicht wurde, kann es jetzt ohne Einschränkung verwendet werden. <br><br>  Der Code im vorherigen und in diesem Abschnitt implementiert nur vorbereitende Vorgänge.  Eingabedateien (ham.txt und spam.txt) sind auf der Webseite des Buches verfügbar: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.manning.com/books/grokking-deep-learning</a> und im GitHub-Repository: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github.com/iamtrask/Grokking-Deep-Learning</a> .  Wir müssen es vorverarbeiten, um es für die Übertragung in die Embedding-Klasse aus Kapitel 13 vorzubereiten, wo wir unser Deep-Learning-Framework erstellt haben.  Nach wie vor werden alle Wörter in diesem Korpus in Indexlisten konvertiert.  Außerdem bringen wir alle Buchstaben auf die gleiche Länge von 500 Wörtern, indem wir sie entweder zuschneiden oder Token hinzufügen.  Dank dessen erhalten wir einen rechteckigen Datensatz. <br><br><pre> <code class="javascript hljs">spam_idx = to_indices(spam) ham_idx = to_indices(ham) train_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] train_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">-1000</span></span>] test_spam_idx = spam_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] test_ham_idx = ham_idx[<span class="hljs-number"><span class="hljs-number">-1000</span></span>:] train_data = list() train_target = list() test_data = list() test_target = list() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(train_spam_idx),len(train_ham_idx))): train_data.append(train_spam_idx[i%len(train_spam_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) train_data.append(train_ham_idx[i%len(train_ham_idx)]) train_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max(len(test_spam_idx),len(test_ham_idx))): test_data.append(test_spam_idx[i%len(test_spam_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">1</span></span>]) test_data.append(test_ham_idx[i%len(test_ham_idx)]) test_target.append([<span class="hljs-number"><span class="hljs-number">0</span></span>]) def train(model, input_data, target_data, batch_size=<span class="hljs-number"><span class="hljs-number">500</span></span>, iterations=<span class="hljs-number"><span class="hljs-number">5</span></span>): n_batches = int(len(input_data) / batch_size) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> iter <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(iterations): iter_loss = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b_i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_batches): #         model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(input_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) target = Tensor(target_data[b_i*bs:(b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>)*bs], autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() loss = criterion.forward(pred,target) loss.backward() optim.step() iter_loss += loss.data[<span class="hljs-number"><span class="hljs-number">0</span></span>] / bs sys.stdout.write(<span class="hljs-string"><span class="hljs-string">"\r\tLoss:"</span></span> + str(iter_loss / (b_i+<span class="hljs-number"><span class="hljs-number">1</span></span>))) print() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model def test(model, test_input, test_output): model.weight.data[w2i[<span class="hljs-string"><span class="hljs-string">'&lt;unk&gt;'</span></span>]] *= <span class="hljs-number"><span class="hljs-number">0</span></span> input = Tensor(test_input, autograd=True) target = Tensor(test_output, autograd=True) pred = model.forward(input).sum(<span class="hljs-number"><span class="hljs-number">1</span></span>).sigmoid() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ((pred.data &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>) == target.data).mean()</code> </pre> <br>  Nachdem wir die Hilfsfunktionen train () und test () definiert haben, können wir das neuronale Netzwerk initialisieren und trainieren, indem wir nur wenige Codezeilen schreiben.  Nach drei Iterationen kann das Netzwerk den Kontrolldatensatz mit einer Genauigkeit von 99,45% klassifizieren (der Kontrolldatensatz ist gut ausbalanciert, daher kann dieses Ergebnis als ausgezeichnet angesehen werden): <br><br><pre> <code class="javascript hljs">model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> criterion = MSELoss() optim = SGD(parameters=model.get_parameters(), alpha=<span class="hljs-number"><span class="hljs-number">0.01</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): model = train(model, train_data, train_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) ______________________________________________________________________________ Loss:<span class="hljs-number"><span class="hljs-number">0.037140416860871446</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">98.65</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.011258669226059114</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.15</span></span> Loss:<span class="hljs-number"><span class="hljs-number">0.008068268387986223</span></span> % Correct on Test <span class="hljs-built_in"><span class="hljs-built_in">Set</span></span>: <span class="hljs-number"><span class="hljs-number">99.45</span></span></code> </pre> <br><h3>  Machen wir das Modell föderal </h3><br>  <b>Oben wurde das häufigste tiefe Lernen durchgeführt.</b>  <b>Fügen Sie jetzt Privatsphäre hinzu</b> <br><br>  Im vorherigen Abschnitt haben wir ein Beispiel für eine E-Mail-Analyse implementiert.  Legen Sie nun alle E-Mails an einem Ort ab.  Dies ist eine gute alte Arbeitsmethode (die weltweit immer noch weit verbreitet ist).  Zunächst werden wir das Umfeld der Bundesbildung imitieren, in dem es verschiedene Briefsammlungen gibt: <br><br><pre> <code class="javascript hljs">bob = (train_data[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1000</span></span>]) alice = (train_data[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>], train_target[<span class="hljs-number"><span class="hljs-number">1000</span></span>:<span class="hljs-number"><span class="hljs-number">2000</span></span>]) sue = (train_data[<span class="hljs-number"><span class="hljs-number">2000</span></span>:], train_target[<span class="hljs-number"><span class="hljs-number">2000</span></span>:])</code> </pre> <br>  Noch nichts kompliziertes.  Jetzt können wir das gleiche Trainingsverfahren wie zuvor durchführen, jedoch bereits für drei separate Datensätze.  Nach jeder Iteration werden die Werte in den Modellen von Bob, Alice und Sue gemittelt und die Ergebnisse ausgewertet.  Bitte beachten Sie, dass einige Verbundlernmethoden das Kombinieren nach jedem Paket (oder jeder Paketsammlung) umfassen.  Ich habe beschlossen, den Code so einfach wie möglich zu halten: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">3</span></span>): print(<span class="hljs-string"><span class="hljs-string">"Starting Training Round..."</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\tStep 1: send the model to Bob"</span></span>) bob_model = train(copy.deepcopy(model), bob[<span class="hljs-number"><span class="hljs-number">0</span></span>], bob[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 2: send the model to Alice"</span></span>) alice_model = train(copy.deepcopy(model), alice[<span class="hljs-number"><span class="hljs-number">0</span></span>], alice[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tStep 3: Send the model to Sue"</span></span>) sue_model = train(copy.deepcopy(model), sue[<span class="hljs-number"><span class="hljs-number">0</span></span>], sue[<span class="hljs-number"><span class="hljs-number">1</span></span>], iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) print(<span class="hljs-string"><span class="hljs-string">"\n\tAverage Everyone's New Models"</span></span>) model.weight.data = (bob_model.weight.data + \ alice_model.weight.data + \ sue_model.weight.data)/<span class="hljs-number"><span class="hljs-number">3</span></span> print(<span class="hljs-string"><span class="hljs-string">"\t% Correct on Test Set: "</span></span> + \ str(test(model, test_data, test_target)*<span class="hljs-number"><span class="hljs-number">100</span></span>)) print(<span class="hljs-string"><span class="hljs-string">"\nRepeat!!\n"</span></span>)</code> </pre> <br><br>  Unten ist ein Ausschnitt mit den Ergebnissen.  Dieses Modell erreichte fast die gleiche Genauigkeit wie das vorherige, und theoretisch hatten wir keinen Zugriff auf Trainingsdaten - oder nicht?  Wie auch immer, aber jede Person ändert das Modell im Lernprozess, oder?  Können wir nicht wirklich etwas aus ihren Datensätzen herausholen? <br><br><pre> <code class="javascript hljs">Starting Training Round... Step <span class="hljs-number"><span class="hljs-number">1</span></span>: send the model to Bob Loss:<span class="hljs-number"><span class="hljs-number">0.21908166249699718</span></span> ...... Step <span class="hljs-number"><span class="hljs-number">3</span></span>: Send the model to Sue Loss:<span class="hljs-number"><span class="hljs-number">0.015368461608470256</span></span> Average Everyone<span class="hljs-string"><span class="hljs-string">'s New Models % Correct on Test Set: 98.8</span></span></code> </pre> <br><h3>  Hacke ein Verbundmodell </h3><br>  <b>Schauen wir uns ein einfaches Beispiel an, wie Informationen aus einem Trainingsdatensatz extrahiert werden.</b> <br><br>  Das föderierte Lernen leidet unter zwei großen Problemen, die besonders schwer zu lösen sind, wenn jede Person nur eine Handvoll Trainingsbeispiele hat - Geschwindigkeit und Vertraulichkeit.  Es stellt sich heraus, dass Sie immer noch viel über die Quelldaten lernen können, wenn jemand nur wenige Trainingsbeispiele hat (oder das an Sie gesendete Modell mit nur wenigen Beispielen trainiert wurde: ein Trainingspaket).  Wenn Sie sich vorstellen, dass Sie 10.000 Personen haben (und jeder eine sehr kleine Datenmenge hat), verbringen Sie die meiste Zeit damit, das Modell hin und her zu senden und nicht so viel zu trainieren (insbesondere wenn das Modell sehr groß ist). <br><br>  Aber lasst uns nicht weiterkommen.  Mal sehen, was Sie herausfinden können, nachdem der Benutzer die Gewichte für ein Paket aktualisiert hat: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> copy bobs_email = [<span class="hljs-string"><span class="hljs-string">"my"</span></span>, <span class="hljs-string"><span class="hljs-string">"computer"</span></span>, <span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"is"</span></span>, <span class="hljs-string"><span class="hljs-string">"pizza"</span></span>] bob_input = np.array([[w2i[x] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bobs_email]]) bob_target = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) model = Embedding(vocab_size=len(vocab), dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model.weight.data *= <span class="hljs-number"><span class="hljs-number">0</span></span> bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  Bob erstellt und trainiert das Modell per E-Mail in seinem Posteingang.  Aber so kam es, dass er sein Passwort speicherte, indem er sich einen Brief mit dem Text schickte: "Mein Computer-Passwort ist Pizza."  Naiver Bob!  Nachdem wir gesehen haben, welche Gewichte sich geändert haben, können wir das Wörterbuch (und die Bedeutung) von Bobs Brief herausfinden: <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(bobs_model.weight.data - model.weight.data): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(v != <span class="hljs-number"><span class="hljs-number">0</span></span>): print(vocab[i])</code> </pre> <br>  Auf so einfache Weise haben wir Bobs streng geheimes Passwort (und möglicherweise seine kulinarischen Vorlieben) herausgefunden.  Und was tun?  Wie kann man dem föderierten Lernen vertrauen, wenn es so einfach ist herauszufinden, welche Trainingsdaten die Gewichtsänderung verursacht haben? <br><br><pre> <code class="javascript hljs">is pizza computer password my</code> </pre> <br>  »Weitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  » <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  30% Rabatt für Habrozhiteli Vorbestellungsbücher auf einen Gutschein - <b>Grokking Deep Learning</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458800/">https://habr.com/ru/post/de458800/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458790/index.html">Einführung in CatBoost. Yandex-Bericht</a></li>
<li><a href="../de458792/index.html">"Verbrannte" Mitarbeiter: Gibt es einen Ausweg?</a></li>
<li><a href="../de458794/index.html">Business Analysts Meeting bei Redmadrobot am 18. Juli</a></li>
<li><a href="../de458796/index.html">So bereiten Sie Ihre Site auf hohe Arbeitslasten vor: 5 praktische Tipps und nützliche Tools</a></li>
<li><a href="../de458798/index.html">Nährstoff-Bot oder wie ich Brot von Fitnesstrainern nehmen möchte</a></li>
<li><a href="../de458804/index.html">Zusammenfassung der Artikel über maschinelles Lernen und künstliche Intelligenz</a></li>
<li><a href="../de458808/index.html">Habr Postmortem-Bericht: Zeitung fiel</a></li>
<li><a href="../de458810/index.html">Corel und Parallels wurden aus den USA an die KKR Investment Group verkauft</a></li>
<li><a href="../de458812/index.html">JVM TI: Wie erstelle ich ein Plugin für eine virtuelle Maschine?</a></li>
<li><a href="../de458814/index.html">Starten einer Website für ein Produkt mit ungeklärter Nachfrage</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>