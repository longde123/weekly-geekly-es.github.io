<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüîß ‚òØÔ∏è ‚óÄÔ∏è Wissenstransfer und neuronale maschinelle √úbersetzung in der Praxis üëèüèΩ ü§∏üèº üÜì</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die neuronale maschinelle √úbersetzung (NMT) entwickelt sich sehr schnell. Um Ihren √úbersetzer zusammenzustellen, m√ºssen Sie heute nicht mehr zwei Hoch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wissenstransfer und neuronale maschinelle √úbersetzung in der Praxis</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475750/"> Die neuronale maschinelle √úbersetzung (NMT) entwickelt sich sehr schnell.  Um Ihren √úbersetzer zusammenzustellen, m√ºssen Sie heute nicht mehr zwei Hochschulen haben.  Zum Trainieren des Modells ben√∂tigen Sie jedoch einen gro√üen Parallelkorpus (einen Korpus, bei dem die √úbersetzung in der Ausgangssprache mit dem Satz verkn√ºpft ist).  In der Praxis sprechen wir von mindestens einer Million Satzpaaren.  Es gibt sogar einen eigenen gro√üen Bereich des IWF, in dem Methoden zum Unterrichten von Sprachpaaren mit einer kleinen Datenmenge in elektronischer Form (English Low Resource NMT) untersucht werden. <br><br>  Wir sammeln das tschuwaschisch-russische Korps und pr√ºfen gleichzeitig, was mit dem verf√ºgbaren Datenvolumen getan werden kann.  In diesem Beispiel wurde ein Fall von 90.000 Satzpaaren verwendet.  Das derzeit beste Ergebnis wurde durch die Methode des Wissenstransfers (engl. Transfer Learning) erzielt und wird im Artikel diskutiert.  Der Artikel soll ein praktisches Beispiel f√ºr die Implementierung geben, das leicht reproduziert werden kann. <a name="habracut"></a><br><br>  Der Trainingsplan ist wie folgt.  Wir m√ºssen ein gro√ües (Eltern-) Geb√§ude nehmen, ein neuronales Modell darauf trainieren und dann unser Tochtermodell trainieren.  Dar√ºber hinaus ist die Zielsprache der √úbersetzung dieselbe: Russisch.  Intuitiv kann dies mit dem Erlernen einer zweiten Sprache verglichen werden.  Es ist einfacher zu lernen, eine Fremdsprache zu beherrschen.  Es sieht auch so aus, als w√ºrde man einen engen Bereich einer Fremdsprache studieren, zum Beispiel die medizinische Terminologie der englischen Sprache: Zuerst muss man allgemein Englisch lernen. <br><br>  Als Elternkorps versuchten sie, 1 Million Satzpaare <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">aus dem englisch-russischen Parallelkorps</a> und 1 Million <a href="" rel="nofollow">aus dem kasachisch-russischen Korps zu nehmen</a> .  Die kasachischen Daten enthalten 5 Millionen S√§tze.  Von diesen wurden nur diejenigen mit einem Erf√ºllungskoeffizienten (dritte Spalte) von mehr als 2. Die kasachische Version ergab etwas bessere Ergebnisse.  Es scheint intuitiv, dass dies verst√§ndlich ist, da die tschuwaschische und die kasachische Sprache einander √§hnlicher sind.  Tats√§chlich ist dies jedoch nicht bewiesen und h√§ngt auch stark von der Qualit√§t des Geh√§uses ab.  Weitere Details zur Auswahl des Elternk√∂rpers finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">in diesem Artikel</a> .  √úber das Tochterunternehmen mit 90.000 Angebotspaaren k√∂nnen Sie hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Musterdaten abrufen und anfordern.</a> <br><br>  Nun zum Code.  Wenn Sie keine eigene schnelle Grafikkarte haben, k√∂nnen Sie das Modell auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Colab-</a> Website trainieren.  F√ºr das Training haben wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Sockeye-</a> Bibliothek benutzt.  Es wird davon ausgegangen, dass Python3 bereits installiert ist. <br><br><pre><code class="bash hljs">pip install sockeye</code> </pre> <br>  M√∂glicherweise m√ºssen Sie auch separat an <a href="" rel="nofollow">MXNet</a> basteln, das f√ºr die Arbeit mit der Grafikkarte zust√§ndig ist.  Colab ben√∂tigt eine zus√§tzliche Bibliotheksinstallation <br><br><pre> <code class="bash hljs">pip install mxnet-cu100mkl</code> </pre> <br>  Bei neuronalen Netzen wird allgemein angenommen, dass es ausreicht, dass sie die Daten so wie sie sind einspeisen, und sie werden es herausfinden.  In Wirklichkeit ist dies jedoch nicht immer der Fall.  In unserem Fall muss der K√∂rper also vorverarbeitet werden.  Zuerst kennzeichnen wir es so, dass die Models leichter verstehen, dass ‚ÄûKatze!‚Äú Und ‚ÄûKatze‚Äú ungef√§hr dasselbe sind.  Zum Beispiel ist nur ein Python-Tokenizer ausreichend. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordPunctTokenizer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(src_filename, new_filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(src_filename, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src_file: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(new_filename, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> new_file: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> src_file: new_file.write(<span class="hljs-string"><span class="hljs-string">"%s"</span></span> % <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(WordPunctTokenizer().tokenize(line))) new_file.write(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)</code> </pre> <br>  Als Ergebnis f√ºttern wir Paare von S√§tzen der Form <br><br><pre> <code class="xml hljs">  ”ó  “´ ”≥ ”ë”ë. ”ë ”ë ”ë ”ë”ó, ”ë ”ë”ë”ó,   ”ë  ”ó”ó -”ë ”ó”ó“´,  “´”ó ”ó ”ó“´ ”ë”ë ”ë”ë, “´ ”ó ”ó   ”ë ”ë ”ë”ë ”ë .</code> </pre> <br>  und <br><br><pre> <code class="xml hljs">     .  , ,       , ,    ,        .</code> </pre> <br>  Die Ausgabe umfasst die folgenden tokenisierten Angebote: <br><br><pre> <code class="xml hljs">  ”ó  “´ ”≥ ”ë”ë . ”ë ”ë ”ë ”ë”ó , ”ë ”ë”ë”ó ,   ”ë  ”ó”ó  - ”ë ”ó”ó“´ ,  “´”ó ”ó ”ó“´ ”ë”ë ”ë”ë , “´ ”ó ”ó   ”ë ”ë ”ë”ë ”ë  .</code> </pre> <br>  und auf russisch <br><br><pre> <code class="xml hljs">      .   ,  ,        ,  ,     ,         .</code> </pre> <br>  In unserem Fall ben√∂tigen wir die kombinierten W√∂rterb√ºcher der √ºbergeordneten und untergeordneten F√§lle, sodass wir gemeinsame Dateien erstellen: <br><br><pre> <code class="bash hljs">cp kk.parent.train.tok kkchv.all.train.tok cat chv.child.train.tok &gt;&gt; kk.parent.train.tok cp ru.parent.train.tok ru.all.train.tok cat ru.child.train.tok &gt;&gt; ru.all.train.tok</code> </pre> <br>  da die weiterbildung des kindmodells im selben w√∂rterbuch erfolgt. <br><br>  Nun ein kleiner aber wichtiger Exkurs.  In MP werden S√§tze in Form von W√∂rtern in Atome unterteilt und dann als Wortfolgen mit S√§tzen bearbeitet.  Dies reicht jedoch in der Regel nicht aus, da sich aus den W√∂rtern, die einmal im Korpus vorkommen, ein riesiger Schwanz bildet.  Es ist schwierig, ein probabilistisches Modell f√ºr sie zu erstellen.  Dies gilt insbesondere f√ºr Sprachen mit entwickelter Morphologie (Fall, Geschlecht, Anzahl).  Sowohl Russisch als auch Tschuwaschisch sind solche Sprachen.  Aber es gibt eine L√∂sung.  Sie k√∂nnen den Satz in eine niedrigere Ebene unterteilen, in Unterw√∂rter.  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Bytepaarkodierung</a> verwendet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">.</a> <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rsennrich/subword-nmt.git</code> </pre> <br>  Wir erhalten ungef√§hr solche Folgen von Unterw√∂rtern <br><br><pre> <code class="xml hljs">@@   ”ó  “´ ”≥@@  ”ë”ë . @@ ”ë ”ë ”ë @@ ”ë”ó , ”ë ”ë”ë@@ ”ó ,   ”ë@@  @@  ”ó”ó  - ”ë@@  ”ó@@ ”ó“´ ,  “´”ó@@  ”ó ”ó“´@@ @@  ”ë”ë ”ë”ë , “´@@ @@ @@  ”ó ”ó @@ @@  @@  ”ë ”ë ”ë”ë ”ë  .</code> </pre> <br>  und <br><br><pre> <code class="xml hljs">@@    @@  @@   . @@  @@ @@ @@ @@  , @@  , @@ @@  @@    @@  @@ @@  @@ @@ @@ @@  ,  ,  @@  @@ @@ @@  @@ @@ @@  ,       @@ @@  @@ @@ @@  .</code> </pre> <br>  Es ist zu sehen, dass Affixe sich gut von W√∂rtern unterscheiden: Nicht @@ f√ºr eine lange Zeit und gut @@ dass. <br>  Bereiten Sie dazu bpe-W√∂rterb√ºcher vor <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input kkchv.all.train.tok ru.all.train.tok -s 10000 -o bpe.codes --write-vocabulary bpe.vocab.kkchv bpe.vocab.ru</code> </pre> <br>  Und wenden Sie sie auf Token an, zum Beispiel: <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.kkchv --vocabulary-threshold 50 &lt; kkchv.all.train.tok &gt; kkchv.all.train.bpe !python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.ru --vocabulary-threshold 50 &lt; ru.all.train.tok &gt; ru.all.train.bpe</code> </pre> <br>  Analog dazu m√ºssen Sie f√ºr alle Dateien Folgendes tun: Schulung, Validierung und Test von √ºber- und untergeordneten Modellen. <br><br>  Nun wenden wir uns direkt dem Training des neuronalen Modells zu.  Zuerst m√ºssen Sie allgemeine Modellw√∂rterb√ºcher vorbereiten: <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.all.train.bpe -t ru.all.train.bpe -o kkru_all_data</code> </pre> <br>  Trainieren Sie als N√§chstes das √ºbergeordnete Modell.  Ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">einfaches Beispiel wird auf der Sockeye-Seite</a> genauer beschrieben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">.</a>  Technisch gesehen besteht der Prozess aus zwei Schritten: Vorbereiten von Daten mithilfe zuvor erstellter Modellw√∂rterb√ºcher <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.parent.train.bpe -t ru.parent.train.bpe -o kkru_parent_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  und das Lernen selbst <br><br><pre> <code class="bash hljs">python -m sockeye.train -d kkru_parent_data -vs kk.parent.dev.bpe -vt ru.parent.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o kkru_parent_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10</code> </pre> <br>  Das Training in den Colab-Einrichtungen dauert ungef√§hr einen Tag.  Wenn das Training des Modells abgeschlossen ist, k√∂nnen Sie es damit √ºbersetzen <br><br><pre> <code class="bash hljs">python -m sockeye.translate --input kk.parent.test.bpe -m kkru_parent_model --output ru.parent.test_kkru_parent.bpe</code> </pre> <br>  Das Kindermodell trainieren <br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s chv.child.train.bpe -t ru.child.train.bpe -o chvru_child_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  Der Startcode f√ºr das Training sieht folgenderma√üen aus <br><br><pre> <code class="bash hljs">python -m sockeye.train -d chvru_child_data -vs chv.child.dev.bpe -vt ru.child.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o ruchv_150K_skv_dev19_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10 --config kkru_parent_model/args.yaml --params kkru_parent_model/params.best</code> </pre> <br>  Es werden Parameter hinzugef√ºgt, die angeben, dass die Konfiguration und Gewichte des √ºbergeordneten Modells als Ausgangspunkt verwendet werden sollen.  Details <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">im Beispiel mit Umschulung von Sockeye</a> .  Das Lernen eines Kindermodells l√§uft in ungef√§hr 12 Stunden zusammen. <br><br>  Vergleichen Sie die Ergebnisse, um sie zusammenzufassen.  Das √ºbliche maschinelle √úbersetzungsmodell ergab eine BLEU-Qualit√§t von 24,96, w√§hrend das Wissenstransfermodell 32,38 BLEU betrug.  Der Unterschied ist auch optisch an √úbersetzungsbeispielen erkennbar.  Daher werden wir dieses Modell verwenden, w√§hrend wir das Geh√§use weiter zusammenbauen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475750/">https://habr.com/ru/post/de475750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475740/index.html">Das Buch "Olympiad Programming" wurde ver√∂ffentlicht</a></li>
<li><a href="../de475742/index.html">Fall von RetouchMe: Was wir durch die Lokalisierung der Anwendung in 35 Sprachen erhalten haben</a></li>
<li><a href="../de475744/index.html">Vierstufiges Systemadministratormodell</a></li>
<li><a href="../de475746/index.html">Anatomie akustischer Systeme: Cermets und Komposite - √úber Monitor-Audiodiffusoren</a></li>
<li><a href="../de475748/index.html">22. November Moskau - AnalyzeIT MeetUp Nr. 3</a></li>
<li><a href="../de475754/index.html">Shorts √ºber Scrum</a></li>
<li><a href="../de475756/index.html">Food Design Digest Oktober 2019</a></li>
<li><a href="../de475758/index.html">Automatisch erneuerbare Abonnementstufen in der iOS-App</a></li>
<li><a href="../de475760/index.html">Nachwuchsentwickler - warum wir sie einstellen und wie wir mit ihnen arbeiten</a></li>
<li><a href="../de475762/index.html">Ein Lebenslauf mit einem Foto fliegt in eine Urne. Merkmale der Jobsuche in den USA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>