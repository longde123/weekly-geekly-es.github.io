<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍🔧 ☯️ ◀️ Wissenstransfer und neuronale maschinelle Übersetzung in der Praxis 👏🏽 🤸🏼 🆓</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die neuronale maschinelle Übersetzung (NMT) entwickelt sich sehr schnell. Um Ihren Übersetzer zusammenzustellen, müssen Sie heute nicht mehr zwei Hoch...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wissenstransfer und neuronale maschinelle Übersetzung in der Praxis</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475750/"> Die neuronale maschinelle Übersetzung (NMT) entwickelt sich sehr schnell.  Um Ihren Übersetzer zusammenzustellen, müssen Sie heute nicht mehr zwei Hochschulen haben.  Zum Trainieren des Modells benötigen Sie jedoch einen großen Parallelkorpus (einen Korpus, bei dem die Übersetzung in der Ausgangssprache mit dem Satz verknüpft ist).  In der Praxis sprechen wir von mindestens einer Million Satzpaaren.  Es gibt sogar einen eigenen großen Bereich des IWF, in dem Methoden zum Unterrichten von Sprachpaaren mit einer kleinen Datenmenge in elektronischer Form (English Low Resource NMT) untersucht werden. <br><br>  Wir sammeln das tschuwaschisch-russische Korps und prüfen gleichzeitig, was mit dem verfügbaren Datenvolumen getan werden kann.  In diesem Beispiel wurde ein Fall von 90.000 Satzpaaren verwendet.  Das derzeit beste Ergebnis wurde durch die Methode des Wissenstransfers (engl. Transfer Learning) erzielt und wird im Artikel diskutiert.  Der Artikel soll ein praktisches Beispiel für die Implementierung geben, das leicht reproduziert werden kann. <a name="habracut"></a><br><br>  Der Trainingsplan ist wie folgt.  Wir müssen ein großes (Eltern-) Gebäude nehmen, ein neuronales Modell darauf trainieren und dann unser Tochtermodell trainieren.  Darüber hinaus ist die Zielsprache der Übersetzung dieselbe: Russisch.  Intuitiv kann dies mit dem Erlernen einer zweiten Sprache verglichen werden.  Es ist einfacher zu lernen, eine Fremdsprache zu beherrschen.  Es sieht auch so aus, als würde man einen engen Bereich einer Fremdsprache studieren, zum Beispiel die medizinische Terminologie der englischen Sprache: Zuerst muss man allgemein Englisch lernen. <br><br>  Als Elternkorps versuchten sie, 1 Million Satzpaare <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">aus dem englisch-russischen Parallelkorps</a> und 1 Million <a href="" rel="nofollow">aus dem kasachisch-russischen Korps zu nehmen</a> .  Die kasachischen Daten enthalten 5 Millionen Sätze.  Von diesen wurden nur diejenigen mit einem Erfüllungskoeffizienten (dritte Spalte) von mehr als 2. Die kasachische Version ergab etwas bessere Ergebnisse.  Es scheint intuitiv, dass dies verständlich ist, da die tschuwaschische und die kasachische Sprache einander ähnlicher sind.  Tatsächlich ist dies jedoch nicht bewiesen und hängt auch stark von der Qualität des Gehäuses ab.  Weitere Details zur Auswahl des Elternkörpers finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">in diesem Artikel</a> .  Über das Tochterunternehmen mit 90.000 Angebotspaaren können Sie hier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Musterdaten abrufen und anfordern.</a> <br><br>  Nun zum Code.  Wenn Sie keine eigene schnelle Grafikkarte haben, können Sie das Modell auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Colab-</a> Website trainieren.  Für das Training haben wir die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Sockeye-</a> Bibliothek benutzt.  Es wird davon ausgegangen, dass Python3 bereits installiert ist. <br><br><pre><code class="bash hljs">pip install sockeye</code> </pre> <br>  Möglicherweise müssen Sie auch separat an <a href="" rel="nofollow">MXNet</a> basteln, das für die Arbeit mit der Grafikkarte zuständig ist.  Colab benötigt eine zusätzliche Bibliotheksinstallation <br><br><pre> <code class="bash hljs">pip install mxnet-cu100mkl</code> </pre> <br>  Bei neuronalen Netzen wird allgemein angenommen, dass es ausreicht, dass sie die Daten so wie sie sind einspeisen, und sie werden es herausfinden.  In Wirklichkeit ist dies jedoch nicht immer der Fall.  In unserem Fall muss der Körper also vorverarbeitet werden.  Zuerst kennzeichnen wir es so, dass die Models leichter verstehen, dass „Katze!“ Und „Katze“ ungefähr dasselbe sind.  Zum Beispiel ist nur ein Python-Tokenizer ausreichend. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> WordPunctTokenizer <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(src_filename, new_filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(src_filename, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> src_file: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(new_filename, <span class="hljs-string"><span class="hljs-string">"w"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">"utf-8"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> new_file: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> src_file: new_file.write(<span class="hljs-string"><span class="hljs-string">"%s"</span></span> % <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(WordPunctTokenizer().tokenize(line))) new_file.write(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)</code> </pre> <br>  Als Ergebnis füttern wir Paare von Sätzen der Form <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ. ӑ ӑ ӑ ӑӗ, ӑ ӑӑӗ,   ӑ  ӗӗ -ӑ ӗӗҫ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ, ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ .</code> </pre> <br>  und <br><br><pre> <code class="xml hljs">     .  , ,       , ,    ,        .</code> </pre> <br>  Die Ausgabe umfasst die folgenden tokenisierten Angebote: <br><br><pre> <code class="xml hljs">  ӗ  ҫ ӳ ӑӑ . ӑ ӑ ӑ ӑӗ , ӑ ӑӑӗ ,   ӑ  ӗӗ  - ӑ ӗӗҫ ,  ҫӗ ӗ ӗҫ ӑӑ ӑӑ , ҫ ӗ ӗ   ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  und auf russisch <br><br><pre> <code class="xml hljs">      .   ,  ,        ,  ,     ,         .</code> </pre> <br>  In unserem Fall benötigen wir die kombinierten Wörterbücher der übergeordneten und untergeordneten Fälle, sodass wir gemeinsame Dateien erstellen: <br><br><pre> <code class="bash hljs">cp kk.parent.train.tok kkchv.all.train.tok cat chv.child.train.tok &gt;&gt; kk.parent.train.tok cp ru.parent.train.tok ru.all.train.tok cat ru.child.train.tok &gt;&gt; ru.all.train.tok</code> </pre> <br>  da die weiterbildung des kindmodells im selben wörterbuch erfolgt. <br><br>  Nun ein kleiner aber wichtiger Exkurs.  In MP werden Sätze in Form von Wörtern in Atome unterteilt und dann als Wortfolgen mit Sätzen bearbeitet.  Dies reicht jedoch in der Regel nicht aus, da sich aus den Wörtern, die einmal im Korpus vorkommen, ein riesiger Schwanz bildet.  Es ist schwierig, ein probabilistisches Modell für sie zu erstellen.  Dies gilt insbesondere für Sprachen mit entwickelter Morphologie (Fall, Geschlecht, Anzahl).  Sowohl Russisch als auch Tschuwaschisch sind solche Sprachen.  Aber es gibt eine Lösung.  Sie können den Satz in eine niedrigere Ebene unterteilen, in Unterwörter.  Wir haben die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">Bytepaarkodierung</a> verwendet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">.</a> <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rsennrich/subword-nmt.git</code> </pre> <br>  Wir erhalten ungefähr solche Folgen von Unterwörtern <br><br><pre> <code class="xml hljs">@@   ӗ  ҫ ӳ@@  ӑӑ . @@ ӑ ӑ ӑ @@ ӑӗ , ӑ ӑӑ@@ ӗ ,   ӑ@@  @@  ӗӗ  - ӑ@@  ӗ@@ ӗҫ ,  ҫӗ@@  ӗ ӗҫ@@ @@  ӑӑ ӑӑ , ҫ@@ @@ @@  ӗ ӗ @@ @@  @@  ӑ ӑ ӑӑ ӑ  .</code> </pre> <br>  und <br><br><pre> <code class="xml hljs">@@    @@  @@   . @@  @@ @@ @@ @@  , @@  , @@ @@  @@    @@  @@ @@  @@ @@ @@ @@  ,  ,  @@  @@ @@ @@  @@ @@ @@  ,       @@ @@  @@ @@ @@  .</code> </pre> <br>  Es ist zu sehen, dass Affixe sich gut von Wörtern unterscheiden: Nicht @@ für eine lange Zeit und gut @@ dass. <br>  Bereiten Sie dazu bpe-Wörterbücher vor <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input kkchv.all.train.tok ru.all.train.tok -s 10000 -o bpe.codes --write-vocabulary bpe.vocab.kkchv bpe.vocab.ru</code> </pre> <br>  Und wenden Sie sie auf Token an, zum Beispiel: <br><br><pre> <code class="bash hljs">python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.kkchv --vocabulary-threshold 50 &lt; kkchv.all.train.tok &gt; kkchv.all.train.bpe !python subword-nmt/subword_nmt/apply_bpe.py -c bpe.codes --vocabulary bpe.vocab.ru --vocabulary-threshold 50 &lt; ru.all.train.tok &gt; ru.all.train.bpe</code> </pre> <br>  Analog dazu müssen Sie für alle Dateien Folgendes tun: Schulung, Validierung und Test von über- und untergeordneten Modellen. <br><br>  Nun wenden wir uns direkt dem Training des neuronalen Modells zu.  Zuerst müssen Sie allgemeine Modellwörterbücher vorbereiten: <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.all.train.bpe -t ru.all.train.bpe -o kkru_all_data</code> </pre> <br>  Trainieren Sie als Nächstes das übergeordnete Modell.  Ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">einfaches Beispiel wird auf der Sockeye-Seite</a> genauer beschrieben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">.</a>  Technisch gesehen besteht der Prozess aus zwei Schritten: Vorbereiten von Daten mithilfe zuvor erstellter Modellwörterbücher <br><br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s kk.parent.train.bpe -t ru.parent.train.bpe -o kkru_parent_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  und das Lernen selbst <br><br><pre> <code class="bash hljs">python -m sockeye.train -d kkru_parent_data -vs kk.parent.dev.bpe -vt ru.parent.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o kkru_parent_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10</code> </pre> <br>  Das Training in den Colab-Einrichtungen dauert ungefähr einen Tag.  Wenn das Training des Modells abgeschlossen ist, können Sie es damit übersetzen <br><br><pre> <code class="bash hljs">python -m sockeye.translate --input kk.parent.test.bpe -m kkru_parent_model --output ru.parent.test_kkru_parent.bpe</code> </pre> <br>  Das Kindermodell trainieren <br><pre> <code class="bash hljs">python -m sockeye.prepare_data -s chv.child.train.bpe -t ru.child.train.bpe -o chvru_child_data --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span>-vocab kkru_all_data/vocab.src.0.json --target-vocab kkru_all_data/vocab.trg.0.json</code> </pre> <br>  Der Startcode für das Training sieht folgendermaßen aus <br><br><pre> <code class="bash hljs">python -m sockeye.train -d chvru_child_data -vs chv.child.dev.bpe -vt ru.child.dev.bpe --encoder transformer --decoder transformer --transformer-model-size 512 --transformer-feed-forward-num-hidden 256 --transformer-dropout-prepost 0.1 --num-embed 512 --max-seq-len 100 --decode-and-evaluate 500 -o ruchv_150K_skv_dev19_model --num-layers 6 --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>-device-locking --batch-size 1024 --optimized-metric bleu --max-num-checkpoint-not-improved 10 --config kkru_parent_model/args.yaml --params kkru_parent_model/params.best</code> </pre> <br>  Es werden Parameter hinzugefügt, die angeben, dass die Konfiguration und Gewichte des übergeordneten Modells als Ausgangspunkt verwendet werden sollen.  Details <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="nofollow">im Beispiel mit Umschulung von Sockeye</a> .  Das Lernen eines Kindermodells läuft in ungefähr 12 Stunden zusammen. <br><br>  Vergleichen Sie die Ergebnisse, um sie zusammenzufassen.  Das übliche maschinelle Übersetzungsmodell ergab eine BLEU-Qualität von 24,96, während das Wissenstransfermodell 32,38 BLEU betrug.  Der Unterschied ist auch optisch an Übersetzungsbeispielen erkennbar.  Daher werden wir dieses Modell verwenden, während wir das Gehäuse weiter zusammenbauen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475750/">https://habr.com/ru/post/de475750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475740/index.html">Das Buch "Olympiad Programming" wurde veröffentlicht</a></li>
<li><a href="../de475742/index.html">Fall von RetouchMe: Was wir durch die Lokalisierung der Anwendung in 35 Sprachen erhalten haben</a></li>
<li><a href="../de475744/index.html">Vierstufiges Systemadministratormodell</a></li>
<li><a href="../de475746/index.html">Anatomie akustischer Systeme: Cermets und Komposite - Über Monitor-Audiodiffusoren</a></li>
<li><a href="../de475748/index.html">22. November Moskau - AnalyzeIT MeetUp Nr. 3</a></li>
<li><a href="../de475754/index.html">Shorts über Scrum</a></li>
<li><a href="../de475756/index.html">Food Design Digest Oktober 2019</a></li>
<li><a href="../de475758/index.html">Automatisch erneuerbare Abonnementstufen in der iOS-App</a></li>
<li><a href="../de475760/index.html">Nachwuchsentwickler - warum wir sie einstellen und wie wir mit ihnen arbeiten</a></li>
<li><a href="../de475762/index.html">Ein Lebenslauf mit einem Foto fliegt in eine Urne. Merkmale der Jobsuche in den USA</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>