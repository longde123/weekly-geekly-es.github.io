<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏖️ 👖 👩‍🔧 Comprendre le Q-learning, le problème «Marcher sur un rocher» 🤦🏻 👪 👩🏻‍🤝‍👨🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! J'attire votre attention sur une traduction de l'article "Comprendre Q-Learning, le problème de la marche en falaise" de Lucas Vazquez ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comprendre le Q-learning, le problème «Marcher sur un rocher»</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443240/">  Bonjour, Habr!  J'attire votre attention sur une traduction de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">"Comprendre Q-Learning, le problème de la marche en falaise"</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lucas Vazquez</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/dl/ol/ezdlol2oj19smcejxtdlt4aon68.jpeg"></div><br><p> Dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dernier article,</a> nous avons présenté le problème «Marcher sur un rocher» et nous sommes installés sur un algorithme terrible qui n'avait pas de sens.  Cette fois, nous allons révéler les secrets de cette boîte grise et voir qu'elle n'est pas si effrayante du tout. </p><br><h3>  Résumé </h3><br><p>  Nous avons conclu qu'en maximisant le montant des récompenses futures, nous trouvons également le chemin le plus rapide vers l'objectif, donc notre objectif est maintenant de trouver un moyen de le faire! </p><br><a name="habracut"></a><br><h3>  Introduction à Q-Learning </h3><br><ul><li>  Commençons par construire un tableau qui mesure la performance d'une certaine action dans n'importe quel état (nous pouvons la mesurer avec une simple valeur scalaire, donc plus la valeur est grande, meilleure est l'action) </li><li>  Ce tableau aura une ligne pour chaque état et une colonne pour chaque action.  Dans notre monde, la grille a 48 états (4 par Y par 12 par X) et 4 actions sont autorisées (haut, bas, gauche, droite), donc la table sera de 48 x 4. </li><li>  Les valeurs stockées dans ce tableau sont appelées «valeurs Q». </li><li>  Ce sont des estimations du montant des récompenses futures.  En d'autres termes, ils estiment combien de récompenses supplémentaires nous pouvons obtenir avant la fin du jeu, en étant dans l'état S et en effectuant l'action A. </li><li>  Nous initialisons le tableau avec des valeurs aléatoires (ou certaines constantes, par exemple, tous les zéros). </li></ul><br><p>  La «table Q» optimale a des valeurs qui nous permettent de prendre les meilleures mesures dans chaque état, nous donnant la meilleure façon de gagner à la fin.  Le problème est résolu, cheers, Robot Lords :). </p><br><img src="https://habrastorage.org/webt/5a/ii/wl/5aiiwljmx4igtrsrhrc3qoymoge.png"><br>  <i>Valeurs Q des cinq premiers états.</i> <br><br><h4>  Q-learning </h4><br><ul><li>  Q-learning est un algorithme qui «apprend» ces valeurs. </li><li>  À chaque étape, nous obtenons plus d'informations sur le monde. </li><li>  Ces informations sont utilisées pour mettre à jour les valeurs du tableau. </li></ul><br><p>  Par exemple, supposons que nous soyons à un pas de la cible (carré [2, 11]), et si nous décidons de descendre, nous obtenons une récompense de 0 au lieu de -1. <br>  Nous pouvons utiliser ces informations pour mettre à jour la valeur de la paire état-action dans notre tableau, et la prochaine fois que nous le visiterons, nous saurons déjà que le fait de descendre nous donne une récompense de 0. </p><br><img src="https://habrastorage.org/webt/7a/iq/u3/7aiqu3ttrz1qnypctrqvlgyd93e.png"><br><p>  Nous pouvons maintenant diffuser ces informations encore plus loin!  Puisque nous connaissons maintenant le chemin vers le but depuis le carré [2, 11], toute action qui nous mène au carré [2, 11] sera également bonne, donc nous mettons à jour la valeur Q du carré, ce qui nous mène à [2, 11] être plus proche de 0. </p><br><p>  <b>Et cela, Mesdames et Messieurs, est l'essence même du Q-learning!</b> </p><br><p>  Veuillez noter que chaque fois que nous atteignons l'objectif, nous augmentons notre «carte» de la façon d'atteindre l'objectif d'un carré, donc après un nombre suffisant d'itérations, nous aurons une carte complète qui nous montrera comment atteindre l'objectif à partir de chaque état. </p><br><img src="https://habrastorage.org/webt/mj/q0/sh/mjq0shtkn3u37zlhdpptbh2oppe.gif"><br>  <i>Un chemin est généré en prenant la meilleure action dans chaque état.</i>  <i>La touche verte représente le sens d'une meilleure action, les touches plus saturées représentent une valeur plus élevée.</i>  <i>Le texte représente une valeur pour chaque action (haut, bas, droite, gauche).</i> <br><br><h3>  Équation de Bellman </h3><br><p>  Avant de parler de code, parlons des mathématiques: le concept de base du Q-learning, l'équation de Bellman. </p><br><img src="https://habrastorage.org/webt/i2/_u/gx/i2_ugxlinshqmsyzkawlbmirxri.png"><br><br><ul><li>  Oublions d'abord γ dans cette équation </li><li>  L'équation indique que la valeur Q pour une paire état-action particulière doit être la récompense reçue lors du passage à un nouvel état (en effectuant cette action), ajoutée à la valeur de la meilleure action dans l'état suivant. </li></ul><br><p>  <b>En d'autres termes, nous diffusons des informations sur les valeurs des actions une étape à la fois!</b> </p><br><p>  Mais nous pouvons décider que recevoir une récompense en ce moment est plus précieux que de recevoir une récompense à l'avenir, et nous avons donc γ, un nombre de 0 à 1 (généralement de 0,9 à 0,99) qui est multiplié par une récompense à l'avenir, réduction des récompenses futures. </p><br><p>  Donc, étant donné γ = 0,9 et en appliquant cela à certains états de notre monde (grille), nous avons: </p><br><img src="https://habrastorage.org/webt/e7/yp/gi/e7ypginhzc-cetmrcbqa6ar3byg.png"><br><br><p>  Nous pouvons comparer ces valeurs avec celles ci-dessus dans GIF et voir qu'elles sont identiques. </p><br><br><h3>  Implémentation </h3><br><p>  Maintenant que nous avons une compréhension intuitive du fonctionnement de Q-learning, nous pouvons commencer à réfléchir à la mise en œuvre de tout cela, et nous utiliserons le pseudo-code Q-learning <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">du livre</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Sutton</a> comme guide. </p><br><img src="https://habrastorage.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png"><br>  <i>Pseudo-code du livre de Sutton.</i> <br><br><p>  Code: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize Q arbitrarily, in this case a table full of zeros q_values = np.zeros((num_states, num_actions)) # Iterate over 500 episodes for _ in range(500): state = env.reset() done = False # While episode is not over while not done: # Choose action action = egreedy_policy(q_values, state, epsilon=0.1) # Do the action next_state, reward, done = env.step(action) # Update q_values td_target = reward + gamma * np.max(q_values[next_state]) td_error = td_target - q_values[state][action] q_values[state][action] += learning_rate * td_error # Update state state = next_state</span></span></code> </pre> <br><ul><li>  Tout d'abord, nous disons: "Pour tous les états et actions, nous initialisons Q (s, a) arbitrairement", cela signifie que nous pouvons créer notre table de valeurs Q avec toutes les valeurs que nous aimons, elles peuvent être aléatoires, elles peuvent être n'importe quel type de permanent n'a pas d'importance.  Nous voyons qu'à la ligne 2, nous créons une table pleine de zéros. </li></ul><br><p>  <b>Nous disons également: "La valeur de Q pour les états finaux est nulle", nous ne pouvons entreprendre aucune action dans les états finaux, donc nous considérons que la valeur de toutes les actions dans cet état est nulle.</b> </p><br><ul><li>  Pour chaque épisode, nous devons «initialiser S», c'est juste une façon élégante de dire «redémarrer le jeu», dans notre cas, cela signifie mettre le joueur dans la position de départ;  dans notre monde, il existe une méthode qui fait exactement cela, et nous l'appelons à la ligne 6. </li><li>  Pour chaque pas de temps (à chaque fois que nous devons agir), nous devons choisir l'action obtenue à partir de Q. </li></ul><br><p>  Rappelez-vous, j'ai dit: «prenons-nous les mesures qui ont le plus de valeur dans toutes les conditions? </p><br><p>  Lorsque nous faisons cela, nous utilisons nos valeurs Q pour créer la politique;  dans ce cas, ce sera une politique gourmande, parce que nous prenons toujours des mesures qui, à notre avis, sont les meilleures dans tous les États, donc on dit que nous agissons avec cupidité. </p><br><br><h3>  Indésirable </h3><br><p>  Mais il y a un problème avec cette approche: imaginez que nous sommes dans un labyrinthe qui a deux récompenses, dont l'une est +1, et l'autre est +100 (et chaque fois que nous en trouvons une, le jeu se termine).  Puisque nous prenons toujours des mesures que nous considérons comme les meilleures, nous serons coincés avec le premier prix trouvé, y revenant toujours, donc, si nous reconnaissons d'abord le prix +1, alors nous manquerons le grand prix +100. </p><br><br><h3>  Solution </h3><br><p>  Nous devons nous assurer que nous avons suffisamment étudié notre monde (c'est une tâche incroyablement difficile).  C'est là que ε entre en jeu.  ε dans l'algorithme gourmand signifie que nous devons agir avec empressement, MAIS faire des actions aléatoires en pourcentage de ε dans le temps, donc avec un nombre infini de tentatives, nous devons examiner tous les états. </p><br><p>  L'action est sélectionnée conformément à cette stratégie à la ligne 12, avec epsilon = 0,1, ce qui signifie que nous effectuons des recherches sur le monde 10% du temps.  La mise en œuvre de la politique est la suivante: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">egreedy_policy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(q_values, state, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get a random number from a uniform distribution between 0 and 1, # if the number is lower than epsilon choose a random action if np.random.random() &lt; epsilon: return np.random.choice(4) # Else choose the action with the highest value else: return np.argmax(q_values[state])</span></span></code> </pre><br><p>  À la ligne 14 de la première liste, nous appelons la méthode par étapes pour effectuer l'action, le monde nous renvoie le prochain état, la récompense et les informations sur la fin du jeu. </p><br><p>  Retour aux maths: </p><br><p>  Nous avons une longue équation, réfléchissons-y: </p><br><img src="https://habrastorage.org/webt/9v/bn/ws/9vbnws8g-1gclwefuvtpjv_yqpm.png"><br><br><p>  Si nous prenons α = 1: </p><br><img src="https://habrastorage.org/webt/7r/aw/er/7rawertkpcilxbfzrorhpygtrok.png"><br><br><p>  Ce qui correspond exactement à l'équation de Bellman, que nous avons vue il y a quelques paragraphes!  Nous savons donc déjà qu'il s'agit de la ligne chargée de diffuser les informations sur les valeurs de l'État. </p><br><p>  Mais généralement α (principalement connu sous le nom de vitesse d'apprentissage) est bien inférieur à 1, son objectif principal est d'éviter de grands changements en une seule mise à jour, donc au lieu de voler dans l'objectif, nous l'abordons lentement.  Dans notre approche tabulaire, la définition de α = 1 ne pose aucun problème, mais lorsque vous travaillez avec des réseaux de neurones (plus à ce sujet dans les articles suivants), tout peut facilement devenir incontrôlable. </p><br><p>  En regardant le code, nous voyons qu'à la ligne 16 de la première liste, nous avons défini td_target, c'est la valeur que nous devrions nous rapprocher, mais au lieu d'aller directement à cette valeur à la ligne 17, nous calculons td_error, nous utiliserons cette valeur en combinaison avec la vitesse apprendre à se déplacer lentement vers l'objectif. </p><br><p>  <b>N'oubliez pas que cette équation est une entité Q-Learning.</b> </p><br><p>  Maintenant, nous avons juste besoin de mettre à jour notre état, et tout est prêt, c'est la ligne 20. Nous répétons ce processus jusqu'à la fin de l'épisode, en mourant dans les rochers ou en atteignant le but. </p><br><br><h3>  Conclusion </h3><br><p>  Maintenant, nous comprenons et savons intuitivement comment encoder Q-Learning (au moins une version tabulaire), assurez-vous de vérifier tout le code utilisé pour cet article, disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> . </p><br><p>  Visualisation du test du processus d'apprentissage: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Vto8n9C7DSQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Veuillez noter que toutes les actions commencent par une valeur supérieure à sa valeur finale, c'est une astuce pour stimuler l'exploration du monde. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443240/">https://habr.com/ru/post/fr443240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443224/index.html">Tapis avec un éléphant et un cheval. Méthode TWIX</a></li>
<li><a href="../fr443232/index.html">Catalyse des muons en termes de chimie quantique. Partie I: hydrogène ordinaire vs hydrogène muon</a></li>
<li><a href="../fr443234/index.html">Ce que font les ingénieurs d'Apple et d'Intel au bureau: un cours en ligne axé sur la carrière en microélectronique moderne pour les étudiants</a></li>
<li><a href="../fr443236/index.html">Démystifier les réseaux de neurones convolutifs</a></li>
<li><a href="../fr443238/index.html">Comment ne pas devenir une libellule si vous avez de nombreuses bases de données différentes</a></li>
<li><a href="../fr443242/index.html">Quarkus est un Java subatomique supersonique. Un bref aperçu du cadre</a></li>
<li><a href="../fr443244/index.html">Tâches de débriefing. Beanpoisk_1</a></li>
<li><a href="../fr443246/index.html">Comment nous avons réinventé le PBX IP Askozia après la vente et la clôture du projet par le développeur</a></li>
<li><a href="../fr443248/index.html">Protocoles de réservation transparente de PRP et HSR</a></li>
<li><a href="../fr443250/index.html">Collecteur de déchets fait maison pour OpenJDK</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>