<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ’® ğŸšŒ ğŸ’ƒğŸ¿ Autos sind den Lesetests bereits voraus. aber verstehen sie, was sie lesen? ğŸ‘¨ ğŸ‘ğŸ¼ ğŸ™‹ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ein Tool namens BERT ist in der Lage, Menschen bei Lese- und VerstÃ¤ndnistests zu Ã¼berholen. Es zeigt jedoch auch, in welche Richtung die KI noch gehen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Autos sind den Lesetests bereits voraus. aber verstehen sie, was sie lesen?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/479446/"><h3>  Ein Tool namens BERT ist in der Lage, Menschen bei Lese- und VerstÃ¤ndnistests zu Ã¼berholen.  Es zeigt jedoch auch, in welche Richtung die KI noch gehen muss. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/1e6/359/58b/1e635958bf5af44bcc9b6256e1a0101f.jpg"><br><br>  Im Herbst 2017 entschied <a href="http://www.nyu.edu/projects/bowman/">Sam Bowman</a> , ein Computerlinguist der New York University, dass Computer den Text immer noch nicht sehr gut verstehen.  NatÃ¼rlich haben sie genug gelernt, um dieses VerstÃ¤ndnis in bestimmten engen Bereichen zu simulieren, wie etwa bei automatischen Ãœbersetzungen oder der Analyse von GefÃ¼hlen (zum Beispiel um festzustellen, ob ein Satz "unhÃ¶flich oder sÃ¼ÃŸ" ist, wie er sagte).  Bowman wollte jedoch ein messbares Zeugnis: ein echtes VerstÃ¤ndnis dessen, was in der menschlichen Sprache geschrieben stand.  Und er kam mit einem Test auf. <br><a name="habracut"></a><br>  In einem Artikel vom April 2018 <a href="https://arxiv.org/abs/1804.07461">,</a> der in Zusammenarbeit mit Kollegen der Washington University und DeepMind, einem Unternehmen von Google, das sich mit kÃ¼nstlicher Intelligenz befasst, verfasst wurde, stellte Bowman eine Reihe von neun LeseverstÃ¤ndnisaufgaben fÃ¼r Computer unter dem allgemeinen Namen GLUE (General Language Understanding Evaluation) vor verallgemeinerte Sprache].  Der Test wurde als "ziemlich bezeichnendes Beispiel dafÃ¼r entworfen, was die Forschungsgemeinschaft fÃ¼r interessante Aufgaben hÃ¤lt", sagte Bowman, aber auf eine Art und Weise, die "fÃ¼r die Menschen einfach" ist.  Beispielsweise wird in einer Aufgabe die Frage nach der Wahrheit eines Satzes gestellt, der auf der Grundlage von Informationen aus einem vorhergehenden Satz geschÃ¤tzt werden muss.  Wenn Sie sagen kÃ¶nnen, dass die Botschaft "PrÃ¤sident Trump ist im Irak gelandet, nachdem er seinen siebentÃ¤gigen Besuch begonnen hat" impliziert, dass "PrÃ¤sident Trump im Ausland ist", bestehen Sie die PrÃ¼fung. <br><br>  Autos versagten ihm.  Sogar die fortgeschrittenen neuronalen Netze erzielten in allen Tests nicht mehr als 69 von 100 Punkten - die ersten drei mit einem Minus.  Bowman und Kollegen waren nicht Ã¼berrascht.  Neuronale Netze - vielschichtige Konstruktionen mit Computerverbindungen, die in etwa der Arbeit von Neuronen im Gehirn von SÃ¤ugetieren Ã¤hneln - zeigen auf dem Gebiet der â€Verarbeitung natÃ¼rlicher Spracheâ€œ gute Ergebnisse, aber die Forscher waren sich nicht sicher, ob diesen Systemen etwas Ernsthaftes beigebracht wurde sprache.  Und GLUE beweist es.  "FrÃ¼he Ergebnisse zeigen, dass das Bestehen von GLUE-Tests Ã¼ber die FÃ¤higkeiten bestehender Modelle und Methoden hinausgeht", so Bowman et al. <br><br>  Ihre EinschÃ¤tzung hielt jedoch nicht lange an.  Im Oktober 2018 stellte Google eine neue Methode vor, BERT (Bidirectional Encoder Representations from Transformers) [bidirektionale Encoder-PrÃ¤sentationen fÃ¼r Transformatoren].  Er erhielt eine Punktzahl von 80,5 in GLUE.  In nur sechs Monaten sprangen die Autos in diesem neuen Test, der das wahre VerstÃ¤ndnis der natÃ¼rlichen Sprache durch Maschinen misst, von drei mit einem Minus auf vier mit einem Minus. <br><br>  "Es war wie verdammt", erinnert sich Bowman mit einem farbenfrohen Wort.  - Diese Nachricht wurde von der Community mit Misstrauen aufgenommen.  BERT hat in vielen Tests Noten erhalten, die nahe an dem liegen, was wir fÃ¼r maximal mÃ¶glich hielten. â€œ  In der Tat gab es vor dem Erscheinen von BERT im GLUE-Test noch nicht einmal Bewertungen menschlicher Leistungen, mit denen man vergleichen konnte.  Als Bowman und einer seiner Doktoranden sie im Februar 2019 zu GLUE hinzufÃ¼gten, dauerten sie nur wenige Monate, und dann <a href="https://blogs.msdn.microsoft.com/stevengu/2019/06/20/microsoft-achieves-human-performance-estimate-on-glue-benchmark/">schlug sie</a> auch das BERT-basierte Modell von Microsoft. <br><br>  Zum Zeitpunkt dieser VerÃ¶ffentlichung sind fast alle <a href="https://gluebenchmark.com/leaderboard/">ersten PlÃ¤tze</a> in den GLUE-Tests von Systemen belegt, die das BERT-Modell enthalten, erweitern oder optimieren.  FÃ¼nf von ihnen sind in ihren menschlichen FÃ¤higkeiten Ã¼berlegen. <br><br>  Aber heiÃŸt das, dass die KI unsere Sprache langsam versteht oder nur lernt, unsere Systeme zu schlagen?  Nachdem die BERT-basierten neuronalen Netze die GLUE-TypprÃ¼fungen im Sturm genommen hatten, erschienen neue Bewertungsmethoden, die diese NLP-Systeme als Computerversionen von â€ <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25BC%25D0%25BD%25D1%258B%25D0%25B9_%25D0%2593%25D0%25B0%25D0%25BD%25D1%2581">smart Hans</a> â€œ betrachteten, einem Pferd, das zu Beginn des 20. Jahrhunderts lebte und fÃ¼r das es angeblich klug genug war arithmetische Berechnungen im Kopf zu machen, aber tatsÃ¤chlich die unbewussten Zeichen zu lesen, die ihm von seinem Besitzer gegeben wurden. <br><br>  "Wir wissen, dass wir uns irgendwo in der Grauzone befinden zwischen dem Verstehen der Sprache in einem sehr langweiligen und engen Sinn und der Schaffung von KI", sagte Bowman.  - Generell lÃ¤sst sich die Reaktion von Spezialisten wie folgt beschreiben: Wie ist das passiert?  Was bedeutet das?  Was sollen wir jetzt tun? " <br><br><h2>  Schreiben Sie Ihre eigenen Regeln </h2><br>  In dem berÃ¼hmten Gedankenexperiment â€ <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B8%25D1%2582%25D0%25B0%25D0%25B9%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BD%25D0%25B0%25D1%2582%25D0%25B0">Chinese Room</a> â€œ sitzt eine Person, die die chinesische Sprache nicht kennt, in einem Raum voller BÃ¼cher mit Regeln.  In den BÃ¼chern finden Sie genaue Anweisungen, wie Sie die Abfolge der chinesischen Zeichen, die den Raum betreten, akzeptieren und eine passende Antwort geben kÃ¶nnen.  Eine Person auÃŸerhalb der Palmen Fragen in Chinesisch unter der TÃ¼r des Raumes geschrieben.  Die Person im Inneren wendet sich an die BÃ¼cher mit den Regeln und formuliert vollkommen vernÃ¼nftige Antworten auf Chinesisch. <br><br>  Dieses Experiment wurde verwendet, um zu beweisen, dass man trotz des Ã¤uÃŸeren Eindrucks nicht sagen kann, dass die Person im Raum Chinesisch versteht.  Selbst eine Simulation des Verstehens war jedoch ein akzeptables Ziel des NLP. <br><br>  Das einzige Problem ist das Fehlen perfekter BÃ¼cher mit Regeln, da die natÃ¼rliche Sprache zu komplex und unsystematisch ist, um auf einen soliden Satz von Spezifikationen reduziert zu werden.  Nehmen Sie zum Beispiel die Syntax: Regeln (einschlieÃŸlich empirischer Regeln), die die Gruppierung von WÃ¶rtern in aussagekrÃ¤ftige SÃ¤tze bestimmen.  Der Satz " <a href="https://books.google.com/books%3Fid%3D55YaAAAAIAAJ%26dq%3Dcolorless%2Bgreen%2Bideas%2Bsleep%2Bfuriously">GewalttÃ¤tig schlafen farblose grÃ¼ne Ideen</a> " hat die Syntax, aber jede Person, die die Sprache kennt, versteht ihre Sinnlosigkeit.  Welches speziell entworfene Regelwerk kÃ¶nnte diese ungeschriebene Tatsache in Bezug auf die natÃ¼rliche Sprache enthalten - von unzÃ¤hligen anderen Tatsachen ganz zu schweigen? <br><br>  NLP-Forscher versuchten, diese <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D1%2583%25D1%2580%25D0%25B0_%25D0%25BA%25D1%2580%25D1%2583%25D0%25B3%25D0%25B0">Quadratur des Kreises zu finden</a> , und zwangen die neuronalen Netze, ihre eigenen handwerklichen RegelbÃ¼cher zu schreiben  "Pre-Training" oder Vorschulung. <br><br>  Bis 2018 war eines der wichtigsten Trainingsinstrumente so etwas wie ein WÃ¶rterbuch.  In diesem WÃ¶rterbuch wurde eine <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D1%2581%25D1%2582%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B2">Vektordarstellung der WÃ¶rter</a> [Worteinbettung] verwendet, um die Verbindungen zwischen WÃ¶rtern in Form von Zahlen zu beschreiben, damit die neuronalen Netze diese Informationen als Eingabe wahrnehmen kÃ¶nnen - so etwas wie ein grobes Glossar fÃ¼r eine Person in einem chinesischen Raum.  Das im VektorwÃ¶rterbuch vorgeÃ¼bte neuronale Netz blieb jedoch weiterhin blind fÃ¼r die Bedeutung von WÃ¶rtern auf Satzebene.  "Aus ihrer Sicht sind die SÃ¤tze" Mann hat den Hund gebissen "und" Hund hat den Mann <a href="http://tallinzen.net/">gebissen</a> "identisch", sagte <a href="http://tallinzen.net/">Tel Linsen</a> , ein Computerlinguist an der Johns Hopkins University. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55a/245/f48/55a245f4866cf5ffcf2689d2161f8536.jpg" width="50%"><br>  <i>Tel Linsen, Computerlinguist an der Johns Hopkins University.</i> <br><br>  Die verbesserte Methode verwendet das Pre-Training, um dem neuronalen Netzwerk umfassendere RegelbÃ¼cher zur VerfÃ¼gung zu stellen - nicht nur ein WÃ¶rterbuch, sondern auch eine Syntax mit einem Kontext -, bevor es eine bestimmte NLP-Aufgabe lehrt.  Anfang 2018 hatten Forscher von OpenAI, der University of San Francisco, dem Allen Institute for Artificial Intelligence und der University of Washington gleichzeitig einen schwierigen Weg gefunden, um dem nÃ¤her zu kommen.  Anstatt nur eine einzige Schicht des Netzwerks zu trainieren, die die Vektordarstellung von WÃ¶rtern verwendet, begannen die Forscher, das gesamte Netzwerk fÃ¼r eine allgemeinere Aufgabe zu trainieren, die als Sprachmodellierung bezeichnet wird. <br><br>  "Der einfachste Weg, eine Sprache zu modellieren, ist folgender: Ich werde eine Reihe von WÃ¶rtern lesen und versuchen, Folgendes vorherzusagen", erklÃ¤rte <a href="https://research.fb.com/people/ott-myle/">Mile Ott</a> , ein Facebook-Forscher.  â€Wenn ich sage, dass George W. Bush geboren wurde, mÃ¼ssen die Models das nÃ¤chste Wort in diesem Satz vorhersagen.â€œ <br><br>  Solche Sprachmodelle mit tiefem Training kÃ¶nnen sehr effizient erstellt werden.  Forscher speisen einfach groÃŸe Mengen an geschriebenem Text aus freien Quellen wie Wikipedia in ihre neuronalen Netze ein - Milliarden von WÃ¶rtern, die in grammatikalisch korrekten SÃ¤tzen angeordnet sind - und lassen das Netzwerk das nÃ¤chste Wort von sich aus vorhersagen.  TatsÃ¤chlich entspricht dies der Tatsache, dass wir eine Person in einem chinesischen Raum einladen, ihre eigenen Regeln zu erstellen, wobei eingehende chinesische Nachrichten als Referenz verwendet werden. <br><br>  "Das SchÃ¶ne an diesem Ansatz ist, dass das Modell eine Menge Syntaxkenntnisse erlangt", sagte Ott. <br><br>  DarÃ¼ber hinaus kÃ¶nnen solche vorab trainierten neuronalen Netze ihre Sprachdarstellungen anwenden, um eine engere Aufgabe, die nicht mit der Wortvorhersage zusammenhÃ¤ngt, fÃ¼r den Feinabstimmungsprozess zu unterrichten. <br><br>  â€Sie kÃ¶nnen das Modell aus der Vorbereitungsphase nehmen und an jede echte Aufgabe anpassen, die Sie benÃ¶tigenâ€œ, erklÃ¤rte Ott.  "Und danach erhalten Sie viel bessere Ergebnisse, als wenn Sie von Anfang an versucht hÃ¤tten, Ihr Problem direkt zu lÃ¶sen." <br><br>  Im Juni 2018, als OpenAI sein <a href="https://openai.com/blog/language-unsupervised/">GPT-Neuronales Netz</a> mit einem darin enthaltenen Sprachmodell vorstellte, das einen Monat lang eine Milliarde WÃ¶rter trainierte (entnommen aus 11.038 digitalen BÃ¼chern), war das Ergebnis im GLUE-Test mit 72,8 Punkten sofort das beste das beste.  Trotzdem schlug Sam Bowman vor, dass sich dieser Bereich fÃ¼r eine sehr lange Zeit entwickeln wird, bevor sich ein System zumindest dem menschlichen Niveau annÃ¤hern kann. <br><br>  Und dann erschien BERT. <br><br><h2>  Vielversprechendes Rezept </h2><br>  Was ist BERT? <br><br>  Erstens ist es kein vollstÃ¤ndig trainiertes neuronales Netzwerk, das sofort Ergebnisse auf menschlicher Ebene liefern kann.  Bowman sagt, dies sei ein "sehr genaues Rezept fÃ¼r das Training des neuronalen Netzes".  Als ein BÃ¤cker nach dem Rezept garantiert leckere Kuchen ausgeben kann - die dann fÃ¼r verschiedene Kuchen verwendet werden kÃ¶nnen, von Blaubeere bis Spinat-Quiche - und Google-Forscher haben ein BERT-Rezept erstellt, das als ideale Grundlage fÃ¼r das "Backen" neuronaler Netze dienen kann (d. H , deren Feinabstimmung), damit sie verschiedene Aufgaben bei der Verarbeitung der natÃ¼rlichen Sprache gut bewÃ¤ltigen kÃ¶nnen.  Google hat den BERT-Code geÃ¶ffnet, was bedeutet, dass andere Forscher dieses Rezept nicht mehr von Grund auf wiederholen mÃ¼ssen - sie kÃ¶nnen es einfach herunterladen.  Es ist so, als wÃ¼rde man vorgebackenen Kuchen fÃ¼r Kuchen im Laden kaufen. <br><br>  Wenn BERT ein Rezept ist, wie lautet dann die Zutatenliste?  "Dies ist das Ergebnis von drei verschiedenen Dingen, die miteinander verbunden sind, damit das System funktioniert", sagte <a href="https://levyomer.wordpress.com/">Omer Levy</a> , ein Facebook-Forscher, der das BERT-GerÃ¤t <a href="https://arxiv.org/abs/1906.04341">analysierte</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/f23/473/1eef234733ede2389825e047170009e0.jpg" width="50%"><br>  <i>Omer Levy, Facebook-Forscher</i> <br><br>  Das erste ist das vorgeÃ¼bte Sprachmodell, dh dieselben Verzeichnisse aus dem chinesischen Raum.  Die zweite MÃ¶glichkeit besteht darin, zu entscheiden, welche Merkmale des Vorschlags am wichtigsten sind. <br><br>  Im Jahr 2017 arbeitete <a href="http://jakob.uszkoreit.net/">Jacob Uzkoreit</a> , ein Ingenieur bei Google Brain, an MÃ¶glichkeiten, die Versuche des Unternehmens, die Sprache zu verstehen, zu beschleunigen.  Er stellte fest, dass alle fortgeschrittenen neuronalen Netze unter ihren inhÃ¤renten EinschrÃ¤nkungen leiden: Sie studieren den Satz mit Worten.  Eine solche â€Sequenzâ€œ schien mit der Vorstellung Ã¼bereinzustimmen, wie die Leute den Text lesen.  Uzkoreit zeigte sich jedoch interessiert: "KÃ¶nnte es nicht sein, dass das Verstehen der Sprache in einem linearen, sequentiellen Modus nicht optimal ist." <br><br>  Die enge Zusammenarbeit mit Kollegen fÃ¼hrte zu einer neuen Architektur neuronaler Netze mit dem Schwerpunkt â€Aufmerksamkeitâ€œ, einem Mechanismus, der es jeder Schicht des neuronalen Netzes ermÃ¶glicht, bestimmten Merkmalen der Eingabedaten im Vergleich zu anderen Merkmalen groÃŸe Gewichte zuzuweisen.  Diese neue Architektur mit Aufmerksamkeit, ein Transformator, kann einen Satz wie "ein Hund beiÃŸt den Mann" als Eingabe nehmen und jedes Wort auf unterschiedliche Weise parallel codieren.  Zum Beispiel kann ein Transformator "Bites" und "Person" als Verb und Subjekt-Objekt binden, wobei der Artikel "a" ignoriert wird.  Gleichzeitig kann sie "Biss" und "Hund" als Verb und Subjekt-Subjekt in Beziehung setzen und den Artikel "the" ignorieren. <br><br>  Die inkonsistente Natur des Transformators prÃ¤sentiert SÃ¤tze ausdrucksvoller oder, wie Uzkoreit sagt, baumartig.  Jede Schicht des neuronalen Netzwerks stellt viele parallele Verbindungen zwischen bestimmten WÃ¶rtern her, wobei der Rest ignoriert wird - ungefÃ¤hr wie ein SchÃ¼ler in der Grundschule einen Satz in Teile zerlegt.  Diese Verbindungen werden hÃ¤ufig zwischen WÃ¶rtern hergestellt, die mÃ¶glicherweise nicht in der NÃ¤he sind.  "Solche Strukturen sehen aus wie eine Ãœberlagerung mehrerer BÃ¤ume", erklÃ¤rte Uzkoreit. <br><br>  Solche baumartigen Darstellungen von SÃ¤tzen geben Transformatoren die MÃ¶glichkeit, kontextbezogene Bedeutungen zu modellieren und die ZusammenhÃ¤nge zwischen WÃ¶rtern, die in komplexen SÃ¤tzen weit voneinander entfernt sind, effektiv zu untersuchen.  "Das ist ein wenig eingÃ¤ngig", sagte Uzkoreit, "aber es kommt aus der Linguistik, die sich seit langem mit baumartigen Sprachmodellen beschÃ¤ftigt." <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/0a1/fe2/21e0a1fe2331c110f5cb2a966b2b173d.jpg" width="50%"><br>  <i>Jacob Uzkoreit, Leiter des Berliner Teams Google AI Brain</i> <br><br>  SchlieÃŸlich erweitert die dritte Zutat im BERT-Rezept den nichtlinearen Messwert noch mehr. <br><br>  Im Gegensatz zu anderen vorgefertigten Sprachmodellen, die durch Verarbeitung von Terabyte Text von links nach rechts durch neuronale Netze erstellt wurden, liest das BERT-Modell von rechts nach links und gleichzeitig von links nach rechts und lernt, vorherzusagen, welche WÃ¶rter zufÃ¤llig aus SÃ¤tzen ausgeschlossen wurden.  Beispielsweise kann BERT einen Satz der Form "George W. Bush in Connecticut im Jahr 1946" akzeptieren und vorhersagen, welches Wort in der Mitte des Satzes verborgen ist (in diesem Fall "geboren"), nachdem der Text in beide Richtungen verarbeitet wurde.  "Diese BidirektionalitÃ¤t zwingt das neuronale Netzwerk, so viele Informationen wie mÃ¶glich aus einer Untergruppe von WÃ¶rtern zu extrahieren", sagte Uzkoreit. <br><br>  Das BERT-basierte VortÃ¤uschen wie ein Wortspiel - Sprachmodellierung mit Maskierung - ist nichts Neues.  Es wird seit Jahrzehnten verwendet, um das VerstÃ¤ndnis der Menschen fÃ¼r die Sprache zu messen.  FÃ¼r Google bot er eine praktische MÃ¶glichkeit, BidirektionalitÃ¤t in neuronalen Netzen anstelle der Einweg-Vortrainingsmethoden zu verwenden, die zuvor in diesem Bereich vorherrschten.  "Vor BERT war die unidirektionale Sprachmodellierung der Standard, obwohl dies eine optionale EinschrÃ¤nkung ist", sagte <a href="https://kentonl.com/">Kenton Lee</a> , ein Google-Forscher. <br><br>  Jeder dieser drei Bestandteile - ein tiefes Sprachmodell mit Vorschulung, Aufmerksamkeit und BidirektionalitÃ¤t - existierte vor BERT separat.  Aber bis Google Ende 2018 sein Rezept verÃ¶ffentlichte, hat niemand sie so erfolgreich kombiniert. <br><br><h2>  Rezept verfeinern </h2><br>  Wie jedes gute Rezept wurde BRET bald von verschiedenen KÃ¶chen an ihren Geschmack angepasst.  Im FrÃ¼hjahr 2019 gab es eine Zeit, in der â€Microsoft und Alibaba einander auf den Fersen waren und wÃ¶chentlich die PlÃ¤tze in der Rangliste wechselten und ihr Modell anpasstenâ€œ, erinnert sich Bowman.  Als die verbesserte Version von BERT im August erstmals unter dem Namen RoBERTa verÃ¶ffentlicht wurde, bemerkte der Forscher <a href="http://ruder.io/">Sebastian Ruder</a> von DeepMind in seinem beliebten <a href="http://newsletter.ruder.io/issues/nlp-in-industry-leaderboard-madness-fast-ai-nlp-transfer-learning-tools-186245">NLP-Newsletter</a> trocken: "Neuer Monat und ein neues fortgeschrittenes Sprachmodell mit Vortraining." <br><br>  Wie der Kuchen hat BERT mehrere Designentscheidungen, die sich auf die QualitÃ¤t seiner Arbeit auswirken.  Dies beinhaltet die GrÃ¶ÃŸe des gebackenen neuronalen Netzwerks, die Datenmenge, die fÃ¼r das Vortraining verwendet wird, die Methode zum Maskieren von WÃ¶rtern und wie lange das neuronale Netzwerk mit diesen Daten gearbeitet hat.  Und in nachfolgenden Rezepten wie RoBERTa optimieren Forscher diese Entscheidungen - wie ein Koch, der ein Rezept festlegt. <br><br>  Im Falle von RoBERTa haben Forscher von Facebook und der Washington University die Anzahl einiger Inhaltsstoffe erhÃ¶ht (Daten vor dem Training, LÃ¤nge der eingehenden Sequenzen, Trainingszeit), ein Inhaltsstoff wurde gestrichen (die Aufgabe, den nÃ¤chsten Satz vorherzusagen), der ursprÃ¼nglich im BERT enthalten war und die Ergebnisse negativ beeinflusste ), und der andere wurde geÃ¤ndert (kompliziert die Aufgabe, einzelne WÃ¶rter zu maskieren).  Damit belegten sie kurzzeitig den ersten Platz im GLUE-Ranking.  Sechs Wochen spÃ¤ter verbesserten Forscher von Microsoft und der University of Maryland RoBERTa und holten den nÃ¤chsten Sieg.  Derzeit hat ein anderes Modell den ersten Platz in GLUE, ALBERT (die AbkÃ¼rzung fÃ¼r "Lite BERT", dh "Lite BERT"), eingenommen, was die Grundstruktur von BERT geringfÃ¼gig verÃ¤nderte. <br><br>  "Wir klÃ¤ren immer noch, welche Rezepte funktionieren und welche nicht", sagte Ott von Facebook, der an RoBERTa arbeitete. <br><br>  Da Ihnen die Verbesserung der Technik des Vorbackens von Kuchen jedoch nicht die Grundlagen der Chemie vermittelt, werden Sie durch die schrittweise Verbesserung des BERT nicht viel theoretisches Wissen Ã¼ber die Entwicklung von NLP erhalten.  "Ich werde Ihnen gegenÃ¼ber Ã¤uÃŸerst ehrlich sein - ich folge diesen Arbeiten nicht, da sie fÃ¼r mich Ã¤uÃŸerst langweilig sind", sagte Linsen, ein Computerlinguist an der Johns Hopkins University.  "Hier gibt es ein gewisses wissenschaftliches RÃ¤tsel", gibt er zu, aber nicht, wie man BERT und all seine Nachkommen schlauer macht und nicht einmal herausfindet, warum sie so schlau sind.  Stattdessen "versuchen wir zu verstehen, wie gut diese Modelle die Sprache wirklich verstehen", sagte er, "anstatt seltsame Tricks zu lernen, die irgendwie mit den DatensÃ¤tzen funktionieren, mit denen wir diese Modelle normalerweise bewerten." <br><br>  Mit anderen Worten, BERT macht etwas richtig.  Aber was ist, wenn er es aus dem falschen Grund tut? <br><br><h2>  Tricky aber nicht schlau </h2><br>  Im Juli 2019 verwendeten zwei Forscher der Taiwan State University, Cheng Kun, das BERT mit beeindruckenden Ergebnissen fÃ¼r einen relativ wenig bekannten Leistungstest, der als â€Argument-VerstÃ¤ndnis-Aufgabeâ€œ bezeichnet wurde.  Um die Aufgabe abzuschlieÃŸen, muss eine implizite Anfangsbedingung (â€Grundlageâ€œ) gewÃ¤hlt werden, die das Argument fÃ¼r eine Aussage unterstÃ¼tzt.  Um beispielsweise zu beweisen, dass â€Rauchen Krebs verursachtâ€œ (Aussage), da â€wissenschaftliche Studien einen Zusammenhang zwischen Rauchen und Krebs gezeigt habenâ€œ (Argumentation), mÃ¼ssen Sie das Argument â€wissenschaftliche Forschung kann vertrauenswÃ¼rdig seinâ€œ (â€Grundlageâ€œ) und nicht eine andere Option wÃ¤hlen: â€Wissenschaftliche Forschung ist teuerâ€œ (dies ist jedoch in diesem Zusammenhang nicht relevant).  Alles klar? <br><br>  Wenn nicht alle, mach dir keine Sorgen.  Selbst Menschen kÃ¶nnen diese Aufgabe ohne Ãœbung nicht sehr gut.  Die durchschnittliche Grundlinie fÃ¼r eine Person ohne Training ist 80 von 100. BERT erreichte 77 - was die Autoren als "unerwartet" bezeichneten. <br><br>   ,  ,  BERT        ,  ,      : BERT      .  ,    ,       .. Â« Â».  ,     ,   Â«Â»,       61% .      ,  ,   BERT   77  53 â€“     .       The Gradient     , <a href="https://thegradient.pub/nlps-clever-hans-moment-has-arrived/"></a> BERT  Â« Â», ,    . <br><br>   , " <a href="https://www.aclweb.org/anthology/P19-1334">   </a> ",      ,    BERT    GLUE           .       ,  ,   BERT    .     (Heuristic Analysis for Natural-Language-Inference Systems, HANS) [  ,      ]. <br><br>   , BERT    ,    â€“   ?      ,     GLUE  .    ,    ,          BERT. Â« -  ,        GLUE,     â€ â€œ,    , â€”  , â€”     Â».    ,    BERT   . Â« , -,  ,  -    , â€”  . â€“           Â». <br><br>    ,         ,           â€“       BERT,       ,    ,        Â« Â».       Â« Â»,        NLP   ,          .     Â« BERT   Â»,  ,  Â«     Â». <br><br>      NLP ,                   .    , BERT        .     Â«   NLP,        Â», â€”  <a href="https://www.cs.uml.edu/~arogers/"> </a> ,         . ,     ,          ,            ,     ,   . <br><br>  ,    ,      ,      .         .    ,    . Â«    ,       ,       ,           Â», â€”  . <br><br>         <a href="https://super.gluebenchmark.com/leaderboard">SuperGLUE</a> ,   ,        BERT.           .    ( )  ,   ,       ,  ?          ? <br><br> Â« , â€”  . â€“  ,    LSAT  MCAT,           ,     Â».   ,   ,        . Â«        ,    ,     , â€”  . â€“      ,        ,   ,     Â». </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de479446/">https://habr.com/ru/post/de479446/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de479428/index.html">Digitale Veranstaltungen in Moskau vom 9. bis 15. Dezember</a></li>
<li><a href="../de479430/index.html">Digitale Veranstaltungen in St. Petersburg vom 9. bis 15. Dezember</a></li>
<li><a href="../de479432/index.html">Yandex.Maps: Ich bin zum Karten-Controller gegangen - ich habe sofort die Position des Benutzers erhalten (okay, jetzt im Ernst)</a></li>
<li><a href="../de479438/index.html">Postgres-Tuesday # 5: â€œPostgreSQL und Kubernetes. CI / CD. Testautomatisierung Â»</a></li>
<li><a href="../de479442/index.html">Alexey Savvateev: Spieltheoretisches Modell der sozialen Spaltung (+ Nginx-Umfrage)</a></li>
<li><a href="../de479450/index.html">AppCode 2019.3: Arbeitet schneller, versteht Swift besser, kennt sich mit Mac Catalyst aus und zeigt Versammlungsnachrichten an</a></li>
<li><a href="../de479452/index.html">Wie sich das Domain Name System entwickelte: Das ARPANET-Zeitalter</a></li>
<li><a href="../de479458/index.html">SchÃ¶nheit oder ZweckmÃ¤ÃŸigkeit im Serverraum</a></li>
<li><a href="../de479460/index.html">Ein Leitfaden fÃ¼r fliegende Autos</a></li>
<li><a href="../de479462/index.html">Serialisierung in C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>