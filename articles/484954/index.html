<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍💻 🚭 🎗️ Guía visual de solución de problemas para Kubernetes 🤖 🕜 ♉️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : Este artículo es parte de los materiales disponibles gratuitamente del proyecto learnk8s , que enseña a las empresas y administradores i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guía visual de solución de problemas para Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/484954/">  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: Este artículo es parte de los materiales disponibles gratuitamente del proyecto <a href="https://learnk8s.io/">learnk8s</a> , que enseña a las empresas y administradores individuales cómo trabajar con Kubernetes.</i>  <i>En él, Daniele Polencic, el gerente del proyecto, comparte una instrucción clara sobre qué pasos tomar en caso de problemas generales para las aplicaciones que se ejecutan en el clúster K8.</i> <br><br><img src="https://habrastorage.org/webt/ch/5u/xa/ch5uxanj-3ivwqu88swqyoi6bsu.png"><br><br>  TL; DR: aquí hay un diagrama que lo ayudará a depurar la implementación en Kubernetes: <a name="habracut"></a><br><br> <a href=""><img src="https://habrastorage.org/webt/4r/qp/si/4rqpsie8dnplkqxahaqntpi2ssw.png"></a> <br><br>  <i>Diagrama de flujo para encontrar y corregir errores en un clúster.</i>  <i>En el original (en inglés) está disponible en <a href="https://learnk8s.io/a/troubleshooting-kubernetes.pdf">PDF</a> y <a href="">como imagen</a> .</i> <br><br>  Al implementar una aplicación en Kubernetes, generalmente necesita definir tres componentes: <br><br><ul><li>  <b>La implementación</b> es una receta para crear copias de una aplicación llamada pods; </li><li>  <b>Servicio</b> : un equilibrador de carga interno que distribuye el tráfico entre los pods; </li><li>  <b>Ingreso</b> : una descripción de cómo fluirá el tráfico del mundo exterior al Servicio. </li></ul><br>  Aquí hay un breve resumen gráfico: <br><br>  1) En Kubernetes, las aplicaciones reciben tráfico del mundo exterior a través de dos capas de equilibradores de carga: internos y externos. <br><br><img src="https://habrastorage.org/webt/3v/cy/z9/3vcyz9a-2ciiqbh9he7idgvo7uy.png"><br><br>  2) El equilibrador interno se llama Servicio, el externo - Ingreso. <br><br><img src="https://habrastorage.org/webt/23/mn/zc/23mnzcfo_b3niccdivn4bc4vzei.png"><br><br>  3) La implementación crea pods y los monitorea (no se crean manualmente). <br><br><img src="https://habrastorage.org/webt/4j/c2/h9/4jc2h9pgzbxmkon4ewkbeuf0vhc.png"><br><br>  Supongamos que desea implementar una aplicación simple a la <i>Hello World</i> .  La configuración de YAML para esto se verá así: <br><br><pre><code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment # &lt;&lt;&lt; metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service # &lt;&lt;&lt; metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 selector: name: app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress # &lt;&lt;&lt; metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: app servicePort: 80 path: /</code> </pre> <br>  La definición es bastante larga y es fácil confundirse acerca de cómo se relacionan los componentes entre sí. <br><br>  Por ejemplo: <br><br><ul><li>  ¿Cuándo debería usar el puerto 80 y cuándo - 8080? </li><li>  ¿Debo crear un nuevo puerto para cada servicio para que no entren en conflicto? </li><li>  ¿Importan los nombres de las etiquetas?  ¿Deberían ser iguales en todas partes? </li></ul><br>  Antes de centrarnos en la depuración, recordemos cómo los tres componentes están relacionados entre sí.  Comencemos con la implementación y el servicio. <br><br><h2>  Implementación de conexión'a y Servicio'a </h2><br>  Se sorprenderá, pero las implementaciones y el servicio no están conectados de ninguna manera.  En cambio, el Servicio apunta directamente a Pods sin pasar por la Implementación. <br><br>  Por lo tanto, estamos interesados ​​en cómo se relacionan los Pods y los Servicios entre sí.  Tres cosas para recordar: <br><br><ol><li>  Un <code>selector</code> servicio debe coincidir con al menos una etiqueta Pod. </li><li>  <code>targetPort</code> debe coincidir con el <code>containerPort</code> contenedor dentro del Pod. </li><li>  <code>port</code> servicio <code>port</code> puede ser cualquier cosa.  Diferentes servicios pueden usar el mismo puerto porque tienen diferentes direcciones IP. </li></ol><br>  El siguiente diagrama representa todo lo anterior en forma gráfica: <br><br>  1) Imagine que el servicio dirige el tráfico a un determinado pod: <br><br><img src="https://habrastorage.org/webt/2a/e5/8f/2ae58fcgoi7aifmcr5rl_0bseym.png"><br><br>  2) Al crear un pod, debe especificar <code>containerPort</code> para cada contenedor en los pods: <br><br><img src="https://habrastorage.org/webt/xc/fa/ow/xcfaowomhbtqhebhodgzhzrkupc.png"><br><br>  3) Al crear el servicio, debe especificar <code>port</code> y <code>targetPort</code> .  <i>¿Pero por cuál se conecta al contenedor?</i> <br><br><img src="https://habrastorage.org/webt/vg/wj/nd/vgwjnde0xyzdblwamomfxjaxb40.png"><br><br>  4) A través de <code>targetPort</code> .  Debe coincidir con <code>containerPort</code> . <br><br><img src="https://habrastorage.org/webt/q4/yx/qn/q4yxqnkxxilupalikahmqqp09x8.png"><br><br>  5) Digamos que el puerto 3000 está abierto en el contenedor, entonces el valor <code>targetPort</code> debería ser el mismo. <br><br><img src="https://habrastorage.org/webt/cq/tj/-s/cqtj-srznih70qh7bxs3w_l7bis.png"><br><br>  En el archivo YAML, las etiquetas y los <code>ports</code> / <code>targetPort</code> deben coincidir: <br><br><pre> <code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt; spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 # &lt;&lt;&lt; --- apiVersion: v1 kind: Service metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 # &lt;&lt;&lt; selector: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt;</code> </pre> <br>  <i>¿Qué pasa con la <code>track: canary</code> en la parte superior de la sección Implementación?</i>  <i>¿Debería coincidir?</i> <br><br>  Esta etiqueta se refiere a la implementación y el servicio no la utiliza para enrutar el tráfico.  En otras palabras, se puede eliminar o asignar un valor diferente. <br><br>  <i>¿Qué pasa con el selector de <code>matchLabels</code> ?</i> <br><br>  <b>Siempre debe coincidir con las etiquetas de Pod</b> , ya que Deployment lo utiliza para rastrear pods. <br><br>  <i>Supongamos que hiciste las ediciones correctas.</i>  <i>¿Cómo verificarlos?</i> <br><br>  Puede verificar la etiqueta del pod con el siguiente comando: <br><br><pre> <code class="bash hljs">kubectl get pods --show-labels</code> </pre> <br>  O, si los pods pertenecen a varias aplicaciones: <br><br><pre> <code class="bash hljs">kubectl get pods --selector any-name=my-app --show-labels</code> </pre> <br>  Donde <code>any-name=my-app</code> es la etiqueta <code>any-name: my-app</code> . <br><br>  <i>¿Hay alguna dificultad?</i> <br><br>  ¡Puedes conectarte al pod!  Para hacer esto, use el comando <code>port-forward</code> en kubectl.  Le permite conectarse al servicio y verificar la conexión. <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service name&gt; 3000:80</code> </pre> <br>  Aquí: <br><br><ul><li>  <code>service/&lt;service name&gt;</code> - nombre del servicio;  en nuestro caso es <code>my-service</code> ; </li><li>  3000: el puerto que desea abrir en la computadora; </li><li>  80: puerto especificado en el campo de <code>port</code> del servicio. </li></ul><br>  Si pudo establecer una conexión, la configuración es correcta. <br><br>  Si no se pudo establecer la conexión, entonces hay un problema con las etiquetas o los puertos no coinciden. <br><br><h2>  Conexión de servicio e ingreso </h2><br>  El siguiente paso para proporcionar acceso a la aplicación está relacionado con la configuración de Ingress.  Ingress debe saber cómo encontrar el servicio, luego encontrar los pods y dirigir el tráfico hacia ellos.  Ingress encuentra el servicio deseado por nombre y puerto abierto. <br><br>  En la descripción de Ingress y Service, deben coincidir dos parámetros: <br><br><ol><li>  <code>servicePort</code> in Ingress debe coincidir con el parámetro de <code>port</code> en Service; </li><li>  <code>serviceName</code> in Ingress debe coincidir con el campo de <code>name</code> en Service. </li></ol><br>  El siguiente diagrama resume la conexión de los puertos: <br><br>  1) Como ya sabe, el Servicio escucha en un determinado <code>port</code> : <br><br><img src="https://habrastorage.org/webt/9q/t0/fz/9qt0fzsyme9mnrd4ki07ezamnkg.png"><br><br>  2) Ingress tiene un parámetro llamado <code>servicePort</code> : <br><br><img src="https://habrastorage.org/webt/rn/du/yw/rnduyw4xvfmpjmhup8fy9ao2d1a.png"><br><br>  3) Este parámetro ( <code>servicePort</code> ) siempre debe coincidir con el <code>port</code> en la definición del Servicio: <br><br><img src="https://habrastorage.org/webt/1d/ap/ty/1daptyulxphnbb2dt6uben-lnzk.png"><br><br>  4) Si el puerto 80 se especifica en Servicio, <code>servicePort</code> también debe ser 80: <br><br><img src="https://habrastorage.org/webt/nc/mi/dl/ncmidlxiegmtmhtozaxxqhznaya.png"><br><br>  En la práctica, debe prestar atención a las siguientes líneas: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: my-service # &lt;&lt;&lt; spec: ports: - port: 80 # &lt;&lt;&lt; targetPort: 8080 selector: any-name: my-app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: my-service # &lt;&lt;&lt; servicePort: 80 # &lt;&lt;&lt; path: /</code> </pre> <br>  <i>¿Cómo verificar si Ingress está funcionando?</i> <br><br>  Puede usar el método con <code>kubectl port-forward</code> , pero en lugar del servicio necesita conectarse al controlador Ingress. <br><br>  Primero debe encontrar el nombre del pod con el controlador Ingress: <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Localice el pod de Ingress (puede referirse a un espacio de nombres diferente) y ejecute el comando <code>describe</code> para averiguar los números de puerto: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc \ --namespace kube-system \ | grep Ports Ports: 80/TCP, 443/TCP, 18080/TCP</code> </pre> <br>  Finalmente, conéctese al pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Ahora, cada vez que envíe una solicitud al puerto 3000 en la computadora, será redirigida al puerto 80 del pod con el controlador Ingress.  Al ir a <a href="http://localhost:3000/">http: // localhost: 3000</a> , debería ver la página creada por la aplicación. <br><br><h2>  Resumen de puerto </h2><br>  Recordemos nuevamente qué puertos y etiquetas deben coincidir: <br><br><ol><li>  El selector en la definición del Servicio debe coincidir con la etiqueta del pod; </li><li>  <code>targetPort</code> en la definición del Servicio debe coincidir con el <code>containerPort</code> contenedor dentro del pod; </li><li>  <code>port</code> en la definición de Servicio puede ser cualquier cosa.  Diferentes servicios pueden usar el mismo puerto porque tienen diferentes direcciones IP; </li><li>  <code>servicePort</code> Ingress debe coincidir con el <code>port</code> en la definición del Servicio; </li><li>  El nombre del servicio debe coincidir con el campo <code>serviceName</code> en Ingress. </li></ol><br>  Por desgracia, no es suficiente saber cómo estructurar adecuadamente su configuración YAML. <br><br>  <i>¿Qué pasa cuando algo sale mal?</i> <br><br>  Quizás la cápsula no se inicia o se bloquea. <br><br><h2>  3 pasos para solucionar fallas de aplicaciones en Kubernetes </h2><br>  Antes de depurar una implementación, debe tener una buena comprensión de cómo funciona Kubernetes. <br><br>  Dado que hay tres componentes en cada aplicación descargada a K8, se deben depurar en un cierto orden, comenzando desde abajo. <br><br><ol><li>  Primero debes asegurarte de que las vainas funcionan, luego ... </li><li>  Compruebe si el servicio entrega tráfico a los pods y luego ... </li><li>  Compruebe si Ingress está configurado correctamente. </li></ol><br>  Presentación visual: <br><br>  1) Iniciar la búsqueda de problemas debe ser desde abajo.  Primero verifique que las vainas tengan estados de <code>Ready</code> y En <code>Running</code> : <br><br><img src="https://habrastorage.org/webt/f-/lc/iz/f-lcizmfav5sb1sc7hvu8samwes.png"><br><br>  2) Si los pods están <code>Ready</code> , debe averiguar si el servicio distribuye el tráfico entre los pods: <br><br><img src="https://habrastorage.org/webt/yg/we/bu/ygwebumu8ga9lmd7krineuw38mq.png"><br><br>  3) Finalmente, debe analizar la conexión entre el servicio e Ingress: <br><br><img src="https://habrastorage.org/webt/y7/ze/uz/y7zeuzkhzgsdzcjmng2ei4fjrxg.png"><br><br><h2>  1. Diagnóstico de vainas </h2><br>  En la mayoría de los casos, el problema es con la cápsula.  Asegúrese de que las vainas estén <code>Ready</code> y <code>Running</code> .  Puede verificar esto usando el comando: <br><br><pre> <code class="bash hljs">kubectl get pods NAME READY STATUS RESTARTS AGE app1 0/1 ImagePullBackOff 0 47h app2 0/1 Error 0 47h app3-76f9fcd46b-xbv4k 1/1 Running 1 47h</code> </pre> <br>  En el resultado del comando anterior, el último pod aparece como <code>Running</code> y <code>Ready</code> , pero para los otros dos no lo está. <br><br>  <i>¿Cómo entender lo que salió mal?</i> <br><br>  Hay cuatro comandos útiles para diagnosticar pods: <br><br><ol><li>  <code>kubectl logs &lt; pod'&gt;</code> permite extraer registros de contenedores en el pod; </li><li>  <code>kubectl describe pod &lt; pod'&gt;</code> permite ver una lista de eventos asociados con el pod; </li><li>  <code>kubectl get pod &lt; pod'&gt;</code> permite obtener la configuración YAML del <code>kubectl get pod &lt; pod'&gt;</code> almacenado en Kubernetes; </li><li>  <code>kubectl exec -ti &lt; pod'&gt; bash</code> permite ejecutar un shell de comando interactivo en uno de los contenedores de pod </li></ol><br>  <i>¿Cuál elegir?</i> <br><br>  El hecho es que no hay un equipo universal.  Se debe usar una combinación de estos. <br><br><h3>  Problemas comunes de la cápsula </h3><br>  Hay dos tipos principales de errores de pod: errores de inicio y errores de tiempo de ejecución. <br><br>  Errores de inicio: <br><br><ul><li> <code>ImagePullBackoff</code> </li> <li> <code>ImageInspectError</code> </li> <li> <code>ErrImagePull</code> </li> <li> <code>ErrImageNeverPull</code> </li> <li> <code>RegistryUnavailable</code> </li> <li> <code>InvalidImageName</code> </li> </ul><br>  Errores de tiempo de ejecución: <br><br><ul><li> <code>CrashLoopBackOff</code> </li> <li> <code>RunContainerError</code> </li> <li> <code>KillContainerError</code> </li> <li> <code>VerifyNonRootError</code> </li> <li> <code>RunInitContainerError</code> </li> <li> <code>CreatePodSandboxError</code> </li> <li> <code>ConfigPodSandboxError</code> </li> <li> <code>KillPodSandboxError</code> </li> <li> <code>SetupNetworkError</code> </li> <li> <code>TeardownNetworkError</code> </li> </ul><br>  Algunos errores son más comunes que otros.  Aquí hay algunos errores comunes y cómo solucionarlos. <br><br><h4>  ImagePullBackOff </h4><br>  Este error aparece cuando Kubernetes no puede obtener una imagen para uno de los contenedores de pod.  Estas son las tres razones más comunes para esto: <br><br><ol><li>  El nombre de la imagen está incorrectamente especificado; por ejemplo, cometió un error o la imagen no existe; </li><li>  Se especifica una etiqueta inexistente para la imagen; </li><li>  La imagen se almacena en un registro privado y Kubernetes no tiene autoridad para acceder a ella. </li></ol><br>  Las dos primeras razones son fáciles de eliminar: solo arregle el nombre y la etiqueta de la imagen.  En el caso de este último, debe ingresar las credenciales para el registro privado en Secret y agregar enlaces a él en pods.  La documentación de Kubernetes <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">tiene un ejemplo</a> de cómo se puede hacer esto. <br><br><h4>  CrashLoopBackOff </h4><br>  Kubenetes arrojará un error CrashLoopBackOff si el contenedor no puede iniciarse.  Esto generalmente ocurre cuando: <br><br><ol><li>  Hay un error en la aplicación que impide que se inicie; </li><li>  El contenedor está <a href="https://stackoverflow.com/questions/41604499/my-kubernetes-pods-keep-crashing-with-crashloopbackoff-but-i-cant-find-any-lo">configurado incorrectamente</a> ; </li><li>  La prueba de vitalidad falló demasiadas veces. </li></ol><br>  Debe intentar llegar a los registros desde el contenedor para averiguar el motivo de su falla.  Si el acceso a los registros es difícil, porque el contenedor se reinicia demasiado rápido, puede usar el siguiente comando: <br><br><pre> <code class="bash hljs">kubectl logs &lt;pod-name&gt; --previous</code> </pre> <br>  Muestra mensajes de error de una reencarnación de contenedor anterior. <br><br><h4>  RunContainerError </h4><br>  Este error ocurre cuando el contenedor no puede iniciarse.  Corresponde al momento anterior al lanzamiento de la aplicación.  Por lo general, su causa es una configuración incorrecta, por ejemplo: <br><br><ul><li>  Intentar montar un volumen inexistente, como ConfigMap o Secrets; </li><li>  Intente montar un volumen de solo lectura como lectura-escritura. </li></ul><br>  El <code>kubectl describe pod &lt;pod-name&gt;</code> es muy adecuado para analizar tales errores. <br><br><h3>  Vainas pendientes </h3><br>  Después de la creación, la cápsula permanece en el estado <code>Pending</code> . <br><br>  <i>¿Por qué está pasando esto?</i> <br><br>  Estas son las posibles razones (supongo que el planificador funciona bien): <br><br><ol><li>  El clúster no tiene suficientes recursos, como potencia de procesamiento y memoria, para ejecutar el pod. </li><li>  El objeto <code>ResourceQuota</code> se instala en el espacio de nombres correspondiente y crear un pod hará que el espacio de nombres vaya más allá de la cuota. </li><li>  Pod está vinculado a Pending <code>PersistentVolumeClaim</code> pendiente. </li></ol><br>  En este caso, se recomienda utilizar el comando <code>kubectl describe</code> y verificar la sección <code>Events</code> : <br><br><pre> <code class="bash hljs">kubectl describe pod &lt;pod name&gt;</code> </pre> <br>  En caso de errores relacionados con <code>ResourceQuotas</code> , se recomienda ver los registros del clúster con el comando <br><br><pre> <code class="bash hljs">kubectl get events --sort-by=.metadata.creationTimestamp</code> </pre> <br><h3>  Vainas no listas </h3><br>  Si el pod aparece como En <code>Running</code> , pero no está en estado <code>Ready</code> , la <i>sonda de</i> preparación no tiene éxito. <br><br>  Cuando esto sucede, el pod no se conecta al servicio y el tráfico no fluye hacia él.  La prueba de preparación falló debido a problemas de aplicación.  En este caso, para encontrar el error, debe analizar la sección <code>Events</code> en la salida del comando <code>kubectl describe</code> . <br><br><h2>  2. Diagnóstico de los servicios. </h2><br>  Si los pods están listados como <code>Running</code> y <code>Ready</code> , pero aún no hay respuesta de la aplicación, debe verificar la configuración del servicio. <br><br>  Los servicios están involucrados en el enrutamiento del tráfico a los pods según sus etiquetas.  Por lo tanto, lo primero que debe hacer es verificar cuántas unidades funcionan con el servicio.  Para hacer esto, puede verificar los puntos finales en el servicio: <br><br><pre> <code class="bash hljs">kubectl describe service &lt;service-name&gt; | grep Endpoints</code> </pre> <br>  El punto final es un par de valores de la forma <code>&lt;IP-:&gt;</code> , y al menos uno de estos pares debe estar presente en la salida (es decir, al menos un pod funciona con el servicio). <br><br>  Si la sección <code>Endpoins</code> vacía, hay dos opciones posibles: <br><br><ol><li>  no hay pods con la etiqueta correcta (pista: compruebe si el espacio de nombres está seleccionado correctamente); </li><li>  Hay un error en las etiquetas de servicio en el selector. </li></ol><br>  Si ve una lista de puntos finales, pero aún no puede acceder a la aplicación, entonces el probable culpable es el error en <code>targetPort</code> en la descripción del servicio. <br><br>  <i>¿Cómo verificar la capacidad de servicio del servicio?</i> <br><br>  Independientemente del tipo de servicio, puede usar el <code>kubectl port-forward</code> para conectarse a él: <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service-name&gt; 3000:80</code> </pre> <br>  Aquí: <br><br><ul><li>  <code>&lt;service-name&gt;</code> - el nombre del servicio; </li><li>  3000 - el puerto que abres en la computadora; </li><li>  80 - puerto en el lado de servicio. </li></ul><br><h2>  3. Diagnóstico de ingreso </h2><br>  Si lees hasta este lugar, entonces: <br><br><ul><li>  los pods se enumeran como <code>Running</code> y <code>Ready</code> ; </li><li>  El servicio distribuye con éxito el tráfico entre los pods. </li></ul><br>  Sin embargo, aún no puede "comunicarse" con la aplicación. <br><br>  Esto significa que, muy probablemente, el controlador Ingress está configurado incorrectamente.  Dado que el controlador Ingress es un componente de terceros en el clúster, existen varios métodos de depuración según su tipo. <br><br>  Pero antes de recurrir a herramientas especiales para configurar Ingress, puede hacer algo muy simple.  Ingress usa <code>serviceName</code> y <code>servicePort</code> para conectarse al servicio.  Debe verificar que estén configurados correctamente.  Puedes hacer esto usando el comando: <br><br><pre> <code class="bash hljs">kubectl describe ingress &lt;ingress-name&gt;</code> </pre> <br>  Si la columna <code>Backend</code> está vacía, existe una alta probabilidad de un error de configuración.  Si los backends están en su lugar, pero todavía no hay acceso a la aplicación, entonces el problema puede estar relacionado con: <br><br><ul><li>  Configuración de accesibilidad de entrada desde Internet público; </li><li>  configuración de accesibilidad del clúster desde Internet público. </li></ul><br>  Puede identificar problemas de infraestructura conectándose directamente al pod de Ingress.  Para hacer esto, primero encuentre el pod del controlador Ingress (puede estar en un espacio de nombres diferente): <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Use el comando <code>describe</code> para configurar el puerto: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc --namespace kube-system \ | grep Ports</code> </pre> <br>  Finalmente, conéctese al pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Ahora todas las solicitudes para el puerto 3000 en la computadora serán redirigidas al puerto 80 pod. <br><br>  <i>¿Funciona ahora?</i> <br><br><ul><li>  Si es así, entonces el problema es con la infraestructura.  Es necesario averiguar exactamente cómo se enruta el tráfico al clúster. </li><li>  De lo contrario, el problema está en el controlador de Ingress. </li></ul><br>  Si no puede hacer que el controlador Ingress funcione, deberá depurarlo. <br><br>  Hay muchas variedades de controladores de entrada.  Los más populares son Nginx, HAProxy, Traefik, etc. <i>(para obtener más información sobre las soluciones existentes, consulte <a href="https://habr.com/ru/company/flant/blog/447180/">nuestra revisión</a> , aprox. Transl.)</i> Debe usar la guía de solución de problemas en la documentación del controlador correspondiente.  Dado que <a href="https://github.com/kubernetes/ingress-nginx">Ingress Nginx</a> es el controlador de Ingress más popular, hemos incluido algunos consejos para resolver problemas relacionados en este artículo. <br><br><h3>  Depuración de un controlador Nginx de Ingress </h3><br><br>  El proyecto Ingress-nginx tiene un <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">complemento</a> oficial <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">para kubectl</a> .  El <code>kubectl ingress-nginx</code> se puede usar para: <br><br><ul><li>  análisis de registros, backends, certificados, etc. </li><li>  conexión a Ingress; </li><li>  estudiando la configuración actual. </li></ul><br>  Los siguientes tres equipos te ayudarán con esto: <br><br><ul><li>  <code>kubectl ingress-nginx lint</code> - comprueba <code>nginx.conf</code> ; </li><li>  <code>kubectl ingress-nginx backend</code> : examina el backend (similar a <code>kubectl describe ingress &lt;ingress-name&gt;</code> ); </li><li>  <code>kubectl ingress-nginx logs</code> : comprueba los registros. </li></ul><br>  Tenga en cuenta que en algunos casos puede ser necesario especificar el espacio de nombres correcto para el controlador Ingress usando el <code>--namespace &lt;name&gt;</code> . <br><br><h2>  Resumen </h2><br>  Diagnosticar Kubernetes puede ser una tarea desalentadora si no sabes por dónde empezar.  El problema siempre debe abordarse de acuerdo con el principio de abajo hacia arriba: comience con pods y luego vaya al servicio e Ingress.  Los métodos de depuración descritos en el artículo se pueden aplicar a otros objetos, como: <br><br><ul><li>  empleos inactivos y CronJobs; </li><li>  StatefulSets y DaemonSets. </li></ul><br>  Gracias a <a href="https://github.com/errge">Gergely Risko</a> , <a href="https://medium.com/%40weibeld">Daniel Weibel</a> y <a href="https://www.linkedin.com/in/charles-christyraj-0bab8a36/">Charles Christyraj</a> por sus valiosos comentarios y adiciones. <br><br><h2>  PD del traductor </h2><br>  Lea también en nuestro blog: <br><br><ul><li>  " <a href="https://habr.com/ru/company/flant/blog/436112/">Complemento Kubectl-debug para depurar en los pods de Kubernetes</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/443458/">6 errores de sistema entretenidos en el funcionamiento de Kubernetes [y su solución]</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/462707/">Herramientas para desarrolladores de aplicaciones que se ejecutan en Kubernetes</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/471892/">6 historias prácticas de nuestra vida cotidiana SRE</a> ". </li></ul></div></div><p>Source: <a href="https://habr.com/ru/post/484954/">https://habr.com/ru/post/484954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../484936/index.html">Conocimientos y competencias en el equipo: encontrar, ver, bombear</a></li>
<li><a href="../484944/index.html">¿Qué estoy en ACID o no nos conviene?</a></li>
<li><a href="../484946/index.html">Modelado GPR</a></li>
<li><a href="../484948/index.html">NEC lanzó un cable submarino con un récord de 20 pares de fibras ópticas</a></li>
<li><a href="../484952/index.html">Sustitución de Redux con observables y ganchos de reacción</a></li>
<li><a href="../484964/index.html">Configurar el equilibrio de carga en InfoWatch Traffic Monitor</a></li>
<li><a href="../484966/index.html">Plantilla lista para usar con Spring</a></li>
<li><a href="../484968/index.html">WPF DataGrid. Lucha por la plantilla</a></li>
<li><a href="../484972/index.html">Wine 5.0 lanzado</a></li>
<li><a href="../484974/index.html">Wang Tiles para la simulación de máquinas de Turing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>