<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèΩ‚Äçüíª üö≠ üéóÔ∏è Gu√≠a visual de soluci√≥n de problemas para Kubernetes ü§ñ üïú ‚ôâÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nota perev. : Este art√≠culo es parte de los materiales disponibles gratuitamente del proyecto learnk8s , que ense√±a a las empresas y administradores i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gu√≠a visual de soluci√≥n de problemas para Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/484954/">  <i><b>Nota</b></i>  <i><b>perev.</b></i>  <i>: Este art√≠culo es parte de los materiales disponibles gratuitamente del proyecto <a href="https://learnk8s.io/">learnk8s</a> , que ense√±a a las empresas y administradores individuales c√≥mo trabajar con Kubernetes.</i>  <i>En √©l, Daniele Polencic, el gerente del proyecto, comparte una instrucci√≥n clara sobre qu√© pasos tomar en caso de problemas generales para las aplicaciones que se ejecutan en el cl√∫ster K8.</i> <br><br><img src="https://habrastorage.org/webt/ch/5u/xa/ch5uxanj-3ivwqu88swqyoi6bsu.png"><br><br>  TL; DR: aqu√≠ hay un diagrama que lo ayudar√° a depurar la implementaci√≥n en Kubernetes: <a name="habracut"></a><br><br> <a href=""><img src="https://habrastorage.org/webt/4r/qp/si/4rqpsie8dnplkqxahaqntpi2ssw.png"></a> <br><br>  <i>Diagrama de flujo para encontrar y corregir errores en un cl√∫ster.</i>  <i>En el original (en ingl√©s) est√° disponible en <a href="https://learnk8s.io/a/troubleshooting-kubernetes.pdf">PDF</a> y <a href="">como imagen</a> .</i> <br><br>  Al implementar una aplicaci√≥n en Kubernetes, generalmente necesita definir tres componentes: <br><br><ul><li>  <b>La implementaci√≥n</b> es una receta para crear copias de una aplicaci√≥n llamada pods; </li><li>  <b>Servicio</b> : un equilibrador de carga interno que distribuye el tr√°fico entre los pods; </li><li>  <b>Ingreso</b> : una descripci√≥n de c√≥mo fluir√° el tr√°fico del mundo exterior al Servicio. </li></ul><br>  Aqu√≠ hay un breve resumen gr√°fico: <br><br>  1) En Kubernetes, las aplicaciones reciben tr√°fico del mundo exterior a trav√©s de dos capas de equilibradores de carga: internos y externos. <br><br><img src="https://habrastorage.org/webt/3v/cy/z9/3vcyz9a-2ciiqbh9he7idgvo7uy.png"><br><br>  2) El equilibrador interno se llama Servicio, el externo - Ingreso. <br><br><img src="https://habrastorage.org/webt/23/mn/zc/23mnzcfo_b3niccdivn4bc4vzei.png"><br><br>  3) La implementaci√≥n crea pods y los monitorea (no se crean manualmente). <br><br><img src="https://habrastorage.org/webt/4j/c2/h9/4jc2h9pgzbxmkon4ewkbeuf0vhc.png"><br><br>  Supongamos que desea implementar una aplicaci√≥n simple a la <i>Hello World</i> .  La configuraci√≥n de YAML para esto se ver√° as√≠: <br><br><pre><code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment # &lt;&lt;&lt; metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: any-name: my-app spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service # &lt;&lt;&lt; metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 selector: name: app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress # &lt;&lt;&lt; metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: app servicePort: 80 path: /</code> </pre> <br>  La definici√≥n es bastante larga y es f√°cil confundirse acerca de c√≥mo se relacionan los componentes entre s√≠. <br><br>  Por ejemplo: <br><br><ul><li>  ¬øCu√°ndo deber√≠a usar el puerto 80 y cu√°ndo - 8080? </li><li>  ¬øDebo crear un nuevo puerto para cada servicio para que no entren en conflicto? </li><li>  ¬øImportan los nombres de las etiquetas?  ¬øDeber√≠an ser iguales en todas partes? </li></ul><br>  Antes de centrarnos en la depuraci√≥n, recordemos c√≥mo los tres componentes est√°n relacionados entre s√≠.  Comencemos con la implementaci√≥n y el servicio. <br><br><h2>  Implementaci√≥n de conexi√≥n'a y Servicio'a </h2><br>  Se sorprender√°, pero las implementaciones y el servicio no est√°n conectados de ninguna manera.  En cambio, el Servicio apunta directamente a Pods sin pasar por la Implementaci√≥n. <br><br>  Por lo tanto, estamos interesados ‚Äã‚Äãen c√≥mo se relacionan los Pods y los Servicios entre s√≠.  Tres cosas para recordar: <br><br><ol><li>  Un <code>selector</code> servicio debe coincidir con al menos una etiqueta Pod. </li><li>  <code>targetPort</code> debe coincidir con el <code>containerPort</code> contenedor dentro del Pod. </li><li>  <code>port</code> servicio <code>port</code> puede ser cualquier cosa.  Diferentes servicios pueden usar el mismo puerto porque tienen diferentes direcciones IP. </li></ol><br>  El siguiente diagrama representa todo lo anterior en forma gr√°fica: <br><br>  1) Imagine que el servicio dirige el tr√°fico a un determinado pod: <br><br><img src="https://habrastorage.org/webt/2a/e5/8f/2ae58fcgoi7aifmcr5rl_0bseym.png"><br><br>  2) Al crear un pod, debe especificar <code>containerPort</code> para cada contenedor en los pods: <br><br><img src="https://habrastorage.org/webt/xc/fa/ow/xcfaowomhbtqhebhodgzhzrkupc.png"><br><br>  3) Al crear el servicio, debe especificar <code>port</code> y <code>targetPort</code> .  <i>¬øPero por cu√°l se conecta al contenedor?</i> <br><br><img src="https://habrastorage.org/webt/vg/wj/nd/vgwjnde0xyzdblwamomfxjaxb40.png"><br><br>  4) A trav√©s de <code>targetPort</code> .  Debe coincidir con <code>containerPort</code> . <br><br><img src="https://habrastorage.org/webt/q4/yx/qn/q4yxqnkxxilupalikahmqqp09x8.png"><br><br>  5) Digamos que el puerto 3000 est√° abierto en el contenedor, entonces el valor <code>targetPort</code> deber√≠a ser el mismo. <br><br><img src="https://habrastorage.org/webt/cq/tj/-s/cqtj-srznih70qh7bxs3w_l7bis.png"><br><br>  En el archivo YAML, las etiquetas y los <code>ports</code> / <code>targetPort</code> deben coincidir: <br><br><pre> <code class="plaintext hljs">apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment labels: track: canary spec: selector: matchLabels: any-name: my-app template: metadata: labels: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt; spec: containers: - name: cont1 image: learnk8s/app:1.0.0 ports: - containerPort: 8080 # &lt;&lt;&lt; --- apiVersion: v1 kind: Service metadata: name: my-service spec: ports: - port: 80 targetPort: 8080 # &lt;&lt;&lt; selector: # &lt;&lt;&lt; any-name: my-app # &lt;&lt;&lt;</code> </pre> <br>  <i>¬øQu√© pasa con la <code>track: canary</code> en la parte superior de la secci√≥n Implementaci√≥n?</i>  <i>¬øDeber√≠a coincidir?</i> <br><br>  Esta etiqueta se refiere a la implementaci√≥n y el servicio no la utiliza para enrutar el tr√°fico.  En otras palabras, se puede eliminar o asignar un valor diferente. <br><br>  <i>¬øQu√© pasa con el selector de <code>matchLabels</code> ?</i> <br><br>  <b>Siempre debe coincidir con las etiquetas de Pod</b> , ya que Deployment lo utiliza para rastrear pods. <br><br>  <i>Supongamos que hiciste las ediciones correctas.</i>  <i>¬øC√≥mo verificarlos?</i> <br><br>  Puede verificar la etiqueta del pod con el siguiente comando: <br><br><pre> <code class="bash hljs">kubectl get pods --show-labels</code> </pre> <br>  O, si los pods pertenecen a varias aplicaciones: <br><br><pre> <code class="bash hljs">kubectl get pods --selector any-name=my-app --show-labels</code> </pre> <br>  Donde <code>any-name=my-app</code> es la etiqueta <code>any-name: my-app</code> . <br><br>  <i>¬øHay alguna dificultad?</i> <br><br>  ¬°Puedes conectarte al pod!  Para hacer esto, use el comando <code>port-forward</code> en kubectl.  Le permite conectarse al servicio y verificar la conexi√≥n. <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service name&gt; 3000:80</code> </pre> <br>  Aqu√≠: <br><br><ul><li>  <code>service/&lt;service name&gt;</code> - nombre del servicio;  en nuestro caso es <code>my-service</code> ; </li><li>  3000: el puerto que desea abrir en la computadora; </li><li>  80: puerto especificado en el campo de <code>port</code> del servicio. </li></ul><br>  Si pudo establecer una conexi√≥n, la configuraci√≥n es correcta. <br><br>  Si no se pudo establecer la conexi√≥n, entonces hay un problema con las etiquetas o los puertos no coinciden. <br><br><h2>  Conexi√≥n de servicio e ingreso </h2><br>  El siguiente paso para proporcionar acceso a la aplicaci√≥n est√° relacionado con la configuraci√≥n de Ingress.  Ingress debe saber c√≥mo encontrar el servicio, luego encontrar los pods y dirigir el tr√°fico hacia ellos.  Ingress encuentra el servicio deseado por nombre y puerto abierto. <br><br>  En la descripci√≥n de Ingress y Service, deben coincidir dos par√°metros: <br><br><ol><li>  <code>servicePort</code> in Ingress debe coincidir con el par√°metro de <code>port</code> en Service; </li><li>  <code>serviceName</code> in Ingress debe coincidir con el campo de <code>name</code> en Service. </li></ol><br>  El siguiente diagrama resume la conexi√≥n de los puertos: <br><br>  1) Como ya sabe, el Servicio escucha en un determinado <code>port</code> : <br><br><img src="https://habrastorage.org/webt/9q/t0/fz/9qt0fzsyme9mnrd4ki07ezamnkg.png"><br><br>  2) Ingress tiene un par√°metro llamado <code>servicePort</code> : <br><br><img src="https://habrastorage.org/webt/rn/du/yw/rnduyw4xvfmpjmhup8fy9ao2d1a.png"><br><br>  3) Este par√°metro ( <code>servicePort</code> ) siempre debe coincidir con el <code>port</code> en la definici√≥n del Servicio: <br><br><img src="https://habrastorage.org/webt/1d/ap/ty/1daptyulxphnbb2dt6uben-lnzk.png"><br><br>  4) Si el puerto 80 se especifica en Servicio, <code>servicePort</code> tambi√©n debe ser 80: <br><br><img src="https://habrastorage.org/webt/nc/mi/dl/ncmidlxiegmtmhtozaxxqhznaya.png"><br><br>  En la pr√°ctica, debe prestar atenci√≥n a las siguientes l√≠neas: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: name: my-service # &lt;&lt;&lt; spec: ports: - port: 80 # &lt;&lt;&lt; targetPort: 8080 selector: any-name: my-app --- apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: my-ingress spec: rules: - http: paths: - backend: serviceName: my-service # &lt;&lt;&lt; servicePort: 80 # &lt;&lt;&lt; path: /</code> </pre> <br>  <i>¬øC√≥mo verificar si Ingress est√° funcionando?</i> <br><br>  Puede usar el m√©todo con <code>kubectl port-forward</code> , pero en lugar del servicio necesita conectarse al controlador Ingress. <br><br>  Primero debe encontrar el nombre del pod con el controlador Ingress: <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Localice el pod de Ingress (puede referirse a un espacio de nombres diferente) y ejecute el comando <code>describe</code> para averiguar los n√∫meros de puerto: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc \ --namespace kube-system \ | grep Ports Ports: 80/TCP, 443/TCP, 18080/TCP</code> </pre> <br>  Finalmente, con√©ctese al pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Ahora, cada vez que env√≠e una solicitud al puerto 3000 en la computadora, ser√° redirigida al puerto 80 del pod con el controlador Ingress.  Al ir a <a href="http://localhost:3000/">http: // localhost: 3000</a> , deber√≠a ver la p√°gina creada por la aplicaci√≥n. <br><br><h2>  Resumen de puerto </h2><br>  Recordemos nuevamente qu√© puertos y etiquetas deben coincidir: <br><br><ol><li>  El selector en la definici√≥n del Servicio debe coincidir con la etiqueta del pod; </li><li>  <code>targetPort</code> en la definici√≥n del Servicio debe coincidir con el <code>containerPort</code> contenedor dentro del pod; </li><li>  <code>port</code> en la definici√≥n de Servicio puede ser cualquier cosa.  Diferentes servicios pueden usar el mismo puerto porque tienen diferentes direcciones IP; </li><li>  <code>servicePort</code> Ingress debe coincidir con el <code>port</code> en la definici√≥n del Servicio; </li><li>  El nombre del servicio debe coincidir con el campo <code>serviceName</code> en Ingress. </li></ol><br>  Por desgracia, no es suficiente saber c√≥mo estructurar adecuadamente su configuraci√≥n YAML. <br><br>  <i>¬øQu√© pasa cuando algo sale mal?</i> <br><br>  Quiz√°s la c√°psula no se inicia o se bloquea. <br><br><h2>  3 pasos para solucionar fallas de aplicaciones en Kubernetes </h2><br>  Antes de depurar una implementaci√≥n, debe tener una buena comprensi√≥n de c√≥mo funciona Kubernetes. <br><br>  Dado que hay tres componentes en cada aplicaci√≥n descargada a K8, se deben depurar en un cierto orden, comenzando desde abajo. <br><br><ol><li>  Primero debes asegurarte de que las vainas funcionan, luego ... </li><li>  Compruebe si el servicio entrega tr√°fico a los pods y luego ... </li><li>  Compruebe si Ingress est√° configurado correctamente. </li></ol><br>  Presentaci√≥n visual: <br><br>  1) Iniciar la b√∫squeda de problemas debe ser desde abajo.  Primero verifique que las vainas tengan estados de <code>Ready</code> y En <code>Running</code> : <br><br><img src="https://habrastorage.org/webt/f-/lc/iz/f-lcizmfav5sb1sc7hvu8samwes.png"><br><br>  2) Si los pods est√°n <code>Ready</code> , debe averiguar si el servicio distribuye el tr√°fico entre los pods: <br><br><img src="https://habrastorage.org/webt/yg/we/bu/ygwebumu8ga9lmd7krineuw38mq.png"><br><br>  3) Finalmente, debe analizar la conexi√≥n entre el servicio e Ingress: <br><br><img src="https://habrastorage.org/webt/y7/ze/uz/y7zeuzkhzgsdzcjmng2ei4fjrxg.png"><br><br><h2>  1. Diagn√≥stico de vainas </h2><br>  En la mayor√≠a de los casos, el problema es con la c√°psula.  Aseg√∫rese de que las vainas est√©n <code>Ready</code> y <code>Running</code> .  Puede verificar esto usando el comando: <br><br><pre> <code class="bash hljs">kubectl get pods NAME READY STATUS RESTARTS AGE app1 0/1 ImagePullBackOff 0 47h app2 0/1 Error 0 47h app3-76f9fcd46b-xbv4k 1/1 Running 1 47h</code> </pre> <br>  En el resultado del comando anterior, el √∫ltimo pod aparece como <code>Running</code> y <code>Ready</code> , pero para los otros dos no lo est√°. <br><br>  <i>¬øC√≥mo entender lo que sali√≥ mal?</i> <br><br>  Hay cuatro comandos √∫tiles para diagnosticar pods: <br><br><ol><li>  <code>kubectl logs &lt; pod'&gt;</code> permite extraer registros de contenedores en el pod; </li><li>  <code>kubectl describe pod &lt; pod'&gt;</code> permite ver una lista de eventos asociados con el pod; </li><li>  <code>kubectl get pod &lt; pod'&gt;</code> permite obtener la configuraci√≥n YAML del <code>kubectl get pod &lt; pod'&gt;</code> almacenado en Kubernetes; </li><li>  <code>kubectl exec -ti &lt; pod'&gt; bash</code> permite ejecutar un shell de comando interactivo en uno de los contenedores de pod </li></ol><br>  <i>¬øCu√°l elegir?</i> <br><br>  El hecho es que no hay un equipo universal.  Se debe usar una combinaci√≥n de estos. <br><br><h3>  Problemas comunes de la c√°psula </h3><br>  Hay dos tipos principales de errores de pod: errores de inicio y errores de tiempo de ejecuci√≥n. <br><br>  Errores de inicio: <br><br><ul><li> <code>ImagePullBackoff</code> </li> <li> <code>ImageInspectError</code> </li> <li> <code>ErrImagePull</code> </li> <li> <code>ErrImageNeverPull</code> </li> <li> <code>RegistryUnavailable</code> </li> <li> <code>InvalidImageName</code> </li> </ul><br>  Errores de tiempo de ejecuci√≥n: <br><br><ul><li> <code>CrashLoopBackOff</code> </li> <li> <code>RunContainerError</code> </li> <li> <code>KillContainerError</code> </li> <li> <code>VerifyNonRootError</code> </li> <li> <code>RunInitContainerError</code> </li> <li> <code>CreatePodSandboxError</code> </li> <li> <code>ConfigPodSandboxError</code> </li> <li> <code>KillPodSandboxError</code> </li> <li> <code>SetupNetworkError</code> </li> <li> <code>TeardownNetworkError</code> </li> </ul><br>  Algunos errores son m√°s comunes que otros.  Aqu√≠ hay algunos errores comunes y c√≥mo solucionarlos. <br><br><h4>  ImagePullBackOff </h4><br>  Este error aparece cuando Kubernetes no puede obtener una imagen para uno de los contenedores de pod.  Estas son las tres razones m√°s comunes para esto: <br><br><ol><li>  El nombre de la imagen est√° incorrectamente especificado; por ejemplo, cometi√≥ un error o la imagen no existe; </li><li>  Se especifica una etiqueta inexistente para la imagen; </li><li>  La imagen se almacena en un registro privado y Kubernetes no tiene autoridad para acceder a ella. </li></ol><br>  Las dos primeras razones son f√°ciles de eliminar: solo arregle el nombre y la etiqueta de la imagen.  En el caso de este √∫ltimo, debe ingresar las credenciales para el registro privado en Secret y agregar enlaces a √©l en pods.  La documentaci√≥n de Kubernetes <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">tiene un ejemplo</a> de c√≥mo se puede hacer esto. <br><br><h4>  CrashLoopBackOff </h4><br>  Kubenetes arrojar√° un error CrashLoopBackOff si el contenedor no puede iniciarse.  Esto generalmente ocurre cuando: <br><br><ol><li>  Hay un error en la aplicaci√≥n que impide que se inicie; </li><li>  El contenedor est√° <a href="https://stackoverflow.com/questions/41604499/my-kubernetes-pods-keep-crashing-with-crashloopbackoff-but-i-cant-find-any-lo">configurado incorrectamente</a> ; </li><li>  La prueba de vitalidad fall√≥ demasiadas veces. </li></ol><br>  Debe intentar llegar a los registros desde el contenedor para averiguar el motivo de su falla.  Si el acceso a los registros es dif√≠cil, porque el contenedor se reinicia demasiado r√°pido, puede usar el siguiente comando: <br><br><pre> <code class="bash hljs">kubectl logs &lt;pod-name&gt; --previous</code> </pre> <br>  Muestra mensajes de error de una reencarnaci√≥n de contenedor anterior. <br><br><h4>  RunContainerError </h4><br>  Este error ocurre cuando el contenedor no puede iniciarse.  Corresponde al momento anterior al lanzamiento de la aplicaci√≥n.  Por lo general, su causa es una configuraci√≥n incorrecta, por ejemplo: <br><br><ul><li>  Intentar montar un volumen inexistente, como ConfigMap o Secrets; </li><li>  Intente montar un volumen de solo lectura como lectura-escritura. </li></ul><br>  El <code>kubectl describe pod &lt;pod-name&gt;</code> es muy adecuado para analizar tales errores. <br><br><h3>  Vainas pendientes </h3><br>  Despu√©s de la creaci√≥n, la c√°psula permanece en el estado <code>Pending</code> . <br><br>  <i>¬øPor qu√© est√° pasando esto?</i> <br><br>  Estas son las posibles razones (supongo que el planificador funciona bien): <br><br><ol><li>  El cl√∫ster no tiene suficientes recursos, como potencia de procesamiento y memoria, para ejecutar el pod. </li><li>  El objeto <code>ResourceQuota</code> se instala en el espacio de nombres correspondiente y crear un pod har√° que el espacio de nombres vaya m√°s all√° de la cuota. </li><li>  Pod est√° vinculado a Pending <code>PersistentVolumeClaim</code> pendiente. </li></ol><br>  En este caso, se recomienda utilizar el comando <code>kubectl describe</code> y verificar la secci√≥n <code>Events</code> : <br><br><pre> <code class="bash hljs">kubectl describe pod &lt;pod name&gt;</code> </pre> <br>  En caso de errores relacionados con <code>ResourceQuotas</code> , se recomienda ver los registros del cl√∫ster con el comando <br><br><pre> <code class="bash hljs">kubectl get events --sort-by=.metadata.creationTimestamp</code> </pre> <br><h3>  Vainas no listas </h3><br>  Si el pod aparece como En <code>Running</code> , pero no est√° en estado <code>Ready</code> , la <i>sonda de</i> preparaci√≥n no tiene √©xito. <br><br>  Cuando esto sucede, el pod no se conecta al servicio y el tr√°fico no fluye hacia √©l.  La prueba de preparaci√≥n fall√≥ debido a problemas de aplicaci√≥n.  En este caso, para encontrar el error, debe analizar la secci√≥n <code>Events</code> en la salida del comando <code>kubectl describe</code> . <br><br><h2>  2. Diagn√≥stico de los servicios. </h2><br>  Si los pods est√°n listados como <code>Running</code> y <code>Ready</code> , pero a√∫n no hay respuesta de la aplicaci√≥n, debe verificar la configuraci√≥n del servicio. <br><br>  Los servicios est√°n involucrados en el enrutamiento del tr√°fico a los pods seg√∫n sus etiquetas.  Por lo tanto, lo primero que debe hacer es verificar cu√°ntas unidades funcionan con el servicio.  Para hacer esto, puede verificar los puntos finales en el servicio: <br><br><pre> <code class="bash hljs">kubectl describe service &lt;service-name&gt; | grep Endpoints</code> </pre> <br>  El punto final es un par de valores de la forma <code>&lt;IP-:&gt;</code> , y al menos uno de estos pares debe estar presente en la salida (es decir, al menos un pod funciona con el servicio). <br><br>  Si la secci√≥n <code>Endpoins</code> vac√≠a, hay dos opciones posibles: <br><br><ol><li>  no hay pods con la etiqueta correcta (pista: compruebe si el espacio de nombres est√° seleccionado correctamente); </li><li>  Hay un error en las etiquetas de servicio en el selector. </li></ol><br>  Si ve una lista de puntos finales, pero a√∫n no puede acceder a la aplicaci√≥n, entonces el probable culpable es el error en <code>targetPort</code> en la descripci√≥n del servicio. <br><br>  <i>¬øC√≥mo verificar la capacidad de servicio del servicio?</i> <br><br>  Independientemente del tipo de servicio, puede usar el <code>kubectl port-forward</code> para conectarse a √©l: <br><br><pre> <code class="bash hljs">kubectl port-forward service/&lt;service-name&gt; 3000:80</code> </pre> <br>  Aqu√≠: <br><br><ul><li>  <code>&lt;service-name&gt;</code> - el nombre del servicio; </li><li>  3000 - el puerto que abres en la computadora; </li><li>  80 - puerto en el lado de servicio. </li></ul><br><h2>  3. Diagn√≥stico de ingreso </h2><br>  Si lees hasta este lugar, entonces: <br><br><ul><li>  los pods se enumeran como <code>Running</code> y <code>Ready</code> ; </li><li>  El servicio distribuye con √©xito el tr√°fico entre los pods. </li></ul><br>  Sin embargo, a√∫n no puede "comunicarse" con la aplicaci√≥n. <br><br>  Esto significa que, muy probablemente, el controlador Ingress est√° configurado incorrectamente.  Dado que el controlador Ingress es un componente de terceros en el cl√∫ster, existen varios m√©todos de depuraci√≥n seg√∫n su tipo. <br><br>  Pero antes de recurrir a herramientas especiales para configurar Ingress, puede hacer algo muy simple.  Ingress usa <code>serviceName</code> y <code>servicePort</code> para conectarse al servicio.  Debe verificar que est√©n configurados correctamente.  Puedes hacer esto usando el comando: <br><br><pre> <code class="bash hljs">kubectl describe ingress &lt;ingress-name&gt;</code> </pre> <br>  Si la columna <code>Backend</code> est√° vac√≠a, existe una alta probabilidad de un error de configuraci√≥n.  Si los backends est√°n en su lugar, pero todav√≠a no hay acceso a la aplicaci√≥n, entonces el problema puede estar relacionado con: <br><br><ul><li>  Configuraci√≥n de accesibilidad de entrada desde Internet p√∫blico; </li><li>  configuraci√≥n de accesibilidad del cl√∫ster desde Internet p√∫blico. </li></ul><br>  Puede identificar problemas de infraestructura conect√°ndose directamente al pod de Ingress.  Para hacer esto, primero encuentre el pod del controlador Ingress (puede estar en un espacio de nombres diferente): <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS kube-system coredns-5644d7b6d9-jn7cq 1/1 Running kube-system etcd-minikube 1/1 Running kube-system kube-apiserver-minikube 1/1 Running kube-system kube-controller-manager-minikube 1/1 Running kube-system kube-proxy-zvf2h 1/1 Running kube-system kube-scheduler-minikube 1/1 Running kube-system nginx-ingress-controller-6fc5bcc 1/1 Running</code> </pre> <br>  Use el comando <code>describe</code> para configurar el puerto: <br><br><pre> <code class="bash hljs">kubectl describe pod nginx-ingress-controller-6fc5bcc --namespace kube-system \ | grep Ports</code> </pre> <br>  Finalmente, con√©ctese al pod: <br><br><pre> <code class="bash hljs">kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system</code> </pre> <br>  Ahora todas las solicitudes para el puerto 3000 en la computadora ser√°n redirigidas al puerto 80 pod. <br><br>  <i>¬øFunciona ahora?</i> <br><br><ul><li>  Si es as√≠, entonces el problema es con la infraestructura.  Es necesario averiguar exactamente c√≥mo se enruta el tr√°fico al cl√∫ster. </li><li>  De lo contrario, el problema est√° en el controlador de Ingress. </li></ul><br>  Si no puede hacer que el controlador Ingress funcione, deber√° depurarlo. <br><br>  Hay muchas variedades de controladores de entrada.  Los m√°s populares son Nginx, HAProxy, Traefik, etc. <i>(para obtener m√°s informaci√≥n sobre las soluciones existentes, consulte <a href="https://habr.com/ru/company/flant/blog/447180/">nuestra revisi√≥n</a> , aprox. Transl.)</i> Debe usar la gu√≠a de soluci√≥n de problemas en la documentaci√≥n del controlador correspondiente.  Dado que <a href="https://github.com/kubernetes/ingress-nginx">Ingress Nginx</a> es el controlador de Ingress m√°s popular, hemos incluido algunos consejos para resolver problemas relacionados en este art√≠culo. <br><br><h3>  Depuraci√≥n de un controlador Nginx de Ingress </h3><br><br>  El proyecto Ingress-nginx tiene un <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">complemento</a> oficial <a href="https://kubernetes.github.io/ingress-nginx/kubectl-plugin/">para kubectl</a> .  El <code>kubectl ingress-nginx</code> se puede usar para: <br><br><ul><li>  an√°lisis de registros, backends, certificados, etc. </li><li>  conexi√≥n a Ingress; </li><li>  estudiando la configuraci√≥n actual. </li></ul><br>  Los siguientes tres equipos te ayudar√°n con esto: <br><br><ul><li>  <code>kubectl ingress-nginx lint</code> - comprueba <code>nginx.conf</code> ; </li><li>  <code>kubectl ingress-nginx backend</code> : examina el backend (similar a <code>kubectl describe ingress &lt;ingress-name&gt;</code> ); </li><li>  <code>kubectl ingress-nginx logs</code> : comprueba los registros. </li></ul><br>  Tenga en cuenta que en algunos casos puede ser necesario especificar el espacio de nombres correcto para el controlador Ingress usando el <code>--namespace &lt;name&gt;</code> . <br><br><h2>  Resumen </h2><br>  Diagnosticar Kubernetes puede ser una tarea desalentadora si no sabes por d√≥nde empezar.  El problema siempre debe abordarse de acuerdo con el principio de abajo hacia arriba: comience con pods y luego vaya al servicio e Ingress.  Los m√©todos de depuraci√≥n descritos en el art√≠culo se pueden aplicar a otros objetos, como: <br><br><ul><li>  empleos inactivos y CronJobs; </li><li>  StatefulSets y DaemonSets. </li></ul><br>  Gracias a <a href="https://github.com/errge">Gergely Risko</a> , <a href="https://medium.com/%40weibeld">Daniel Weibel</a> y <a href="https://www.linkedin.com/in/charles-christyraj-0bab8a36/">Charles Christyraj</a> por sus valiosos comentarios y adiciones. <br><br><h2>  PD del traductor </h2><br>  Lea tambi√©n en nuestro blog: <br><br><ul><li>  " <a href="https://habr.com/ru/company/flant/blog/436112/">Complemento Kubectl-debug para depurar en los pods de Kubernetes</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/443458/">6 errores de sistema entretenidos en el funcionamiento de Kubernetes [y su soluci√≥n]</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/462707/">Herramientas para desarrolladores de aplicaciones que se ejecutan en Kubernetes</a> "; </li><li>  " <a href="https://habr.com/ru/company/flant/blog/471892/">6 historias pr√°cticas de nuestra vida cotidiana SRE</a> ". </li></ul></div></div><p>Source: <a href="https://habr.com/ru/post/484954/">https://habr.com/ru/post/484954/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../484936/index.html">Conocimientos y competencias en el equipo: encontrar, ver, bombear</a></li>
<li><a href="../484944/index.html">¬øQu√© estoy en ACID o no nos conviene?</a></li>
<li><a href="../484946/index.html">Modelado GPR</a></li>
<li><a href="../484948/index.html">NEC lanz√≥ un cable submarino con un r√©cord de 20 pares de fibras √≥pticas</a></li>
<li><a href="../484952/index.html">Sustituci√≥n de Redux con observables y ganchos de reacci√≥n</a></li>
<li><a href="../484964/index.html">Configurar el equilibrio de carga en InfoWatch Traffic Monitor</a></li>
<li><a href="../484966/index.html">Plantilla lista para usar con Spring</a></li>
<li><a href="../484968/index.html">WPF DataGrid. Lucha por la plantilla</a></li>
<li><a href="../484972/index.html">Wine 5.0 lanzado</a></li>
<li><a href="../484974/index.html">Wang Tiles para la simulaci√≥n de m√°quinas de Turing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>