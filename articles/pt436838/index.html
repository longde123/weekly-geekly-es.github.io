<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèæ‚Äçüíº üåÅ ü§üüèº Entendendo redes neurais convolucionais atrav√©s de visualiza√ß√µes no PyTorch ‚ÄºÔ∏è üôã üôÇ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em nossa era, as m√°quinas alcan√ßaram 99% de precis√£o na compreens√£o e defini√ß√£o de recursos e objetos nas imagens. Encontramos isso todos os dias, por...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Entendendo redes neurais convolucionais atrav√©s de visualiza√ß√µes no PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/">  Em nossa era, as m√°quinas alcan√ßaram 99% de precis√£o na compreens√£o e defini√ß√£o de recursos e objetos nas imagens.  Encontramos isso todos os dias, por exemplo: reconhecimento de rosto na c√¢mera do smartphone, capacidade de procurar fotos no google, digitalizar texto de um c√≥digo de barras ou livros a uma boa velocidade etc. Essa efici√™ncia da m√°quina √© poss√≠vel gra√ßas a um tipo especial de rede neural chamada convolucional neural a rede.  Se voc√™ √© um entusiasta do aprendizado profundo, provavelmente j√° ouviu falar sobre isso e pode desenvolver v√°rios classificadores de imagem.  As estruturas modernas de aprendizado profundo, como Tensorflow e PyTorch, simplificam o aprendizado de m√°quina de imagem.  No entanto, a quest√£o ainda permanece: como os dados passam pelas camadas da rede neural e como o computador aprende com elas?  Para obter uma vis√£o clara do zero, mergulhamos em uma convolu√ß√£o, visualizando a imagem de cada camada. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="imagem"><br><a name="habracut"></a><br><h2>  Redes neurais convolucionais </h2><br>  Antes de come√ßar a estudar redes neurais convolucionais (SNA), voc√™ precisa aprender a trabalhar com redes neurais.  As redes neurais imitam o c√©rebro humano para resolver problemas complexos e procurar padr√µes nos dados.  Nos √∫ltimos anos, eles substitu√≠ram muitos algoritmos de aprendizado de m√°quina e vis√£o computacional.  O modelo b√°sico de uma rede neural consiste em neur√¥nios organizados em camadas.  Cada rede neural possui uma camada de entrada e sa√≠da e v√°rias camadas ocultas, dependendo da complexidade do problema.  Ao transmitir dados atrav√©s de camadas, os neur√¥nios s√£o treinados e reconhecem sinais.  Essa representa√ß√£o de uma rede neural √© chamada de modelo.  Ap√≥s o treinamento do modelo, solicitamos √† rede que fa√ßa previs√µes com base nos dados do teste. <br><br>  O SNS √© um tipo especial de rede neural que funciona bem com imagens.  Ian Lekun os prop√¥s em 1998, onde reconheceram o n√∫mero presente na imagem de entrada.  O SNA tamb√©m √© usado para reconhecimento de fala, segmenta√ß√£o de imagens e processamento de texto.  Antes da cria√ß√£o de redes neurais convolucionais, perceptrons multicamadas eram usados ‚Äã‚Äãna constru√ß√£o de classificadores de imagem.  A classifica√ß√£o de imagem refere-se √† tarefa de extrair classes de uma imagem raster multicanal (colorida, preto e branco).  Os perceptrons de m√∫ltiplas camadas levam muito tempo para procurar informa√ß√µes nas imagens, pois cada entrada deve estar associada a cada neur√¥nio na pr√≥xima camada.  O SNA os contornou usando um conceito chamado conectividade local.  Isso significa que conectaremos cada neur√¥nio apenas √† regi√£o de entrada local.  Isso minimiza o n√∫mero de par√¢metros, permitindo que v√°rias partes da rede se especializem em atributos de alto n√≠vel, como textura ou padr√£o de repeti√ß√£o.  Confuso?  Vamos comparar como as imagens s√£o transmitidas atrav√©s de perceptrons de m√∫ltiplas camadas (MPs) e redes neurais convolucionais. <br><br><h2>  Compara√ß√£o de MP e SNA </h2><br>  O n√∫mero total de entradas na camada de entrada para o perceptron de m√∫ltiplas camadas ser√° 784, pois a imagem de entrada √© 28x28 = 784 (o conjunto de dados MNIST √© considerado).  A rede deve ser capaz de prever o n√∫mero na imagem de entrada, o que significa que a sa√≠da pode pertencer a qualquer uma das seguintes classes no intervalo de 0 a 9. Na camada de sa√≠da, retornamos estimativas de classe, digamos, se essa entrada for a imagem com o n√∫mero "3", ent√£o, na camada de sa√≠da, o neur√¥nio correspondente "3" tem um valor mais alto em compara√ß√£o com outros neur√¥nios.  Novamente, surge a pergunta: "Quantas camadas ocultas precisamos e quantos neur√¥nios devem existir em cada uma?"  Por exemplo, use o seguinte c√≥digo MP: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="imagem"><br><br>  O c√≥digo acima √© implementado usando uma estrutura chamada Keras.  A primeira camada oculta possui 512 neur√¥nios conectados √† camada de entrada de 784 neur√¥nios.  A pr√≥xima camada oculta: a camada de exclus√£o, que resolve o problema de reciclagem.  0.2 significa que h√° uma chance de 20% de n√£o levar em considera√ß√£o os neur√¥nios da camada oculta anterior.  Novamente, adicionamos uma segunda camada oculta com o mesmo n√∫mero de neur√¥nios da primeira camada oculta (512) e depois outra camada exclusiva.  Finalmente, finalizando esse conjunto de camadas com uma camada de sa√≠da composta por 10 classes.  A classe que mais importa ser√° o n√∫mero previsto pelo modelo.  √â assim que uma rede multicamada cuida da identifica√ß√£o de todas as camadas.  Uma das desvantagens do perceptron de v√°rios n√≠veis √© que ele est√° totalmente conectado, o que demanda muito tempo e espa√ßo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="imagem"><br><br>  Convolts n√£o usam camadas totalmente unidas.  Eles usam camadas esparsas, que recebem matrizes como entrada, o que oferece uma vantagem sobre o MP.  No MP, cada n√≥ √© respons√°vel por entender toda a imagem.  No SNA, dividimos a imagem em √°reas (pequenas √°reas locais de pixels).  A camada de sa√≠da combina os dados recebidos de cada n√≥ oculto para encontrar padr√µes.  Abaixo est√° uma imagem de como as camadas est√£o conectadas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="imagem"><br><br>  Agora vamos ver como o SNA encontra informa√ß√µes nas fotografias.  Antes disso, precisamos entender como os sinais s√£o extra√≠dos.  No SNA, usamos camadas diferentes, cada camada preserva os sinais da imagem, por exemplo, leva em conta a imagem do c√£o, quando a rede precisa classific√°-lo, deve identificar todos os sinais, como olhos, ouvidos, l√≠ngua, pernas, etc.  Esses sinais s√£o quebrados e reconhecidos nos n√≠veis da rede local usando filtros e n√∫cleos. <br><br><h2>  Como os computadores olham para uma imagem? </h2><br>  Uma pessoa que olha uma imagem e entende seu significado parece bastante razo√°vel.  Digamos que voc√™ ande e observe as muitas paisagens ao seu redor.  Como entendemos a natureza nesse caso?  Tiramos fotos do ambiente usando nosso principal √≥rg√£o sensorial - o olho, e depois o enviamos para a retina.  Tudo parece bem interessante, certo?  Agora vamos imaginar que um computador fa√ßa o mesmo.  Nos computadores, as imagens s√£o interpretadas usando um conjunto de valores de pixel que variam de 0 a 255. O computador examina esses valores de pixel e os entende.  √Ä primeira vista, ele n√£o conhece objetos e cores.  Ele simplesmente reconhece os valores de pixel e a imagem √© equivalente a um conjunto de valores de pixel para o computador.  Mais tarde, analisando os valores de pixel, ele aprende gradualmente se a imagem √© cinza ou colorida.  As imagens em escala de cinza t√™m apenas um canal, pois cada pixel representa a intensidade de uma cor.  0 significa preto e 255 significa branco, as outras variantes de preto e branco, ou seja, cinza, est√£o entre elas. <br><br>  As imagens coloridas possuem tr√™s canais, vermelho, verde e azul.  Eles representam a intensidade de tr√™s cores (matriz tridimensional) e, quando os valores mudam simultaneamente, isso fornece um grande conjunto de cores, realmente uma paleta de cores!  Depois disso, o computador reconhece as curvas e contornos dos objetos na imagem.  Tudo isso pode ser estudado na rede neural convolucional.  Para isso, usaremos o PyTorch para carregar um conjunto de dados e aplicar filtros √†s imagens.  A seguir, √© um trecho de c√≥digo. <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="imagem"><br><br>  Agora vamos ver como uma √∫nica imagem √© inserida em uma rede neural. <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="imagem"><br><br>  √â assim que o n√∫mero "3" √© dividido em pixels.  No conjunto de d√≠gitos manuscritos, ‚Äú3‚Äù √© selecionado aleatoriamente, no qual os valores de pixel s√£o exibidos.  Aqui ToTensor () normaliza os valores reais de pixel (0‚Äì255) e os limita a um intervalo de 0 a 1. Por que isso?  Porque facilita os c√°lculos nas se√ß√µes subseq√ºentes, seja para interpretar imagens ou para encontrar padr√µes comuns que existem nelas. <br><br><h2>  Crie seu pr√≥prio filtro </h2><br>  Os filtros, como o nome indica, filtram as informa√ß√µes.  No caso de redes neurais convolucionais, ao trabalhar com imagens, as informa√ß√µes sobre os pixels s√£o filtradas.  Por que devemos filtrar?  Lembre-se de que um computador deve passar por um processo de aprendizado para entender imagens, muito parecido com o que a crian√ßa faz.  Nesse caso, no entanto, n√£o precisaremos de muitos anos!  Em suma, ele aprende do zero e depois avan√ßa para o todo. <br><br>  Portanto, a rede deve conhecer inicialmente todas as partes grosseiras da imagem, como bordas, contornos e outros elementos de baixo n√≠vel.  Uma vez descobertos, o caminho para sintomas complexos √© pavimentado.  Para chegar a eles, precisamos primeiro extrair os atributos de baixo n√≠vel, depois o meio e, em seguida, os de n√≠vel superior.  Os filtros s√£o uma maneira de extrair as informa√ß√µes de que o usu√°rio precisa, e n√£o apenas a transfer√™ncia cega de dados, por causa da qual o computador n√£o entende a estrutura das imagens.  No come√ßo, fun√ß√µes de baixo n√≠vel podem ser extra√≠das com base em um filtro espec√≠fico.  O filtro aqui tamb√©m √© um conjunto de valores de pixel, semelhante a uma imagem.  Pode ser entendido como os pesos que conectam as camadas na rede neural convolucional.  Esses pesos ou filtros s√£o multiplicados pelos valores de entrada para produzir imagens intermedi√°rias que representam o entendimento da imagem por computador.  Em seguida, eles s√£o multiplicados por mais alguns filtros para expandir a exibi√ß√£o.  Em seguida, ele detecta os √≥rg√£os vis√≠veis de uma pessoa (desde que uma pessoa esteja presente na imagem).  Mais tarde, com a inclus√£o de v√°rios outros filtros e v√°rias camadas, o computador exclama: ‚ÄúAh, sim!  Este √© um homem. " <br><br>  Se falamos de filtros, temos muitas op√ß√µes.  Voc√™ pode desfocar a imagem e aplicar um filtro de desfoque; se precisar adicionar nitidez, um filtro de nitidez ser√° resgatado etc. <br><br>  Vejamos alguns trechos de c√≥digo para entender a funcionalidade dos filtros. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="imagem"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="imagem"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="imagem"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="imagem"><br><br>  √â assim que a imagem √© aplicada ap√≥s a aplica√ß√£o do filtro. Nesse caso, usamos o filtro Sobel. <br><br><h2>  Redes neurais convolucionais </h2><br>  At√© agora, vimos como os filtros s√£o usados ‚Äã‚Äãpara extrair recursos das imagens.  Agora, para completar toda a rede neural convolucional, precisamos conhecer todas as camadas que usamos para projet√°-la.  As camadas usadas no SNA, <br><br><ol><li>  Camada convolucional </li><li>  Camada de pool </li><li>  Camada totalmente colada </li></ol><br>  Com todas as tr√™s camadas, o classificador de imagem convolucional fica assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="imagem"><br><br>  Agora vamos ver o que cada camada faz. <br><br>  <b>A camada convolucional (CONV)</b> usa filtros que executam opera√ß√µes de convolu√ß√£o digitalizando a imagem de entrada.  Seus hiperpar√¢metros incluem um tamanho de filtro, que pode ser 2x2, 3x3, 4x4, 5x5 (mas n√£o limitado a isso) e a etapa S. O resultado O √© chamado de mapa de recurso ou mapa de ativa√ß√£o no qual todos os recursos s√£o calculados usando camadas e filtros de entrada.  Abaixo est√° uma imagem da gera√ß√£o de mapas de recursos ao aplicar a convolu√ß√£o, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="imagem"><br><br>  <b>A camada de mesclagem (POOL) √©</b> usada para compactar os recursos normalmente usados ‚Äã‚Äãap√≥s a camada de convolu√ß√£o.  Existem dois tipos de opera√ß√µes de uni√£o - essa √© a uni√£o m√°xima e m√©dia, na qual os valores m√°ximo e m√©dio das caracter√≠sticas s√£o obtidos, respectivamente.  A seguir est√° a opera√ß√£o de opera√ß√µes de mesclagem, <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="imagem"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="imagem"><br><br>  <b>As camadas totalmente conectadas (FCs)</b> operam com uma entrada plana, onde cada entrada √© conectada a todos os neur√¥nios.  Eles geralmente s√£o usados ‚Äã‚Äãno final da rede para conectar camadas ocultas √† camada de sa√≠da, o que ajuda a otimizar as pontua√ß√µes das turmas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="imagem"><br><br><h3>  Visualiza√ß√£o SNA no PyTorch </h3><br>  Agora que temos a ideologia completa da cria√ß√£o do SNA, vamos implement√°-lo usando a estrutura PyTorch do Facebook. <br><br>  <b>Etapa 1</b> : fa√ßa o download da imagem de entrada a ser enviada pela rede.  (Aqui fazemos isso com Numpy e OpenCV) <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="imagem"><br><br>  <b>Etapa 2</b> : renderizar filtros <br><br>  Vamos visualizar os filtros para entender melhor quais usaremos, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="imagem"><br><br>  <b>Etapa 3</b> : Determinar o SNA <br><br>  Esse SNA possui uma camada convolucional e uma camada de pool com uma fun√ß√£o m√°xima, e os pesos s√£o inicializados usando os filtros mostrados acima, <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>Etapa 4</b> : renderizar filtros <br>  Uma r√°pida olhada nos filtros usados, <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Filtros: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="imagem"><br><br>  <b>Etapa 5</b> : Resultados filtrados por camada <br><br>  As imagens que aparecem na camada CONV e POOL s√£o mostradas abaixo. <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br>  Camadas convolucionais <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="imagem"><br><br>  Camadas de pool <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="imagem"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Fonte</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt436838/">https://habr.com/ru/post/pt436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt436822/index.html">Rob√≥tica Android at√© 2019: a hist√≥ria real; em 5 partes; parte 3</a></li>
<li><a href="../pt436826/index.html">Rob√≥tica Android at√© 2019: a hist√≥ria real; em 5 partes; parte 4</a></li>
<li><a href="../pt436828/index.html">A transi√ß√£o para o Boost-1.65.1 e os erros que surgiram</a></li>
<li><a href="../pt436830/index.html">Rob√≥tica Android at√© 2019: a hist√≥ria real; em 5 partes; parte 5</a></li>
<li><a href="../pt436836/index.html">Benef√≠cios da an√°lise de aplicativos de n√≠vel 7 em firewalls. Parte 2. Seguran√ßa</a></li>
<li><a href="../pt436840/index.html">O caminho do brilho para a neuroci√™ncia: um podcast tem√°tico sobre carreiras em marketing de m√≠dia e conte√∫do</a></li>
<li><a href="../pt436842/index.html">Solu√ß√£o Veeam para backup e recupera√ß√£o de m√°quinas virtuais na plataforma Nutanix AHV. Parte 2</a></li>
<li><a href="../pt436846/index.html">O resumo de materiais frescos do mundo do front-end da √∫ltima semana n ¬∞ 348 (14-20 de janeiro de 2019)</a></li>
<li><a href="../pt436848/index.html">NSA anuncia lan√ßamento de ferramenta interna para engenharia reversa</a></li>
<li><a href="../pt436850/index.html">Erros comuns ao escrever testes de unidade. Palestra Yandex</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>