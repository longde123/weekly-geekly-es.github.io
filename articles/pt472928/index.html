<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêÖ ü§∞üèæ üñï Servi√ßo de computa√ß√£o GPU altamente carregado üï∫üèø üè£ üå∞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Lidero o desenvolvimento da plataforma Vision - esta √© a nossa plataforma p√∫blica, que fornece acesso a modelos de vis√£o computacional e per...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Servi√ßo de computa√ß√£o GPU altamente carregado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472928/"><img src="https://habrastorage.org/webt/_v/yk/sa/_vyksasjmhcbsn1feox_egbqs_4.jpeg"><br><br>  Ol√° Habr!  Lidero o desenvolvimento da plataforma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Vision</a> - esta √© a nossa plataforma p√∫blica, que fornece acesso a modelos de vis√£o computacional e permite que voc√™ resolva tarefas como reconhecer rostos, n√∫meros, objetos e cenas inteiras.  E hoje quero dizer, pelo exemplo da Vision, como implementar um servi√ßo r√°pido e altamente carregado usando placas de v√≠deo, como implant√°-lo e oper√°-lo. <br><a name="habracut"></a><br><h1>  O que √© a vis√£o? </h1><br>  Esta √© essencialmente uma API REST.  O usu√°rio gera uma solicita√ß√£o HTTP com uma foto e a envia ao servidor. <br><br>  Suponha que voc√™ precise reconhecer um rosto em uma imagem.  O sistema encontra, corta, extrai algumas propriedades da face, salva no banco de dados e atribui um n√∫mero condicional.  Por exemplo, person42.  O usu√°rio ent√£o carrega a pr√≥xima foto, que tem a mesma pessoa.  O sistema extrai propriedades de sua face, pesquisa no banco de dados e retorna o n√∫mero condicional que foi atribu√≠do √† pessoa inicialmente, ou seja,  person42. <br><br>  Hoje, os principais usu√°rios do Vision s√£o v√°rios projetos do Mail.ru Group.  A maioria dos pedidos vem do Mail and Cloud. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c77/448/9a8/c774489a8bcd0a3badf0a8413d95e97c.png" width="400"></div><br>  Na nuvem, os usu√°rios t√™m pastas nas quais as fotos s√£o carregadas.  A nuvem executa arquivos atrav√©s do Vision e os agrupa em categorias.  Depois disso, o usu√°rio pode folhear suas fotos comodamente.  Por exemplo, quando voc√™ deseja mostrar fotos a amigos ou familiares, pode encontrar rapidamente as que precisa. <br><br>  O Mail e o Cloud s√£o servi√ßos muito grandes, com milh√µes de pessoas; portanto, o Vision processa centenas de milhares de solicita√ß√µes por minuto.  Ou seja, √© um servi√ßo cl√°ssico de alta carga, mas com uma reviravolta: ele possui nginx, um servidor web, um banco de dados e filas, mas no n√≠vel mais baixo desse servi√ßo √© a infer√™ncia - executando imagens atrav√©s de redes neurais.  √â o funcionamento das redes neurais que ocupa a maior parte do tempo e requer recursos.  As redes de computa√ß√£o consistem em uma sequ√™ncia de opera√ß√µes da matriz que geralmente levam muito tempo na CPU, mas s√£o perfeitamente paralelas na GPU.  Para executar redes com efici√™ncia, usamos um cluster de servidores com placas de v√≠deo. <br><br>  Neste artigo, quero compartilhar um conjunto de dicas que podem ser √∫teis ao criar um servi√ßo desse tipo. <br><br><h1>  Desenvolvimento de Servi√ßos </h1><br><h3>  Tempo de processamento para uma solicita√ß√£o </h3><br>  Para um sistema com carga pesada, o tempo de processamento de uma solicita√ß√£o e a taxa de transfer√™ncia do sistema s√£o importantes.  A alta velocidade do processamento de consultas √© fornecida, primeiramente, pela sele√ß√£o correta da arquitetura de rede neural.  No ML, como em qualquer outra tarefa de programa√ß√£o, as mesmas tarefas podem ser resolvidas de maneiras diferentes.  Vamos dar uma detec√ß√£o de rosto: para resolver esse problema, primeiro pegamos redes neurais com arquitetura R-FCN.  Eles mostram uma qualidade razoavelmente alta, mas levaram cerca de 40 ms em uma imagem, o que n√£o era adequado para n√≥s.Em seguida, voltamos para a arquitetura MTCNN e obtivemos um aumento duplo de velocidade com uma leve perda de qualidade. <br><br>  √Äs vezes, para otimizar o tempo de computa√ß√£o das redes neurais, pode ser vantajoso implementar infer√™ncia em outra estrutura, n√£o na que foi ensinada.  Por exemplo, √†s vezes faz sentido converter seu modelo em NVIDIA TensorRT.  Aplica v√°rias otimiza√ß√µes e √© especialmente bom em modelos bastante complexos.  Por exemplo, ele pode de alguma forma reorganizar algumas camadas, mesclar e at√© jog√°-las fora;  o resultado n√£o ser√° alterado e a velocidade de c√°lculo da infer√™ncia aumentar√°.  O TensorRT tamb√©m permite gerenciar melhor a mem√≥ria e, ap√≥s alguns truques, pode reduzi-la ao c√°lculo de n√∫meros com menos precis√£o, o que tamb√©m aumenta a velocidade da infer√™ncia de computa√ß√£o. <br><br><h3>  Baixar placa de v√≠deo </h3><br>  A infer√™ncia de rede √© realizada na GPU, a placa de v√≠deo √© a parte mais cara do servidor, por isso √© importante us√°-la da maneira mais eficiente poss√≠vel.  Como entender, carregamos totalmente a GPU ou podemos aumentar a carga?  Esta pergunta pode ser respondida, por exemplo, usando o par√¢metro GPU Utilization no utilit√°rio nvidia-smi do pacote padr√£o do driver de v√≠deo.  Esta figura, √© claro, n√£o mostra quantos n√∫cleos CUDA s√£o carregados diretamente na placa de v√≠deo, mas quantos est√£o ociosos, mas permite avaliar de alguma forma o carregamento da GPU.  Por experi√™ncia, podemos dizer que 80-90% de carga √© boa.  Se o carregamento estiver entre 10 e 20%, isso √© ruim e ainda h√° potencial. <br><br>  Uma conseq√º√™ncia importante dessa observa√ß√£o: voc√™ precisa tentar organizar o sistema para maximizar o carregamento das placas de v√≠deo.  Al√©m disso, se voc√™ tiver 10 placas de v√≠deo, cada uma com 10 a 20% de carga, provavelmente duas placas de v√≠deo de alta carga poder√£o resolver o mesmo problema. <br><br><h3>  Taxa de transfer√™ncia do sistema </h3><br>  Quando voc√™ envia uma imagem para a entrada de uma rede neural, o processamento da imagem √© reduzido a uma variedade de opera√ß√µes da matriz.  A placa de v√≠deo √© um sistema com v√°rios n√∫cleos e as imagens de entrada que geralmente enviamos s√£o pequenas.  Digamos que haja 1.000 n√∫cleos em nossa placa de v√≠deo e tenhamos 250 x 250 pixels na imagem.  Sozinhos, eles n√£o poder√£o carregar todos os n√∫cleos devido ao seu tamanho modesto.  E se enviarmos essas fotos para o modelo, uma de cada vez, o carregamento da placa de v√≠deo n√£o exceder√° 25%. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b9/b25/191/4b9b2519170a86b151420cc0a746a2fe.png"></div><br>  Portanto, voc√™ precisa fazer upload de v√°rias imagens para infer√™ncia de uma s√≥ vez e formar um lote delas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/367/51b/04c36751b27fa428e9828a6fa007e25d.png"></div><br>  Nesse caso, a carga da placa de v√≠deo aumenta para 95% e o c√°lculo da infer√™ncia levar√° tempo como para uma √∫nica imagem. <br><br>  Mas e se n√£o houver 10 fotos na fila para que possamos combin√°-las em um lote?  Voc√™ pode esperar um pouco, por exemplo, 50-100 ms na esperan√ßa de que as solicita√ß√µes cheguem.  Essa estrat√©gia √© chamada de estrat√©gia de lat√™ncia de corre√ß√£o.  Permite combinar solicita√ß√µes de clientes em um buffer interno.  Como resultado, aumentamos nosso atraso em uma quantia fixa, mas aumentamos significativamente a taxa de transfer√™ncia do sistema. <br><br><h3>  Infer√™ncia de lan√ßamento </h3><br>  Treinamos modelos em imagens de formato e tamanho fixos (por exemplo, 200 x 200 pixels), mas o servi√ßo deve suportar a capacidade de fazer upload de v√°rias imagens.  Portanto, todas as imagens antes de enviar para a infer√™ncia, voc√™ precisa preparar adequadamente (redimensionar, centralizar, normalizar, converter para flutuar etc.).  Se todas essas opera√ß√µes forem executadas em um processo que inicia a infer√™ncia, seu ciclo de trabalho ser√° mais ou menos assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f75/5be/112/f755be112bc557d3ab602aee47e40d2e.png"><br><br>  Ele passa algum tempo no processador, preparando os dados de entrada, aguardando uma resposta da GPU.  √â melhor minimizar os intervalos entre infer√™ncias para que a GPU fique ociosa menos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/783/cd7/fdb783cd70c90fcc06ba2cfca5f9c7f2.png"><br><br>  Para fazer isso, voc√™ pode iniciar outro fluxo ou transferir a prepara√ß√£o de imagens para outros servidores, sem placas de v√≠deo, mas com processadores poderosos. <br><br>  Se poss√≠vel, o processo respons√°vel pela infer√™ncia deve lidar apenas com ele: acessar a mem√≥ria compartilhada, coletar dados de entrada, copi√°-los imediatamente para a mem√≥ria da placa de v√≠deo e executar infer√™ncia. <br><br><h3>  Turbo boost </h3><br>  O lan√ßamento de redes neurais √© uma opera√ß√£o que consome recursos n√£o apenas da GPU, mas tamb√©m do processador.  Mesmo que tudo esteja organizado corretamente em termos de largura de banda e o encadeamento que realiza a infer√™ncia j√° esteja aguardando novos dados, em um processador fraco, voc√™ simplesmente n√£o ter√° tempo para saturar esse fluxo com novos dados. <br><br>  Muitos processadores suportam a tecnologia Turbo Boost.  Ele permite aumentar a frequ√™ncia do processador, mas nem sempre √© ativado por padr√£o.  Vale a pena conferir.  Para isso, o Linux possui o utilit√°rio CPU Power: <code>$ cpupower frequency-info -m</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fd4/78f/ca4/fd478fca423620ee3bd684d2adfe9d73.png"></div><br>  Os processadores tamb√©m possuem um modo de consumo de energia que pode ser reconhecido por esse comando CPU Power: <code>performance</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/12a/ed4/a1b/12aed4a1b0dec7240bd744232b5d4770.png"></div><br>  No modo de economia de energia, o processador pode acelerar sua frequ√™ncia e rodar mais devagar.  Voc√™ deve entrar no BIOS e selecionar o modo de desempenho.  Ent√£o o processador sempre funcionar√° na frequ√™ncia m√°xima. <br><br><h1>  Implanta√ß√£o de aplicativo </h1><br>  O Docker √© √≥timo para implantar o aplicativo, pois permite executar aplicativos na GPU dentro do cont√™iner.  Para acessar as placas de v√≠deo, primeiro voc√™ precisa instalar os drivers da placa de v√≠deo no sistema host - um servidor f√≠sico.  Ent√£o, para iniciar o cont√™iner, voc√™ precisa fazer muito trabalho manual: jogue corretamente as placas de v√≠deo dentro do cont√™iner com os par√¢metros corretos.  Depois de iniciar o cont√™iner, ainda ser√° necess√°rio instalar drivers de v√≠deo dentro dele.  E somente depois disso voc√™ poder√° usar seu aplicativo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20a/ae0/cb2/20aae0cb286e3cef415f2a32f4f12bfa.png"><br><br>  Essa abordagem tem uma ressalva.  Os servidores podem desaparecer do cluster e serem adicionados.  √â poss√≠vel que servidores diferentes tenham vers√µes diferentes de drivers e sejam diferentes da vers√£o instalada dentro do cont√™iner.  Nesse caso, um Docker simples ser√° interrompido: o aplicativo receber√° um erro de incompatibilidade de vers√£o do driver ao tentar acessar a placa de v√≠deo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb4/1af/0b2/bb41af0b281cf8a660d2cc35db133101.png"><br><br>  Como lidar com isso?  Existe uma vers√£o do Docker da NVIDIA, gra√ßas √† qual se torna mais f√°cil e mais agrad√°vel usar o cont√™iner.  De acordo com a pr√≥pria NVIDIA e de acordo com observa√ß√µes pr√°ticas, a sobrecarga do uso da nvidia-docker √© de cerca de 1%. <br><br>  Nesse caso, os drivers precisam ser instalados apenas na m√°quina host.  Quando voc√™ inicia o cont√™iner, n√£o precisa jogar nada dentro, e o aplicativo ter√° acesso imediato √†s placas de v√≠deo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c95/967/a3a/c95967a3a8ac668acaed1fc14550dcb8.png"><br><br>  A "independ√™ncia" do nvidia-docker dos drivers permite executar um cont√™iner da mesma imagem em m√°quinas diferentes nas quais vers√µes diferentes de drivers est√£o instaladas.  Como isso √© implementado?  O Docker possui um conceito chamado docker-runtime: √© um conjunto de padr√µes que descreve como um cont√™iner deve se comunicar com o kernel do host, como deve iniciar e parar, como interagir com o kernel e o driver.  Come√ßando com uma vers√£o espec√≠fica do Docker, √© poss√≠vel substituir esse tempo de execu√ß√£o.  Foi o que a NVIDIA fez: eles substituem o tempo de execu√ß√£o, capturam as chamadas para o driver de v√≠deo interno e convertem a vers√£o correta em chamadas para o driver de v√≠deo. <br><br><h1>  Orquestra√ß√£o </h1><br>  Escolhemos Kubernetes como orquestra.  Ele suporta muitos recursos muito √∫teis que s√£o √∫teis para qualquer sistema muito carregado.  Por exemplo, a descoberta autom√°tica permite que os servi√ßos acessem um ao outro em um cluster sem regras complexas de roteamento.  Ou toler√¢ncia a falhas - quando o Kubernetes sempre mant√©m v√°rios cont√™ineres prontos e, se algo aconteceu com o seu, o Kubernetes lan√ßa imediatamente um novo cont√™iner. <br><br>  Se voc√™ j√° possui um cluster Kubernetes configurado, n√£o precisa muito para come√ßar a usar placas de v√≠deo dentro do cluster: <br><br><ul><li>  drivers relativamente novos <br></li><li>  instalada nvidia-docker vers√£o 2 <br></li><li>  tempo de execu√ß√£o do docker definido por padr√£o como `nvidia` em /etc/docker/daemon.json: <br> <code>"default-runtime": "nvidia"</code> <br> </li><li>  Plug-in instalado <code>kubectl create -f https://githubusercontent.com/k8s-device-plugin/v1.12/plugin.yml</code> </li></ul><br>  Depois de configurar o cluster e instalar o plug-in do dispositivo, voc√™ pode especificar uma placa de v√≠deo como um recurso. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/414/c90/8f6/414c908f6d3ba6d4a9bded55921790c6.png"><br><br>  O que isso afeta?  Digamos que temos dois n√≥s, m√°quinas f√≠sicas.  Por um lado, h√° uma placa de v√≠deo, por outro, n√£o.  O Kubernetes ir√° detectar uma m√°quina com uma placa de v√≠deo e pegar nosso pod nela. <br><br>  √â importante observar que o Kubernetes n√£o sabe como atrapalhar competentemente uma placa de v√≠deo entre os pods.  Se voc√™ possui 4 placas de v√≠deo e precisa de 1 GPU para iniciar o cont√™iner, poder√° gerar no m√°ximo 4 pods no cluster. <br><br>  Tomamos como regra 1 Pod = 1 Modelo = 1 GPU. <br><br>  H√° uma op√ß√£o para executar mais inst√¢ncias em 4 placas de v√≠deo, mas n√£o a consideraremos neste artigo, pois essa op√ß√£o n√£o sai da caixa. <br><br>  Se v√°rios modelos girarem ao mesmo tempo, √© conveniente criar a Implanta√ß√£o no Kubernetes para cada modelo.  Em seu arquivo de configura√ß√£o, voc√™ pode especificar o n√∫mero de lares para cada modelo, levando em considera√ß√£o a popularidade do modelo.  Se muitas solicita√ß√µes vierem para o modelo, ser√° necess√°rio especificar muitos pods para ele, se houver poucas solicita√ß√µes, haver√° poucos pods.  No total, o n√∫mero de lares deve ser igual ao n√∫mero de placas de v√≠deo no cluster. <br><br>  Considere um ponto interessante.  Digamos que temos 4 placas de v√≠deo e 3 modelos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/767/23a/04c76723a17fef7311eb41462db1d5a7.png"></div><br>  Nas duas primeiras placas de v√≠deo, suba a infer√™ncia do modelo de reconhecimento de rosto, em outro reconhecimento de objetos e em outro reconhecimento de n√∫meros de carros. <br><br>  Voc√™ trabalha, os clientes v√£o e v√™m e, uma vez, por exemplo, √† noite, surge uma situa√ß√£o em que uma placa de v√≠deo com objetos de infer√™ncia simplesmente n√£o √© carregada, uma pequena quantidade de solicita√ß√µes chega e as placas de v√≠deo com reconhecimento de rosto ficam sobrecarregadas.  Eu gostaria de montar um modelo com objetos neste momento e lan√ßar faces em seu lugar para descarregar as linhas. <br><br>  Para o dimensionamento autom√°tico de modelos em placas de v√≠deo, existem ferramentas dentro do Kubernetes - dimensionamento autom√°tico da lareira horizontal (HPA, autoescalador de pod horizontal). <br>  Pronto para uso, o Kubernetes suporta o dimensionamento autom√°tico na utiliza√ß√£o da CPU.  Por√©m, em uma tarefa com placas de v√≠deo, ser√° muito mais razo√°vel usar informa√ß√µes sobre o n√∫mero de tarefas de cada modelo para dimensionamento. <br><br>  Fazemos isso: coloque solicita√ß√µes para cada modelo em uma fila.  Quando as solicita√ß√µes s√£o conclu√≠das, n√≥s as removemos dessa fila.  Se conseguirmos processar rapidamente solicita√ß√µes de modelos populares, a fila n√£o aumentar√°.  Se o n√∫mero de solicita√ß√µes para um modelo espec√≠fico aumentar repentinamente, a fila come√ßar√° a crescer.  Torna-se claro que voc√™ precisa adicionar placas de v√≠deo que ajudar√£o a aumentar a linha. <br><br>  Informa√ß√µes sobre as filas que proxy atrav√©s do HPA atrav√©s do Prometheus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa3/da3/6ba/aa3da36baec71d52dcd0bee93c8e52cf.png"><br><br>  E ent√£o fazemos o dimensionamento autom√°tico dos modelos nas placas de v√≠deo no cluster, dependendo do n√∫mero de solicita√ß√µes para eles. <br><br><h3>  CI / CD </h3><br>  Depois de encerrar o aplicativo e envolv√™-lo no Kubernetes, voc√™ tem literalmente um passo para a parte superior do projeto.  Voc√™ pode adicionar CI / CD, aqui est√° um exemplo do nosso pipeline: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d3/982/b23/3d3982b23d9ff4f1d413cee86c601d8d.png"><br><br>  Aqui, o programador lan√ßou o novo c√≥digo na ramifica√ß√£o principal, ap√≥s o qual a imagem do Docker com nossos daemons de back-end √© coletada automaticamente e os testes s√£o executados.  Se todas as marcas de verifica√ß√£o estiverem verdes, o aplicativo ser√° derramado no ambiente de teste.  Se n√£o houver problemas, voc√™ poder√° enviar a imagem para opera√ß√£o sem dificuldades. <br><br><h1>  Conclus√£o </h1><br>  No meu artigo, abordamos alguns aspectos do trabalho de um servi√ßo altamente carregado usando uma GPU.  Falamos sobre maneiras de reduzir o tempo de resposta de um servi√ßo, como: <br><br><ul><li>  sele√ß√£o da arquitetura de rede neural ideal para reduzir a lat√™ncia; </li><li>  Aplicativos de estruturas de otimiza√ß√£o como o TensorRT. </li></ul><br>  Levantou os problemas do aumento da taxa de transfer√™ncia: <br><br><ul><li>  o uso de lotes de imagens; </li><li>  aplicar a estrat√©gia de lat√™ncia de corre√ß√£o para reduzir o n√∫mero de execu√ß√µes de infer√™ncia, mas cada infer√™ncia processaria um n√∫mero maior de imagens; </li><li>  otimiza√ß√£o do pipeline de entrada de dados para minimizar o tempo de inatividade da GPU; </li><li>  "Combate" com trote do processador, remo√ß√£o de opera√ß√µes vinculadas √† CPU para outros servidores. </li></ul><br>  Analisamos o processo de implanta√ß√£o de um aplicativo com uma GPU: <br><br><ul><li>  Usando a nvidia-docker dentro do Kubernetes </li><li>  escala com base no n√∫mero de solicita√ß√µes e HPA (autoscaler horizontal do pod). </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472928/">https://habr.com/ru/post/pt472928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472912/index.html">Banco de dados ClickHouse para humanos ou Alien Technology</a></li>
<li><a href="../pt472916/index.html">Back-end, aprendizado de m√°quina e sem servidor s√£o os mais interessantes da confer√™ncia Habr de julho</a></li>
<li><a href="../pt472918/index.html">ZX Spectrum na R√∫ssia e na CEI: como a busca pelo online se transformou offline</a></li>
<li><a href="../pt472922/index.html">Programador do Defender mais forte que a entropia</a></li>
<li><a href="../pt472926/index.html">A lei dos retornos acelerados (parte 1)</a></li>
<li><a href="../pt472930/index.html">Astrof√≠sicos do Vale do Sil√≠cio Quantificando Moda</a></li>
<li><a href="../pt472932/index.html">An√°lise est√°tica do IntelliJ IDEA versus mente humana</a></li>
<li><a href="../pt472934/index.html">O que √© um Zero Trust? Modelo de seguran√ßa</a></li>
<li><a href="../pt472936/index.html">Opera√ß√£o TA505: Agrupando infraestrutura de rede. Parte 3</a></li>
<li><a href="../pt472944/index.html">Dados como servi√ßo: o que √©, dificuldades t√©cnicas e como contorn√°-las usando proxies residentes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>