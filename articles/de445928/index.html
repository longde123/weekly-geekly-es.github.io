<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìÆ üë∂üèΩ ‚ô¶Ô∏è Wie wir die Produktivit√§t des Tensorflow-Dienstes um 70% steigern konnten üéÇ ü§úüèª üöµ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow ist zur Standardplattform f√ºr maschinelles Lernen (ML) geworden, die sowohl in der Industrie als auch in der Forschung beliebt ist. Viele k...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir die Produktivit√§t des Tensorflow-Dienstes um 70% steigern konnten</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/"> Tensorflow ist zur Standardplattform f√ºr maschinelles Lernen (ML) geworden, die sowohl in der Industrie als auch in der Forschung beliebt ist.  Viele kostenlose Bibliotheken, Tools und Frameworks wurden f√ºr die Schulung und Pflege von ML-Modellen erstellt.  Das Tensorflow Serving-Projekt hilft bei der Pflege von ML-Modellen in einer verteilten Produktionsumgebung. <br><br>  Unser Mux-Dienst verwendet Tensorflow Serving in mehreren Teilen der Infrastruktur. Wir haben bereits die Verwendung von Tensorflow Serving beim Codieren von Videotiteln er√∂rtert.  Heute konzentrieren wir uns auf Methoden, die die Latenz verbessern, indem sie sowohl den Prognoseserver als auch den Client optimieren.  Modellprognosen sind in der Regel ‚ÄûOnline‚Äú -Operationen (auf dem kritischen Pfad der Anforderung einer Anwendung). Daher besteht das Hauptziel der Optimierung darin, gro√üe Mengen von Anforderungen mit der geringstm√∂glichen Verz√∂gerung zu verarbeiten. <br><a name="habracut"></a><br><h1>  Was ist Tensorflow Serving? </h1><br>  Tensorflow Serving bietet eine flexible Serverarchitektur f√ºr die Bereitstellung und Wartung von ML-Modellen.  Sobald das Modell trainiert und f√ºr die Prognose bereit ist, muss es f√ºr Tensorflow Serving in ein kompatibles (wartbares) Format exportiert werden. <br><br>  <i>Servable</i> ist eine zentrale Abstraktion, die Tensorflow-Objekte umschlie√üt.  Beispielsweise kann ein Modell als ein oder mehrere Servable-Objekte dargestellt werden.  Servable sind daher die Basisobjekte, mit denen der Client Berechnungen durchf√ºhrt.  Auf die wartbare Gr√∂√üe kommt es an: Kleinere Modelle ben√∂tigen weniger Platz, ben√∂tigen weniger Speicher und werden schneller geladen.  Zum Herunterladen und Verwalten mithilfe der Predict-API m√ºssen Modelle im SavedModel-Format vorliegen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving kombiniert die grundlegenden Komponenten, um einen gRPC / HTTP-Server zu erstellen, der mehrere ML-Modelle (oder mehrere Versionen) bedient, √úberwachungskomponenten und eine benutzerdefinierte Architektur bereitstellt. <br><br><h1>  Tensorflow Serving mit Docker </h1><br>  Werfen wir einen Blick auf die grundlegenden Latenzmetriken bei der Prognoseleistung mit den Standardeinstellungen f√ºr Tensorflow Serving (ohne CPU-Optimierung). <br><br>  Laden Sie zun√§chst das neueste Bild vom TensorFlow Docker-Hub herunter: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  In diesem Artikel werden alle Container auf einem Host mit vier Kernen, 15 GB, Ubuntu 16.04 ausgef√ºhrt. <br><br><h3>  Exportieren Sie das Tensorflow-Modell nach SavedModel </h3><br>  Wenn ein Modell mit Tensorflow trainiert wird, kann die Ausgabe als variable Kontrollpunkte (Dateien auf der Festplatte) gespeichert werden.  Die Ausgabe erfolgt direkt durch Wiederherstellen von Kontrollpunkten des Modells oder in einem eingefrorenen eingefrorenen Diagrammformat (Bin√§rdatei). <br><br>  F√ºr Tensorflow Serving muss dieses eingefrorene Diagramm in das SavedModel-Format exportiert werden.  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow-Dokumentation</a> enth√§lt Beispiele f√ºr den Export trainierter Modelle in das SavedModel-Format. <br><br>  Tensorflow bietet auch viele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offizielle und Forschungsmodelle</a> als Ausgangspunkt f√ºr Experimente, Forschung oder Produktion. <br><br>  Als Beispiel verwenden wir das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ResNet-Modell (Deep Residual Neural Network)</a> , um ein ImageNet-Dataset mit 1000 Klassen zu klassifizieren.  Laden Sie das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorgefertigte</a> <code>ResNet-50 v2</code> Modell herunter, insbesondere die <i>Option</i> Channels_last (NHWC) in <i>SavedModel</i> : In der Regel funktioniert es besser auf der CPU. <br><br>  Kopieren Sie das RestNet-Modellverzeichnis in die folgende Struktur: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving erwartet eine numerisch geordnete Verzeichnisstruktur f√ºr die Versionierung.  In unserem Fall entspricht das Verzeichnis <code>1/</code> dem Modell der Version 1, das die Architektur des Modells <code>saved_model.pb</code> mit einer Momentaufnahme der Modellgewichte (Variablen) enth√§lt. <br><br><h3>  Laden und Verarbeiten von SavedModel </h3><br>  Der folgende Befehl startet den Tensorflow Serving-Modellserver in einem Docker-Container.  Um SavedModel zu laden, m√ºssen Sie das Modellverzeichnis im erwarteten Containerverzeichnis bereitstellen. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  Das √úberpr√ºfen der Containerprotokolle zeigt, dass ModelServer aktiv ist, um Ausgabeanforderungen f√ºr das <code>resnet</code> Modell an den gRPC- und HTTP-Endpunkten zu verarbeiten: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Prognose-Client </h3><br>  Tensorflow Serving definiert ein API-Schema im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Protokollpufferformat</a> (Protobufs).  GRPC-Client-Implementierungen f√ºr die Prognose-API werden als Python-Paket <code>tensorflow_serving.apis</code> .  Wir ben√∂tigen ein weiteres Python-Paket <code>tensorflow</code> f√ºr Dienstprogrammfunktionen. <br><br>  Installieren Sie die Abh√§ngigkeiten, um einen einfachen Client zu erstellen: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  Das <code>ResNet-50 v2</code> Modell erwartet die Eingabe von Gleitkomma-Tensoren in eine formatierte NHWC-Datenstruktur (canal_last).  Daher wird das Eingabebild mit opencv-python gelesen und als float32-Datentyp in das numpy-Array (H√∂he √ó Breite √ó Kan√§le) geladen.  Das folgende Skript erstellt einen Vorhersage-Client-Stub, l√§dt die JPEG-Daten in ein Numpy-Array und konvertiert sie in tensor_proto, um eine Prognoseanforderung f√ºr gRPC zu stellen: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Nach Erhalt einer JPEG-Eingabe erzeugt ein funktionierender Client das folgende Ergebnis: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  Der resultierende Tensor enth√§lt eine Prognose in Form eines ganzzahligen Werts und einer Vorzeichenwahrscheinlichkeit. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  F√ºr eine einzelne Anfrage ist eine solche Verz√∂gerung nicht akzeptabel.  Kein Wunder: Die Tensorflow Serving-Bin√§rdatei ist standardm√§√üig f√ºr die meisten Ger√§te in den meisten Anwendungsf√§llen ausgelegt.  Sie haben wahrscheinlich die folgenden Zeilen in den Protokollen des Standard-Tensorflow-Serviercontainers bemerkt: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  Dies weist auf eine TensorFlow Serving-Bin√§rdatei hin, die auf einer CPU-Plattform ausgef√ºhrt wird, f√ºr die sie nicht optimiert wurde. <br><br><h3>  Erstellen Sie eine optimierte Bin√§rdatei </h3><br>  Gem√§√ü der Tensorflow- <a href="">Dokumentation</a> wird empfohlen, Tensorflow aus der Quelle mit allen f√ºr die CPU verf√ºgbaren Optimierungen auf dem Host zu kompilieren, auf dem die Bin√§rdatei funktioniert.  Beim Zusammenbau erm√∂glichen spezielle Flags die Aktivierung von CPU-Befehlss√§tzen f√ºr eine bestimmte Plattform: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Befehlssatz </th><th>  Flaggen </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  Alles vom Prozessor unterst√ºtzt </td><td>  --copt = -march = native </td></tr></tbody></table></div><br>  Klonen Sie eine Tensorflow-Portion einer bestimmten Version.  In unserem Fall ist dies 1,13 (der letzte zum Zeitpunkt der Ver√∂ffentlichung dieses Artikels): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  Das Tensorflow Serving-Entwicklerbild verwendet das Basel-Tool zum Erstellen.  Wir konfigurieren es f√ºr bestimmte S√§tze von CPU-Anweisungen: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  Wenn nicht gen√ºgend Speicher vorhanden ist, begrenzen Sie den Speicherverbrauch w√§hrend des Erstellungsprozesses mit dem Flag <code>--local_resources=2048,.5,1.0</code> .  Informationen zu Flags finden Sie in der Hilfe zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tensorflow Serving und Docker</a> sowie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bazel-Dokumentation</a> . <br><br>  Erstellen Sie ein Arbeitsbild basierend auf dem vorhandenen: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  ModelServer wird mithilfe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow-Flags</a> konfiguriert, um die Parallelit√§t zu unterst√ºtzen.  Mit den folgenden Optionen werden zwei Thread-Pools f√ºr den Parallelbetrieb konfiguriert: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  steuert die maximale Anzahl von Threads f√ºr die parallele Ausf√ºhrung einer Operation; <br></li><li>  Wird verwendet, um Operationen mit Unteroperationen zu parallelisieren, die von Natur aus unabh√§ngig sind. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  steuert die maximale Anzahl von Threads f√ºr die parallele Ausf√ºhrung unabh√§ngiger Operationen; <br></li><li>  Tensorflow-Graph-Operationen, die unabh√§ngig voneinander sind und daher in verschiedenen Threads ausgef√ºhrt werden k√∂nnen. </li></ul><br>  Standardm√§√üig sind beide Parameter auf <code>0</code> .  Dies bedeutet, dass das System selbst die entsprechende Nummer ausw√§hlt, was meistens einen Thread pro Kern bedeutet.  Der Parameter kann jedoch f√ºr die Mehrkern-Parallelit√§t manuell ge√§ndert werden. <br><br>  F√ºhren Sie dann den Serving-Container auf die gleiche Weise wie den vorherigen aus, diesmal mit einem aus den Quellen kompilierten Docker-Image und mit Tensorflow-Optimierungsflags f√ºr einen bestimmten Prozessor: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Containerprotokolle sollten keine Warnungen mehr √ºber eine undefinierte CPU enthalten.  Ohne den Code bei derselben Prognoseanforderung zu √§ndern, wird die Verz√∂gerung um ca. 35,8% reduziert: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Erh√∂hen Sie die Geschwindigkeit bei der Kundenprognose </h3><br>  Kann man noch beschleunigen?  Wir haben die Serverseite f√ºr unsere CPU optimiert, aber eine Verz√∂gerung von mehr als 1 Sekunde scheint immer noch zu gro√ü. <br><br>  So kam es, dass das Laden der Bibliotheken <code>tensorflow_serving</code> und <code>tensorflow</code> einen wesentlichen Beitrag zur Verz√∂gerung leistet.  Jeder unn√∂tige Aufruf von <code>tf.contrib.util.make_tensor_proto</code> f√ºgt auch einen Sekundenbruchteil hinzu. <br><br>  Sie fragen sich m√∂glicherweise: "Ben√∂tigen wir keine TensorFlow Python-Pakete, um tats√§chlich Vorhersageanforderungen an den Tensorflow-Server zu senden?"  Tats√§chlich besteht kein wirklicher <i>Bedarf</i> an <code>tensorflow_serving</code> und <code>tensorflow</code> Paketen. <br><br>  Wie bereits erw√§hnt, sind die Tensorflow-Vorhersage-APIs als Protopuffer definiert.  Daher k√∂nnen zwei externe Abh√§ngigkeiten durch die entsprechenden <code>tensorflow_serving</code> <code>tensorflow</code> und " <code>tensorflow_serving</code> Anschlie√üend m√ºssen Sie nicht die gesamte (schwere) Tensorflow-Bibliothek auf dem Client abrufen. <br><br>  <code>tensorflow</code> <code>tensorflow_serving</code> <code>tensorflow</code> und <code>tensorflow_serving</code> und f√ºgen Sie das Paket <code>grpcio-tools</code> . <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  <code>tensorflow/tensorflow</code> die <code>tensorflow/tensorflow</code> und <code>tensorflow/serving</code> Repositorys und kopieren Sie die folgenden Protobuf-Dateien in das Client-Projekt: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Kopieren Sie diese Protobuf-Dateien mit den urspr√ºnglichen Pfaden in das Verzeichnis <code>protos/</code> : <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  Der Einfachheit halber kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prediction_service.proto</a> vereinfacht werden, um nur Predict RPC zu implementieren, um die verschachtelten Abh√§ngigkeiten anderer im Service angegebener RPCs nicht herunterzuladen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist</a> ein Beispiel f√ºr eine vereinfachte <code>prediction_service.</code> . <br><br>  Erstellen Sie Python-gRPC-Implementierungen mit <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Jetzt kann das gesamte Modul <code>tensorflow_serving</code> entfernt werden: <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... und durch die generierten Protobuffer aus <code>protos/tensorflow_serving/apis</code> ersetzen: <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  Die Tensorflow-Bibliothek wird importiert, um die <code>make_tensor_proto</code> , die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zum</a> Umschlie√üen eines Python / Numpy-Objekts als TensorProto-Objekt erforderlich ist. <br><br>  Somit k√∂nnen wir das folgende Abh√§ngigkeits- und Codefragment ersetzen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  Protobuffer importieren und ein TensorProto-Objekt erstellen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  Das vollst√§ndige Python-Skript finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> .  F√ºhren Sie einen aktualisierten Starter-Client aus, der eine Vorhersageanforderung f√ºr eine optimierte Tensorflow-Bedienung stellt: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  Das folgende Diagramm zeigt die prognostizierte Ausf√ºhrungszeit in der optimierten Version von Tensorflow Serving im Vergleich zum Standard √ºber 10 L√§ufe: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  Die durchschnittliche Verz√∂gerung verringerte sich um das 3,38-fache. <br><br><h1>  Bandbreitenoptimierung </h1><br>  Tensorflow Serving kann f√ºr die Verarbeitung gro√üer Datenmengen konfiguriert werden.  Die Bandbreitenoptimierung wird normalerweise f√ºr die "eigenst√§ndige" Stapelverarbeitung durchgef√ºhrt, bei der enge Latenzgrenzen keine strenge Anforderung sind. <br><br><h3>  Serverseitige Stapelverarbeitung </h3><br>  Wie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation angegeben</a> , wird die serverseitige Stapelverarbeitung in Tensorflow Serving nativ unterst√ºtzt. <br><br>  Die Kompromisse zwischen Latenz und Durchsatz werden durch Stapelverarbeitungsparameter bestimmt.  Mit ihnen k√∂nnen Sie den maximalen Durchsatz erzielen, den Hardwarebeschleuniger erreichen k√∂nnen. <br><br>  Um das <code>--enable_batching</code> zu aktivieren, setzen Sie die <code>--enable_batching</code> und <code>--batching_parameters_file</code> .  Die Parameter werden gem√§√ü <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SessionBundleConfig festgelegt</a> .  Setzen <code>num_batch_threads</code> f√ºr Systeme auf der CPU <code>num_batch_threads</code> auf die Anzahl der verf√ºgbaren Kerne.  Informationen zur GPU finden Sie <a href="">hier</a> . <br><br>  Nach dem Ausf√ºllen des gesamten Pakets auf der Serverseite werden die Ausgabeanforderungen zu einer gro√üen Anforderung (Tensor) zusammengefasst und mit einer kombinierten Anforderung an die Tensorflow-Sitzung gesendet.  In dieser Situation ist die CPU / GPU-Parallelit√§t wirklich involviert. <br><br>  Einige h√§ufige Anwendungen f√ºr die Tensorflow-Stapelverarbeitung: <br><br><ul><li>  Verwenden von asynchronen Clientanforderungen zum Auff√ºllen serverseitiger Pakete <br></li><li>  Schnellere Stapelverarbeitung durch √úbertragung der Komponenten des Modellgraphen auf die CPU / GPU <br></li><li>  Anfragen von mehreren Modellen von einem einzigen Server aus bearbeiten <br></li><li>  Die Stapelverarbeitung wird f√ºr die "Offline" -Verarbeitung einer gro√üen Anzahl von Anforderungen dringend empfohlen </li></ul><br><h3>  Clientseitige Stapelverarbeitung </h3><br>  Die clientseitige Stapelverarbeitung gruppiert mehrere eingehende Anforderungen zu einer. <br><br>  Da das ResNet-Modell auf die Eingabe im NHWC-Format wartet (die erste Dimension ist die Anzahl der Eingaben), k√∂nnen wir mehrere Eingabebilder zu einer RPC-Anforderung kombinieren: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  F√ºr ein Paket von N Bildern enth√§lt der Ausgangstensor in der Antwort die Vorhersageergebnisse f√ºr die gleiche Anzahl von Eingaben.  In unserem Fall ist N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Hardwarebeschleunigung </h1><br>  Ein paar Worte zu GPUs. <br><br>  Der Lernprozess verwendet nat√ºrlich die Parallelisierung auf der GPU, da der Aufbau tiefer neuronaler Netze umfangreiche Berechnungen erfordert, um die optimale L√∂sung zu erzielen. <br><br>  F√ºr die Ausgabe von Ergebnissen ist die Parallelisierung jedoch nicht so offensichtlich.  Oft k√∂nnen Sie die Ausgabe eines neuronalen Netzwerks an eine GPU beschleunigen, aber Sie m√ºssen die Ger√§te sorgf√§ltig ausw√§hlen und testen sowie eingehende technische und wirtschaftliche Analysen durchf√ºhren.  Hardware-Parallelisierung ist f√ºr die Stapelverarbeitung von "autonomen" Schlussfolgerungen (massive Volumina) wertvoller. <br><br>  Ber√ºcksichtigen Sie vor dem Wechsel zu einer GPU die Gesch√§ftsanforderungen mit einer sorgf√§ltigen Analyse der Kosten (monet√§r, betrieblich, technisch), um den gr√∂√üten Nutzen (reduzierte Latenz, hoher Durchsatz) zu erzielen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de445928/">https://habr.com/ru/post/de445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de445918/index.html">20 Jahre RollerCoaster Tycoon: Ein Interview mit dem Sch√∂pfer des Spiels</a></li>
<li><a href="../de445920/index.html">Live: Wie Sie die iOS-Entwicklung in gro√üen Teams eind√§mmen k√∂nnen</a></li>
<li><a href="../de445922/index.html">Warum Online-Sendungen ansehen, wenn Sie Habr lesen k√∂nnen?</a></li>
<li><a href="../de445924/index.html">SCH√ÑTZE: Wenn intelligente Uhren komisch werden</a></li>
<li><a href="../de445926/index.html">Das US-amerikanische UFO-Geheimprogramm hat auch Wurml√∂cher und zus√§tzliche Dimensionen untersucht.</a></li>
<li><a href="../de445932/index.html">Sicherheit von Clientanwendungen: Praktische Tipps f√ºr einen Front-End-Entwickler</a></li>
<li><a href="../de445936/index.html">Elektronikentwicklung. √úber Mikrocontroller an den Fingern</a></li>
<li><a href="../de445940/index.html">AMA mit Habr, v 7.0. Zitrone, Donuts und Nachrichten</a></li>
<li><a href="../de445946/index.html">MWC: Gebrauchsanweisung</a></li>
<li><a href="../de445948/index.html">Vererbung in C ++: Anf√§nger, Mittelstufe, Fortgeschrittene</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>