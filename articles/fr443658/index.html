<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèæ üë©üèΩ‚ÄçüöÄ üåò Configuration du cluster Kubernetes HA sur du m√©tal nu, surveillance, journaux et exemples d'utilisation. Partie 3/3 üíç üëåüèº üíô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Partie 1/3 ici . 


 Partie 2/3 ici . 


 Bonjour √† tous! Et voici la troisi√®me partie du guide Kubernetes on bare metal! Je ferai attention √† la surv...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configuration du cluster Kubernetes HA sur du m√©tal nu, surveillance, journaux et exemples d'utilisation. Partie 3/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443658/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  Partie 1/3 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p><br><p>  Partie 2/3 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </p><br><p>  Bonjour √† tous!  Et voici la troisi√®me partie du guide Kubernetes on bare metal!  Je ferai attention √† la surveillance du cluster et √† la collecte des journaux, nous lancerons √©galement une application de test pour utiliser les composants du cluster pr√©configur√©s.  Ensuite, nous effectuerons plusieurs tests de r√©sistance et v√©rifierons la stabilit√© de ce sch√©ma de cluster. </p><a name="habracut"></a><br><p>  L'outil le plus populaire que la communaut√© Kubernetes propose pour fournir une interface Web et obtenir des statistiques de cluster est le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>tableau de bord Kubernetes</strong></a> .  En fait, il est toujours en cours de d√©veloppement, mais m√™me maintenant, il peut fournir des donn√©es suppl√©mentaires pour r√©soudre les probl√®mes d'application et g√©rer les ressources du cluster. </p><br><p>  Le sujet est en partie controvers√©.  Est-il vrai que vous avez besoin d'une sorte d'interface Web pour g√©rer le cluster, ou est-ce suffisant pour utiliser l'outil de console <strong>kubectl</strong> ?  Eh bien, parfois ces options se compl√®tent. </p><br><p>  D√©veloppons notre <strong>tableau de bord Kubernetes</strong> et voyons.  Avec un d√©ploiement standard, ce tableau de bord ne d√©marre qu'√† l'adresse de l'h√¥te local.  Ainsi, vous devez utiliser la commande <strong>proxy kubectl</strong> pour l' <strong>expansion</strong> , mais elle n'est toujours disponible que sur votre p√©riph√©rique de contr√¥le kubectl local.  Pas mal du point de vue de la s√©curit√©, mais je veux avoir acc√®s dans le navigateur, en dehors du cluster, et je suis pr√™t √† prendre des risques (apr√®s tout, ssl avec un token efficace est utilis√©). </p><br><p>  Pour appliquer ma m√©thode, vous devez l√©g√®rement modifier le fichier de d√©ploiement standard dans la section service.  Pour ouvrir ce tableau de bord sur une adresse ouverte, nous utilisons notre √©quilibreur de charge. </p><br><p>  Nous entrons dans le syst√®me de la machine avec l'utilitaire <strong>kubectl</strong> configur√© et cr√©ons: </p><br><pre><code class="plaintext hljs">control# vi kube-dashboard.yaml # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the "License"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role &amp; Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [""] resources: ["secrets"] verbs: ["create"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [""] resources: ["configmaps"] verbs: ["create"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [""] resources: ["secrets"] resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"] verbs: ["get", "update", "delete"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [""] resources: ["configmaps"] resourceNames: ["kubernetes-dashboard-settings"] verbs: ["get", "update"] # Allow Dashboard to get metrics from heapster. - apiGroups: [""] resources: ["services"] resourceNames: ["heapster"] verbs: ["proxy"] - apiGroups: [""] resources: ["services/proxy"] resourceNames: ["heapster", "http:heapster:", "https:heapster:"] verbs: ["get"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimal subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: LoadBalancer ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard</code> </pre> <br><p>  Ex√©cutez ensuite: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f kube-dashboard.yaml control# kubectl get svc --namespace=kube-system kubernetes-dashboard LoadBalancer 10.96.164.141 192.168.0.240 443:31262/TCP 8h</code> </pre> <br><p>  Eh bien, comme vous pouvez le voir, notre BN a ajout√© l'IP 192.168.0.240 pour ce service.  Essayez maintenant d'ouvrir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://192.168.0.240</a> pour afficher le tableau de bord Kubernetes. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ml/pc/dh/mlpcdhrwki9fh_pzaa7by9mq7i8.png"></a> </p><br><p>  Il y a 2 fa√ßons d'acc√©der: utilisez le fichier <code>admin.conf</code> de notre n≈ìud ma√Ætre, que nous avons utilis√© pr√©c√©demment lors de la configuration de kubectl, ou cr√©ez un compte de service sp√©cial avec un jeton de s√©curit√©. </p><br><p>  Cr√©ons un utilisateur administrateur: </p><br><pre> <code class="plaintext hljs">control# vi kube-dashboard-admin.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system control# kubectl create -f kube-dashboard-admin.yaml serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created</code> </pre> <br><p>  Vous avez maintenant besoin d'un jeton pour entrer dans le syst√®me: </p><br><pre> <code class="plaintext hljs">control# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}') Name: admin-user-token-vfh66 Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: 3775471a-3620-11e9-9800-763fc8adcb06 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: erJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwna3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJr dWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VmLXRva2VuLXZmaDY2Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZ XJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIzNzc1NDcxYS0zNjIwLTExZTktOTgwMC03Nj NmYzhhZGNiMDYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.JICASwxAJHFX8mLoSikJU1tbij4Kq2pneqAt6QCcGUizFLeSqr2R5x339ZR8W4 9cIsbZ7hbhFXCATQcVuWnWXe2dgXP5KE8ZdW9uvq96rm_JvsZz0RZO03UFRf8Exsss6GLeRJ5uNJNCAr8No5pmRMJo-_4BKW4OFDFxvSDSS_ZJaLMqJ0LNpwH1Z09SfD8TNW7VZqax4zuTSMX_yVS ts40nzh4-_IxDZ1i7imnNSYPQa_Oq9ieJ56Q-xuOiGu9C3Hs3NmhwV8MNAcniVEzoDyFmx4z9YYcFPCDIoerPfSJIMFIWXcNlUTPSMRA-KfjSb_KYAErVfNctwOVglgCISA</code> </pre> <br><p>  Copiez le jeton et collez-le dans le champ du jeton sur l'√©cran de connexion. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/mr/yz/8x/mryz8xkchmzk4mx-wcxd-9jh4c8.png"></a> </p><br><p>  Apr√®s √™tre entr√© dans le syst√®me, vous pouvez √©tudier le cluster un peu plus en profondeur - j'aime cet outil. <br>  La prochaine √©tape vers l'approfondissement du syst√®me de surveillance de notre cluster est d'installer le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>tas</strong></a> . </p><br><blockquote>  Heapster vous permet de surveiller le cluster de conteneurs et d'analyser les performances de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes</a> (version v1.0.6 et sup√©rieure).  Il propose des plateformes appropri√©es. </blockquote><p>  Cet outil offre des statistiques sur l'utilisation du cluster via la console et ajoute √©galement plus d'informations sur les ressources de n≈ìuds et de foyer au tableau de bord Kubernetes. </p><br><p>  Il n'y a pas de difficult√© √† l'installer sur du m√©tal nu, et j'avais besoin de mener une enqu√™te: pourquoi l'outil ne fonctionne pas dans la version d'origine, mais j'ai trouv√© une solution. </p><br><p>  Continuons donc et approuvons ce module compl√©mentaire: </p><br><pre> <code class="plaintext hljs">control# vi heapster.yaml apiVersion: v1 kind: ServiceAccount metadata: name: heapster namespace: kube-system --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: heapster namespace: kube-system spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster image: gcr.io/google_containers/heapster-amd64:v1.4.2 imagePullPolicy: IfNotPresent command: - /heapster - --source=kubernetes.summary_api:''?useServiceAccount=true&amp;kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true --- apiVersion: v1 kind: Service metadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: Heapster name: heapster namespace: kube-system spec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapster --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapster subjects: - kind: ServiceAccount name: heapster namespace: kube-system</code> </pre> <br><p>  Il s'agit du fichier de d√©ploiement standard le plus courant de la communaut√© Heapster, avec seulement une l√©g√®re diff√©rence: pour le faire fonctionner sur notre cluster, la ligne " <strong>source =</strong> " dans le d√©ploiement heapster est modifi√©e comme suit: </p><br><pre> <code class="plaintext hljs">--source=kubernetes.summary_api:''?useServiceAccount=true&amp;kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true</code> </pre> <br><p>  Dans cette <a href="">description,</a> vous trouverez toutes ces options.  J'ai chang√© le port kubelet en 10250 et d√©sactiv√© la v√©rification du certificat SSL (il y avait un petit probl√®me avec √ßa). </p><br><p>  Nous devons √©galement ajouter des autorisations pour obtenir des statistiques sur les n≈ìuds dans le r√¥le RBAC Heapster;  ajoutez ces quelques lignes √† la fin du r√¥le: </p><br><pre> <code class="plaintext hljs">control# kubectl edit clusterrole system:heapster ...... ... - apiGroups: - "" resources: - nodes/stats verbs: - get</code> </pre> <br><p>  En r√©sum√©, votre r√¥le RBAC devrait ressembler √† ceci: </p><br><pre> <code class="plaintext hljs"># Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" creationTimestamp: "2019-02-22T18:58:32Z" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:heapster resourceVersion: "6799431" selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Aheapster uid: d99065b5-36d3-11e9-a7e6-763fc8adcb06 rules: - apiGroups: - "" resources: - events - namespaces - nodes - pods verbs: - get - list - watch - apiGroups: - extensions resources: - deployments verbs: - get - list - watch - apiGroups: - "" resources: - nodes/stats verbs: - get</code> </pre> <br><p>  Ok, ex√©cutons maintenant la commande pour nous assurer que le d√©ploiement du tas est correctement lanc√©. </p><br><pre> <code class="plaintext hljs">control# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% kube-master1 183m 9% 1161Mi 60% kube-master2 235m 11% 1130Mi 59% kube-worker1 189m 4% 1216Mi 41% kube-worker2 218m 5% 1290Mi 44% kube-worker3 181m 4% 1305Mi 44%</code> </pre> <br><p>  Eh bien, si vous avez re√ßu des donn√©es sur la sortie, tout se fait correctement.  Revenons √† notre page de tableau de bord et jetons un ≈ìil aux nouveaux graphiques qui sont d√©sormais disponibles. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/gj/rt/wu/gjrtwuo1wlzl0ixrqazhakojfdo.jpeg"></a> <br> <a href=""><img src="https://habrastorage.org/webt/q1/z1/n3/q1z1n3m1jqjmqiid7a2bgdfdra4.jpeg"></a> </p><br><p>  D√©sormais, nous pouvons √©galement suivre l'utilisation r√©elle des ressources pour les n≈ìuds de cluster, les foyers, etc. </p><br><p>  Si cela ne suffit pas, vous pouvez encore am√©liorer les statistiques en ajoutant InfluxDB + Grafana.  Cela ajoutera la possibilit√© de dessiner vos propres panneaux Grafana. </p><br><p>  Nous utiliserons cette version de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">installation InfluxDB + Grafana √†</a> partir de la page Heapster Git, mais, comme d'habitude, nous apporterons des corrections.  Comme nous avions d√©j√† configur√© le d√©ploiement de tas, nous avons seulement besoin d'ajouter Grafana et InfluxDB, puis de modifier le d√©ploiement de tas existant afin qu'il place √©galement des mesures dans Influx. </p><br><p>  Ok, cr√©ons les d√©ploiements InfluxDB et Grafana: </p><br><pre> <code class="plaintext hljs">control# vi influxdb.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: monitoring-influxdb namespace: kube-system spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb image: k8s.gcr.io/heapster-influxdb-amd64:v1.5.2 volumeMounts: - mountPath: /data name: influxdb-storage volumes: - name: influxdb-storage emptyDir: {} --- apiVersion: v1 kind: Service metadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-system spec: ports: - port: 8086 targetPort: 8086 selector: k8s-app: influxdb</code> </pre> <br><p>  Ensuite, Grafana, et n'oubliez pas de modifier les param√®tres du service pour activer l'√©quilibreur de charge MetaLB et obtenir l'adresse IP externe pour le service Grafana. </p><br><pre> <code class="plaintext hljs">control# vi grafana.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: monitoring-grafana namespace: kube-system spec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /etc/ssl/certs name: ca-certificates readOnly: true - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST value: monitoring-influxdb - name: GF_SERVER_HTTP_PORT value: "3000" # The following env variables are required to make Grafana accessible via # the kubernetes api-server proxy. On production clusters, we recommend # removing these env variables, setup auth for grafana, and expose the grafana # service using a LoadBalancer or a public IP. - name: GF_AUTH_BASIC_ENABLED value: "false" - name: GF_AUTH_ANONYMOUS_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL # If you're only using the API Server proxy, set this value instead: # value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy value: / volumes: - name: ca-certificates hostPath: path: /etc/ssl/certs - name: grafana-storage emptyDir: {} --- apiVersion: v1 kind: Service metadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-system spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port # type: NodePort type: LoadBalancer ports: - port: 80 targetPort: 3000 selector: k8s-app: grafana</code> </pre> <br><p>  Et cr√©ez-les: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f influxdb.yaml deployment.extensions/monitoring-influxdb created service/monitoring-influxdb created control# kubectl create -f grafana.yaml deployment.extensions/monitoring-grafana created service/monitoring-grafana created</code> </pre> <br><p>  Il est temps de changer le d√©ploiement du tas et d'y ajouter la connexion InfluxDB;  vous devez ajouter une seule ligne: </p><br><pre> <code class="plaintext hljs">- --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</code> </pre> <br><p>  Modifiez le d√©ploiement du tas: </p><br><pre> <code class="plaintext hljs">control# kubectl get deployments --namespace=kube-system NAME READY UP-TO-DATE AVAILABLE AGE coredns 2/2 2 2 49d heapster 1/1 1 1 2d12h kubernetes-dashboard 1/1 1 1 3d21h monitoring-grafana 1/1 1 1 115s monitoring-influxdb 1/1 1 1 2m18s control# kubectl edit deployment heapster --namespace=kube-system ... beginning bla bla bla spec: containers: - command: - /heapster - --source=kubernetes.summary_api:''?useServiceAccount=true&amp;kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086 image: gcr.io/google_containers/heapster-amd64:v1.4.2 imagePullPolicy: IfNotPresent .... end</code> </pre> <br><p>  Trouvez maintenant l'adresse IP externe du service Grafana et connectez-vous au syst√®me √† l'int√©rieur: </p><br><pre> <code class="plaintext hljs">control# kubectl get svc --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ..... some other services here monitoring-grafana LoadBalancer 10.98.111.200 192.168.0.241 80:32148/TCP 18m</code> </pre> <br><p>  Ouvrez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://192.168.0.241</a> dans un navigateur, pour la premi√®re fois utilisez les informations d'identification admin / admin: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/mt/l9/lg/mtl9lgxajgzy671ayddlz4hfuku.jpeg"></a> </p><br><p>  Lorsque je me suis connect√©, mon Grafana √©tait vide, mais, heureusement, nous pouvons obtenir tous les tableaux de bord n√©cessaires sur <strong>grafana.com</strong> .  Vous devez importer les panneaux n ¬∞ 3649 et 3646. Lors de l'importation, s√©lectionnez la source de donn√©es appropri√©e. </p><br><p>  Apr√®s cela, surveillez l'utilisation des ressources des n≈ìuds et des foyers et, bien s√ªr, cr√©ez vos propres tableaux de bord uniques. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/vp/nm/9a/vpnm9awb7sfgn5pv-0qo4-srzxu.jpeg"></a> <br> <a href=""><img src="https://habrastorage.org/webt/1i/pr/yi/1ipryi2agtmgqfm2kwzylfsteay.jpeg"></a> </p><br><p>  Eh bien, pour l'instant, terminons avec la surveillance;  Les √©l√©ments suivants dont nous pourrions avoir besoin sont les journaux pour stocker nos applications et le cluster.  Il existe plusieurs fa√ßons de l'impl√©menter, et elles sont toutes d√©crites dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation de</a> Kubernetes.  Sur la base de ma propre exp√©rience, je pr√©f√®re utiliser des installations externes des services Elasticsearch et Kibana, ainsi que des agents d'enregistrement qui s'ex√©cutent sur chaque n≈ìud de travail Kubernetes.  Cela prot√©gera le cluster des surcharges associ√©es √† un grand nombre de journaux et autres probl√®mes, et permettra de recevoir des journaux, m√™me si le cluster devient compl√®tement compl√®tement non fonctionnel. </p><br><p>  La pile de collecte de journaux la plus populaire pour les fans de Kubernetes est Elasticsearch, Fluentd et Kibana (pile EFK).  Dans cet exemple, nous ex√©cuterons Elasticsearch et Kibana sur un n≈ìud externe (vous pouvez utiliser la pile ELK existante), ainsi que Fluentd √† l'int√©rieur de notre cluster en tant que jeu de d√©mons pour chaque n≈ìud en tant qu'agent de collecte de journaux. </p><br><p>  Je vais sauter la partie sur la cr√©ation d'une machine virtuelle avec des installations Elasticsearch et Kibana;  C'est un sujet assez populaire, vous pouvez donc trouver beaucoup de mat√©riel sur la meilleure fa√ßon de le faire.  Par exemple, dans mon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> .  Supprimez simplement le <strong>fragment</strong> de configuration <strong>logstash du fichier docker-compose.yml</strong> et supprimez √©galement 127.0.0.1 de la section des ports elasticsearch. </p><br><p>  Apr√®s cela, vous devriez avoir un elasticsearch fonctionnel connect√© au port VM-IP: 9200.  Pour plus de s√©curit√©, configurez login: pass ou cl√©s de s√©curit√© entre fluentd et elasticsearch.  Cependant, je les prot√®ge souvent simplement avec des r√®gles iptables. </p><br><p>  Il ne reste plus qu'√† cr√©er un daemonset fluentd dans Kubernetes et √† sp√©cifier l'adresse externe du <strong>noeud</strong> elasticsearch <strong>: port</strong> dans la configuration. </p><br><p>  Nous utilisons l'add-on Kubernetes officiel avec la configuration yaml <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d'ici</a> , avec des modifications mineures: </p><br><pre> <code class="plaintext hljs">control# vi fluentd-es-ds.yaml apiVersion: v1 kind: ServiceAccount metadata: name: fluentd-es namespace: kube-system labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - "" resources: - "namespaces" - "pods" verbs: - "get" - "watch" - "list" --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: fluentd-es labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: fluentd-es namespace: kube-system apiGroup: "" roleRef: kind: ClusterRole name: fluentd-es apiGroup: "" --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-es-v2.4.0 namespace: kube-system labels: k8s-app: fluentd-es version: v2.4.0 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: fluentd-es version: v2.4.0 template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" version: v2.4.0 # This annotation ensures that fluentd does not get evicted if the node # supports critical pod annotation based priority scheme. # Note that this does not guarantee admission on the nodes (#40573). annotations: scheduler.alpha.kubernetes.io/critical-pod: '' seccomp.security.alpha.kubernetes.io/pod: 'docker/default' spec: priorityClassName: system-node-critical serviceAccountName: fluentd-es containers: - name: fluentd-es image: k8s.gcr.io/fluentd-elasticsearch:v2.4.0 env: - name: FLUENTD_ARGS value: --no-supervisor -q resources: limits: memory: 500Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: config-volume mountPath: /etc/fluent/config.d terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: config-volume configMap: name: fluentd-es-config-v0.2.0</code> </pre> <br><p>  Ensuite, nous allons faire une configuration sp√©cifique de fluentd: </p><br><pre> <code class="plaintext hljs">control# vi fluentd-es-configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: fluentd-es-config-v0.2.0 namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile data: system.conf: |- &lt;system&gt; root_dir /tmp/fluentd-buffers/ &lt;/system&gt; containers.input.conf: |-</code> </pre> <br><pre> <code class="plaintext hljs"> @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos tag raw.kubernetes.* read_from_head true &lt;parse&gt; @type multi_format &lt;pattern&gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ &lt;/pattern&gt; &lt;pattern&gt; format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z &lt;/pattern&gt; &lt;/parse&gt;</code> </pre> <br><pre> <code class="plaintext hljs"># Detect exceptions in the log output and forward them as one log entry. &lt;match raw.kubernetes.**&gt; @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 &lt;/match&gt; # Concatenate multi-line logs &lt;filter **&gt; @id filter_concat @type concat key message multiline_end_regexp /\n$/ separator "" &lt;/filter&gt; # Enriches records with Kubernetes metadata &lt;filter kubernetes.**&gt; @id filter_kubernetes_metadata @type kubernetes_metadata &lt;/filter&gt; # Fixes json fields in Elasticsearch &lt;filter kubernetes.**&gt; @id filter_parser @type parser key_name log reserve_data true remove_key_name_field true &lt;parse&gt; @type multi_format &lt;pattern&gt; format json &lt;/pattern&gt; &lt;pattern&gt; format none &lt;/pattern&gt; &lt;/parse&gt; &lt;/filter&gt; output.conf: |- &lt;match **&gt; @id elasticsearch @type elasticsearch @log_level info type_name _doc include_tag_key true host 192.168.1.253 port 9200 logstash_format true &lt;buffer&gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block &lt;/buffer&gt; &lt;/match&gt;</code> </pre> <br><p>  La configuration est √©l√©mentaire, mais elle suffit amplement pour un d√©marrage rapide;  il collectera les journaux du syst√®me et des applications.  Si vous avez besoin de quelque chose de plus compliqu√©, vous pouvez consulter la documentation officielle sur les plugins fluentd et les configurations Kubernetes. </p><br><p>  Cr√©ons maintenant un daemonset fluentd dans notre cluster: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f fluentd-es-ds.yaml serviceaccount/fluentd-es created clusterrole.rbac.authorization.k8s.io/fluentd-es created clusterrolebinding.rbac.authorization.k8s.io/fluentd-es created daemonset.apps/fluentd-es-v2.4.0 created control# kubectl create -f fluentd-es-configmap.yaml configmap/fluentd-es-config-v0.2.0 created</code> </pre> <br><p>  Assurez-vous que tous les pods fluentd et autres ressources fonctionnent correctement, puis ouvrez Kibana.  Dans Kibana, recherchez et ajoutez un nouvel index de fluentd.  Si vous trouvez quelque chose, alors tout est fait correctement, sinon, v√©rifiez les √©tapes pr√©c√©dentes et recr√©ez le daemonset ou √©ditez le configmap: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/zg/o_/og/zgo_ogau6dqddp69ffnqxtvzhwi.jpeg"></a> <br> <a href=""><img src="https://habrastorage.org/webt/cr/wj/fa/crwjfahq7fsbizu40xvf8_7xf90.jpeg"></a> </p><br><p>  Eh bien, maintenant que nous obtenons les journaux du cluster, vous pouvez cr√©er des tableaux de bord.  Bien s√ªr, la configuration est la plus simple, vous devez donc probablement la modifier par vous-m√™me.  L'objectif principal √©tait de montrer comment cela se fait. </p><br><p>  Apr√®s avoir termin√© toutes les √©tapes pr√©c√©dentes, nous avons obtenu un tr√®s bon cluster Kubernetes pr√™t √† l'emploi.  Il est temps d'int√©grer une application de test et de voir ce qui se passe. </p><br><p>  Pour cet exemple, prenez ma petite application Python / Flask Kubyk, qui poss√®de d√©j√† un conteneur Docker, alors prenez-la dans le registre ouvert Docker.  Nous allons maintenant ajouter un fichier de base de donn√©es externe √† cette application - pour cela, nous utiliserons le stockage GlusterFS configur√©. </p><br><p>  Tout d'abord, nous cr√©ons un nouveau volume <strong>pvc</strong> pour cette application (demande de volume permanent), o√π nous allons stocker la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">base de</a> donn√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SQLite</a> avec les informations d'identification de l'utilisateur.  Vous pouvez utiliser la classe de m√©moire pr√©-cr√©√©e de la partie 2 de ce guide. </p><br><pre> <code class="plaintext hljs">control# mkdir kubyk &amp;&amp; cd kubyk control# vi kubyk-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: kubyk annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi control# kubectl create -f kubyk-pvc.yaml</code> </pre> <br><p>  Ayant cr√©√© un nouveau PVC pour l'application, nous sommes pr√™ts pour le d√©ploiement. </p><br><pre> <code class="plaintext hljs">control# vi kubyk-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kubyk-deployment spec: selector: matchLabels: app: kubyk replicas: 1 template: metadata: labels: app: kubyk spec: containers: - name: kubyk image: ratibor78/kubyk ports: - containerPort: 80 volumeMounts: - name: kubyk-db mountPath: /kubyk/sqlite volumes: - name: kubyk-db persistentVolumeClaim: claimName: kubyk control# vi kubyk-service.yaml apiVersion: v1 kind: Service metadata: name: kubyk spec: type: LoadBalancer selector: app: kubyk ports: - port: 80 name: http</code> </pre> <br><p>  Cr√©ons maintenant un d√©ploiement et un service: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f kubyk-deploy.yaml deployment.apps/kubyk-deployment created control# kubectl create -f kubyk-service.yaml service/kubyk created</code> </pre> <br><p>  V√©rifiez la nouvelle adresse IP attribu√©e au service, ainsi que le statut du sous: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE glusterfs-2wxk7 1/1 Running 1 2d1h glusterfs-5dtdj 1/1 Running 1 41d glusterfs-zqxwt 1/1 Running 0 2d1h heketi-b8c5f6554-f92rn 1/1 Running 0 8d kubyk-deployment-75d5447d46-jrttp 1/1 Running 0 11s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... some text.. kubyk LoadBalancer 10.109.105.224 192.168.0.242 80:32384/TCP 10s</code> </pre> <br><p>  Il semble donc que nous ayons lanc√© avec succ√®s une nouvelle application;  si nous ouvrons l'adresse IP <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://192.168.0.242</a> dans le navigateur, nous devrions voir la page de connexion de cette application.  Vous pouvez utiliser les informations d'identification admin / admin pour vous connecter, mais si nous essayons de nous connecter √† ce stade, nous obtiendrons une erreur car aucune base de donn√©es n'est encore disponible. </p><br><p>  Voici un exemple de message d'erreur de journal du foyer dans le tableau de bord Kubernetes: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/st/7c/ne/st7cnedlbkybwhj2k_7v3xviiia.jpeg"></a> </p><br><p>  Pour r√©soudre ce probl√®me, vous devez copier le fichier DB SQlite de mon r√©f√©rentiel git vers le volume pvc pr√©c√©demment cr√©√©.  L'application commencera √† utiliser cette base de donn√©es. </p><br><pre> <code class="plaintext hljs">control# git pull https://github.com/ratibor78/kubyk.git control# kubectl cp ./kubyk/sqlite/database.db kubyk-deployment-75d5447d46-jrttp:/kubyk/sqlite</code> </pre> <br><p>  Nous utilisons le sous de l'application et la commande <strong>kubectl cp</strong> pour copier ce fichier sur le volume. <br>  Vous devez √©galement donner √† l'utilisateur nginx un acc√®s en √©criture √† ce r√©pertoire;  mon application est lanc√©e via l'utilisateur nginx √† l'aide de <a href="">supervisord</a> . </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti kubyk-deployment-75d5447d46-jrttp -- chown -R nginx:nginx /kubyk/sqlite/</code> </pre> <br><p>  Essayons de nous reconnecter: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/79/jt/u_/79jtu_nd4nmednpk1nb2ycys9ss.jpeg"></a> </p><br><p>  Tr√®s bien, maintenant notre application fonctionne correctement, et nous pouvons faire √©voluer le <strong>d√©ploiement de</strong> kubyk sur 3 r√©pliques, par exemple, pour placer une copie de l'application dans un n≈ìud de travail.  Puisque nous avons pr√©c√©demment cr√©√© le volume pvc, tous nos pods avec des r√©pliques d'applications utiliseront la m√™me base de donn√©es, et le service r√©partira ainsi le trafic entre les r√©pliques de mani√®re circulaire. </p><br><pre> <code class="plaintext hljs">control# kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE heketi 1/1 1 1 39d kubyk-deployment 1/1 1 1 4h5m control# kubectl scale deployments kubyk-deployment --replicas=3 deployment.extensions/kubyk-deployment scaled control# kubectl get po NAME READY STATUS RESTARTS AGE glusterfs-2wxk7 1/1 Running 1 2d5h glusterfs-5dtdj 1/1 Running 21 41d glusterfs-zqxwt 1/1 Running 0 2d5h heketi-b8c5f6554-f92rn 1/1 Running 0 8d kubyk-deployment-75d5447d46-bdnqx 1/1 Running 0 26s kubyk-deployment-75d5447d46-jrttp 1/1 Running 0 4h7m kubyk-deployment-75d5447d46-wz9xz 1/1 Running 0 26s</code> </pre> <br><p> <a href=""><img src="https://habrastorage.org/webt/oq/3l/6k/oq3l6kxibffrw5_gwa1d5edarh8.jpeg"></a> </p><br><p>  Nous avons maintenant des r√©pliques d'application pour chaque n≈ìud actif, de sorte que l'application ne cessera pas de fonctionner si elle perd des n≈ìuds.  De plus, nous obtenons un moyen simple d'√©quilibrer la charge, comme je l'ai dit plus t√¥t.  Pas un mauvais endroit pour commencer. </p><br><p>  Cr√©ons un nouvel utilisateur dans notre application: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/94/d4/a1/94d4a1414x2ksjt9c8qzzxd_szw.jpeg"></a> <br> <a href=""><img src="https://habrastorage.org/webt/rz/sl/pc/rzslpcgucf593ooo2cetibyfdra.jpeg"></a> </p><br><p>  Toutes les nouvelles demandes seront trait√©es par le prochain foyer de la liste.  Cela peut √™tre v√©rifi√© par les journaux des foyers.  Par exemple, un nouvel utilisateur est cr√©√© par l'application dans un sous, puis le sous suivant r√©pond √† la requ√™te suivante, et ainsi de suite.  √âtant donn√© que cette application utilise un seul volume persistant pour stocker la base de donn√©es, toutes les donn√©es seront en s√©curit√© m√™me si toutes les r√©plicas sont perdues. </p><br><p>  Dans les applications volumineuses et complexes, vous aurez besoin non seulement d'un volume d√©sign√© pour la base de donn√©es, mais de divers volumes pour accueillir les informations persistantes et de nombreux autres √©l√©ments. </p><br><p>  Eh bien, nous avons presque fini.  Vous pouvez ajouter beaucoup plus d'aspects, car Kubernetes est un sujet volumineux et dynamique, mais nous nous arr√™terons l√†.  L'objectif principal de cette s√©rie d'articles √©tait de montrer comment cr√©er votre propre cluster Kubernetes, et j'esp√®re que ces informations vous ont √©t√© utiles. </p><br><h3 id="p-s">  PS </h3><br><p>  Des tests de stabilit√© et des tests de r√©sistance, bien s√ªr. </p><br><p>  Le diagramme de cluster de notre exemple fonctionne sans 2 n≈ìuds de travail, 1 n≈ìuds ma√Ætres et 1 n≈ìuds etcd.  Si vous le souhaitez, d√©sactivez-les et v√©rifiez si l'application de test fonctionnera. </p><br><p>  En compilant ces guides, j'ai pr√©par√© un cluster de production pour un sch√©ma presque similaire.  Une fois, apr√®s avoir cr√©√© un cluster et y avoir d√©ploy√© une application, j'ai rencontr√© une panne de courant majeure;  absolument tous les serveurs du cluster ont √©t√© coup√©s - un cauchemar anim√© de l'administrateur syst√®me.  Certains serveurs se sont arr√™t√©s pendant longtemps, puis des erreurs de syst√®me de fichiers se sont produites sur eux.  Mais la relance m'a beaucoup surpris: le cluster Kubernetes s'est compl√®tement r√©tabli.  Tous les volumes et d√©ploiements de GlusterFS ont √©t√© lanc√©s.  Pour moi, c'est une d√©monstration du grand potentiel de cette technologie. </p><br><p>  Bien √† vous et, j'esp√®re, √† bient√¥t! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr443658/">https://habr.com/ru/post/fr443658/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr443648/index.html">Documentation utilisateur: qu'est-ce qui la rend mauvaise et comment y rem√©dier</a></li>
<li><a href="../fr443650/index.html">Pas du tout et pas de tic. Quelle est la diff√©rence entre les processeurs Intel Core de diff√©rentes g√©n√©rations bas√©s sur une architecture</a></li>
<li><a href="../fr443652/index.html">Chaque jour sur Outlook j'ai juste de la farine ...</a></li>
<li><a href="../fr443654/index.html">Suivre la trace des calculatrices: Qalculate</a></li>
<li><a href="../fr443656/index.html">Sur les traces des calculatrices: Qalculate</a></li>
<li><a href="../fr443660/index.html">Experts: ¬´Un scanner 3D co√ªtera 10 fois moins cher qu'une erreur avec un contr√¥le qualit√© traditionnel¬ª</a></li>
<li><a href="../fr443662/index.html">Comprendre le code propre sur Android</a></li>
<li><a href="../fr443664/index.html">Station m√©t√©o Arduino</a></li>
<li><a href="../fr443666/index.html">Notre approche de la coloration des fils</a></li>
<li><a href="../fr443668/index.html">Retour aux microservices avec Istio. 3e partie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>