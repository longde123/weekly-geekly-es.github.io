<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòº üè¥‚Äç‚ò†Ô∏è üëºüèº L√ºcken in einem Kubernetes-Cluster schlie√üen. Bericht und Transkription mit DevOpsConf üë®‚Äçüëß‚Äçüëß üéØ üëµüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pavel Selivanov, Southbridge Solution Architect und Slurm Lecturer, hielt auf der DevOpsConf 2019 einen Vortrag. Dieser Bericht ist Teil des ausf√ºhrli...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>L√ºcken in einem Kubernetes-Cluster schlie√üen. Bericht und Transkription mit DevOpsConf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/472484/"><p>  Pavel Selivanov, Southbridge Solution Architect und Slurm Lecturer, hielt auf der DevOpsConf 2019 einen Vortrag. Dieser Bericht ist Teil des ausf√ºhrlichen Slur Mega-Kurses von Kubernetes. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Basic: Eine Einf√ºhrung in Kubernetes</a> findet vom 18. bis 20. November in Moskau statt. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Mega: Wir schauen unter die Haube von Kubernetes</a> - Moskau, 22.-24. November. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Online: Beide Kubernetes-Kurse sind immer</a> verf√ºgbar. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Gt4Q1du5FXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Unter dem Schnittprotokoll des Berichts. </p><a name="habracut"></a><br><p>  Guten Tag, Kollegen und Sympathisanten.  Heute werde ich √ºber Sicherheit sprechen. </p><br><p>  Ich sehe, dass heute viele Sicherheitskr√§fte in der Halle sind.  Ich entschuldige mich im Voraus bei Ihnen, wenn ich die Begriffe aus der Sicherheitswelt nicht so verwende, wie Sie es akzeptiert haben. </p><br><p>  So kam es, dass ich vor ungef√§hr sechs Monaten in die H√§nde eines √∂ffentlichen Kubernetes-Clusters geriet.  √ñffentlich - bedeutet, dass es eine n-te Anzahl von Namespaces gibt. In diesem Namespace befinden sich Benutzer, die in ihrem Namespace isoliert sind.  Alle diese Benutzer geh√∂ren verschiedenen Unternehmen an.  Nun, es wurde angenommen, dass dieser Cluster als CDN verwendet werden sollte.  Das hei√üt, sie geben Ihnen einen Cluster, sie geben dem Benutzer dort, Sie gehen in Ihrem Namespace dorthin und stellen Ihre Fronten bereit. </p><br><p>  Sie haben versucht, einen solchen Service an meine vorherige Firma zu verkaufen.  Und ich wurde gebeten, einen Cluster zu diesem Thema zu erstellen - ist diese L√∂sung geeignet oder nicht? </p><br><p>  Ich bin zu diesem Cluster gekommen.  Ich erhielt begrenzte Rechte, begrenzten Namespace.  Dort verstanden die Jungs, was Sicherheit ist.  Sie haben gelesen, was Kubernetes √ºber eine rollenbasierte Zugriffskontrolle (RBAC) verf√ºgt - und sie haben sie verdreht, damit ich Pods nicht getrennt von der Bereitstellung ausf√ºhren kann.  Ich erinnere mich nicht an die Aufgabe, die ich l√∂sen wollte, indem ich ohne Bereitstellung ausgef√ºhrt wurde, aber ich wollte wirklich nur unter ausf√ºhren.  Ich habe mich f√ºr viel Gl√ºck entschieden, um zu sehen, welche Rechte ich im Cluster habe, was ich kann, was ich nicht kann, was sie dort vermasselt haben.  Gleichzeitig werde ich Ihnen mitteilen, was sie im RBAC falsch konfiguriert haben. </p><br><p>  So kam es, dass ich zwei Minuten sp√§ter einen Administrator f√ºr ihren Cluster bekam, mir alle benachbarten Namespaces ansah, die Produktionsfronten von Unternehmen sah, die den Service bereits gekauft hatten und dort feststeckten.  Ich hielt mich kaum auf, um nicht zu jemandem in der Front zu kommen und kein obsz√∂nes Wort auf die Hauptseite zu setzen. </p><br><p>  Ich werde Ihnen anhand von Beispielen erz√§hlen, wie ich das gemacht habe und wie ich mich davor sch√ºtzen kann. </p><br><p>  Aber stell mich zuerst vor.  Ich hei√üe Pavel Selivanov.  Ich bin Architekt bei Southbridge.  Ich verstehe Kubernetes, DevOps und alle m√∂glichen ausgefallenen Sachen.  Die Ingenieure von Southbridge und ich bauen das alles und ich rate. </p><br><p>  Zus√§tzlich zu unserem Kerngesch√§ft haben wir k√ºrzlich Projekte namens Slory gestartet.  Wir versuchen, unsere F√§higkeit, mit Kubernetes zu arbeiten, der Masse nahe zu bringen und anderen Menschen beizubringen, wie man auch mit K8s arbeitet. </p><br><p> Wor√ºber ich heute sprechen werde.  Das Thema des Berichts liegt auf der Hand - √ºber die Sicherheit des Kubernetes-Clusters.  Aber ich m√∂chte sofort sagen, dass dieses Thema sehr gro√ü ist - und deshalb m√∂chte ich sofort festlegen, wor√ºber ich nicht sicher erz√§hlen werde.  Ich werde nicht √ºber abgedroschene Begriffe sprechen, die im Internet bereits hundertmal √ºberstrapaziert sind.  Alle RBAC und Zertifikate. </p><br><p>  Ich werde dar√ºber sprechen, wie es mir und meinen Kollegen aus Sicherheitsgr√ºnden im Kubernetes-Cluster weh tut.  Wir sehen diese Probleme sowohl bei Anbietern, die Kubernetes-Cluster bereitstellen, als auch bei Kunden, die zu uns kommen.  Und selbst bei Kunden, die von anderen Beratungsunternehmen zu uns kommen.  Das hei√üt, das Ausma√ü der Trag√∂die ist in der Tat sehr gro√ü. </p><br><p>  Buchst√§blich drei Punkte, √ºber die ich heute sprechen werde: </p><br><ol><li>  Benutzerrechte gegen Pod-Rechte.  Benutzer- und Herdrechte sind nicht dasselbe. </li><li>  Sammlung von Clusterinformationen.  Ich werde zeigen, dass Sie im Cluster alle Informationen sammeln k√∂nnen, die Sie ben√∂tigen, ohne √ºber besondere Rechte in diesem Cluster zu verf√ºgen. </li><li>  DoS-Angriff auf den Cluster.  Wenn wir keine Informationen sammeln k√∂nnen, k√∂nnen wir den Cluster auf jeden Fall platzieren.  Ich werde √ºber DoS-Angriffe auf Cluster-Steuerelemente sprechen. </li></ol><br><p>  Eine andere h√§ufige Sache, die ich erw√§hnen werde, ist, wo ich alles getestet habe, und ich kann definitiv sagen, dass alles funktioniert. </p><br><p>  Als Basis nehmen wir die Installation eines Kubernetes-Clusters mit Kubespray.  Wenn jemand es nicht wei√ü, ist dies tats√§chlich eine Reihe von Rollen f√ºr Ansible.  Wir verwenden es st√§ndig in unserer Arbeit.  Das Gute ist, dass Sie √ºberall rollen k√∂nnen - Sie k√∂nnen auf den Dr√ºsen und irgendwo in der Wolke rollen.  Eine Installationsmethode ist grunds√§tzlich f√ºr alles geeignet. </p><br><p>  In diesem Cluster werde ich Kubernetes v1.14.5 haben.  Der gesamte Cluster Kubas, den wir betrachten werden, ist in Namespaces unterteilt. Jeder Namespace geh√∂rt zu einem separaten Team. Mitglieder dieses Teams haben Zugriff auf jeden Namespace.  Sie k√∂nnen nicht zu verschiedenen Namespaces gehen, sondern nur zu ihren eigenen.  Es gibt jedoch ein Administratorkonto, das Rechte f√ºr den gesamten Cluster hat. </p><br><p><img src="https://habrastorage.org/webt/xm/rj/rx/xmrjrxw6toktjj1ukm-tir_remm.jpeg"></p><br><p>  Ich habe versprochen, dass wir als erstes Administratorrechte f√ºr den Cluster erhalten werden.  Wir brauchen einen speziell vorbereiteten Pod, der den Kubernetes-Cluster zerst√∂rt.  Wir m√ºssen es nur auf den Kubernetes-Cluster anwenden. </p><br><pre><code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Dieser Pod wird bei einem der Master des Kubernetes-Clusters ankommen.  Danach gibt der Cluster gerne eine Datei mit dem Namen admin.conf an uns zur√ºck.  In Kuba werden alle Administratorzertifikate in dieser Datei gespeichert und gleichzeitig die Cluster-API konfiguriert.  Auf diese Weise k√∂nnen Sie, glaube ich, Administratorzugriff auf 98% der Kubernetes-Cluster erhalten. </p><br><p>  Ich wiederhole, dieser Pod wurde von einem Entwickler in Ihrem Cluster erstellt, der Zugriff auf die Bereitstellung seiner Vorschl√§ge in einem kleinen Namespace hat. Er wird vollst√§ndig von RBAC geklemmt.  Er hatte keine Rechte.  Trotzdem ist das Zertifikat zur√ºckgekehrt. </p><br><p>  Und nun zum speziell vorbereiteten Herd.  Auf einem beliebigen Bild ausf√ºhren.  Nehmen Sie zum Beispiel debian: jessie. </p><br><p>  Wir haben so etwas: </p><br><pre> <code class="plaintext hljs">tolerations: - effect: NoSchedule operator: Exists nodeSelector: node-role.kubernetes.io/master: ""</code> </pre> <br><p>  Was ist Toleranz?  Die Meister im Kubernetes-Cluster sind normalerweise mit einem so genannten Taint ("Infektion" auf Englisch) gekennzeichnet.  Und die Essenz dieser "Infektion" - sie sagt, dass Pods nicht Master-Knoten zugewiesen werden k√∂nnen.  Aber niemand st√∂rt sich daran, in irgendeiner Weise anzuzeigen, dass er gegen√ºber der "Infektion" tolerant ist.  Der Abschnitt Toleranz sagt nur, dass wenn NoSchedule auf einem Knoten ist, unsere unter einer solchen Infektion tolerant ist - und keine Probleme. </p><br><p>  Weiter sagen wir, dass unser Unter nicht nur tolerant ist, sondern auch gezielt auf den Meister fallen will.  Weil die Meister die leckersten sind, die wir brauchen - alle Zertifikate.  Daher sagen wir nodeSelector - und wir haben eine Standardbezeichnung auf den Assistenten, mit der wir genau die Knoten ausw√§hlen k√∂nnen, die Assistenten aus allen Knoten des Clusters sind. </p><br><p>  Mit solchen zwei Abschnitten wird er definitiv zum Meister kommen.  Und er darf dort leben. </p><br><p>  Aber nur zum Meister zu kommen, reicht uns nicht.  Es wird uns nichts geben.  Daher haben wir weiter diese zwei Dinge: </p><br><pre> <code class="plaintext hljs">hostNetwork: true hostPID: true</code> </pre> <br><p>  Wir geben an, dass unser under, das wir starten, im Kernel-Namespace, im Netzwerk-Namespace und im PID-Namespace leben wird.  Sobald der Assistent gestartet wird, kann er alle realen Live-Schnittstellen dieses Knotens anzeigen, den gesamten Datenverkehr abh√∂ren und die PID aller Prozesse anzeigen. </p><br><p>  Als n√§chstes ist es klein.  Nehmen Sie etcd und lesen Sie, was Sie wollen. </p><br><p>  Am interessantesten ist diese Kubernetes-Funktion, die dort standardm√§√üig vorhanden ist. </p><br><pre> <code class="plaintext hljs">volumeMounts: - mountPath: /host name: host volumes: - hostPath: path: / type: Directory name: host</code> </pre> <br><p>  Und das Wesentliche ist, dass wir sagen k√∂nnen, dass wir in dem von uns ausgef√ºhrten Pod ein Volume vom Typ hostPath erstellen m√∂chten, auch ohne die Rechte an diesem Cluster.  Es bedeutet, den Pfad von dem Host zu nehmen, auf dem wir beginnen werden - und ihn als Volume zu nehmen.  Und dann nenne es Name: Host.  All diesen HostPath montieren wir im Kamin.  In diesem Beispiel in das Verzeichnis / host. </p><br><p>  Ich wiederhole noch einmal.  Wir haben dem Pod gesagt, er soll zum Master kommen, dort hostNetwork und hostPID holen - und die gesamte Wurzel des Masters in diesen Pod einbinden. </p><br><p>  Sie verstehen, dass in Debian Bash ausgef√ºhrt wird und dieser Bash unter unserer Wurzel funktioniert.  Das hei√üt, wir haben gerade die Wurzel f√ºr den Master erhalten, ohne Rechte im Kubernetes-Cluster zu haben. </p><br><p>  Dann besteht die ganze Aufgabe darin, in das Verzeichnis unter / host / etc / kubernetes / pki zu wechseln. Wenn ich mich nicht irre, alle Master-Zertifikate des Clusters dort abzurufen und dementsprechend der Cluster-Administrator zu werden. </p><br><p>  Wenn Sie so aussehen, sind dies einige der gef√§hrlichsten Rechte in Pods - trotz der Benutzerrechte: <br><img src="https://habrastorage.org/webt/ax/07/cj/ax07cjhm7y0dwpueikrj-qwqv2k.jpeg"></p><br><p>  Wenn ich Rechte habe, unter denen ich in einem Cluster-Namespace ausgef√ºhrt werden kann, verf√ºgt dieses Sub standardm√§√üig √ºber diese Rechte.  Ich kann privilegierte Pods ausf√ºhren, und dies sind im Allgemeinen alle Rechte, praktisch root auf dem Knoten. </p><br><p>  Mein Favorit ist Root.  Und Kubernetes hat eine solche Option Als Nicht-Root ausf√ºhren.  Dies ist eine Art Hackerschutz.  Wissen Sie, was das "Moldauische Virus" ist?  Wenn Sie ein Hacker sind und zu meinem Kubernetes-Cluster kommen, fragen wir, arme Administratoren: ‚ÄûBitte geben Sie in Ihren Pods an, mit welchen Sie meinen Cluster hacken und als Nicht-Root ausf√ºhren.  Und es kommt einfach so vor, dass Sie den Prozess in Ihrem Herd unter der Wurzel starten und es f√ºr Sie sehr einfach sein wird, mich zu hacken.  Bitte sch√ºtze dich vor dir. ‚Äú </p><br><p>  Host-Pfad-Volume - meiner Meinung nach der schnellste Weg, um das gew√ºnschte Ergebnis aus dem Kubernetes-Cluster zu erhalten. </p><br><p>  Aber was tun mit all dem? </p><br><p>  Gedanken, die jedem normalen Administrator einfallen sollten, der auf Kubernetes trifft: ‚ÄûJa, ich habe dir gesagt, Kubernetes funktioniert nicht.  Es gibt L√∂cher darin.  Und der ganze W√ºrfel ist Schwachsinn. "  Tats√§chlich gibt es so etwas wie Dokumentation, und wenn Sie dort nachsehen, gibt es einen Abschnitt mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pod-Sicherheitsrichtlinien</a> . </p><br><p>  Dies ist ein solches Yaml-Objekt - wir k√∂nnen es im Kubernetes-Cluster erstellen -, das die Sicherheitsaspekte in der Beschreibung der Herde steuert.  Das hei√üt, es steuert tats√§chlich diese Rechte zur Verwendung aller Arten von hostNetwork, hostPID und bestimmten Datentr√§gertypen, die sich beim Start in den Pods befinden.  Mit der Pod-Sicherheitsrichtlinie kann all dies beschrieben werden. </p><br><p>  Das Interessanteste an der Pod-Sicherheitsrichtlinie ist, dass im Kubernetes-Cluster nicht alle PSP-Installationsprogramme in irgendeiner Weise beschrieben, sondern standardm√§√üig deaktiviert werden.  Die Pod-Sicherheitsrichtlinie wird √ºber das Zulassungs-Plugin aktiviert. </p><br><p>  Okay, lassen Sie uns in einer Cluster-Pod-Sicherheitsrichtlinie landen. Nehmen wir an, wir haben eine Art Service-Pods im Namespace, auf die nur Administratoren Zugriff haben.  Nehmen wir an, in allen anderen Pods haben sie eingeschr√§nkte Rechte.  Da Entwickler h√∂chstwahrscheinlich keine privilegierten Pods in Ihrem Cluster ausf√ºhren m√ºssen. </p><br><p>  Und bei uns scheint alles in Ordnung zu sein.  Und unser Kubernetes-Cluster kann nicht in zwei Minuten gehackt werden. </p><br><p>  Es gibt ein Problem.  Wenn Sie einen Kubernetes-Cluster haben, wird h√∂chstwahrscheinlich die √úberwachung in Ihrem Cluster installiert.  Ich gehe sogar davon aus, dass Prometheus als √úberwachung bezeichnet wird, wenn in Ihrem Cluster eine √úberwachung stattfindet. </p><br><p>  Was ich Ihnen jetzt sagen werde, gilt sowohl f√ºr den Prometheus-Betreiber als auch f√ºr den Prometheus, der in seiner reinen Form geliefert wird.  Die Frage ist, dass ich mehr suchen muss, wenn ich den Administrator nicht so schnell in den Cluster einbinden kann.  Und ich kann mit Ihrer √úberwachung suchen. </p><br><p>  Wahrscheinlich lesen alle die gleichen Artikel √ºber Habr√©, und die √úberwachung ist in der √úberwachung.  Die Helmkarte wird f√ºr alle ungef√§hr gleich genannt.  Ich gehe davon aus, dass Sie ungef√§hr die gleichen Namen erhalten, wenn Sie Stable / Prometheus installieren.  Und selbst h√∂chstwahrscheinlich muss ich den DNS-Namen in Ihrem Cluster nicht erraten.  Weil es Standard ist. </p><br><p><img src="https://habrastorage.org/webt/o6/rv/fw/o6rvfw0idykw0wyxivpfmwkd_vy.jpeg"></p><br><p>  Weiter haben wir eine bestimmte Entwicklung, in der es m√∂glich ist, eine bestimmte Unter zu starten.  Und weiter von diesem Herd entfernt ist es sehr einfach, dies zu tun: </p><br><pre> <code class="plaintext hljs">$ curl http://prometheus-kube-state-metrics.monitoring</code> </pre> <br><p>  prometheus-kube-state -metrics ist einer der prometheus-Exporteure, der Metriken aus der Kubernetes-API sammelt.  In Ihrem Cluster werden viele Daten ausgef√ºhrt, was es ist, welche Probleme Sie damit haben. </p><br><p>  Als einfaches Beispiel: </p><br><p>  kube_pod_container_info {namespace = "kube-system", pod = "kube-apiserver-k8s-1", container = "kube-apiserver", image = </p><br><p>  <strong>"gcr.io/google-containers/kube-apiserver:v1.14.5"</strong> </p><br><p>  , Image_id = "Docker-ziehbar: //gcr.io/google-containers/kube- apiserver @ sha256: e29561119a52adad9edc72bfe0e7fcab308501313b09bf99df4a96 38ee634989", container_id = "Docker: // 7cbe7b1fea33f811fdd8f7e0e079191110268f2 853397d7daf08e72c22d3cf8b"} 1 </p><br><p>  Nachdem Sie eine einfache Curl-Anfrage aus einer nicht privilegierten Datei gestellt haben, k√∂nnen Sie solche Informationen erhalten.  Wenn Sie nicht wissen, in welcher Version von Kubernetes Sie ausgef√ºhrt werden, k√∂nnen Sie dies leicht feststellen. </p><br><p>  Und das Interessanteste ist, dass Sie sich neben der Tatsache, dass Sie sich den Kubikzustandsmetriken zuwenden, genauso direkt auf Prometheus selbst anwenden k√∂nnen.  Von dort aus k√∂nnen Sie Metriken sammeln.  Von dort aus k√∂nnen Sie sogar Metriken erstellen.  Selbst theoretisch k√∂nnen Sie eine solche Anforderung aus einem Cluster in Prometheus erstellen, wodurch sie einfach deaktiviert wird.  Und Ihre √úberwachung funktioniert im Allgemeinen nicht mehr im Cluster. </p><br><p>  Und hier stellt sich bereits die Frage, ob eine externe √úberwachung Ihre √úberwachung √ºberwacht.  Ich hatte gerade die Gelegenheit, im Kubernetes-Cluster ohne Konsequenzen f√ºr mich selbst zu agieren.  Sie werden nicht einmal wissen, dass ich dort agiere, da es keine √úberwachung mehr gibt. </p><br><p>  Genau wie bei PSP scheint das Problem zu sein, dass all diese trendigen Technologien - Kubernetes, Prometheus - einfach nicht funktionieren und voller L√∂cher sind.  Nicht wirklich. </p><br><p>  Es gibt so etwas - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerkrichtlinien</a> . </p><br><p>  Wenn Sie ein normaler Administrator sind, wissen Sie h√∂chstwahrscheinlich √ºber die Netzwerkrichtlinie, dass dies ein weiteres Yaml ist, das im Cluster bereits dofig ist.  Und einige Netzwerkrichtlinien werden definitiv nicht ben√∂tigt.  Und selbst wenn Sie lesen, was Netzwerkrichtlinie ist, was die Kubernetes-Yaml-Firewall ist, k√∂nnen Sie damit die Zugriffsrechte zwischen Namespaces und Pods einschr√§nken. Dann haben Sie sicher entschieden, dass die Yaml-Firewall in Kubernetes in den n√§chsten Abstraktionen enthalten ist ... Nein, nicht .  Dies ist definitiv nicht notwendig. </p><br><p>  Selbst wenn Ihren Sicherheitsspezialisten nicht mitgeteilt wurde, dass Sie mit Ihren Kubernetes sehr einfach und unkompliziert eine Firewall erstellen k√∂nnen, ist diese sehr detailliert.  Wenn sie dies immer noch nicht wissen und Sie nicht ziehen: "Nun, geben, geben ..." In jedem Fall ben√∂tigen Sie eine Netzwerkrichtlinie, um den Zugriff auf einige offizielle Orte zu blockieren, die Sie ohne Genehmigung aus Ihrem Cluster abrufen k√∂nnen. </p><br><p>  Wie in dem von mir zitierten Beispiel k√∂nnen Sie die Kube-Statusmetriken aus einem beliebigen Namespace im Kubernetes-Cluster abrufen, ohne Rechte daf√ºr zu haben.  Netzwerkrichtlinien schlossen den Zugriff aller anderen Namespaces auf die Namespace-√úberwachung und sozusagen auf alles: kein Zugriff, keine Probleme.  In allen vorhandenen Diagrammen, sowohl im Standard-Prometeus als auch im Prometeus im Operator, gibt es in den Steuerwerten lediglich eine Option, um einfach Netzwerkrichtlinien f√ºr sie zu aktivieren.  Sie m√ºssen es nur einschalten und sie werden funktionieren. </p><br><p>  Hier gibt es wirklich ein Problem.  Als normaler b√§rtiger Administrator haben Sie h√∂chstwahrscheinlich entschieden, dass Netzwerkrichtlinien nicht ben√∂tigt werden.  Und nachdem Sie alle Arten von Artikeln √ºber Ressourcen wie Habr gelesen haben, haben Sie entschieden, dass Flanell, insbesondere im Host-Gateway-Modus, das Beste ist, was Sie ausw√§hlen k√∂nnen. </p><br><p>  Was zu tun ist? </p><br><p>  Sie k√∂nnen versuchen, die Netzwerkl√∂sung in Ihrem Kubernetes-Cluster erneut bereitzustellen und durch eine funktionalere zu ersetzen.  Zum Beispiel auf demselben Calico.  Aber sofort m√∂chte ich sagen, dass die Aufgabe, die Netzwerkl√∂sung im Kubernetes-Arbeitscluster zu √§ndern, nicht trivial ist.  Ich habe es zweimal gel√∂st (beide Male jedoch theoretisch), aber wir haben sogar gezeigt, wie man das auf den Slurms macht.  F√ºr unsere Sch√ºler haben wir gezeigt, wie die Netzwerkl√∂sung im Kubernetes-Cluster ge√§ndert werden kann.  Im Prinzip k√∂nnen Sie versuchen, sicherzustellen, dass im Produktionscluster keine Ausfallzeiten auftreten.  Aber Sie werden wahrscheinlich keinen Erfolg haben. </p><br><p>  Und das Problem ist eigentlich sehr einfach gel√∂st.  Der Cluster enth√§lt Zertifikate, und Sie wissen, dass Ihre Zertifikate in einem Jahr fehlerhaft sein werden.  Nun, und normalerweise eine normale L√∂sung mit Zertifikaten im Cluster - warum werden wir d√§mpfen, wir werden einen neuen Cluster daneben erstellen, ihn im alten faulen lassen und alles wiederholen.  Es stimmt, wenn es schlecht wird, wird sich in unserer Zeit alles hinlegen, aber dann ein neuer Cluster. </p><br><p>  Wenn Sie einen neuen Cluster erstellen, f√ºgen Sie gleichzeitig Calico anstelle von Flanell ein. </p><br><p>  Was tun, wenn Sie seit hundert Jahren Zertifikate ausgestellt haben und den Cluster nicht erneut gruppieren?  Es gibt so etwas wie Kube-RBAC-Proxy.  Dies ist eine sehr coole Entwicklung, mit der Sie sich als Beiwagencontainer in jeden Herd im Kubernetes-Cluster einbetten k√∂nnen.  Und sie f√ºgt diesem Pod tats√§chlich eine Autorisierung durch Kubernetes RBAC hinzu. </p><br><p>  Es gibt ein Problem.  Zuvor war Kube-RBAC-Proxy in den Prometheus des Betreibers eingebaut.  Aber dann war er weg.  Moderne Versionen basieren jetzt auf der Tatsache, dass Sie √ºber eine Netzwerkrichtlinie verf√ºgen und diese nicht mehr verwenden.  Und so m√ºssen Sie das Diagramm ein wenig umschreiben.  Wenn Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Repository gehen</a> , gibt es Beispiele f√ºr die Verwendung als Beiwagen, und Sie m√ºssen die Diagramme minimal neu schreiben. </p><br><p>  Es gibt noch ein kleines Problem.  Nicht nur Prometheus gibt seine Metriken an jeden weiter, der sie erh√§lt.  Wir haben auch alle Komponenten des Kubernetes-Clusters, sie k√∂nnen ihre Metriken angeben. </p><br><p>  Aber wie gesagt, wenn Sie keinen Zugriff auf den Cluster erhalten und keine Informationen sammeln k√∂nnen, k√∂nnen Sie zumindest Schaden anrichten. </p><br><p>  Ich zeige Ihnen also schnell zwei M√∂glichkeiten, wie Sie Ihren Kubernetes-Cluster verderben k√∂nnen. </p><br><p>  Sie werden lachen, wenn ich Ihnen sage, dies sind zwei F√§lle aus dem wirklichen Leben. </p><br><p>  Der erste Weg.  Die Ressourcen gehen zur Neige. </p><br><p>  Wir starten ein weiteres Special unter.  Er wird einen solchen Abschnitt haben. </p><br><pre> <code class="plaintext hljs">resources: requests: cpu: 4 memory: 4Gi</code> </pre> <br><p>  Wie Sie wissen, sind Anforderungen die Menge an CPU und Speicher, die auf dem Host f√ºr bestimmte Pods mit Anforderungen reserviert ist.  Wenn wir einen Vier-Kern-Host im Kubernetes-Cluster haben und vier CPUs mit Anforderungen dorthin kommen, bedeutet dies, dass keine Pods mit Anforderungen an diesen Host mehr kommen k√∂nnen. </p><br><p>  Wenn ich dies unter ausf√ºhre, mache ich einen Befehl: </p><br><pre> <code class="plaintext hljs">$ kubectl scale special-pod --replicas=...</code> </pre> <br><p>  Dann kann niemand anderes im Kubernetes-Cluster bereitstellen.  Denn in allen Knoten enden die Anfragen.  Und so stoppe ich Ihren Kubernetes-Cluster.  Wenn ich dies abends mache, kann ich den Einsatz f√ºr einige Zeit einstellen. </p><br><p>  Wenn wir uns die Kubernetes-Dokumentation noch einmal ansehen, werden wir so etwas wie den Grenzbereich sehen.  Es legt Ressourcen f√ºr Clusterobjekte fest.  Sie k√∂nnen ein Limit Range-Objekt in yaml schreiben, es auf bestimmte Namespaces anwenden - und weiter in diesem Namespace k√∂nnen Sie sagen, dass Sie √ºber die Ressourcen f√ºr die Standard-, Maximum- und Minimum-Pods verf√ºgen. </p><br><p>  Mit Hilfe eines solchen Dokuments k√∂nnen wir Benutzer in bestimmten Produkt-Namespaces von Teams auf die M√∂glichkeit beschr√§nken, b√∂se Dinge auf ihren Pods anzuzeigen.  Aber selbst wenn Sie dem Benutzer mitteilen, dass es unm√∂glich ist, Pods mit Anforderungen von mehr als einer CPU auszuf√ºhren, gibt es leider einen so wunderbaren Skalierungsbefehl, oder √ºber das Dashboard k√∂nnen sie skalieren. </p><br><p>  Und von hier kommt die Methode Nummer zwei.  Wir starten 11 111 111 111 111 Herde.  Das sind elf Milliarden.  Das liegt nicht daran, dass ich eine solche Nummer gefunden habe, sondern daran, dass ich sie selbst gesehen habe. </p><br><p>  Die wahre Geschichte.  Am sp√§ten Abend wollte ich gerade das B√ºro verlassen.  Ich sehe, eine Gruppe von Entwicklern sitzt in der Ecke und macht hektisch etwas mit Laptops.  Ich gehe zu den Jungs und frage: "Was ist mit dir passiert?" </p><br><p>  Etwas fr√ºher, um neun Uhr abends, ging einer der Entwickler nach Hause.  Und er entschied: "Ich √ºberspringe meine Bewerbung jetzt bis zu einer."  Ich klickte ein wenig und das Internet ein wenig langweilig.  Er klickte erneut auf das Ger√§t, dr√ºckte auf das Ger√§t und klickte auf die Eingabetaste.  Stocherte in allem, was er konnte.  Dann wurde das Internet lebendig - und alles begann bis zu diesem Datum zu skalieren. </p><br><p>  Diese Geschichte kam zwar nicht auf Kubernetes vor, damals war es Nomad.  Es endete mit der Tatsache, dass Nomad nach einer Stunde unserer Versuche, Nomad von hartn√§ckigen Versuchen abzuhalten, zusammenzuhalten, antwortete, dass er nicht aufh√∂ren w√ºrde zu kleben und nichts anderes tun w√ºrde.  "Ich bin m√ºde, ich gehe."  Und zusammengerollt. </p><br><p>  Ich habe nat√ºrlich versucht, dasselbe bei Kubernetes zu tun.  Die elf Milliarden Schoten von Kubernetes waren nicht erfreut, sagte er: "Ich kann nicht.  √úbertrifft die inneren Mundsch√ºtzer. "  Aber 1.000.000.000 Herde k√∂nnten. </p><br><p>  Als Antwort auf eine Milliarde ging der W√ºrfel nicht hinein.  Er begann wirklich zu skalieren.  Je weiter der Prozess ging, desto mehr Zeit brauchte er, um neue Herde zu schaffen.  Trotzdem ging der Prozess weiter.  Das einzige Problem ist, dass ich, wenn ich Pods unbegrenzt in meinem Namespace ausf√ºhren kann, auch ohne Anforderungen und Einschr√§nkungen eine solche Anzahl von Pods mit einigen Aufgaben starten kann, dass sich bei diesen Aufgaben die Knoten aus dem Speicher und aus der CPU addieren.  Wenn ich so viele Herde betreibe, sollten die Informationen von ihnen in das Repository gehen, d. H. Usw.  Und wenn dort zu viele Informationen eintreffen, beginnt das Lagerhaus zu langsam zu verschenken - und bei Kubernetes beginnen langweilige Dinge. </p><br><p>  Und noch ein Problem ... Wie Sie wissen, sind die Steuerelemente von Kubernetes nicht nur eine zentrale Sache, sondern mehrere Komponenten.  Dort gibt es insbesondere einen Controller-Manager, einen Scheduler usw.  Alle diese Leute werden gleichzeitig unn√∂tig dumme Arbeit verrichten, was im Laufe der Zeit immer mehr Zeit in Anspruch nehmen wird.  Der Controller-Manager erstellt neue Pods.  Der Scheduler versucht, einen neuen Knoten zu finden.  Neue Knoten in Ihrem Cluster werden h√∂chstwahrscheinlich bald enden.  Der Kubernetes-Cluster beginnt langsamer und langsamer zu arbeiten. </p><br><p>  Aber ich beschloss, noch weiter zu gehen.  Wie Sie wissen, gibt es in Kubernetes so etwas wie Service.  Nun, und standardm√§√üig in Ihren Clustern funktioniert der Dienst h√∂chstwahrscheinlich mithilfe von IP-Tabellen. </p><br><p>  Wenn Sie beispielsweise eine Milliarde Herde betreiben und dann mithilfe von Skripten Kubernetis zwingen, neue Dienste zu erstellen: </p><br><pre> <code class="plaintext hljs">for i in {1..1111111}; do kubectl expose deployment test --port 80 \ --overrides="{\"apiVersion\": \"v1\", \"metadata\": {\"name\": \"nginx$i\"}}"; done</code> </pre> <br><p>  Auf allen Knoten des Clusters werden ungef√§hr gleichzeitig ungef√§hr neue iptables-Regeln generiert.  Dar√ºber hinaus werden f√ºr jeden Dienst eine Milliarde Iptables-Regeln generiert. </p><br><p>  Ich habe das Ganze an mehreren tausend, bis zu einem Dutzend, √ºberpr√ºft.  Und das Problem ist, dass bereits bei dieser Schwelle ssh auf dem Knoten ziemlich problematisch zu tun ist.  Weil sich die Pakete, die eine solche Anzahl von Ketten passieren, nicht sehr gut anf√ºhlen. </p><br><p>  Und all dies wird auch mit Hilfe von Kubernetes gel√∂st.  Es gibt ein solches Ressourcenkontingentobjekt.  Legt die Anzahl der verf√ºgbaren Ressourcen und Objekte f√ºr einen Namespace in einem Cluster fest.  Wir k√∂nnen in jedem Namespace des Kubernetes-Clusters ein yaml-Objekt erstellen.  Mit diesem Objekt k√∂nnen wir sagen, dass wir eine bestimmte Anzahl von Anforderungen und Grenzwerten f√ºr diesen Namespace zugewiesen haben, und dann k√∂nnen wir sagen, dass es m√∂glich ist, 10 Dienste und 10 Pods in diesem Namespace zu erstellen.  Und ein einzelner Entwickler kann sich zumindest abends quetschen.  Kubernetes wird zu ihm sagen: "Sie k√∂nnen Ihre Pods nicht auf einen solchen Betrag bringen, weil er die Ressourcenquote √ºberschreitet."  Alles, das Problem ist gel√∂st.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Dokumentation finden Sie hier</a> . </p><br><p>  Ein problematischer Punkt ergibt sich im Zusammenhang damit.  Sie sp√ºren, wie schwierig es wird, in Kubernetes einen Namespace zu erstellen.  Um es zu schaffen, m√ºssen wir eine Reihe von Dingen ber√ºcksichtigen. </p><br><p>  Ressourcenkontingent + Grenzbereich + RBAC <br>  ‚Ä¢ Erstellen Sie einen Namespace <br>  ‚Ä¢ Erstellen Sie einen inneren Grenzbereich <br>  ‚Ä¢ Innerhalb des Ressourcenkontingents erstellen <br>  ‚Ä¢ Erstellen Sie ein Servicekonto f√ºr CI <br>  ‚Ä¢ Erstellen Sie eine Rollenbindung f√ºr CI und Benutzer <br>  ‚Ä¢ F√ºhren Sie optional die erforderlichen Service-Pods aus </p><br><p>  Bei dieser Gelegenheit m√∂chte ich daher meine Entwicklungen mitteilen.  Es gibt so etwas, den SDK-Operator.  Dies ist eine M√∂glichkeit im Kubernetes-Cluster, Operatoren daf√ºr zu schreiben.  Sie k√∂nnen Anweisungen mit Ansible schreiben. </p><br><p>  Zuerst wurde es in Ansible geschrieben, und dann habe ich nach einem SDK-Operator gesucht und die Ansible-Rolle im Operator neu geschrieben.  Mit diesem Operator k√∂nnen Sie ein Objekt im Kubernetes-Cluster erstellen, das als Team bezeichnet wird.       yaml    .       ,    - . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> . </p><br><p>   .     ? <br> . Pod Security Policy ‚Äî  .     ,            , -      . </p><br><p> Network Policy ‚Äî   -    .  ,     . </p><br><p> LimitRange/ResourceQuota ‚Äî   .     ,     ,     . ,   . </p><br><p>  ,      ,   ,    .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> . </p><br><p>      .  ,           warlocks ,   . </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  ,   ,   .      ,  ResourceQuota, Pod Security Policy .     . </p><br><p>  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472484/">https://habr.com/ru/post/de472484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472468/index.html">ClusterJ - Arbeiten mit MySQL NDB Cluster von Java</a></li>
<li><a href="../de472470/index.html">Transgene M√§use und Anti-Aging</a></li>
<li><a href="../de472472/index.html">Ferienhaus im Winter: sein oder nicht sein?</a></li>
<li><a href="../de472474/index.html">Lustiger kosmetischer Fehler in Google Chrome</a></li>
<li><a href="../de472482/index.html">Radioaktiver Unfall: Entdeckung einer festen stabilen Phase von Plutonium</a></li>
<li><a href="../de472486/index.html">Langzeitdatenspeicherung. (Artikel - Diskussion)</a></li>
<li><a href="../de472488/index.html">Drei√üig Berichte von DevOops 2019: Tim Lister, Hadi Hariri, Roman Shaposhnik und andere Stars der internationalen DevOps</a></li>
<li><a href="../de472490/index.html">Wie ich mithilfe der Verarbeitung nat√ºrlicher Sprache nach einem Sch√∂nheitsstandard gesucht habe (und ihn nicht gefunden habe)</a></li>
<li><a href="../de472492/index.html">Analyse des ROOT-Codes, Scientific Data Analysis Framework</a></li>
<li><a href="../de472496/index.html">Erstellen eines Sammelalbum-Layouts in CSS Grid</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>