<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😼 🏴‍☠️ 👼🏼 Lücken in einem Kubernetes-Cluster schließen. Bericht und Transkription mit DevOpsConf 👨‍👧‍👧 🎯 👵🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pavel Selivanov, Southbridge Solution Architect und Slurm Lecturer, hielt auf der DevOpsConf 2019 einen Vortrag. Dieser Bericht ist Teil des ausführli...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Lücken in einem Kubernetes-Cluster schließen. Bericht und Transkription mit DevOpsConf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/472484/"><p>  Pavel Selivanov, Southbridge Solution Architect und Slurm Lecturer, hielt auf der DevOpsConf 2019 einen Vortrag. Dieser Bericht ist Teil des ausführlichen Slur Mega-Kurses von Kubernetes. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Basic: Eine Einführung in Kubernetes</a> findet vom 18. bis 20. November in Moskau statt. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Mega: Wir schauen unter die Haube von Kubernetes</a> - Moskau, 22.-24. November. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Slurm Online: Beide Kubernetes-Kurse sind immer</a> verfügbar. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Gt4Q1du5FXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Unter dem Schnittprotokoll des Berichts. </p><a name="habracut"></a><br><p>  Guten Tag, Kollegen und Sympathisanten.  Heute werde ich über Sicherheit sprechen. </p><br><p>  Ich sehe, dass heute viele Sicherheitskräfte in der Halle sind.  Ich entschuldige mich im Voraus bei Ihnen, wenn ich die Begriffe aus der Sicherheitswelt nicht so verwende, wie Sie es akzeptiert haben. </p><br><p>  So kam es, dass ich vor ungefähr sechs Monaten in die Hände eines öffentlichen Kubernetes-Clusters geriet.  Öffentlich - bedeutet, dass es eine n-te Anzahl von Namespaces gibt. In diesem Namespace befinden sich Benutzer, die in ihrem Namespace isoliert sind.  Alle diese Benutzer gehören verschiedenen Unternehmen an.  Nun, es wurde angenommen, dass dieser Cluster als CDN verwendet werden sollte.  Das heißt, sie geben Ihnen einen Cluster, sie geben dem Benutzer dort, Sie gehen in Ihrem Namespace dorthin und stellen Ihre Fronten bereit. </p><br><p>  Sie haben versucht, einen solchen Service an meine vorherige Firma zu verkaufen.  Und ich wurde gebeten, einen Cluster zu diesem Thema zu erstellen - ist diese Lösung geeignet oder nicht? </p><br><p>  Ich bin zu diesem Cluster gekommen.  Ich erhielt begrenzte Rechte, begrenzten Namespace.  Dort verstanden die Jungs, was Sicherheit ist.  Sie haben gelesen, was Kubernetes über eine rollenbasierte Zugriffskontrolle (RBAC) verfügt - und sie haben sie verdreht, damit ich Pods nicht getrennt von der Bereitstellung ausführen kann.  Ich erinnere mich nicht an die Aufgabe, die ich lösen wollte, indem ich ohne Bereitstellung ausgeführt wurde, aber ich wollte wirklich nur unter ausführen.  Ich habe mich für viel Glück entschieden, um zu sehen, welche Rechte ich im Cluster habe, was ich kann, was ich nicht kann, was sie dort vermasselt haben.  Gleichzeitig werde ich Ihnen mitteilen, was sie im RBAC falsch konfiguriert haben. </p><br><p>  So kam es, dass ich zwei Minuten später einen Administrator für ihren Cluster bekam, mir alle benachbarten Namespaces ansah, die Produktionsfronten von Unternehmen sah, die den Service bereits gekauft hatten und dort feststeckten.  Ich hielt mich kaum auf, um nicht zu jemandem in der Front zu kommen und kein obszönes Wort auf die Hauptseite zu setzen. </p><br><p>  Ich werde Ihnen anhand von Beispielen erzählen, wie ich das gemacht habe und wie ich mich davor schützen kann. </p><br><p>  Aber stell mich zuerst vor.  Ich heiße Pavel Selivanov.  Ich bin Architekt bei Southbridge.  Ich verstehe Kubernetes, DevOps und alle möglichen ausgefallenen Sachen.  Die Ingenieure von Southbridge und ich bauen das alles und ich rate. </p><br><p>  Zusätzlich zu unserem Kerngeschäft haben wir kürzlich Projekte namens Slory gestartet.  Wir versuchen, unsere Fähigkeit, mit Kubernetes zu arbeiten, der Masse nahe zu bringen und anderen Menschen beizubringen, wie man auch mit K8s arbeitet. </p><br><p> Worüber ich heute sprechen werde.  Das Thema des Berichts liegt auf der Hand - über die Sicherheit des Kubernetes-Clusters.  Aber ich möchte sofort sagen, dass dieses Thema sehr groß ist - und deshalb möchte ich sofort festlegen, worüber ich nicht sicher erzählen werde.  Ich werde nicht über abgedroschene Begriffe sprechen, die im Internet bereits hundertmal überstrapaziert sind.  Alle RBAC und Zertifikate. </p><br><p>  Ich werde darüber sprechen, wie es mir und meinen Kollegen aus Sicherheitsgründen im Kubernetes-Cluster weh tut.  Wir sehen diese Probleme sowohl bei Anbietern, die Kubernetes-Cluster bereitstellen, als auch bei Kunden, die zu uns kommen.  Und selbst bei Kunden, die von anderen Beratungsunternehmen zu uns kommen.  Das heißt, das Ausmaß der Tragödie ist in der Tat sehr groß. </p><br><p>  Buchstäblich drei Punkte, über die ich heute sprechen werde: </p><br><ol><li>  Benutzerrechte gegen Pod-Rechte.  Benutzer- und Herdrechte sind nicht dasselbe. </li><li>  Sammlung von Clusterinformationen.  Ich werde zeigen, dass Sie im Cluster alle Informationen sammeln können, die Sie benötigen, ohne über besondere Rechte in diesem Cluster zu verfügen. </li><li>  DoS-Angriff auf den Cluster.  Wenn wir keine Informationen sammeln können, können wir den Cluster auf jeden Fall platzieren.  Ich werde über DoS-Angriffe auf Cluster-Steuerelemente sprechen. </li></ol><br><p>  Eine andere häufige Sache, die ich erwähnen werde, ist, wo ich alles getestet habe, und ich kann definitiv sagen, dass alles funktioniert. </p><br><p>  Als Basis nehmen wir die Installation eines Kubernetes-Clusters mit Kubespray.  Wenn jemand es nicht weiß, ist dies tatsächlich eine Reihe von Rollen für Ansible.  Wir verwenden es ständig in unserer Arbeit.  Das Gute ist, dass Sie überall rollen können - Sie können auf den Drüsen und irgendwo in der Wolke rollen.  Eine Installationsmethode ist grundsätzlich für alles geeignet. </p><br><p>  In diesem Cluster werde ich Kubernetes v1.14.5 haben.  Der gesamte Cluster Kubas, den wir betrachten werden, ist in Namespaces unterteilt. Jeder Namespace gehört zu einem separaten Team. Mitglieder dieses Teams haben Zugriff auf jeden Namespace.  Sie können nicht zu verschiedenen Namespaces gehen, sondern nur zu ihren eigenen.  Es gibt jedoch ein Administratorkonto, das Rechte für den gesamten Cluster hat. </p><br><p><img src="https://habrastorage.org/webt/xm/rj/rx/xmrjrxw6toktjj1ukm-tir_remm.jpeg"></p><br><p>  Ich habe versprochen, dass wir als erstes Administratorrechte für den Cluster erhalten werden.  Wir brauchen einen speziell vorbereiteten Pod, der den Kubernetes-Cluster zerstört.  Wir müssen es nur auf den Kubernetes-Cluster anwenden. </p><br><pre><code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Dieser Pod wird bei einem der Master des Kubernetes-Clusters ankommen.  Danach gibt der Cluster gerne eine Datei mit dem Namen admin.conf an uns zurück.  In Kuba werden alle Administratorzertifikate in dieser Datei gespeichert und gleichzeitig die Cluster-API konfiguriert.  Auf diese Weise können Sie, glaube ich, Administratorzugriff auf 98% der Kubernetes-Cluster erhalten. </p><br><p>  Ich wiederhole, dieser Pod wurde von einem Entwickler in Ihrem Cluster erstellt, der Zugriff auf die Bereitstellung seiner Vorschläge in einem kleinen Namespace hat. Er wird vollständig von RBAC geklemmt.  Er hatte keine Rechte.  Trotzdem ist das Zertifikat zurückgekehrt. </p><br><p>  Und nun zum speziell vorbereiteten Herd.  Auf einem beliebigen Bild ausführen.  Nehmen Sie zum Beispiel debian: jessie. </p><br><p>  Wir haben so etwas: </p><br><pre> <code class="plaintext hljs">tolerations: - effect: NoSchedule operator: Exists nodeSelector: node-role.kubernetes.io/master: ""</code> </pre> <br><p>  Was ist Toleranz?  Die Meister im Kubernetes-Cluster sind normalerweise mit einem so genannten Taint ("Infektion" auf Englisch) gekennzeichnet.  Und die Essenz dieser "Infektion" - sie sagt, dass Pods nicht Master-Knoten zugewiesen werden können.  Aber niemand stört sich daran, in irgendeiner Weise anzuzeigen, dass er gegenüber der "Infektion" tolerant ist.  Der Abschnitt Toleranz sagt nur, dass wenn NoSchedule auf einem Knoten ist, unsere unter einer solchen Infektion tolerant ist - und keine Probleme. </p><br><p>  Weiter sagen wir, dass unser Unter nicht nur tolerant ist, sondern auch gezielt auf den Meister fallen will.  Weil die Meister die leckersten sind, die wir brauchen - alle Zertifikate.  Daher sagen wir nodeSelector - und wir haben eine Standardbezeichnung auf den Assistenten, mit der wir genau die Knoten auswählen können, die Assistenten aus allen Knoten des Clusters sind. </p><br><p>  Mit solchen zwei Abschnitten wird er definitiv zum Meister kommen.  Und er darf dort leben. </p><br><p>  Aber nur zum Meister zu kommen, reicht uns nicht.  Es wird uns nichts geben.  Daher haben wir weiter diese zwei Dinge: </p><br><pre> <code class="plaintext hljs">hostNetwork: true hostPID: true</code> </pre> <br><p>  Wir geben an, dass unser under, das wir starten, im Kernel-Namespace, im Netzwerk-Namespace und im PID-Namespace leben wird.  Sobald der Assistent gestartet wird, kann er alle realen Live-Schnittstellen dieses Knotens anzeigen, den gesamten Datenverkehr abhören und die PID aller Prozesse anzeigen. </p><br><p>  Als nächstes ist es klein.  Nehmen Sie etcd und lesen Sie, was Sie wollen. </p><br><p>  Am interessantesten ist diese Kubernetes-Funktion, die dort standardmäßig vorhanden ist. </p><br><pre> <code class="plaintext hljs">volumeMounts: - mountPath: /host name: host volumes: - hostPath: path: / type: Directory name: host</code> </pre> <br><p>  Und das Wesentliche ist, dass wir sagen können, dass wir in dem von uns ausgeführten Pod ein Volume vom Typ hostPath erstellen möchten, auch ohne die Rechte an diesem Cluster.  Es bedeutet, den Pfad von dem Host zu nehmen, auf dem wir beginnen werden - und ihn als Volume zu nehmen.  Und dann nenne es Name: Host.  All diesen HostPath montieren wir im Kamin.  In diesem Beispiel in das Verzeichnis / host. </p><br><p>  Ich wiederhole noch einmal.  Wir haben dem Pod gesagt, er soll zum Master kommen, dort hostNetwork und hostPID holen - und die gesamte Wurzel des Masters in diesen Pod einbinden. </p><br><p>  Sie verstehen, dass in Debian Bash ausgeführt wird und dieser Bash unter unserer Wurzel funktioniert.  Das heißt, wir haben gerade die Wurzel für den Master erhalten, ohne Rechte im Kubernetes-Cluster zu haben. </p><br><p>  Dann besteht die ganze Aufgabe darin, in das Verzeichnis unter / host / etc / kubernetes / pki zu wechseln. Wenn ich mich nicht irre, alle Master-Zertifikate des Clusters dort abzurufen und dementsprechend der Cluster-Administrator zu werden. </p><br><p>  Wenn Sie so aussehen, sind dies einige der gefährlichsten Rechte in Pods - trotz der Benutzerrechte: <br><img src="https://habrastorage.org/webt/ax/07/cj/ax07cjhm7y0dwpueikrj-qwqv2k.jpeg"></p><br><p>  Wenn ich Rechte habe, unter denen ich in einem Cluster-Namespace ausgeführt werden kann, verfügt dieses Sub standardmäßig über diese Rechte.  Ich kann privilegierte Pods ausführen, und dies sind im Allgemeinen alle Rechte, praktisch root auf dem Knoten. </p><br><p>  Mein Favorit ist Root.  Und Kubernetes hat eine solche Option Als Nicht-Root ausführen.  Dies ist eine Art Hackerschutz.  Wissen Sie, was das "Moldauische Virus" ist?  Wenn Sie ein Hacker sind und zu meinem Kubernetes-Cluster kommen, fragen wir, arme Administratoren: „Bitte geben Sie in Ihren Pods an, mit welchen Sie meinen Cluster hacken und als Nicht-Root ausführen.  Und es kommt einfach so vor, dass Sie den Prozess in Ihrem Herd unter der Wurzel starten und es für Sie sehr einfach sein wird, mich zu hacken.  Bitte schütze dich vor dir. “ </p><br><p>  Host-Pfad-Volume - meiner Meinung nach der schnellste Weg, um das gewünschte Ergebnis aus dem Kubernetes-Cluster zu erhalten. </p><br><p>  Aber was tun mit all dem? </p><br><p>  Gedanken, die jedem normalen Administrator einfallen sollten, der auf Kubernetes trifft: „Ja, ich habe dir gesagt, Kubernetes funktioniert nicht.  Es gibt Löcher darin.  Und der ganze Würfel ist Schwachsinn. "  Tatsächlich gibt es so etwas wie Dokumentation, und wenn Sie dort nachsehen, gibt es einen Abschnitt mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pod-Sicherheitsrichtlinien</a> . </p><br><p>  Dies ist ein solches Yaml-Objekt - wir können es im Kubernetes-Cluster erstellen -, das die Sicherheitsaspekte in der Beschreibung der Herde steuert.  Das heißt, es steuert tatsächlich diese Rechte zur Verwendung aller Arten von hostNetwork, hostPID und bestimmten Datenträgertypen, die sich beim Start in den Pods befinden.  Mit der Pod-Sicherheitsrichtlinie kann all dies beschrieben werden. </p><br><p>  Das Interessanteste an der Pod-Sicherheitsrichtlinie ist, dass im Kubernetes-Cluster nicht alle PSP-Installationsprogramme in irgendeiner Weise beschrieben, sondern standardmäßig deaktiviert werden.  Die Pod-Sicherheitsrichtlinie wird über das Zulassungs-Plugin aktiviert. </p><br><p>  Okay, lassen Sie uns in einer Cluster-Pod-Sicherheitsrichtlinie landen. Nehmen wir an, wir haben eine Art Service-Pods im Namespace, auf die nur Administratoren Zugriff haben.  Nehmen wir an, in allen anderen Pods haben sie eingeschränkte Rechte.  Da Entwickler höchstwahrscheinlich keine privilegierten Pods in Ihrem Cluster ausführen müssen. </p><br><p>  Und bei uns scheint alles in Ordnung zu sein.  Und unser Kubernetes-Cluster kann nicht in zwei Minuten gehackt werden. </p><br><p>  Es gibt ein Problem.  Wenn Sie einen Kubernetes-Cluster haben, wird höchstwahrscheinlich die Überwachung in Ihrem Cluster installiert.  Ich gehe sogar davon aus, dass Prometheus als Überwachung bezeichnet wird, wenn in Ihrem Cluster eine Überwachung stattfindet. </p><br><p>  Was ich Ihnen jetzt sagen werde, gilt sowohl für den Prometheus-Betreiber als auch für den Prometheus, der in seiner reinen Form geliefert wird.  Die Frage ist, dass ich mehr suchen muss, wenn ich den Administrator nicht so schnell in den Cluster einbinden kann.  Und ich kann mit Ihrer Überwachung suchen. </p><br><p>  Wahrscheinlich lesen alle die gleichen Artikel über Habré, und die Überwachung ist in der Überwachung.  Die Helmkarte wird für alle ungefähr gleich genannt.  Ich gehe davon aus, dass Sie ungefähr die gleichen Namen erhalten, wenn Sie Stable / Prometheus installieren.  Und selbst höchstwahrscheinlich muss ich den DNS-Namen in Ihrem Cluster nicht erraten.  Weil es Standard ist. </p><br><p><img src="https://habrastorage.org/webt/o6/rv/fw/o6rvfw0idykw0wyxivpfmwkd_vy.jpeg"></p><br><p>  Weiter haben wir eine bestimmte Entwicklung, in der es möglich ist, eine bestimmte Unter zu starten.  Und weiter von diesem Herd entfernt ist es sehr einfach, dies zu tun: </p><br><pre> <code class="plaintext hljs">$ curl http://prometheus-kube-state-metrics.monitoring</code> </pre> <br><p>  prometheus-kube-state -metrics ist einer der prometheus-Exporteure, der Metriken aus der Kubernetes-API sammelt.  In Ihrem Cluster werden viele Daten ausgeführt, was es ist, welche Probleme Sie damit haben. </p><br><p>  Als einfaches Beispiel: </p><br><p>  kube_pod_container_info {namespace = "kube-system", pod = "kube-apiserver-k8s-1", container = "kube-apiserver", image = </p><br><p>  <strong>"gcr.io/google-containers/kube-apiserver:v1.14.5"</strong> </p><br><p>  , Image_id = "Docker-ziehbar: //gcr.io/google-containers/kube- apiserver @ sha256: e29561119a52adad9edc72bfe0e7fcab308501313b09bf99df4a96 38ee634989", container_id = "Docker: // 7cbe7b1fea33f811fdd8f7e0e079191110268f2 853397d7daf08e72c22d3cf8b"} 1 </p><br><p>  Nachdem Sie eine einfache Curl-Anfrage aus einer nicht privilegierten Datei gestellt haben, können Sie solche Informationen erhalten.  Wenn Sie nicht wissen, in welcher Version von Kubernetes Sie ausgeführt werden, können Sie dies leicht feststellen. </p><br><p>  Und das Interessanteste ist, dass Sie sich neben der Tatsache, dass Sie sich den Kubikzustandsmetriken zuwenden, genauso direkt auf Prometheus selbst anwenden können.  Von dort aus können Sie Metriken sammeln.  Von dort aus können Sie sogar Metriken erstellen.  Selbst theoretisch können Sie eine solche Anforderung aus einem Cluster in Prometheus erstellen, wodurch sie einfach deaktiviert wird.  Und Ihre Überwachung funktioniert im Allgemeinen nicht mehr im Cluster. </p><br><p>  Und hier stellt sich bereits die Frage, ob eine externe Überwachung Ihre Überwachung überwacht.  Ich hatte gerade die Gelegenheit, im Kubernetes-Cluster ohne Konsequenzen für mich selbst zu agieren.  Sie werden nicht einmal wissen, dass ich dort agiere, da es keine Überwachung mehr gibt. </p><br><p>  Genau wie bei PSP scheint das Problem zu sein, dass all diese trendigen Technologien - Kubernetes, Prometheus - einfach nicht funktionieren und voller Löcher sind.  Nicht wirklich. </p><br><p>  Es gibt so etwas - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netzwerkrichtlinien</a> . </p><br><p>  Wenn Sie ein normaler Administrator sind, wissen Sie höchstwahrscheinlich über die Netzwerkrichtlinie, dass dies ein weiteres Yaml ist, das im Cluster bereits dofig ist.  Und einige Netzwerkrichtlinien werden definitiv nicht benötigt.  Und selbst wenn Sie lesen, was Netzwerkrichtlinie ist, was die Kubernetes-Yaml-Firewall ist, können Sie damit die Zugriffsrechte zwischen Namespaces und Pods einschränken. Dann haben Sie sicher entschieden, dass die Yaml-Firewall in Kubernetes in den nächsten Abstraktionen enthalten ist ... Nein, nicht .  Dies ist definitiv nicht notwendig. </p><br><p>  Selbst wenn Ihren Sicherheitsspezialisten nicht mitgeteilt wurde, dass Sie mit Ihren Kubernetes sehr einfach und unkompliziert eine Firewall erstellen können, ist diese sehr detailliert.  Wenn sie dies immer noch nicht wissen und Sie nicht ziehen: "Nun, geben, geben ..." In jedem Fall benötigen Sie eine Netzwerkrichtlinie, um den Zugriff auf einige offizielle Orte zu blockieren, die Sie ohne Genehmigung aus Ihrem Cluster abrufen können. </p><br><p>  Wie in dem von mir zitierten Beispiel können Sie die Kube-Statusmetriken aus einem beliebigen Namespace im Kubernetes-Cluster abrufen, ohne Rechte dafür zu haben.  Netzwerkrichtlinien schlossen den Zugriff aller anderen Namespaces auf die Namespace-Überwachung und sozusagen auf alles: kein Zugriff, keine Probleme.  In allen vorhandenen Diagrammen, sowohl im Standard-Prometeus als auch im Prometeus im Operator, gibt es in den Steuerwerten lediglich eine Option, um einfach Netzwerkrichtlinien für sie zu aktivieren.  Sie müssen es nur einschalten und sie werden funktionieren. </p><br><p>  Hier gibt es wirklich ein Problem.  Als normaler bärtiger Administrator haben Sie höchstwahrscheinlich entschieden, dass Netzwerkrichtlinien nicht benötigt werden.  Und nachdem Sie alle Arten von Artikeln über Ressourcen wie Habr gelesen haben, haben Sie entschieden, dass Flanell, insbesondere im Host-Gateway-Modus, das Beste ist, was Sie auswählen können. </p><br><p>  Was zu tun ist? </p><br><p>  Sie können versuchen, die Netzwerklösung in Ihrem Kubernetes-Cluster erneut bereitzustellen und durch eine funktionalere zu ersetzen.  Zum Beispiel auf demselben Calico.  Aber sofort möchte ich sagen, dass die Aufgabe, die Netzwerklösung im Kubernetes-Arbeitscluster zu ändern, nicht trivial ist.  Ich habe es zweimal gelöst (beide Male jedoch theoretisch), aber wir haben sogar gezeigt, wie man das auf den Slurms macht.  Für unsere Schüler haben wir gezeigt, wie die Netzwerklösung im Kubernetes-Cluster geändert werden kann.  Im Prinzip können Sie versuchen, sicherzustellen, dass im Produktionscluster keine Ausfallzeiten auftreten.  Aber Sie werden wahrscheinlich keinen Erfolg haben. </p><br><p>  Und das Problem ist eigentlich sehr einfach gelöst.  Der Cluster enthält Zertifikate, und Sie wissen, dass Ihre Zertifikate in einem Jahr fehlerhaft sein werden.  Nun, und normalerweise eine normale Lösung mit Zertifikaten im Cluster - warum werden wir dämpfen, wir werden einen neuen Cluster daneben erstellen, ihn im alten faulen lassen und alles wiederholen.  Es stimmt, wenn es schlecht wird, wird sich in unserer Zeit alles hinlegen, aber dann ein neuer Cluster. </p><br><p>  Wenn Sie einen neuen Cluster erstellen, fügen Sie gleichzeitig Calico anstelle von Flanell ein. </p><br><p>  Was tun, wenn Sie seit hundert Jahren Zertifikate ausgestellt haben und den Cluster nicht erneut gruppieren?  Es gibt so etwas wie Kube-RBAC-Proxy.  Dies ist eine sehr coole Entwicklung, mit der Sie sich als Beiwagencontainer in jeden Herd im Kubernetes-Cluster einbetten können.  Und sie fügt diesem Pod tatsächlich eine Autorisierung durch Kubernetes RBAC hinzu. </p><br><p>  Es gibt ein Problem.  Zuvor war Kube-RBAC-Proxy in den Prometheus des Betreibers eingebaut.  Aber dann war er weg.  Moderne Versionen basieren jetzt auf der Tatsache, dass Sie über eine Netzwerkrichtlinie verfügen und diese nicht mehr verwenden.  Und so müssen Sie das Diagramm ein wenig umschreiben.  Wenn Sie zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesem Repository gehen</a> , gibt es Beispiele für die Verwendung als Beiwagen, und Sie müssen die Diagramme minimal neu schreiben. </p><br><p>  Es gibt noch ein kleines Problem.  Nicht nur Prometheus gibt seine Metriken an jeden weiter, der sie erhält.  Wir haben auch alle Komponenten des Kubernetes-Clusters, sie können ihre Metriken angeben. </p><br><p>  Aber wie gesagt, wenn Sie keinen Zugriff auf den Cluster erhalten und keine Informationen sammeln können, können Sie zumindest Schaden anrichten. </p><br><p>  Ich zeige Ihnen also schnell zwei Möglichkeiten, wie Sie Ihren Kubernetes-Cluster verderben können. </p><br><p>  Sie werden lachen, wenn ich Ihnen sage, dies sind zwei Fälle aus dem wirklichen Leben. </p><br><p>  Der erste Weg.  Die Ressourcen gehen zur Neige. </p><br><p>  Wir starten ein weiteres Special unter.  Er wird einen solchen Abschnitt haben. </p><br><pre> <code class="plaintext hljs">resources: requests: cpu: 4 memory: 4Gi</code> </pre> <br><p>  Wie Sie wissen, sind Anforderungen die Menge an CPU und Speicher, die auf dem Host für bestimmte Pods mit Anforderungen reserviert ist.  Wenn wir einen Vier-Kern-Host im Kubernetes-Cluster haben und vier CPUs mit Anforderungen dorthin kommen, bedeutet dies, dass keine Pods mit Anforderungen an diesen Host mehr kommen können. </p><br><p>  Wenn ich dies unter ausführe, mache ich einen Befehl: </p><br><pre> <code class="plaintext hljs">$ kubectl scale special-pod --replicas=...</code> </pre> <br><p>  Dann kann niemand anderes im Kubernetes-Cluster bereitstellen.  Denn in allen Knoten enden die Anfragen.  Und so stoppe ich Ihren Kubernetes-Cluster.  Wenn ich dies abends mache, kann ich den Einsatz für einige Zeit einstellen. </p><br><p>  Wenn wir uns die Kubernetes-Dokumentation noch einmal ansehen, werden wir so etwas wie den Grenzbereich sehen.  Es legt Ressourcen für Clusterobjekte fest.  Sie können ein Limit Range-Objekt in yaml schreiben, es auf bestimmte Namespaces anwenden - und weiter in diesem Namespace können Sie sagen, dass Sie über die Ressourcen für die Standard-, Maximum- und Minimum-Pods verfügen. </p><br><p>  Mit Hilfe eines solchen Dokuments können wir Benutzer in bestimmten Produkt-Namespaces von Teams auf die Möglichkeit beschränken, böse Dinge auf ihren Pods anzuzeigen.  Aber selbst wenn Sie dem Benutzer mitteilen, dass es unmöglich ist, Pods mit Anforderungen von mehr als einer CPU auszuführen, gibt es leider einen so wunderbaren Skalierungsbefehl, oder über das Dashboard können sie skalieren. </p><br><p>  Und von hier kommt die Methode Nummer zwei.  Wir starten 11 111 111 111 111 Herde.  Das sind elf Milliarden.  Das liegt nicht daran, dass ich eine solche Nummer gefunden habe, sondern daran, dass ich sie selbst gesehen habe. </p><br><p>  Die wahre Geschichte.  Am späten Abend wollte ich gerade das Büro verlassen.  Ich sehe, eine Gruppe von Entwicklern sitzt in der Ecke und macht hektisch etwas mit Laptops.  Ich gehe zu den Jungs und frage: "Was ist mit dir passiert?" </p><br><p>  Etwas früher, um neun Uhr abends, ging einer der Entwickler nach Hause.  Und er entschied: "Ich überspringe meine Bewerbung jetzt bis zu einer."  Ich klickte ein wenig und das Internet ein wenig langweilig.  Er klickte erneut auf das Gerät, drückte auf das Gerät und klickte auf die Eingabetaste.  Stocherte in allem, was er konnte.  Dann wurde das Internet lebendig - und alles begann bis zu diesem Datum zu skalieren. </p><br><p>  Diese Geschichte kam zwar nicht auf Kubernetes vor, damals war es Nomad.  Es endete mit der Tatsache, dass Nomad nach einer Stunde unserer Versuche, Nomad von hartnäckigen Versuchen abzuhalten, zusammenzuhalten, antwortete, dass er nicht aufhören würde zu kleben und nichts anderes tun würde.  "Ich bin müde, ich gehe."  Und zusammengerollt. </p><br><p>  Ich habe natürlich versucht, dasselbe bei Kubernetes zu tun.  Die elf Milliarden Schoten von Kubernetes waren nicht erfreut, sagte er: "Ich kann nicht.  Übertrifft die inneren Mundschützer. "  Aber 1.000.000.000 Herde könnten. </p><br><p>  Als Antwort auf eine Milliarde ging der Würfel nicht hinein.  Er begann wirklich zu skalieren.  Je weiter der Prozess ging, desto mehr Zeit brauchte er, um neue Herde zu schaffen.  Trotzdem ging der Prozess weiter.  Das einzige Problem ist, dass ich, wenn ich Pods unbegrenzt in meinem Namespace ausführen kann, auch ohne Anforderungen und Einschränkungen eine solche Anzahl von Pods mit einigen Aufgaben starten kann, dass sich bei diesen Aufgaben die Knoten aus dem Speicher und aus der CPU addieren.  Wenn ich so viele Herde betreibe, sollten die Informationen von ihnen in das Repository gehen, d. H. Usw.  Und wenn dort zu viele Informationen eintreffen, beginnt das Lagerhaus zu langsam zu verschenken - und bei Kubernetes beginnen langweilige Dinge. </p><br><p>  Und noch ein Problem ... Wie Sie wissen, sind die Steuerelemente von Kubernetes nicht nur eine zentrale Sache, sondern mehrere Komponenten.  Dort gibt es insbesondere einen Controller-Manager, einen Scheduler usw.  Alle diese Leute werden gleichzeitig unnötig dumme Arbeit verrichten, was im Laufe der Zeit immer mehr Zeit in Anspruch nehmen wird.  Der Controller-Manager erstellt neue Pods.  Der Scheduler versucht, einen neuen Knoten zu finden.  Neue Knoten in Ihrem Cluster werden höchstwahrscheinlich bald enden.  Der Kubernetes-Cluster beginnt langsamer und langsamer zu arbeiten. </p><br><p>  Aber ich beschloss, noch weiter zu gehen.  Wie Sie wissen, gibt es in Kubernetes so etwas wie Service.  Nun, und standardmäßig in Ihren Clustern funktioniert der Dienst höchstwahrscheinlich mithilfe von IP-Tabellen. </p><br><p>  Wenn Sie beispielsweise eine Milliarde Herde betreiben und dann mithilfe von Skripten Kubernetis zwingen, neue Dienste zu erstellen: </p><br><pre> <code class="plaintext hljs">for i in {1..1111111}; do kubectl expose deployment test --port 80 \ --overrides="{\"apiVersion\": \"v1\", \"metadata\": {\"name\": \"nginx$i\"}}"; done</code> </pre> <br><p>  Auf allen Knoten des Clusters werden ungefähr gleichzeitig ungefähr neue iptables-Regeln generiert.  Darüber hinaus werden für jeden Dienst eine Milliarde Iptables-Regeln generiert. </p><br><p>  Ich habe das Ganze an mehreren tausend, bis zu einem Dutzend, überprüft.  Und das Problem ist, dass bereits bei dieser Schwelle ssh auf dem Knoten ziemlich problematisch zu tun ist.  Weil sich die Pakete, die eine solche Anzahl von Ketten passieren, nicht sehr gut anfühlen. </p><br><p>  Und all dies wird auch mit Hilfe von Kubernetes gelöst.  Es gibt ein solches Ressourcenkontingentobjekt.  Legt die Anzahl der verfügbaren Ressourcen und Objekte für einen Namespace in einem Cluster fest.  Wir können in jedem Namespace des Kubernetes-Clusters ein yaml-Objekt erstellen.  Mit diesem Objekt können wir sagen, dass wir eine bestimmte Anzahl von Anforderungen und Grenzwerten für diesen Namespace zugewiesen haben, und dann können wir sagen, dass es möglich ist, 10 Dienste und 10 Pods in diesem Namespace zu erstellen.  Und ein einzelner Entwickler kann sich zumindest abends quetschen.  Kubernetes wird zu ihm sagen: "Sie können Ihre Pods nicht auf einen solchen Betrag bringen, weil er die Ressourcenquote überschreitet."  Alles, das Problem ist gelöst.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Dokumentation finden Sie hier</a> . </p><br><p>  Ein problematischer Punkt ergibt sich im Zusammenhang damit.  Sie spüren, wie schwierig es wird, in Kubernetes einen Namespace zu erstellen.  Um es zu schaffen, müssen wir eine Reihe von Dingen berücksichtigen. </p><br><p>  Ressourcenkontingent + Grenzbereich + RBAC <br>  • Erstellen Sie einen Namespace <br>  • Erstellen Sie einen inneren Grenzbereich <br>  • Innerhalb des Ressourcenkontingents erstellen <br>  • Erstellen Sie ein Servicekonto für CI <br>  • Erstellen Sie eine Rollenbindung für CI und Benutzer <br>  • Führen Sie optional die erforderlichen Service-Pods aus </p><br><p>  Bei dieser Gelegenheit möchte ich daher meine Entwicklungen mitteilen.  Es gibt so etwas, den SDK-Operator.  Dies ist eine Möglichkeit im Kubernetes-Cluster, Operatoren dafür zu schreiben.  Sie können Anweisungen mit Ansible schreiben. </p><br><p>  Zuerst wurde es in Ansible geschrieben, und dann habe ich nach einem SDK-Operator gesucht und die Ansible-Rolle im Operator neu geschrieben.  Mit diesem Operator können Sie ein Objekt im Kubernetes-Cluster erstellen, das als Team bezeichnet wird.       yaml    .       ,    - . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> . </p><br><p>   .     ? <br> . Pod Security Policy —  .     ,            , -      . </p><br><p> Network Policy —   -    .  ,     . </p><br><p> LimitRange/ResourceQuota —   .     ,     ,     . ,   . </p><br><p>  ,      ,   ,    .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a> . </p><br><p>      .  ,           warlocks ,   . </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a>  ,   ,   .      ,  ResourceQuota, Pod Security Policy .     . </p><br><p>  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472484/">https://habr.com/ru/post/de472484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472468/index.html">ClusterJ - Arbeiten mit MySQL NDB Cluster von Java</a></li>
<li><a href="../de472470/index.html">Transgene Mäuse und Anti-Aging</a></li>
<li><a href="../de472472/index.html">Ferienhaus im Winter: sein oder nicht sein?</a></li>
<li><a href="../de472474/index.html">Lustiger kosmetischer Fehler in Google Chrome</a></li>
<li><a href="../de472482/index.html">Radioaktiver Unfall: Entdeckung einer festen stabilen Phase von Plutonium</a></li>
<li><a href="../de472486/index.html">Langzeitdatenspeicherung. (Artikel - Diskussion)</a></li>
<li><a href="../de472488/index.html">Dreißig Berichte von DevOops 2019: Tim Lister, Hadi Hariri, Roman Shaposhnik und andere Stars der internationalen DevOps</a></li>
<li><a href="../de472490/index.html">Wie ich mithilfe der Verarbeitung natürlicher Sprache nach einem Schönheitsstandard gesucht habe (und ihn nicht gefunden habe)</a></li>
<li><a href="../de472492/index.html">Analyse des ROOT-Codes, Scientific Data Analysis Framework</a></li>
<li><a href="../de472496/index.html">Erstellen eines Sammelalbum-Layouts in CSS Grid</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>