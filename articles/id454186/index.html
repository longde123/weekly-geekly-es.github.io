<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¤½ğŸ¿ ğŸ™‡ğŸ¼ ğŸ•´ğŸ» Buat saluran pipa untuk memproses data streaming. Bagian 1 ğŸ‘¨ğŸ¿â€ğŸ”¬ ğŸ¤µğŸ» ğŸ³</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Halo semuanya. Teman, kami berbagi dengan Anda terjemahan dari artikel yang disiapkan khusus untuk siswa kursus Insinyur Data . Ayo pergi! 



 Apache...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buat saluran pipa untuk memproses data streaming. Bagian 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/454186/">  Halo semuanya.  Teman, kami berbagi dengan Anda terjemahan dari artikel yang disiapkan khusus untuk siswa kursus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Insinyur Data</a> .  Ayo pergi! <br><br><img src="https://habrastorage.org/webt/fv/mo/wz/fvmowz46naiimojhlq03dhxm38w.png"><br><br><h2>  Apache Beam dan DataFlow untuk jaringan pipa real-time </h2><br>  Posting hari ini didasarkan pada tugas yang baru-baru ini saya kerjakan di tempat kerja.  Saya sangat senang mengimplementasikannya dan menggambarkan pekerjaan yang dilakukan dalam format posting blog, karena memberi saya kesempatan untuk bekerja di bidang teknik data, serta melakukan sesuatu yang akan sangat berguna bagi tim saya.  Belum lama ini, saya menemukan bahwa sistem kami menyimpan sejumlah besar log pengguna yang terkait dengan salah satu produk kami untuk bekerja dengan data.  Ternyata tidak ada yang menggunakan data ini, jadi saya langsung tertarik pada apa yang bisa kami ketahui jika kami mulai menganalisisnya secara teratur.  Namun, ada beberapa masalah di sepanjang jalan.  Masalah pertama adalah bahwa data disimpan dalam banyak file teks berbeda yang tidak tersedia untuk analisis instan.  Masalah kedua adalah bahwa mereka disimpan dalam sistem tertutup, jadi saya tidak bisa menggunakan alat analisis data favorit saya. <a name="habracut"></a><br><br>  Saya harus memutuskan bagaimana membuat akses lebih mudah bagi kami dan menambahkan setidaknya beberapa nilai dengan menanamkan sumber data ini di beberapa solusi interaksi pengguna kami.  Setelah berpikir sebentar, saya memutuskan untuk membangun saluran pipa untuk mentransfer data ini ke basis data cloud sehingga saya dan tim dapat mengaksesnya dan mulai membuat kesimpulan apa pun.  Setelah saya menyelesaikan spesialisasi saya di bidang Rekayasa Data di Coursera beberapa waktu lalu, saya sangat ingin menggunakan beberapa alat kursus dalam proyek ini. <br><br>  Jadi menempatkan data dalam database cloud sepertinya cara cerdas untuk menyelesaikan masalah pertama saya, tetapi apa yang bisa saya lakukan dengan masalah nomor 2?  Untungnya, ada cara untuk mentransfer data ini ke lingkungan di mana saya dapat mengakses alat-alat seperti Python dan Google Cloud Platform (GCP).  Namun, itu adalah proses yang panjang, jadi saya perlu melakukan sesuatu yang akan memungkinkan saya untuk melanjutkan pengembangan sementara saya sedang menunggu akhir transfer data.  Solusi yang saya buat adalah membuat data palsu menggunakan pustaka <b>Faker</b> dengan Python.  Saya belum pernah menggunakan perpustakaan ini sebelumnya, tetapi segera menyadari betapa bergunanya itu.  Menggunakan pendekatan ini memungkinkan saya untuk mulai menulis kode dan menguji pipa tanpa data aktual. <br><br>  Berdasarkan hal tersebut di atas, dalam posting ini saya akan memberi tahu Anda bagaimana saya membangun pipa yang dijelaskan di atas menggunakan beberapa teknologi yang tersedia di GCP.  Secara khusus, saya akan menggunakan <b>Apache Beam (versi untuk Python), Dataflow, Pub / Sub dan Big Query</b> untuk mengumpulkan log pengguna, mengonversi data dan mentransfernya ke database untuk analisis lebih lanjut.  Dalam kasus saya, saya hanya memerlukan fungsionalitas batch Beam, karena data saya tidak tiba secara real time, jadi Pub / Sub tidak diperlukan.  Namun, saya akan fokus pada versi streaming, karena inilah yang mungkin Anda temui dalam praktik. <br><br><h2>  Pengantar GCP dan Apache Beam </h2><br>  Google Cloud Platform menyediakan seperangkat alat yang sangat berguna untuk memproses data besar.  Berikut adalah beberapa alat yang akan saya gunakan: <br><br><ul>
<li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pub / Sub</a> adalah layanan pesan menggunakan template Publisher-Subscriber yang memungkinkan kita menerima data secara real time. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DataFlow</a> adalah layanan yang menyederhanakan pembuatan jalur pipa data dan secara otomatis menyelesaikan tugas-tugas seperti penskalaan infrastruktur, yang berarti bahwa kita hanya dapat fokus pada penulisan kode untuk saluran pipa kami. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BigQuery</a> adalah gudang data berbasis cloud.  Jika Anda terbiasa dengan database SQL lainnya, Anda tidak akan harus berurusan dengan BigQuery lama. </li><li>  Dan akhirnya, kita akan menggunakan Apache Beam, yaitu, fokus pada versi Python untuk membuat pipeline kita.  Alat ini akan memungkinkan kami untuk membuat saluran pipa untuk streaming atau pemrosesan batch yang terintegrasi dengan GCP.  Sangat berguna untuk pemrosesan paralel dan cocok untuk tugas-tugas seperti ekstraksi, transformasi, dan pemuatan (ETL), jadi jika kita perlu memindahkan data dari satu tempat ke tempat lain dengan transformasi atau perhitungan, Beam adalah pilihan yang baik. </li></ul><br><br>  Sejumlah besar alat tersedia di GCP, sehingga bisa jadi sulit untuk mencakup semuanya, termasuk tujuannya, namun demikian, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">berikut ini</a> ringkasan singkat untuk referensi. <br><br><h2>  Visualisasi conveyor kami </h2><br>  Mari kita visualisasikan komponen dari pipeline kita pada <i>Gambar 1</i> .  Pada tingkat tinggi, kami ingin mengumpulkan data pengguna secara real time, memprosesnya dan mentransfernya ke BigQuery.  Log dibuat ketika pengguna berinteraksi dengan produk dengan mengirimkan permintaan ke server, yang kemudian dicatat.  Data ini dapat sangat berguna untuk memahami bagaimana pengguna berinteraksi dengan produk kami dan apakah mereka berfungsi dengan benar.  Secara umum, konveyor akan berisi langkah-langkah berikut: <br><br><ol><li>  Data log pengguna kami dipublikasikan di bagian Pub / Sub. </li><li>  Kami akan terhubung ke Pub / Sub dan mengonversi data ke format yang sesuai menggunakan Python dan Beam (langkah 3 dan 4 pada Gambar 1). </li><li>  Setelah mengonversi data, Beam kemudian akan terhubung ke BigQuery dan menambahkannya ke tabel kami (langkah 4 dan 5 pada Gambar 1). </li><li>  Untuk analisis, kita dapat terhubung ke BigQuery menggunakan berbagai alat seperti Tableau dan Python. </li></ol><br>  Beam membuat proses ini sangat sederhana, terlepas dari apakah kita memiliki sumber data streaming atau file CSV, dan kami ingin melakukan pemrosesan batch.  Kemudian Anda akan melihat bahwa kode hanya berisi perubahan minimum yang diperlukan untuk beralih di antara mereka.  Ini adalah salah satu manfaat menggunakan Beam. <br><br><img src="https://habrastorage.org/webt/2s/nv/kc/2snvkcz6-jwybsxr2kyz3awipaw.png"><br>  <i>Gambar 1: Pipa data utama</i> <br><br><h2>  Membuat Data Pseudo Menggunakan Faker </h2><br>  Seperti yang saya sebutkan sebelumnya, karena keterbatasan akses ke data, saya memutuskan untuk membuat pseudo-data dalam format yang sama dengan yang sebenarnya.  Ini adalah latihan yang sangat berguna, karena saya bisa menulis kode dan menguji pipa sementara saya mengharapkan data.  Saya sarankan untuk melihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> Faker jika Anda ingin tahu apa lagi yang ditawarkan perpustakaan ini.  Data pengguna kami umumnya akan mirip dengan contoh di bawah ini.  Berdasarkan format ini, kita dapat menghasilkan data baris demi baris untuk mensimulasikan data waktu nyata.  Log ini memberi kami informasi seperti tanggal, jenis permintaan, respons dari server, alamat IP, dll. <br><br> <code>192.52.197.161 - - [30/Apr/2019:21:11:42] "PUT /tag/category/tag HTTP/1.1" [401] 155 "https://harris-lopez.com/categories/about/" "Mozilla/5.0 (Macintosh; PPC Mac OS X 10_11_2) AppleWebKit/5312 (KHTML, like Gecko) Chrome/34.0.855.0 Safari/5312"</code> <br> <br>  Berdasarkan baris di atas, kami ingin membuat variabel <b>LINE</b> kami menggunakan 7 variabel dalam kurung di bawah ini.  Kami juga akan menggunakannya sebagai nama variabel dalam skema tabel kami beberapa saat kemudian. <br><br> <code>LINE = """\ <br> {remote_addr} - - [{time_local}] "{request_type} {request_path} HTTP/1.1" [{status}] {body_bytes_sent} "{http_referer}" "{http_user_agent}"\ <br> """</code> <br> <br>  Jika kita melakukan pemrosesan batch, kodenya akan sangat mirip, meskipun kita perlu membuat satu set sampel dalam rentang waktu tertentu.  Untuk menggunakan pemalsu, kita cukup membuat objek dan memanggil metode yang kita butuhkan.  Secara khusus, Faker berguna untuk membuat alamat IP dan juga situs web.  Saya menggunakan metode berikut: <br><br> <code>fake.ipv4() <br> fake.uri_path() <br> fake.uri() <br> fake.user_agent() <br></code> <br><br><pre> <code class="php hljs">from faker import Faker import time import random import os import numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np from datetime import datetime, timedelta LINE = <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"\ {remote_addr} - - [{time_local}] "</span></span>{request_type} {request_path} HTTP/<span class="hljs-number"><span class="hljs-number">1.1</span></span><span class="hljs-string"><span class="hljs-string">" [{status}] {body_bytes_sent} "</span></span>{http_referer}<span class="hljs-string"><span class="hljs-string">" "</span></span>{http_user_agent}<span class="hljs-string"><span class="hljs-string">"\ "</span></span><span class="hljs-string"><span class="hljs-string">""</span></span> def generate_log_line(): fake = Faker() now = datetime.now() remote_addr = fake.ipv4() time_local = now.strftime(<span class="hljs-string"><span class="hljs-string">'%d/%b/%Y:%H:%M:%S'</span></span>) request_type = random.choice([<span class="hljs-string"><span class="hljs-string">"GET"</span></span>, <span class="hljs-string"><span class="hljs-string">"POST"</span></span>, <span class="hljs-string"><span class="hljs-string">"PUT"</span></span>]) request_path = <span class="hljs-string"><span class="hljs-string">"/"</span></span> + fake.uri_path() status = np.random.choice([<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">401</span></span>, <span class="hljs-number"><span class="hljs-number">404</span></span>], p = [<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>]) body_bytes_sent = random.choice(range(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) http_referer = fake.uri() http_user_agent = fake.user_agent() log_line = LINE.format( remote_addr=remote_addr, time_local=time_local, request_type=request_type, request_path=request_path, status=status, body_bytes_sent=body_bytes_sent, http_referer=http_referer, http_user_agent=http_user_agent ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> log_line</code> </pre> <br><br>  Akhir dari bagian pertama. <br><br>  Dalam beberapa hari mendatang, kami akan berbagi dengan Anda kelanjutan artikel, tetapi sekarang kami secara tradisional menunggu komentar ;-). <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian kedua</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id454186/">https://habr.com/ru/post/id454186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id454176/index.html">Uibook - alat pengujian visual untuk Bereaksi komponen dengan kueri media</a></li>
<li><a href="../id454178/index.html">Contoh menghitung pensiun karyawan TI dari Moskow</a></li>
<li><a href="../id454180/index.html">Cadangan Cloud SchrÃ¶dinger</a></li>
<li><a href="../id454182/index.html">Wawancara lengkap dengan Dekan Departemen Python di GeekBrains - bagaimana dan mengapa pemula belajar bahasa</a></li>
<li><a href="../id454184/index.html">KubeCon Europe 2019: Bagaimana Kami Pertama Kali Mengunjungi Acara Utama Kubernetes</a></li>
<li><a href="../id454188/index.html">Saluran rekrutmen alternatif</a></li>
<li><a href="../id454190/index.html">Apa yang tidak perlu Anda lakukan jika ponsel Anda dicuri</a></li>
<li><a href="../id454196/index.html">Pencetakan 3D elektronik menggunakan contoh drone: kabel dan papan tidak lagi diperlukan</a></li>
<li><a href="../id454198/index.html">Membuat proyek multi-modul Gradle SpringBoot + Angular di IDEA</a></li>
<li><a href="../id454204/index.html">Perayapan perilaku bukan obat mujarab?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>