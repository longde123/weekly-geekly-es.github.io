<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äçüî¨ üçÜ üà∫ Estamos desenvolvendo um ambiente para trabalhar com microsservi√ßos. Parte 1 instalando o Kubernetes HA no bare metal (Debian) ‚¨ÜÔ∏è üîö ‚ôäÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° queridos leitores da Habr! 


 Com esta publica√ß√£o, desejo iniciar uma s√©rie de artigos sobre a implanta√ß√£o de um ambiente de orquestra√ß√£o complet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Estamos desenvolvendo um ambiente para trabalhar com microsservi√ßos. Parte 1 instalando o Kubernetes HA no bare metal (Debian)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462473/"><p><img src="https://habrastorage.org/webt/aq/dx/l3/aqdxl3a5akxfl3rh9b7bzc_kqji.jpeg"></p><br><h4>  Ol√° queridos leitores da Habr! </h4><br><p>  Com esta publica√ß√£o, desejo iniciar uma s√©rie de artigos sobre a implanta√ß√£o de um ambiente de orquestra√ß√£o completo com cont√™ineres Kubernetes, que estar√° pronto para opera√ß√£o e lan√ßamento de aplicativos. <br>  Quero contar n√£o apenas sobre como implantar um cluster Kubernetes, mas tamb√©m sobre como configurar um cluster ap√≥s a instala√ß√£o, como adicionar ferramentas e complementos convenientes a ele para usar a arquitetura de microsservi√ßo. </p><br><h2>  Esse ciclo consistir√° em no m√≠nimo quatro artigos: </h2><br><ol><li>  No primeiro deles, mostrarei como instalar um cluster kubernetes √† prova de falhas no ferro puro, como instalar um painel padr√£o e configurar o acesso a ele, como instalar um controlador de entrada. </li><li>  No segundo artigo, mostrarei como implantar um cluster de failover Ceph e como come√ßar a usar volumes RBD em nosso cluster Kubernetes.  Tamb√©m abordarei outros tipos de armazenamento (armazenamento) e considerarei o armazenamento local com mais detalhes.  Al√©m disso, mostrarei como organizar o armazenamento tolerante a falhas do S3 com base no cluster CEPH criado </li><li>  No terceiro artigo, descreverei como implantar um cluster de failover MySql em nosso cluster Kubernetes, a saber, o Percona XtraDB Cluster no Kubernetes.  E tamb√©m descreverei todos os problemas que encontramos quando decidimos transferir o banco de dados para o kubernetes. </li><li>  No quarto artigo, tentarei reunir tudo e explicar como implantar e executar um aplicativo que usar√° os volumes de banco de dados e ceph.  Vou explicar como configurar o controlador de ingresso para acessar nosso aplicativo de fora e o servi√ßo de pedido autom√°tico de certificados em Let's Encrypt.  Outra √© como manter automaticamente esses certificados atualizados.  Tamb√©m abordaremos o RBAC no contexto de acesso ao painel de controle.  Vou falar em poucas palavras sobre o Helm e sua instala√ß√£o. <br>  Se voc√™ estiver interessado nas informa√ß√µes dessas publica√ß√µes, seja bem-vindo! <a name="habracut"></a></li></ol><br><h2>  Entrada: </h2><br><p>  Para quem s√£o esses artigos?  Primeiro de tudo, para aqueles que est√£o apenas come√ßando sua jornada no estudo de Kubernetes.  Al√©m disso, esse ciclo ser√° √∫til para engenheiros que est√£o pensando em mudar de um mon√≥lito para microsservi√ßos.  Tudo o que foi descrito √© a minha experi√™ncia, incluindo o obtido ao traduzir v√°rios projetos de um mon√≥lito para Kubernetes.  √â poss√≠vel que algumas partes das publica√ß√µes sejam de interesse de engenheiros experientes. </p><br><h4>  O que n√£o considerarei em detalhes nesta s√©rie de publica√ß√µes: </h4><br><ul><li>  explique em detalhes o que s√£o os primitivos do kubernetes, como: pod, implanta√ß√£o, servi√ßo, entrada etc. </li><li>  Considerarei a CNI (Container Networking Interface) muito superficialmente, usamos o callico, portanto, outras solu√ß√µes, apenas listarei. </li><li>  processo de cria√ß√£o da imagem do docker. </li><li>  Processos CI \ CD.  (Talvez uma publica√ß√£o separada, mas ap√≥s todo o ciclo) </li><li>  leme;  muito j√° foi escrito sobre ele, vou abordar apenas o processo de instala√ß√£o no cluster e configura√ß√£o do cliente. </li></ul><br><h4>  O que eu quero considerar em detalhes: </h4><br><ul><li>  Processo passo a passo da implanta√ß√£o do cluster Kubernetes.  Vou usar o kubeadm.  Mas, ao mesmo tempo, examinarei passo a passo o processo de instala√ß√£o de um cluster no bare metal, v√°rios tipos de instala√ß√£o do ETCD e a configura√ß√£o de arquivos para o kube admina.  Tentarei esclarecer todas as op√ß√µes de balanceamento para o controlador Ingress e a diferen√ßa nos v√°rios esquemas de acesso dos n√≥s de trabalho √† API do servidor. <br>  Eu sei que hoje para a implanta√ß√£o do kubernetes existem muitas ferramentas excelentes, como o kubespray ou o mesmo fazendeiro.  Talvez seja mais conveniente para algu√©m us√°-los.  Mas acho que existem muitos engenheiros que querem considerar o assunto com mais detalhes. </li><li>  Terminologia do CEPH e instala√ß√£o passo a passo do cluster CEPH, bem como instru√ß√µes passo a passo sobre como conectar o armazenamento ceph ao cluster criado pelo Kubernetes. </li><li>  armazenamentos locais, conex√£o com o cluster kubernetes, bem como diferen√ßas de conex√µes como caminho do host, etc. </li><li>  operadores kubernetes e a implanta√ß√£o do Percona XtraDB Cluster com a ajuda do operador, al√©m de tentar falar sobre os pr√≥s e contras dessa solu√ß√£o ap√≥s seis meses de experi√™ncia em produ√ß√£o.  E tamb√©m vou compartilhar alguns planos para finalizar o operador da percona. </li></ul><br><h2>  Sum√°rio: </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lista de hosts, recursos de host, vers√µes de SO e software</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Diagrama de HA do cluster Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Antes de come√ßar ou antes de come√ßar</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Preencha o arquivo create-config.sh</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Atualiza√ß√£o do kernel do SO</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Preparando n√≥s Instalando o Kubelet, Kubectl, Kubeadm e Docker</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalando o ETCD (v√°rias op√ß√µes)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Iniciando o primeiro assistente do kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instala√ß√£o CNI Callico</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lan√ßamento do segundo e terceiro assistentes kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Adicionar um n√≥ de trabalho ao cluster</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalar haproxy nos n√≥s do trabalhador para HA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalando o Ingress Controller</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalar interface da web (painel)</a> </li></ol><br><a name="vm"></a><br><h2>  Lista e destino de hosts </h2><br><p>  Todos os n√≥s do meu cluster estar√£o localizados em m√°quinas virtuais com um sistema extens√≠vel Debian 9 pr√©-instalado com o kernel 4.19.0-0.bpo.5-amd64.  Para virtualiza√ß√£o, eu uso o Proxmox VE. </p><br><h4>  Tabela VM e suas caracter√≠sticas de desempenho: </h4><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nome</b> </th><th>  <b>Endere√ßo IP</b> </th><th>  <b>Comente</b> </th><th>  <b>CPU</b> </th><th>  <b>Mem</b> </th><th>  <b>DISK1</b> </th><th>  <b>DISK2</b> </th></tr><tr><td>  master01 </td><td>  10.73.71.25 </td><td>  n√≥ principal </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  master02 </td><td>  10.73.71.26 </td><td>  n√≥ principal </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  master03 </td><td>  10.73.71.27 </td><td>  n√≥ principal </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  --- </td></tr><tr><td>  worknode01 </td><td>  10.73.75.241 </td><td>  n√≥ de trabalho </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr><tr><td>  worknode02 </td><td>  10.73.75.242 </td><td>  n√≥ de trabalho </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr><tr><td>  worknode03 </td><td>  10.73.75.243 </td><td>  n√≥ de trabalho </td><td>  4vcpu </td><td>  4gb </td><td>  HDD </td><td>  SSD </td></tr></tbody></table></div><br><p>  N√£o √© necess√°rio que voc√™ tenha exatamente essa configura√ß√£o de m√°quinas, mas ainda assim aconselho a seguir as recomenda√ß√µes da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o oficial</a> e, para mestres, aumentar a quantidade de RAM para pelo menos 4 GB.  Olhando para o futuro, direi que com um n√∫mero menor, peguei falhas no trabalho da CNI Callico <br>  O Ceph tamb√©m √© bastante voraz em termos de desempenho de mem√≥ria e disco. <br>  Nossas instala√ß√µes de produ√ß√£o funcionam sem virtualiza√ß√£o bare-metal, mas conhe√ßo muitos exemplos em que m√°quinas virtuais com recursos modestos eram suficientes.  Tudo depende de suas necessidades e cargas de trabalho. </p><br><h2>  Lista e vers√µes de software </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nome</b> </th><th>  <b>Vers√£o</b> </th></tr><tr><td>  Kubernetes </td><td>  1.15.1 </td></tr><tr><td>  Docker </td><td>  19.3.1 </td></tr></tbody></table></div><br><p>  A partir da vers√£o 1.14, o Kubeadm parou de oferecer suporte √† vers√£o da API v1alpha3 e mudou completamente para a vers√£o da API v1beta1, que ser√° suportada no futuro pr√≥ximo, portanto, neste artigo, falarei apenas da v1beta1. <br>  Portanto, acreditamos que voc√™ preparou as m√°quinas para o cluster kubernetes.  Eles s√£o acess√≠veis entre si pela rede, t√™m uma "conex√£o com a Internet" e um sistema operacional "limpo" instalado. <br>  Para cada etapa da instala√ß√£o, vou esclarecer em quais m√°quinas o comando ou bloco de comandos √© executado.  Execute todos os comandos como root, a menos que especificado de outra forma. <br>  Todos os arquivos de configura√ß√£o, bem como um script para sua prepara√ß√£o, est√£o dispon√≠veis para download no meu <a href="">github</a> <br>  Ent√£o, vamos come√ßar. </p><br><a name="ha-image"></a><br><h2>  Diagrama de HA do cluster Kubernetes </h2><br><p><img src="https://habrastorage.org/webt/ch/mx/pg/chmxpgzdyk0bvxwx7-6lawqfmvo.jpeg"><br>  Um diagrama aproximado do cluster de alta disponibilidade.  O artista meu √© mais ou menos assim, para ser honesto, mas tentarei explicar em poucas palavras e de maneira bastante simplista, sem me aprofundar na teoria. <br>  Portanto, nosso cluster consistir√° em tr√™s n√≥s principais e tr√™s n√≥s de trabalho.  Em cada n√≥ mestre do kubernetes, etcd (setas verdes no diagrama) e pe√ßas de servi√ßo do kubernetes funcionar√£o para n√≥s;  vamos cham√°-los genericamente - kubeapi. <br>  Atrav√©s do cluster mestre etcd, os n√≥s trocam o estado do cluster kubernetes.  Indicarei os mesmos endere√ßos dos pontos de entrada do controlador de ingresso para tr√°fego externo (setas vermelhas no diagrama) <br>  Nos n√≥s do trabalhador, o kubelet funciona para n√≥s, que se comunica com o servidor api do kubernetes por meio do haproxy instalado localmente em cada n√≥ do trabalhador.  Como o endere√ßo da API do servidor para o kubelet, usarei o host local 127.0.0.1:6443, e o haproxy no roundrobin dispersar√° as solicita√ß√µes em tr√™s n√≥s principais, al√©m de verificar a disponibilidade dos n√≥s principais.  Esse esquema nos permitir√° criar HA e, no caso de falha de um dos n√≥s principais, os n√≥s trabalhadores enviar√£o silenciosamente solicita√ß√µes aos dois n√≥s principais restantes. </p><br><a name="begin"></a><br><h2>  Antes de come√ßar </h2><br><p>  Antes de iniciar o trabalho em cada n√≥ do cluster, forneceremos os pacotes que precisaremos para trabalhar: </p><br><pre><code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y curl apt-transport-https git</code> </pre> <br><p>  Nos n√≥s principais, copie o reposit√≥rio com modelos de configura√ß√£o </p><br><pre> <code class="plaintext hljs">sudo -i git clone https://github.com/rjeka/kubernetes-ceph-percona.git</code> </pre> <br><p>  Verifique se o endere√ßo IP dos hosts nos assistentes corresponde ao endere√ßo no qual o servidor kubernetes escutar√° </p><br><pre> <code class="plaintext hljs">hostname &amp;&amp; hostname -i master01 10.73.71.25</code> </pre> <br><p>  e assim para todos os n√≥s principais. </p><br><p>  Certifique-se de desativar o SWAP, caso contr√°rio, o kubeadm gerar√° um erro </p><br><pre> <code class="plaintext hljs">[ERROR Swap]: running with swap on is not supported. Please disable swap</code> </pre> <br><p>  Voc√™ pode desativar o comando </p><br><pre> <code class="plaintext hljs">swapoff -a</code> </pre> <br><p>  Lembre-se de comentar em / etc / fstab </p><br><a name="create-config"></a><br><h2>  Preencha o arquivo create-config.sh </h2><br><p>  Para preencher automaticamente as configura√ß√µes necess√°rias para instalar o cluster kubernetes, enviei um pequeno script create-config.sh.  Voc√™ precisa preench√™-lo literalmente 8 linhas.  Indique os endere√ßos IP e o nome do host de seus mestres.  E tamb√©m especifique etcd tocken, voc√™ n√£o pode alter√°-lo.  Darei a seguir a parte do script na qual voc√™ precisa fazer altera√ß√µes. </p><br><pre> <code class="plaintext hljs">#!/bin/bash ####################################### # all masters settings below must be same ####################################### # master01 ip address export K8SHA_IP1=10.73.71.25 # master02 ip address export K8SHA_IP2=10.73.71.26 # master03 ip address export K8SHA_IP3=10.73.71.27 # master01 hostname export K8SHA_HOSTNAME1=master01 # master02 hostname export K8SHA_HOSTNAME2=master02 # master03 hostname export K8SHA_HOSTNAME3=master03 #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd #etcd version export ETCD_VERSION="v3.3.10"</code> </pre><br><a name="kernel"></a><br><h2>  Atualiza√ß√£o do kernel do SO </h2><br><p>  Esta etapa √© opcional, pois o kernel precisar√° ser atualizado pelas portas traseiras, e voc√™ faz isso por seu pr√≥prio risco e risco.  Talvez voc√™ nunca encontre esse problema e, se encontrar, poder√° atualizar o kernel mesmo ap√≥s a implanta√ß√£o do kubernetes.  Em geral, voc√™ decide. <br>  √â necess√°ria uma atualiza√ß√£o do kernel para corrigir o bug do docker antigo, que foi corrigido apenas na vers√£o 4.18 do kernel do linux.  Voc√™ pode ler mais sobre esse bug <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Um erro foi expresso no travamento peri√≥dico da interface de rede nos n√≥s do kubernetes com o erro: </p><br><pre> <code class="plaintext hljs">waiting for eth0 to become free. Usage count = 1</code> </pre> <br><p>  Depois de instalar o SO, tive a vers√£o 4.9 do kernel </p><br><pre> <code class="bash hljs">uname -a Linux master01 4.9.0-7-amd64 <span class="hljs-comment"><span class="hljs-comment">#1 SMP Debian 4.9.110-3+deb9u2 (2018-08-13) x86_64 GNU/Linux</span></span></code> </pre> <br><p>  Em cada m√°quina para kubernetes, executamos <br>  Etapa 1 <br>  Adicionar portas de volta √† lista de fontes </p><br><pre> <code class="plaintext hljs">echo deb http://ftp.debian.org/debian stretch-backports main &gt; /etc/apt/sources.list apt-get update apt-cache policy linux-compiler-gcc-6-x86</code> </pre> <br><p>  Etapa n√∫mero 2 <br>  Instala√ß√£o do pacote </p><br><pre> <code class="plaintext hljs">apt install -y -t stretch-backports linux-image-amd64 linux-headers-amd64</code> </pre> <br><p>  Etapa n√∫mero 3 <br>  Reiniciar </p><br><pre> <code class="plaintext hljs">reboot</code> </pre> <br><p>  Verifique se est√° tudo bem </p><br><pre> <code class="plaintext hljs">uname -a Linux master01 4.19.0-0.bpo.5-amd64 #1 SMP Debian 4.19.37-4~bpo9+1 (2019-06-19) x86_64 GNU/Linux</code> </pre><br><a name="kubelet"></a><br><h2>  Preparando n√≥s Instalando o Kubelet, Kubectl, Kubeadm e Docker </h2><br><h4>  Instale Kubelet, Kubectl, Kubeadm </h4><br><p>  Colocamos todos os n√≥s do cluster, de acordo com a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o dos kubernetes</a> </p><br><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Instalar o Docker </h4><br><p>  Instale a janela de encaixe de acordo com as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">instru√ß√µes da documenta√ß√£o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instala√ß√£o Instala√ß√£o do Kubelet, Kubectl, Kubeadm e docker usando o ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  No grupo de mestres, registre os mestres de ip. <br>  No grupo de trabalhadores, escreva o ip dos n√≥s de trabalho. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div><br><p>  Se, por algum motivo, voc√™ n√£o quiser usar a janela de encaixe, poder√° usar qualquer CRI.  Voc√™ pode ler sobre isso, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> , mas este t√≥pico est√° al√©m do escopo deste artigo. </p><br><a name="etcd"></a><br><h2>  Instala√ß√£o do ETCD </h2><br><p>  N√£o vou aprofundar a teoria, em poucas palavras: etcd √© um armazenamento de valor-chave distribu√≠do de c√≥digo aberto.  O etcd √© escrito no GO e usado no kubernetes de fato, como um banco de dados para armazenar o status do cluster.  Para uma revis√£o mais detalhada, consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes</a> . <br>  O etcd pode ser instalado de v√°rias maneiras.  Voc√™ pode instal√°-lo localmente e executar como um daemon, voc√™ pode execut√°-lo em cont√™ineres de encaixe, voc√™ pode instal√°-lo mesmo como parte inferior do kubernetes.  Voc√™ pode instal√°-lo manualmente ou instal√°-lo usando o kubeadm (eu n√£o tentei esse m√©todo).  Pode ser instalado em m√°quinas de cluster ou servidores individuais. <br>  Instalarei o etcd localmente nos n√≥s principais e executarei como um daemon atrav√©s do systemd, al√©m de considerar a instala√ß√£o no docker.  Eu uso o etcd sem TLS, se voc√™ precisar do TLS, consulte a documenta√ß√£o do etcd ou do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√≥prio kubernetes</a> <br>  Tamb√©m no meu github estar√° o ansible-playbook para instalar o etcd com o lan√ßamento pelo systemd. </p><br><h4>  Op√ß√£o n√∫mero 1 <br>  Instale localmente, execute o systemd </h4><br><p>  Em todos os mestres: (nos n√≥s de trabalho do cluster, esta etapa n√£o √© necess√°ria) <br>  Etapa 1 <br>  Fa√ßa o download e descompacte o arquivo com o etcd: </p><br><pre> <code class="plaintext hljs">mkdir archives cd archives export etcdVersion=v3.3.10 wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1</code> </pre> <br><p>  Etapa n√∫mero 2 <br>  Crie um arquivo de configura√ß√£o para ETCD </p><br><pre> <code class="plaintext hljs">cd .. ./create-config.sh etcd</code> </pre> <br><p>  O script aceita o valor etcd como uma entrada e gera um arquivo de configura√ß√£o no diret√≥rio etcd.  Ap√≥s a execu√ß√£o do script, o arquivo de configura√ß√£o finalizado estar√° localizado no diret√≥rio etcd. <br>  Para todas as outras configura√ß√µes, o script funciona com o mesmo princ√≠pio.  √â preciso alguma entrada e cria uma configura√ß√£o em um diret√≥rio espec√≠fico. </p><br><p>  Etapa n√∫mero 3 <br>  Iniciamos o cluster etcd e verificamos seu desempenho </p><br><pre> <code class="plaintext hljs">systemctl start etcd</code> </pre> <br><p>  Verificando o desempenho do daemon </p><br><pre> <code class="plaintext hljs">systemctl status etcd ‚óè etcd.service - etcd Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2019-07-07 02:34:28 MSK; 4min 46s ago Docs: https://github.com/coreos/etcd Main PID: 7471 (etcd) Tasks: 14 (limit: 4915) CGroup: /system.slice/etcd.service ‚îî‚îÄ7471 /usr/local/bin/etcd --name master01 --data-dir /var/lib/etcd --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 --advertise-client-urls http://10.73.71.25:2379,http://10.73.71. Jul 07 02:34:28 master01 etcd[7471]: b11e73358a31b109 [logterm: 1, index: 3, vote: 0] cast MsgVote for f67dd9aaa8a44ab9 [logterm: 2, index: 5] at term 554 Jul 07 02:34:28 master01 etcd[7471]: raft.node: b11e73358a31b109 elected leader f67dd9aaa8a44ab9 at term 554 Jul 07 02:34:28 master01 etcd[7471]: published {Name:master01 ClientURLs:[http://10.73.71.25:2379 http://10.73.71.25:4001]} to cluster d0979b2e7159c1e6 Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:4001, this is strongly discouraged! Jul 07 02:34:28 master01 systemd[1]: Started etcd. Jul 07 02:34:28 master01 etcd[7471]: ready to serve client requests Jul 07 02:34:28 master01 etcd[7471]: serving insecure client requests on [::]:2379, this is strongly discouraged! Jul 07 02:34:28 master01 etcd[7471]: set the initial cluster version to 3.3 Jul 07 02:34:28 master01 etcd[7471]: enabled capabilities for version 3.3 lines 1-19</code> </pre> <br><p>  E a sa√∫de do pr√≥prio cluster: </p><br><pre> <code class="plaintext hljs">etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy etcdctl member list 61db137992290fc: name=master03 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=master01 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=false f67dd9aaa8a44ab9: name=master02 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=true</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instale o etcd localmente com o ansible, execute o systemd</b> <div class="spoiler_text"><p>  Com o github, clonaremos o reposit√≥rio com o c√≥digo na m√°quina a partir da qual voc√™ executar√° o manual.  Esta m√°quina deve ter acesso ssh em uma chave para o mestre do nosso futuro cluster. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  No grupo de mestres, registre os mestres de ip. <br>  etcd_version √© a vers√£o do etcd.  Voc√™ pode v√™-lo na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">p√°gina etcd no github</a> .  No momento da reda√ß√£o deste artigo, havia a vers√£o v3.3.13 que eu estava usando a v3.3.10. <br>  etcdToken - voc√™ pode deixar o mesmo ou gerar o seu pr√≥prio. <br>  Execute a equipe do manual </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sudo c  ansible-playbook -i hosts.ini -l masters etcd.yaml -K BECOME password: &lt;sudo &gt; # sudo  ansible-playbook -i hosts.ini -l masters etcd.yaml</span></span></code> </pre> </div></div><br><p>  Se voc√™ deseja executar o etcd na janela de encaixe, h√° uma instru√ß√£o sob o spoiler. </p><br><div class="spoiler">  <b class="spoiler_title">Instale o etcd com docker-compose, inicie-o no docker</b> <div class="spoiler_text"><p>  Esses comandos devem ser executados em todos os n√≥s principais do cluster. <br>  Com o github, clonamos o reposit√≥rio com c√≥digo </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ceph-percona.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ceph-percona</code> </pre> <br><p>  etcd_version √© a vers√£o do etcd.  Voc√™ pode v√™-lo na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">p√°gina etcd no github</a> .  No momento da reda√ß√£o deste artigo, havia a vers√£o v3.3.13 que eu estava usando a v3.3.10. <br>  etcdToken - voc√™ pode deixar o mesmo ou gerar o seu pr√≥prio. </p><br><p>  Colocamos docker-compor </p><br><pre> <code class="plaintext hljs">apt-get install -y docker-compose</code> </pre> <br><p>  N√≥s geramos uma configura√ß√£o </p><br><pre> <code class="plaintext hljs">./create-config.sh docker</code> </pre> <br><p>  Execute a instala√ß√£o do cluster etcd na janela de encaixe </p><br><pre> <code class="plaintext hljs">docker-compose --file etcd-docker/docker-compose.yaml up -d</code> </pre> <br><p>  Verifique se os cont√™ineres est√£o abertos </p><br><pre> <code class="plaintext hljs">docker ps</code> </pre> <br><p>  E status do cluster etcd </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl cluster-health member 61db137992290fc is healthy: got healthy result from http://10.73.71.27:2379 member b11e73358a31b109 is healthy: got healthy result from http://10.73.71.25:2379 member f67dd9aaa8a44ab9 is healthy: got healthy result from http://10.73.71.26:2379 cluster is healthy root@master01:~/kubernetes-ceph-percona# docker exec -ti etcd etcdctl member list 61db137992290fc: name=etcd3 peerURLs=http://10.73.71.27:2380 clientURLs=http://10.73.71.27:2379,http://10.73.71.27:4001 isLeader=false b11e73358a31b109: name=etcd1 peerURLs=http://10.73.71.25:2380 clientURLs=http://10.73.71.25:2379,http://10.73.71.25:4001 isLeader=true f67dd9aaa8a44ab9: name=etcd2 peerURLs=http://10.73.71.26:2380 clientURLs=http://10.73.71.26:2379,http://10.73.71.26:4001 isLeader=false</code> </pre> <br><p>  Se algo desse errado </p><br><pre> <code class="plaintext hljs">docker logs etcd</code> </pre> </div></div><br><a name="master-one"></a><br><h2>  Iniciando o primeiro assistente do kubernetes </h2><br><p>  Primeiro de tudo, precisamos gerar uma configura√ß√£o para o kubeadmin </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Desmontamos uma configura√ß√£o para o kubeadm</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.73.71.25 #    API- --- apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: stable #      apiServer: #    kubeadm   certSANs: - 127.0.0.1 - 10.73.71.25 - 10.73.71.26 - 10.73.71.27 controlPlaneEndpoint: 10.73.71.25 #     etcd: #  etc external: endpoints: - http://10.73.71.25:2379 - http://10.73.71.26:2379 - http://10.73.71.27:2379 networking: podSubnet: 192.168.0.0/16 #   ,   CNI  .</code> </pre> <br><p>  Voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ler</a> sobre sub-redes CNI na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubernetes.</a> <br>  Esta √© uma configura√ß√£o minimamente funcional.  Para um cluster com tr√™s assistentes, voc√™ pode alter√°-lo para a configura√ß√£o do seu cluster.  Por exemplo, se voc√™ quiser usar 2 assistentes, basta especificar dois endere√ßos nos certSANs. <br>  Todos os par√¢metros de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">configura√ß√£o</a> podem ser encontrados na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">descri√ß√£o da API do kubeadm</a> . </p></div></div><br><p>  Iniciamos o primeiro mestre </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><p>  Se o kubeadm funcionar sem erros, na sa√≠da obteremos aproximadamente a seguinte sa√≠da: </p><br><pre> <code class="plaintext hljs">You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3 \ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><a name="callica"></a><br><h2>  Instala√ß√£o da CNI Calico </h2><br><p>  Chegou a hora de estabelecer uma rede na qual nossos pods funcionem.  Eu uso chita, e vamos coloc√°-lo. <br>  E para iniciantes, configure o acesso ao kubelet.  Executamos todos os comandos no master01 <br>  Se voc√™ estiver executando como root </p><br><pre> <code class="plaintext hljs">export KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><p>  Se sob o usu√°rio simples </p><br><pre> <code class="plaintext hljs">mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config</code> </pre> <br><p>  Voc√™ tamb√©m pode gerenciar o cluster no seu laptop ou em qualquer m√°quina local.  Para fazer isso, copie o arquivo /etc/kubernetes/admin.conf para o seu laptop ou qualquer outra m√°quina em $ HOME / .kube / config </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Colocamos a</a> CNI de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">acordo com a documenta√ß√£o do Kubernetes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml</code> </pre> <br><p>  Esperamos at√© que todos os pods subam </p><br><pre> <code class="plaintext hljs">watch -n1 kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-59f54d6bbc-psr2z 1/1 Running 0 96s kube-system calico-node-hm49z 1/1 Running 0 96s kube-system coredns-5c98db65d4-svcx9 1/1 Running 0 77m kube-system coredns-5c98db65d4-zdlb8 1/1 Running 0 77m kube-system kube-apiserver-master01 1/1 Running 0 76m kube-system kube-controller-manager-master01 1/1 Running 0 77m kube-system kube-proxy-nkdqn 1/1 Running 0 77m kube-system kube-scheduler-master01 1/1 Running 0 77m</code> </pre> <br><a name="mastes-other"></a><br><h2>  Lan√ßamento do segundo e terceiro assistentes kubernetes </h2><br><p>  Antes de iniciar o master02 e o master03, √© necess√°rio copiar os certificados com o master01 que o kubeadm gerou ao criar o cluster.  Vou copiar via scp <br>  Em master01 </p><br><pre> <code class="plaintext hljs">export master02=10.73.71.26 export master03=10.73.71.27 scp -r /etc/kubernetes/pki $master02:/etc/kubernetes/ scp -r /etc/kubernetes/pki $master03:/etc/kubernetes/</code> </pre> <br><p>  Em master02 e master03 <br>  Crie uma configura√ß√£o para o kubeadm </p><br><pre> <code class="plaintext hljs">./create-config.sh kubeadm</code> </pre> <br><p>  E adicione master02 e master03 ao cluster </p><br><pre> <code class="plaintext hljs">kubeadm init --config=kubeadmin/kubeadm-init.yaml</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Falhas em v√°rias interfaces de rede !!!!</b> <div class="spoiler_text"><p>  Na produ√ß√£o, eu uso o kubernetes v1.13.5 e o chico v3.3.  E eu n√£o tive essas falhas. <br>  Mas, ao preparar o artigo e usar a vers√£o est√°vel (no momento em que escrevi, isso era o kubernetes v1.15.1 e a vers√£o 3.8 callico), encontrei um problema expresso em erros de inicializa√ß√£o da CNI </p><br><pre> <code class="plaintext hljs">root@master01:~/kubernetes-ceph-percona# kubectl get pods -A -w NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-658558ddf8-t6gfs 0/1 ContainerCreating 0 11s kube-system calico-node-7td8g 1/1 Running 0 11s kube-system calico-node-dthg5 0/1 CrashLoopBackOff 1 11s kube-system calico-node-tvhkq 0/1 CrashLoopBackOff 1 11s</code> </pre> <br><p>  Esta √© uma falha do conjunto daemon chico quando o servidor possui v√°rias interfaces de rede <br>  No githab, h√° um problema nesta falha <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://github.com/projectcalico/calico/issues/2720</a> <br>  Isso √© resolvido editando o daemon set calico-node e adicionando o par√¢metro IP_AUTODETECTION_METHOD ao env </p><br><pre> <code class="plaintext hljs">kubectl edit -n kube-system ds calico-node</code> </pre> <br><p>  adicione o par√¢metro IP_AUTODETECTION_METHOD com o nome da sua interface na qual o assistente trabalha;  no meu caso, √© ens19 </p><br><pre> <code class="plaintext hljs">- name: IP_AUTODETECTION_METHOD value: ens19</code> </pre> <br><p><img src="https://habrastorage.org/webt/xe/pq/7h/xepq7hbpjwhgowfk0hkc6-ryw8a.png"><br>  Verifique se todos os n√≥s no cluster est√£o em funcionamento </p><br><pre> <code class="plaintext hljs"># kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 28m v1.15.1 master02 Ready master 26m v1.15.1 master03 Ready master 18m v1.15.1</code> </pre> <br><p>  E o que est√° vivo calica </p><br><pre> <code class="plaintext hljs"># kubectl get pods -A -o wide | grep calico kube-system calico-kube-controllers-59f54d6bbc-5lxgn 1/1 Running 0 27m kube-system calico-node-fngpz 1/1 Running 1 24m kube-system calico-node-gk7rh 1/1 Running 0 8m55s kube-system calico-node-w4xtt 1/1 Running 0 25m</code> </pre> </div></div><br><a name="worknodes"></a><br><h2>  Adicionar n√≥s de trabalho ao cluster </h2><br><p>  No momento, temos um cluster no qual tr√™s n√≥s principais est√£o em execu√ß√£o.  Mas n√≥s principais s√£o m√°quinas executando API, agendador e outros servi√ßos do cluster kubernetes.  Para podermos executar nossos pods, precisamos dos chamados n√≥s de trabalho. <br>  Se voc√™ tem recursos limitados, pode executar pods em n√≥s principais, mas pessoalmente n√£o aconselho fazer isso. </p><br><div class="spoiler">  <b class="spoiler_title">Executando as Lareiras em Masternodes</b> <div class="spoiler_text"><p>  Para permitir o lan√ßamento de lareiras nos n√≥s principais, execute o seguinte comando em qualquer um dos assistentes </p><br><pre> <code class="plaintext hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre> </div></div><br><p>  Instale os n√≥s kubelet, kubeadm, kubectl e docker no trabalhador e nos n√≥s principais </p><br><div class="spoiler">  <b class="spoiler_title">Instale o kubelet, kubeadm, kubectl e docker</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl</code> </pre> <br><h4>  Instalar o Docker </h4><br><p>  Instale a janela de encaixe de acordo com as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">instru√ß√µes da documenta√ß√£o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><br></a> </p><br><pre> <code class="plaintext hljs">apt-get remove docker docker-engine docker.io containerd runc apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</code> </pre> <br><pre> <code class="plaintext hljs">curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add - apt-key fingerprint 0EBFCD88</code> </pre> <br><pre> <code class="plaintext hljs">add-apt-repository \ "deb [arch=amd64] https://download.docker.com/linux/debian \ $(lsb_release -cs) \ stable"</code> </pre> <br><pre> <code class="plaintext hljs">apt-get update apt-get install docker-ce docker-ce-cli containerd.io</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Instala√ß√£o Instala√ß√£o do Kubelet, Kubectl, Kubeadm e docker usando o ansible</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/playbooks vim masters.ini</code> </pre> <br><p>  No grupo de mestres, registre os mestres de ip. <br>  No grupo de trabalhadores, escreva o ip dos n√≥s de trabalho. </p><br><pre> <code class="plaintext hljs"># sudo c  ansible-playbook -i hosts.ini kubelet.yaml -K ansible-playbook -i hosts.ini docker.yaml -K # sudo  ansible-playbook -i hosts.ini kubelet.yaml ansible-playbook -i hosts.ini docker.yaml</code> </pre> </div></div></div></div><br><p>  Agora √© hora de retornar √† linha que o kubeadm gerou quando instalamos o n√≥ principal. <br>  Ela parece assim para mim. </p><br><pre> <code class="plaintext hljs">kubeadm join 10.73.71.25:6443 --token ivwoap.259retezqf34amx8 \ --discovery-token-ca-cert-hash sha256:b5c93e32457c8e6478782ff62e8ef77acf72738dda59cd603cdf4821abe12ca3</code> </pre> <br><p>  √â necess√°rio executar este comando em cada n√≥ do trabalhador. <br>  Se voc√™ n√£o tiver escrito um token, poder√° gerar um novo </p><br><pre> <code class="plaintext hljs">kubeadm token create --print-join-command --ttl=0</code> </pre> <br><p>  Depois que o kubeadm funciona, seu novo n√≥ √© inserido no cluster e pronto para o trabalho </p><br><pre> <code class="plaintext hljs">This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster.</code> </pre> <br><p>  Agora vamos ver o resultado </p><br><pre> <code class="plaintext hljs">root@master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 10d v1.15.1 master02 Ready master 10d v1.15.1 master03 Ready master 10d v1.15.1 worknode01 Ready &lt;none&gt; 5m44s v1.15.1 worknode02 Ready &lt;none&gt; 59s v1.15.1 worknode03 Ready &lt;none&gt; 51s v1.15.1</code> </pre> <br><a name="haproxy"></a><br><h2>  Instalar haproxy em n√≥s de trabalho </h2><br><p>  Agora, temos um cluster de trabalho com tr√™s n√≥s principais e tr√™s n√≥s de trabalho. <br>  O problema √© que agora nossos n√≥s de trabalho n√£o t√™m o modo HA. <br>  Se voc√™ observar o arquivo de configura√ß√£o do kubelet, veremos que nossos n√≥s de trabalho acessam apenas um dos tr√™s n√≥s principais. </p><br><pre> <code class="plaintext hljs">root@worknode01:~# cat /etc/kubernetes/kubelet.conf | grep server: server: https://10.73.71.27:6443</code> </pre> <br><p>  No meu caso, isso √© master03.  Com essa configura√ß√£o, se o master03 travar, o n√≥ do trabalhador perder√° a comunica√ß√£o com o servidor da API do cluster.  Para tornar nosso cluster totalmente HA, instalaremos um Load Balancer (Haproxy) em cada um dos trabalhadores, que, de acordo com o round robin, espalhar√° solicita√ß√µes para tr√™s n√≥s principais e, na configura√ß√£o do kubelet nos n√≥s de trabalhadores, alteraremos o endere√ßo do servidor para 127.0.0.1:6443 <br>  Primeiro, instale o HAProxy em cada n√≥ do trabalhador. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">H√° uma boa folha de dicas para instala√ß√£o</a> </p><br><pre> <code class="plaintext hljs">curl https://haproxy.debian.net/bernat.debian.org.gpg | \ apt-key add - echo deb http://haproxy.debian.net stretch-backports-2.0 main | \ tee /etc/apt/sources.list.d/haproxy.list apt-get update apt-get install haproxy=2.0.\*</code> </pre> <br><p>  Ap√≥s a instala√ß√£o do HAproxy, precisamos criar uma configura√ß√£o para ele. <br>  Se nos n√≥s de trabalho n√£o houver um diret√≥rio com os arquivos de configura√ß√£o, n√≥s o clonaremos </p><br><pre> <code class="plaintext hljs">git clone https://github.com/rjeka/kubernetes-ceph-percona.git cd kubernetes-ceph-percona/</code> </pre> <br><p>  E execute o script de configura√ß√£o com o sinalizador haproxy </p><br><pre> <code class="plaintext hljs">./create-config.sh haproxy</code> </pre> <br><p>  O script ir√° configurar e reiniciar o haproxy. <br>  Verifique se o haproxy come√ßou a ouvir a porta 6443. </p><br><pre> <code class="plaintext hljs">root@worknode01:~/kubernetes-ceph-percona# netstat -alpn | grep 6443 tcp 0 0 127.0.0.1:6443 0.0.0.0:* LISTEN 30675/haproxy tcp 0 0 10.73.75.241:6443 0.0.0.0:* LISTEN 30675/haproxy</code> </pre> <br><p>  Agora precisamos dizer ao kubelet para acessar o host local em vez do n√≥ mestre.  Para fazer isso, edite o valor do servidor nos arquivos /etc/kubernetes/kubelet.conf e /etc/kubernetes/bootstrap-kubelet.conf em todos os n√≥s do trabalhador. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/kubelet.conf vim nano /etc/kubernetes/bootstrap-kubelet.conf</code> </pre> <br><p>  O valor do servidor deve assumir este formul√°rio: </p><br><pre> <code class="plaintext hljs">server: https://127.0.0.1:6443</code> </pre> <br><p>  Ap√≥s fazer as altera√ß√µes, reinicie os servi√ßos kubelet e docker </p><br><pre> <code class="plaintext hljs">systemctl restart kubelet &amp;&amp; systemctl restart docker</code> </pre> <br><p>  Verifique se todos os n√≥s est√£o funcionando corretamente. </p><br><pre> <code class="plaintext hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION master01 Ready master 29m v1.15.1 master02 Ready master 27m v1.15.1 master03 Ready master 26m v1.15.1 worknode01 Ready &lt;none&gt; 25m v1.15.1 worknode02 Ready &lt;none&gt; 3m15s v1.15.1 worknode03 Ready &lt;none&gt; 3m16s v1.15.1</code> </pre> <br><p>  At√© o momento, n√£o temos aplicativos no cluster para testar a HA.  Mas podemos parar a opera√ß√£o do kubelet no primeiro n√≥ principal e garantir que nosso cluster permane√ßa operacional. </p><br><pre> <code class="plaintext hljs">systemctl stop kubelet &amp;&amp; systemctl stop docker</code> </pre> <br><p>  Verifique a partir do segundo n√≥ principal </p><br><pre> <code class="plaintext hljs">root@master02:~# kubectl get nodes NAME STATUS ROLES AGE VERSION master01 NotReady master 15h v1.15.1 master02 Ready master 15h v1.15.1 master03 Ready master 15h v1.15.1 worknode01 Ready &lt;none&gt; 15h v1.15.1 worknode02 Ready &lt;none&gt; 15h v1.15.1 worknode03 Ready &lt;none&gt; 15h v1.15.1</code> </pre> <br><p>  Todos os n√≥s funcionam normalmente, exceto aquele em que paramos os servi√ßos. <br>  N√£o se esque√ßa de retornar os servi√ßos kubernetes no primeiro n√≥ principal </p><br><pre> <code class="plaintext hljs">systemctl start kubelet &amp;&amp; systemctl start docker</code> </pre> <br><a name="ingress"></a><br><h2>  Instalando o Ingress Controller </h2><br><p>  O controlador do Ingress √© o complemento do Kubernetes, com o qual podemos acessar nossos aplicativos de fora.  Uma descri√ß√£o detalhada est√° na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Kuberbnetes</a> .  Existem muitos controladores de entrada, eu uso um controlador da Nginx.  Eu vou falar sobre a instala√ß√£o dele.  A documenta√ß√£o sobre opera√ß√£o, configura√ß√£o e instala√ß√£o do controlador Ingress da Nginx pode ser lida no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site oficial</a> </p><br><p>  Vamos come√ßar a instala√ß√£o, todos os comandos podem ser executados com master01. <br>  Instale o pr√≥prio controlador </p><br><pre> <code class="plaintext hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml</code> </pre> <br><p>  E agora - um servi√ßo atrav√©s do qual a entrada estar√° dispon√≠vel <br>  Para fazer isso, prepare a configura√ß√£o </p><br><pre> <code class="plaintext hljs">./create-config.sh ingress</code> </pre> <br><p>  E envie para o nosso cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f ingress/service-nodeport.yaml</code> </pre> <br><p>  Verifique se o nosso Ingress funciona nos endere√ßos corretos e escuta nas portas corretas. </p><br><pre> <code class="plaintext hljs"># kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.99.35.95 10.73.71.25,10.73.71.26,10.73.71.27 80:31669/TCP,443:31604/TCP 10m</code> </pre> <br><pre> <code class="plaintext hljs"> kubectl describe svc -n ingress-nginx ingress-nginx Name: ingress-nginx Namespace: ingress-nginx Labels: app.kubernetes.io/name=ingress-nginx app.kubernetes.io/part-of=ingress-nginx Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"ingress-nginx","app.kubernetes.io/par... Selector: app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx Type: NodePort IP: 10.99.35.95 External IPs: 10.73.71.25,10.73.71.26,10.73.71.27 Port: http 80/TCP TargetPort: 80/TCP NodePort: http 31669/TCP Endpoints: 192.168.142.129:80 Port: https 443/TCP TargetPort: 443/TCP NodePort: https 31604/TCP Endpoints: 192.168.142.129:443 Session Affinity: None External Traffic Policy: Cluster Events: &lt;none&gt;</code> </pre> <br><a name="dashboard"></a><br><h2>  Instalar interface da web (painel) </h2><br><p>  O Kubernetes possui uma interface de usu√°rio da Web padr√£o, por meio da qual √†s vezes √© conveniente examinar rapidamente o estado de um cluster ou suas partes individuais.  No meu trabalho, costumo usar o painel para o diagn√≥stico inicial de implanta√ß√µes ou o estado de partes de um cluster. <br>  O link para a documenta√ß√£o est√° no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">site kubernetes</a> <br>  Instala√ß√£o  Estou usando a vers√£o est√°vel, ainda n√£o tentei o 2.0. </p><br><pre> <code class="plaintext hljs">#  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml # 2.0 kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta1/aio/deploy/recommended.yaml</code> </pre> <br><p>  Depois de instalar o painel em nosso cluster, o painel ficou dispon√≠vel em </p><br><pre> <code class="plaintext hljs">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.</code> </pre> <br><p>  Mas, para conseguir isso, precisamos encaminhar portas da m√°quina local usando o proxy kubectl.  Para mim, esse esquema n√£o √© muito conveniente.  Portanto, alterarei o servi√ßo do painel de controle para que o painel fique dispon√≠vel no endere√ßo de qualquer n√≥ do cluster na porta 30443. Ainda existem outras maneiras de acessar o painel, por exemplo, atrav√©s da entrada.  Talvez eu considere esse m√©todo nas seguintes publica√ß√µes. <br>  Para alterar o servi√ßo, execute a implanta√ß√£o do servi√ßo alterado </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/service-nodeport.yaml</code> </pre> <br><p>  Resta criar o usu√°rio e o token de administrador para acessar o cluster por meio do painel </p><br><pre> <code class="plaintext hljs">kubectl apply -f dashboard/rbac.yaml kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')</code> </pre> <br><p>  Depois disso, voc√™ pode fazer login no painel de controle em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://10.73.71.25:30443</a> <br><img src="https://habrastorage.org/webt/p7/zu/8q/p7zu8qv47mwdsmdydtvyo7gs_y4.png"><br>  Tela inicial do painel <br><img src="https://habrastorage.org/webt/h2/ks/jq/h2ksjq_7egqatf4ulnl0zkwqvqk.png"></p><br><p>  Parab√©ns!  Se voc√™ atingiu esta etapa, possui um cluster de HA de kubernetes em funcionamento, pronto para a implanta√ß√£o de seus aplicativos. <br> Kubernetes    ,      .          . <br>       ,  GitHub,    ,    . <br> C ,  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt462473/">https://habr.com/ru/post/pt462473/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt462461/index.html">Resultados da investiga√ß√£o de colis√£o GOES-17</a></li>
<li><a href="../pt462465/index.html">Usando os locais nativos da Apple</a></li>
<li><a href="../pt462467/index.html">Frontend Weekly Digest (29 de julho a 4 de agosto de 2019)</a></li>
<li><a href="../pt462469/index.html">Algumas considera√ß√µes para computa√ß√£o simult√¢nea no R para tarefas "corporativas"</a></li>
<li><a href="../pt462471/index.html">Resolvendo um trabalho com pwnable.kr 16 - uaf. Usar ap√≥s vulnerabilidade gratuita</a></li>
<li><a href="../pt462475/index.html">Alexey Savvateev: Como combater a corrup√ß√£o com a ajuda da matem√°tica (Pr√™mio Nobel de Economia para 2016)</a></li>
<li><a href="../pt462477/index.html">Cientistas afirmam que a IA √© o autor de uma nova patente e est√£o tentando mudar a lei de patentes</a></li>
<li><a href="../pt462479/index.html">Encaminhamento de privil√©gio local do cliente Windows do Steam 0 dia</a></li>
<li><a href="../pt462481/index.html">Tipo Perguntas frequentes do sistema</a></li>
<li><a href="../pt462483/index.html">Programa√ß√£o funcional: um brinquedo maluco que mata a produtividade do trabalho. Parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>