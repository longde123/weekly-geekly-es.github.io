<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚è∏Ô∏è üñêüèª üê≤ Grokay PyTorch üçí üìΩÔ∏è üôà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! 

 Wir haben ein lang erwartetes Buch √ºber die PyTorch-Bibliothek vorbestellt . 



 Da Sie in diesem Buch alle erforderlichen Grundmateri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Grokay PyTorch</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/471228/"> Hallo Habr! <br><br>  Wir haben ein lang erwartetes Buch √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die PyTorch-Bibliothek vorbestellt</a> . <br><br><img src="https://habrastorage.org/webt/bt/am/vl/btamvlvzqw01qwk-_2mq08t-sh4.jpeg"><br><br>  Da Sie in diesem Buch alle erforderlichen Grundmaterialien zu PyTorch kennenlernen, erinnern wir Sie an die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vorteile eines</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prozesses</a> namens ‚ÄûGrokking‚Äú oder ‚Äûtiefgreifendes Verst√§ndnis‚Äú des Themas, das Sie lernen m√∂chten.  Im heutigen Beitrag werden wir Ihnen erz√§hlen, wie Kai Arulkumaran PyTorch zugeschlagen hat (kein Bild).  Willkommen bei Katze. <br><a name="habracut"></a><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PyTorch</a> ist ein flexibles Deep-Learning-Framework, das automatisch zwischen Objekten unterscheidet, die dynamische neuronale Netzwerke verwenden ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dh</a> Netzwerke, die eine dynamische Flusssteuerung verwenden, z. B. <code>if</code> und <code>while</code> Schleifen).  PyTorch unterst√ºtzt GPU-Beschleunigung, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verteiltes Training</a> , verschiedene Arten der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Optimierung</a> und viele andere nette Funktionen.  Hier habe ich einige Gedanken dar√ºber gemacht, wie PyTorch meiner Meinung nach verwendet werden sollte.  Alle Aspekte der Bibliothek und empfohlene Vorgehensweisen werden hier nicht behandelt, aber ich hoffe, dieser Text wird Ihnen n√ºtzlich sein. <br><br>  Neuronale Netze sind eine Unterklasse von Rechengraphen.  Berechnungsgraphen empfangen Daten als Eingabe, dann werden diese Daten an den Knoten weitergeleitet (und k√∂nnen konvertiert werden), an denen sie verarbeitet werden.  Beim Deep Learning transformieren Neuronen (Knoten) normalerweise Daten, indem sie Parameter und differenzierbare Funktionen auf sie anwenden, so dass die Parameter optimiert werden k√∂nnen, um Verluste durch die Gradientenabstiegsmethode zu minimieren.  Im weiteren Sinne stelle ich fest, dass Funktionen stochastisch und graphendynamisch sein k√∂nnen.  W√§hrend neuronale Netze gut zum Paradigma der Datenflussprogrammierung passen, konzentriert sich die PyTorch-API auf das Paradigma der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">imperativen Programmierung</a> , und diese Art der Interpretation der erstellten Programme ist viel vertrauter.  Aus diesem Grund ist PyTorch-Code einfacher zu lesen und das Design komplexer Programme zu beurteilen. Dies erfordert jedoch keine ernsthaften Kompromisse bei der Leistung: PyTorch ist schnell genug und bietet viele Optimierungen, √ºber die Sie sich als Endbenutzer √ºberhaupt keine Sorgen machen k√∂nnen (Wenn Sie sich jedoch wirklich f√ºr sie interessieren, k√∂nnen Sie etwas tiefer graben und sie kennenlernen). <br><br>  Der Rest dieses Artikels ist eine Analyse des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Beispiels f√ºr den MNIST-Datensatz</a> .  Hier <i>spielen</i> wir PyTorch, daher empfehle ich, den Artikel erst nach Kenntnis der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Anf√§ngerhandb√ºcher</a> zu verstehen.  Der Einfachheit halber wird der Code in Form kleiner Fragmente mit Kommentaren dargestellt, dh er wird nicht in separate Funktionen / Dateien verteilt, die Sie in rein modularem Code gewohnt sind. <br><br><h4>  Importe </h4><br><pre> <code class="plaintext hljs">import argparse import os import torch from torch import nn, optim from torch.nn import functional as F from torch.utils.data import DataLoader from torchvision import datasets, transforms</code> </pre> <br>  All dies sind Standardimporte, mit Ausnahme von <code>torchvision</code> , die besonders aktiv zur L√∂sung von Aufgaben im Zusammenhang mit Computer Vision eingesetzt werden. <br><br><h4>  Anpassung </h4><br><pre> <code class="python hljs">parser = argparse.ArgumentParser(description=<span class="hljs-string"><span class="hljs-string">'PyTorch MNIST Example'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--batch-size'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">64</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'input batch size for training (default: 64)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--epochs'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'number of epochs to train (default: 10)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--lr'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'LR'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'learning rate (default: 0.01)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--momentum'</span></span>, type=float, default=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'M'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'SGD momentum (default: 0.5)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--no-cuda'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'disables CUDA training'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--seed'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">1</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'S'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'random seed (default: 1)'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--save-interval'</span></span>, type=int, default=<span class="hljs-number"><span class="hljs-number">10</span></span>, metavar=<span class="hljs-string"><span class="hljs-string">'N'</span></span>, help=<span class="hljs-string"><span class="hljs-string">'how many batches to wait before checkpointing'</span></span>) parser.add_argument(<span class="hljs-string"><span class="hljs-string">'--resume'</span></span>, action=<span class="hljs-string"><span class="hljs-string">'store_true'</span></span>, default=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, help=<span class="hljs-string"><span class="hljs-string">'resume training from checkpoint'</span></span>) args = parser.parse_args() use_cuda = torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> args.no_cuda device = torch.device(<span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span>) torch.manual_seed(args.seed) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> use_cuda: torch.cuda.manual_seed(args.seed)</code> </pre> <br>  <code>argparse</code> ist die Standardmethode zum <code>argparse</code> Befehlszeilenargumenten in Python. <br><br>  Wenn Sie Code schreiben m√ºssen, der f√ºr die Arbeit auf verschiedenen Ger√§ten ausgelegt ist (mithilfe der GPU-Beschleunigung, sofern verf√ºgbar, aber nicht auf Berechnungen auf der CPU zur√ºckgesetzt), w√§hlen Sie das entsprechende <code>torch.device</code> und speichern <code>torch.device</code> , mit dem Sie festlegen k√∂nnen, wo Sie es verwenden sollen Tensoren werden gespeichert.  Weitere Informationen zum Erstellen eines solchen Codes finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> .  Der Ansatz von PyTorch besteht darin, die Auswahl der Ger√§te der Benutzersteuerung zu unterziehen, was in einfachen Beispielen unerw√ºnscht erscheinen kann.  Dieser Ansatz vereinfacht jedoch die Arbeit erheblich, wenn Sie sich mit Tensoren befassen m√ºssen. A) ist praktisch f√ºr das Debuggen, b) erm√∂glicht es Ihnen, Ger√§te effektiv manuell zu verwenden. <br><br>  Um die Reproduzierbarkeit von Experimenten zu <code>numpy</code> , m√ºssen Sie zuf√§llige Anfangswerte f√ºr alle Komponenten festlegen, die eine Zufallszahlengenerierung verwenden (einschlie√ülich <code>random</code> oder <code>numpy</code> , wenn Sie diese <code>numpy</code> ).  Bitte beachten Sie: cuDNN verwendet nicht deterministische Algorithmen und ist optional mit <code>torch.backends.cudnn.enabled = False</code> deaktiviert. <br><br><h4>  Daten </h4><br><pre> <code class="python hljs">data_path = os.path.join(os.path.expanduser(<span class="hljs-string"><span class="hljs-string">'~'</span></span>), <span class="hljs-string"><span class="hljs-string">'.torch'</span></span>, <span class="hljs-string"><span class="hljs-string">'datasets'</span></span>, <span class="hljs-string"><span class="hljs-string">'mnist'</span></span>) train_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, download=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) test_data = datasets.MNIST(data_path, train=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((<span class="hljs-number"><span class="hljs-number">0.1307</span></span>,), (<span class="hljs-number"><span class="hljs-number">0.3081</span></span>,))])) train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) test_loader = DataLoader(test_data, batch_size=args.batch_size, num_workers=<span class="hljs-number"><span class="hljs-number">4</span></span>, pin_memory=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><br>  Da <code>torchvision</code> Modelle unter <code>~/.torch/models/</code> gespeichert <code>torchvision</code> , bevorzuge ich es, Torchvision- <code>torchvision</code> unter <code>~/.torch/datasets</code> zu speichern.  Dies ist meine Urheberrechtsvereinbarung, aber es ist sehr praktisch, sie in Projekten zu verwenden, die auf der Grundlage von MNIST, CIFAR-10 usw. entwickelt wurden.  Im Allgemeinen sollten Datasets getrennt vom Code gespeichert werden, wenn Sie mehrere Datasets wiederverwenden m√∂chten. <br><br>  <code>torchvision.transforms</code> enth√§lt viele praktische Konvertierungsoptionen f√ºr einzelne Bilder, z. B. Zuschneiden und Normalisieren. <br><br>  Es gibt viele Optionen im <code>batch_size</code> , aber neben <code>batch_size</code> und <code>shuffle</code> sollten Sie auch <code>num_workers</code> und <code>pin_memory</code> , um die Effizienz zu steigern.  <code>num_workers &gt; 0</code> verwendet <code>num_workers &gt; 0</code> zum asynchronen Laden von Daten und blockiert den Hauptprozess daf√ºr nicht.  Ein typischer Anwendungsfall besteht darin, Daten (z. B. Bilder) von einer Festplatte zu laden und m√∂glicherweise zu konvertieren.  All dies kann parallel zur Netzwerkdatenverarbeitung erfolgen.  Der Verarbeitungsgrad muss m√∂glicherweise angepasst werden, um a) die Anzahl der Mitarbeiter und folglich die Menge der verwendeten CPU und des verwendeten Arbeitsspeichers zu minimieren (jeder Mitarbeiter l√§dt einen separaten Stapel anstelle einzelner im Stapel enthaltener Stichproben). B) die Wartezeit der Daten im Netzwerk zu minimieren.  <code>pin_memory</code> verwendet <code>pin_memory</code> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Speicher</a> (im Gegensatz zu gepumptem), um Daten√ºbertragungsvorg√§nge vom RAM zur GPU zu beschleunigen (und macht nichts mit CPU-spezifischem Code). <br><br><h4>  Modell </h4><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.conv1 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2 = nn.Conv2d(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, kernel_size=<span class="hljs-number"><span class="hljs-number">5</span></span>) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">320</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = F.relu(F.max_pool2d(self.conv1(x), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="hljs-number"><span class="hljs-number">2</span></span>)) x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">320</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> F.log_softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) model = Net().to(device) optimiser = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> args.resume: model.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>)) optimiser.load_state_dict(torch.load(<span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>))</code> </pre> <br>  Die Netzwerkinitialisierung erstreckt sich normalerweise auf Mitgliedsvariablen, Ebenen, die Lernparameter und m√∂glicherweise einzelne Lernparameter und nicht trainierte Puffer enthalten.  Bei einem direkten Durchlauf werden sie dann in Kombination mit Funktionen von <code>F</code> , die rein funktional sind und keine Parameter enthalten.  Einige Leute arbeiten gerne mit rein funktionalen Netzwerken (z. B. Parameter <code>F.conv2d</code> und <code>F.conv2d</code> anstelle von <code>nn.Conv2d</code> ) oder Netzwerken, die vollst√§ndig aus Schichten bestehen (z. B. <code>nn.ReLU</code> anstelle von <code>F.relu</code> ). <br><br>  <code>.to(device)</code> ist eine bequeme M√∂glichkeit, Ger√§teparameter (und Puffer) an die GPU zu senden, wenn das <code>device</code> auf GPU eingestellt ist, da sonst (wenn das Ger√§t auf CPU eingestellt ist) nichts unternommen wird.  Es ist wichtig, die Ger√§teparameter auf das entsprechende Ger√§t zu √ºbertragen, bevor Sie sie an den Optimierer √ºbergeben.  Andernfalls kann der Optimierer die Parameter nicht korrekt verfolgen! <br><br>  Sowohl neuronale Netze ( <code>nn.Module</code> ) als auch Optimierer ( <code>optim.Optimizer</code> ) k√∂nnen ihren internen Status speichern und laden. Es wird empfohlen, dies mit <code>.load_state_dict(state_dict)</code> zu tun. Es ist erforderlich, den Status beider <code>.load_state_dict(state_dict)</code> zu laden, um das Training basierend auf zuvor gespeicherten W√∂rterb√ºchern <code>.load_state_dict(state_dict)</code> Staaten.  Das Speichern des gesamten Objekts kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mit Fehlern behaftet sein</a> .  Wenn Sie die Tensoren auf der GPU gespeichert haben und sie auf die CPU oder eine andere GPU laden m√∂chten, k√∂nnen Sie sie am einfachsten mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Option</a> <code>torch.load('model.pth'</code> direkt auf die CPU <code>map_location</code> , z. B. <code>torch.load('model.pth'</code> , <code>map_location='cpu'</code> ). <br><br>  Hier sind einige andere Punkte, die hier nicht gezeigt werden, aber erw√§hnenswert sind, dass Sie den Kontrollfluss mit einem direkten Durchlauf verwenden k√∂nnen (zum Beispiel kann die Ausf√ºhrung der <code>if</code> von der Mitgliedsvariablen oder von den Daten selbst abh√§ngen. Dar√ºber hinaus ist sie in der Mitte des auszugebenden Prozesses vollkommen g√ºltig ( <code>print</code> ) Tensoren, was das Debuggen erheblich vereinfacht. Schlie√ülich k√∂nnen mit einem direkten Durchlauf viele Argumente verwendet werden. Ich werde diesen Punkt mit einer kurzen Auflistung veranschaulichen, die nicht an eine bestimmte Idee gebunden ist: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, hx, drop=False)</span></span></span><span class="hljs-function">:</span></span> hx2 = self.rnn(x, hx) print(hx.mean().item(), hx.var().item()) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> hx.max.item() &gt; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> self.can_drop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> drop: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> hx2</code> </pre> <br><h4>  Schulung </h4><br><pre> <code class="python hljs">model.train() train_losses = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, (data, target) <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_loader): data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) optimiser.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() train_losses.append(loss.item()) optimiser.step() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(i, loss.item()) torch.save(model.state_dict(), <span class="hljs-string"><span class="hljs-string">'model.pth'</span></span>) torch.save(optimiser.state_dict(), <span class="hljs-string"><span class="hljs-string">'optimiser.pth'</span></span>) torch.save(train_losses, <span class="hljs-string"><span class="hljs-string">'train_losses.pth'</span></span>)</code> </pre> <br>  Netzwerkmodule werden standardm√§√üig in den Trainingsmodus versetzt - was sich in gewissem Ma√üe auf den Betrieb von Modulen auswirkt, vor allem auf das Ausd√ºnnen und die Chargennormalisierung.  Auf die eine oder andere Weise ist es besser, solche Dinge manuell mit <code>.train()</code> , wodurch das Flag "Training" f√ºr alle <code>.train()</code> Module gefiltert wird. <br><br>  Hier akzeptiert die <code>.to()</code> -Methode nicht nur das Ger√§t, sondern setzt auch <code>non_blocking=True</code> , wodurch ein asynchrones Kopieren von Daten aus dem festgeschriebenen Speicher auf die GPU sichergestellt wird, sodass die CPU w√§hrend der Daten√ºbertragung betriebsbereit bleibt.  Andernfalls ist <code>non_blocking=True</code> einfach keine Option. <br><br>  Bevor Sie mit <code>loss.backward()</code> einen neuen Satz von Verl√§ufen <code>loss.backward()</code> und mit <code>loss.backward()</code> <code>optimiser.step()</code> , m√ºssen Sie die Verl√§ufe der zu optimierenden Parameter manuell mit <code>optimiser.step()</code> zur√ºcksetzen.  Standardm√§√üig sammelt PyTorch Farbverl√§ufe. Dies ist sehr praktisch, wenn Sie nicht √ºber gen√ºgend Ressourcen verf√ºgen, um alle ben√∂tigten Farbverl√§ufe in einem Durchgang zu berechnen. <br><br>  PyTorch verwendet ein "Band" -System mit automatischen Verl√§ufen - es sammelt Informationen dar√ºber, welche Operationen und in welcher Reihenfolge an den Tensoren ausgef√ºhrt wurden, und spielt sie dann in die entgegengesetzte Richtung ab, um die Differenzierung in umgekehrter Reihenfolge durchzuf√ºhren (Differenzierung im umgekehrten Modus).  Deshalb ist es so superflexibel und erm√∂glicht beliebige Rechengraphen.  Wenn f√ºr keinen dieser Tensoren Farbverl√§ufe erforderlich sind (Sie m√ºssen <code>requires_grad=True</code> festlegen und zu diesem Zweck einen Tensor erstellen), wird kein Diagramm gespeichert!  Netzwerke haben jedoch normalerweise Parameter, die Gradienten erfordern, sodass alle Berechnungen, die auf der Grundlage der Netzwerkausgabe durchgef√ºhrt werden, in der Grafik gespeichert werden.  Wenn Sie also die aus diesem Schritt resultierenden Daten speichern m√∂chten, m√ºssen Sie die Verl√§ufe manuell deaktivieren oder (ein h√§ufigerer Ansatz) diese Informationen als Python-Nummer (mit <code>.item()</code> im PyTorch-Skalar) oder als <code>numpy</code> Array <code>numpy</code> .  Lesen Sie mehr √ºber <code>autograd</code> in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offiziellen Dokumentation</a> . <br><br>  Eine M√∂glichkeit, den Rechengraphen zu verk√ºrzen, besteht darin, <code>.detach()</code> wenn der verborgene Zustand beim Lernen von RNN mit einer abgeschnittenen Version der Backpropagation-through-Time √ºbergeben wird.  Es ist auch praktisch, wenn Verluste unterschieden werden, wenn eine der Komponenten die Ausgabe eines anderen Netzwerks ist, dieses andere Netzwerk jedoch nicht in Bezug auf Verluste optimiert werden sollte.  Als Beispiel werde ich den diskriminierenden Teil des bei der Arbeit mit dem GAN erzeugten Ausgabematerials oder das Richtlinientraining im Akteur-Kritiker-Algorithmus unter Verwendung der Zielfunktion als Basisfunktion (z. B. A2C) unterrichten.  Eine andere Technik, die die Berechnung von Gradienten verhindert, ist beim Training von GAN (Training des erzeugenden Teils auf diskriminantem Material) effektiv und typisch f√ºr die Feinabstimmung ist die zyklische Aufz√§hlung von Netzwerkparametern, f√ºr die <code>param.requires_grad = False</code> . <br><br>  Es ist wichtig, nicht nur die Ergebnisse in der Konsolen- / Protokolldatei aufzuzeichnen, sondern auch Kontrollpunkte in den Modellparametern (und im Optimierungsstatus) f√ºr alle F√§lle festzulegen.  Sie k√∂nnen auch <code>torch.save()</code> , um regul√§re Python-Objekte zu speichern, oder eine andere Standardl√∂sung verwenden - die integrierte <code>pickle</code> . <br><br><h4>  Testen </h4><br><pre> <code class="python hljs">model.eval() test_loss, correct = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> test_loader: data = data.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) target = target.to(device=device, non_blocking=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) output = model(data) test_loss += F.nll_loss(output, target, reduction=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>).item() pred = output.argmax(<span class="hljs-number"><span class="hljs-number">1</span></span>, keepdim=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_data) acc = correct / len(test_data) print(acc, test_loss)</code> </pre> <br>  In Reaktion auf <code>.train()</code> Netzwerke mit <code>.eval()</code> explizit in den Evaluierungsmodus versetzt werden. <br><br>  Wie oben erw√§hnt, wird bei Verwendung eines Netzwerks normalerweise ein Berechnungsgraph erstellt.  Um dies zu verhindern, verwenden Sie den <code>no_grad</code> <code>with torch.no_grad()</code> . <br><br><h4>  Noch mehr </h4><br>  Dies ist ein zus√§tzlicher Abschnitt, in dem ich einige weitere n√ºtzliche Abschweifungen vorgenommen habe. <br>  Hier ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">offizielle Dokumentation</a> , die das Arbeiten mit dem Ged√§chtnis erkl√§rt. <br><br>  CUDA-Fehler?  Es ist schwierig, sie zu beheben, und normalerweise sind sie mit logischen Inkonsistenzen verbunden, nach denen auf der CPU sinnvollere Fehlermeldungen angezeigt werden als auf der GPU.  Das Beste ist, wenn Sie mit der GPU arbeiten m√∂chten, k√∂nnen Sie schnell zwischen CPU und GPU wechseln.  Ein allgemeinerer Entwicklungstipp besteht darin, den Code so zu organisieren, dass er schnell √ºberpr√ºft werden kann, bevor eine vollst√§ndige Aufgabe gestartet wird.  Bereiten Sie beispielsweise einen kleinen oder synthetischen Datensatz vor, f√ºhren Sie einen Zug + Test f√ºr eine √Ñra aus usw.  Wenn es sich um einen CUDA-Fehler handelt oder Sie √ºberhaupt nicht zur CPU wechseln k√∂nnen, setzen Sie CUDA_LAUNCH_BLOCKING = 1.  Dadurch wird der CUDA-Kernel synchron gestartet und Sie erhalten genauere Fehlermeldungen. <br><br>  Ein Hinweis zu <code>torch.multiprocessing</code> oder zum gleichzeitigen Ausf√ºhren mehrerer PyTorch-Skripte.  Da PyTorch BLAS-Bibliotheken mit mehreren Threads verwendet, um lineare Algebra-Berechnungen auf der CPU zu beschleunigen, sind normalerweise mehrere Kerne beteiligt.  Wenn Sie mehrere Dinge gleichzeitig <code>OMP_NUM_THREADS</code> m√∂chten, indem Sie eine Multithread-Verarbeitung oder mehrere Skripte verwenden, kann es ratsam sein, deren Anzahl manuell zu reduzieren, indem Sie die Umgebungsvariable <code>OMP_NUM_THREADS</code> auf 1 oder einen anderen niedrigen Wert setzen.  Somit wird die Wahrscheinlichkeit eines Durchrutschens des Prozessors verringert.  Die offizielle Dokumentation enth√§lt weitere Kommentare zur Multithread-Verarbeitung. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de471228/">https://habr.com/ru/post/de471228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de471212/index.html">10 Tipps und Tricks, mit denen Sie der beste Entwickler f√ºr VueJS werden k√∂nnen</a></li>
<li><a href="../de471216/index.html">Die lange Geschichte des Reisef√ºhrers - wie ich 5 Jahre lang einen Dienst f√ºr intelligente Wanderwege geschrieben habe</a></li>
<li><a href="../de471220/index.html">Cockpit - Vereinfachen Sie typische Verwaltungsaufgaben unter Linux √ºber eine praktische Weboberfl√§che</a></li>
<li><a href="../de471222/index.html">Das Verst√§ndnis der Datenschutzrichtlinien f√ºr Anwendungen und Dienste hilft neuronalen Netzen</a></li>
<li><a href="../de471226/index.html">Linux hat viele Gesichter: wie man an jeder Distribution arbeitet</a></li>
<li><a href="../de471232/index.html">Meine Erfahrung mit der Verbindung von LPS331AP mit Omega Onion2</a></li>
<li><a href="../de471236/index.html">Dosimeter f√ºr Seryozha. Teil III. Nationales Radiometer</a></li>
<li><a href="../de471240/index.html">"Bitchy Betty" und moderne Audio-Interfaces: Warum sprechen sie mit weiblicher Stimme?</a></li>
<li><a href="../de471242/index.html">Einf√ºhrung in Bash Shell</a></li>
<li><a href="../de471244/index.html">Rosetta-Code: Messen Sie die L√§nge des Codes in einer Vielzahl von Programmiersprachen und untersuchen Sie die N√§he der Sprachen zueinander</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>