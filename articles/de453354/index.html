<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õπÔ∏è üìé üßïüèª Klassifizierung der Landbedeckung mittels Eo-Learn. Teil 3 üë®üèø‚Äçüåæ üöù üí™</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wenn Sie bessere Ergebnisse als zufriedenstellend ben√∂tigen 


 Teil 1 
 Teil 2 





 Der √úbergang der Zone vom Winter zum Sommer besteht aus Sentine...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Klassifizierung der Landbedeckung mittels Eo-Learn. Teil 3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453354/"><p>  Wenn Sie bessere Ergebnisse als zufriedenstellend ben√∂tigen </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2</a> </p><br><p><img src="https://habrastorage.org/webt/c0/ls/b2/c0lsb2it_c9qwggm74kdk3uglw4.png"></p><br><p> <em>Der √úbergang der Zone vom Winter zum Sommer besteht aus Sentinel-2-Bildern.</em>  <em>Sie k√∂nnen einige Unterschiede in den Arten der Abdeckung im Schnee feststellen, die in einem vorherigen Artikel beschrieben wurden.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Vorwort </h2><br><p> Die letzten Wochen waren sehr schwierig.  Wir haben den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweiten</a> Teil unserer Artikel √ºber die Klassifizierung der Deckung im ganzen Land unter Verwendung des <code>eo-learn</code> Frameworks ver√∂ffentlicht.  <code>eo-learn</code> ist eine Open-Source-Bibliothek zum Erstellen einer Ebene zwischen dem Empfangen und Verarbeiten von Satellitenbildern und maschinellem Lernen.  In fr√ºheren Artikeln in den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beispielen haben</a> wir nur einen kleinen Teil der Daten angegeben und die Ergebnisse nur f√ºr einen kleinen Prozentsatz des gesamten Interessenbereichs (AOI - Interessenbereich) gezeigt.  Ich wei√ü, dass dies zumindest nicht sehr beeindruckend und vielleicht sehr unh√∂flich von unserer Seite aussieht.  W√§hrend dieser ganzen Zeit wurden Sie von Fragen gequ√§lt, wie Sie dieses Wissen nutzen und auf die <em>n√§chste</em> Ebene √ºbertragen k√∂nnen. </p><br><p>  Keine Sorge, daf√ºr ist der dritte Artikel in dieser Reihe gedacht!  Nehmen Sie sich eine Tasse Kaffee und nehmen Sie Platz ... </p><br><h2 id="all-our-data-are-belong-to-you">  Alle unsere Daten geh√∂ren Ihnen! </h2><br><p>  Sitzen Sie schon  Vielleicht lassen Sie den Kaffee noch eine Sekunde auf dem Tisch, denn jetzt h√∂ren Sie die besten Nachrichten f√ºr heute ... <br>  Wir bei Sinergise haben beschlossen, den vollst√§ndigen Datensatz f√ºr Slowenien f√ºr 2017 zu ver√∂ffentlichen.  Kostenlos.  Sie k√∂nnen frei auf 200 GB Daten in Form von ~ 300 EOPatch-Fragmenten zugreifen, die jeweils ungef√§hr die Gr√∂√üe von 1000 x 1000 haben und eine Aufl√∂sung von 10 m haben!  Weitere Informationen zum EOPatch-Format finden Sie im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letzten Beitrag</a> zu <code>eo-learn</code> . Tats√§chlich handelt es sich jedoch um einen Container f√ºr <em>geo-temporale</em> EO- (Earth Observation) und Nicht-EO-Daten: z. B. Satellitenbilder, Masken, Karten usw. </p><br><p><img src="https://habrastorage.org/webt/dc/nt/gy/dcntgywsu4la7pdpwegv5m6eskc.png"><br>  <em>EOPatch-Struktur</em> ) </p><br><p>  Wir haben nicht gehackt, als wir diese Daten heruntergeladen haben.  Jedes EOPatch enth√§lt Sentinel-2 L1C-Bilder, die entsprechende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">s2cloudless-</a> Maske und die offizielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Landbedeckungskarte</a> im Rasterformat! </p><br><p>  Die Daten werden in AWS S3 unter folgender <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adresse</a> gespeichert: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://eo-learn.sentinel-hub.com/</a> </p><br><p>  Das Deserialisieren eines EOPatch-Objekts ist ganz einfach: </p><br><pre> <code class="python hljs">EOPatch.load(<span class="hljs-string"><span class="hljs-string">'path_to_eopatches/eopatch-0x6/'</span></span>)</code> </pre> <br><p>  Als Ergebnis erhalten Sie ein Objekt mit folgender Struktur: </p><br><pre> <code class="python hljs">EOPatch( data: { BANDS: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>), dtype=float32) } mask: { CLM: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_DATA: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) IS_VALID: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=bool) } mask_timeless: { LULC: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=uint8) VALID_COUNT: numpy.ndarray(shape=(<span class="hljs-number"><span class="hljs-number">1010</span></span>, <span class="hljs-number"><span class="hljs-number">999</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), dtype=int64) } meta_info: { maxcc: <span class="hljs-number"><span class="hljs-number">0.8</span></span> service_type: <span class="hljs-string"><span class="hljs-string">'wcs'</span></span> size_x: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> size_y: <span class="hljs-string"><span class="hljs-string">'10m'</span></span> time_difference: datetime.timedelta(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">86399</span></span>) time_interval: (datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">31</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)) } bbox: BBox(((<span class="hljs-number"><span class="hljs-number">370230.5261411405</span></span>, <span class="hljs-number"><span class="hljs-number">5085303.344972428</span></span>), (<span class="hljs-number"><span class="hljs-number">380225.31836121203</span></span>, <span class="hljs-number"><span class="hljs-number">5095400.767924464</span></span>)), crs=EPSG:<span class="hljs-number"><span class="hljs-number">32633</span></span>) timestamp: [datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>), ..., datetime.datetime(<span class="hljs-number"><span class="hljs-number">2017</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)], length=<span class="hljs-number"><span class="hljs-number">80</span></span> )</code> </pre> <br><p>  Der Zugriff auf die verschiedenen EOPatch-Attribute ist wie folgt: </p><br><pre> <code class="python hljs">eopatch.timestamp eopatch.mask[<span class="hljs-string"><span class="hljs-string">'LULC'</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'CLM'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] eopatch.data[<span class="hljs-string"><span class="hljs-string">'BANDS'</span></span>][<span class="hljs-number"><span class="hljs-number">5</span></span>][..., [<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]</code> </pre> <br><h3 id="eoexecute-order-66">  EOExecute Order 66 </h3><br><p>  Gro√üartig, die Daten werden geladen.  W√§hrend wir auf den Abschluss dieses Prozesses warten, werfen wir einen Blick auf die Funktionen einer Klasse, die in diesen Artikeln noch nicht behandelt wurde - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>EOExecutor</code></a> .  Dieses Modul befasst sich mit der Ausf√ºhrung und √úberwachung der Pipeline und erm√∂glicht die Verwendung von Multithreading ohne unn√∂tigen Aufwand.  Keine Suche mehr im Stapel√ºberlauf, wie die Pipeline korrekt parallelisiert werden kann oder wie der Fortschrittsbalken in diesem Modus funktioniert - wir haben bereits alles f√ºr Sie erledigt! </p><br><p>  Dar√ºber hinaus werden auftretende Fehler behandelt und eine kurze Zusammenfassung des Ausf√ºhrungsprozesses erstellt.  Letzteres ist der wichtigste Moment, um die Wiederholbarkeit Ihrer Ergebnisse in Zukunft sicherzustellen, damit der Benutzer keine kostbare Arbeitszeit damit verbringen muss, nach Parametern zu suchen, die er letzten Donnerstag um 9 Uhr nach einer ganzen Nacht voller Feierlichkeiten verwendet hat (mischen Sie keinen Alkohol und keine Programmierung) es lohnt sich!).  Diese Klasse kann auch ein sch√∂nes Abh√§ngigkeitsdiagramm f√ºr die Pipeline erstellen, das Sie Ihrem Chef zeigen k√∂nnen! </p><br><p><img src="https://habrastorage.org/webt/_o/x7/0q/_ox70q41_uiebqp7opyqbeu0nx0.png"><br>  <em>Von <code>eo-learn</code> generiertes Pipeline-Abh√§ngigkeitsdiagramm</em> </p><br><h3 id="eksperimenty-s-mashinnym-obucheniem">  Experimente zum maschinellen Lernen </h3><br><p>  Wie versprochen, soll dieser Artikel haupts√§chlich verschiedene Modelle mit <code>eo-learn</code> anhand der von uns bereitgestellten Daten untersuchen.  Im Folgenden haben wir zwei Experimente vorbereitet, in denen wir die Auswirkung von Wolken und verschiedenen Resampling-Algorithmen w√§hrend der zeitlichen Interpolation auf das Endergebnis untersuchen.  Nach all dem werden wir mit Faltungsnetzwerken (CNN) arbeiten und die Ergebnisse zweier Ans√§tze vergleichen - Pixel-f√ºr-Pixel-Analyse des Entscheidungsbaums und Deep Learning unter Verwendung von Faltungs-Neuronalen Netzen. </p><br><p>  Leider kann man keine eindeutige Antwort geben, welche Entscheidungen w√§hrend der Experimente getroffen werden sollten.  Sie k√∂nnen den Themenbereich genauer studieren und Annahmen treffen, um zu entscheiden, ob das Spiel die Kerze wert ist, aber letztendlich wird die Arbeit immer noch auf Versuch und Irrtum hinauslaufen. </p><br><h3 id="igraem-s-oblakami">  Spiel mit den Wolken </h3><br><p>  Wolken sind ein gro√üer Schmerz in der Welt von EO, insbesondere wenn es um Algorithmen f√ºr maschinelles Lernen geht, bei denen Sie sie bestimmen und aus dem Datensatz f√ºr die Interpolation basierend auf fehlenden Werten entfernen m√∂chten.  Aber wie gro√ü ist der Nutzen dieses Verfahrens?  Lohnt es sich?  Ru√üwurm und K√∂rner haben in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber die zeitliche Klassifizierung der Landbedeckung mit sequentiellen wiederkehrenden Encodern</a> sogar gezeigt, dass das Verfahren zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wolkenfilterung</a> f√ºr tiefes Lernen wahrscheinlich absolut unwichtig ist, da der Klassifikator selbst Wolken erkennen und ignorieren kann. </p><br><p><img src="https://habrastorage.org/webt/gz/c8/zs/gzc8zsp0nrdjtgbewqqysxulaiu.png"><br>  Aktivierung der Eingangsschicht (oben) und der Modulationsschicht (unten) in der Folge von Bildern eines bestimmten Fragments f√ºr ein neuronales Netzwerk.  M√∂glicherweise stellen Sie fest, dass dieses Netzwerkfragment gelernt hat, Cloud-Masken zu erstellen und die erzielten Ergebnisse zu filtern.  (Seite 9 unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.researchgate.net/publication/322975904_Multi-Temporal_Land_Cover_Classification_with_Sequential_Recurrent_Encoders</a> ) </p><br><p>  Wir erinnern uns kurz an die Struktur des Datenfilterungsschritts (Einzelheiten siehe [vorheriger Artikel] ()).  Nachdem wir Sentinel-2-Snapshots erstellt haben, filtern wir Cloud-Snapshots.  Alle Bilder, bei denen die Anzahl der nicht tr√ºben Pixel 80% nicht √ºberschreitet, werden √ºberpr√ºft (Schwellenwerte k√∂nnen f√ºr verschiedene interessierende Bereiche unterschiedlich sein).  Danach werden Wolkenmasken verwendet, um Pixelwerte an beliebigen Tagen zu erhalten, um solche Daten nicht zu ber√ºcksichtigen. </p><br><p>  Insgesamt sind vier Verhaltensweisen m√∂glich: </p><br><ol><li>  <strong>mit</strong> Bildfilter, <strong>gegebene</strong> Wolkenmasken </li><li>  <strong>Kein</strong> Schnappschussfilter <strong>bei</strong> Cloud-Masken </li><li>  <strong>mit</strong> Bildfilter ohne Wolkenmasken </li><li>  <strong>ohne</strong> Bildfilter, <strong>ohne</strong> Wolkenmasken </li></ol><br><p><img src="https://habrastorage.org/webt/rd/3i/ne/rd3ineypd8f0akhs41yve8mtgso.png"><br>  <em>Visuelle Anzeige des Bildstapels vom Sentinel-2-Satelliten.</em>  <em>Transparente Pixel auf der linken Seite bedeuten fehlende Pixel aufgrund der Wolkendecke.</em>  <em>Der mittlere Stapel zeigt die Pixelwerte, nachdem die Bilder gefiltert und mit einer Wolkenmaske interpoliert wurden (Fall 4), und der Stapel rechts zeigt das Interpolationsergebnis in dem Fall ohne Filterung der Bilder und ohne Wolkenmasken (1).</em>  <em>(Notizspur - anscheinend enth√§lt der Artikel einen Tippfehler, und es war das Gegenteil gemeint - Fall 1 in der Mitte und 4 rechts).</em> </p><br><p>  Im letzten Artikel haben wir bereits eine Variation von Fall 1 durchgef√ºhrt und die Ergebnisse gezeigt, sodass wir sie zum Vergleich verwenden werden.  Das Vorbereiten anderer F√∂rderer und das Trainieren des Modells klingt nach einer einfachen Aufgabe - Sie m√ºssen nur sicherstellen, dass wir die richtigen Werte vergleichen.  Verwenden Sie dazu einfach denselben Pixelsatz, um das Modell zu trainieren und zu validieren. </p><br><p>  Die Ergebnisse sind in der folgenden Tabelle aufgef√ºhrt.  Sie sehen, dass der Einfluss von Wolken auf das Ergebnis des Modells im Allgemeinen recht gering ist!  Dies kann daran liegen, dass die Referenzkarte von sehr guter Qualit√§t ist und das Modell die meisten Bilder ignorieren kann.  In jedem Fall kann dieses Verhalten f√ºr keinen AOI garantiert werden. Nehmen Sie sich also Zeit, um diesen Schritt aus Ihren Modellen herauszuholen! </p><br><div class="scrollable-table"><table><thead><tr><th>  Modell </th><th>  Genauigkeit [%] </th><th>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">F_1</a> [%] </th></tr></thead><tbody><tr><td>  Keine Filter, keine Maske </td><td>  92.8 </td><td>  92.6 </td></tr><tr><td>  Keine Filter mit Maske </td><td>  94.2 </td><td>  93.9 </td></tr><tr><td>  Mit Filter, ohne Maske </td><td>  94.0 </td><td>  93.8 </td></tr><tr><td>  Mit Filter, mit Maske </td><td>  94.4 </td><td>  94.1 </td></tr></tbody></table></div><br><h3 id="vliyanie-raznyh-podhodov-k-resemplingu">  Die Auswirkungen verschiedener Resampling-Ans√§tze </h3><br><p>  Die Wahl der zeitlichen Resampling-Optionen ist nicht offensichtlich.  Einerseits ben√∂tigen wir eine detaillierte Reihe von Bildern, die die Details der Quellbilder gut darstellen - wir m√∂chten den Quelldaten die n√§chstm√∂gliche Anzahl von Bildern hinzuf√ºgen.  Andererseits sind wir durch Rechenressourcen begrenzt.  Durch Reduzieren des Resampling-Schritts wird die Anzahl der Frames nach der Interpolation verdoppelt und somit die Anzahl der Attribute erh√∂ht, die im Training verwendet werden.  Ist eine solche Verbesserung die Kosten der Ressourcen wert?  Das m√ºssen wir herausfinden. </p><br><p>  F√ºr dieses Experiment verwenden wir Variation 1 aus dem vorherigen Schritt.  Nach der Interpolation werden die folgenden Variationen erneut abgetastet: </p><br><ol><li>  Gleichm√§√üiges Resampling im Abstand von 16 Tagen </li><li>  Einheitliches Resampling im Abstand von 8 Tagen </li><li>  Die Wahl der "besten" Daten, die Anzahl stimmt mit Fall 2 √ºberein. </li></ol><br><p>  Die Stichprobe in Fall 3 basiert auf der gr√∂√üten Anzahl gemeinsamer Daten f√ºr alle EOPatchs in der ausgew√§hlten AOI <br><img src="https://habrastorage.org/webt/xg/qa/9w/xgqa9w17-oe4dbtxca22yejwhzo.png"><br>  <em>Die Grafik zeigt die Anzahl der EOPatch-Fragmente, die Daten f√ºr jeden Tag des Jahres 2017 enthalten (blau).</em>  <em>Die roten Linien zeigen die optimalen Daten f√ºr das Resampling, die auf den Daten der Sentinel-2-Bilder f√ºr den angegebenen AOI 2017 basieren.</em> </p><br><p>  In der folgenden Tabelle k√∂nnen Sie sehen, dass die Ergebnisse nicht sehr beeindruckend sind, wie in fr√ºheren Erfahrungen.  In den F√§llen 2 und 3 verdoppelt sich der Zeitaufwand, aber der Unterschied zum urspr√ºnglichen Ansatz betr√§gt weniger als 1%.  Solche Verbesserungen sind f√ºr die praktische Anwendung zu unauff√§llig, sodass wir das f√ºr die Aufgabe geeignete 16-Tage-Intervall in Betracht ziehen k√∂nnen. </p><br><div class="scrollable-table"><table><thead><tr><th>  Modell </th><th>  Genauigkeit [%] </th><th>  F_1 [%] </th></tr></thead><tbody><tr><td>  Gleichm√§√üig alle 16 Tage </td><td>  94.4 </td><td>  94.1 </td></tr><tr><td>  Gleichm√§√üig alle 8 Tage </td><td>  94.5 </td><td>  94.3 </td></tr><tr><td>  Auswahl der besten Daten </td><td>  94.6 </td><td>  94.4 </td></tr></tbody></table></div><br><p>  <em>Ergebnisse der Gesamtgenauigkeit und der gewichteten F1 f√ºr verschiedene Pipelines mit einer √Ñnderung des Ansatzes zur erneuten Probenahme.</em> </p><br><h2 id="glubokoe-obuchenie-ispolzuem-svyortochnuyu-neyronnuyu-set-cnn">  Deep Learning: Verwenden des Convolutional Neural Network (CNN) </h2><br><p>  Deep Learning ist zum Standardansatz f√ºr viele Aufgaben geworden, wie z. B. Computer Vision, Textverarbeitung in nat√ºrlicher Sprache und Signalverarbeitung.  Dies liegt an ihrer F√§higkeit, Muster aus komplexen mehrdimensionalen Eingaben zu extrahieren.  Klassische Ans√§tze des maschinellen Lernens (wie Entscheidungsb√§ume) wurden in vielen zeitlichen Geodatenaufgaben verwendet.  Faltungsnetzwerke wurden andererseits verwendet, um die r√§umliche Korrelation zwischen benachbarten Bildern zu analysieren.  Grunds√§tzlich beschr√§nkte sich ihre Verwendung auf die Arbeit mit einzelnen Bildern. </p><br><p>  Wir wollten die Architektur von Deep-Learning-Modellen untersuchen und versuchen, eines auszuw√§hlen, das gleichzeitig r√§umliche und zeitliche Aspekte von Satellitendaten analysieren kann. </p><br><p>  Zu diesem Zweck verwendeten wir Temporal Fully-Convolutional Netvork, TFCN bzw. die in TensorFlow implementierte zeitliche Erweiterung auf U-Net.  Insbesondere verwendet die Architektur r√§umlich-zeitliche Korrelationen, um das Ergebnis zu verbessern.  Ein zus√§tzlicher Vorteil ist, dass Sie dank der Netzwerkstruktur dank des Kodierungs- / Dekodierungsprozesses in U-net r√§umliche Beziehungen in verschiedenen Ma√üst√§ben besser darstellen k√∂nnen.  Wie bei den klassischen Modellen erhalten wir am Ausgang eine zweidimensionale Matrix von Beschriftungen, die wir mit der Wahrheit vergleichen werden. </p><br><p><img src="https://habrastorage.org/webt/p0/jl/mg/p0jlmgxi9euwvodwonx4zrmezsw.png"></p><br><p>  Wir haben das trainierte Modell verwendet, um Markierungen auf dem Testsatz vorherzusagen, und die erhaltenen Werte wurden mit der Wahrheit √ºberpr√ºft.  Insgesamt betrug die Genauigkeit 84,4% und F1 85,4%. </p><br><p><img src="https://habrastorage.org/webt/ol/z2/zj/olz2zjp3waghaak9hnirzcwa258.png"></p><br><p>  <em>Vergleich verschiedener Vorhersagen f√ºr unsere Aufgabe.</em>  <em>Visuelles Bild (oben links), echte Referenzkarte (oben rechts), LightGBM-Modellvorhersage (unten links) und U-Net-Vorhersage (unten rechts)</em> </p><br><p>  Diese Ergebnisse zeigen nur die ersten Arbeiten an diesem Prototyp, der f√ºr die aktuelle Aufgabe nicht stark optimiert ist.  Trotzdem stimmen die Ergebnisse mit einigen in der Region erhaltenen Statistiken √ºberein.  Um das Potenzial eines neuronalen Netzwerks auszusch√∂pfen, m√ºssen die Architektur (Satz von Attributen, Netzwerktiefe, Anzahl der Faltungen) sowie Hyperparameter (Lerngeschwindigkeit, Anzahl der Epochen, Klassengewichtung) optimiert werden.  Wir gehen davon aus, dass wir uns noch eingehender mit diesem Thema befassen (ha ha) und planen, unseren Code zu verteilen, wenn er in akzeptabler Form vorliegt. </p><br><h3 id="drugie-eksperimenty">  Andere Experimente </h3><br><p>  Sie k√∂nnen <em>viele</em> M√∂glichkeiten finden, um Ihre aktuellen Ergebnisse zu verbessern, aber wir k√∂nnen sie nicht alle aussortieren oder ausprobieren.  In diesem Moment erscheinen Sie auf der Szene!  Zeigen Sie, was Sie mit diesem Datensatz tun k√∂nnen, und helfen Sie uns, die Ergebnisse zu verbessern! </p><br><p>  In naher Zukunft wird sich beispielsweise einer unserer Kollegen mit der Klassifizierung der Abdeckung anhand des zeitlichen Stapels <em>einzelner</em> Bilder unter Verwendung von Faltungsnetzwerken befassen.  Die Idee ist, dass einige Oberfl√§chen, zum Beispiel k√ºnstliche, ohne zeitliche Merkmale unterschieden werden k√∂nnen - ziemlich r√§umlich.  Wir werden gerne einen separaten Artikel schreiben, wenn diese Arbeit zu Ergebnissen f√ºhrt! </p><br><h3 id="ot-perevodchika">  Vom √úbersetzer </h3><br><p>  Leider ist der n√§chste Teil dieser Artikelserie nicht erschienen, was bedeutet, dass die Autoren beim Erstellen von U-Net keine Beispiele f√ºr Quellcode gezeigt haben.  Alternativ kann ich folgende Quellen anbieten: </p><br><ol><li>  <em>U-Net: Faltungsnetzwerke f√ºr die biomedizinische Bildsegmentierung - Olaf Ronneberger, Philipp Fischer und Thomas Brox</em> sind einer der grundlegenden Artikel zur U-Net-Architektur, die keine zeitlichen Daten enthalten. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://eo-learn.readthedocs.io/en/latest/examples/land-cover-map/SI_LULC_pipeline.html</a> - Die eo-learn-Dokumentationsseite, auf der sich (m√∂glicherweise) eine neuere Version von Pipelines mit 1,2 Teilen befindet. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/divamgupta/image-segmentation-keras</a> - Ein Repository mit mehreren Netzwerken, die mit Keras implementiert wurden.  Ich habe einige Fragen zu Implementierungen (sie unterscheiden sich geringf√ºgig von den in den Originalartikeln beschriebenen), aber im Allgemeinen lassen sich die L√∂sungen leicht f√ºr pers√∂nliche Zwecke anpassen und funktionieren recht gut. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de453354/">https://habr.com/ru/post/de453354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de453342/index.html">Mythen √ºber entfernte Mitarbeiter, die wir selbst zerst√∂rt haben</a></li>
<li><a href="../de453346/index.html">Speicher- und Datenschutztechnologien - der dritte Tag auf der VMware EMPOWER 2019</a></li>
<li><a href="../de453348/index.html">Was ist in Asyncio</a></li>
<li><a href="../de453350/index.html">Offene Sendung der Haupthalle von RIT ++ 2019</a></li>
<li><a href="../de453352/index.html">Wie Drohnen in Ghana lebenswichtige Medikamente liefern</a></li>
<li><a href="../de453356/index.html">Aktuelle Trends und Empfehlungen zur Agglomeration gro√üer Finanzinstitute</a></li>
<li><a href="../de453360/index.html">Stadt ohne Stau</a></li>
<li><a href="../de453362/index.html">HabraConf # 1 - R√ºckw√§rts f√ºr Backend</a></li>
<li><a href="../de453364/index.html">Eine Rollout-Geschichte, die alles betraf</a></li>
<li><a href="../de453366/index.html">Verwendung von Kommas in Englisch: 15 Regeln und Fehlerbeispiele</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>