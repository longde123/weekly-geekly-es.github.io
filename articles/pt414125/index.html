<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïï üë®üèø‚Äçüé§ üí´ Como testamos o VMware vSAN ‚Ñ¢: por que ele funciona na pr√°tica ü§õ üåõ üë©üèæ‚Äçüè´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="H√° um ano, montei um data center a partir de uma pilha do Intel NUC . Havia um sistema de armazenamento de software que a faxineira n√£o conseguia dest...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como testamos o VMware vSAN ‚Ñ¢: por que ele funciona na pr√°tica</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croc/blog/414125/">  H√° um ano, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">montei um data center a partir de uma pilha do Intel NUC</a> .  Havia um sistema de armazenamento de software que a faxineira n√£o conseguia destruir, algumas vezes o cluster entrou em colapso. <br><br>  E agora decidimos executar o vSAN em v√°rios servidores em uma configura√ß√£o muito boa para avaliar completamente o desempenho e a toler√¢ncia a falhas da solu√ß√£o.  Obviamente, tivemos v√°rias implementa√ß√µes bem-sucedidas em produ√ß√£o para nossos clientes, onde o vSAN resolve com √™xito as tarefas definidas, mas n√£o foi poss√≠vel realizar testes abrangentes.  Na verdade, quero compartilhar os resultados dos testes hoje. <br><br>  Vamos atormentar o armazenamento com uma carga, descart√°-la e, de todas as maneiras poss√≠veis, testar a toler√¢ncia a falhas.  Para mais detalhes, convido todos a gato. <br><a name="habracut"></a><br><h3>  O que √© o VMware vSAN em geral e por que entramos nele? </h3><br>  H√° um cluster de servidor regular para m√°quinas virtuais.  Possui v√°rios componentes independentes, o hipervisor √© executado diretamente no hardware e o armazenamento √© configurado separadamente com base no DAS, NAS ou SAN.  Dados lentos no HDD, dados quentes no SSD.  Tudo √© familiar.  Mas aqui surge o problema da implanta√ß√£o e administra√ß√£o deste zool√≥gico.  Torna-se especialmente divertido em situa√ß√µes em que elementos individuais do sistema de diferentes fornecedores.  Em caso de problemas, a ancoragem de tickets para suporte t√©cnico de diferentes fabricantes tem uma atmosfera especial. <br><br>  Existem peda√ßos de ferro separados que, do ponto de vista do servidor, parecem discos para grava√ß√£o. <br><br>  E existem sistemas hiperconvergentes.  Neles, voc√™ recebe uma unidade universal que absorve toda a dor de cabe√ßa da intera√ß√£o da rede, discos, processadores, mem√≥ria e as m√°quinas virtuais que rodam neles.  Todos os dados fluem para um painel de controle e, se necess√°rio, voc√™ pode simplesmente adicionar mais algumas unidades para compensar o aumento de carga.  A administra√ß√£o √© bastante simplificada e padronizada. <br><br><img src="https://habrastorage.org/webt/xi/4m/uw/xi4muweeppbjng1b8mrpc-bzi3m.png"><br>  O VMware vSAN refere-se apenas √†s solu√ß√µes com base nas quais √© implantado <br>  infraestrutura hiperconvergente.  O principal recurso do produto √© a forte integra√ß√£o com a plataforma de virtualiza√ß√£o VMware vSphere, l√≠der em solu√ß√µes de virtualiza√ß√£o que permite implantar o armazenamento de software para m√°quinas virtuais em servidores de virtualiza√ß√£o em minutos.  O vSAN assume diretamente o controle das opera√ß√µes de E / S em um n√≠vel baixo, distribuindo de maneira ideal a carga, armazenando em cache as opera√ß√µes de leitura / grava√ß√£o e fazendo muito mais com uma carga m√≠nima na mem√≥ria e no processador.  A transpar√™ncia do sistema √© um pouco reduzida, mas, como resultado, tudo funciona, como eles dizem, automagically vSAN pode ser configurado como um armazenamento h√≠brido e na forma de uma vers√£o totalmente em flash.  Ele √© dimensionado horizontalmente adicionando novos n√≥s ao cluster e verticalmente, aumentando o n√∫mero de discos em n√≥s individuais.  O gerenciamento com seu pr√≥prio web client vSphere √© muito conveniente devido √† forte integra√ß√£o com outros produtos. <br><br>  Optamos por uma configura√ß√£o limpa totalmente em flash, que deve ser ideal em termos de pre√ßo e desempenho.  √â claro que a capacidade total √© um pouco menor em compara√ß√£o com a configura√ß√£o h√≠brida usando discos magn√©ticos, mas aqui decidimos verificar como isso pode ser parcialmente contornado usando a codifica√ß√£o de apagamento, bem como a desduplica√ß√£o e compress√£o em tempo real.  Como resultado, a efici√™ncia do armazenamento fica mais pr√≥xima das solu√ß√µes h√≠bridas, mas significativamente mais r√°pida com o m√≠nimo de sobrecarga. <br><br><h3>  Como testar </h3><br>  Para testar o desempenho, usamos o software <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HCIBench v1.6.6</a> , que automatiza o processo de cria√ß√£o de muitas m√°quinas virtuais e compila√ß√£o dos resultados.  O pr√≥prio teste de desempenho √© realizado com o software Vdbench, um dos mais populares softwares de teste de carga sint√©tica.  O ferro estava nas seguintes op√ß√µes de configura√ß√£o: <br><br><ol><li>  Tudo em flash - 2 grupos de discos: 1xNVMe SSD Samsung PM1725 800 GB + 3xSATA </li><li>  SSD Toshiba HK4E de 1,6 TB. </li><li>  Totalmente flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 800 GB + SSD 6xSATA Toshiba HK4E 1,6 TB. </li><li>  Totalmente flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 de 800 GB + SSD 6xSATA Toshiba HK4E 1,6 TB + Efici√™ncia espacial (desduplica√ß√£o e compacta√ß√£o). </li><li>  Totalmente flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 de 800 GB + SSD 6xSATA Toshiba HK4E 1,6 TB + Codifica√ß√£o de apagamento (RAID 5/6). </li><li>  Totalmente flash - 1 grupo de discos: SSD 1xNVMe Samsung PM1725 de 800 GB + SSD 6xSATA Toshiba HK4E 1,6 TB + Codifica√ß√£o de apagamento (RAID 5/6) + Efici√™ncia de espa√ßo (desduplica√ß√£o e compacta√ß√£o). </li></ol><br>  Durante os testes, emulamos tr√™s volumes diferentes de dados ativos usados ‚Äã‚Äãpelos aplicativos: 1 TB (250 GB por servidor), 2 TB (500 GB por servidor) e 4 TB (1 TB cada). <br><br>  Para cada configura√ß√£o, o mesmo conjunto de testes foi realizado com os seguintes perfis de carga: <br><br><ol><li>  0 leitura / 100 grava√ß√£o, aleatoriamente 50%, tamanho do bloco - 4k. </li><li>  30 de leitura / 70 de grava√ß√£o, aleatoriamente 50%, tamanho do bloco - 4k. </li><li>  70 leitura / grava√ß√£o de 30, aleatoriamente 50%, tamanho do bloco - 4k. </li><li>  100 leitura / grava√ß√£o 0, aleatoriamente 50%, tamanho do bloco - 4k. </li></ol><br>  As primeira e quarta op√ß√µes foram necess√°rias para entender como o sistema se comportar√° sob cargas m√°ximas e m√≠nimas.  Mas o segundo e o terceiro est√£o o mais pr√≥ximo poss√≠vel do caso de uso t√≠pico real: por exemplo, 30 read / 70 write - VDI.  Essas cargas que encontrei na produ√ß√£o eram muito pr√≥ximas a elas.  No processo, testamos a efic√°cia do mecanismo de gerenciamento de dados vSAN. <br><br>  Em geral, o sistema provou ser muito bom.  Com base nos resultados do teste, percebemos que podemos contar com desempenho na regi√£o de 20 mil IOPS por n√≥.  Para tarefas comuns e muito carregadas, esses s√£o bons indicadores, considerando atrasos de 5 ms.  Abaixo dei gr√°ficos com os resultados: <br><br><img src="https://habrastorage.org/webt/2w/l5/q3/2wl5q38sg-f3f0t5a9smo2nz9m0.png"><br><img src="https://habrastorage.org/webt/fn/zm/bf/fnzmbff0apti4eqq8dk9o3emrym.png"><br><img src="https://habrastorage.org/webt/lc/_c/yv/lc_cyvqfd1hwe15iggyo8lxxagg.png"><br><img src="https://habrastorage.org/webt/i6/at/yu/i6atyu48yarwd5d31qgcstxx2so.png"><br><img src="https://habrastorage.org/webt/ih/y3/8o/ihy38oa_emlvkdokq9ryzt3hhum.png"><br><br>  Tabela de resumo com resultados do teste: <br><img src="https://habrastorage.org/webt/1v/xg/tm/1vxgtm0tpyiwqzujesrkzjcjlj8.jpeg"><br>  A cor verde indica dados ativos completamente armazenados em cache. <br><br><h3>  Toler√¢ncia a falhas </h3><br>  Cortei um n√≥ ap√≥s o outro, apesar da indigna√ß√£o da m√°quina, e observei a rea√ß√£o do sistema como um todo.  Ap√≥s desconectar o primeiro n√≥, nada aconteceu, exceto por uma pequena queda no desempenho, em cerca de 10 a 15%.  Coloquei o segundo n√≥ - algumas das m√°quinas virtuais foram encerradas, mas o restante continuou trabalhando com uma leve degrada√ß√£o no desempenho.  Sobreviv√™ncia geral satisfeito.  Reiniciei todos os n√≥s - o sistema pensou um pouco e sincronizou novamente sem problemas, todas as m√°quinas virtuais foram iniciadas sem problemas.  Como ao mesmo tempo em NUCs.  A integridade dos dados tamb√©m n√£o √© afetada, o que √© muito satisfeito. <br><br><h3>  Impress√µes gerais </h3><br>  Sistemas de armazenamento definido por software (SDS) j√° √© uma tecnologia madura. <br><br>  Hoje, um dos principais fatores de parada que impedem a implementa√ß√£o do vSAN √© o alto custo de uma licen√ßa em rublos.  Se voc√™ criar a infraestrutura do zero, pode acontecer que um sistema de armazenamento tradicional em uma configura√ß√£o semelhante tenha o mesmo custo.  Mas ser√° menos flex√≠vel, tanto em termos de administra√ß√£o quanto de escala.  Portanto, hoje, ao escolher uma solu√ß√£o para armazenar dados de m√°quinas virtuais na plataforma de virtualiza√ß√£o vSphere, √© muito bom avaliar todos os pr√≥s e contras do uso de solu√ß√µes tradicionais e da implementa√ß√£o da tecnologia de armazenamento definido por software. <br><br>  Voc√™ pode criar uma solu√ß√£o no mesmo Ceph ou GlusterFS, mas ao trabalhar com a infraestrutura VMware, a integra√ß√£o do vSAN com componentes individuais √© cativante, bem como a facilidade de administra√ß√£o, implanta√ß√£o e desempenho significativamente maior, especialmente em um pequeno n√∫mero de n√≥s.  Portanto, se voc√™ j√° estiver trabalhando na infraestrutura do VMware, ser√° muito mais f√°cil implantar.  Voc√™ realmente faz uma d√∫zia de cliques e obt√©m um SDS pronto para uso. <br><br>  Outra motiva√ß√£o para implantar o vSAN √© us√°-lo para ramifica√ß√µes, o que permite espelhar n√≥s em unidades remotas com um host testemunha no datacenter.  Essa configura√ß√£o fornece armazenamento tolerante a falhas para m√°quinas virtuais com todas as tecnologias vSAN e desempenho em apenas dois n√≥s.  A prop√≥sito, para usar o vSAN, existe um esquema de licenciamento separado para o n√∫mero de m√°quinas virtuais, o que permite reduzir custos em compara√ß√£o com o esquema tradicional de licenciamento do vSAN para processadores. <br><br>  Arquitetonicamente, a solu√ß√£o requer Ethernet de 10 Gb com dois links por n√≥ para uma distribui√ß√£o de tr√°fego adequada ao usar uma solu√ß√£o totalmente em flash.  Comparado aos sistemas tradicionais, voc√™ economiza espa√ßo em rack e em redes SAN, eliminando o Fibre Channel em favor do padr√£o Ethernet mais universal.  Para garantir a toler√¢ncia a falhas, s√£o necess√°rios pelo menos tr√™s n√≥s, em duas r√©plicas de objetos com dados e, no terceiro - objetos testemunha para esses dados, resolve o problema do c√©rebro dividido. <br><br>  Agora, algumas perguntas para voc√™: <br><br><ol><li>  Quando voc√™ decide um sistema de armazenamento, quais crit√©rios s√£o mais importantes para voc√™? </li><li>  Que fatores de parada voc√™ v√™ no caminho para implementar sistemas de armazenamento definidos por software? </li><li>  Quais sistemas de armazenamento definidos por software voc√™ considera basicamente como uma op√ß√£o para implementa√ß√£o? </li></ol><br><br>  UPD: Esqueci completamente de escrever os par√¢metros de configura√ß√£o e carregamento do suporte: <br><br>  1. Descri√ß√£o do ferro.  Por exemplo: <br>  Servidores - 4xDellR630, cada um: <br>  ‚Ä¢ 2xE5-2680v4 <br>  ‚Ä¢ 128GB RAM <br>  ‚Ä¢ 2x10GbE <br>  ‚Ä¢ 2x1GbE para gerenciamento / rede VM <br>  ‚Ä¢ Dell HBA330 <br>  Configura√ß√£o de armazenamento n¬∫ 1: <br>  2xPM1725 800GB <br>  6xToshiba HK4E 1.6TB <br>  Configura√ß√£o de armazenamento n¬∫ 2: <br>  1xPM1725 800GB <br>  6xToshiba HK4E 1.6TB <br><br>  2. Descri√ß√£o das vers√µes do software: <br>  vSphere 6.5U1 (7967591) (vSAN 6.6.1), ou seja,  manchas ap√≥s Meltdown / Spectre <br>  vCenter 6.5U1g <br>  Drivers e FW mais recentes suportados pelo vSAN e ESXi para todos os componentes <br>  LACP para tr√°fego vSAN e vMotion (com compartilhamentos / limites / reservas NIOC ativados) <br>  Todas as outras configura√ß√µes s√£o padr√£o <br><br>  3. Par√¢metros de carga: <br>  ‚Ä¢ HCIBench 1.6.6 <br>  ‚Ä¢ Oracle Vdbench - 04/05/06 <br>  ‚Ä¢ 40VM por cluster (10 por n√≥) <br>  ‚Ä¢ 10 vmdk por VM <br>  ‚Ä¢ Tamanho de 10 GB de vmdk e porcentagem do conjunto de carga de trabalho de 100/50/25% <br>  ‚Ä¢ Tempo de aquecimento-1800seg (0,5 horas), tempo de teste 3600 (1 hora) <br>  ‚Ä¢ 1 threads por vmdk </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt414125/">https://habr.com/ru/post/pt414125/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt414115/index.html">O que √© o Lazy FP State Restore: uma nova vulnerabilidade descoberta nos processadores Intel</a></li>
<li><a href="../pt414117/index.html">Formato bin√°rio decimal misto vs IEEE754</a></li>
<li><a href="../pt414119/index.html">A oportunidade adormeceu devido a uma tempestade de areia em Marte. N√£o est√° claro se o rover poder√° trabalhar novamente</a></li>
<li><a href="../pt414121/index.html">Drone aut√¥nomo DIY com controle da Internet</a></li>
<li><a href="../pt414123/index.html">Atualizamos os protocolos de texto para bin√°rios e combatemos o c√≥digo legado em uma reuni√£o do Grupo de Usu√°rios C ++</a></li>
<li><a href="../pt414127/index.html">Instale o 3CX em uma hospedagem por 2,99 euros / m√™s. em 10 minutos</a></li>
<li><a href="../pt414129/index.html">Mestre em Ci√™ncia da Computa√ß√£o Te√≥rica na Universidade Estadual de S√£o Petersburgo</a></li>
<li><a href="../pt414131/index.html">O efeito da frequ√™ncia do sinal na energia dos links de r√°dio no espa√ßo livre</a></li>
<li><a href="../pt414133/index.html">Design de jogos de quebra-cabe√ßa com o exemplo de In The Shadows</a></li>
<li><a href="../pt414135/index.html">Como a portabilidade de um jogo para o PSVita melhorou o desempenho geral</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>