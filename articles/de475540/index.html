<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèø ü§¥üèº ü•° Achten Sie auf Schwachstellen, die zu Problemumgehungen f√ºhren. Teil 1: FragmentSmack / SegmentSmack ü•ö üßñüèΩ üë©‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Mein Name ist Dmitry Samsonov, ich arbeite als f√ºhrender Systemadministrator bei Odnoklassniki. Wir haben mehr als 7.000 physische S...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Achten Sie auf Schwachstellen, die zu Problemumgehungen f√ºhren. Teil 1: FragmentSmack / SegmentSmack</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/475540/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/qy/tg/kcqytg4znoagvdqffwtwxhwzcjs.jpeg"></div><br><br>  Hallo allerseits!  Mein Name ist Dmitry Samsonov, ich arbeite als f√ºhrender Systemadministrator bei Odnoklassniki.  Wir haben mehr als 7.000 physische Server, 11.000 Container in unserer Cloud und 200 Anwendungen, die in unterschiedlichen Konfigurationen 700 verschiedene Cluster bilden.  Auf den meisten Servern wird CentOS 7 ausgef√ºhrt. <br>  Informationen zur Sicherheitsanf√§lligkeit in FragmentSmack Ver√∂ffentlicht am 14. August 2018 <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CVE-2018-5391</a> ) und SegmentSmack ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CVE-2018-5390</a> ).  Hierbei handelt es sich um Schwachstellen mit einem Netzwerkangriffsvektor und einer relativ hohen Bewertung (7,5), die mit einem Denial-of-Service (DoS) aufgrund von Ressourcenersch√∂pfung (CPU) bedroht sind.  Ein Fix im Kernel f√ºr FragmentSmack wurde zu diesem Zeitpunkt noch nicht vorgeschlagen, er erschien jedoch viel sp√§ter als die Ver√∂ffentlichung von Informationen √ºber die Sicherheitsanf√§lligkeit.  Um SegmentSmack zu eliminieren, wurde vorgeschlagen, den Kernel zu aktualisieren.  Das Update-Paket selbst wurde am selben Tag ver√∂ffentlicht, es musste nur noch installiert werden. <br>  Nein, wir sind √ºberhaupt nicht gegen das Kernel-Update!  Es gibt jedoch Nuancen ... <br><a name="habracut"></a><br><h4>  Wie aktualisieren wir den Kern des Produkts? </h4><br>  Im Allgemeinen nichts kompliziertes: <br><ol><li>  Pakete herunterladen </li><li>  Installieren Sie sie auf einer Reihe von Servern (einschlie√ülich Servern, die unsere Cloud hosten). </li><li>  Stellen Sie sicher, dass nichts kaputt ist. </li><li>  Stellen Sie sicher, dass alle Standard-Kernel-Einstellungen fehlerfrei sind. </li><li>  Warte ein paar Tage; </li><li>  √úberpr√ºfen Sie die Serverleistung. </li><li>  Stellen Sie die Bereitstellung neuer Server auf einen neuen Kernel um. </li><li>  Aktualisieren Sie alle Server von Rechenzentren (jeweils ein Rechenzentrum, um die Auswirkungen auf Benutzer bei Problemen zu minimieren). </li><li>  Starten Sie alle Server neu. </li></ol><br>  Wiederholen Sie dies f√ºr alle Zweige der Kerne, die wir haben.  Im Moment ist dies: <br><br><ul><li>  Stock CentOS 7 3.10 - f√ºr die meisten normalen Server; </li><li>  Vanilla 4.19 ist f√ºr unsere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">One-Cloud,</a> da wir BFQ, BBR usw. Ben√∂tigen. </li><li>  Elrepo Kernel-ml 5.2 ist f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hochlastverteiler</a> gedacht, da 4.19 sich fr√ºher instabil verh√§lt und die gleichen Funktionen ben√∂tigt. </li></ul><br>  Wie Sie vielleicht erraten haben, dauert das Neustarten von Tausenden von Servern am l√§ngsten.  Da nicht alle Schwachstellen f√ºr alle Server kritisch sind, starten wir nur diejenigen neu, auf die direkt √ºber das Internet zugegriffen werden kann.  Um die Flexibilit√§t in der Cloud nicht einzuschr√§nken, binden wir extern zug√§ngliche Container nicht an einzelne Server mit einem neuen Core, sondern starten alle Hosts ausnahmslos neu.  Gl√ºcklicherweise ist die Prozedur dort einfacher als bei normalen Servern.  Beispielsweise k√∂nnen zustandslose Container beim Neustart einfach auf einen anderen Server verschoben werden. <br><br>  Trotzdem gibt es noch viel Arbeit, und es kann mehrere Wochen dauern, und im Falle von Problemen mit der neuen Version - bis zu mehreren Monaten.  Angreifer sind sich dessen bewusst, daher wird Plan B ben√∂tigt. <br><br><h4>  FragmentSmack / SegmentSmack.  Umgehung </h4><br>  Gl√ºcklicherweise gibt es f√ºr einige Sicherheitsl√ºcken einen solchen Plan "B", der als Workaround bezeichnet wird.  In den meisten F√§llen handelt es sich hierbei um eine √Ñnderung der Kernel- / Anwendungseinstellungen, die den m√∂glichen Effekt minimieren oder die Ausnutzung von Sicherheitsanf√§lligkeiten vollst√§ndig ausschlie√üen kann. <br><br>  F√ºr FragmentSmack / SegmentSmack <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde die</a> folgende Problemumgehung vorgeschlagen: <br><br><blockquote>  ‚Äû <i>Sie k√∂nnen die Standardwerte von 4 MB und 3 MB in net.ipv4.ipfrag_high_thresh und net.ipv4.ipfrag_low_thresh (und ihre Analoga f√ºr ipv6-net.ipv6.ipfrag_high_thresh und net.ipv6.ipfrag_low_thresh) um 256 kB bzw. 192 kB √§ndern.</i>  <i>Tests zeigen einen leichten bis signifikanten R√ºckgang der CPU-Auslastung w√§hrend eines Angriffs, abh√§ngig von Ausr√ºstung, Einstellungen und Bedingungen.</i>  <i>Aufgrund von ipfrag_high_thresh = 262144 Bytes kann es jedoch zu Leistungseinbu√üen kommen, da jeweils nur zwei 64-KB-Fragmente in die Neuerstellungswarteschlange passen.</i>  <i>Beispielsweise besteht das Risiko, dass Anwendungen, die mit gro√üen UDP-Paketen arbeiten, nicht mehr funktionieren</i> . ‚Äú </blockquote><br>  Die Parameter selbst <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Kerneldokumentation werden</a> wie folgt beschrieben: <br><br><blockquote><code>ipfrag_high_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments.</code> </blockquote> <br><p></p><blockquote> <code>ipfrag_low_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments before the kernel <br> begins to remove incomplete fragment queues to free up resources. <br> The kernel still accepts new fragments for defragmentation.</code> </blockquote> <br>  Wir haben kein gro√ües UDP f√ºr Produktionsdienste.  Es gibt keinen fragmentierten Datenverkehr im LAN, aber keinen signifikanten Datenverkehr im WAN.  Nichts ist verheerend - Sie k√∂nnen Workaround rollen! <br><br><h4>  FragmentSmack / SegmentSmack.  Erstes Blut </h4><br>  Das erste Problem, auf das wir stie√üen, war, dass Cloud-Container die neuen Einstellungen manchmal nur teilweise anwendeten (nur ipfrag_low_thresh) und manchmal √ºberhaupt nicht verwendeten - sie st√ºrzten einfach beim Start ab.  Das Problem konnte nicht stabil reproduziert werden (manuell wurden alle Einstellungen problemlos √ºbernommen).  Es ist auch nicht so einfach zu verstehen, warum der Container zu Beginn herunterf√§llt: Es wurden keine Fehler gefunden.  Eines war sicher: Das Zur√ºcksetzen der Einstellungen l√∂st das Problem des Ablegens von Containern. <br><br>  Warum reicht es nicht aus, Sysctl auf dem Host zu verwenden?  Der Container befindet sich in seinem dedizierten Netzwerk-Namespace. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Daher</a> kann sich zumindest ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil der Netzwerk-Sysctl-Parameter</a> im Container vom Host unterscheiden. <br><br>  Wie genau gelten die Sysctl-Einstellungen im Container?  Da es sich um nichtprivilegierte Container handelt, schl√§gt das √Ñndern von Sysctl-Einstellungen durch Wechseln in den Container selbst fehl - es gibt einfach nicht gen√ºgend Rechte.  Zu dieser Zeit verwendete unsere Cloud Docker (jetzt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Podman</a> ), um Container zu starten.  Der Docker √ºbermittelte √ºber die API die Parameter des neuen Containers, einschlie√ülich der erforderlichen Sysctl-Einstellungen. <br>  Bei der Aufz√§hlung der Versionen stellte sich heraus, dass die Docker-API nicht alle Fehler verursachte (zumindest in Version 1.10).  Als wir versuchten, den Container durch "Docker Run" zu starten, sahen wir endlich etwas: <br><br> <code>write /proc/sys/net/ipv4/ipfrag_high_thresh: invalid argument docker: Error response from daemon: Cannot start container &lt;...&gt;: [9] System error: could not synchronise with container process.</code> <br> <br>  Der Wert des Parameters ist ung√ºltig.  Aber warum?  Und warum ist es nur manchmal nicht g√ºltig?  Es stellte sich heraus, dass Docker nicht die Reihenfolge garantierte, in der Sysctl-Parameter verwendet wurden (die letzte getestete Version war 1.13.1). Daher versuchte ipfrag_high_thresh manchmal, sich auf 256 KB zu setzen, wenn ipfrag_low_thresh immer noch 3 MB betrug, d. H. Die Obergrenze war niedriger als die Untergrenze, was zu einem Fehler f√ºhrte. <br><br>  Zu diesem Zeitpunkt haben wir bereits unseren eigenen Mechanismus zum Neukonfigurieren des Containers nach dem Start verwendet (Einfrieren des Containers durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cgroup freezer</a> und Ausf√ºhren von Befehlen im Containernamensraum √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ip netns</a> ) und diesem Teil Sysctl-Parameter hinzugef√ºgt.  Das Problem wurde behoben. <br><br><h4>  FragmentSmack / SegmentSmack.  Erstes Blut 2 </h4><br>  Bevor wir Workaround in der Cloud einsetzen konnten, tauchten die ersten seltenen Beschwerden von Benutzern auf.  Zu diesem Zeitpunkt vergingen mehrere Wochen seit dem Start der Problemumgehung auf den ersten Servern.  Die erste Untersuchung ergab, dass Beschwerden √ºber einzelne Dienste und nicht alle Server dieser Dienste eingingen.  Das Problem hat einen √§u√üerst vagen Charakter wiedererlangt. <br><br>  Zun√§chst haben wir nat√ºrlich versucht, die Sysctl-Einstellungen zur√ºckzusetzen, dies hat jedoch keine Auswirkungen.  Verschiedene Manipulationen an den Server- und Anwendungseinstellungen haben ebenfalls nicht geholfen.  Ein Neustart hat geholfen.  Ein Neustart unter Linux ist genauso unnat√ºrlich wie fr√ºher unter Windows.  Trotzdem hat es geholfen und wir haben alles auf einen ‚ÄûFehler im Kernel‚Äú abgeschrieben, als wir die neuen Einstellungen in Sysctl angewendet haben.  Wie leichtfertig es war ... <br><br>  Drei Wochen sp√§ter trat das Problem erneut auf.  Die Konfiguration dieser Server war recht einfach: Nginx im Proxy / Balancer-Modus.  Der Verkehr ist ein bisschen.  Neue Einf√ºhrung: Die Anzahl der 504 Fehler ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gateway Timeout</a> ) auf Clients steigt t√§glich.  Die Grafik zeigt die Anzahl von 504 Fehlern pro Tag f√ºr diesen Dienst: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xk/hk/rj/xkhkrjedsakcdrx6m_z8hgcgjsw.png"></div><br><br>  Alle Fehler betreffen dasselbe Backend - dasjenige, das sich in der Cloud befindet.  Das Diagramm des Speicherverbrauchs f√ºr Paketfragmente in diesem Backend sieht wie folgt aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/fb/gt/jyfbgtoqrbqwv6cird2zawvatza.png"></div><br><br>  Dies ist eine der auff√§lligsten Erscheinungsformen des Problems in der Grafik des Betriebssystems.  Gleichzeitig wurde in der Cloud ein weiteres Netzwerkproblem mit den QoS-Einstellungen (Traffic Control) behoben.  In der Grafik zum Speicherverbrauch f√ºr Paketfragmente sah es genauso aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/09/nf/4r/09nf4rxogxv9xppeumwujvmcr3m.png"></div><br><br>  Die Annahme war einfach: Wenn sie in den Diagrammen gleich aussehen, dann haben sie den gleichen Grund.  Dar√ºber hinaus sind Probleme mit dieser Art von Speicher √§u√üerst selten. <br><br>  Die Essenz des behobenen Problems bestand darin, dass wir den fq-Paket-Sheduler mit Standardeinstellungen in QoS verwendeten.  Standardm√§√üig k√∂nnen Sie f√ºr eine Verbindung 100 Pakete zur Warteschlange hinzuf√ºgen, und einige Verbindungen in einer Situation mit Kanalengp√§ssen begannen, die Warteschlange bis zum Ausfall zu verstopfen.  In diesem Fall werden die Pakete verworfen.  In der tc-Statistik (tc -s qdisc) sieht dies folgenderma√üen aus: <br><br> <code>qdisc fq 2c6c: parent 1:2c6c limit 10000p flow_limit 100p buckets 1024 orphan_mask 1023 quantum 3028 initial_quantum 15140 refill_delay 40.0ms <br> Sent 454701676345 bytes 491683359 pkt (dropped 464545, overlimits 0 requeues 0) <br> backlog 0b 0p requeues 0 <br> 1024 flows (1021 inactive, 0 throttled) <br> 0 gc, 0 highprio, 0 throttled, 464545 flows_plimit</code> <br> <br>  "464545 flows_plimit" - Dies sind die Pakete, die aufgrund des √úberschreitens der Warteschlange einer Verbindung verworfen wurden, und "verworfene 464545" ist die Summe aller verworfenen Pakete dieses Zeitplans.  Nachdem die L√§nge der Warteschlange auf 1000 erh√∂ht und die Container neu gestartet wurden, trat das Problem nicht mehr auf.  Sie k√∂nnen sich in einem Stuhl zur√ºcklehnen und einen Smoothie trinken. <br><br><h4>  FragmentSmack / SegmentSmack.  Letztes Blut </h4><br>  Ein paar Monate nach der Ank√ºndigung von Sicherheitsl√ºcken im Kernel erschien schlie√ülich ein Fix f√ºr FragmentSmack (ich erinnere mich, dass mit der Ank√ºndigung im August ein Fix nur f√ºr SegmentSmack ver√∂ffentlicht wurde), der uns die M√∂glichkeit gab, Workaround abzubrechen, was uns viele Probleme bereitete.  Einige der Server konnten wir in dieser Zeit bereits auf einen neuen Kernel √ºbertragen, und nun mussten wir von vorne beginnen.  Warum haben wir den Kernel aktualisiert, ohne auf das FragmentSmack-Update zu warten?  Tatsache ist, dass der Prozess des Schutzes vor diesen Schwachstellen mit dem Prozess des Aktualisierens von CentOS selbst zusammenfiel (und zusammengef√ºhrt wurde) (was noch mehr Zeit in Anspruch nimmt als nur das Aktualisieren des Kernels).  Dar√ºber hinaus ist SegmentSmack eine gef√§hrlichere Sicherheitsanf√§lligkeit, die sofort behoben wurde.  Wir konnten den Kernel unter CentOS jedoch nicht einfach aktualisieren, da die FragmentSmack-Sicherheitsanf√§lligkeit, die w√§hrend CentOS 7.5 auftrat, nur in Version 7.6 behoben wurde. Daher mussten wir das Upgrade auf 7.5 beenden und mit dem Upgrade auf 7.6 von vorne beginnen.  Und so ist es auch. <br><br>  Zweitens sind seltene Anwenderbeschwerden √ºber Probleme an uns zur√ºckgekehrt.  Jetzt wissen wir bereits, dass alle mit dem Herunterladen von Dateien von Clients auf einige unserer Server verbunden sind.  Und durch diese Server gab es eine sehr kleine Anzahl von Uploads aus der Gesamtmasse. <br><br>  Wie wir uns aus der obigen Geschichte erinnern, hat das Rollback von Sysctl nicht geholfen.  Neustart geholfen, aber vor√ºbergehend. <br>  Der Verdacht auf Sysctl wurde nicht ausger√§umt, diesmal war es jedoch erforderlich, so viele Informationen wie m√∂glich zu sammeln.  Au√üerdem fehlte die M√∂glichkeit, das Problem beim Hochladen des Clients zu reproduzieren, um genauer untersuchen zu k√∂nnen, was gerade geschah. <br><br>  Die Analyse aller verf√ºgbaren Statistiken und Protokolle brachte uns dem Verst√§ndnis des Geschehens nicht n√§her.  Es bestand ein akuter Mangel an der F√§higkeit, das Problem zu reproduzieren, um eine bestimmte Verbindung zu "f√ºhlen".  Schlie√ülich gelang es den Entwicklern der speziellen Version der Anwendung, eine stabile Wiedergabe der Probleme auf dem Testger√§t zu erzielen, wenn eine Verbindung √ºber WLAN hergestellt wurde.  Dies war ein Durchbruch in der Untersuchung.  Der Client, der mit Nginx verbunden war, war ein Proxy f√ºr das Backend, das unsere Java-Anwendung war. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/g8/4a/xwg84aziibg7aaglsv6hdc2vyn8.png"></div><br><br>  Der Dialog mit Problemen war wie folgt (auf der Nginx-Proxy-Seite behoben): <br><br><ol><li>  Client: Anforderung von Informationen zum Herunterladen einer Datei. </li><li>  Java Server: Antwort. </li><li>  Client: POST mit Datei. </li><li>  Java-Server: Fehler. </li></ol><br>  Gleichzeitig schreibt der Java-Server in das Protokoll, dass 0 Byte Daten vom Client empfangen wurden, und der Nginx-Proxy, dass die Anforderung l√§nger als 30 Sekunden gedauert hat (30 Sekunden ist das Timeout der Clientanwendung).  Warum Timeout und warum 0 Bytes?  Aus der Sicht von HTTP funktioniert alles wie es sollte, aber der POST mit der Datei scheint aus dem Netzwerk zu verschwinden.  Und es verschwindet zwischen dem Client und Nginx.  Es ist Zeit, sich mit Tcpdump zu r√ºsten!  Aber zuerst m√ºssen Sie die Netzwerkkonfiguration verstehen.  Der Nginx-Proxy befindet sich hinter dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">N3ware</a> L3-Balancer.  Tunneling wird verwendet, um Pakete vom L3-Balancer an den Server zu √ºbermitteln, der seine Header zu den Paketen hinzuf√ºgt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hs/ge/mf/hsgemfebrlkpjzvtz-cdrghudxu.png"></div><br><br>  Gleichzeitig erreicht das Netzwerk diesen Server in Form von Vlan-markiertem Datenverkehr, der auch seine Felder zu Paketen hinzuf√ºgt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-a/ns/ap/-ansap0a5oyjsnqrf0onvsxg0tw.png"></div><br><br>  Dieser Datenverkehr kann fragmentiert sein (der sehr kleine Prozentsatz des eingehenden fragmentierten Datenverkehrs, √ºber den wir bei der Bewertung der Risiken aus der Problemumgehung gesprochen haben), wodurch sich auch der Inhalt der Header √§ndert: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gd/yx/48/gdyx48_bc15aj2xmhn5kb0_omci.png"></div><br><br>  Noch einmal: Pakete werden von einem Vlan-Tag gekapselt, von einem Tunnel gekapselt, fragmentiert.  Um dies besser zu verstehen, verfolgen wir die Paketroute vom Client zum Nginx-Proxy. <br><br><ol><li>  Das Paket gelangt zum L3-Balancer.  Zur korrekten Weiterleitung innerhalb des Rechenzentrums wird das Paket im Tunnel gekapselt und an die Netzwerkkarte gesendet. </li><li>  Da die Paket- + Tunnel-Header nicht in die MTU passen, wird das Paket in Fragmente geschnitten und an das Netzwerk gesendet. </li><li>  Der Switch nach dem L3-Balancer f√ºgt beim Empfang des Pakets ein Vlan-Tag hinzu und sendet es weiter. </li><li>  Der Switch vor dem Nginx-Proxy erkennt (gem√§√ü den Porteinstellungen), dass der Server ein Vlan-gekapseltes Paket erwartet, und sendet es so, wie es ist, ohne das Vlan-Tag zu entfernen. </li><li>  Linux empf√§ngt Fragmente einzelner Pakete und f√ºgt sie zu einem gro√üen Paket zusammen. </li><li>  Dann gelangt das Paket zur Vlan-Schnittstelle, wo die erste Schicht entfernt wird - die Vlan-Kapselung. </li><li>  Linux sendet es dann an die Tunnel-Schnittstelle, wo eine weitere Schicht entfernt wird - die Tunnel-Kapselung. </li></ol><br>  Die Schwierigkeit besteht darin, all dies als Parameter an tcpdump zu √ºbergeben. <br>  Fangen wir am Ende an: Gibt es saubere (ohne zus√§tzliche Header) IP-Pakete von Clients, bei denen die VLAN- und Tunnelkapselung entfernt wurde? <br><br> <code>tcpdump host &lt;ip &gt;</code> <br> <br>  Nein, auf dem Server befanden sich keine derartigen Pakete.  Daher sollte das Problem fr√ºher sein.  Gibt es Pakete, bei denen nur die Vlan-Kapselung entfernt wurde? <br><br> <code>tcpdump ip[32:4]=0xx390x2xx</code> <br> <br>  0xx390x2xx ist die Client-IP-Adresse im Hex-Format. <br>  32: 4 - Adresse und L√§nge des Feldes, in dem die SCR-IP in das Tunnelpaket geschrieben wird. <br><br>  Die Adresse des Feldes musste mit brachialer Gewalt ausgew√§hlt werden, da das Internet ungef√§hr 40, 44, 50, 54 schreibt, aber es gab keine IP-Adresse.  Sie k√∂nnen auch eines der Pakete in hex (der Parameter -xx oder -XX in tcpdump) anzeigen und berechnen, unter welcher Adresse die IP bekannt ist. <br><br>  Gibt es Paketfragmente, bei denen die Vlan- und Tunnel-Kapselung nicht entfernt wurde? <br><br> <code>tcpdump ((ip[6:2] &gt; 0) and (not ip[6] = 64)) <br></code> <br>  Diese Magie wird uns alle Fragmente zeigen, einschlie√ülich der letzten.  Wahrscheinlich kann dasselbe nach IP gefiltert werden, aber ich habe es nicht versucht, da es nicht sehr viele solcher Pakete gibt und die, die ich brauchte, leicht im allgemeinen Stream gefunden wurden.  Hier sind sie: <br><br> <code>14:02:58.471063 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 1516: (tos 0x0, ttl 63, <b>id 53652, offset 0</b> , flags [+], proto IPIP (4), length 1500) <br> 11.11.11.11 &gt; 22.22.22.22: truncated-ip - 20 bytes missing! (tos 0x0, ttl 50, id 57750, offset 0, flags [DF], proto TCP (6), length 1500) <br> 33.33.33.33.33333 &gt; 44.44.44.44.80: Flags [.], seq 0:1448, ack 1, win 343, options [nop,nop,TS val 11660691 ecr 2998165860], length 1448 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 05dc d194 2000 3f09 d5fb 0a66 387d E.......?....f8} <br> 0x0020: 1x67 7899 4500 06xx e198 4000 3206 6xx4 .faEE.....@.2.m. <br> 0x0030: b291 x9xx x345 2541 83b9 0050 9740 0x04 .......A...P.@.. <br> 0x0040: 6444 4939 8010 0257 8c3c 0000 0101 080x dDI9...W.\...... <br> 0x0050: 00b1 ed93 b2b4 6964 xxd8 ffe1 006a 4578 ......ad.....j <b>Ex</b> <br> 0x0060: 6966 0000 4x4d 002a 0500 0008 0004 0100 <b>if</b> ..MM.*........ <br> <br> 14:02:58.471103 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 62: (tos 0x0, ttl 63, <b>id 53652, offset 1480</b> , flags [none], proto IPIP (4), length 40) <br> 11.11.11.11 &gt; 22.22.22.22: ip-proto-4 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 0028 d194 00b9 3f04 faf6 2x76 385x E..(....?....f8} <br> 0x0020: 1x76 6545 xxxx 1x11 2d2c 0c21 8016 8e43 .faE...D-,.!...C <br> 0x0030: x978 e91d x9b0 d608 0000 0000 0000 7c31 .x............|Q <br> 0x0040: 881d c4b6 0000 0000 0000 0000 0000 ..............</code> <br> <br>  Dies sind zwei Fragmente einer Packung (die gleiche ID 53652) mit einem Foto (das Wort Exif ist in der ersten Packung sichtbar).  Aufgrund der Tatsache, dass es Pakete auf dieser Ebene gibt, die jedoch nicht in M√ºllhalden eingeklebt sind, liegt das Problem eindeutig bei der Montage.  Endlich gibt es Belege daf√ºr! <br><br>  Der Paketdecoder hat keine Probleme aufgedeckt, die den Zusammenbau verhinderten.  Versucht hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hpd.gasmi.net</a> .  Beim Versuch, dort etwas zu stopfen, gef√§llt dem Decoder zun√§chst das Paketformat nicht.  Es stellte sich heraus, dass es zwischen Srcmac und Ethertype zwei zus√§tzliche Oktette gab (nicht im Zusammenhang mit Fragmentinformationen).  Nach dem Entfernen funktionierte der Decoder.  Er zeigte jedoch keine Probleme. <br>  Sagen Sie, was Sie m√∂gen, au√üer diesen Sysctl wurde nichts mehr gefunden.  Es blieb eine M√∂glichkeit zu finden, problematische Server zu identifizieren, um den Umfang zu verstehen und weitere Ma√ünahmen zu beschlie√üen.  Schnell fand ich den richtigen Z√§hler: <br><br> <code>netstat -s | grep "packet reassembles failed‚Äù</code> <br> <br>  Es ist in snmpd unter OID = 1.3.6.1.2.1.4.31.1.16.1 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ipSystemStatsReasmFails</a> ). <br><br><blockquote>  <i>"Die Anzahl der Fehler, die vom IP-Wiederherstellungsalgorithmus erkannt wurden (aus welchen Gr√ºnden auch immer: Zeit√ºberschreitung, Fehler usw.)."</i> </blockquote><br>  In der Gruppe der Server, auf denen das Problem untersucht wurde, stieg dieser Z√§hler bei zwei Servern schneller, bei zwei Servern langsamer und bei zwei Servern √ºberhaupt nicht an.  Ein Vergleich der Dynamik dieses Z√§hlers mit der Dynamik von HTTP-Fehlern auf dem Java-Server ergab eine Korrelation.  Das hei√üt, der Z√§hler k√∂nnte zur √úberwachung gesetzt werden. <br><br>  Ein zuverl√§ssiger Indikator f√ºr Probleme ist sehr wichtig, damit Sie genau feststellen k√∂nnen, ob Sysctl-Rollback hilft, da wir aus der vorherigen Geschichte wissen, dass dies aus der Anwendung nicht sofort ersichtlich ist.  Dieser Indikator w√ºrde es erm√∂glichen, alle Problembereiche in der Produktion zu identifizieren, bevor Benutzer es entdecken. <br>  Nach dem Sysctl-Rollback wurden die √úberwachungsfehler gestoppt, sodass die Ursache der Probleme sowie die Tatsache, dass das Rollback hilft, nachgewiesen wurden. <br><br>  Wir haben die Fragmentierungseinstellungen auf anderen Servern zur√ºckgesetzt, auf denen eine neue √úberwachung in Brand geraten ist, und haben den Fragmenten standardm√§√üig noch mehr Speicher zugewiesen als zuvor (dies war udp-statistics, dessen teilweiser Verlust vor dem allgemeinen Hintergrund nicht erkennbar war). <br><br><h4>  Die wichtigsten Fragen </h4><br>  Warum fragmentieren Pakete auf unserem L3-Balancer?  Die meisten Pakete, die von Benutzern an die Balancer gesendet werden, sind SYN und ACK.  Die Gr√∂√üen dieser Taschen sind klein.  Da der Anteil solcher Pakete jedoch sehr gro√ü ist, haben wir vor diesem Hintergrund nicht bemerkt, dass gro√üe Pakete zu fragmentieren begannen. <br><br>  Der Grund war das defekte advmss-Konfigurationsskript auf Servern mit Vlan-Schnittstellen (es gab zu diesem Zeitpunkt nur sehr wenige Server mit markiertem Datenverkehr in der Produktion).  Mit Advmss k√∂nnen Sie dem Kunden mitteilen, dass die Pakete in unserer Richtung kleiner sein sollten, damit sie nach dem Aufkleben der Tunnelk√∂pfe nicht fragmentiert werden m√ºssen. <br><br>  Warum hat Sysctl-Rollback nicht geholfen, aber Hilfe beim Neustart?  Durch Sysctl-Rollback wurde der f√ºr das Verkleben von Paketen verf√ºgbare Speicherplatz ge√§ndert.  Gleichzeitig f√ºhrte anscheinend die Tatsache des Speicher√ºberlaufs f√ºr Fragmente zu einer Hemmung der Verbindungen, was dazu f√ºhrte, dass die Fragmente f√ºr lange Zeit in der Warteschlange verz√∂gert wurden.  Das hei√üt, der Prozess ist eine Schleife. <br>  Rebut machte die Erinnerung ung√ºltig und alles war in Ordnung. <br><br>  K√∂nnten Sie ohne Workaround auskommen?  Ja, aber es besteht ein hohes Risiko, dass Benutzer im Falle eines Angriffs unbeaufsichtigt bleiben.  Nat√ºrlich f√ºhrte die Verwendung von Workaround zu verschiedenen Problemen, einschlie√ülich der Sperrung eines der Dienste durch die Benutzer, aber wir sind dennoch der Ansicht, dass die Ma√ünahmen gerechtfertigt waren. <br><br>  Vielen Dank an Andrei Timofeev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">atimofeyev</a> ) f√ºr die Unterst√ºtzung bei der Untersuchung und an Alexei Krenev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">devicex</a> ) f√ºr die gro√üartige Arbeit, Centos und Kernel auf den Servern zu aktualisieren.  Der Prozess, der in diesem Fall von Anfang an mehrmals gestartet werden musste, zog sich deshalb √ºber viele Monate hin. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475540/">https://habr.com/ru/post/de475540/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475518/index.html">Das kanadische Unternehmen hat Material entwickelt, das Sie unsichtbar macht</a></li>
<li><a href="../de475520/index.html">CSS-√úbergang der Eigenschaft height von 0px zu auto</a></li>
<li><a href="../de475522/index.html">HP: Ihr urspr√ºngliches Laufwerk ist √ºberhaupt nicht original. Wer ist schuld und was zu tun?</a></li>
<li><a href="../de475536/index.html">Lebenslauf f√ºr einen freiberuflichen √úbersetzer</a></li>
<li><a href="../de475538/index.html">Wo beginnt die Schaffung eines Marktplatzes? Teil eins</a></li>
<li><a href="../de475544/index.html">Kataloge von Produkten, Dienstleistungen und mehr</a></li>
<li><a href="../de475546/index.html">Suchtsyndrome IT</a></li>
<li><a href="../de475548/index.html">Langweiliges Matchmaking ohne Ungleichgewicht und Warteschlangen: ein praktischer Leitfaden</a></li>
<li><a href="../de475550/index.html">Akustiksysteme f√ºr offene R√§ume</a></li>
<li><a href="../de475552/index.html">Blitz-Testing-Algorithmen f√ºr maschinelles Lernen: Geben Sie Ihren Datensatz in die Scikit-Learn-Bibliothek ein</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>