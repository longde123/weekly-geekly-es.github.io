<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèáüèº üîÑ üë©üèø Probleme bei der Big Data-Analyse üêµ ü•û ü§¥üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Was sind die Herausforderungen der Big Data-Analyse? 
 Big Data erstellt Funktionen, die von herk√∂mmlichen Datens√§tzen nicht gemeinsam genutzt werden....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Probleme bei der Big Data-Analyse</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/456088/"><h3>  Was sind die Herausforderungen der Big Data-Analyse? </h3><br>  Big Data erstellt Funktionen, die von herk√∂mmlichen Datens√§tzen nicht gemeinsam genutzt werden.  Diese Merkmale verursachen erhebliche Probleme bei der Datenanalyse und motivieren die Entwicklung neuer statistischer Methoden.  Im Gegensatz zu herk√∂mmlichen Datens√§tzen, bei denen die Stichprobengr√∂√üe normalerweise gr√∂√üer als die Messung ist, zeichnet sich Big Data durch eine gro√üe Stichprobengr√∂√üe und eine hohe Dimension aus.  Zun√§chst werden wir die Auswirkung gro√üer Stichprobengr√∂√üen auf das Verst√§ndnis der Heterogenit√§t diskutieren: Einerseits erm√∂glichen gro√üe Stichprobengr√∂√üen die Aufdeckung verborgener Muster, die mit kleinen Untergruppen der Bev√∂lkerung und einer schlechten Allgemeinheit der gesamten Bev√∂lkerung verbunden sind.  Andererseits erfordert die Modellierung der internen Heterogenit√§t von Big Data komplexere statistische Methoden.  Zweitens werden wir einige einzigartige Ph√§nomene diskutieren, die mit einer hohen Dimensionalit√§t verbunden sind, einschlie√ülich Rauschakkumulation, falscher Korrelation und zuf√§lliger Endogenit√§t.  Diese einzigartigen Merkmale machen herk√∂mmliche statistische Verfahren ung√ºltig. <br><a name="habracut"></a><br><h3>  Heterogenit√§t </h3><br>  Big Data wird h√§ufig durch Kombinieren mehrerer Datenquellen erstellt, die verschiedenen Untergruppen entsprechen.  Jede Untergruppe kann einige einzigartige Merkmale aufweisen, die von anderen nicht gemeinsam genutzt werden.  Unter klassischen Bedingungen, wenn die Stichprobengr√∂√üe klein oder m√§√üig ist, werden Datenpunkte aus kleinen Subpopulationen normalerweise als ‚ÄûAbweichungen‚Äú klassifiziert, und die Modellierung ist aufgrund der unzureichenden Anzahl von Beobachtungen systematisch schwierig.  Im Zeitalter von Big Data k√∂nnen wir aufgrund der gro√üen Stichprobengr√∂√üe die Heterogenit√§t besser verstehen, indem wir die Forschung beleuchten, beispielsweise die Beziehung zwischen bestimmten Kovariaten (z. B. Genen oder SNPs) und seltenen Ergebnissen (z. B. seltenen Krankheiten oder Krankheiten in kleinen Populationen) untersuchen und verstehen warum bestimmte Behandlungen (wie Chemotherapie) einer Bev√∂lkerung zugute kommen und einer anderen schaden.  Um diesen Punkt besser zu veranschaulichen, f√ºhren wir das folgende Modell f√ºr die Bev√∂lkerung ein: <br><br><p><math> </math> $$ Anzeige $$ Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), Œª1p1 (y; Œ∏1 (x)) + ‚ãØ + Œªmpm (y; Œ∏m (x)), ( 1) $$ Anzeige $$ </p><br>  Wenn Œªj ‚â• 0 den Bruchteil der j-ten Untergruppe darstellt, ist pj (y; Œ∏j (x)) die Wahrscheinlichkeitsverteilung der Antwort der j-ten Untergruppe bei gegebenen Kovariaten von x mit Œ∏j (x) als Parametervektor.  In der Praxis werden viele Subpopulationen selten beobachtet, dh Œªj ist sehr klein.  Wenn die Stichprobengr√∂√üe n moderat ist, kann nŒªj klein sein, was es aufgrund fehlender Informationen unm√∂glich macht, kovariatenabh√§ngige Parameter Œ∏j (x) abzuleiten.  Da Big Data jedoch eine gro√üe Stichprobengr√∂√üe n hat, kann die Stichprobengr√∂√üe nŒªj f√ºr die j-te Bev√∂lkerungsgruppe m√§√üig gro√ü sein, selbst wenn Œªj sehr klein ist.  Dies erm√∂glicht es uns, eine genauere Schlussfolgerung √ºber die Parameter der Subpopulation Œ∏j (¬∑) zu ziehen.  Kurz gesagt, der Hauptvorteil von Big Data ist das Verst√§ndnis der Heterogenit√§t von Subpopulationen, beispielsweise der Vorteile bestimmter personalisierter Behandlungen, die mit einer kleinen oder moderaten Stichprobengr√∂√üe nicht m√∂glich sind. <br><br>  Mit Big Data k√∂nnen wir aufgrund der gro√üen Stichprobengr√∂√üe auch eine schwache Community in der gesamten Bev√∂lkerung identifizieren.  Beispielsweise kann es ohne eine gro√üe Stichprobe schwierig sein, den Herznutzen eines Glases Rotwein pro Tag zu beurteilen.  Ebenso k√∂nnen Gesundheitsrisiken, die mit der Exposition gegen√ºber bestimmten Umweltfaktoren verbunden sind, nur dann √ºberzeugender bewertet werden, wenn die Stichprobengr√∂√üen gro√ü genug sind. <br><br>  Zus√§tzlich zu den oben genannten Vorteilen stellt die Heterogenit√§t von Big Data auch die statistische Inferenz vor erhebliche Herausforderungen.  Die Ableitung des Mischungsmodells in (1) f√ºr gro√üe Datenmengen erfordert komplexe statistische und rechnerische Methoden.  Bei kleinen Messungen k√∂nnen Standardmethoden wie der Wartemaximierungsalgorithmus f√ºr die endg√ºltigen Mischungsmodelle verwendet werden.  In gro√üem Ma√üstab m√ºssen wir jedoch das Bewertungsverfahren sorgf√§ltig rationalisieren, um eine √úberanpassung oder Akkumulation von Rauschen zu vermeiden und gute Berechnungsalgorithmen zu entwickeln. <br><br><h3>  Ger√§uschansammlung </h3><br>  Bei der Big Data-Analyse m√ºssen wir viele Parameter gleichzeitig bewerten und verifizieren.  Sch√§tzfehler h√§ufen sich, wenn die Entscheidungs- oder Vorhersageregel von einer gro√üen Anzahl solcher Parameter abh√§ngt.  Dieser Effekt der Rauschakkumulation ist in gro√üen Dimensionen besonders schwerwiegend und kann sogar echte Signale dominieren.  Dies wird normalerweise unter der Annahme von Sp√§rlichkeit behandelt. <br><br>  Nehmen Sie zum Beispiel eine mehrdimensionale Klassifizierung.  Eine schlechte Klassifizierung ist auf das Vorhandensein vieler Schwachstellen zur√ºckzuf√ºhren, die nicht zur Reduzierung von Klassifizierungsfehlern beitragen.  Betrachten Sie als Beispiel das Klassifizierungsproblem, wenn Daten aus zwei Klassen stammen: <br><br><p><math> </math> $$ Anzeige $$ X1 und Y1, ........ Xn‚àºNd (Œº1, Id), Yn‚àºNd (Œº2, Id) .X1, ..., Xn‚àºNd (Œº1, Id) und Y1, ..., Yn‚àº Nd (Œº2, Id).  (2) $$ Anzeige $$ </p><br>  Wir wollen eine Klassifizierungsregel erstellen, die eine neue Beobachtung Z‚ààRdZ‚ààRd entweder in der ersten oder in der zweiten Klasse klassifiziert.  Um den Effekt der Rauschakkumulation in der Klassifizierung zu veranschaulichen, setzen wir n = 100 und d = 1000. Wir setzen Œº1 = 0Œº1 = 0 und Œº2 als sp√§rlich, d.h.  Nur die ersten 10 Datens√§tze von Œº2 sind ungleich Null mit einem Wert von 3, und alle anderen Datens√§tze sind Null.  Abbildung 1 zeigt die ersten beiden Hauptkomponenten unter Verwendung der ersten m = 2, 40, 200 Elemente und bis zu 1000 Elemente.  Wie in diesen Diagrammen gezeigt, erhalten wir bei m = 2 einen hohen Grad an Diskriminierung.  Die Unterscheidungsleistung wird jedoch sehr gering, wenn m aufgrund der Rauschakkumulation zu gro√ü ist.  Die ersten 10 Funktionen tragen zur Klassifizierung bei, der Rest nicht.  Wenn also m&gt; 10 ist, empfangen die Prozeduren keine zus√§tzlichen Signale, sondern akkumulieren Rauschen: Je mehr m, desto mehr Rauschen akkumuliert, was das Klassifizierungsverfahren aufgrund der Dimensionalit√§t verschlechtert.  Bei m = 40 kompensieren die akkumulierten Signale das akkumulierte Rauschen, so dass die ersten beiden Hauptkomponenten noch eine gute Erkennungsf√§higkeit aufweisen.  Wenn m = 200 ist, √ºberschreitet das akkumulierte Rauschen die Signalverst√§rkung. <br><br>  Die obige Diskussion motiviert die Verwendung sp√§rlicher Modelle und die Auswahl von Variablen, um den Effekt der Rauschakkumulation zu √ºberwinden.  Zum Beispiel k√∂nnten wir im Klassifizierungsmodell (2) anstelle aller Funktionen eine Teilmenge der Merkmale ausw√§hlen, die das beste Signal-Rausch-Verh√§ltnis erzielen.  Ein solches sp√§rliches Modell bietet eine h√∂here Klassifizierungseffizienz.  Mit anderen Worten, die Wahl der Variablen spielt eine Schl√ºsselrolle bei der √úberwindung der Rauschakkumulation bei der Klassifizierung und Vorhersage der Regression.  Die Auswahl von Variablen in gro√üen Dimensionen ist jedoch aufgrund falscher Korrelation, zuf√§lliger Endogenit√§t, Heterogenit√§t und Messfehlern schwierig. <br><br><h3>  Falsche Korrelation </h3><br>  Eine hohe Dimensionalit√§t enth√§lt auch eine falsche Korrelation, da viele nicht korrelierte Zufallsvariablen in gro√üen Dimensionen hohe Stichprobenkorrelationen aufweisen k√∂nnen.  Eine falsche Korrelation kann zu fehlerhaften wissenschaftlichen Entdeckungen und falschen statistischen Schlussfolgerungen f√ºhren. <br><br>  Betrachten Sie das Problem der Sch√§tzung des Koeffizientenvektors Œ≤ eines linearen Modells <br><br><p><math> </math> $$ Anzeige $$ y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, y = XŒ≤ + œµ, Var (œµ) = œÉ2Id, (3) $$ Anzeige $$ </p><br>  wobei y‚ààRny‚ààRn den Antwortvektor darstellt, X = [x1, ..., xn] T‚ààRn √ó dX = [x1, ..., xn] T‚ààRn √ó d die Projektionsmatrix darstellt ,, ‚ààRnœµ‚ààRn den unabh√§ngigen Zufallsvektor darstellt Rauschen und Id ist die d √ó d-Identit√§tsmatrix.  Um das Problem der Rauschakkumulation zu bew√§ltigen, wird angenommen, dass die Antwort nur eine kleine Anzahl von Variablen ergibt, wenn die Gr√∂√üe d mit der Stichprobengr√∂√üe n vergleichbar oder gr√∂√üer als diese ist, dh Œ≤ ist ein sp√§rlicher Vektor.  In √úbereinstimmung mit dieser Sp√§rlichkeitsannahme kann eine Variable ausgew√§hlt werden, um eine Rauschakkumulation zu vermeiden, die Vorhersageleistung zu verbessern und die Interpretierbarkeit eines Modells mit einer konservativen Darstellung zu verbessern. <br><br>  Bei gro√üen Gr√∂√üen ist die Auswahl von Variablen selbst f√ºr ein so einfaches Modell wie (3) aufgrund des Vorhandenseins einer falschen Korrelation schwierig.  Insbesondere bei hoher Dimensionalit√§t k√∂nnen wichtige Variablen stark mit mehreren falschen Variablen korreliert werden, die nicht wissenschaftlich verwandt sind.  Betrachten Sie ein einfaches Beispiel, das dieses Ph√§nomen veranschaulicht.  Sei x1, ..., xn unabh√§ngige Beobachtungen eines d-dimensionalen Gau√üschen Zufallsvektors X = (X1, ..., Xd) T‚àºNd (0, Id) X = (X1, ..., Xd) T‚àºNd (0, Id) ‚Å† .  Wir simulieren Daten wiederholt mit n = 60 und d = 800 und 6400 1000 Mal.  Fig. 2a zeigt die empirische Verteilung des maximalen absoluten Probenkorrelationskoeffizienten zwischen der ersten Variablen und dem Rest, definiert als <br><br><p><math> </math> $$ Anzeige $$ rÀÜ = maxj‚â•2 | CorrÀÜ (X1, Xj) |, r ^ = maxj‚â•2 | Corr ^ (X1, Xj) |, (4) $$ Anzeige $$ </p><br>  wobei Corr ^ (X1, Xj) Corr ^ (X1, Xj) die Stichprobenkorrelation zwischen den Variablen X1 und Xj ist.  Wir sehen, dass die maximale absolute Korrelation der Probe mit zunehmender Dimension h√∂her wird. <br><br>  Zus√§tzlich k√∂nnen wir die maximale absolute Mehrfachkorrelation zwischen X1 und linearen Kombinationen mehrerer irrelevanter Seitenvariablen berechnen: <br><br><p><math> </math> $$ display $$ RÀÜ = max | S | = 4max {Œ≤j} 4j = 1‚à£‚à£‚à£‚à£CorrÀÜ (X1, ‚àëj‚àëSŒ≤jXj) ‚à£‚à£‚à£‚à£.R ^ = max | S | = 4max {Œ≤j} j = 14 | Corr ^ (X1, ‚àëj‚àëSŒ≤jXj) |.  (5) $$ Anzeige $$ </p><br>  Unter Verwendung der Standardkonfiguration wird die empirische Verteilung des maximalen absoluten Koeffizienten der Probenkorrelation zwischen X1 und ‚àëj ‚àà SŒ≤jXj angegeben, wobei S eine beliebige Teilmenge der vierten Gr√∂√üe von {2, ..., d} ist und Œ≤j der Regressionskoeffizient Xj der kleinsten Quadrate ist, wenn X1 auf {Xj} zur√ºckgeht j ‚àà S. Wiederum sehen wir, dass, obwohl X1 v√∂llig unabh√§ngig von X2, ..., Xd ist, die Korrelation zwischen X1 und der n√§chsten linearen Kombination von vier Variablen von {Xj} j ‚â† 1 bis X1 sehr hoch sein kann. <br><br>  Eine falsche Korrelation hat einen erheblichen Einfluss auf die Auswahl der Variablen und kann zu fehlerhaften wissenschaftlichen Entdeckungen f√ºhren.  Sei XS = (Xj) j ‚àà S ein durch S indizierter Zufallsvektor, und sei SÀÜS ^ die ausgew√§hlte Menge, die eine h√∂here parasit√§re Korrelation mit X1 aufweist, wie in Fig. 1 dargestellt.  2. Wenn zum Beispiel n = 60 und d = 6400 ist, sehen wir, dass X1 f√ºr die Menge SS ^  | praktisch nicht von XSXS ^ zu unterscheiden ist  SÀÜ |  = 4 |  S ^ |  = 4‚Å†.  Wenn X1 das Expressionsniveau des f√ºr die Krankheit verantwortlichen Gens darstellt, k√∂nnen wir es nicht von den anderen vier Genen in SS ^ unterscheiden, die eine √§hnliche Prognosekraft haben, obwohl sie aus wissenschaftlicher Sicht keine Rolle spielen. <br><br>  Neben der Auswahl der Variablen kann eine falsche Korrelation auch zu einer falschen statistischen Schlussfolgerung f√ºhren.  Wir erkl√§ren dies, indem wir erneut dasselbe lineare Modell wie in (3) betrachten.  Hier m√∂chten wir den Standardfehler œÉ des Restes sch√§tzen, der sich sp√ºrbar in den statistischen Schlussfolgerungen der Regressionskoeffizienten, der Modellauswahl, des Korrespondenztests und der marginalen Regression manifestiert.  Sei SÀÜS ^ die Menge ausgew√§hlter Variablen und PSÀÜPS ^ die Projektionsmatrix auf den Spaltenraum XSÀÜXS ^ ‚Å†.  Standardsch√§tzung der Restvarianz basierend auf ausgew√§hlten Variablen: <br><br><p><math> </math> $$ display $$ œÉÀÜ2 = yT (In - PSÀÜ) yn - | SÀÜ | .œÉ ^ 2 = yT (In - PS ^) yn - | S ^ |.  (6) $$ Anzeige $$ </p><br>  Der Bewerter (6) ist unparteiisch, wenn die Variablen nicht aus den Daten ausgew√§hlt werden und das Modell korrekt ist.  Die Situation ist jedoch v√∂llig anders, wenn Variablen basierend auf Daten ausgew√§hlt werden.  Insbesondere haben die Autoren gezeigt, dass œÉ2 bei vielen falschen Variablen stark untersch√§tzt wird, was zu fehlerhaften statistischen Schlussfolgerungen f√ºhrt, einschlie√ülich der Auswahl von Modellen oder Signifikanztests, und zu fehlerhaften wissenschaftlichen Entdeckungen wie der Suche nach den falschen Genen f√ºr molekulare Mechanismen.  Sie bieten auch eine erweiterte Kreuzvalidierungsmethode, um das Problem zu l√∂sen. <br><br><h3>  Zuf√§llige Endogenit√§t </h3><br>  Zuf√§llige Endogenit√§t ist ein weiteres subtiles Problem, das sich aus der hohen Dimensionalit√§t ergibt.  In der Regressionseinstellung Y = ‚àëdj = 1Œ≤jXj + ŒµY = ‚àëj = 1dŒ≤jXj + Œµ‚Å† bedeutet der Begriff ‚ÄûEndogenit√§t‚Äú, dass einige Pr√§diktoren {Xj} mit dem Restrauschen Œµ korrelieren.  Das √ºbliche sp√§rliche Modell geht davon aus <br><br><p><math> </math> $$ Anzeige $$ Y = ‚àëjŒ≤jXj + Œµ und E (ŒµXj) = 0 f√ºr j = 1, ..., d, Y = ‚àëjŒ≤jXj + Œµ und E (ŒµXj) = 0 f√ºr j = 1, ..., d , (7) $$ Anzeige $$ </p><br>  mit einer kleinen Menge S = {j: Œ≤j ‚â† 0}.  Die exogene Annahme (7), dass das Restrauschen Œµ nicht mit allen Pr√§diktoren korreliert, ist entscheidend f√ºr die Zuverl√§ssigkeit der meisten vorhandenen statistischen Methoden, einschlie√ülich der Konsistenz bei der Auswahl der Variablen.  Obwohl diese Annahme unschuldig erscheint, ist es leicht, sie in gro√üen Dimensionen zu verletzen, da einige Variablen {Xj} zuf√§llig mit Œµ korrelieren, was die meisten mehrdimensionalen Verfahren statistisch ung√ºltig macht. <br><br>  Um das Endogenit√§tsproblem genauer zu erkl√§ren, nehmen wir an, dass die unbekannte Antwort Y wie folgt mit den drei Kovariaten assoziiert ist: <br><br><p><math> </math> $$ Anzeige $$ Y = X1 + X2 + X3 + Œµ mit EŒµXj = 0 f√ºr j = 1, 2, 3.Y = X1 + X2 + X3 + Œµ mit EŒµXj = 0 f√ºr j = 1, 2, 3 $$ Anzeige $$ </p><br>  In der Phase der Datenerfassung kennen wir das wahre Modell nicht und erfassen daher so viele Kovariaten, wie m√∂glicherweise mit Y assoziiert sind, in der Hoffnung, alle Begriffe in S in (7) aufzunehmen.  √úbrigens k√∂nnen einige dieser Xj (f√ºr jj 1, 2, 3) mit Restrauschen &amp; egr; assoziiert sein.  Dies widerlegt die Annahme einer exogenen Modellierung in (7).  Je mehr Kovariaten gesammelt oder gemessen werden, desto komplexer ist diese Annahme. <br><br>  Im Gegensatz zur falschen Korrelation bezieht sich die zuf√§llige Endogenit√§t auf die tats√§chliche Existenz von Korrelationen zwischen unbeabsichtigten Variablen.  Das erste √§hnelt der Tatsache, dass zwei Menschen einander √§hnlich sind, aber keine genetische Verbindung haben, und das zweite ist wie eine Bekanntschaft, die leicht in einer Gro√üstadt stattfindet.  Im Allgemeinen ergibt sich die Endogenit√§t aus einer Verzerrung der Wahl, Messfehlern und fehlenden Variablen.  Diese Ph√§nomene treten h√§ufig bei der Analyse von Big Data auf, haupts√§chlich aus zwei Gr√ºnden: <br><br><ul><li>  Dank neuer Hochleistungsmessmethoden k√∂nnen Wissenschaftler m√∂glichst viele Funktionen sammeln und danach streben.  Dies erh√∂ht dementsprechend die Wahrscheinlichkeit, dass einige von ihnen mit Restrauschen korrelieren. </li><li>  Big Data wird normalerweise aus mehreren Quellen mit m√∂glicherweise unterschiedlichen Datengenerierungsschemata kombiniert.  Dies erh√∂ht die Wahrscheinlichkeit einer Verzerrung der Auswahl- und Messfehler, die ebenfalls eine potenzielle zuf√§llige Endogenit√§t verursachen. </li></ul><br>  Erscheint zuf√§llige Endogenit√§t in realen Datens√§tzen und wie k√∂nnen wir dies in der Praxis testen?  Wir erw√§gen eine Genomstudie, in der 148 Microarray-Proben aus den Datenbanken GEO und ArrayExpress heruntergeladen werden.  Diese Proben wurden auf der Affymetrix HGU133a-Plattform f√ºr Menschen mit Prostatakrebs erstellt.  Der erhaltene Datensatz enth√§lt 22.283 Sonden, was 12.719 Genen entspricht.  In diesem Beispiel interessieren wir uns f√ºr ein Gen namens "Discoidin Domain Rezeptor Family Member 1" (abgek√ºrzt DDR1).  DDR1 codiert Rezeptortyrosinkinasen, die eine wichtige Rolle bei der Verbindung von Zellen mit ihrer Mikroumgebung spielen.  Es ist bekannt, dass DDR1 eng mit Prostatakrebs verwandt ist, und wir m√∂chten seine Beziehung zu anderen Genen bei Krebspatienten untersuchen.  Wir nahmen die DDR1-Genexpression als Antwortvariable Y und die Expression aller verbleibenden 12.718 Gene als Pr√§diktoren.  Im linken Bereich Abb.  Abbildung 3 zeigt die empirische Verteilung der Korrelationen zwischen der Antwort und einzelnen Pr√§diktoren. <br><br>  Um die Existenz von Endogenit√§t zu veranschaulichen, passen wir die L1-Regression der kleinsten Quadrate (Lasso) an die Daten an, und die Strafe wird automatisch unter Verwendung einer 10-fachen Kreuzvalidierung ausgew√§hlt (37 Gene ausgew√§hlt).  Dann werden wir die √ºbliche Regression der kleinsten Quadrate f√ºr das ausgew√§hlte Modell wiederherstellen, um den Restvektor zu berechnen.  Im rechten Bereich Abb.  3 konstruieren wir eine empirische Verteilung der Korrelationen zwischen Pr√§diktoren und Residuen.  Wir sehen, dass das Restrauschen stark mit vielen Pr√§diktoren korreliert.  Um sicherzustellen, dass diese Korrelationen nicht durch eine rein falsche Korrelation verursacht werden, f√ºhren wir eine ‚ÄûNullverteilung‚Äú falscher Korrelationen ein, indem wir die Zeilenreihenfolgen in der Projektmatrix zuf√§llig neu anordnen, sodass die Pr√§diktoren wirklich unabh√§ngig vom Restrauschen sind.  Beim Vergleich dieser beiden Verteilungen sehen wir, dass die Verteilung der Korrelationen zwischen Pr√§diktoren und Restrauschen in den Rohdaten (als ‚ÄûRohdaten‚Äú gekennzeichnet) einen schwereren Schwanz aufweist als in den neu angeordneten Daten (als ‚Äûneu angeordnete Daten‚Äú gekennzeichnet).  Dieses Ergebnis liefert starke Hinweise auf Endogenit√§t. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456088/">https://habr.com/ru/post/de456088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456076/index.html">PyDaCon-Treffen bei Mail.ru Group: 22. Juni</a></li>
<li><a href="../de456078/index.html">Projektion von Unternehmenskonflikten auf die Netzwerkkonnektivit√§t</a></li>
<li><a href="../de456082/index.html">Wie wir personalisierte Produktempfehlungen entwickeln</a></li>
<li><a href="../de456084/index.html">Kubernetes 1.15: Highlights √úbersicht</a></li>
<li><a href="../de456086/index.html">iOS Storyboards: Analyse der Vor- und Nachteile, Best Practices</a></li>
<li><a href="../de456090/index.html">Einf√ºhrung in Unit Testing in Unity</a></li>
<li><a href="../de456092/index.html">Sieben beunruhigende Anzeichen daf√ºr, dass Sie wetterabh√§ngig sind, auch wenn Sie dies nicht glauben</a></li>
<li><a href="../de456094/index.html">Wir lesen Datenbl√§tter 2: SPI auf STM32; PWM-Timer und Interrupts auf dem STM8</a></li>
<li><a href="../de456096/index.html">Was der durchschnittliche Geektimes-Leser tut, w√§hrend er in den Wolken schwebt</a></li>
<li><a href="../de456100/index.html">Jetzt in der neuen Verpackung - Kingston A400 im M.2-Format eilt auf den Markt</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>