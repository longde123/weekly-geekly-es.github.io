<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📱 🐑 🤴🏼 Intuitivo RL (aprendizaje de refuerzo): Introducción a Advantage-Actor-Critic (A2C) 🙏🏽 ⚠️ 🥣</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Les traigo a su atención una traducción del artículo de Rudy Gilman y Katherine Wang RL intuitiva: Introducción a Advantage-Actor-Critic (A...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intuitivo RL (aprendizaje de refuerzo): Introducción a Advantage-Actor-Critic (A2C)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442522/"><p>  Hola Habr!  Les traigo a su atención una traducción del artículo de Rudy Gilman y Katherine Wang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">RL intuitiva: Introducción a Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Los especialistas en aprendizaje reforzado (RL) han producido muchos tutoriales excelentes.  Sin embargo, la mayoría describe RL en términos de ecuaciones matemáticas y diagramas abstractos.  Nos gusta pensar sobre el tema desde una perspectiva diferente.  El RL en sí mismo está inspirado en cómo aprenden los animales, entonces, ¿por qué no traducir el mecanismo de RL subyacente nuevamente en fenómenos naturales que se pretende simular?  La gente aprende mejor a través de las historias. </p><br><p>  Esta es la historia del modelo Actor Advantage Critic (A2C).  El modelo de sujeto crítico es una forma popular del modelo Policy Gradient, que en sí mismo es un algoritmo RL tradicional.  Si entiendes A2C, entiendes RL profundo. </p><br><a name="habracut"></a><p>  Después de obtener una comprensión intuitiva de A2C, verifique: </p><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Nuestra implementación simple</a> del código A2C (para capacitación) o nuestra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versión</a> industrial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de PyTorch</a> basada en el modelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI TensorFlow Baselines</a> ; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Una introducción a RL por Barto &amp; Sutton</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el curso canónico de David Silver</a> , una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">revisión de Yusi Lee</a> y el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">repositorio de Denny Brits en GitHub</a> para una inmersión profunda en RL; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El increíble curso fast.ai</a> para una cobertura intuitiva y práctica del aprendizaje profundo en general, implementado en PyTorch; </li><li>  Tutoriales de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Arthur Giuliani</a> RL implementados en TensorFlow. </li></ul><p>  Ilustraciones <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">@embermarke</a> </p><br><p>  En RL, el agente, el zorro Klyukovka, se mueve a través de estados rodeados de acciones, tratando de maximizar las recompensas en el camino. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C recibe entradas de estado - entradas de sensor en el caso de Klukovka - y genera dos salidas: <br>  1) Una evaluación de cuánta remuneración se recibirá, a partir del momento del estado actual, con la excepción de la remuneración actual (existente). <br>  2) Una recomendación sobre qué medidas tomar (política). <br><br>  Crítico: wow, qué maravilloso valle!  ¡Será un día fructífero para buscar comida!  Apuesto a que hoy recogeré 20 puntos antes del atardecer. <br>  "Sujeto": estas flores se ven hermosas, siento un antojo por "A". </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Los modelos Deep RL son máquinas de mapeo de entrada-salida, como cualquier otro modelo de clasificación o regresión.  En lugar de categorizar imágenes o texto, los modelos RL profundos traen estados a acciones y / o estados a valores de estado.  A2C hace las dos cosas. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  Este conjunto de recompensa de acción estatal es una observación.  Ella escribirá esta línea de datos en su diario, pero todavía no va a pensar en ello.  Ella lo llenará cuando se detenga a pensar. <br><br>  Algunos autores asocian la recompensa 1 con el tiempo paso 1, otros la asocian con el paso 2, pero todos tienen en cuenta el mismo concepto: la recompensa está asociada con el estado y la acción la precede inmediatamente. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Enganchar repite el proceso nuevamente.  Primero, percibe su entorno y desarrolla una función V (S) y una recomendación para la acción. <br><br>  Crítico: Este valle parece bastante estándar.  V (S) = 19. <br>  Asunto: Las opciones de acción son muy similares.  Creo que simplemente iré por la pista "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Entonces actúa. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  ¡Recibe una recompensa de +20!  Y registra la observación. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  Ella repite el proceso nuevamente. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  Después de recoger tres observaciones, Klyukovka se detiene a pensar. <br><br>  Otras familias modelo esperan hasta el final del día (Monte Carlo), mientras que otras piensan después de cada paso (un paso). <br>  Antes de que pueda configurar su crítico interno, Klukovka necesita calcular cuántos puntos recibirá realmente en cada estado. <br><br>  Pero primero! <br>  Veamos cómo la prima de Klukovka, Lis Monte Carlo, calcula el verdadero significado de cada estado. <br><br>  Los modelos de Monte Carlo no reflejan su experiencia hasta el final del juego, y dado que el valor del último estado es cero, es muy simple encontrar el verdadero valor de este estado anterior como la suma de las recompensas recibidas después de este momento. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  De hecho, esta es solo una muestra de alta dispersión V (S).  El agente podría seguir fácilmente una trayectoria diferente desde el mismo estado, recibiendo así una recompensa agregada diferente. </p><br><p>  Pero Klyukovka se va, se detiene y reflexiona muchas veces hasta que el día llega a su fin.  Ella quiere saber cuántos puntos obtendrá realmente de cada estado hasta el final del juego, porque quedan varias horas hasta el final del juego. <br><br>  Ahí es donde hace algo realmente inteligente: la zorra Klyukovka estima cuántos puntos recibirá por el último estado en este set.  Afortunadamente, tiene una evaluación correcta de su condición: su crítica. <br>  Con esta evaluación, Klyukovka puede calcular los valores "correctos" de los estados anteriores exactamente como lo hace el zorro de Monte Carlo. <br><br>  Lis Monte Carlo evalúa las marcas de destino, realiza el despliegue de la trayectoria y agrega recompensas hacia adelante desde cada estado.  A2C corta esta trayectoria y la reemplaza con una evaluación de su crítico.  Esta carga inicial reduce la varianza de la puntuación y permite que el A2C se ejecute continuamente, aunque introduciendo un pequeño sesgo. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  Las recompensas a menudo se reducen para reflejar el hecho de que la remuneración ahora es mejor que en el futuro.  Por simplicidad, Klukovka no reduce sus recompensas. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p>  Klukovka ahora puede pasar por cada fila de datos y comparar sus estimaciones de valores de estado con sus valores reales.  Ella usa la diferencia entre estos números para perfeccionar sus habilidades de predicción.  Cada tres pasos durante el día, Klyukovka recopila una valiosa experiencia que vale la pena considerar. <br><br>  “Califiqué mal los estados 1 y 2. ¿Qué hice mal?  Si!  La próxima vez que vea plumas como estas, aumentaré V (S). <br><br>  Puede parecer una locura que Klukovka pueda usar su calificación V (S) como base para compararlo con otros pronósticos.  ¡Pero los animales (incluidos nosotros) hacen esto todo el tiempo!  Si siente que las cosas van bien, no necesita volver a capacitar las acciones que lo llevaron a este estado. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  Al recortar nuestros resultados calculados y reemplazarlos con una estimación de carga inicial, reemplazamos la gran variación de Monte Carlo con un pequeño sesgo.  Los modelos RL suelen sufrir una alta dispersión (que representa todos los caminos posibles), y tal reemplazo generalmente vale la pena. </p><br><p>  Klukovka repite este proceso todo el día, recogiendo tres observaciones de estado-acción-recompensa y reflexionando sobre ellas. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Cada conjunto de tres observaciones es una pequeña serie autocorrelacionada de datos de entrenamiento etiquetados.  Para reducir esta autocorrelación, muchas A2C capacitan a muchos agentes en paralelo, sumando su experiencia antes de enviarla a una red neuronal común. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  El día finalmente está llegando a su fin.  Solo quedan dos pasos. <br>  Como dijimos anteriormente, las recomendaciones de las acciones de Klukovka se expresan en porcentaje de confianza sobre sus capacidades.  En lugar de simplemente elegir la opción más confiable, Klukovka elige de esta distribución de acciones.  Esto asegura que ella no siempre acepta acciones seguras, pero potencialmente mediocres. </p><br><p>  Podría arrepentirme, pero ... A veces, explorando cosas desconocidas, puedes llegar a nuevos descubrimientos emocionantes ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  Para alentar aún más la investigación, un valor llamado entropía se resta de la función de pérdida.  Entropía significa el "alcance" de la distribución de acciones. <br>  - ¡Parece que el juego ha valido la pena! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  O no? <br><br>  A veces, el agente se encuentra en un estado donde todas las acciones conducen a resultados negativos.  A2C, sin embargo, hace frente a situaciones malas. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  Cuando se puso el sol, Klyukovka reflexionó sobre el último conjunto de soluciones. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  Hablamos sobre cómo Klyukovka configura su crítico interno.  Pero, ¿cómo afina su "sujeto" interior?  ¿Cómo aprende a tomar decisiones tan exquisitas? </p><br><p>  La política de gradiente de zorro de mente simple miraría los ingresos reales después de la acción y ajustaría su política para hacer que los buenos ingresos sean más probables: - Parece que mi política en este estado condujo a una pérdida de 20 puntos, creo que en el futuro es mejor hacer "C" menos probable <br><br>  - Pero espera!  Es injusto culpar a la acción "C".  Este estado tenía un valor estimado de -100, por lo que elegir "C" y terminar con -20 fue en realidad una mejora relativa de 80.  Tengo que hacer que "C" sea más probable en el futuro. <br><br>  En lugar de ajustar su política en respuesta a los ingresos totales que recibió al seleccionar la acción C, sintoniza su acción con los ingresos relativos de la acción C. Esto se denomina "ventaja". </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  Lo que llamamos una ventaja es simplemente un error.  Como ventaja, Klukovka lo usa para hacer actividades que fueron sorprendentemente buenas, más probables.  Como error, ella usa la misma cantidad para presionar a su crítico interno a mejorar su evaluación del valor del estado. <br><br>  El sujeto aprovecha: <br>  - "Wow, eso funcionó mejor de lo que pensaba, la acción C debe ser una buena idea". <br>  El crítico usa el error: <br>  “¿Pero por qué me sorprendió?  Probablemente no debería haber evaluado esta condición tan negativamente ". <br><br>  Ahora podemos mostrar cómo se calculan las pérdidas totales: minimizamos esta función para mejorar nuestro modelo. <br>  "Pérdida total = pérdida de acción + pérdida de valor - entropía" <br><br>  Tenga en cuenta que para calcular los gradientes de tres tipos cualitativamente diferentes, tomamos los valores "a través de uno".  Esto es efectivo, pero puede dificultar la convergencia. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Como todos los animales, a medida que Klyukovka crezca, perfeccionará su capacidad de predecir los valores de los estados, ganará más confianza en sus acciones y, con menos frecuencia, se sorprenderá de los premios. </p><br><p>  Los agentes de RL, como Klukovka, no solo generan todos los datos necesarios, simplemente interactúan con el entorno, sino que también evalúan las etiquetas de destino.  Así es, los modelos RL actualizan las calificaciones anteriores para que coincidan mejor con las calificaciones nuevas y mejoradas. <br><br>  Como dice el Dr. David Silver, jefe del grupo RL en Google Deepmind: AI = DL + RL.  Cuando un agente como Klyukovka puede establecer su propia inteligencia, las posibilidades son infinitas ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/442522/">https://habr.com/ru/post/442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442512/index.html">Personalización de Django ORM en el ejemplo de ZomboDB</a></li>
<li><a href="../442514/index.html">Sistemas distribuidos. Patrones de diseño. Reseña del libro</a></li>
<li><a href="../442516/index.html">Pandas Guide to Big Data Analysis</a></li>
<li><a href="../442518/index.html">Las 10 mejores técnicas de piratería web 2018</a></li>
<li><a href="../442520/index.html">Caso. Ahorro de 300 000 p. por mes en publicidad contextual</a></li>
<li><a href="../442524/index.html">Cómo aumentar la seguridad en la identificación personal y los sistemas de control de acceso</a></li>
<li><a href="../442526/index.html">La historia de los reproductores de cassette soviéticos (segunda parte): el auge de Walkmen, un dispositivo para el KGB y las grabadoras</a></li>
<li><a href="../442528/index.html">Cómo hacer que el juego funcione a 60 fps</a></li>
<li><a href="../442530/index.html">Wireshark 3.0.0: revisión de innovaciones</a></li>
<li><a href="../442532/index.html">Grabadoras de video para video vigilancia - gratis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>