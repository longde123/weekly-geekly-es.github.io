<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∂üèæ üë©‚Äçüî¨ üï∫üèΩ Cat Ghonim: Wie kann man Katzen dazu bringen, sich zu Hause nicht auf dem Rasen zu entspannen? üë®‚Äçüë©‚Äçüë¶‚Äçüë¶ üî• ü•¢</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Da war Robert Bond, ein 65-j√§hriger kalifornischer Programmierer. Und er hatte eine Gartenfrau, die ihren sauberen Rasen sehr liebte. Aber dies ist Ka...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cat Ghonim: Wie kann man Katzen dazu bringen, sich zu Hause nicht auf dem Rasen zu entspannen?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ua-hosting/blog/473334/"> Da war Robert Bond, ein 65-j√§hriger kalifornischer Programmierer.  Und er hatte eine Gartenfrau, die ihren sauberen Rasen sehr liebte.  Aber dies ist Kalifornien, es gibt keine zwei Meter hohen Z√§une mit einem Katzenschutzsystem.  Benachbarte Katzen laufen auf dem Rasen und schei√üen! <br><br><img src="https://habrastorage.org/webt/ee/li/u5/eeliu5qcoz1urdayl5mvncwbjzo.jpeg"><br><br>  Das Problem musste gel√∂st werden.  Wie hat Robert das entschieden?  Er kaufte etwas Eisen an seinen Computer, schloss eine √úberwachungskamera im Freien an, die √ºber den Rasen blickte, und tat dann etwas Ungew√∂hnliches. Er lud die verf√ºgbare kostenlose Open Source-Software herunter - ein neuronales Netzwerk - und begann sie zu trainieren, um Katzen im Kamerabild zu erkennen.  Und die Aufgabe am Anfang scheint trivial zu sein, denn wenn Sie etwas lernen und es einfach ist, ist es f√ºr Katzen, weil Katzen mit dem Internet √ºbers√§t sind, gibt es zig Millionen von ihnen.  Wenn alles so einfach war, aber die Dinge schlimmer sind, gehen Katzen im wirklichen Leben meistens nachts zum Mist.  Im Internet gibt es praktisch keine Bilder von Nachtkatzen, die auf dem Rasen pinkeln.  Und einige der Katzen schaffen es sogar, w√§hrend der Arbeit aus dem Bew√§sserungssystem zu trinken, lassen es aber trotzdem fallen. <br><br><img src="https://habrastorage.org/webt/0i/uw/_p/0iuw_pgxfmlmkw_w-ztq2t_hbls.jpeg"><a name="habracut"></a><br><br>  Nachfolgend finden Sie eine Beschreibung des Projekts des Autors. Die englische Version finden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie hier</a> . <br><br>  Dieses Projekt wurde durch zwei Dinge motiviert: den Wunsch, mehr √ºber neuronale Netzwerksoftware zu erfahren, und den Wunsch, benachbarte Katzen zu ermutigen, sich an einem anderen Ort als meinem Rasen aufzuhalten. <br><br>  Das Projekt umfasst nur drei Hardwarekomponenten: die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nvidia Jetson TX1-Karte</a> , die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Foscam FI9800P</a> IP-Kamera und das an das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Relais</a> angeschlossene <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Partikelphoton</a> .  Die Kamera ist an der Seite des Hauses an der Seite des Rasens angebracht.  Sie kontaktiert den WI-FI-Zugangspunkt, gefolgt von Jetson.  Partikelphoton und Relais sind in der Steuereinheit meines Bew√§sserungssystems installiert und mit einem WI-FI-Zugangspunkt in der K√ºche verbunden. <br><br>  Dabei ist die Kamera so konfiguriert, dass √Ñnderungen im Hof ‚Äã‚Äã√ºberwacht werden.  Wenn sich etwas √§ndert, √ºbertr√§gt die Kamera einen Satz von 7 Bildern an Jetson, eines pro Sekunde.  Der von Jetson betriebene Dienst verfolgt eingehende Bilder und √ºbertr√§gt sie an das Deep-Training-Neuronale Netzwerk von Caffe.  Wenn das Netzwerk eine Katze erkennt, signalisiert Jetson dem Partikelphotonenserver in der Cloud, der eine Nachricht an Photon sendet.  Photon reagiert, indem er die Sprinkler zwei Minuten lang einschaltet. <br><br>  Hier ging die Katze in den Rahmen und schaltete die Kamera ein: <br><br><img src="https://habrastorage.org/webt/db/yb/o6/dbybo6fzxy4rfhhqpcuqkr0q02q.jpeg"><br><br>  Nach ein paar Sekunden kam die Katze in die Mitte des Hofes, schaltete die Kamera wieder ein und aktivierte die Sprinkler des Bew√§sserungssystems: <br><br><img src="https://habrastorage.org/webt/oy/44/b2/oy44b223ivw14to9lca8bxwfbhc.jpeg"><br><br><h3>  Kamerainstallation </h3><br>  Die Installation einer Kamera war nichts Ungew√∂hnliches.  Die einzige dauerhafte Verbindung ist eine 12-Volt-Kabelverbindung, die durch ein kleines Loch unter der Kante verl√§uft.  Ich montierte die Kamera auf einer Holzkiste, um den Vorgarten mit einem Rasen festzuhalten.  Ein paar Dr√§hte sind an die Kamera angeschlossen, die ich in einer Box versteckt habe. <br><br>  Befolgen Sie die Anweisungen von Foscam, um es mit Jetsons AP zu verkn√ºpfen (siehe unten).  In meinem Setup befindet sich Jetson unter 10.42.0.1.  Ich habe der Kamera eine feste IP-Adresse von 10.42.0.11 zugewiesen, damit sie leicht zu finden ist.  Schlie√üen Sie anschlie√üend den Windows-Laptop an die Kamera an und konfigurieren Sie den Parameter ‚ÄûWarnung‚Äú, um die √Ñnderung zu aktivieren.  Richten Sie den Upload von 7 Bildern per FTP durch Warnung ein (Warnung).  Geben Sie dann die Benutzer-ID und das Kennwort f√ºr Jetson ein.  Meine Kamera sendet 640x360 Bilder per FTP in ihr Ausgangsverzeichnis. <br><br>  Unten sehen Sie die Parameter, die f√ºr die Kamerakonfiguration ausgew√§hlt wurden. <br><br><img src="https://habrastorage.org/webt/zu/mn/uq/zumnuqz05cz7nq1mvf_jsid-xi0.jpeg"><br><br><h3>  Einrichten eines Teilchenphotons </h3><br>  Photon war einfach einzurichten.  Ich habe es in die Bew√§sserungssteuereinheit gelegt. <br><br><img src="https://habrastorage.org/webt/vr/v4/hg/vrv4hg7qfnd1cctippafwmihbp4.jpeg"><br><br>  Die schwarze Box links mit der blauen LED ist ein bei eBay gekaufter 24-V-Wechselstrom- (5 V) zu 5-V-Gleichstromwandler.  Sie k√∂nnen das wei√üe Relais auf der Relaisplatine und den blauen Anschluss an der Vorderseite sehen.  Das Photon selbst ist auf der rechten Seite.  Beide werden auf ein St√ºck Pappe geklebt, um sie zusammenzuhalten. <br><br>  Der 5-V-Ausgang des Konverters ist mit dem VIN-Anschluss f√ºr Partikelphotonen verbunden.  Die Relaisplatine ist gr√∂√ütenteils analog: Sie verf√ºgt √ºber einen NPN-Transistor mit offenem Kollektor mit einem nominalen 3,3-V-Eingang zur Transistorbasis und ein 3-V-Relais.  Der Photonen-Controller konnte nicht genug Strom liefern, um das Relais zu steuern. Deshalb habe ich den Kollektor des Transistoreingangs √ºber einen Widerstand mit einem Widerstand von 15 Ohm und einer Leistung von 1/2 W an 5 V angeschlossen, um den Strom zu begrenzen.  Die Relaiskontakte sind parallel zum normalen Steuerkreis mit dem Wassergebl√§se verbunden. <br><br>  Hier ist das Anschlussdiagramm: <br><br>  24VAC Konverter 24VAC &lt;---&gt; Steuerkasten 24VAC OUT <br>  24VAC Wandler + 5V &lt;---&gt; Photonen-VIN, Widerstand zur Relaisplatine + 3,3V <br>  24VAC Wandler GND &lt;---&gt; Photon GND, Relais GND <br>  Photon D0 &lt;---&gt; Signaleingang der Relaisplatine <br>  Relais COM &lt;---&gt; Steuerkasten 24VAC OUT <br>  Relais NEIN &lt;---&gt; Wasserventil im Vorgarten <br><br><h3>  Installieren Sie Jetson </h3><br>  Die einzigen Hardwarekomponenten, die Jetson hinzugef√ºgt wurden, sind eine SATA-SSD und ein kleiner Belkin-USB-Hub.  Der Hub verf√ºgt √ºber zwei drahtlose Tasten, die eine Tastatur und eine Maus verbinden. <br><br>  SSD kam ohne Probleme.  Ich habe es auf EXT4 neu formatiert und als / caffe installiert.  Ich empfehle dringend, den gesamten Projektcode, die Git-Repositorys und die Anwendungsdaten von der internen SD-Karte von Jetson zu l√∂schen, da es oft am einfachsten ist, das System w√§hrend eines Jetpack-Upgrades zu l√∂schen. <br><br><img src="https://habrastorage.org/webt/8d/u9/j6/8du9j6xa2inpeyxcvjkneoeuxre.jpeg"><br><br>  Das Einrichten eines drahtlosen Zugangspunkts war ziemlich einfach (wahr!), Wenn Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diese Anleitung</a> befolgt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben</a> .  Verwenden Sie einfach das angegebene Ubuntu-Men√º und f√ºgen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Konfigurationsparameter hinzu</a> . <br><br>  Ich habe vsftpd als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FTP-Server</a> installiert.  Die Konfiguration ist weitgehend serienm√§√üig.  Ich habe anonymes FTP nicht aktiviert.  Ich habe der Kamera einen Benutzernamen und ein Passwort gegeben, die f√ºr nichts mehr verwendet werden. <br><br>  Ich habe Caffe nach dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JetsonHacks-</a> Rezept installiert.  Ich glaube, dass es in aktuellen Versionen kein LMDB_MAP_SIZE-Problem mehr gibt. Versuchen Sie also, es zu erstellen, bevor Sie √Ñnderungen vornehmen.  Sie sollten in der Lage sein, die im JetsonHacks-Shell-Skript genannten Tests und Timing-Demos auszuf√ºhren.  Ich verwende derzeit Cuda 7.0, bin mir jedoch nicht sicher, ob dies zu diesem Zeitpunkt von Bedeutung ist.  Wenn Sie CDNN verwenden, wird in diesen kleinen Systemen eine erhebliche Menge an Speicherplatz gespart.  F√ºgen Sie nach der Erstellung das Build-Verzeichnis zur PATH-Variablen hinzu, damit Skripte Caffe finden k√∂nnen.  F√ºgen Sie au√üerdem das Caffe Python lib-Verzeichnis zu Ihrem PYTHONPATH hinzu. <br><br><pre><code class="plaintext hljs">~ $ echo $PATH /home/rgb/bin:/caffe/drive_rc/src:/caffe/drive_rc/std_caffe/caffe/build/tools:/usr/local/cuda-7.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ~ $ echo $PYTHONPATH /caffe/drive_rc/std_caffe/caffe/python: ~ $ echo $LD_LIBRARY_PATH /usr/local/cuda-7.0/lib:/usr/local/lib</code> </pre> <br>  Ich verwende die Option Fully Convolutional Network for Semantic Segmentation (FCN).  Siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Berkeley Model Zoo</a> , <a href="">Github</a> . <br><br>  Ich habe mehrere andere Netzwerke ausprobiert und mich schlie√ülich f√ºr FCN entschieden.  Weitere Informationen zum Auswahlverfahren finden Sie im n√§chsten Artikel.  Fcn32s funktioniert gut mit TX1 - es nimmt etwas mehr als 1 GB Speicher in Anspruch, l√§uft in etwa 10 Sekunden und segmentiert ein 640x360-Bild in etwa einer Drittelsekunde.  Das aktuelle Github-Repository enth√§lt eine Reihe guter Skripte. Die Einstellung ist unabh√§ngig von der Gr√∂√üe des Bildes. Die Gr√∂√üe des Netzwerks wird so angepasst, dass es Ihren Vorstellungen entspricht. <br><br>  Um es zu versuchen, m√ºssen Sie die bereits trainierten Caffe-Modelle bereitstellen.  Es dauert einige Minuten: Die Dateigr√∂√üe fcn32s-heavy-pascal.caffemodel √ºberschreitet 500 MB. <br><br><pre> <code class="plaintext hljs">$ cd voc-fcn32s $ wget `cat caffemodel-url`</code> </pre><br>  Bearbeiten Sie infer.py, indem Sie den Pfad im Befehl Image.open () in die entsprechende JPG-Datei √§ndern.  √Ñndern Sie die Linie "net" so, dass sie auf das gerade geladene Modell zeigt: <br><br><pre> <code class="plaintext hljs"> -net = caffe.Net('fcn8s/deploy.prototxt', 'fcn8s/fcn8s-heavy-40k.caffemodel', caffe.TEST) +net = caffe.Net('voc-fcn32s/deploy.prototxt', 'voc-fcn32s/fcn32s-heavy-pascal.caffemodel', caffe.TEST)</code> </pre><br>  Sie ben√∂tigen die Datei voc-fcn32s / deploy.prototxt.  Es kann leicht aus voc-fcn32s / train.prototxt generiert werden.  Schauen Sie sich die √Ñnderungen zwischen voc-fcn8s / train.prototxt und voc-fcn8s / deploy.prototxt an, um zu sehen, wie das geht, oder Sie k√∂nnen sie aus meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Chasing-Cats-</a> Repository auf Github herunterladen.  Sie sollten jetzt laufen k√∂nnen. <br><br><pre> <code class="plaintext hljs"> $ python infer.py</code> </pre><br>  Mein Repository enth√§lt mehrere Versionen von infer.py, mehrere Python-Dienstprogramme, die sich mit segmentierten Dateien, Photon-Code und Verwaltungsskripten sowie Betriebsskripten auskennen, mit denen ich das System starte und √ºberwache.  Lesen Sie unten mehr √ºber die Software. <br><br><h3>  Netzwerkauswahl </h3><br>  Neuronale Netze zur Bilderkennung werden normalerweise trainiert, um eine Reihe von Objekten zu erkennen.  Angenommen, wir geben jedem Objekt einen Index von eins bis n.  Das Klassifizierungsnetzwerk beantwortet die Frage "Welche Objekte in diesem Bild?"  R√ºckgabe eines Arrays von Null bis n-1, wobei jeder Array-Eintrag einen Wert von Null bis Eins hat.  Null bedeutet, dass sich das Objekt nicht im Bild befindet.  Ein Wert ungleich Null bedeutet, dass er mit zunehmender Wahrscheinlichkeit vorhanden sein kann, wenn sich der Wert der Einheit n√§hert.  Hier ist eine Katze und ein Mann in einer Reihe von 5 Elementen: <br><br><img src="https://habrastorage.org/webt/oa/ww/ff/oawwffcoudbw13p25vfa44guxna.png"><br><br>  Ein segmentiertes Netzwerk segmentiert Bildpixel von Bereichen, die von Objekten aus unserer Liste belegt sind.  Sie beantwortet die Frage, indem sie ein Array mit einem Datensatz zur√ºckgibt, der jedem Pixel im Bild entspricht.  Jeder Datensatz hat den Wert Null, wenn es sich um ein Hintergrundpixel handelt, oder einen Wert von eins bis n f√ºr n verschiedene Objekte, die er erkennen kann.  Dieses fiktive Beispiel kann der Fu√ü einer Person sein: <br><br><img src="https://habrastorage.org/webt/ud/l6/tw/udl6twr0wz9obfkxqbx3njhrsms.png"><br><br>  Dieses Projekt ist Teil eines gr√∂√üeren Projekts zur Steuerung eines ferngesteuerten Autos mithilfe eines Computers.  Die Idee ist, ein neuronales Netzwerk zu verwenden, um die Position (globale dreidimensionale Position und Ausrichtung) eines Autos zu bestimmen, um Navigationsbefehle an dieses zu √ºbertragen.  Die Kamera ist fest und der Rasen ist meist flach.  Ich kann den Trigger ein wenig verwenden, um die 3D-Position zu √§ndern, damit das neuronale Netzwerk die Bildschirmpixel und die Ausrichtung finden kann.  Die Rolle der Katze bei all dem ist der "beabsichtigte Zweck". <br><br>  Ich dachte zun√§chst haupts√§chlich an das Auto, weil ich nicht wusste, wie es ausgehen w√ºrde, vorausgesetzt, dass das Erkennen einer Katze mit einem vorab trainierten Netzwerk trivial w√§re.  Nach viel Arbeit, die ich in diesem Artikel nicht im Detail beschreiben werde, habe ich beschlossen, dass Sie die Ausrichtung des Autos mit einem ziemlich hohen Ma√ü an Zuverl√§ssigkeit bestimmen k√∂nnen.  Hier ist ein Trainingsschuss in einem Winkel von 292,5 Grad: <br><br><img src="https://habrastorage.org/webt/7m/kz/hv/7mkzhvtay1em-qvcuomfxjuvdy4.jpeg"><br><br>  Der gr√∂√üte Teil dieser Arbeit wurde mit dem Klassifizierungsnetzwerk, dem Caffe-Modell bvlc_reference_caffenet, durchgef√ºhrt.  Aus diesem Grund habe ich beschlossen, dass die Aufgabe des Segmentierungsnetzwerks die Position der Maschine auf dem Bildschirm bestimmt. <br><br>  Das erste Netzwerk, das ich verwendet habe, ist Faster R-CNN [1].  Es werden Begrenzungsrahmen f√ºr Objekte im Bild zur√ºckgegeben, nicht f√ºr Pixel.  Das Netzwerk auf Jetson war jedoch f√ºr diese Anwendung zu langsam.  Die Idee eines Begrenzungsrahmens war sehr attraktiv, deshalb habe ich mir auch das fahrerorientierte Netzwerk angesehen [2].  Sie war auch zu langsam.  FCN [3] war das schnellste Segmentierungsnetzwerk, das ich ausprobiert habe.  "FCN" bedeutet "Vollfaltungsnetzwerk", ein vollst√§ndig Faltungsnetzwerk, da keine bestimmte Bildgr√∂√üe mehr eingegeben werden muss und nur noch aus Faltungen / Pools besteht.  Das Umschalten nur auf Faltungsschichten f√ºhrt zu einer erheblichen Beschleunigung, wodurch meine Bilder auf Jetson um etwa 1/3 Sekunde klassifiziert werden.  FCN enth√§lt eine Reihe guter Python-Skripte f√ºr Schulungen und einfache Bereitstellung.  Python-Skripte passen die Gr√∂√üe des Netzwerks an jede Gr√∂√üe des eingehenden Images an, wodurch die Verarbeitung des Hauptimages vereinfacht wird.  Ich hatte einen Gewinner! <br><br>  Die FCN GitHub-Version bietet mehrere Optionen.  Zuerst habe ich voc-fcn32s ausprobiert.  Es hat perfekt funktioniert.  Voc-fcn32s wurde in 20 Standard-Voc-Klassen vorab trainiert.  Da dies zu einfach ist, habe ich pascalcontext-fcn32s ausprobiert.  Er wurde in 59 Klassen ausgebildet, darunter Gras und B√§ume, also dachte ich, es sollte besser sein.  Es stellte sich jedoch heraus, dass die Ausgabebilder nicht immer viel mehr Pixel hatten und die Segmentierung von Katzen und Menschen, die Gras und B√ºschen √ºberlagert waren, nicht so genau war.  Die Segmentierung von Siftflow war noch komplexer, sodass ich schnell zu den Voc-Optionen zur√ºckkehrte. <br><br>  Die Auswahl von Voc-Netzwerken bedeutet noch drei Dinge zu beachten: Voc-Fcn32s, Voc-Fcn16s und Voc-Fcn8s.  Sie unterscheiden sich im ‚ÄûSchritt‚Äú der Ausgabesegmentierung.  Schritt 32 ist der Hauptschritt des Netzwerks: Das 640 x 360-Bild wird bis zum Abschluss der Faltungsschichten auf ein 20 x 11-Netzwerk reduziert.  Diese grobe Segmentierung "dekonvolviert" sich dann zur√ºck auf 640x360, wie in [3] beschrieben.  Schritt 16 und Schritt 8 werden erreicht, indem dem Netzwerk mehr Logik zur besseren Segmentierung hinzugef√ºgt wird.  Ich habe es nicht einmal versucht - die 32-Segment-Segmentierung ist die erste, die ich ausprobiert habe, und sie ist aufgetaucht, und ich habe mich daran gehalten, weil die Segmentierung f√ºr dieses Projekt gut genug aussieht und das Training, wie beschrieben, f√ºr die beiden anderen Netzwerke komplizierter aussieht. <br><br><h3>  Schulung </h3><br>  Das erste, was mir beim Einschalten und Starten des Systems auffiel, war, dass nur etwa 30% der Katzen vom Netzwerk erkannt wurden.  Ich habe zwei Gr√ºnde daf√ºr gefunden.  Erstens kommen Katzen oft nachts, sodass die Kamera sie im Infrarotlicht sieht.  Dies kann leicht behoben werden - f√ºgen Sie einfach einige segmentierte Infrarotbilder von Katzen zum Training hinzu.  Das zweite Problem, das ich nach Durchsicht mehrerer hundert Fotos von Katzen aus dem Trainingskit entdeckte, ist, dass viele der Fotos zur Sorte ‚ÄûSchau dir meine s√º√üe Katze an‚Äú geh√∂ren.  Dies sind Frontalbilder einer Katze auf Augenh√∂he.  Entweder liegt die Katze auf dem R√ºcken oder auf dem Scho√ü ihres Besitzers.  Sie sehen nicht aus wie Katzen, die in meinem Garten herumlaufen.  Auch hier kann es mit einigen segmentierten Tagesbildern leicht behoben werden. <br><br><img src="https://habrastorage.org/webt/oz/yh/us/ozyhuswgxhoqy0tndmzvqer_lko.jpeg"><br><br>  Wie segmentiere ich ein Objekt in ein Trainingsbild?  Mein Ansatz ist es, das Hintergrundbild zu subtrahieren und dann die Vordergrundpixel zu verarbeiten, um anzuzeigen, dass das Objekt verfolgt werden soll.  In der Praxis funktioniert dies ziemlich gut, da in meinem Archiv von der Kamera normalerweise ein Bild vorhanden ist, das einige Sekunden vor dem segmentierten Bild aufgenommen wurde.  Es gibt jedoch Artefakte, die bereinigt werden m√ºssen, und die Segmentierung muss h√§ufig gekl√§rt werden. Daher habe ich ein grobes Vorbereitungsprogramm zum Bearbeiten von Bildsegmenten geschrieben: src / extract_fg.cpp.  Siehe den Hinweis oben in der Quelldatei zur Verwendung.  Es ist etwas ungeschickt und hat kleine √úberpr√ºfungsfehler und muss verfeinert werden, aber es funktioniert gut genug f√ºr die Aufgabe. <br><br>  Nachdem wir einige Bilder f√ºr das Training haben, wollen wir sehen, wie es geht.  Ich habe voc-fcn32s in das Verzeichnis rgb_voc_fcn32s geklont.  Alle Dateinamen beziehen sich bis zum Ende dieser Lektion auf dieses Verzeichnis. <br><br><pre> <code class="plaintext hljs"> $ cp -r voc-fcn32s rgb_voc_fcn32s</code> </pre><br>  Code auf meinem Github, einschlie√ülich Beispiel-Trainingsdatei in data / rgb_voc.  Die wichtigsten √Ñnderungen sind unten angegeben. <br><br><h3>  Trainingsdateiformat </h3><br>  Die verteilte Datenschicht erwartet fest codierte Bilder und Segmentierungsverzeichnisse.  Die Trainingsdatei enth√§lt eine Zeile pro Datei.  Dann erh√§lt die Datenschicht die Namen der Bilddateien und Segmente und f√ºgt fest codierte Verzeichnisnamen hinzu.  Dies hat bei mir nicht funktioniert, da ich mehrere Klassen von Trainingsdaten habe.  Meine Trainingsdaten enthalten eine Reihe von Linien, von denen jede ein Bild und eine Segmentierung f√ºr dieses Bild enth√§lt. <br><br><pre> <code class="plaintext hljs"> $ head data/rgb_voc/train.txt /caffe/drive_rc/images/negs/MDAlarm_20160620-083644.jpg /caffe/drive_rc/images/empty_seg.png /caffe/drive_rc/images/yardp.fg/0128.jpg /caffe/drive_rc/images/yardp.seg/0128.png /caffe/drive_rc/images/negs/MDAlarm_20160619-174354.jpg /caffe/drive_rc/images/empty_seg.png /caffe/drive_rc/images/yardp.fg/0025.jpg /caffe/drive_rc/images/yardp.seg/0025.png /caffe/drive_rc/images/yardp.fg/0074.jpg /caffe/drive_rc/images/yardp.seg/0074.png /caffe/drive_rc/images/yard.fg/0048.jpg /caffe/drive_rc/images/yard.seg/0048.png /caffe/drive_rc/images/yard.fg/0226.jpg /caffe/drive_rc/images/yard.seg/0226.png</code> </pre><br>  Ich habe voc_layers.py durch rgb_voc_layers.py ersetzt, das das neue Schema versteht: <br><br><pre> <code class="plaintext hljs"> --- voc_layers.py 2016-05-20 10:04:35.426326765 -0700 +++ rgb_voc_layers.py 2016-05-31 08:59:29.680669202 -0700 ... - # load indices for images and labels - split_f = '{}/ImageSets/Segmentation/{}.txt'.format(self.voc_dir, - self.split) - self.indices = open(split_f, 'r').read().splitlines() + # load lines for images and labels + self.lines = open(self.input_file, 'r').read().splitlines()</code> </pre><br>  Und train.prototxt wurde ge√§ndert, um meinen Code rgb_voc_layers zu verwenden.  Beachten Sie, dass die Argumente auch unterschiedlich sind. <br><br><pre> <code class="plaintext hljs"> --- voc-fcn32s/train.prototxt 2016-05-03 09:32:05.276438269 -0700 +++ rgb_voc_fcn32s/train.prototxt 2016-05-27 15:47:36.496258195 -0700 @@ -4,9 +4,9 @@ top: "data" top: "label" python_param { - module: "layers" - layer: "SBDDSegDataLayer" - param_str: "{\'sbdd_dir\': \'../../data/sbdd/dataset\', \'seed\': 1337, \'split\': \'train\', \'mean\': (104.00699, 116.66877, 122.67892)}" + module: "rgb_voc_layers" + layer: "rgbDataLayer" + param_str: "{\'input_file\': \'data/rgb_voc/train.txt\', \'seed\': 1337, \'split\': \'train\', \'mean\': (104.00699, 1</code> </pre><br>  Fast die gleiche √Ñnderung in val.prototxt: <br><br><pre> <code class="plaintext hljs"> --- voc-fcn32s/val.prototxt 2016-05-03 09:32:05.276438269 -0700 +++ rgb_voc_fcn32s/val.prototxt 2016-05-27 15:47:44.092258203 -0700 @@ -4,9 +4,9 @@ top: "data" top: "label" python_param { - module: "layers" - layer: "VOCSegDataLayer" - param_str: "{\'voc_dir\': \'../../data/pascal/VOC2011\', \'seed\': 1337, \'split\': \'seg11valid\', \'mean\': (104.00699, 116.66877, 122.67892)}" + module: "rgb_voc_layers" + layer: "rgbDataLayer" + param_str: "{\'input_file\': \'data/rgb_voc/test.txt\', \'seed\': 1337, \'split\': \'seg11valid\', \'mean\': (104.00699, 116.66877, 122.67892)}"</code> </pre><br><h3>  Solver.py </h3><br>  F√ºhren Sie l√∂sen.py aus, um Ihr Training zu starten: <br><br><pre> <code class="plaintext hljs"> $ python rgb_voc_fcn32s / solve.py</code> </pre><br>  Es ver√§ndert einige der normalen Mechanismen von Caffe.  Insbesondere wird die Anzahl der Iterationen am Ende der Datei festgelegt.  In dieser speziellen Einstellung ist die Iteration ein Bild, da sich die Netzwerkgr√∂√üe f√ºr jedes Bild √§ndert und die Bilder einzeln √ºbersprungen werden. <br><br>  Eines der gro√üartigen Dinge bei der Arbeit mit Nvidia ist, dass wirklich gro√üartige Ger√§te verf√ºgbar sind.  Ich habe einen Titan in eine Workstation eingebaut, und es hat meinem Management nichts ausgemacht, ihn f√ºr etwas so zweifelhaftes wie dieses Projekt verwenden zu lassen.  Mein letzter Trainingslauf war 4.000 Iterationen, was auf Titan etwas mehr als zwei Stunden dauerte. <br><br><h3>  Ich habe ein paar Dinge gelernt </h3><br><ul><li>  Eine Handvoll Bilder (weniger als 50) reichten aus, um das Netzwerk zu trainieren, um Eindringlinge in der Nacht zu erkennen. </li><li>  Nachtaufnahmen lehrten das Netzwerk zu glauben, dass die Schatten auf dem Fu√üweg Katzen sind. </li><li>  Negative Aufnahmen, dh Bilder ohne segmentierte Pixel, helfen bei der Bew√§ltigung des Schattenproblems. </li><li>  Es ist einfach, das Netzwerk mit einer station√§ren Kamera neu zu trainieren, sodass alles, was sich unterscheidet, als zuf√§llig eingestuft wird. </li><li>  Katzen und Menschen, die zuf√§lligen Hintergr√ºnden √ºberlagert sind, helfen bei Problemen, die durch √úbertraining entstehen. </li></ul><br>  Wie Sie sehen k√∂nnen, ist der Prozess iterativ. <br><br><h3>  Empfehlungen </h3><br>  [1] Schnelleres R-CNN: Auf dem Weg zur Echtzeit-Objekterkennung mit regionalen Vorschlagsnetzwerken Shaoqing Ren, Kaiming He, Ross Girshick und Jian Sun <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abs / 1506.01497v3</a> . <br>  [2] Eine empirische Bewertung des tiefen Lernens beim Fahren auf der Autobahn Brody Huval, Tao Wang, Sameep Tandon, Jeff Kiske, Will Song, Joel Pazhayampallil, Mykhaylo Andriluka, Pranav Rajpurkar, Toki Migimatsu, Royce Cheng-Yue, Fernando Cojuju, Fernando Mujica, Andrew Y. Ng <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">arXiv: 1504.01716v3</a> , <a href="">github.com/brodyh/caffe.git</a> . <br>  [3] Vollfaltungsnetzwerke f√ºr die semantische Segmentierung Jonathan Long, Evan Shelhamer, Trevor Darrell <a href="">arXiv: 1411.4038v2</a> , <a href="">github.com/shelhamer/fcn.berkeleyvision.org.git</a> . <br><br><h3>  Schlussfolgerungen </h3><br>  Um dem neuronalen Netzwerk das Erkennen von Nachtkatzen beizubringen, mussten die erforderlichen Daten hinzugef√ºgt und akkumuliert werden.  Danach wurde der letzte Schritt unternommen - das System wird an das Ventil angeschlossen, das das Spritzger√§t startet.  Die Idee ist, dass sobald die Katze den Rasen betritt und sich anpassen m√∂chte, sie zu w√§ssern beginnt.  Die Katze wirft ab.  Damit ist die Aufgabe gel√∂st, die Frau ist gl√ºcklich, und all dieses seltsame Wunder ist ein neuronales Netzwerk, das das Erkennen von Katzen lehrt, herausfindet, dass das Internet nicht √ºber gen√ºgend Quellbilder f√ºr das Training verf√ºgt und das dies gelernt hat. Es wurde das einzige neuronale Netzwerk der Welt, das Nachtkatzen erkennen kann. <br><br>  Es ist erw√§hnenswert, dass all dies von einer Person gemacht wurde, die kein Hyperprogrammierer ist, der sein ganzes Leben lang in Yandex oder Google gearbeitet hat und mit Hilfe von Hardware recht billig, kompakt und einfach ist. <br><br><h3>  Ein bisschen Werbung :) </h3><br>  Vielen Dank f√ºr Ihren Aufenthalt bei uns.  Gef√§llt dir unser Artikel?  M√∂chten Sie weitere interessante Materialien sehen?  Unterst√ºtzen Sie uns, indem Sie eine Bestellung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufgeben</a> oder Ihren Freunden empfehlen, einen <b>Rabatt von 30% f√ºr Habr-Benutzer auf einem einzigartigen analogen Einstiegsserver, den wir f√ºr Sie erfunden haben:</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die ganze Wahrheit √ºber VPS (KVM) E5-2650 v4 (6 Kerne) 10 GB DDR4 240 GB SSD 1 Gbit / s ab 20 $ oder wie teilt man den Server?</a>  (Optionen sind mit RAID1 und RAID10, bis zu 24 Kernen und bis zu 40 GB DDR4 verf√ºgbar). <br><br>  <b>Dell R730xd 2 mal g√ºnstiger?</b>  Nur wir haben <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2 x Intel TetraDeca-Core Xeon 2x E5-2697v3 2,6 GHz 14C 64 GB DDR4 4 x 960 GB SSD 1 Gbit / s 100 TV von 199 US-Dollar</a> in den Niederlanden!</b>  <b><b>Dell R420 - 2x E5-2430 2,2 GHz 6C 128 GB DDR3 2x960 GB SSD 1 Gbit / s 100 TB - ab 99 US-Dollar!</b></b>  Lesen Sie mehr √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Aufbau eines Infrastrukturgeb√§udes.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Klasse mit Dell R730xd E5-2650 v4 Servern f√ºr 9.000 Euro f√ºr einen Cent?</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de473334/">https://habr.com/ru/post/de473334/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de473320/index.html">SpecFlowMaster: So verbessern Sie die Testqualit√§t</a></li>
<li><a href="../de473324/index.html">Jira aus der M√ºllkippe holen, wo ich anfangen soll</a></li>
<li><a href="../de473328/index.html">Wie Methodius zu Anna wurde: die Erfahrung, Sprachnachrichten-Klassifikatoren zu entwickeln und zu starten. Teil 2</a></li>
<li><a href="../de473330/index.html">Hom√∂opathische Tester oder chronische Rekrutierungsprobleme</a></li>
<li><a href="../de473332/index.html">Mitya Alexandrov und Dmitry Konstantinov beim Treffen jug.msk.ru</a></li>
<li><a href="../de473338/index.html">TDD: Wie man Spezifikationen richtig schreibt (beschreibt)</a></li>
<li><a href="../de473340/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends f√ºr die letzte Woche Nr. 386 (21. - 27. Oktober 2019)</a></li>
<li><a href="../de473342/index.html">"Der lange Weg wartet auf Sie ..." oder L√∂sen des Prognoseproblems in C # mithilfe von Ml.NET (DataScience)</a></li>
<li><a href="../de473344/index.html">Konzerte und Events KudaGo in deinem Spiegel</a></li>
<li><a href="../de473346/index.html">Erstellen einer REST-API mit Node.js und einer Oracle-Datenbank. Teil 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>