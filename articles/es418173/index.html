<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôèüèº üò£ üë®üèæ‚Äçüè´ Ida y vuelta para redes neuronales, o una revisi√≥n del uso de codificadores autom√°ticos en an√°lisis de texto üë∑üèº ü¶Ä ü§∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ya escribimos en el primer art√≠culo de nuestro blog corporativo sobre c√≥mo funciona el algoritmo para detectar pr√©stamos transferibles. Solo un par de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ida y vuelta para redes neuronales, o una revisi√≥n del uso de codificadores autom√°ticos en an√°lisis de texto</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/antiplagiat/blog/418173/"> Ya escribimos en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primer art√≠culo de nuestro blog corporativo</a> sobre c√≥mo funciona el algoritmo para detectar pr√©stamos transferibles.  Solo un par de p√°rrafos en ese art√≠culo est√°n dedicados al tema de comparar textos, aunque la idea merece una descripci√≥n mucho m√°s detallada.  Sin embargo, como saben, no se puede contar de inmediato sobre todo, aunque realmente se quiere.  En un intento de rendir homenaje a este tema y a la arquitectura de la red llamada " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">auto-encoder</a> ", con la que tenemos sentimientos muy c√°lidos, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">Oleg_Bakhteev</a> y yo escribimos esta rese√±a. <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Fuente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Learning para PNL (sin magia)</a> <br><br>  Como mencionamos en ese art√≠culo, la comparaci√≥n de textos fue "sem√°ntica": no comparamos los fragmentos de texto en s√≠, sino los vectores correspondientes a ellos.  Dichos vectores se obtuvieron como resultado del entrenamiento de una red neuronal, que mostraba un fragmento de texto de una longitud arbitraria en un vector de una dimensi√≥n grande pero fija.  C√≥mo obtener un mapeo de este tipo y c√≥mo ense√±ar a la red a producir los resultados deseados es un tema separado, que se discutir√° a continuaci√≥n. <br><a name="habracut"></a><br><h1>  ¬øQu√© es un codificador autom√°tico? </h1><br>  Formalmente, una red neuronal se denomina codificador autom√°tico (o codificador autom√°tico), que se entrena para restaurar los objetos recibidos en la entrada de la red. <br><img src="https://habrastorage.org/webt/jy/jw/ip/jyjwipnzwzlyidenzeovey3jba4.png"><br>  El codificador autom√°tico consta de dos partes: un codificador <b>f</b> , que codifica la muestra <b>X</b> en su representaci√≥n interna <b>H</b> , y un decodificador <b>g</b> , que restaura la muestra original.  Por lo tanto, el codificador autom√°tico intenta combinar la versi√≥n restaurada de cada objeto de muestra con el objeto original. <br><br>  Al entrenar un codificador autom√°tico, se minimiza la siguiente funci√≥n: <br><img src="https://habrastorage.org/webt/9f/ay/cm/9faycmmbldgcxvehefjrurcusyq.png"><br><br>  Donde <b>r</b> representa la versi√≥n restaurada del objeto original: <br><img src="https://habrastorage.org/webt/zf/oe/oi/zfoeoiwfxnrscvv0n5cv4keku5k.png"><br><br>  Considere el ejemplo proporcionado en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">blog.keras.io</a> : <br><img src="https://habrastorage.org/webt/lw/wp/i_/lwwpi_kn0wyrduexhkqyrg9jttk.png"><br>  La red recibe un objeto <b>x</b> como entrada (en nuestro caso, el n√∫mero 2). <br><br>  Nuestra red codifica este objeto en un estado oculto.  Luego, de acuerdo con el estado latente, se restaura la reconstrucci√≥n del objeto <b>r</b> , que deber√≠a ser similar a x.  Como vemos, la imagen restaurada (a la derecha) se ha vuelto m√°s borrosa.  Esto se explica por el hecho de que tratamos de mantener en una vista oculta solo los signos m√°s importantes del objeto, por lo que el objeto se restaura con p√©rdidas. <br><br>  El modelo de autocodificador est√° entrenado sobre el principio de un tel√©fono da√±ado, donde una persona (codificador) transmite informaci√≥n <b>(x</b> ) a la segunda persona (decodificador <b>)</b> , y √©l, a su vez, le dice a la tercera <b>(r (x))</b> . <br><br>  Uno de los prop√≥sitos principales de tales codificadores autom√°ticos es reducir la dimensi√≥n del espacio fuente.  Cuando se trata de codificadores autom√°ticos, el procedimiento de entrenamiento de la red neuronal hace que el codificador autom√°tico recuerde las caracter√≠sticas principales de los objetos a partir de los cuales ser√° m√°s f√°cil restaurar los objetos de muestra originales. <br><br>  Aqu√≠ podemos hacer una analog√≠a con el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">m√©todo de los componentes principales</a> : este es un m√©todo para reducir la dimensi√≥n, cuyo resultado es la proyecci√≥n de la muestra en un subespacio en el que la varianza de esta muestra es m√°xima. <br><br>  De hecho, el codificador autom√°tico es una generalizaci√≥n del m√©todo del componente principal: en el caso de que nos limitemos a la consideraci√≥n de modelos lineales, el codificador autom√°tico y el m√©todo del componente principal dan las mismas representaciones vectoriales.  La diferencia surge cuando consideramos modelos m√°s complejos, por ejemplo, redes neuronales multicapa totalmente conectadas, como codificador y decodificador. <br><br>  En el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Reducci√≥n de la dimensionalidad de los datos con redes neuronales</a> se presenta un ejemplo de una comparaci√≥n del m√©todo del componente principal y el codificador autom√°tico: <br><img src="https://habrastorage.org/webt/bj/ap/hq/bjaphq8tpjla39pbn2djsrm84y4.png"><br><br>  Aqu√≠, se muestran los resultados del entrenamiento del codificador autom√°tico y el m√©todo del componente principal para muestrear im√°genes de rostros humanos.  La primera l√≠nea muestra las caras de las personas de la muestra de control, es decir.  de una parte especialmente diferida de la muestra que no fue utilizada por los algoritmos en el proceso de aprendizaje.  En la segunda y tercera l√≠neas se encuentran las im√°genes restauradas de los estados ocultos del codificador autom√°tico y el m√©todo del componente principal, respectivamente, de la misma dimensi√≥n.  Aqu√≠ puede ver claramente cu√°nto mejor funcion√≥ el codificador autom√°tico. <br><br>  En el mismo art√≠culo, otro ejemplo ilustrativo: una comparaci√≥n de los resultados del codificador autom√°tico y el m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">LSA</a> para la tarea de b√∫squeda de informaci√≥n.  El m√©todo LSA, como el m√©todo del componente principal, es un m√©todo cl√°sico de aprendizaje autom√°tico y a menudo se usa en tareas relacionadas con el procesamiento del lenguaje natural. <br><img src="https://habrastorage.org/webt/di/h5/j3/dih5j3wsflfohomgzzrjo9e6n7k.png"><br>  La figura muestra una proyecci√≥n 2D de m√∫ltiples documentos obtenidos usando el codificador autom√°tico y el m√©todo LSA.  Los colores indican el tema del documento.  Se puede ver que la proyecci√≥n del codificador autom√°tico desglosa bien los documentos por tema, mientras que el LSA produce un resultado mucho m√°s ruidoso. <br><br>  Otra aplicaci√≥n importante de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">codificadores</a> autom√°ticos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">es la formaci√≥n previa en red</a> .  El entrenamiento previo de la red se utiliza cuando la red optimizada es lo suficientemente profunda.  En este caso, entrenar la red "desde cero" puede ser bastante dif√≠cil, por lo tanto, primero toda la red se representa como una cadena de codificadores. <br><br>  El algoritmo de pre-entrenamiento es bastante simple: para cada capa entrenamos nuestro propio codificador autom√°tico, y luego establecemos que la salida del siguiente codificador es simult√°neamente la entrada para la siguiente capa de red.  El modelo resultante consiste en una cadena de codificadores entrenados para preservar con entusiasmo las caracter√≠sticas m√°s importantes de los objetos, cada uno en su propia capa.  El esquema de pre-entrenamiento se presenta a continuaci√≥n: <br><img src="https://habrastorage.org/webt/yy/mc/ui/yymcuigpqgegoa_7gwndzfxulia.png"><br>  Fuente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">psyyz10.github.io</a> <br><br>  Esta estructura se llama Autoencoder apilado y a menudo se usa como "overclocking" para entrenar a√∫n m√°s el modelo de red profunda completo.  La motivaci√≥n para tal entrenamiento de una red neuronal es que una red neuronal profunda es una funci√≥n no convexa: en el proceso de entrenamiento de una red, la optimizaci√≥n de los par√°metros puede "estancarse" en un m√≠nimo local.  El preentrenamiento codicioso de los par√°metros de red le permite encontrar un buen punto de partida para el entrenamiento final y, por lo tanto, tratar de evitar tales m√≠nimos locales. <br><br>  Por supuesto, no consideramos todas las estructuras posibles, porque hay <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">autoencoders dispersos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">autoencoders denoising</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">autoencoder contractivo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">autoencoder contractivo de reconstrucci√≥n</a> .  Se diferencian entre s√≠ al usar varias funciones de error y t√©rminos de penalizaci√≥n.  Todas estas arquitecturas, en nuestra opini√≥n, merecen una revisi√≥n por separado.  En nuestro art√≠culo, mostramos, en primer lugar, el concepto general de codificadores autom√°ticos y aquellas tareas espec√≠ficas de an√°lisis de texto que se resuelven al usarlo. <br><br><h2>  ¬øC√≥mo funciona en textos? </h2><br>  Ahora pasamos a ejemplos espec√≠ficos del uso de autocodificadores para tareas de an√°lisis de texto.  Estamos interesados ‚Äã‚Äãen ambos lados de la aplicaci√≥n, ambos modelos para obtener representaciones internas y el uso de estas representaciones internas como atributos, por ejemplo, en el problema de clasificaci√≥n adicional.  Los art√≠culos sobre este tema a menudo abordan tareas como el an√°lisis de sentimientos o la detecci√≥n de reformulaci√≥n, pero tambi√©n hay trabajos que describen el uso de codificadores autom√°ticos para comparar textos en diferentes idiomas o para traducci√≥n autom√°tica. <br><br>  En las tareas de an√°lisis de texto, con mayor frecuencia el objeto es la oraci√≥n, es decir.  secuencia ordenada de palabras.  Por lo tanto, el codificador autom√°tico recibe exactamente esta secuencia de palabras, o m√°s bien, representaciones vectoriales de estas palabras tomadas de alg√∫n modelo previamente entrenado.  En cuanto a las representaciones vectoriales de palabras, se consider√≥ en Habr√© con suficiente detalle, por ejemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  Por lo tanto, el codificador autom√°tico, tomando una secuencia de palabras como entrada, debe entrenar alguna representaci√≥n interna de la oraci√≥n completa que cumpla con las caracter√≠sticas que son importantes para nosotros, en funci√≥n de la tarea.  En problemas de an√°lisis de texto, necesitamos mapear oraciones a vectores para que est√©n cerca en el sentido de alguna funci√≥n de distancia, m√°s a menudo una medida de coseno: <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Fuente: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Deep Learning para PNL (sin magia)</a> <br><br>  Uno de los primeros autores en mostrar el uso exitoso de los codificadores autom√°ticos en el an√°lisis de texto fue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Richard Socher</a> . <br><br>  En su art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Agrupaci√≥n din√°mica y despliegue de codificadores autom√°ticos recursivos para la detecci√≥n de par√°frasis,</a> describe una nueva estructura de codificaci√≥n autom√°tica: desplegar codificador autom√°tico recursivo (desplegar RAE) (ver la figura a continuaci√≥n). <br><img src="https://habrastorage.org/webt/zn/va/o9/znvao90juxs6ywwmm5ywwe6nrdm.png"><br>  Desplegando RAE <br><br>  Se supone que la estructura de la oraci√≥n est√° definida por un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">analizador sint√°ctico</a> .  Se considera la estructura m√°s simple: la estructura de un √°rbol binario.  Tal √°rbol consiste en hojas, palabras de un fragmento, nodos internos (nodos de rama), frases y un v√©rtice terminal.  Tomando la secuencia de palabras (x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> ) como entrada (tres representaciones vectoriales de palabras en este ejemplo), el autocodificador codifica secuencialmente, en este caso, de derecha a izquierda, representaciones vectoriales de palabras de oraci√≥n en representaciones vectoriales de frases, y luego en vector Presentaci√≥n de toda la oferta.  Espec√≠ficamente, en este ejemplo, primero concatenamos los vectores x <sub>2</sub> yx <sub>3</sub> , luego los multiplicamos por la matriz donde <i>tenemos</i> la dimensi√≥n <i>oculta √ó 2 visible</i> , donde <i>oculto</i> es donde est√° el tama√±o de la representaci√≥n interna oculta, <i>visible</i> es la dimensi√≥n de la palabra vector.  Por lo tanto, reducimos la dimensi√≥n, luego agregamos no linealidad utilizando la funci√≥n tanh.  En el primer paso, obtenemos una representaci√≥n vectorial oculta para la frase dos palabras <i>x <sub>2</sub></i> y <i>x <sub>3</sub></i> : <i>h <sub>1</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [x <sub>2</sub> , x <sub>3</sub> ] + b <sub>e</sub> )</i> .  En el segundo, lo combinamos y la palabra restante <i>h <sub>2</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [h <sub>1</sub> , x <sub>1</sub> ] + b <sub>e</sub> )</i> y obtenemos una representaci√≥n vectorial para toda la oraci√≥n - <i>h <sub>2</sub></i> .  Como se mencion√≥ anteriormente, en la definici√≥n de un codificador autom√°tico, debemos minimizar el error entre los objetos y sus versiones restauradas.  En nuestro caso, estas son palabras.  Por lo tanto, habiendo recibido la representaci√≥n vectorial final de toda la oraci√≥n <i>h <sub>2</sub></i> , decodificaremos sus versiones restauradas (x <sub>1</sub> ', x <sub>2</sub> ', x <sub>3</sub> ').  El decodificador aqu√≠ funciona seg√∫n el mismo principio que el codificador, solo la matriz de par√°metros y el vector de desplazamiento son diferentes aqu√≠: <i>W <sub>d</sub></i> y <i>b <sub>d</sub></i> . <br><br>  Usando la estructura de un √°rbol binario, es posible codificar oraciones de cualquier longitud en un vector de dimensi√≥n fija: siempre combinamos un par de vectores de la misma dimensi√≥n, usando la misma matriz de par√°metros <i>W <sub>e</sub></i> .  En el caso de un √°rbol no binario, solo necesita inicializar las matrices de antemano si queremos combinar m√°s de dos palabras: 3, 4, ... n, en este caso la matriz tendr√° la dimensi√≥n <i>oculta √ó invisible</i> . <br><br>  Es de destacar que en este art√≠culo, las representaciones vectoriales formadas de frases se utilizan no solo para resolver el problema de clasificaci√≥n, sino que se reformulan un par de oraciones.  Tambi√©n se presentan los datos de un experimento sobre la b√∫squeda de los vecinos m√°s cercanos: seg√∫n el vector de oferta recibido, se buscan los vectores m√°s cercanos de la muestra que tengan un significado cercano: <br><br><img src="https://habrastorage.org/webt/d6/pv/ae/d6pvaevnd18k2ouizgs3t9k0xbk.png"><br><br>  Sin embargo, nadie nos molesta para usar otras arquitecturas de red para codificar y decodificar para combinar secuencialmente palabras en oraciones. <br><br>  Aqu√≠ hay un ejemplo de un art√≠culo de NIPS 2017 - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aprendizaje de representaci√≥n de p√°rrafos desconvolucional</a> : <br><img src="https://habrastorage.org/webt/g9/u7/0m/g9u70mec8rpyrbnxuqtnflyxcfo.png"><br><br>  Vemos que la codificaci√≥n de la muestra <b>X</b> en la representaci√≥n oculta <b>h</b> ocurre usando una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">red neuronal convolucional</a> , y el decodificador funciona con el mismo principio. <br><br>  O aqu√≠ hay un ejemplo usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GRU-GRU</a> en el art√≠culo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Skip-Thought Vectors</a> . <br><br>  Una caracter√≠stica interesante aqu√≠ es que el modelo funciona con triples de oraciones: ( <i>s <sub>i-1</sub> , s <sub>i</sub> , s <sub>i + 1</sub></i> ).  La oraci√≥n <i>s <sub>i</sub></i> se codifica usando f√≥rmulas GRU est√°ndar, y el decodificador, usando la informaci√≥n de representaci√≥n interna <i>s <sub>i</sub></i> , intenta decodificar <i>s <sub>i-1</sub></i> y <i>s <sub>i + 1</sub></i> , tambi√©n usando GRU. <br><br><img src="https://habrastorage.org/webt/ed/od/-r/edod-radj66y43mbsgu31zxo7nu.png"><br><br>  El principio de funcionamiento en este caso se asemeja al modelo est√°ndar de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">traducci√≥n autom√°tica de redes neuronales</a> , que funciona de acuerdo con el esquema codificador-decodificador.  Sin embargo, aqu√≠ no tenemos dos idiomas, enviamos una frase en un idioma a la entrada de nuestra unidad de codificaci√≥n e intentamos restaurarla.  En el proceso de aprendizaje, hay una minimizaci√≥n de algunas funciones internas de calidad (esto no siempre es un error de reconstrucci√≥n), luego, si es necesario, los vectores previamente entrenados se utilizan como caracter√≠sticas en otro problema. <br><br>  Otro art√≠culo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Autoencoders recursivos biling√ºes por correspondencia para la traducci√≥n autom√°tica estad√≠stica</a> , presenta una arquitectura que da una nueva mirada a la traducci√≥n autom√°tica.  Primero, para dos idiomas, los codificadores autom√°ticos recursivos se entrenan por separado (de acuerdo con el principio descrito anteriormente, donde se introdujo el Desplegamiento RAE).  Luego, entre ellos, se entrena un tercer codificador autom√°tico: un mapeo entre dos idiomas.  Dicha arquitectura tiene una clara ventaja: al mostrar textos en diferentes idiomas en un espacio oculto com√∫n, podemos compararlos sin utilizar la traducci√≥n autom√°tica como un paso intermedio. <br><br><img src="https://habrastorage.org/webt/tj/rq/un/tjrqunxjtnbgsivzz7iohnm8ibs.png"><br><br>  La capacitaci√≥n de codificadores autom√°ticos en fragmentos de texto a menudo se encuentra en art√≠culos sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capacitaci√≥n</a> en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">clasificaci√≥n</a> .  Aqu√≠, nuevamente, el hecho de que estamos entrenando el funcional final de la calidad de la clasificaci√≥n es importante, primero entrenamos previamente el codificador autom√°tico para inicializar mejor los vectores de solicitudes y respuestas enviadas a la entrada de la red. <br><br><img src="https://habrastorage.org/webt/5t/a1/qn/5ta1qnx9gqzl9dypb0t4nhgcjtg.jpeg"><br><br>  Y, por supuesto, no podemos dejar de mencionar los <a href="">Autoencoders Variacionales</a> , o <a href="">VAE</a> , como modelos generativos.  Es mejor, por supuesto, ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">esta entrada de la conferencia de Yandex</a> .  Es suficiente para nosotros decir lo siguiente: si queremos <i>generar</i> objetos desde el espacio oculto de un codificador autom√°tico convencional, entonces la calidad de dicha generaci√≥n ser√° baja, ya que no sabemos nada sobre la distribuci√≥n de la variable oculta.  Pero puede entrenar inmediatamente al codificador autom√°tico para generar, introduciendo un supuesto de distribuci√≥n. <br><br>  Y luego, usando VAE, puede generar textos desde este espacio oculto, por ejemplo, como lo hacen los autores del art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Generando oraciones desde un espacio continuo</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un autoencoder variacional convolucional h√≠brido para la generaci√≥n de texto</a> . <br><br>  Las propiedades generativas de VAE tambi√©n funcionan bien en tareas que comparan textos en diferentes idiomas: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un enfoque de codificaci√≥n autom√°tica variacional para inducir incrustaciones de palabras</a> en varios idiomas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">es</a> un excelente ejemplo de esto. <br><br>  Como conclusi√≥n, queremos hacer un peque√±o pron√≥stico.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El aprendizaje de representaci√≥n</a> : la capacitaci√≥n en representaciones internas utilizando exactamente VAE, especialmente en conjunto con las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Redes adversarias generativas</a> , es uno de los enfoques m√°s desarrollados en los √∫ltimos a√±os; esto se puede juzgar por al menos los temas m√°s frecuentes de los art√≠culos en las √∫ltimas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conferencias principales de</a> aprendizaje autom√°tico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICLR 2018</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICML 2018</a> .  Esto es bastante l√≥gico, porque su uso ha ayudado a mejorar la calidad en una serie de tareas, y no solo en relaci√≥n con los textos.  Pero este es el tema de una revisi√≥n completamente diferente ... </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es418173/">https://habr.com/ru/post/es418173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es418163/index.html">Pruebas de producci√≥n: Netflix Chaos Automation Platform</a></li>
<li><a href="../es418165/index.html">Cuasar, Sobaken y Alima√±as: revelando detalles de la actual campa√±a de espionaje cibern√©tico</a></li>
<li><a href="../es418167/index.html">ScadaPy: agregue el protocolo IEC 60870-5-104</a></li>
<li><a href="../es418169/index.html">¬øQu√© hay de nuevo en Veeam Availability Console 2.0 Update 1?</a></li>
<li><a href="../es418171/index.html">¬øEn qu√© m√©tricas confiar si los usuarios realizan pocas conversiones en el sitio?</a></li>
<li><a href="../es418177/index.html">Edici√≥n de im√°genes .heic sin p√©rdida de color</a></li>
<li><a href="../es418183/index.html">Aplicaci√≥n de la anal√≠tica del habla en los negocios.</a></li>
<li><a href="../es418185/index.html">Una historia de autopsia: c√≥mo revirtimos a Hancitor</a></li>
<li><a href="../es418187/index.html">En Estados Unidos, sugirieron reemplazar todas las bibliotecas con centros de Amazon. El p√∫blico est√° indignado.</a></li>
<li><a href="../es418189/index.html">Heredero de Zeus: por qu√© el troyano IcedID es peligroso para los clientes bancarios</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>