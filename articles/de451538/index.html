<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõë üêó üë®üèæ‚Äçüî¨ Amazon Redshift Parallel Scaling Guide und Testergebnisse üëßüèø üöæ üë®‚Äçüë®‚Äçüë¶‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei Skyeng verwenden wir Amazon Redshift, einschlie√ülich paralleler Skalierung. Daher erschien uns der Artikel von Stefan Gromall, dem Gr√ºnder von dot...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Amazon Redshift Parallel Scaling Guide und Testergebnisse</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/skyeng/blog/451538/"><img src="https://habrastorage.org/webt/mo/pi/gt/mopigtqlz7wpuhwhxsmrlcvgpky.png" alt="Bild"><br><br>  <i>Bei Skyeng verwenden wir Amazon Redshift, einschlie√ülich paralleler Skalierung. Daher erschien uns der Artikel von Stefan Gromall, dem Gr√ºnder von dotgo.com, f√ºr intermix.io interessant.</i>  <i>Nach dem Transfer - ein kleiner Teil unserer Erfahrung vom Ingenieur laut Daniyar Belkhodzhaev.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Amazon Redshift-Architektur</a> erm√∂glicht die Skalierung durch Hinzuf√ºgen neuer Knoten zum Cluster.  Die Bew√§ltigung der Spitzenanzahl von Anforderungen kann zu einer √úberversorgung von Knoten f√ºhren.  Die Parallelit√§tsskalierung erh√∂ht im Gegensatz zum Hinzuf√ºgen neuer Knoten die Rechenleistung nach Bedarf. <br><br>  Die parallele Skalierung von Amazon Redshift bietet Redshift-Clustern zus√§tzliche Leistung f√ºr die Bearbeitung von Spitzenanforderungen.  Es funktioniert durch √úbertragen von Anforderungen an neue "parallele" Cluster im Hintergrund.  Anforderungen werden basierend auf der WLM-Konfiguration und den Regeln weitergeleitet. <br><a name="habracut"></a><br>  Die Preisgestaltung f√ºr die parallele Skalierung basiert auf einem frei verwendbaren Kreditmodell.  √úber der Norm f√ºr kostenlose Kredite basiert die Zahlung auf dem Zeitpunkt, zu dem der Cluster mit paralleler Skalierung Anforderungen verarbeitet. <br><br>  Der Autor testete die parallele Skalierung in einem der internen Cluster.  In diesem Beitrag wird er √ºber die Testergebnisse sprechen und Tipps f√ºr den Einstieg geben. <br><br><h3>  Clusteranforderungen </h3><br>  Um die parallele Skalierung verwenden zu k√∂nnen, muss ein Amazon Redshift-Cluster die folgenden Anforderungen erf√ºllen: <br><br><ul><li>  <b>Plattform:</b> EC2-VPC; </li><li>  <b>Knotentyp:</b> dc2.8xlarge, ds2.8xlarge, dc2.large oder ds2.xlarge; </li><li>  <b>Anzahl der Knoten:</b> von 2 bis 32 (Cluster mit einem Knoten werden nicht unterst√ºtzt). </li></ul><br><h3>  G√ºltige Anforderungstypen </h3><br>  Die parallele Skalierung ist nicht f√ºr alle Arten von Abfragen geeignet.  In der ersten Version werden nur Leseanforderungen verarbeitet, die drei Bedingungen erf√ºllen: <br><br><ul><li>  Schreibgesch√ºtzte SELECT-Abfragen (obwohl weitere Typen geplant sind); </li><li>  Die Abfrage bezieht sich nicht auf eine Tabelle mit der INTERLEAVED-Sortierung. </li><li>  Die Abfrage verwendet Amazon Redshift Spectrum nicht, um auf externe Tabellen zu verweisen. </li></ul><br>  Um zu einem parallelen Skalierungscluster zu routen, muss die Anforderung in die Warteschlange gestellt werden.  Dar√ºber hinaus werden Abfragen, die f√ºr die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SQA-</a> Warteschlange <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Short Query Acceleration)</a> geeignet sind, nicht in Clustern mit paralleler Skalierung ausgef√ºhrt. <br><br>  Warteschlangen und SQAs erfordern die korrekte Konfiguration von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redshift Workload Management (WLM)</a> .  Wir empfehlen, dass Sie zuerst Ihr WLM optimieren - dies reduziert die Notwendigkeit einer parallelen Skalierung.  Dies ist wichtig, da die parallele Skalierung nur f√ºr eine bestimmte Anzahl von Stunden kostenlos ist.  AWS behauptet, dass die parallele Skalierung f√ºr 97% der Kunden kostenlos sein wird, was uns zum Thema Preisgestaltung bringt. <br><br><h3>  Kosten f√ºr die parallele Skalierung </h3><br>  F√ºr die parallele Skalierung bietet AWS ein Kreditmodell an.  Jeder aktive <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Redshift-</a> Cluster sammelt st√ºndlich Kredite, bis zu einer Stunde kostenloser paralleler Skalierungskredite pro Tag. <br><br>  Sie zahlen nur, wenn die Verwendung von parallelen Skalierungsclustern die Anzahl der erhaltenen Kredite √ºberschreitet. <br><br>  Die Kosten werden mit der Nachfragerate pro Sekunde f√ºr einen parallelen Cluster berechnet, der √ºber der freien Rate verwendet wird.  Die Zahlung erfolgt nur w√§hrend der Ausf√ºhrung Ihrer Anforderungen mit einer Mindestzahlung von einer Minute bei jeder Aktivierung eines parallelen Skalierungsclusters.  Die On-Demand-Rate pro Sekunde wird auf der Grundlage der allgemeinen Grunds√§tze der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Redshift-</a> Preisgestaltung berechnet, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dh</a> sie h√§ngt vom Knotentyp und der Anzahl der Knoten in Ihrem Cluster ab. <br><br><h3>  Parallele Skalierung ausf√ºhren </h3><br>  Die parallele Skalierung beginnt f√ºr jede WLM-Warteschlange.  Gehen Sie zur AWS Redshift-Konsole und w√§hlen Sie im linken Navigationsmen√º ‚ÄûWorkload Management‚Äú.  W√§hlen Sie im n√§chsten Dropdown-Men√º die WLM-Gruppe Ihres Clusters aus. <br><br>  Neben jeder Warteschlange wird eine neue Spalte mit dem Namen "Concurrency Scaling Mode" angezeigt.  Der Standardwert ist "Aus".  Klicken Sie auf "√Ñndern" und Sie k√∂nnen die Einstellungen f√ºr jede Warteschlange √§ndern. <br><br><img src="https://habrastorage.org/webt/rz/g9/mk/rzg9mkvwxcmlhry2v522jjrtcdk.png"><br><br><h3>  Konfiguration </h3><br>  Die parallele Skalierung leitet relevante Abfragen an neue dedizierte Cluster weiter.  Neue Cluster haben dieselbe Gr√∂√üe (Typ und Anzahl der Knoten) wie der Hauptcluster. <br><br>  Die Standardanzahl der f√ºr die parallele Skalierung verwendeten Cluster ist eins (1) mit der M√∂glichkeit, insgesamt bis zu zehn (10) Cluster zu konfigurieren. <br>  Die Gesamtzahl der Cluster f√ºr die parallele Skalierung kann mit dem Parameter max_concurrency_scaling_clusters festgelegt werden.  Durch Erh√∂hen dieser Einstellung werden zus√§tzliche redundante Cluster bereitgestellt. <br><br><img src="https://habrastorage.org/webt/p6/we/qu/p6wequjhe3-yxannbvr_zndpqdg.png"><br><br><h3>  √úberwachung </h3><br>  Die AWS Redshift-Konsole verf√ºgt √ºber mehrere zus√§tzliche Diagramme.  Das Diagramm Max Configured Concurrency Scaling Clusters zeigt den Wert von max_concurrency_scaling_clusters im Zeitverlauf an. <br><br><img src="https://habrastorage.org/webt/ti/mw/au/timwauz4qrbalhetjn3ttccxui4.png"><br><br>  Die Anzahl der aktiven Skalierungscluster wird im Abschnitt "Parallelit√§tsskalierungsaktivit√§t" in der Benutzeroberfl√§che angezeigt: <br><br><img src="https://habrastorage.org/webt/zq/ai/xx/zqaixxhdn7i-pkwockkal_7ujvw.png"><br><br>  Auf der Registerkarte "Anforderungen" gibt es eine Spalte, in der angegeben ist, ob die Anforderung im Hauptcluster oder im Cluster mit paralleler Skalierung ausgef√ºhrt wurde: <br><br><img src="https://habrastorage.org/webt/p-/sw/p8/p-swp8elamy2ecxarcydioa0_ac.png"><br><br>  Unabh√§ngig davon, ob eine bestimmte Anforderung im Hauptcluster oder √ºber einen Cluster mit paralleler Skalierung ausgef√ºhrt wurde, wird sie in stl_query.concurrency_scaling_status gespeichert. <br><br><img src="https://habrastorage.org/webt/kf/ns/_4/kfns_4r21bi4fbmqzpk2nfgv1ba.png"><br><br>  Der Wert 1 gibt an, dass die Anforderung in einem Cluster mit paralleler Skalierung ausgef√ºhrt wurde, w√§hrend andere Werte angeben, dass sie im Hauptcluster ausgef√ºhrt wurde. <br><br>  Ein Beispiel: <br><br><img src="https://habrastorage.org/webt/cn/3u/vh/cn3uvh7rn90c8bydqicyz3zlir8.png"><br><br>  Informationen zur parallelen Skalierung werden auch in einigen anderen Tabellen und Ansichten gespeichert, z. B. SVCS_CONCURRENCY_SCALING_USAGE.  Dar√ºber hinaus gibt es eine Reihe von Katalogtabellen, in denen Informationen zur parallelen Skalierung gespeichert sind. <br><br><h3>  Ergebnisse </h3><br>  Die Autoren haben am 29.03.19 um ca. 18:30 Uhr GMT eine parallele Skalierung f√ºr eine Warteschlange im internen Cluster gestartet. Sie haben den Parameter max_concurrency_scaling_clusters um ca. 20:30 Uhr am 29. M√§rz 2019 in 3 ge√§ndert. <br><br>  So simulieren Sie die Anforderungswarteschlange und reduzieren die Anzahl der Steckpl√§tze f√ºr diese Warteschlange von 15 auf 5. <br><br>  Das folgende Dashboard-Diagramm von intermix.io zeigt die Anzahl der Anforderungen, die ausgef√ºhrt und in die Warteschlange gestellt werden, nachdem die Anzahl der Slots verringert wurde. <br><br><img src="https://habrastorage.org/webt/d4/0p/7q/d40p7qzi5ty79bbw_irhtk4qfzs.png"><br><br>  Wir sehen, dass sich die Wartezeit f√ºr Anforderungen in der Warteschlange erh√∂ht hat, w√§hrend die maximale Zeit mehr als 5 Minuten betr√§gt. <br><br><img src="https://habrastorage.org/webt/ov/xp/rn/ovxprnqxnz1fy9ds4xm4vmn6vte.png"><br><br>  Hier sind die relevanten Informationen von der AWS-Konsole dar√ºber, was w√§hrend dieser Zeit passiert ist: <br><br><img src="https://habrastorage.org/webt/rm/f-/ip/rmf-ipbke1vkled8fyo8zch9jpu.png"><br><br>  Redshift hat drei (3) parallele Skalierungscluster wie konfiguriert gestartet.  Es scheint, dass diese Cluster nicht vollst√§ndig genutzt wurden, obwohl viele der Anforderungen in unserem Cluster in der Warteschlange standen. <br><br>  Das Verwendungsdiagramm korreliert mit dem Skalierungsaktivit√§tsdiagramm: <br><br><img src="https://habrastorage.org/webt/am/ho/yk/amhoykwqbkcttpfdiv5gvizkvog.png"><br><br>  Nach einigen Stunden √ºberpr√ºften die Autoren die Warteschlange, und es scheint, dass 6 Anforderungen mit paralleler Skalierung ausgef√ºhrt wurden.  Wir haben auch zwei Anfragen selektiv √ºber die Benutzeroberfl√§che gepr√ºft.  Sie haben nicht √ºberpr√ºft, wie diese Werte verwendet werden sollen, wenn mehrere parallele Cluster gleichzeitig aktiv sind. <br><br><img src="https://habrastorage.org/webt/fi/2f/rb/fi2frbpdkxqfcj1q-4rp12wrjz4.png"><br><br><h3>  Schlussfolgerungen </h3><br>  Parallele Skalierung kann die Warteschlangenzeit von Anforderungen w√§hrend Spitzenlasten reduzieren. <br><br>  Nach den Ergebnissen des Basistests stellte sich heraus, dass sich die Situation mit Ladeanforderungen teilweise verbessert hat.  Die parallele Skalierung allein l√∂ste jedoch nicht alle Probleme mit der Parallelit√§t. <br><br>  Dies ist auf Einschr√§nkungen bei den Abfragetypen zur√ºckzuf√ºhren, die die parallele Skalierung verwenden k√∂nnen.  Zum Beispiel haben Autoren viele Tabellen mit verschachtelten Sortierschl√ºsseln, und der gr√∂√üte Teil unserer Arbeitslast ist ein Datensatz. <br><br>  Obwohl die parallele Skalierung in der WLM-Konfiguration keine universelle L√∂sung darstellt, ist die Verwendung dieser Funktion in jedem Fall einfach und verst√§ndlich. <br><br>  Daher empfiehlt der Autor, es f√ºr Ihre WLM-Warteschlangen zu verwenden.  Beginnen Sie mit einem einzelnen parallelen Cluster und √ºberwachen Sie die Spitzenlast √ºber die Konsole, um festzustellen, ob die neuen Cluster vollst√§ndig ausgelastet sind. <br><br>  Da AWS zus√§tzliche Arten von Abfragen und Tabellen unterst√ºtzt, sollte die parallele Skalierung schrittweise effizienter werden. <br><blockquote>  <b>Kommentar von Belkhodzhaev Daniyar, einem Ingenieur nach Skyeng</b> <br><br>  Wir bei Skyeng haben auch sofort auf die sich abzeichnende M√∂glichkeit der parallelen Skalierung hingewiesen. <br>  Die Funktionalit√§t ist sehr attraktiv, insbesondere angesichts der Tatsache, dass laut AWS die meisten Benutzer nicht einmal extra daf√ºr bezahlen m√ºssen. <br><br>  So kam es, dass wir Mitte April eine ungew√∂hnliche Flut von Anfragen an den Redshift-Cluster hatten.  In dieser Zeit haben wir h√§ufig auf die Parallelit√§tsskalierung zur√ºckgegriffen. Manchmal arbeitete der zus√§tzliche Cluster 24 Stunden am Tag, ohne anzuhalten. <br><br>  Dies erm√∂glichte es, die Situation akzeptabel zu machen, wenn sie das Warteschlangenproblem nicht vollst√§ndig l√∂ste. <br><br>  Unsere Beobachtungen stimmen weitgehend mit dem Eindruck der Jungs von intermix.io √ºberein. <br><br>  Wir haben auch festgestellt, dass trotz des Vorhandenseins ausstehender Anforderungen in der Warteschlange nicht alle Anforderungen sofort an einen parallelen Cluster umgeleitet wurden.  Anscheinend liegt dies daran, dass der parallele Cluster noch einige Zeit zum Starten ben√∂tigt.  Infolgedessen haben wir bei kurzfristigen Spitzenlasten immer noch kleine Warteschlangen, und die entsprechenden Alarme haben Zeit zum Arbeiten. <br><br>  Nachdem wir die abnormalen Belastungen im April beseitigt hatten, gingen wir, wie AWS erwartet hatte, in den episodischen Nutzungsmodus - als Teil der freien Norm. <br>  Sie k√∂nnen die gleichzeitigen Skalierungskosten in AWS Cost Explorer verfolgen.  Sie m√ºssen Service - Rotverschiebung, Verwendungstyp - CS ausw√§hlen, z. B. USW2-CS: dc2.large. <br><br>  Details zu den Preisen in russischer Sprache finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier.</a> </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451538/">https://habr.com/ru/post/de451538/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451522/index.html">Wir stellen die Automatisierung in wenigen Stunden bereit: TypeScript, Protractor, Jasmine</a></li>
<li><a href="../de451524/index.html">Die Geschichte, wie der Autoplay Media Studio 8.5.3.0-Wasserkocher kaputt ging</a></li>
<li><a href="../de451528/index.html">"Und so geht es": Cloud-Anbieter sind sich nicht √ºber personenbezogene Daten einig</a></li>
<li><a href="../de451532/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 459 (30.04.2019 - 05.06.2019)</a></li>
<li><a href="../de451534/index.html">12 Prinzipien der Animation bei der Entwicklung von Videospielen</a></li>
<li><a href="../de451540/index.html">Wie viele Entwickler m√ºssen einen Service wie Airbnb erstellen?</a></li>
<li><a href="../de451542/index.html">Wie und warum haben wir die Meilensteinerkennung in Mail.ru Cloud durchgef√ºhrt?</a></li>
<li><a href="../de451552/index.html">Wir bauen Netzwerkvertriebskan√§le des Gadgets DO-RA auf</a></li>
<li><a href="../de451556/index.html">Flattern: Anwendungslokalisierung mit Android Studio</a></li>
<li><a href="../de451558/index.html">Ein Tag im Leben der QS-Automatisierung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>