<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛑 🐗 👨🏾‍🔬 Amazon Redshift Parallel Scaling Guide und Testergebnisse 👧🏿 🚾 👨‍👨‍👦‍👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bei Skyeng verwenden wir Amazon Redshift, einschließlich paralleler Skalierung. Daher erschien uns der Artikel von Stefan Gromall, dem Gründer von dot...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Amazon Redshift Parallel Scaling Guide und Testergebnisse</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/skyeng/blog/451538/"><img src="https://habrastorage.org/webt/mo/pi/gt/mopigtqlz7wpuhwhxsmrlcvgpky.png" alt="Bild"><br><br>  <i>Bei Skyeng verwenden wir Amazon Redshift, einschließlich paralleler Skalierung. Daher erschien uns der Artikel von Stefan Gromall, dem Gründer von dotgo.com, für intermix.io interessant.</i>  <i>Nach dem Transfer - ein kleiner Teil unserer Erfahrung vom Ingenieur laut Daniyar Belkhodzhaev.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Amazon Redshift-Architektur</a> ermöglicht die Skalierung durch Hinzufügen neuer Knoten zum Cluster.  Die Bewältigung der Spitzenanzahl von Anforderungen kann zu einer Überversorgung von Knoten führen.  Die Parallelitätsskalierung erhöht im Gegensatz zum Hinzufügen neuer Knoten die Rechenleistung nach Bedarf. <br><br>  Die parallele Skalierung von Amazon Redshift bietet Redshift-Clustern zusätzliche Leistung für die Bearbeitung von Spitzenanforderungen.  Es funktioniert durch Übertragen von Anforderungen an neue "parallele" Cluster im Hintergrund.  Anforderungen werden basierend auf der WLM-Konfiguration und den Regeln weitergeleitet. <br><a name="habracut"></a><br>  Die Preisgestaltung für die parallele Skalierung basiert auf einem frei verwendbaren Kreditmodell.  Über der Norm für kostenlose Kredite basiert die Zahlung auf dem Zeitpunkt, zu dem der Cluster mit paralleler Skalierung Anforderungen verarbeitet. <br><br>  Der Autor testete die parallele Skalierung in einem der internen Cluster.  In diesem Beitrag wird er über die Testergebnisse sprechen und Tipps für den Einstieg geben. <br><br><h3>  Clusteranforderungen </h3><br>  Um die parallele Skalierung verwenden zu können, muss ein Amazon Redshift-Cluster die folgenden Anforderungen erfüllen: <br><br><ul><li>  <b>Plattform:</b> EC2-VPC; </li><li>  <b>Knotentyp:</b> dc2.8xlarge, ds2.8xlarge, dc2.large oder ds2.xlarge; </li><li>  <b>Anzahl der Knoten:</b> von 2 bis 32 (Cluster mit einem Knoten werden nicht unterstützt). </li></ul><br><h3>  Gültige Anforderungstypen </h3><br>  Die parallele Skalierung ist nicht für alle Arten von Abfragen geeignet.  In der ersten Version werden nur Leseanforderungen verarbeitet, die drei Bedingungen erfüllen: <br><br><ul><li>  Schreibgeschützte SELECT-Abfragen (obwohl weitere Typen geplant sind); </li><li>  Die Abfrage bezieht sich nicht auf eine Tabelle mit der INTERLEAVED-Sortierung. </li><li>  Die Abfrage verwendet Amazon Redshift Spectrum nicht, um auf externe Tabellen zu verweisen. </li></ul><br>  Um zu einem parallelen Skalierungscluster zu routen, muss die Anforderung in die Warteschlange gestellt werden.  Darüber hinaus werden Abfragen, die für die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SQA-</a> Warteschlange <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Short Query Acceleration)</a> geeignet sind, nicht in Clustern mit paralleler Skalierung ausgeführt. <br><br>  Warteschlangen und SQAs erfordern die korrekte Konfiguration von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Redshift Workload Management (WLM)</a> .  Wir empfehlen, dass Sie zuerst Ihr WLM optimieren - dies reduziert die Notwendigkeit einer parallelen Skalierung.  Dies ist wichtig, da die parallele Skalierung nur für eine bestimmte Anzahl von Stunden kostenlos ist.  AWS behauptet, dass die parallele Skalierung für 97% der Kunden kostenlos sein wird, was uns zum Thema Preisgestaltung bringt. <br><br><h3>  Kosten für die parallele Skalierung </h3><br>  Für die parallele Skalierung bietet AWS ein Kreditmodell an.  Jeder aktive <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Redshift-</a> Cluster sammelt stündlich Kredite, bis zu einer Stunde kostenloser paralleler Skalierungskredite pro Tag. <br><br>  Sie zahlen nur, wenn die Verwendung von parallelen Skalierungsclustern die Anzahl der erhaltenen Kredite überschreitet. <br><br>  Die Kosten werden mit der Nachfragerate pro Sekunde für einen parallelen Cluster berechnet, der über der freien Rate verwendet wird.  Die Zahlung erfolgt nur während der Ausführung Ihrer Anforderungen mit einer Mindestzahlung von einer Minute bei jeder Aktivierung eines parallelen Skalierungsclusters.  Die On-Demand-Rate pro Sekunde wird auf der Grundlage der allgemeinen Grundsätze der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Amazon Redshift-</a> Preisgestaltung berechnet, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dh</a> sie hängt vom Knotentyp und der Anzahl der Knoten in Ihrem Cluster ab. <br><br><h3>  Parallele Skalierung ausführen </h3><br>  Die parallele Skalierung beginnt für jede WLM-Warteschlange.  Gehen Sie zur AWS Redshift-Konsole und wählen Sie im linken Navigationsmenü „Workload Management“.  Wählen Sie im nächsten Dropdown-Menü die WLM-Gruppe Ihres Clusters aus. <br><br>  Neben jeder Warteschlange wird eine neue Spalte mit dem Namen "Concurrency Scaling Mode" angezeigt.  Der Standardwert ist "Aus".  Klicken Sie auf "Ändern" und Sie können die Einstellungen für jede Warteschlange ändern. <br><br><img src="https://habrastorage.org/webt/rz/g9/mk/rzg9mkvwxcmlhry2v522jjrtcdk.png"><br><br><h3>  Konfiguration </h3><br>  Die parallele Skalierung leitet relevante Abfragen an neue dedizierte Cluster weiter.  Neue Cluster haben dieselbe Größe (Typ und Anzahl der Knoten) wie der Hauptcluster. <br><br>  Die Standardanzahl der für die parallele Skalierung verwendeten Cluster ist eins (1) mit der Möglichkeit, insgesamt bis zu zehn (10) Cluster zu konfigurieren. <br>  Die Gesamtzahl der Cluster für die parallele Skalierung kann mit dem Parameter max_concurrency_scaling_clusters festgelegt werden.  Durch Erhöhen dieser Einstellung werden zusätzliche redundante Cluster bereitgestellt. <br><br><img src="https://habrastorage.org/webt/p6/we/qu/p6wequjhe3-yxannbvr_zndpqdg.png"><br><br><h3>  Überwachung </h3><br>  Die AWS Redshift-Konsole verfügt über mehrere zusätzliche Diagramme.  Das Diagramm Max Configured Concurrency Scaling Clusters zeigt den Wert von max_concurrency_scaling_clusters im Zeitverlauf an. <br><br><img src="https://habrastorage.org/webt/ti/mw/au/timwauz4qrbalhetjn3ttccxui4.png"><br><br>  Die Anzahl der aktiven Skalierungscluster wird im Abschnitt "Parallelitätsskalierungsaktivität" in der Benutzeroberfläche angezeigt: <br><br><img src="https://habrastorage.org/webt/zq/ai/xx/zqaixxhdn7i-pkwockkal_7ujvw.png"><br><br>  Auf der Registerkarte "Anforderungen" gibt es eine Spalte, in der angegeben ist, ob die Anforderung im Hauptcluster oder im Cluster mit paralleler Skalierung ausgeführt wurde: <br><br><img src="https://habrastorage.org/webt/p-/sw/p8/p-swp8elamy2ecxarcydioa0_ac.png"><br><br>  Unabhängig davon, ob eine bestimmte Anforderung im Hauptcluster oder über einen Cluster mit paralleler Skalierung ausgeführt wurde, wird sie in stl_query.concurrency_scaling_status gespeichert. <br><br><img src="https://habrastorage.org/webt/kf/ns/_4/kfns_4r21bi4fbmqzpk2nfgv1ba.png"><br><br>  Der Wert 1 gibt an, dass die Anforderung in einem Cluster mit paralleler Skalierung ausgeführt wurde, während andere Werte angeben, dass sie im Hauptcluster ausgeführt wurde. <br><br>  Ein Beispiel: <br><br><img src="https://habrastorage.org/webt/cn/3u/vh/cn3uvh7rn90c8bydqicyz3zlir8.png"><br><br>  Informationen zur parallelen Skalierung werden auch in einigen anderen Tabellen und Ansichten gespeichert, z. B. SVCS_CONCURRENCY_SCALING_USAGE.  Darüber hinaus gibt es eine Reihe von Katalogtabellen, in denen Informationen zur parallelen Skalierung gespeichert sind. <br><br><h3>  Ergebnisse </h3><br>  Die Autoren haben am 29.03.19 um ca. 18:30 Uhr GMT eine parallele Skalierung für eine Warteschlange im internen Cluster gestartet. Sie haben den Parameter max_concurrency_scaling_clusters um ca. 20:30 Uhr am 29. März 2019 in 3 geändert. <br><br>  So simulieren Sie die Anforderungswarteschlange und reduzieren die Anzahl der Steckplätze für diese Warteschlange von 15 auf 5. <br><br>  Das folgende Dashboard-Diagramm von intermix.io zeigt die Anzahl der Anforderungen, die ausgeführt und in die Warteschlange gestellt werden, nachdem die Anzahl der Slots verringert wurde. <br><br><img src="https://habrastorage.org/webt/d4/0p/7q/d40p7qzi5ty79bbw_irhtk4qfzs.png"><br><br>  Wir sehen, dass sich die Wartezeit für Anforderungen in der Warteschlange erhöht hat, während die maximale Zeit mehr als 5 Minuten beträgt. <br><br><img src="https://habrastorage.org/webt/ov/xp/rn/ovxprnqxnz1fy9ds4xm4vmn6vte.png"><br><br>  Hier sind die relevanten Informationen von der AWS-Konsole darüber, was während dieser Zeit passiert ist: <br><br><img src="https://habrastorage.org/webt/rm/f-/ip/rmf-ipbke1vkled8fyo8zch9jpu.png"><br><br>  Redshift hat drei (3) parallele Skalierungscluster wie konfiguriert gestartet.  Es scheint, dass diese Cluster nicht vollständig genutzt wurden, obwohl viele der Anforderungen in unserem Cluster in der Warteschlange standen. <br><br>  Das Verwendungsdiagramm korreliert mit dem Skalierungsaktivitätsdiagramm: <br><br><img src="https://habrastorage.org/webt/am/ho/yk/amhoykwqbkcttpfdiv5gvizkvog.png"><br><br>  Nach einigen Stunden überprüften die Autoren die Warteschlange, und es scheint, dass 6 Anforderungen mit paralleler Skalierung ausgeführt wurden.  Wir haben auch zwei Anfragen selektiv über die Benutzeroberfläche geprüft.  Sie haben nicht überprüft, wie diese Werte verwendet werden sollen, wenn mehrere parallele Cluster gleichzeitig aktiv sind. <br><br><img src="https://habrastorage.org/webt/fi/2f/rb/fi2frbpdkxqfcj1q-4rp12wrjz4.png"><br><br><h3>  Schlussfolgerungen </h3><br>  Parallele Skalierung kann die Warteschlangenzeit von Anforderungen während Spitzenlasten reduzieren. <br><br>  Nach den Ergebnissen des Basistests stellte sich heraus, dass sich die Situation mit Ladeanforderungen teilweise verbessert hat.  Die parallele Skalierung allein löste jedoch nicht alle Probleme mit der Parallelität. <br><br>  Dies ist auf Einschränkungen bei den Abfragetypen zurückzuführen, die die parallele Skalierung verwenden können.  Zum Beispiel haben Autoren viele Tabellen mit verschachtelten Sortierschlüsseln, und der größte Teil unserer Arbeitslast ist ein Datensatz. <br><br>  Obwohl die parallele Skalierung in der WLM-Konfiguration keine universelle Lösung darstellt, ist die Verwendung dieser Funktion in jedem Fall einfach und verständlich. <br><br>  Daher empfiehlt der Autor, es für Ihre WLM-Warteschlangen zu verwenden.  Beginnen Sie mit einem einzelnen parallelen Cluster und überwachen Sie die Spitzenlast über die Konsole, um festzustellen, ob die neuen Cluster vollständig ausgelastet sind. <br><br>  Da AWS zusätzliche Arten von Abfragen und Tabellen unterstützt, sollte die parallele Skalierung schrittweise effizienter werden. <br><blockquote>  <b>Kommentar von Belkhodzhaev Daniyar, einem Ingenieur nach Skyeng</b> <br><br>  Wir bei Skyeng haben auch sofort auf die sich abzeichnende Möglichkeit der parallelen Skalierung hingewiesen. <br>  Die Funktionalität ist sehr attraktiv, insbesondere angesichts der Tatsache, dass laut AWS die meisten Benutzer nicht einmal extra dafür bezahlen müssen. <br><br>  So kam es, dass wir Mitte April eine ungewöhnliche Flut von Anfragen an den Redshift-Cluster hatten.  In dieser Zeit haben wir häufig auf die Parallelitätsskalierung zurückgegriffen. Manchmal arbeitete der zusätzliche Cluster 24 Stunden am Tag, ohne anzuhalten. <br><br>  Dies ermöglichte es, die Situation akzeptabel zu machen, wenn sie das Warteschlangenproblem nicht vollständig löste. <br><br>  Unsere Beobachtungen stimmen weitgehend mit dem Eindruck der Jungs von intermix.io überein. <br><br>  Wir haben auch festgestellt, dass trotz des Vorhandenseins ausstehender Anforderungen in der Warteschlange nicht alle Anforderungen sofort an einen parallelen Cluster umgeleitet wurden.  Anscheinend liegt dies daran, dass der parallele Cluster noch einige Zeit zum Starten benötigt.  Infolgedessen haben wir bei kurzfristigen Spitzenlasten immer noch kleine Warteschlangen, und die entsprechenden Alarme haben Zeit zum Arbeiten. <br><br>  Nachdem wir die abnormalen Belastungen im April beseitigt hatten, gingen wir, wie AWS erwartet hatte, in den episodischen Nutzungsmodus - als Teil der freien Norm. <br>  Sie können die gleichzeitigen Skalierungskosten in AWS Cost Explorer verfolgen.  Sie müssen Service - Rotverschiebung, Verwendungstyp - CS auswählen, z. B. USW2-CS: dc2.large. <br><br>  Details zu den Preisen in russischer Sprache finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier.</a> </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de451538/">https://habr.com/ru/post/de451538/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de451522/index.html">Wir stellen die Automatisierung in wenigen Stunden bereit: TypeScript, Protractor, Jasmine</a></li>
<li><a href="../de451524/index.html">Die Geschichte, wie der Autoplay Media Studio 8.5.3.0-Wasserkocher kaputt ging</a></li>
<li><a href="../de451528/index.html">"Und so geht es": Cloud-Anbieter sind sich nicht über personenbezogene Daten einig</a></li>
<li><a href="../de451532/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 459 (30.04.2019 - 05.06.2019)</a></li>
<li><a href="../de451534/index.html">12 Prinzipien der Animation bei der Entwicklung von Videospielen</a></li>
<li><a href="../de451540/index.html">Wie viele Entwickler müssen einen Service wie Airbnb erstellen?</a></li>
<li><a href="../de451542/index.html">Wie und warum haben wir die Meilensteinerkennung in Mail.ru Cloud durchgeführt?</a></li>
<li><a href="../de451552/index.html">Wir bauen Netzwerkvertriebskanäle des Gadgets DO-RA auf</a></li>
<li><a href="../de451556/index.html">Flattern: Anwendungslokalisierung mit Android Studio</a></li>
<li><a href="../de451558/index.html">Ein Tag im Leben der QS-Automatisierung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>