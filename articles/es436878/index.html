<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé´ ü§öüèΩ üë®üèº‚Äçüç≥ BERT es un modelo de lenguaje de vanguardia para 104 idiomas. Tutorial para lanzar BERT localmente y en Google Colab üë®‚Äçüë®‚Äçüëß‚Äçüëß üöê üîº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERT es una red neuronal de Google, que mostr√≥ por un amplio margen resultados de vanguardia en una serie de tareas. Con BERT, puede crear programas d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>BERT es un modelo de lenguaje de vanguardia para 104 idiomas. Tutorial para lanzar BERT localmente y en Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="imagen"></p><br><p>  BERT es una red neuronal de Google, que mostr√≥ por un amplio margen resultados de vanguardia en una serie de tareas.  Con BERT, puede crear programas de inteligencia artificial para procesar un lenguaje natural: responda preguntas formuladas de cualquier forma, cree bots de chat, traductores autom√°ticos, analice texto, etc. </p><br><p>  Google ha publicado modelos BERT previamente entrenados, pero como suele ser el caso con Machine Learning, carecen de documentaci√≥n.  Por lo tanto, en este tutorial aprenderemos c√≥mo ejecutar la red neuronal BERT en la computadora local, as√≠ como en la GPU del servidor gratuito en Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  ¬øPor qu√© es necesario? </h2><br><p>  Para enviar texto a la entrada de una red neuronal, debe presentarlo de alguna manera en forma de n√∫meros.  Es m√°s f√°cil hacer esta letra por letra, aplicando una letra a cada entrada de la red neuronal.  Luego, cada letra se codificar√° con un n√∫mero del 0 al 32 (m√°s alg√∫n tipo de margen para los signos de puntuaci√≥n).  Este es el llamado nivel de personaje. </p><br><p>  Pero se obtienen resultados mucho mejores si presentamos propuestas no por una letra, sino al enviar a cada entrada de la red neuronal inmediatamente una palabra completa (o al menos s√≠labas).  Ya ser√° un nivel de palabra.  La opci√≥n m√°s f√°cil es compilar un diccionario con todas las palabras existentes y alimentar a la red el n√∫mero de palabras en este diccionario.  Por ejemplo, si la palabra "perro" est√° en este diccionario en el lugar 1678, entonces ingresamos el n√∫mero 1678 para la entrada de la red neuronal para esta palabra. </p><br><p>  Pero solo en un lenguaje natural, con la palabra "perro", surgen muchas asociaciones a la vez en una persona: "esponjoso", "malvado", "amigo de una persona".  ¬øEs posible codificar de alguna manera esta caracter√≠stica de nuestro pensamiento en la presentaci√≥n de la red neuronal?  Resulta que puedes.  Para hacer esto, es suficiente reordenar los n√∫meros de palabras para que las palabras que tienen un significado cercano est√©n una al lado de la otra.  Sea, por ejemplo, para "perro" el n√∫mero 1678, y para la palabra "esponjoso" el n√∫mero 1680. Y para la palabra "tetera" el n√∫mero es 9000. Como puede ver, los n√∫meros 1678 y 1680 est√°n mucho m√°s cerca entre s√≠ que el n√∫mero 9000. </p><br><p>  En la pr√°ctica, a cada palabra se le asigna no un n√∫mero, sino varios: un vector, digamos, de 32 n√∫meros.  Y las distancias se miden como las distancias entre los puntos a los que apuntan estos vectores en el espacio de la dimensi√≥n correspondiente (para un vector de 32 d√≠gitos de largo, este es un espacio con 32 dimensiones o con 32 ejes).  Esto le permite comparar una palabra a la vez con varias palabras que tienen un significado cercano (dependiendo de qu√© eje contar).  Adem√°s, las operaciones aritm√©ticas se pueden realizar con vectores.  Un ejemplo cl√°sico: si resta el vector "hombre" del vector que denota la palabra "rey" y agrega el vector para la palabra "mujer", obtendr√° un determinado vector de resultado.  Y √©l corresponder√° milagrosamente a la palabra "reina".  Y de hecho, "rey es hombre + mujer = reina".  La magia!  Y este no es un ejemplo abstracto, pero <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">realmente sucede</a> .  Teniendo en cuenta que las redes neuronales est√°n bien adaptadas para las transformaciones matem√°ticas sobre sus entradas, esto aparentemente proporciona una eficacia tan alta de este m√©todo. </p><br><p>  Este enfoque se llama incrustaciones.  Todos los paquetes de aprendizaje autom√°tico (TensorFlow, PyTorch) permiten que la primera capa de la red neuronal coloque una capa especial de Capa de incrustaci√≥n, que lo hace autom√°ticamente.  Es decir, en la entrada de la red neuronal, enviamos el n√∫mero de palabra habitual en el diccionario, y Embedded Layer, el autoaprendizaje, traduce cada palabra en un vector de la longitud especificada, digamos, 32 n√∫meros. </p><br><p>  Pero r√°pidamente se dieron cuenta de que es mucho m√°s rentable entrenar previamente una representaci√≥n de palabras de este tipo en un corpus de textos enorme, por ejemplo, en toda Wikipedia, y usar vectores de palabras ya preparados en redes neuronales espec√≠ficas en lugar de entrenarlos nuevamente. </p><br><p>  Hay varias formas de representar palabras como vectores; evolucionaron gradualmente: word2vec, GloVe, Elmo. </p><br><p>  En el verano de 2018, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI not√≥</a> que si pre-entrena una red neuronal en la arquitectura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformer</a> en grandes vol√∫menes de texto, inesperadamente y por un amplio margen muestra excelentes resultados en muchos tipos diferentes de tareas de procesamiento de lenguaje natural.  De hecho, dicha red neuronal en su salida crea representaciones vectoriales de palabras e incluso frases completas.  Y al colgar encima de un modelo de lenguaje de este tipo un peque√±o bloque de un par de capas adicionales de neuronas, puede entrenar esta red neuronal para cualquier tarea. </p><br><p>  BERT de Google es una red avanzada de GPA de OpenAI (bidireccional en lugar de unidireccional, etc.), tambi√©n basada en la arquitectura Transformer.  Por el momento, BERT es lo √∫ltimo en casi todos los puntos de referencia populares de PNL. </p><br><h2 id="kak-oni-eto-sdelali">  Como lo hicieron </h2><br><p>  La idea detr√°s de BERT es muy simple: alimentemos la red neuronal con frases en las que reemplazamos el 15% de las palabras con [M√ÅSCARA] y entrenemos la red neuronal para predecir estas palabras enmascaradas. </p><br><p>  Por ejemplo, si enviamos la frase "Vine a [MASK] y compr√© [MASK]" a la entrada de la red neuronal, deber√≠a mostrar las palabras "store" y "milk" en la salida.  Este es un ejemplo simplificado de la p√°gina oficial BERT; en oraciones m√°s largas, el rango de opciones posibles se vuelve m√°s peque√±o, y la respuesta de la red neuronal es inequ√≠voca. </p><br><p>  Y para que la red neuronal aprenda a comprender la relaci√≥n entre diferentes oraciones, tambi√©n la entrenaremos para predecir si la segunda frase es una continuaci√≥n l√≥gica de la primera.  ¬øO es una frase aleatoria que no tiene nada que ver con la primera. </p><br><p>  Entonces, para dos oraciones: "Fui a la tienda".  y "Y compr√© leche all√≠", la red neuronal deber√≠a responder que esto es l√≥gico.  Y si la segunda frase es "Crucian sky Pluto", entonces debo responder que esta propuesta no tiene nada que ver con la primera.  Vamos a jugar con estos dos modos BERT a continuaci√≥n. </p><br><p>  Habiendo entrenado as√≠ la red neuronal en el cuerpo de textos de Wikipedia y la colecci√≥n de libros BookCorpus durante 4 d√≠as a 16 TPU, obtuvimos BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Instalaci√≥n y configuraci√≥n </h2><br><p>  <em><strong>Nota</strong> : en esta secci√≥n iniciaremos y jugaremos con BERT en la computadora local.</em>  <em>Para ejecutar esta red neuronal en una GPU local, necesitar√° una NVidia GTX 970 con 4 GB de memoria de video o superior.</em>  <em>Si solo desea ejecutar BERT en un navegador (ni siquiera necesita una GPU en su computadora para esto), vaya a la secci√≥n Google Colab.</em> </p><br><p>  Primero instale TensorFlow, si a√∫n no lo tiene, siguiendo las instrucciones de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://www.tensorflow.org/install</a> .  Para admitir la GPU, primero debe instalar CUDA Toolkit 9.0, luego cuDNN SDK 7.2, y solo luego TensorFlow con soporte de GPU: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  B√°sicamente, esto es suficiente para ejecutar BERT.  Pero no hay instrucciones como tales, puede componerlo usted mismo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ordenando</a> las fuentes en el archivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">run_classifier.py</a> (la situaci√≥n habitual en Machine Learning es cuando tiene que ir a las fuentes en lugar de la documentaci√≥n).  Pero lo haremos m√°s f√°cil y usaremos el shell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Keras BERT</a> (tambi√©n puede ser √∫til para ajustar la red m√°s adelante, ya que proporciona una interfaz Keras conveniente). </p><br><p>  Para hacer esto, instale Keras: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  Y despu√©s de Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Tambi√©n necesitaremos el archivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tokenization.py</a> del github BERT original.  Haga clic en el bot√≥n Sin procesar y gu√°rdelo en la carpeta con el script futuro, o descargue todo el repositorio y tome el archivo desde all√≠, o tome una copia del repositorio con este c√≥digo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Ahora es el momento de descargar la red neuronal pre-entrenada.  Hay varias opciones para BERT, todas las cuales se enumeran en la p√°gina oficial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">github.com/google-research/bert</a> .  Tomaremos la "Base BERT universal multiling√ºe", con funda multiling√ºe, para 104 idiomas.  Descargue el archivo <a href="">multi_cased_L-12_H-768_A-12.zip</a> (632 Mb) y descompr√≠malo en la carpeta con el script futuro. </p><br><p>  Todo est√° listo, cree el archivo BERT.py, luego habr√° un poco de c√≥digo. </p><br><p>  Importar bibliotecas necesarias y establecer rutas </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Como tendremos que traducir l√≠neas de texto ordinarias a un formato especial de tokens, crearemos un objeto especial para esto.  Presta atenci√≥n a do_lower_case = False, ya que estamos usando el modelo Cased BERT, que distingue entre may√∫sculas y min√∫sculas. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Modelo de carga </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT puede funcionar en dos modos: adivinar las palabras perdidas en la frase o adivinar si la segunda frase es l√≥gica despu√©s de la primera.  Haremos ambas opciones. </p><br><p>  Para el primer modo, debe enviar una frase en el formato: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  La red neuronal deber√≠a devolver una oraci√≥n completa con las palabras completadas en lugar de las m√°scaras: "Vine a la tienda y compr√© leche". </p><br><p>  Para el segundo modo, ambas frases separadas por un separador deben alimentarse a la entrada de la red neuronal: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  La red neuronal debe responder si la segunda frase es una continuaci√≥n l√≥gica de la primera.  ¬øO es una frase aleatoria que no tiene nada que ver con la primera. </p><br><p>  Para que BERT funcione, debe preparar tres vectores, cada uno con una longitud de 512 n√∫meros: token_input, seg_input y mask_input. </p><br><p>  <strong>Token_input</strong> almacenar√° nuestro c√≥digo fuente traducido a tokens usando tokenizer.  La frase en forma de √≠ndices en el diccionario estar√° al comienzo de este vector, y el resto estar√° lleno de ceros. </p><br><p>  En <strong>mask_input,</strong> debemos poner 1 para todas las posiciones donde est√° la m√°scara [MASK], y completar el resto con ceros. </p><br><p>  En <strong>seg_input,</strong> debemos <strong>denotar la</strong> primera frase (incluyendo el CLS inicial y el separador SEP) como 0, la segunda frase (incluyendo el SEP final) como 1, y completar el resto hasta el final del vector con ceros. </p><br><p>  BERT no usa un diccionario de palabras completas, sino de las s√≠labas m√°s comunes.  Aunque tambi√©n tiene palabras completas.  Puede abrir el archivo vocab.txt en la red neuronal descargada y ver qu√© palabras usa la red neuronal en su entrada.  Hay palabras enteras como Francia.  Pero la mayor√≠a de las palabras rusas deben dividirse en s√≠labas.  Por lo tanto, la palabra "vino" debe dividirse en "con" y "## fue".  Para ayudar a convertir l√≠neas de texto regulares al formato requerido por BERT, utilizamos el m√≥dulo tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Modo 1: Predicci√≥n de palabras cerradas por token [M√ÅSCARA] en una frase </h2><br><p>  La frase de entrada que se alimenta a la entrada de la red neuronal </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Convi√©rtelo en tokens.  El problema es que el tokenizer no puede procesar marcas de servicio como [CLS] y [MASK], aunque est√°n en vocab.txt.  Por lo tanto, tendremos que romper manualmente nuestra l√≠nea con marcadores [M√ÅSCARA] y extraer fragmentos de texto sin formato para convertirla en tokens BERT utilizando el tokenizador.  Agregue tambi√©n [CLS] al principio y [SEP] al final de la frase. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Los tokens ahora tienen tokens que est√°n garantizados para convertirse en √≠ndices en el diccionario.  Hag√°moslo: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Ahora en token_input hay una serie de n√∫meros (n√∫meros de palabras en el diccionario vocab.txt) que se deben alimentar a la entrada de la red neuronal.  Solo queda extender este vector a una longitud de 512 elementos.  La construcci√≥n Python [0] * length crea una matriz de longitud, llena de ceros.  Simplemente agr√©guelo a nuestros tokens, que en Python combina dos matrices en una. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Ahora cree una m√°scara m√°scara de 512 de longitud, poniendo 1 en todas partes, donde el n√∫mero 103 aparece en los tokens (que corresponde al marcador [MASK] en el diccionario vocab.txt), y llenando el resto con 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Para el primer modo de operaci√≥n BERT, seg_input debe estar completamente lleno de ceros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  El √∫ltimo paso, debe convertir las matrices de python en matrices numpy con forma (1,512), para lo cual las colocamos en una submatriz []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  OK, listo  ¬°Ahora ejecute la predicci√≥n de la red neuronal! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Ahora formatee el resultado de los tokens nuevamente en una cadena separada por espacios </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  Y muestra el resultado: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  En nuestro ejemplo, para la frase "Vine a [MASK] y compr√© [MASK]".  la red neuronal produjo el resultado "casa" y "eso": "Vine a la casa y la compr√©".  Bueno, no tan mal por primera vez.  Comprar una casa es definitivamente mejor que la leche). </p><br><div class="spoiler">  <b class="spoiler_title">Otros ejemplos (no doy los que no tuvieron √©xito, hay mucho m√°s que los exitosos. En la mayor√≠a de los casos, la red da una respuesta vac√≠a):</b> <div class="spoiler_text"><p>  La Tierra es la tercera [M√ÅSCARA] del Sol <br>  Resultado: estrella </p><br><p>  mejor sandwich [M√ÅSCARA] con mantequilla <br>  Resultado: Cumple </p><br><p>  despu√©s del almuerzo [MASK] se supone que debe dormir <br>  Resultado: de esto </p><br><p>  alejarse de [M√ÅSCARA] <br>  Resultado: ## oh - ¬øes alg√∫n tipo de maldici√≥n?  ) </p><br><p>  [M√ÅSCARA] desde la puerta <br>  Resultado: ver </p><br><p>  Con [MASK] martillo y clavos pueden hacer gabinete <br>  Resultado: ayuda </p><br><p>  ¬øY si ma√±ana no es as√≠?  ¬°Hoy, por ejemplo, no es [M√ÅSCARA]! <br>  Resultado: ser√° </p><br><p>  ¬øC√≥mo te cansas de ignorar [M√ÅSCARA]? <br>  Resultado: ella </p><br><p>  Hay l√≥gica cotidiana, hay l√≥gica femenina, pero no se sabe nada sobre el hombre [M√ÅSCARA] <br>  Resultado: filosof√≠a </p><br><p>  En las mujeres, a la edad de treinta a√±os, se forma una imagen del pr√≠ncipe, que se adapta a cualquier [M√ÅSCARA]. <br>  Resultado: hombre </p><br><p>  Por mayor√≠a de votos, Blancanieves y los siete enanos votaron a favor de [M√ÅSCARA], con un voto en contra. <br>  Resultado: pueblo - la primera letra es correcta </p><br><p>  Califique su tedio en una escala de 10 puntos: puntos [M√ÅSCARA] <br>  Resultado: 10 </p><br><p>  ¬°Su [M√ÅSCARA], [M√ÅSCARA] y [M√ÅSCARA]! <br>  Resultado: √°mame, no, BERT, no quise decir nada </p></div></div><br><p>  Puede ingresar frases en ingl√©s (y cualquiera en 104 idiomas, una lista de los cuales <a href="">est√° aqu√≠</a> ) </p><br><p>  [M√ÅSCARA] debe continuar! <br>  Resultado: yo </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Modo 2: verificar la consistencia de dos frases </h2><br><p>  Establecemos dos frases consecutivas que ser√°n alimentadas a la entrada de la red neuronal </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Crearemos tokens en el formato [CLS] frase_1 [SEP] frase_2 [SEP], convirtiendo texto plano en tokens usando el tokenizador: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Convertimos tokens de cadena en √≠ndices num√©ricos (n√∫meros de palabras en el diccionario vocab.txt) y ampliamos el vector a 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  La palabra m√°scara en este caso est√° completamente llena de ceros </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Pero la m√°scara de la propuesta debe completarse bajo la segunda frase (incluido el SEP final) con unidades, y todo lo dem√°s con ceros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Pasamos las frases a trav√©s de la red neuronal (esta vez el resultado est√° en [1], y no en [0], como estaba arriba) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  Y derivamos la probabilidad de que la segunda frase sea un conjunto de palabras normal y no aleatorio </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  En dos frases: </p><br><p>  Yo vine a la tienda.  -&gt; Y compr√≥ leche. </p><br><p>  Respuesta de red neuronal: </p><br><p>  La oraci√≥n es okey: 99% </p><br><p>  Y si la segunda frase es "Crucian sky Pluto", entonces la respuesta ser√°: </p><br><p>  La oraci√≥n es okey: 4% </p><br><h2 id="google-colab">  Google colab </h2><br><p>  Google proporciona una GPU de servidor Tesla K80 gratuita con 12 Gb de memoria de video (las TPU ahora est√°n disponibles, pero su configuraci√≥n es un poco m√°s complicada).  Todo el c√≥digo para Colab debe dise√±arse como un cuaderno jupyter.  Para iniciar BERT en un navegador, solo abra el enlace </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  En el men√∫ <strong>Tiempo de ejecuci√≥n</strong> , seleccione <strong>Ejecutar todo</strong> , de modo que por primera vez se inicien todas las celdas, el modelo se descargue y se conecten las bibliotecas necesarias.  Acepte restablecer todo el tiempo de ejecuci√≥n si es necesario. </p><br><div class="spoiler">  <b class="spoiler_title">Si algo sali√≥ mal ...</b> <div class="spoiler_text"><p>  Aseg√∫rese de que GPU y Python 3 est√©n seleccionados en el men√∫ Tiempo de ejecuci√≥n -&gt; Cambiar tipo de tiempo de ejecuci√≥n </p><br><p>  Si el bot√≥n de conexi√≥n no est√° activo, haga clic para conectarse. </p></div></div><br><p>  Ahora cambie las l√≠neas de entrada <strong>oraci√≥n</strong> , <strong>oraci√≥n_1</strong> y <strong>oraci√≥n_2</strong> , y haga clic en el icono Reproducir a la izquierda para comenzar solo la celda actual.  Ejecutar todo el port√°til ya no es necesario. </p><br><p>  Puede ejecutar BERT en Google Colab incluso desde un tel√©fono inteligente, pero si no se abre, es posible que deba habilitar la casilla de verificaci√≥n Versi√≥n completa en la configuraci√≥n de su navegador. </p><br><h2 id="chto-dalshe">  Que sigue </h2><br><p>  Para entrenar BERT para una tarea espec√≠fica, debe agregar una o dos capas de una red de Feed Forward simple encima y solo entrenarla sin tocar la red BERT principal.  Esto se puede hacer en TensorFlow desnudo o a trav√©s del shell Keras BERT.  Dicha capacitaci√≥n adicional para un dominio espec√≠fico ocurre muy r√°pidamente y es completamente similar al Ajuste fino en redes de convoluci√≥n.  Entonces, para la tarea SQuAD, puede entrenar una red neuronal en un TPU en solo 30 minutos (en comparaci√≥n con 4 d√≠as en 16 TPU para entrenar el BERT). </p><br><p>  Para hacer esto, tendr√° que estudiar c√≥mo se representan las √∫ltimas capas en BERT, as√≠ como tener un conjunto de datos adecuado.  En la p√°gina oficial de BERT <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/google-research/bert</a> hay varios ejemplos para diferentes tareas, as√≠ como instrucciones sobre c√≥mo comenzar a readaptarse en TPU en la nube.  Y todo lo dem√°s tendr√° que buscar en la fuente en los archivos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">run_classifier.py</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  El c√≥digo y el cuaderno jupyter para Google Colab presentados aqu√≠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>est√°n alojados en el repositorio</strong></a> . </p><br><p>  No se deben esperar milagros.  No esperes que BERT hable como una persona.  El estado del estado de la t√©cnica no significa en absoluto que el progreso en la PNL haya alcanzado un nivel aceptable.  Simplemente significa que BERT es mejor que los modelos anteriores, que eran a√∫n peores.  La IA de conversaci√≥n fuerte todav√≠a est√° muy lejos.  Adem√°s, BERT es principalmente un modelo de lenguaje, no un bot de chat listo, por lo que muestra buenos resultados solo despu√©s de volver a capacitarse para una tarea espec√≠fica. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es436878/">https://habr.com/ru/post/es436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es436866/index.html">C√≥mo reconocer proyectos √°giles falsos</a></li>
<li><a href="../es436868/index.html">Incruste el an√°lisis est√°tico en el proceso, no busque errores con √©l.</a></li>
<li><a href="../es436872/index.html">PGConf.Russia 2019 Pr√≥ximamente</a></li>
<li><a href="../es436874/index.html">Bailes de A√±o Nuevo alrededor del adaptador FC o una historia sobre cu√°n lejos est√°n las causas del problema de los s√≠ntomas</a></li>
<li><a href="../es436876/index.html">[SAP] SAPUI5 para dummies parte 1: un ejercicio completo paso a paso</a></li>
<li><a href="../es436880/index.html">Conceptos b√°sicos de la plantilla de C ++: plantillas de funciones</a></li>
<li><a href="../es436884/index.html">Dominamos async / wait en un ejemplo real</a></li>
<li><a href="../es436886/index.html">Usando Babel y Webpack para configurar un proyecto React desde cero</a></li>
<li><a href="../es436888/index.html">Historia sobre c√≥mo dise√±ar una API</a></li>
<li><a href="../es436890/index.html">Tutorial React Parte 10: Taller sobre c√≥mo trabajar con propiedades de componentes y estilo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>