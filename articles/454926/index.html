<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äç‚öïÔ∏è üë®üèΩ‚Äçüî¨ üë∞üèº Todo lo que sab√≠as sobre word2vec no es cierto üë©‚ÄçüöÄ ü§∑üèæ üö®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La explicaci√≥n cl√°sica de word2vec como una arquitectura Skip-gram de muestra negativa en el art√≠culo cient√≠fico original y en innumerables publicacio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Todo lo que sab√≠as sobre word2vec no es cierto</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454926/">  La explicaci√≥n cl√°sica de word2vec como una arquitectura Skip-gram de muestra negativa en el art√≠culo cient√≠fico original y en innumerables publicaciones de blog se ve as√≠: <br><br><pre><code class="plaintext hljs">while(1) { 1. vf = vector of focus word 2. vc = vector of focus word 3. train such that (vc . vf = 1) 4. for(0 &lt;= i &lt;= negative samples): vneg = vector of word *not* in context train such that (vf . vneg = 0) }</code> </pre> <br>  De hecho, si buscas en Google [word2vec skipgram], lo que vemos: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">P√°gina de Wikipedia que describe el algoritmo a un alto nivel</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">P√°gina de Tensorflow con la misma explicaci√≥n</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Hacia el blog Data Science con una descripci√≥n del mismo algoritmo</a> , y la lista contin√∫a. </li></ul><br>  <b>Pero todas estas implementaciones est√°n mal</b> . <br><a name="habracut"></a><br>  La implementaci√≥n original de word2vec en C funciona de manera diferente y es <i>fundamentalmente diferente</i> de esto.  Aquellos que implementan profesionalmente sistemas con incrustaciones de palabras de word2vec hacen uno de los siguientes: <br><br><ol><li>  Llame directamente a la implementaci√≥n original de C. <br></li><li>  Use la implementaci√≥n <code>gensim</code> , que se <i>transcribe</i> desde la fuente C en la medida en que coincidan los nombres de las variables. </li></ol><br>  De hecho, <code>gensim</code> es la <i>√∫nica implementaci√≥n verdadera de C que conozco</i> . <br><br><h3>  Implementaci√≥n C </h3><br>  La implementaci√≥n de C en realidad admite <i>dos vectores para cada palabra</i> .  Un vector para la palabra est√° en foco, y el segundo para la palabra en contexto.  (¬øParece familiar? Correcto, ¬°los desarrolladores de GloVe tomaron una idea de word2vec sin mencionar este hecho!) <br><br>  La implementaci√≥n en el c√≥digo C es excepcionalmente competente: <br><br><ul><li>  La matriz <code>syn0</code> contiene la incrustaci√≥n vectorial de la palabra si aparece como una palabra en foco.  Aqu√≠ hay una <b>inicializaci√≥n aleatoria</b> . <br><br><pre> <code class="cpp hljs">https:<span class="hljs-comment"><span class="hljs-comment">//github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L369 for (a = 0; a &lt; vocab_size; a++) for (b = 0; b &lt; layer1_size; b++) { next_random = next_random * (unsigned long long)25214903917 + 11; syn0[a * layer1_size + b] = (((next_random &amp; 0xFFFF) / (real)65536) - 0.5) / layer1_size; }</span></span></code> </pre> </li><li>  Otra matriz <code>syn1neg</code> contiene el vector de la palabra cuando aparece como una palabra de contexto.  Aqu√≠ la <b>inicializaci√≥n es cero</b> . <br></li><li>  Durante el entrenamiento (Skip-gram, muestra negativa, aunque otros casos son casi iguales), primero seleccionamos la palabra de enfoque.  Se mantiene durante toda la capacitaci√≥n sobre ejemplos positivos y negativos.  Los gradientes del vector de enfoque se acumulan en el b√∫fer y se aplican a la palabra de enfoque despu√©s del entrenamiento en ejemplos positivos y negativos. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (negative &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (d = <span class="hljs-number"><span class="hljs-number">0</span></span>; d &lt; negative + <span class="hljs-number"><span class="hljs-number">1</span></span>; d++) { <span class="hljs-comment"><span class="hljs-comment">// if we are performing negative sampling, in the 1st iteration, // pick a word from the context and set the dot product target to 1 if (d == 0) { target = word; label = 1; } else { // for all other iterations, pick a word randomly and set the dot //product target to 0 next_random = next_random * (unsigned long long)25214903917 + 11; target = table[(next_random &gt;&gt; 16) % table_size]; if (target == 0) target = next_random % (vocab_size - 1) + 1; if (target == word) continue; label = 0; } l2 = target * layer1_size; f = 0; // find dot product of original vector with negative sample vector // store in f for (c = 0; c &lt; layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2]; // set g = sigmoid(f) (roughly, the actual formula is slightly more complex) if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; // 1. update the vector syn1neg, // 2. DO NOT UPDATE syn0 // 3. STORE THE syn0 gradient in a temporary buffer neu1e for (c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2]; for (c = 0; c &lt; layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1]; } // Finally, after all samples, update syn1 from neu1e https://github.com/tmikolov/word2vec/blob/20c129af10659f7c50e86e3be406df663beff438/word2vec.c#L541 // Learn weights input -&gt; hidden for (c = 0; c &lt; layer1_size; c++) syn0[c + l1] += neu1e[c];</span></span></code> </pre> </li></ul><br><h3>  ¬øPor qu√© inicializaci√≥n aleatoria y cero? </h3><br>  Una vez m√°s, dado que esto no se explica en absoluto en los art√≠culos originales <i>y en cualquier lugar de Internet</i> , solo puedo especular. <br><br>  La hip√≥tesis es que cuando las muestras negativas provienen de todo el texto y no se ponderan por frecuencia, puede elegir <i>cualquier palabra</i> , y con mayor frecuencia una palabra cuyo <i>vector no est√© entrenado en absoluto</i> .  Si este vector tiene un significado, cambiar√° aleatoriamente la palabra realmente importante en foco. <br><br>  La conclusi√≥n es establecer todos los ejemplos negativos en cero, de modo que <i>solo los vectores que ocurran con mayor o menor frecuencia</i> afecten la presentaci√≥n de otro vector. <br><br>  Esto es realmente bastante complicado, y nunca hab√≠a pensado en lo importantes que son las estrategias de inicializaci√≥n. <br><br><h3>  ¬øPor qu√© estoy escribiendo esto? </h3><br>  Pas√© dos meses de mi vida tratando de reproducir word2vec como se describe en la publicaci√≥n cient√≠fica original e innumerables art√≠culos en Internet, pero fall√©.  No pude lograr los mismos resultados que word2vec, aunque hice mi mejor esfuerzo. <br><br>  No pod√≠a imaginar que los autores de la publicaci√≥n literalmente fabricaran un algoritmo que no funciona, mientras que la implementaci√≥n hace algo completamente diferente. <br><br>  Al final, decid√≠ estudiar la fuente.  Durante tres d√≠as estuve seguro de que entend√≠ mal el c√≥digo, ya que literalmente todos en Internet hablaron de una implementaci√≥n diferente. <br><br>  No tengo idea de por qu√© la publicaci√≥n original y los art√≠culos en Internet no dicen nada sobre el mecanismo <i>real</i> de word2vec, as√≠ que decid√≠ publicar esta informaci√≥n yo mismo. <br><br>  Esto tambi√©n explica la elecci√≥n radical de GloVe de establecer vectores separados para el contexto negativo: simplemente hicieron lo que hace word2vec, pero le dijeron a la gente al respecto :). <br><br>  ¬øEs este un truco cient√≠fico?  No s√©, una pregunta dif√≠cil.  Pero para ser honesto, estoy incre√≠blemente enojado.  Probablemente, nunca m√°s podr√© volver a tomarme en serio la explicaci√≥n de los algoritmos en el aprendizaje autom√°tico: la pr√≥xima vez ir√© <i>inmediatamente</i> a ver las fuentes. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/454926/">https://habr.com/ru/post/454926/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454916/index.html">Arquitectura de red neuronal para implementar el algoritmo RL con la capacidad de establecer acciones simult√°neas</a></li>
<li><a href="../454918/index.html">C√≥mo combinar los respaldos de dos minoristas en SAP en 12 horas</a></li>
<li><a href="../454920/index.html">Rendimiento de front-end: an√°lisis de m√©tricas importantes</a></li>
<li><a href="../454922/index.html">Historias sobre clientes extranjeros y sus caracter√≠sticas de trabajo en Rusia despu√©s de la Ley de DP</a></li>
<li><a href="../454924/index.html">Configuraci√≥n de autenticaci√≥n en Veeam Backup para Microsoft Office 365 v3</a></li>
<li><a href="../454930/index.html">Recolecci√≥n de basura en V8: como funciona el nuevo Orinoco GC</a></li>
<li><a href="../454932/index.html">Inversiones y software: 5 terminales comerciales para operar en bolsa</a></li>
<li><a href="../454936/index.html">Vivaldi: el bloqueo de anuncios debe ser la elecci√≥n del usuario</a></li>
<li><a href="../454938/index.html">Desarrollo de su propio n√∫cleo para incrustar en un sistema de procesador basado en FPGA</a></li>
<li><a href="../454940/index.html">Seguro m√©dico de viaje: instrucciones detalladas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>