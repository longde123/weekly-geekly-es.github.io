<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò∂ üö¥üèæ üë®üèæ‚Äçüè´ Neuronale Netze und tiefes Lernen: Ein Online-Tutorial, Kapitel 6, Teil 2: Aktuelle Fortschritte bei der Bilderkennung üßô ü•ï üòê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inhalt 

- Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen 
- Kapitel 2: Funktionsweise des Backpropagation-Algorithmus 
-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und tiefes Lernen: Ein Online-Tutorial, Kapitel 6, Teil 2: Aktuelle Fortschritte bei der Bilderkennung</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/464039/"><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li>
</ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  1998, als die MNIST-Datenbank erschien, dauerte es Wochen, um die fortschrittlichsten Computer zu trainieren, was viel schlechtere Ergebnisse erzielte als die heutigen Computer, die mit der GPU weniger als eine Stunde ben√∂tigen.  Daher ist MNIST keine Aufgabe mehr, die die Grenzen der Technologie √ºberschreitet.  Die Geschwindigkeit des Trainings legt nahe, dass diese Aufgabe f√ºr das Studium dieser Technologie gut geeignet ist.  In der Zwischenzeit geht die Forschung weiter und moderne Arbeiten untersuchen viel komplexere Probleme.  In diesem Abschnitt werde ich kurz einige Beispiele f√ºr laufende Arbeiten im Zusammenhang mit der Bilderkennung mithilfe neuronaler Netze beschreiben. <br><br>  Dieser Abschnitt unterscheidet sich vom Rest des Buches.  In dem Buch konzentrierte ich mich auf vermutlich langlebige Ideen - Backpropagation, Regularisierung, Faltungsnetzwerke.  Ich habe versucht, die Ergebnisse zu vermeiden, die zum Zeitpunkt des Schreibens als modisch galten und deren langfristiger Wert zweifelhaft schien.  In der Wissenschaft erweisen sich solche Ergebnisse meist als kurzlebig, verschwinden schnell und haben keine langfristige Wirkung.  Vor diesem Hintergrund w√ºrde der Skeptiker sagen: ‚ÄûNat√ºrlich k√∂nnen die j√ºngsten Fortschritte bei der Bilderkennung als Beispiel f√ºr eine solche eint√§gige Reise angesehen werden.  In zwei oder drei Jahren wird sich alles √§ndern.  Sind diese Ergebnisse wahrscheinlich f√ºr eine kleine Anzahl von Fachleuten von Interesse, die im Vordergrund stehen?  Warum √ºberhaupt dar√ºber diskutieren? " <br><a name="habracut"></a><br>  Ein solcher Skeptiker wird insofern Recht haben, als die kleinen Details der j√ºngsten Arbeiten allm√§hlich an wahrgenommener Bedeutung verlieren.  In den letzten Jahren gab es jedoch unglaubliche Verbesserungen bei der L√∂sung besonders komplexer Probleme der Bilderkennung unter Verwendung von tiefen neuronalen Netzen (GNS).  Stellen Sie sich einen Wissenschaftshistoriker vor, der 2100 Material √ºber Computer Vision schreibt.  Sie werden 2011-2015 (und wahrscheinlich einige Jahre danach) als eine Zeit bedeutender Durchbr√ºche definieren, die von Deep Convolution Networks (GSS) angetrieben werden.  Dies bedeutet nicht, dass das GOS auch im Jahr 2100 verwendet wird, ganz zu schweigen von Details wie Ausnahme, ReLU und mehr.  Dies bedeutet jedoch, dass es derzeit einen wichtigen √úbergang in der Ideengeschichte gibt.  Dies √§hnelt der Beobachtung der Entdeckung des Atoms, der Erfindung von Antibiotika: der Erfindung und Entdeckung historischer Proportionen.  Ohne auf Details einzugehen, lohnt es sich daher, sich ein Bild von den interessanten Entdeckungen zu machen, die heute gemacht werden. <br><br><h3>  Arbeit 2012 LRMD </h3><br>  Lassen Sie mich mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit von 2012 beginnen,</a> die von einer Gruppe von Forschern aus Stanford und Google verfasst wurde.  Ich werde sie LRMD nennen, mit den Anfangsbuchstaben der Namen der ersten vier Autoren.  LRMD verwendete NS, um Bilder aus der ImageNet-Datenbank zu klassifizieren, was eine sehr schwierige Aufgabe der Mustererkennung ist.  Die Daten, die sie aus dem ImageNet 2011 verwendeten, umfassten 16 Millionen Vollfarbbilder, die in 20.000 Kategorien unterteilt waren.  Die Bilder wurden aus dem Internet heruntergeladen und von Amazon Mechanical Turk klassifiziert.  Hier sind einige davon: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/323/b71/89a/323b7189a538cc4532b293b48f587cca.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bed/423/c27/bed423c2789cb88297bf8946f1e92e82.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/855/3e5/38d/8553e538d4514742d4ebf554eae7c490.jpg"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/326/bc0/f49/326bc0f4920430fb06490cab8bb8eeeb.jpg"><br><br>  Sie geh√∂ren jeweils zu den Kategorien: Mumps, brauner Wurzelpilz, pasteurisierte Milch, Spulw√ºrmer.  Wenn Sie √ºben m√∂chten, empfehle ich Ihnen, die Liste der Handwerkzeuge von ImagNet zu besuchen, in der Unterschiede zwischen H√ºgeln, Endhobeln, Hobeln zum Anfasen und Dutzenden anderer Arten von Hobeln gemacht werden, ganz zu schweigen von anderen Kategorien.  Ich wei√ü nichts √ºber Sie, aber ich kann all diese Tools nicht mit Sicherheit unterscheiden.  Dies ist offensichtlich weitaus schwieriger als MNIST!  Das LRMD-Netzwerk erzielte mit ImageNet ein anst√§ndiges Ergebnis bei einer Bilderkennungsgenauigkeit von 15,8%.  Dies scheint kein so beeindruckendes Ergebnis zu sein, aber es war eine enorme Verbesserung gegen√ºber dem vorherigen Ergebnis von 9,3%.  Ein solcher Sprung deutet darauf hin, dass NS einen effektiven Ansatz f√ºr sehr komplexe Bilderkennungsaufgaben wie ImageNet bieten k√∂nnen. <br><br><h3>  Arbeit 2012 KSH </h3><br>  Auf die Arbeit von LRMD im Jahr 2012 folgten die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeiten von</a> Krizhevsky, Sutskever und Hinton (KSH).  KSH trainierte und testete GSS unter Verwendung einer begrenzten Teilmenge von ImagNet-Daten.  Diese Untergruppe wird durch den beliebten Wettbewerb f√ºr maschinelles Lernen definiert - ImageNet Large-Scale Visual Recognition Challenge (ILSVRC).  Die Verwendung dieser Untergruppe gab ihnen eine bequeme M√∂glichkeit, ihren Ansatz mit anderen f√ºhrenden Techniken zu vergleichen.  Das ILSVRC 2012-Set enth√§lt ungef√§hr 1,2 Millionen Bilder aus 1000 Kategorien.  Die Verifizierungs- und Best√§tigungss√§tze enthalten 150.000 bzw. 50.000 Bilder aus denselben 1000 Kategorien. <br><br>  Eine der Herausforderungen des ILSVRC-Wettbewerbs besteht darin, dass viele Bilder aus ImageNet mehrere Objekte enthalten.  Im Bild l√§uft der Labrador Retriever beispielsweise einem Fu√üball nach.  T.N.  Die ‚Äûkorrekte‚Äú Klassifizierung von ILSVRC entspricht m√∂glicherweise dem Labrador Retriever-Label.  Ist es notwendig, Punkte aus dem Algorithmus auszuw√§hlen, wenn das Bild wie ein Fu√üball markiert wird?  Aufgrund dieser Mehrdeutigkeit wurde der Algorithmus als korrekt angesehen, wenn die ImageNet-Klassifizierung zu den 5 wahrscheinlichsten Vermutungen des Algorithmus hinsichtlich des Bildinhalts geh√∂rte.  Nach diesem Kriterium erreichte das GSS von KSH unter den Top 5 eine Genauigkeit von 84,7%, viel besser als der vorherige Gegner, der eine Genauigkeit von 73,8% erreichte.  Unter Verwendung einer strengeren Metrik erreichte die KSH-Genauigkeit 63,3%, wenn das Etikett genau der vorgeschriebenen entsprechen muss. <br><br>  Es lohnt sich, das KSH-Netzwerk kurz zu beschreiben, da es so viele Werke inspirierte, die folgten.  Wie wir sehen werden, ist es auch eng mit den Netzwerken verbunden, die wir in diesem Kapitel trainiert haben, obwohl es komplexer ist.  KSH verwendete GSS, das auf zwei GPUs trainiert wurde.  Sie verwendeten zwei GPUs, weil ihre spezielle Karte (NVIDIA GeForce GTX 580) nicht √ºber gen√ºgend Speicher verf√ºgte, um das gesamte Netzwerk zu speichern.  Daher teilen sie das Netzwerk in zwei Teile. <br><br>  Das KSH-Netzwerk besteht aus 7 Schichten versteckter Neuronen.  Die ersten f√ºnf verborgenen Schichten sind Faltungsschichten (einige verwenden maximales Pooling), und die n√§chsten zwei sind vollst√§ndig verbunden.  Die ausgegebene Softmax-Schicht besteht aus 1000 Neuronen, die 1000 Bildklassen entsprechen.  Hier ist eine Skizze des Netzwerks aus der Arbeit von KSH.  Details werden unten beschrieben.  Beachten Sie, dass viele Ebenen in zwei Teile unterteilt sind, die zwei GPUs entsprechen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/986/143/0bb/9861430bb7c8c79540298a2db5122ec2.jpg"><br><br>  In der Eingabeebene befindet sich ein 3x224x224-Neuron, das die RGB-Werte f√ºr ein Bild mit einer Gr√∂√üe von 224x224 angibt.  Denken Sie daran, dass ImageNet Bilder mit verschiedenen Aufl√∂sungen enth√§lt.  Dies stellt ein Problem dar, da die Eingangsnetzwerkschicht normalerweise eine feste Gr√∂√üe hat.  KSH hat dies behoben, indem jedes Bild so skaliert wurde, dass seine kurze Seite eine L√§nge von 256 Pixel hatte.  Dann schneiden sie einen Bereich von 256 x 256 Pixel aus der Mitte des verkleinerten Bildes aus.  Schlie√ülich ruft KSH zuf√§llige Bildst√ºcke (224 x 224) (und ihre horizontalen Reflexionen) aus 256 x 256 Bildern ab.  Dieser zuf√§llige Schnitt ist eine M√∂glichkeit, Trainingsdaten zu erweitern, um die Umschulung zu reduzieren.  Dies hilft insbesondere dabei, ein so gro√ües Netzwerk wie KSH zu trainieren.  Und schlie√ülich werden diese 224x224-Bilder als Eingabe in das Netzwerk verwendet.  In den meisten F√§llen enth√§lt das zugeschnittene Bild das Hauptobjekt des Originalbilds. <br><br>  Wir gelangen zu den verborgenen Schichten des KSH-Netzwerks.  Die erste verborgene Schicht ist eine Faltungsschicht mit einem maximalen Zugschritt.  Es werden lokale Empfangsfelder der Gr√∂√üe 11 x 11 und ein Schritt von 4 Pixeln verwendet.  Insgesamt werden 96 Feature-Karten erhalten.  Charakterkarten sind in zwei Gruppen zu je 48 Teilen unterteilt, wobei sich die ersten 48 Karten auf einer GPU und die zweite auf der anderen befinden.  Das maximale Pooling in dieser und den nachfolgenden Ebenen wird von 3 √ó 3-Abschnitten ausgef√ºhrt, aber die Pooling-Abschnitte k√∂nnen sich √ºberlappen und befinden sich in einem Abstand von nur 2 Pixeln voneinander. <br><br>  Die zweite verborgene Schicht ist ebenfalls Faltungsschicht mit maximalem Pooling.  Es verwendet lokale 5x5-Empfangsfelder und verf√ºgt √ºber 256 Funktionskarten, die f√ºr jede GPU in 128 Teile unterteilt sind.  Feature-Maps verwenden nur 48 eingehende Kan√§le und nicht wie √ºblich alle 96 Ausg√§nge der vorherigen Ebene.  Dies liegt daran, dass jede Funktionskarte Eingaben von der GPU empf√§ngt, auf der sie gespeichert ist.  In diesem Sinne entfernt sich das Netzwerk von der Faltungsarchitektur, die wir weiter oben in diesem Kapitel beschrieben haben, obwohl die Grundidee offensichtlich dieselbe bleibt. <br><br>  Die dritte, vierte und f√ºnfte Schicht sind Faltungsschichten, jedoch ohne maximale Poolbildung.  Ihre Parameter: (3) 384 Merkmalskarten, lokale Empfangsfelder 3x3, 256 eingehende Kan√§le;  (4) 384 Merkmalskarten, lokale Empfangsfelder 3 √ó 3, 192 eingehende Kan√§le;  (5) 256 Feature-Karten, lokale Empfangsfelder 3x3, 192 eingehende Kan√§le.  Auf der dritten Ebene werden Daten zwischen den GPUs ausgetauscht (wie in der Abbildung gezeigt), sodass die Feature-Maps alle 256 eingehenden Kan√§le verwenden k√∂nnen. <br><br>  Die sechste und siebte verborgene Schicht sind vollst√§ndig miteinander verbunden, jeweils 4096 Neuronen. <br><br>  Die Ausgabeschicht ist Softmax und besteht aus 1000 Einheiten. <br><br>  Das KSH-Netzwerk nutzt viele Techniken.  Anstatt Sigmoid oder hyperbolische Tangente als Aktivierungsfunktion zu verwenden, werden ReLUs verwendet, die das Lernen erheblich beschleunigen.  Das KSH-Netzwerk enth√§lt etwa 60 Millionen Trainingsparameter und muss daher auch bei einem gro√üen Satz von Trainingsdaten umgeschult werden.  Um dies zu bew√§ltigen, erweiterten die Autoren das Trainingsset durch zuf√§lliges Zuschneiden von Bildern, wie oben beschrieben.  Sie verwendeten dann die L2-Regularisierungsvariante und die Ausnahme.  Das Netzwerk wurde unter Verwendung eines stochastischen Gradientenabfalls basierend auf dem Impuls und mit Minipaketen trainiert. <br><br>  Dies ist ein kurzer √úberblick √ºber viele der wichtigsten Erkenntnisse von KSH.  Ich habe einige Details weggelassen und sie selbst im Artikel gesucht.  Sie k√∂nnen sich auch das Projekt von Alex Krizhevsky <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cuda-convnet</a> (und seinen Anh√§ngern) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ansehen</a> , das Code enth√§lt, der viele der beschriebenen Ideen umsetzt.  <a href="">Eine</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf Theano basierende</a> Version dieses Netzwerks <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wurde</a> ebenfalls <a href="">entwickelt</a> .  Sie k√∂nnen im Code Ideen erkennen, die denen √§hneln, die wir in diesem Kapitel entwickelt haben, obwohl die Verwendung mehrerer GPUs die Sache kompliziert macht.  Das Caffe-Framework verf√ºgt √ºber eine eigene Version des KSH-Netzwerks. Weitere Informationen finden Sie in den " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zoomodellen</a> ". <br><br><h3>  ILSVRC-Wettbewerb 2014 </h3><br>  Seit 2012 sind recht schnelle Fortschritte zu verzeichnen.  Nehmen Sie am ILSVRC-Wettbewerb 2014 teil.  Wie im Jahr 2012 mussten die Teilnehmer Netzwerke f√ºr 1,2 Millionen Bilder aus 1000 Kategorien trainieren, und eine der 5 wahrscheinlichen Vorhersagen in der richtigen Kategorie war ein Qualit√§tskriterium.  <a href="">Das Gewinnerteam</a> , das haupts√§chlich aus Google-Mitarbeitern bestand, verwendete das GSS mit 22 Schichten von Neuronen.  Sie benannten ihr Netzwerk GoogLeNet nach LeNet-5.  Nach dem Kriterium f√ºr das Erreichen der f√ºnf besten Optionen erreichte GoogLeNet einen Indikator mit einer Genauigkeit von 93,33%, was die Ergebnisse des Gewinners von 2013 (Clarifai von 88,3%) und des Gewinners von 2012 (KSH von 84,7%) erheblich verbesserte. <br><br>  Wie gut ist die Genauigkeit von GoogLeNet 93,33%?  Im Jahr 2014 schrieb ein Forschungsteam eine <a href="">√úberpr√ºfung des</a> ILSVRC-Wettbewerbs.  Eines der angesprochenen Themen war, wie gut die Menschen mit der Aufgabe umgehen k√∂nnen.  F√ºr das Experiment haben sie ein System erstellt, mit dem Personen Bilder mit ILSVRC klassifizieren k√∂nnen.  Wie einer der Autoren der Arbeit, Andrei Karpaty, in einem informativen Eintrag in seinem Blog erkl√§rt, war es sehr schwierig, die Effektivit√§t von Menschen auf GoogLeNet-Indikatoren zu bringen: <br><blockquote>  Die Aufgabe, Bilder mit f√ºnf von 1000 m√∂glichen Kategorien zu markieren, wurde selbst f√ºr diejenigen meiner Freunde im Labor, die seit einiger Zeit mit ILSVRC und seinen Kategorien arbeiten, sehr schwierig.  Zun√§chst wollten wir die Aufgabe bei Amazon Mechanical Turk einreichen.  Dann beschlossen wir, Studenten f√ºr Geld einzustellen.  Deshalb habe ich unter Experten in meinem Labor eine Markierungsparty organisiert.  Danach entwickelte ich eine modifizierte Schnittstelle, die GoogLeNet-Vorhersagen verwendete, um die Anzahl der Kategorien von 1000 auf 100 zu reduzieren. Trotzdem war die Aufgabe schwierig - die Leute √ºbersprangen Kategorien und gaben Fehler in der Gr√∂√üenordnung von 13-15% an.  Am Ende wurde mir klar, dass der effektivste Ansatz f√ºr mich darin besteht, mich hinzusetzen und einen unglaublich langen Lernprozess und den anschlie√üenden Prozess des gr√ºndlichen Markups zu durchlaufen, um dem GoogLeNet-Ergebnis noch n√§her zu kommen.  Die Markierung hatte zun√§chst eine Geschwindigkeit von etwa 1 Einheit pro Minute, wurde jedoch mit der Zeit beschleunigt.  Einige Bilder waren leicht zu erkennen, w√§hrend andere (z. B. bestimmte Hunderassen, Vogelarten oder Affen) mehrere Minuten Konzentration erforderten.  Ich konnte sehr gut zwischen Hunderassen unterscheiden.  Basierend auf meiner Bildprobe wurden die folgenden Ergebnisse erhalten: GoogLeNet war in 6,8% der F√§lle falsch;  Meine Fehlerquote betrug 5,1%, was ungef√§hr 1,7% besser war. </blockquote><br><br>  Mit anderen Worten, der Experte, der nur mit ernsthaften Anstrengungen sehr sorgf√§ltig arbeitete, konnte dem STS leicht einen Schritt voraus sein.  Karpaty berichtet, dass es dem zweiten Experten, der mit weniger Bildern geschult wurde, gelungen ist, den Fehler bei der Auswahl von bis zu 5 Etiketten pro Bild um nur 12% zu reduzieren, was viel weniger als bei GoogLeNet ist. <br><br>  Tolle Ergebnisse.  Und seit dem Aufkommen dieser Arbeit haben mehrere Teams √ºber die Entwicklung von Systemen berichtet, deren Fehlerrate bei der Auswahl der 5 besten Tags sogar unter 5,1% lag.  Manchmal wurden diese Errungenschaften in den Medien als die Entstehung von Systemen behandelt, die Bilder besser erkennen k√∂nnen als Menschen.  Obwohl die Ergebnisse im Allgemeinen bemerkenswert sind, gibt es viele Nuancen, die nicht ber√ºcksichtigt werden k√∂nnen, dass Computer Vision auf diesen Systemen besser funktioniert als auf Menschen.  In vielerlei Hinsicht ist der ILSVRC-Wettbewerb eine sehr begrenzte Aufgabe - die Ergebnisse einer Bildsuche in einem offenen Netzwerk entsprechen nicht unbedingt dem, was das Programm in einer praktischen Aufgabe erwartet.  Und nat√ºrlich ist das Kriterium ‚Äûeine der f√ºnf besten Noten‚Äú ziemlich k√ºnstlich.  Wir haben noch einen langen Weg vor uns, um das Problem der Bilderkennung zu l√∂sen, ganz zu schweigen von der allgemeineren Aufgabe des Computer Vision.  Trotzdem ist es sehr cool zu sehen, wie viel Fortschritt bei der L√∂sung einer so schwierigen Aufgabe in nur wenigen Jahren erzielt wurde. <br><br><h3>  Andere Aufgaben </h3><br>  Ich habe mich auf ImageNet konzentriert, es gibt jedoch einige andere Projekte, die NS zur Bilderkennung verwenden.  Lassen Sie mich kurz einige interessante Ergebnisse beschreiben, die k√ºrzlich erzielt wurden, um sich ein Bild von der modernen Arbeit zu machen. <br><br>  Eine inspirierende praktische <a href="">Reihe von Ergebnissen</a> wurde von einem Team von Google erzielt, das GSS auf die Aufgabe der Adressschilderkennung in Google Street View anwendete.  In ihrer Arbeit berichten sie, wie sie fast 100 Millionen Adressschilder mit einer Genauigkeit entdeckt und automatisch erkannt haben, die mit der menschlichen Arbeit vergleichbar ist.  Und ihr System ist schnell: Es konnte Daten von allen Google Street View-Bildern in Frankreich in weniger als einer Stunde entschl√ºsseln!  Sie schreiben: "Durch das Abrufen dieses neuen Datensatzes hat sich die Qualit√§t der Geokodierung von Google Maps in mehreren L√§ndern erheblich verbessert, insbesondere dort, wo es keine anderen Geokodierungsquellen gab."  Dann machen sie eine allgemeinere Aussage: ‚ÄûWir glauben, dass wir dank dieses Modells das Problem der optischen Erkennung kurzer Sequenzen auf eine Weise gel√∂st haben, die in vielen praktischen Anwendungen anwendbar ist.‚Äú <br><br>  Vielleicht habe ich den Eindruck einer Parade siegreicher und inspirierender Ergebnisse erweckt.  Die interessantesten Berichte betreffen nat√ºrlich grundlegende Dinge, die uns noch nicht klar sind.  Zum Beispiel wurde in der <a href="">Arbeit von 2013</a> gezeigt, dass die Nationalversammlung tats√§chlich blinde Flecken hat.  Schauen Sie sich die Bilder unten an.  Links ist das Bild aus ImageNet zu sehen, das das Forschernetzwerk korrekt klassifiziert hat.  Auf der rechten Seite befindet sich ein leicht modifiziertes Bild (in der Mitte sind die Unterschiede dargestellt), das das Netzwerk nicht mehr richtig erkennen konnte.  Und die Autoren fanden heraus, dass solche "kontroversen" √Ñnderungen f√ºr jedes Bild aus der Datenbank ausgew√§hlt werden k√∂nnen, und nicht nur f√ºr die Elite. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ddf/c77/f29/ddfc77f2996c1fb482fa448f1384d08a.jpg"><br><br>  Unangenehmes Ergebnis.  Wir haben ein Netzwerk verwendet, das auf demselben Code basiert wie das KSH-Netzwerk - das hei√üt, es ist ein solches Netzwerk, das immer h√§ufiger verwendet wird.  Und obwohl solche NS im Prinzip kontinuierliche Funktionen berechnen, legen √§hnliche Ergebnisse nahe, dass sie wahrscheinlich fast diskrete Funktionen berechnen.  Schlimmer noch, sie erweisen sich als diskret in einer Weise, die unsere intuitive Vorstellung von intelligentem Verhalten verletzt.  Das ist ein Problem.  Au√üerdem ist nicht sehr klar, was genau zu Diskretion f√ºhrt, was ist das Problem: in der Verlustfunktion?  Welche Aktivierungsfunktionen sind zu verwenden?  In der Netzwerkarchitektur?  In etwas anderem?  Wir wissen es nicht. <br><br>  Aber diese Ergebnisse sind nicht so schlecht, wie sie scheinen.  Obwohl solche kontroversen Ver√§nderungen recht h√§ufig sind, ist es unwahrscheinlich, dass sie in der Praxis gefunden werden.  Wie in der Arbeit angegeben: <br><blockquote>  Das Vorhandensein von kontroversen Negativen widerspricht der F√§higkeit des Netzwerks, eine hohe Generalisierbarkeit zu erreichen.  Wenn das Netzwerk gut verallgemeinern k√∂nnte, wie k√∂nnte es dann durch solche kontroversen Negative get√§uscht werden, die nicht von gew√∂hnlichen Beispielen zu unterscheiden sind?  Die Erkl√§rung ist, dass eine Reihe von Wettbewerbsnegativen eine extrem geringe Wahrscheinlichkeit hat und daher im Trainingsdatensatz nicht beobachtet (oder fast nicht beobachtet) wird, jedoch eine hohe Dichte aufweist (ungef√§hr wie rationale Zahlen) und daher f√ºr fast jeden Fall gefunden werden kann . </blockquote><br><br>  Trotzdem ist es unangenehm, dass wir die Arbeit der Nationalversammlung so schlecht verstehen, dass dieses Ergebnis k√ºrzlich entdeckt wurde.  Der Hauptvorteil solcher Ergebnisse wird nat√ºrlich sein, dass sie das Auftreten sp√§terer Arbeiten zu diesem Thema stimulierten.  Eine <a href="">k√ºrzlich durchgef√ºhrte Arbeit aus dem Jahr 2014 hat</a> gezeigt, dass es einem geschulten Netzwerk m√∂glich ist, Bilder zu erstellen, die f√ºr eine Person wie wei√ües Rauschen aussehen, und das Netzwerk wird sie mit einem hohen Ma√ü an Vertrauen in bekannte Kategorien einteilen.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist eine weitere Demonstration, die wir in der Arbeit der NS und in ihrer Verwendung f√ºr die Bilderkennung noch viel zu verstehen haben.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Trotz √§hnlicher Ergebnisse ist das Gesamtbild inspirierend. </font><font style="vertical-align: inherit;">Wir sehen schnelle Fortschritte bei der Durchf√ºhrung √§u√üerst komplexer Tests wie ImageNet. </font><font style="vertical-align: inherit;">Wir sehen auch schnelle Fortschritte bei der L√∂sung von Problemen aus der realen Welt, z. B. beim Erkennen von Adressschildern in StreetView. </font><font style="vertical-align: inherit;">Trotz der Inspiration reicht es jedoch nicht aus, nur Verbesserungen bei der Leistung von Geschwindigkeitstests oder sogar realen Aufgaben zu beobachten. </font><font style="vertical-align: inherit;">Es gibt grundlegende Ph√§nomene, deren Wesen wir noch immer schlecht verstehen, zum Beispiel die Existenz von Wettbewerbsbildern. </font><font style="vertical-align: inherit;">Und w√§hrend sich solche grundlegenden Probleme immer noch √∂ffnen (ganz zu schweigen von ihrer L√∂sung), w√§re es verfr√ºht, √ºber die Ann√§herung an die L√∂sung des Bilderkennungsproblems zu sprechen. </font><font style="vertical-align: inherit;">Gleichzeitig sind solche Probleme hervorragende Anreize f√ºr die weitere Arbeit.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Ans√§tze f√ºr tiefe neuronale Netze </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Buch haben wir uns auf eine Aufgabe konzentriert: die Klassifizierung von Zahlen MNIST. Eine hervorragende Aufgabe, die uns viele effektive Ideen verst√§ndlich machte: stochastischer Gradientenabstieg, Backpropagation, Faltungsnetzwerke, Regularisierung usw. Dies ist jedoch auch eine eher enge Aufgabe. Nachdem Sie die Literatur zu neuronalen Netzen gelesen haben, werden Sie auf viele Ideen sto√üen, die wir nicht besprochen haben: wiederkehrende NS, Boltzmann-Maschinen, generative Modelle, Trainingstransfer, verst√§rktes Lernen und so weiter und so fort! Neuronale Netze sind ein weites Gebiet. Viele wichtige Ideen sind jedoch Variationen dieser Ideen, die wir bereits besprochen haben, und sie sind recht einfach zu verstehen. In diesem Abschnitt werde ich den Vorhang √ºber diesen riesigen Weiten leicht √∂ffnen. Ihre Diskussion w√§re nicht detailliert und umfassend - dies w√ºrde das Buch extrem aufblasen. Es wird impressionistisch sein,ein Versuch, den konzeptuellen Reichtum dieses Bereichs aufzuzeigen und einige Konzepte mit denen zu verbinden, die wir bereits gesehen haben. Im Text werde ich einige Verweise auf andere Quellen sowie auf Materialien f√ºr die Weiterbildung geben. Nat√ºrlich werden viele von ihnen bald von anderen abgel√∂st, und Sie m√∂chten vielleicht nach neuerer Literatur suchen. Trotzdem glaube ich, dass viele Grundideen noch lange interessant bleiben werden.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wiederkehrende neuronale Netze (RNS) </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In den von uns verwendeten direkten Ausbreitungsnetzwerken gibt es einen Eingang, der die Aktivierung aller Neuronen in nachfolgenden Schichten vollst√§ndig bestimmt. </font><font style="vertical-align: inherit;">Dies ist ein sehr statisches Bild: Alles im Netzwerk ist fest und hat einen gefrorenen, kristallinen Charakter. </font><font style="vertical-align: inherit;">Angenommen, wir lassen zu, dass sich Netzwerkelemente dynamisch √§ndern. </font><font style="vertical-align: inherit;">Zum Beispiel kann das Verhalten von versteckten Neuronen nicht nur durch Aktivierungen in vorherigen Schichten bestimmt werden, sondern auch durch Aktivierungen, die fr√ºher in der Zeit stattfanden. </font><font style="vertical-align: inherit;">Die Aktivierung eines Neurons kann teilweise durch seine fr√ºhere Aktivierung bestimmt werden. </font><font style="vertical-align: inherit;">In Netzwerken mit direkter Verteilung ist dies eindeutig nicht der Fall. </font><font style="vertical-align: inherit;">Oder vielleicht wird die Aktivierung von versteckten und ausgegebenen Neuronen nicht nur durch die aktuelle Eingabe in das Netzwerk bestimmt, sondern auch durch vorherige.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neuronale Netze mit dieser Art von zeitlich variierendem Verhalten werden als wiederkehrende neuronale Netze oder RNS bezeichnet. Es gibt viele M√∂glichkeiten, die informelle Beschreibung des vorherigen Absatzes mathematisch zu formalisieren. Sie k√∂nnen sich ein Bild davon machen, indem Sie den </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wikipedia-Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> lesen </font><font style="vertical-align: inherit;">. Zum Zeitpunkt des Schreibens sind in der englischen Version des Artikels mindestens 13 verschiedene Modelle beschrieben [zum Zeitpunkt der √úbersetzung im Jahr 2019 bereits 18 / ca.</font></font> √ºbersetzt.].<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn wir jedoch die mathematischen Details beiseite lassen, ist die allgemeine Idee des RNS das Vorhandensein dynamischer √Ñnderungen im Netzwerk, die im Laufe der Zeit auftreten. Und es √ºberrascht nicht, dass sie besonders n√ºtzlich sind, um Daten oder Prozesse zu analysieren, die sich im Laufe der Zeit √§ndern. Solche Daten und Prozesse treten nat√ºrlich in Aufgaben wie der Sprachanalyse oder der nat√ºrlichen Sprache auf. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine der aktuellen M√∂glichkeiten zur Verwendung von RNS besteht darin, neuronale Netze besser in traditionelle Methoden zur Darstellung von Algorithmen zu integrieren, mit Konzepten wie einer Turing-Maschine und g√§ngigen Programmiersprachen. In </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeit von 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RNS wurde entwickelt, das in der Lage ist, eine buchstabenweise Beschreibung eines sehr einfachen Python-Programms zu akzeptieren und das Ergebnis seiner Arbeit vorherzusagen. Informell gesehen lernt das Netzwerk, bestimmte Python-Programme zu ‚Äûverstehen‚Äú. </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der zweiten Arbeit aus dem Jahr 2014</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> wurde das RNS als Ausgangspunkt f√ºr die Entwicklung der Turing-Neuromaschine (BDC) verwendet. Dies ist ein universeller Computer, dessen gesamte Struktur durch Gradientenabstieg trainiert werden kann. Sie trainierten ihren BDC, um Algorithmen f√ºr verschiedene einfache Aufgaben wie Sortieren oder Kopieren zu erstellen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies sind nat√ºrlich sehr einfache Spielzeugmodelle. Wenn Sie lernen, wie man ein Programm in Python wie print (398345 + 42598) ausf√ºhrt, wird ein neuronales Netzwerk nicht zu einem vollwertigen Interpreter der Sprache! Es ist nicht klar, wie viel st√§rker diese Ideen sein werden. Trotzdem sind die Ergebnisse sehr interessant. In der Vergangenheit haben neuronale Netze Muster gut erkannt, die auf herk√∂mmliche algorithmische Ans√§tze gesto√üen sind. Umgekehrt l√∂sen herk√∂mmliche algorithmische Ans√§tze Probleme, die f√ºr NS komplex sind, gut. Heute versucht niemand mehr, einen Webserver oder eine Datenbank auf Basis von NS zu implementieren! Es w√§re gro√üartig, integrierte Modelle zu entwickeln, die die St√§rken sowohl von NS als auch von traditionellen algorithmischen Ans√§tzen integrieren. RNS und von ihnen inspirierte Ideen k√∂nnen uns dabei helfen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In den letzten Jahren wurde RNS verwendet, um viele andere Probleme zu l√∂sen. Sie waren besonders n√ºtzlich bei der Spracherkennung. RNS-basierte Ans√§tze </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">stellen Rekorde</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr die Qualit√§t der </font><font style="vertical-align: inherit;">Phonemerkennung auf </font><font style="vertical-align: inherit;">. Sie wurden auch </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">verwendet, um</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> verbesserte Modelle der von Menschen verwendeten Sprache </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">zu entwickeln</font></a><font style="vertical-align: inherit;"> . Verbesserte Sprachmodelle helfen dabei, Mehrdeutigkeiten in der Sprache zu erkennen, die √§hnlich klingen. Ein gutes Sprachmodell kann uns sagen, dass der Ausdruck ‚Äûvorw√§rts ins Unendliche‚Äú viel wahrscheinlicher ist als der Ausdruck ‚Äûvorw√§rts ohne Gliedma√üen‚Äú, obwohl sie √§hnlich klingen. RNS wurde verwendet, um Rekordleistungen in bestimmten Sprachtests zu erzielen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diese Arbeit ist Teil der breiteren Verwendung von NS aller Art, nicht nur von RNS, zur L√∂sung des Problems der Spracherkennung. Beispielsweise hat ein GNS-basierter Ansatz </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hervorragende Ergebnisse</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bei der Erkennung kontinuierlicher Sprache mit einem gro√üen Wortschatz gezeigt. Ein weiteres </font><font style="vertical-align: inherit;">GNS-basiertes </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">System ist</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Android-Betriebssystem von Google </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementiert</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich habe ein wenig dar√ºber gesprochen, wozu die RNCs in der Lage sind, aber nicht erkl√§rt, wie sie funktionieren. </font><font style="vertical-align: inherit;">Sie werden nicht √ºberrascht sein zu erfahren, dass viele der Ideen aus der Welt der direkten Vertriebsnetze auch in RNS verwendet werden k√∂nnen. </font><font style="vertical-align: inherit;">Insbesondere k√∂nnen wir den RNS trainieren, indem wir den Gradientenabstieg und die R√ºckenausbreitung in der Stirn modifizieren. </font><font style="vertical-align: inherit;">Viele andere Ideen, die in direkten Verteilungsnetzen verwendet werden, von Regularisierungstechniken √ºber Faltungs- und Aktivierungs- bis hin zu Kostenfunktionen, werden ebenfalls n√ºtzlich sein. </font><font style="vertical-align: inherit;">Viele der Ideen, die wir im Rahmen des Buches entwickelt haben, k√∂nnen auch f√ºr die Verwendung im RNS angepasst werden.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DCT-Module (Long Term Short Term Memory) </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eines der Probleme von RNS ist, dass fr√ºhe Modelle sehr schwer zu trainieren waren, komplizierter als selbst GNS. Der Grund waren die Probleme des instabilen Gradienten, die wir in Kapitel 5 besprochen haben. Wir erinnern daran, dass die √ºbliche Manifestation dieses Problems darin bestand, dass der Gradient st√§ndig abnimmt, wenn er sich in entgegengesetzter Richtung durch die Schichten ausbreitet. Dies verlangsamt das Lernen der fr√ºhen Schichten extrem. In RNS wird dieses Problem noch schlimmer, da sich die Gradienten nicht nur in der entgegengesetzten Richtung entlang der Schichten, sondern auch zeitlich in der entgegengesetzten Richtung ausbreiten. Wenn das Netzwerk ziemlich lange funktioniert, kann der Gradient extrem instabil werden und auf seiner Basis sehr schwer zu erlernen sein. Gl√ºcklicherweise kann eine Idee, die als </font><font style="vertical-align: inherit;">DCT- </font><font style="vertical-align: inherit;">Module </font><font style="vertical-align: inherit;">( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Long Term Short Term Memory</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) bekannt </font><font style="vertical-align: inherit;">ist, in das RNS aufgenommen werden </font><font style="vertical-align: inherit;">. Zum ersten Mal wurden die Module vorgestellt</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hochreiter und Schmidguber im Jahr 1997</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , speziell um das Problem eines instabilen Gradienten zu l√∂sen. </font><font style="vertical-align: inherit;">DCTs erleichtern das Erzielen guter Ergebnisse beim Erlernen von RNS, und viele neuere Arbeiten (einschlie√ülich der Arbeiten, auf die ich bereits verwiesen habe) verwenden DCT oder √§hnliche Ideen.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Deep Trust-Netzwerke, generative Modelle und Boltzmann-Maschinen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Heutzutage hat das Interesse an Deep Learning 2006 nach der Ver√∂ffentlichung von Werken ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), in denen erkl√§rt wird, wie eine spezielle Art von NS, das so genannte Deep Trust Network (GDS), unterrichtet </font><font style="vertical-align: inherit;">wird, einen zweiten Wind bekommen </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">GDS beeinflusste mehrere Jahre lang das Forschungsfeld, aber dann begann ihre Popularit√§t abzunehmen, und direkte Vertriebsnetze und wiederkehrende NS wurden in Mode. </font><font style="vertical-align: inherit;">Trotzdem machen einige der Eigenschaften von GDS sie sehr interessant.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Erstens sind GDS ein Beispiel f√ºr ein generatives Modell. In einem direkten Verteilungsnetzwerk spezifizieren wir Eingabeaktivierungen und sie bestimmen die Aktivierung von Merkmalsneuronen weiter unten im Netzwerk. Das generative Modell kann auf √§hnliche Weise verwendet werden, aber Sie k√∂nnen die Werte der darin enthaltenen Neuronen festlegen und dann das Netzwerk ‚Äûin die entgegengesetzte Richtung‚Äú ausf√ºhren, um die Werte der Eingabeaktivierungen zu generieren. Insbesondere kann ein GDS, der auf handgeschriebenen Ziffernbildern trainiert ist, selbst Bilder erzeugen, die handgeschriebenen Ziffern √§hnlich sind (m√∂glicherweise und nach bestimmten Aktionen). Mit anderen Worten, GDM kann gewisserma√üen schreiben lernen. In diesem Sinne √§hneln generative Modelle dem menschlichen Gehirn: Sie k√∂nnen Zahlen nicht nur lesen, sondern auch schreiben. </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Jeffrey Hintons</font></a><font style="vertical-align: inherit;"> ber√ºhmtes </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sprichwort</font></font></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gibt an, dass Sie f√ºr die Mustererkennung zun√§chst lernen m√ºssen, wie Bilder generiert werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zweitens k√∂nnen sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ohne Lehrer</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und fast ohne Lehrer </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">lernen</font></a><font style="vertical-align: inherit;"> . Wenn Sie beispielsweise Bilder trainieren, k√∂nnen GDS Zeichen lernen, die zum Verst√§ndnis anderer Bilder n√ºtzlich sind, selbst wenn die Trainingsbilder keine Markierungen aufweisen. Die F√§higkeit, ohne Lehrer zu lernen, ist sowohl aus grundlegender wissenschaftlicher als auch aus praktischer Sicht √§u√üerst interessant - wenn sie gut genug funktioniert.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Warum ging ihre Popularit√§t angesichts all dieser attraktiven Punkte der GDS als Modelle f√ºr tiefes Lernen zur√ºck? Teilweise aufgrund der Tatsache, dass andere Modelle wie Direktverteilung und wiederkehrende Netzwerke erstaunliche Ergebnisse erzielt haben, insbesondere Durchbr√ºche in den Bereichen Bilderkennung und Sprache. Es ist nicht √ºberraschend, dass diese Modelle solche Aufmerksamkeit erhalten haben und sehr verdient sind. Daraus folgt jedoch eine unangenehme Schlussfolgerung. Der Markt der Ideen funktioniert oft nach dem Schema ‚ÄûGewinner bekommt alles‚Äú, und fast die gesamte Aufmerksamkeit gilt dem, was derzeit in diesem Bereich am angesagtesten ist. Es kann f√ºr Menschen √§u√üerst schwierig sein, an derzeit unpopul√§ren Ideen zu arbeiten, auch wenn es offensichtlich ist, dass sie von langfristigem Interesse sind. Meine pers√∂nliche Meinung ist, dass die GDS und andere generative Modelle mehr Aufmerksamkeit verdienen als sie bekommen.Ich bin nicht √ºberrascht, wenn das GDM oder ein √§hnliches Modell jemals die heutigen popul√§ren Modelle √ºbertrifft. Lesen Sie</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Artikel dient der</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Einf√ºhrung in den Bereich GDM. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Artikel</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> kann auch n√ºtzlich sein </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Es geht nicht nur um GDM, aber es gibt viele n√ºtzliche Dinge √ºber limitierte Boltzmann-Maschinen, eine Schl√ºsselkomponente von GDM.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Ideen </font></font></h3><br>  Was passiert sonst noch im Bereich der Nationalversammlung und des Zivilschutzes?  Eine Menge interessanter Arbeiten.  Zu den aktiven Forschungsgebieten geh√∂rt die Verwendung von NS zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verarbeitung</a> nat√ºrlicher <a href="">Sprache</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschineller √úbersetzung</a> und unerwarteterer Anwendungen, beispielsweise der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Musikinformatik</a> .  Es gibt viele andere Bereiche.  In vielen F√§llen k√∂nnen Sie nach dem Lesen dieses Buches die neuesten Arbeiten verstehen, obwohl Sie nat√ºrlich m√∂glicherweise einige Wissensl√ºcken schlie√üen m√ºssen. <br><br>  Ich werde diesen Abschnitt mit der Erw√§hnung einer besonders interessanten Arbeit beenden.  Sie kombiniert tiefe Faltungsnetzwerke mit einer Technik namens Verst√§rkungslernen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">um</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lernen, wie man Videospiele spielt</a> (und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem weiteren Artikel</a> dar√ºber).  Die Idee ist, ein Faltungsnetzwerk zu verwenden, um Pixeldaten vom Spielbildschirm aus zu vereinfachen und sie in einen einfacheren Satz von Attributen umzuwandeln, die dann verwendet werden k√∂nnen, um Entscheidungen √ºber weitere Aktionen zu treffen: "nach links gehen", "nach rechts gehen", "schie√üen" und usw.  Besonders interessant ist, dass ein Netzwerk ziemlich gut gelernt hat, sieben verschiedene klassische Videospiele zu spielen, vor Experten in drei von ihnen.  Dies sieht nat√ºrlich nach einem Trick aus, und die Arbeit wurde aktiv unter der √úberschrift ‚ÄûAtari-Spiele mit Reinforcement Learning spielen‚Äú beworben.  Hinter einem oberfl√§chlichen Glanz ist jedoch die Tatsache zu ber√ºcksichtigen, dass das System Rohpixeldaten verwendet - es kennt nicht einmal die Spielregeln - und auf ihrer Grundlage darauf trainiert ist, qualitativ hochwertige Entscheidungen in mehreren sehr unterschiedlichen und sehr wettbewerbsorientierten Situationen zu treffen, von denen jede ihre eigenen komplexen Regeln hat.  Ziemlich gut. <br><br><h2>  Die Zukunft neuronaler Netze </h2><br><h3>  User Intent Interfaces </h3><br>  In einem alten Witz sagt ein ungeduldiger Professor zu einem verwirrten Studenten: "H√∂re nicht auf meine Worte, h√∂r auf das, was ich meine."  In der Vergangenheit haben Computer wie ein verwirrter Sch√ºler oft nicht verstanden, was ein Benutzer bedeutet.  Die Situation √§ndert sich jedoch.  Ich erinnere mich noch an das erste Mal, als ich √ºberrascht war, als ich f√§lschlicherweise eine Anfrage an Google schrieb und die Suchmaschine zu mir sagte: "Meinten Sie [richtige Anfrage]?"  Google Director Larry Page hat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einmal die</a> perfekte Suchmaschine als ein System beschrieben, das genau versteht, was Ihre Abfragen bedeuten, und Ihnen genau das gibt, was Sie wollen. <br><br>  Dies ist die Idee einer Schnittstelle, die auf der Absicht des Benutzers basiert.  Anstatt auf w√∂rtliche Benutzeranfragen zu antworten, verwendet die Suchmaschine das MO, um eine vage Benutzeranfrage entgegenzunehmen, genau zu verstehen, was dies bedeutet, und auf dieser Basis zu handeln. <br><br>  Die Idee einer Schnittstelle, die auf der Absicht des Benutzers basiert, kann breiter angewendet werden als nur bei der Suche.  In den n√§chsten Jahrzehnten werden Tausende von Unternehmen Produkte entwickeln, in denen MO f√ºr Benutzeroberfl√§chen verwendet wird, wobei sie sich ruhig auf ungenaue Benutzeraktionen beziehen und ihre wahren Absichten erraten.  Wir sehen bereits fr√ºhe Beispiele f√ºr solche absichtsbasierten Schnittstellen: Apple Siri;  Wolfram Alpha;  IBM Watson  Systeme, die Fotos und Videos automatisch markieren und vieles mehr. <br><br>  Die meisten von ihnen werden scheitern.  Die Entwicklung von Schnittstellen ist eine komplizierte Sache, und ich vermute, dass viele Unternehmen, anstatt Schnittstellen zu inspirieren, leblose Schnittstellen auf der Basis von MO erstellen werden.  Das beste MO der Welt hilft Ihnen nicht, wenn Ihre Schnittstelle nicht funktioniert.  Einige Produkte werden jedoch erfolgreich sein.  Dies wird im Laufe der Zeit zu einer ernsthaften √Ñnderung unserer Beziehung zu Computern f√ºhren.  Vor nicht allzu langer Zeit, zum Beispiel im Jahr 2005, hielten Benutzer es f√ºr selbstverst√§ndlich, dass die Interaktion mit Computern eine hohe Genauigkeit erfordert.  Die w√∂rtliche Natur des Computers diente dazu, die Idee zu verbreiten, dass Computer sehr w√∂rtlich sind;  Das einzige vergessene Semikolon k√∂nnte die Art der Interaktion mit dem Computer vollst√§ndig ver√§ndern.  Ich glaube jedoch, dass wir in den n√§chsten Jahrzehnten mehrere erfolgreiche Schnittstellen entwickeln werden, die auf der Absicht der Benutzer basieren, und dies wird unsere Erwartungen bei der Arbeit mit Computern radikal ver√§ndern. <br><br><h3>  Maschinelles Lernen, Datenwissenschaft und der makellose Innovationskreis </h3><br>  Nat√ºrlich wird MO nicht nur zum Erstellen von Schnittstellen verwendet, die auf der Absicht des Benutzers basieren.  Eine weitere interessante Anwendung von MO ist die Datenwissenschaft, bei der nach ‚Äûbekannten Unbekannten‚Äú gesucht wird, die in den erhaltenen Daten verborgen sind.  Dies ist bereits ein modisches Thema, √ºber das viele Artikel geschrieben wurden, daher werde ich nicht lange darauf eingehen.  Ich m√∂chte eine Konsequenz dieser Mode erw√§hnen, die nicht oft erw√§hnt wird: Auf lange Sicht ist es m√∂glich, dass der gr√∂√üte Durchbruch in der Region Moskau nicht nur ein konzeptioneller Durchbruch sein wird.  Der gr√∂√üte Durchbruch wird darin bestehen, dass die Forschung auf dem Gebiet der MO durch die Verwendung von Daten in der Wissenschaft und in anderen Bereichen rentabel wird.  Wenn ein Unternehmen einen Dollar in MO-Forschung investieren und ziemlich schnell einen Dollar und zehn Cent Umsatz erzielen kann, wird viel Geld in die MO-Region flie√üen.  Mit anderen Worten, MO ist der Motor, der uns zur Entstehung mehrerer gro√üer M√§rkte und Bereiche des Technologiewachstums treibt.  Infolgedessen werden gro√üe Teams von Experten auf diesem Gebiet erscheinen, die Zugang zu unglaublichen Ressourcen haben.  Dies wird das MO noch weiter bewegen und noch mehr M√§rkte und M√∂glichkeiten schaffen, die den makellosen Innovationskreis bilden werden. <br><br><h3>  Die Rolle neuronaler Netze und tiefes Lernen </h3><br>  Ich habe MO allgemein als einen Weg beschrieben, neue M√∂glichkeiten f√ºr die Technologieentwicklung zu schaffen.  Welche spezifische Rolle wird die Nationalversammlung und die Zivilgesellschaft dabei spielen? <br><br>  Um die Frage zu beantworten, ist es n√ºtzlich, sich der Geschichte zuzuwenden.  In den 1980er Jahren gab es eine aktive freudige Wiederbelebung und Optimismus in Verbindung mit neuronalen Netzen, insbesondere nach der Popularisierung der R√ºckausbreitung.  Die Erholung lie√ü jedoch nach, und in den neunziger Jahren wurde der MO-Stab auf andere Technologien √ºbertragen, beispielsweise auf die Support-Vektor-Methode.  Heute ist die Nationalversammlung wieder auf dem Pferd, stellt alle m√∂glichen Rekorde auf und √ºberholt viele Gegner in verschiedenen Problemen.  Aber wer garantiert, dass morgen kein neuer Ansatz entwickelt wird, der die NA erneut √ºberschattet?  Oder werden die Fortschritte auf dem Gebiet der Nationalversammlung vielleicht ins Stocken geraten und nichts wird sie ersetzen? <br><br>  Daher ist es viel einfacher, √ºber die Zukunft des gesamten Verteidigungsministeriums nachzudenken, als speziell √ºber die Nationalversammlung.  Ein Teil des Problems ist, dass wir die Nationalversammlung sehr schlecht verstehen.  Warum kann NS so gut Informationen zusammenstellen?  Wie vermeiden sie eine so gute Umschulung angesichts der Vielzahl von Optionen?  Warum funktioniert der stochastische Gradientenabstieg so gut?  Wie gut funktioniert NS beim Skalieren von Datens√§tzen?  Wenn wir beispielsweise die ImageNet-Basis zehnmal erweitern, wird sich die Leistung des NS mehr oder weniger verbessern als die Effektivit√§t anderer MO-Technologien?  All dies sind einfache, grundlegende Fragen.  Und bis jetzt haben wir ein sehr schlechtes Verst√§ndnis f√ºr die Antworten auf diese Fragen.  In dieser Hinsicht ist es schwierig zu sagen, welche Rolle die Nationalversammlung in der Zukunft der Region Moskau spielen wird. <br><br>  Ich werde eine Vorhersage machen: Ich denke, dass GO nirgendwo hingehen wird.  Die F√§higkeit, Hierarchien von Konzepten zu studieren und verschiedene Abstraktionsebenen aufzubauen, ist offensichtlich von grundlegender Bedeutung f√ºr das Wissen der Welt.  Dies bedeutet nicht, dass sich die GO-Netzwerke von morgen nicht radikal von denen von heute unterscheiden werden.  Wir k√∂nnen auf gro√üe √Ñnderungen in ihren Bestandteilen, Architekturen oder Lernalgorithmen sto√üen.  Diese √Ñnderungen k√∂nnen sich als dramatisch genug herausstellen, damit wir die resultierenden Systeme nicht mehr als neuronale Netze betrachten.  Sie werden sich jedoch weiterhin im Zivilschutz engagieren. <br><br><h3>  Werden NS und GO bald zum Auftreten k√ºnstlicher Intelligenz f√ºhren? </h3><br>  In diesem Buch haben wir uns auf die Verwendung von NS bei der L√∂sung spezifischer Probleme konzentriert, z. B. der Bildklassifizierung.  Lassen Sie uns unsere Fragen erweitern: Was ist mit universell denkenden Computern?  K√∂nnen uns die Nationalversammlung und die Zivilgesellschaft helfen, das Problem der Schaffung einer Allzweck-KI zu l√∂sen?  Und wenn ja, werden wir angesichts der hohen Fortschritte im Bereich des Zivilschutzes in naher Zukunft die Entstehung von KI sehen? <br><br>  Eine detaillierte Antwort auf eine solche Frage w√ºrde ein separates Buch erfordern.  Lassen Sie mich stattdessen eine Beobachtung anbieten, die auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem Gesetz</a> von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Conway basiert</a> : <br><blockquote>  Organisationen, die Systeme entwerfen, sind auf ein Design beschr√§nkt, das die Kommunikationsstruktur dieser Organisation kopiert. </blockquote><br><br>  Das hei√üt, das Gesetz von Conway besagt beispielsweise, dass das Layout des Boeing 747-Flugzeugs die erweiterte Struktur von Boeing und seinen Auftragnehmern zum Zeitpunkt der Entwicklung des 747-Modells widerspiegelt. Oder ein anderes einfaches und konkretes Beispiel: Betrachten Sie ein Unternehmen, das komplexe Software entwickelt.  Wenn das Software-Control-Panel mit dem MO-Algorithmus verbunden werden soll, sollte der Panel-Designer mit dem MO-Experten des Unternehmens kommunizieren.  Conways Gesetz formalisiert diese Beobachtung einfach. <br><br>  Zum ersten Mal, als sie Conways Gesetz h√∂rten, sagten viele Leute entweder: "Ist das kein allt√§glicher Beweis?" Oder "Ist es so?"  Ich werde mit einer Bemerkung √ºber seine Untreue beginnen.  Denken wir mal: Wie spiegelt sich die Boeing-Buchhaltung im Modell 747 wider?  Was ist mit der Reinigungsabteilung?  Ein F√ºtterungspersonal?  Die Antwort ist, dass diese Teile der Organisation h√∂chstwahrscheinlich nirgendwo anders in Schema 747 explizit erscheinen.  Daher m√ºssen Sie verstehen, dass das Gesetz von Conway nur f√ºr diejenigen Teile der Organisation gilt, die direkt mit Design und Engineering befasst sind. <br><br>  Was ist mit der Bemerkung √ºber Banalit√§t und Beweise?  Vielleicht ist das so, aber ich denke nicht, weil Organisationen oft daran arbeiten, das Gesetz von Conway abzulehnen.  Teams, die neue Produkte entwickeln, sind h√§ufig aufgrund der √ºberm√§√üigen Anzahl von Mitarbeitern aufgeblasen, oder umgekehrt fehlt ihnen eine Person mit kritischem Wissen.  Denken Sie an alle Produkte mit nutzlosen und komplizierten Funktionen.  Oder denken Sie an Produkte mit offensichtlichen M√§ngeln - zum Beispiel mit einer schrecklichen Benutzeroberfl√§che.  In beiden Programmklassen treten h√§ufig Probleme auf, weil das Team, das f√ºr die Ver√∂ffentlichung eines guten Produkts erforderlich ist, nicht mit dem Team √ºbereinstimmt, das sich wirklich versammelt hat.  Das Gesetz von Conway mag offensichtlich sein, aber das bedeutet nicht, dass die Leute es nicht regelm√§√üig ignorieren k√∂nnen. <br><br>  Das Gesetz von Conway gilt f√ºr den Entwurf und die Erstellung von Systemen in F√§llen, in denen wir uns von Anfang an vorstellen, aus welchen Bestandteilen das Produkt bestehen wird und wie sie hergestellt werden sollen.  Es kann nicht direkt auf die Entwicklung der KI angewendet werden, da die KI (noch) keine solche Aufgabe ist: Wir wissen nicht, aus welchen Teilen sie besteht.  Wir sind uns nicht einmal sicher, welche grundlegenden Fragen Sie stellen k√∂nnen.  Mit anderen Worten, im Moment ist KI eher ein Problem der Wissenschaft als der Ingenieure.  Stellen Sie sich vor, Sie m√ºssen mit der Entwicklung des 747 beginnen, ohne etwas √ºber D√ºsentriebwerke oder die Prinzipien der Aerodynamik zu wissen.  Sie wissen nicht, welche Experten Sie in Ihrer Organisation einstellen sollen.  Wie Werner von Braun schrieb: "Grundlagenforschung ist das, was ich mache, wenn ich nicht wei√ü, was ich mache."  Gibt es eine Version des Conway-Gesetzes, die f√ºr Aufgaben gilt, die mehr mit der Wissenschaft als mit Ingenieuren zu tun haben? <br><br>  Um die Antwort auf diese Frage zu finden, erinnern wir uns an die Geschichte der Medizin.  In den fr√ºhen Tagen war die Medizin die Dom√§ne von Praktizierenden wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Galen</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hippokrates</a> , die den gesamten menschlichen K√∂rper untersuchten.  Aber mit der Zunahme unseres Wissensvolumens musste ich mich spezialisieren.  Wir haben viele tiefe Ideen entdeckt - erinnern Sie sich an die mikrobielle Theorie von Krankheiten oder an das Verst√§ndnis des Wirkungsprinzips von Antik√∂rpern oder an die Tatsache, dass Herz, Lunge, Venen und Arterien das Herz-Kreislauf-System bilden.  Solche tiefen Ideen bildeten die Grundlage f√ºr engere Disziplinen wie Epidemiologie, Immunologie und die Anh√§ufung √ºberlappender Bereiche im Zusammenhang mit dem Herz-Kreislauf-System.  So bildete die Struktur unseres Wissens die soziale Struktur der Medizin.  Dies macht sich insbesondere in der Immunologie bemerkbar: Die Vorstellung, dass ein Immunsystem existiert, das einer gesonderten Studie w√ºrdig ist, war sehr trivial.  Wir haben also ein ganzes Gebiet der Medizin - mit Spezialisten, Konferenzen, Auszeichnungen usw. -, das sich um etwas k√ºmmert, das nicht nur unsichtbar, sondern vielleicht auch nicht getrennt ist. <br><br>  Eine solche Entwicklung von Ereignissen wurde oft in vielen etablierten wissenschaftlichen Disziplinen wiederholt: nicht nur in der Medizin, sondern auch in Physik, Mathematik, Chemie und anderen.  Regionen werden monolithisch geboren und haben nur wenige tiefgreifende Ideen auf Lager.  Die ersten Experten k√∂nnen sie alle abdecken.  Aber im Laufe der Zeit √§ndert sich die Solidit√§t.  Wir entdecken viele neue tiefe Ideen, und es gibt zu viele, als dass jemand sie alle wirklich beherrschen k√∂nnte.  Infolgedessen wird die soziale Struktur der Region neu organisiert und geteilt, wobei diese Ideen im Mittelpunkt stehen.  Anstelle eines Monolithen haben wir Felder, die durch Felder geteilt sind, die durch Felder getrennt sind - eine komplexe, rekursive soziale Struktur, die sich auf sich selbst bezieht und deren Organisation die Verbindungen zwischen den tiefgreifendsten Ideen widerspiegelt.  So bildet die Struktur unseres Wissens die soziale Organisation der Wissenschaft.  Diese soziale Form begrenzt jedoch wiederum und hilft zu bestimmen, was wir erkennen k√∂nnen.  Dies ist das wissenschaftliche Analogon zu Conways Gesetz. <br><br>  Aber was hat das alles mit Deep Learning oder KI zu tun? <br><br>  Nun, seit den Anf√§ngen der KI-Entwicklung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gab es Debatten dar√ºber,</a> dass alles entweder "nicht zu kompliziert, dank der Superwaffe, die wir haben" oder "Superwaffe wird nicht genug sein".  Deep Learning ist das neueste Beispiel f√ºr eine Superwaffe, die in den Streitigkeiten verwendet wurde, die ich gesehen habe.  In den fr√ºhen Versionen solcher Streitigkeiten wurde Logik oder Prolog oder Expertensysteme oder eine andere Technologie verwendet, die damals die m√§chtigste war.  Das Problem bei solchen Streitigkeiten ist, dass sie Ihnen nicht die M√∂glichkeit geben, genau zu sagen, wie m√§chtig einer der Kandidaten f√ºr Superwaffen sein wird.  Nat√ºrlich haben wir nur ein ganzes Kapitel damit verbracht, Beweise daf√ºr zu pr√ºfen, dass Zivilschutz √§u√üerst komplexe Probleme l√∂sen kann.  Es sieht auf jeden Fall sehr interessant und vielversprechend aus.  Dies war jedoch bei Systemen wie Prolog oder Eurisko oder bei Expertensystemen der Fall.  Daher bedeutet nur die Tatsache, dass eine Reihe von Ideen vielversprechend aussieht, nichts Besonderes.  Woher wissen wir, dass GO sich tats√§chlich von diesen fr√ºhen Ideen unterscheidet?  Gibt es eine M√∂glichkeit zu messen, wie m√§chtig und vielversprechend eine Reihe von Ideen ist?  Aus Conways Gesetz folgt, dass wir die Komplexit√§t der mit diesen Ideen verbundenen sozialen Struktur als grobe und heuristische Metrik verwenden k√∂nnen. <br><br>  Daher haben wir zwei Fragen.  Erstens, wie m√§chtig sind die zivilgesellschaftlichen Ideen gem√§√ü dieser Metrik der sozialen Komplexit√§t?  Zweitens, wie m√§chtig ist eine Theorie, um eine Allzweck-KI zu erstellen? <br><br>  Zur ersten Frage: Wenn wir uns heute den Zivilschutz ansehen, sieht dieses Feld interessant aus und entwickelt sich schnell, aber relativ monolithisch.  Es hat mehrere tiefgreifende Ideen und es werden mehrere gro√üe Konferenzen abgehalten, von denen sich einige stark √ºberschneiden.  Bei der Arbeit werden dieselben Ideen verwendet: der stochastische Gradientenabstieg (oder dessen nahe √Ñquivalent), um die Kostenfunktion zu optimieren.  Es ist gro√üartig, dass diese Ideen so erfolgreich sind.  Was wir bisher nicht beobachten, ist eine gro√üe Anzahl gut entwickelter kleinerer Gebiete, von denen jedes seine eigenen tiefgreifenden Ideen erforschen w√ºrde, die die Zivilgesellschaft in viele Richtungen bewegen w√ºrden.  Entsprechend der Metrik der sozialen Komplexit√§t tut Deep Learning dem Wortspiel leid, w√§hrend es ein sehr flaches Forschungsgebiet bleibt.  Eine Person ist immer noch in der Lage, die meisten tiefen Ideen aus diesem Bereich zu beherrschen. <br><br>  Zur zweiten Frage: Wie viel komplexe und leistungsstarke Ideen werden ben√∂tigt, um KI zu schaffen?  Die Antwort lautet nat√ºrlich: Niemand wei√ü es genau.  Aber im Nachwort zum Buch habe ich einige der vorhandenen Beweise zu diesem Thema studiert.  Ich kam zu dem Schluss, dass die Schaffung von KI selbst nach optimistischen Sch√§tzungen viele, viele tiefgreifende Ideen erfordern wird.  Nach dem Gesetz von Conway m√ºssen, um diesen Punkt zu erreichen, viele miteinander verbundene Disziplinen mit einer komplexen und unerwarteten Struktur entstehen, die die Struktur unserer tiefsten Ideen widerspiegelt.  Wir beobachten noch keine so komplexe soziale Struktur, wenn wir NS und Zivilschutz einsetzen.  Daher glaube ich, dass wir zumindest einige Jahrzehnte davon entfernt sind, GO zur Entwicklung einer Allzweck-KI zu verwenden. <br><br>  Ich habe mich sehr bem√ºht, ein spekulatives Argument zu entwickeln, das vielleicht ganz offensichtlich erscheint und nicht zu einer bestimmten Schlussfolgerung f√ºhrt.  Dies wird sicherlich sicherheitsliebende Menschen entt√§uschen.  Ich treffe viele Leute online, die √∂ffentlich ihre sehr eindeutigen und selbstbewussten Meinungen √ºber KI verk√ºnden, oft basierend auf wackeligen Argumenten und nicht existierenden Beweisen.  Ich kann ehrlich sagen: Ich denke, es ist zu fr√ºh, um es beurteilen zu k√∂nnen.  Wie im alten Witz: Wenn Sie einen Wissenschaftler fragen, wie viel mehr wir auf eine Entdeckung warten m√ºssen und er ‚Äû10 Jahre‚Äú (oder mehr) sagt, dann bedeutet er tats√§chlich ‚ÄûIch habe keine Ahnung‚Äú.  Vor dem Aufkommen der KI, wie im Fall der kontrollierten Kernfusion und einiger anderer Technologien, sind ‚Äû10 Jahre‚Äú mehr als 60 Jahre geblieben.  Auf der anderen Seite haben wir im Bereich des Zivilschutzes definitiv eine wirksame Technologie, deren Grenzen wir noch nicht entdeckt haben, und viele offene grundlegende Aufgaben.  Und es er√∂ffnet erstaunliche kreative M√∂glichkeiten. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de464039/">https://habr.com/ru/post/de464039/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de464021/index.html">Was ist Feature Toggle oder wie kann man qu√§lende Walrosse und langlebige Zweige loswerden?</a></li>
<li><a href="../de464023/index.html">"Grundlagen der Programmierung" f√ºr einen kostenlosen Kurs mit Beispielen in JavaScript</a></li>
<li><a href="../de464027/index.html">Wie man Inhalte im Zeitalter der Informationsexplosion √ºberlebt</a></li>
<li><a href="../de464031/index.html">‚ÄûFunde eines Audiomanen‚Äú: Soundkarten, um in die Atmosph√§re einer unbekannten Stadt einzutauchen</a></li>
<li><a href="../de464037/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 472 (30.07.2019 - 05.08.2019)</a></li>
<li><a href="../de464041/index.html">Warum die besten Kampfpiloten oft in gro√üe Schwierigkeiten geraten</a></li>
<li><a href="../de464043/index.html">Geschichte des Ethernet-CAN-Konverters</a></li>
<li><a href="../de464045/index.html">Wie ich 1997 fast in Echtzeit Rennstrecken gefahren bin</a></li>
<li><a href="../de464053/index.html">Hinweis: Spurauswahl- und Rotationsalgorithmus</a></li>
<li><a href="../de464055/index.html">Wir untersuchen die von Xiaomi Mi Band f√ºr das Jahr gesammelten Daten</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>