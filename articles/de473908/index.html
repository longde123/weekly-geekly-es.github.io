<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåÜ ü•• üí∂ Ressourcen in Kuba versichern üëπ üë©‚Äçüëß üòê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cluster-Ressourcenmanagement ist immer ein komplexes Thema. Wie kann die Notwendigkeit erkl√§rt werden, Ressourcen f√ºr den Benutzer zu konfigurieren, d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ressourcen in Kuba versichern</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473908/"><p><img src="https://habrastorage.org/webt/bp/bf/_p/bpbf_pwx_8zuvlwptjwkfnbxb5y.jpeg" alt="Bild"></p><br><p>  Cluster-Ressourcenmanagement ist immer ein komplexes Thema.  Wie kann die Notwendigkeit erkl√§rt werden, Ressourcen f√ºr den Benutzer zu konfigurieren, der seine Anwendungen im Cluster bereitstellt?  Vielleicht ist es einfacher, dies zu automatisieren? </p><a name="habracut"></a><br><h3 id="opisanie-problemy">  Problembeschreibung </h3><br><p>  Das Ressourcenmanagement ist eine wichtige Aufgabe im Kontext der Kubernetes-Clusterverwaltung.  Aber warum ist es wichtig, wenn Kubernetes die ganze harte Arbeit f√ºr Sie erledigt?  Weil es nicht so ist.  Kubernetes bietet Ihnen praktische Tools zur L√∂sung vieler Probleme ... wenn Sie diese Tools verwenden.  F√ºr jeden Pod in Ihrem Cluster k√∂nnen Sie die f√ºr seine Container erforderlichen Ressourcen angeben.  Und Kubernetes verwendet diese Informationen, um Instanzen Ihrer Anwendung auf Clusterknoten zu verteilen. </p><br><p> Nur wenige Menschen nehmen das Ressourcenmanagement bei Kubernetes ernst.  Dies ist normal f√ºr einen leicht geladenen Cluster mit einigen statischen Anwendungen.  Aber was ist, wenn Sie einen sehr dynamischen Cluster haben?  Woher kommen und gehen Anwendungen, wo wird der Namespace st√§ndig erstellt und gel√∂scht?  Ein Cluster mit einer gro√üen Anzahl von Benutzern, die ihren eigenen Namespace erstellen und Anwendungen bereitstellen k√∂nnen?  In diesem Fall kommt es anstelle einer stabilen und vorhersehbaren Orchestrierung zu einer Reihe zuf√§lliger Abst√ºrze in Anwendungen und manchmal sogar in Komponenten von Kubernetes selbst! </p><br><p>  Hier ist ein Beispiel f√ºr einen solchen Cluster: </p><br><p><img src="https://habrastorage.org/webt/0y/ar/2p/0yar2pe_8-bk-8cwrkd95bz8leu.png" alt="Bild"></p><br><p>  Sie sehen 3 Herde im Status "Beenden".  Dies ist jedoch nicht die √ºbliche Entfernung von Herden - sie stecken in diesem Zustand fest, weil der Containerd-Daemon auf ihrem Knoten von etwas sehr ressourcenhungrigem getroffen wurde. </p><br><p>  Solche Probleme k√∂nnen gel√∂st werden, indem der Mangel an Ressourcen richtig behandelt wird. Dies ist jedoch nicht das Thema dieses Artikels (es gibt einen guten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> ) sowie keine Silberkugel, um alle Probleme mit Ressourcen zu l√∂sen. </p><br><p>  Der Hauptgrund f√ºr solche Probleme ist falsch oder mangelnde Ressourcenverwaltung im Cluster.  Und wenn diese Art von Problem f√ºr Bereitstellungen keine Katastrophe darstellt, da sie leicht ein neues Problem erstellen, unter dem sie arbeiten, sind solche Einfrierungen f√ºr Entit√§ten wie DaemonSet oder noch mehr f√ºr StatefulSet fatal und erfordern manuelle Eingriffe. </p><br><p>  Sie k√∂nnen einen riesigen Cluster mit viel CPU und Speicher haben.  Wenn Sie viele Anwendungen ohne die richtigen Ressourceneinstellungen ausf√ºhren, besteht die M√∂glichkeit, dass alle ressourcenintensiven Pods auf demselben Knoten platziert werden.  Sie k√§mpfen um Ressourcen, auch wenn die verbleibenden Knoten des Clusters praktisch frei bleiben. </p><br><p>  Sie k√∂nnen auch h√§ufig weniger kritische F√§lle sehen, in denen einige der Anwendungen von ihren Nachbarn betroffen sind.  Selbst wenn die Ressourcen dieser "unschuldigen" Anwendungen korrekt konfiguriert wurden, kann ein Wandern unter sie kommen und sie t√∂ten.  Ein Beispiel f√ºr ein solches Szenario: </p><br><ol><li>  Ihre Anwendung fordert 4 GB Speicher an, ben√∂tigt jedoch zun√§chst nur 1 GB. </li><li>  Ein Wandern unter ohne Ressourcenkonfiguration wird demselben Knoten zugewiesen. </li><li>  Das Wandern unter verbraucht den gesamten verf√ºgbaren Speicher. </li><li>  Ihre Anwendung versucht, mehr Speicher zuzuweisen, und st√ºrzt ab, weil nicht mehr vorhanden ist. </li></ol><br><p>  Ein weiterer recht beliebter Fall ist die Neubewertung.  Einige Entwickler stellen gro√üe Anfragen in Manifesten "nur f√ºr den Fall" und verwenden diese Ressourcen nie.  Das Ergebnis ist eine Geldverschwendung. </p><br><h3 id="teoriya-resheniya">  Entscheidungstheorie </h3><br><p>  Horror!  Richtig? <br>  Gl√ºcklicherweise bietet Kubernetes eine M√∂glichkeit, Pods einige Einschr√§nkungen aufzuerlegen, indem Standardressourcenkonfigurationen sowie Mindest- und H√∂chstwerte angegeben werden.  Dies wird mit dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LimitRange-Objekt</a> implementiert.  LimitRange ist ein sehr praktisches Tool, wenn Sie nur √ºber eine begrenzte Anzahl von Namespaces verf√ºgen oder die vollst√§ndige Kontrolle √ºber deren Erstellung haben.  Auch ohne die richtige Konfiguration der Ressourcen sind Ihre Anwendungen in ihrer Verwendung eingeschr√§nkt.  "Unschuldige", richtig abgestimmte Herde sind sicher und vor sch√§dlichen Nachbarn gesch√ºtzt.  Wenn jemand eine gierige Anwendung ohne Ressourcenkonfiguration bereitstellt, erh√§lt diese Anwendung Standardwerte und st√ºrzt wahrscheinlich ab.  Und das ist alles!  Die Anwendung zieht niemanden mehr mit sich. </p><br><p>  Somit haben wir ein Werkzeug, um die Konfiguration von Ressourcen f√ºr Herde zu steuern und zu erzwingen. Jetzt scheinen wir sicher zu sein.  Also?  Nicht wirklich.  Tatsache ist, dass, wie bereits beschrieben, unsere Namespaces von Benutzern erstellt werden k√∂nnen und LimitRange daher m√∂glicherweise nicht in solchen Namespaces vorhanden ist, da sie in jedem Namespace separat erstellt werden m√ºssen.  Daher brauchen wir etwas nicht nur auf Namespace-Ebene, sondern auch auf Cluster-Ebene.  Eine solche Funktion gibt es in Kubernetes jedoch noch nicht. </p><br><p>  Deshalb habe ich beschlossen, meine L√∂sung f√ºr dieses Problem zu schreiben.  Lassen Sie mich Ihnen vorstellen - Limit Operator.  Dies ist ein Operator, der auf der Grundlage des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Operator SDK-</a> Frameworks erstellt wurde, das die benutzerdefinierte ClusterLimit-Ressource verwendet und dabei hilft, alle "unschuldigen" Anwendungen im Cluster zu sichern.  Mit diesem Operator k√∂nnen Sie die Standardwerte und Ressourcenlimits f√ºr alle Namespaces mit minimalem Konfigurationsaufwand steuern.  Au√üerdem k√∂nnen Sie mithilfe von NamespaceSelector genau ausw√§hlen, wo die Konfiguration angewendet werden soll. </p><br><div class="spoiler">  <b class="spoiler_title">Beispiel f√ºr ClusterLimit</b> <div class="spoiler_text"><pre><code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: default-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"limited"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"800m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"1Gi"</span></span> min: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"99Mi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"700m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"900Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"110m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"111Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Mit dieser Konfiguration erstellt der Operator einen LimitRange nur im Namespace mit der Bezeichnung <code>limit: limited</code> .  Dies ist n√ºtzlich, um strengere Einschr√§nkungen f√ºr eine bestimmte Gruppe von Namespaces bereitzustellen.  Wenn NamespaceSelector nicht angegeben ist, wendet der Operator einen LimitRange auf alle Namespaces an.  Wenn Sie LimitRange manuell f√ºr einen bestimmten Namespace konfigurieren m√∂chten, k√∂nnen Sie die Anmerkung <code>"limit.myafq.com/unlimited": true</code> wird der Bediener angewiesen, diesen Namespace zu √ºberspringen und LimitRange nicht automatisch zu erstellen. </p><br><p>  Beispielskript zur Verwendung des Operators: </p><br><ul><li>  Erstellen Sie Standard-ClusterLimit mit liberalen Einschr√§nkungen und ohne NamespaceSelector - es wird √ºberall angewendet. </li><li>  Erstellen Sie f√ºr eine Reihe von Namespaces mit kompakten Anwendungen ein zus√§tzliches, strengeres ClusterLimit mit NamespaceSelector.  F√ºgen Sie diesen Namespaces entsprechende Beschriftungen hinzu. </li><li>  F√ºgen Sie in einem Namespace mit sehr ressourcenintensiven Anwendungen die Anmerkung "limit.myafq.com/unlimited": true ein und konfigurieren Sie LimitRange manuell mit viel gr√∂√üeren Grenzwerten als im Standard-ClusteLimit angegeben. </li></ul><br><blockquote>  <strong>Das Wichtigste, was Sie √ºber mehrere LimitRange in einem Namespace wissen sollten:</strong> <br>  Wenn ein Sub in einem Namespace mit mehreren LimitRange erstellt wird, werden die gr√∂√üten Standardeinstellungen zum Konfigurieren seiner Ressourcen verwendet.  Die Maximal- und Minimalwerte werden jedoch gem√§√ü den strengsten Grenzwerten √ºberpr√ºft. </blockquote><br><h3 id="prakticheskiy-primer">  Praktisches Beispiel </h3><br><p>  Der Bediener verfolgt alle √Ñnderungen in allen Namespace-, ClusterLimits- und untergeordneten LimitRanges und initiiert die Koordination des Status des Clusters bei jeder √Ñnderung der √ºberwachten Objekte.  Mal sehen, wie es in der Praxis funktioniert. </p><br><p>  Erstellen Sie zun√§chst unter ohne Einschr√§nkungen: </p><br><div class="spoiler">  <b class="spoiler_title">kubectl run / get output</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash pod/bash created ‚ùØ() kubectl get pod bash -o yaml apiVersion: v1 kind: Pod metadata: labels: run: bash name: bash namespace: default spec: containers: - image: bash name: bash resources: {}</code> </pre> <br><p>  <em>Hinweis: Ein Teil der Befehlsausgabe wurde weggelassen, um das Beispiel zu vereinfachen.</em> </p></div></div><br><p>  Wie Sie sehen k√∂nnen, ist das Feld "Ressourcen" leer, was bedeutet, dass dieses Sub √ºberall gestartet werden kann. <br>  Jetzt erstellen wir das Standard-ClusterLimit f√ºr den gesamten Cluster mit ziemlich liberalen Werten: </p><br><div class="spoiler">  <b class="spoiler_title">default-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: default-limit spec: limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"4"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"5Gi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"700m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"900Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"500m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"512Mi"</span></span></code> </pre> </div></div><br><p>  Und auch strenger f√ºr eine Teilmenge von Namespaces: </p><br><div class="spoiler">  <b class="spoiler_title">restriktive-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: restrictive-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"800m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"1Gi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"128Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"50m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"64Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Erstellen Sie dann die Namespaces und Pods, um zu sehen, wie es funktioniert. <br>  Normaler Namespace mit Standardeinschr√§nkung: </p><br><pre> <code class="python hljs">apiVersion: v1 kind: Namespace metadata: name: regular</code> </pre> <br><p>  Und ein etwas eingeschr√§nkterer Namespace, der Legende nach - f√ºr leichte Anwendungen: </p><br><pre> <code class="python hljs">apiVersion: v1 kind: Namespace metadata: labels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> name: lightweight</code> </pre> <br><p>  Wenn Sie sich die Protokolle des Bedieners unmittelbar nach dem Erstellen des Namespace ansehen, finden Sie unter dem Spoiler Folgendes: </p><br><div class="spoiler">  <b class="spoiler_title">Bedienerprotokolle</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{...,"msg":"Reconciling ClusterLimit","Triggered by":"/regular"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"regular","LimitRange":"default-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"regular","Name":"default-limit"} {...,"msg":"Reconciling ClusterLimit","Triggered by":"/lightweight"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"lightweight","LimitRange":"default-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"lightweight","Name":"default-limit"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"lightweight","LimitRange":"restrictive-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"lightweight","Name":"restrictive-limit"}</code> </pre> <br><p>  <em>Der fehlende Teil des Protokolls enth√§lt 3 weitere Felder, die derzeit nicht relevant sind</em> </p></div></div><br><p>  Wie Sie sehen k√∂nnen, wurde bei der Erstellung jedes Namespace die Erstellung eines neuen LimitRange gestartet.  Ein eingeschr√§nkterer Namespace hat zwei LimitRange - Standard und strenger. </p><br><p>  Versuchen wir nun, ein Paar Herde in diesen Namespaces zu erstellen. </p><br><div class="spoiler">  <b class="spoiler_title">kubectl run / get output</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n regular pod/bash created ‚ùØ() kubectl get pod bash -o yaml -n regular apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container bash; cpu, memory limit for container bash' labels: run: bash name: bash namespace: regular spec: containers: - image: bash name: bash resources: limits: cpu: 700m memory: 900Mi requests: cpu: 500m memory: 512Mi</code> </pre> </div></div><br><p>  Wie Sie sehen, ist das Ressourcenfeld jetzt gef√ºllt, obwohl wir die Art und Weise, wie der Pod erstellt wird, nicht ge√§ndert haben.  M√∂glicherweise bemerken Sie auch die von LimitRanger automatisch erstellte Anmerkung. </p><br><p>  Erstellen Sie jetzt unter in einem leichten Namespace: </p><br><div class="spoiler">  <b class="spoiler_title">kubectl run / get output</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n lightweight pod/bash created ‚ùØ() kubectl get pods -n lightweight bash -o yaml apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container bash; cpu, memory limit for container bash' labels: run: bash name: bash namespace: lightweight spec: containers: - image: bash name: bash resources: limits: cpu: 700m memory: 900Mi requests: cpu: 500m memory: 512Mi</code> </pre> </div></div><br><p>  Bitte beachten Sie, dass die Ressourcen im Kamin dieselben sind wie im vorherigen Beispiel.  Dies liegt daran, dass bei mehreren LimitRange beim Erstellen von Pods weniger strenge Standardwerte verwendet werden.  Aber warum brauchen wir dann einen begrenzteren LimitRange?  Es wird verwendet, um die Maximal- und Minimalwerte von Ressourcen zu √ºberpr√ºfen.  Um dies zu demonstrieren, werden wir unser begrenztes ClusterLimit noch eingeschr√§nkter gestalten: </p><br><div class="spoiler">  <b class="spoiler_title">restriktive-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: restrictive-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"200m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"250Mi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"128Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"50m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"64Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Beachten Sie den Abschnitt: </p><br><pre> <code class="plaintext hljs">- type: Container max: cpu: "200m" memory: "250Mi"</code> </pre> <br><p>  Jetzt haben wir maximal 200m CPU und 250Mi Speicher f√ºr den Container im Kamin eingestellt.  Und jetzt versuchen Sie noch einmal zu erstellen unter: </p><br><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n lightweight Error from server (Forbidden): pods "bash" is forbidden: [maximum cpu usage per Container is 200m, but limit is 700m., maximum memory usage per Container is 250Mi, but limit is 900Mi.]</code> </pre> <br><p>  Unser Sub hat gro√üe Werte, die vom Standard-LimitRange festgelegt wurden, und konnte nicht gestartet werden, da es die maximal zul√§ssige Ressourcenpr√ºfung nicht bestanden hat. </p><br><hr><br><p>  Dies war ein Beispiel f√ºr die Verwendung des Limit-Operators.  Probieren Sie es selbst aus und spielen Sie mit ClusterLimit in Ihrer lokalen Kubernetes-Instanz. </p><br><p>  Im GitHub <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Limit Operator-Repository</a> finden Sie das Manifest f√ºr die Bereitstellung des Operators sowie den Quellcode.  Wenn Sie die Funktionalit√§t des Bedieners erweitern m√∂chten, sind Pull-Quests und Feature-Quests willkommen! </p><br><h3 id="zaklyuchenie">  Fazit </h3><br><p>  Das Ressourcenmanagement bei Kubernetes ist entscheidend f√ºr die Stabilit√§t und Zuverl√§ssigkeit Ihrer Anwendungen.  Passen Sie Ihre Herdressourcen nach M√∂glichkeit an.  Und verwenden Sie LimitRange, um sich gegen F√§lle zu versichern, in denen dies nicht m√∂glich ist.  Automatisieren Sie die Erstellung von LimitRange mit dem Limit Operator. </p><br><p>  Befolgen Sie diese Tipps, und Ihr Cluster ist immer vor dem ressourcenlosen Chaos streunender Herde gesch√ºtzt. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de473908/">https://habr.com/ru/post/de473908/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de473888/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 483 (15.10.2019 - 21.10.2019)</a></li>
<li><a href="../de473890/index.html">Operation Calypso: Neue APT-Gruppe greift Regierungsb√ºros weltweit an</a></li>
<li><a href="../de473894/index.html">Sechs Aufgaben f√ºr Front-End-Entwickler</a></li>
<li><a href="../de473904/index.html">Vivaldi 2.9 - Verbesserung Verbesserungen</a></li>
<li><a href="../de473906/index.html">7 kostenlose Kurse f√ºr Entwickler von Microsoft</a></li>
<li><a href="../de473910/index.html">Englischer Code</a></li>
<li><a href="../de473916/index.html">Geschichten aus dem Rechenzentrum: Halloween-Horrorgeschichten √ºber Diesel, Diplomatie und selbstschneidende Schrauben in der Heizung</a></li>
<li><a href="../de473918/index.html">Internetreisen in Russland: Geschwindigkeit und Chance</a></li>
<li><a href="../de473922/index.html">Selectel Networking Academy Konferenz</a></li>
<li><a href="../de473924/index.html">Nicht nur SMS und Token: Multi-Faktor-Authentifizierung basierend auf SafeNet Authentication Service</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>