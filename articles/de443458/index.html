<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ∏Ô∏è üë∂üèΩ üçÆ 6 unterhaltsame Systemfehler beim Betrieb von Kubernetes [und deren L√∂sung] ü•ù üìÜ ‚¨õÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Laufe der Jahre, in denen Kubernetes in der Produktion betrieben wurde, haben wir viele interessante Geschichten gesammelt, da Fehler in verschiede...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>6 unterhaltsame Systemfehler beim Betrieb von Kubernetes [und deren L√∂sung]</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/443458/"><img src="https://habrastorage.org/webt/7o/mz/o2/7omzo2vcqpqijlxsewel9gyhcsq.png"><br><br>  Im Laufe der Jahre, in denen Kubernetes in der Produktion betrieben wurde, haben wir viele interessante Geschichten gesammelt, da Fehler in verschiedenen Systemkomponenten zu unangenehmen und / oder unverst√§ndlichen Konsequenzen f√ºhrten, die sich auf den Betrieb von Containern und Pods auswirken.  In diesem Artikel haben wir einige der h√§ufigsten oder interessantesten ausgew√§hlt.  Selbst wenn Sie nie das Gl√ºck haben, auf solche Situationen zu sto√üen, ist das Lesen √ºber so kurze Detektive - umso mehr aus erster Hand - immer unterhaltsam, nicht wahr? <a name="habracut"></a><br><br><h2>  Geschichte 1. Supercronic und eiskalter Docker </h2><br>  Auf einem der Cluster erhielten wir regelm√§√üig einen "eingefrorenen" Docker, der die normale Funktion des Clusters beeintr√§chtigte.  Gleichzeitig wurde in den Docker-Protokollen Folgendes beobachtet <br><br><pre><code class="plaintext hljs">level=error msg="containerd: start init process" error="exit status 2: \"runtime/cgo: pthread_create failed: No space left on device SIGABRT: abort PC=0x7f31b811a428 m=0 goroutine 0 [idle]: goroutine 1 [running]: runtime.systemstack_switch() /usr/local/go/src/runtime/asm_amd64.s:252 fp=0xc420026768 sp=0xc420026760 runtime.main() /usr/local/go/src/runtime/proc.go:127 +0x6c fp=0xc4200267c0 sp=0xc420026768 runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc4200267c8 sp=0xc4200267c0 goroutine 17 [syscall, locked to thread]: runtime.goexit() /usr/local/go/src/runtime/asm_amd64.s:2086 +0x1 ‚Ä¶</code> </pre> <br>  In diesem Fehler interessiert uns am meisten die Meldung: <code>pthread_create failed: No space left on device</code> .  Eine kurze Untersuchung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> erkl√§rte, dass Docker den Prozess nicht verzweigen konnte, was dazu f√ºhrte, dass er regelm√§√üig ‚Äûeinfrierte‚Äú. <br><br>  Bei der √úberwachung des Geschehens entspricht das folgende Bild: <br><br><img src="https://habrastorage.org/webt/7r/cq/gv/7rcqgvafvtlmis1kl7maxyz5jgk.png"><br><br>  Eine √§hnliche Situation wird auf anderen Knoten beobachtet: <br><br><img src="https://habrastorage.org/webt/lj/mw/-h/ljmw-hrrlyukwgmigltivjwyxig.png"><br><br><img src="https://habrastorage.org/webt/ap/tw/5u/aptw5ufxl-9woo5zszegfg1nqvc.png"><br><br>  Auf denselben Knoten sehen wir: <br><br><pre> <code class="bash hljs">root@kube-node-1 ~ <span class="hljs-comment"><span class="hljs-comment"># ps auxfww | grep curl -c 19782 root@kube-node-1 ~ # ps auxfww | grep curl | head root 16688 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 17398 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16852 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 9473 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 4664 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 30571 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 24113 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 16475 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 7176 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt; root 1090 0.0 0.0 0 0 ? Z Feb06 0:00 | \_ [curl] &lt;defunct&gt;</span></span></code> </pre> <br>  Es stellte sich heraus, dass dieses Verhalten eine Folge der Arbeit des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pods</a> mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Supercronic ist</a> (dem Dienstprogramm auf Go, mit dem wir Cron-Aufgaben in Pods ausf√ºhren): <br><br><pre> <code class="plaintext hljs"> \_ docker-containerd-shim 833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 /var/run/docker/libcontainerd/833b60bb9ff4c669bb413b898a5fd142a57a21695e5dc42684235df907825567 docker-runc | \_ /usr/local/bin/supercronic -json /crontabs/cron | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true | | \_ /usr/bin/newrelic-daemon --agent --pidfile /var/run/newrelic-daemon.pid --logfile /dev/stderr --port /run/newrelic.sock --tls --define utilization.detect_aws=true --define utilization.detect_azure=true --define utilization.detect_gcp=true --define utilization.detect_pcf=true --define utilization.detect_docker=true -no-pidfile | \_ [newrelic-daemon] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; | \_ [curl] &lt;defunct&gt; ‚Ä¶</code> </pre> <br>  Das Problem ist folgendes: Wenn eine Aufgabe in Supercronic gestartet wird, kann der von ihr erzeugte Prozess <b>nicht korrekt abgeschlossen werden</b> und verwandelt sich in einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zombie</a> . <br><br>  <i><b>Hinweis</b> : Genauer gesagt werden Prozesse durch Cron-Tasks generiert. Supercronic ist jedoch kein Init-System und kann die Prozesse, die seine untergeordneten Elemente erzeugt haben, nicht ‚Äû√ºbernehmen‚Äú.</i>  <i>Wenn SIGHUP- oder SIGTERM-Signale auftreten, werden sie nicht an die erzeugten Prozesse √ºbertragen, wodurch die untergeordneten Prozesse nicht beendet werden und im Zombie-Status verbleiben.</i>  <i>Mehr dazu k√∂nnen Sie beispielsweise in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">solchen Artikel</a> lesen.</i> <br><br>  Es gibt verschiedene M√∂glichkeiten, Probleme zu l√∂sen: <br><br><ol><li>  Als vor√ºbergehende Problemumgehung: Erh√∂hen Sie die Anzahl der PIDs im System zu einem bestimmten Zeitpunkt: <br><br><pre> <code class="plaintext hljs"> /proc/sys/kernel/pid_max (since Linux 2.5.34) This file specifies the value at which PIDs wrap around (ie, the value in this file is one greater than the maximum PID). PIDs greater than this value are not allo‚Äê cated; thus, the value in this file also acts as a system-wide limit on the total number of processes and threads. The default value for this file, 32768, results in the same range of PIDs as on earlier kernels</code> </pre> </li><li>  Oder starten Sie Aufgaben in Supercronic nicht direkt, sondern mit Hilfe derselben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tini</a> , die Prozesse korrekt beenden und keinen Zombie generieren kann. </li></ol><br><h2>  Verlauf 2. "Zombies" beim Entfernen von cgroup </h2><br>  Kubelet verbrauchte viel CPU: <br><br><img src="https://habrastorage.org/webt/ns/lh/mp/nslhmpwfnmennya-btg5icbkh8e.png"><br><br>  Niemand mag das, also haben wir uns mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Perf</a> bewaffnet und angefangen, uns mit dem Problem zu befassen.  Die Ergebnisse der Untersuchung waren wie folgt: <br><br><ul><li>  Kubelet verbringt mehr als ein Drittel der CPU-Zeit damit, Speicherdaten aus allen Gruppen abzurufen: <br><br><img src="https://habrastorage.org/webt/yf/u7/qv/yfu7qvvcnryl5iknz4zosagh0is.png"></li><li>  In der Mailingliste der Kernelentwickler finden Sie eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diskussion des Problems</a> .  Kurz gesagt, das Fazit ist, dass <b>verschiedene tmpfs-Dateien und √§hnliche Dinge nicht vollst√§ndig aus dem System entfernt werden,</b> wenn cgroup gel√∂scht wird - der sogenannte <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">memcg-</a> Zombie</b> bleibt bestehen.  Fr√ºher oder sp√§ter werden sie immer noch aus dem Seiten-Cache entfernt, aber der Speicher auf dem Server ist gro√ü und der Kernel sieht keinen Grund, Zeit zu verschwenden.  Daher sammeln sie sich weiter an.  Warum passiert das √ºberhaupt?  Dies ist ein Server mit Cron-Jobs, der st√§ndig neue Jobs und mit ihnen neue Pods erstellt.  Daher werden neue cgroups f√ºr Container in ihnen erstellt, die bald gel√∂scht werden. </li><li>  Warum verbringt cAdvisor in kubelet so viel Zeit?  Dies l√§sst sich leicht an der einfachsten Ausf√ºhrung der <code>time cat /sys/fs/cgroup/memory/memory.stat</code> .  Wenn der Vorgang auf einer fehlerfreien Maschine 0,01 Sekunden dauert, auf einer problematischen cron02 1,2 Sekunden.  Die Sache ist, dass cAdvisor, der Daten aus sysfs sehr langsam liest, versucht, den verwendeten Speicher auch in Zombie-Gruppen zu ber√ºcksichtigen. </li><li>  Um Zombies gewaltsam zu entfernen, haben wir versucht, die Caches zu l√∂schen, wie in LKML empfohlen: <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code>  <code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code> , aber der Kernel erwies sich als komplizierter und lie√ü die Maschine h√§ngen. </li></ul><br>  Was tun?  Das Problem wird behoben ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Commit</a> und Beschreibung siehe Versionsmeldung), indem der Linux-Kernel auf Version 4.16 aktualisiert wird. <br><br><h2>  Verlauf 3. Systemd und sein Mount </h2><br>  Auch hier verbraucht Kubelet auf einigen Knoten zu viele Ressourcen, aber diesmal ist es bereits Speicher: <br><br><img src="https://habrastorage.org/webt/ud/l3/vl/udl3vlr9r5c6hzxoamdmnzvvm5m.png"><br><br>  Es stellte sich heraus, dass das in Ubuntu 16.04 verwendete Systemd ein Problem aufweist. Es tritt auf, wenn die Mounts gesteuert werden, die zum Verbinden von <code>subPath</code> aus ConfigMaps oder Geheimnissen erstellt werden.  Nach Abschluss des Pods <b>verbleiben</b> der <b>systemd-Dienst und sein Service-Mount</b> auf dem System.  Im Laufe der Zeit sammeln sie eine gro√üe Menge an.  Es gibt sogar Probleme zu diesem Thema: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kops # 5916</a> ; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kubernetes # 57345</a> . </li></ol><br>  ... in letzterem wird auf PR in systemd verwiesen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="># 7811</a> (Problem in systemd ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="># 7798</a> ). <br><br>  Das Problem liegt nicht mehr in Ubuntu 18.04, aber wenn Sie Ubuntu 16.04 weiterhin verwenden m√∂chten, kann unsere Problemumgehung zu diesem Thema hilfreich sein. <br><br>  Also haben wir das folgende DaemonSet erstellt: <br><br><pre> <code class="plaintext hljs">--- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: systemd-slices-cleaner name: systemd-slices-cleaner namespace: kube-system spec: updateStrategy: type: RollingUpdate selector: matchLabels: app: systemd-slices-cleaner template: metadata: labels: app: systemd-slices-cleaner spec: containers: - command: - /usr/local/bin/supercronic - -json - /app/crontab Image: private-registry.org/systemd-slices-cleaner/systemd-slices-cleaner:v0.1.0 imagePullPolicy: Always name: systemd-slices-cleaner resources: {} securityContext: privileged: true volumeMounts: - name: systemd mountPath: /run/systemd/private - name: docker mountPath: /run/docker.sock - name: systemd-etc mountPath: /etc/systemd - name: systemd-run mountPath: /run/systemd/system/ - name: lsb-release mountPath: /etc/lsb-release-host imagePullSecrets: - name: antiopa-registry priorityClassName: cluster-low tolerations: - operator: Exists volumes: - name: systemd hostPath: path: /run/systemd/private - name: docker hostPath: path: /run/docker.sock - name: systemd-etc hostPath: path: /etc/systemd - name: systemd-run hostPath: path: /run/systemd/system/ - name: lsb-release hostPath: path: /etc/lsb-release</code> </pre> <br>  ... und es verwendet das folgende Skript: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # we will work only on xenial hostrelease="/etc/lsb-release-host" test -f ${hostrelease} &amp;&amp; grep xenial ${hostrelease} &gt; /dev/null || exit 0 # sleeping max 30 minutes to dispense load on kube-nodes sleep $((RANDOM % 1800)) stoppedCount=0 # counting actual subpath units in systemd countBefore=$(systemctl list-units | grep subpath | grep "run-" | wc -l) # let's go check each unit for unit in $(systemctl list-units | grep subpath | grep "run-" | awk '{print $1}'); do # finding description file for unit (to find out docker container, who born this unit) DropFile=$(systemctl status ${unit} | grep Drop | awk -F': ' '{print $2}') # reading uuid for docker container from description file DockerContainerId=$(cat ${DropFile}/50-Description.conf | awk '{print $5}' | cut -d/ -f6) # checking container status (running or not) checkFlag=$(docker ps | grep -c ${DockerContainerId}) # if container not running, we will stop unit if [[ ${checkFlag} -eq 0 ]]; then echo "Stopping unit ${unit}" # stoping unit in action systemctl stop $unit # just counter for logs ((stoppedCount++)) # logging current progress echo "Stopped ${stoppedCount} systemd units out of ${countBefore}" fi done</span></span></code> </pre> <br>  ... und es beginnt alle 5 Minuten mit dem bereits erw√§hnten Supercronic.  Sein Dockerfile sieht folgenderma√üen aus: <br><br><pre> <code class="plaintext hljs">FROM ubuntu:16.04 COPY rootfs / WORKDIR /app RUN apt-get update &amp;&amp; \ apt-get upgrade -y &amp;&amp; \ apt-get install -y gnupg curl apt-transport-https software-properties-common wget RUN add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable" &amp;&amp; \ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - &amp;&amp; \ apt-get update &amp;&amp; \ apt-get install -y docker-ce=17.03.0* RUN wget https://github.com/aptible/supercronic/releases/download/v0.1.6/supercronic-linux-amd64 -O \ /usr/local/bin/supercronic &amp;&amp; chmod +x /usr/local/bin/supercronic ENTRYPOINT ["/bin/bash", "-c", "/usr/local/bin/supercronic -json /app/crontab"]</code> </pre> <br><h2>  Geschichte 4. Wettbewerb bei der Planung von Pods </h2><br>  Es wurde Folgendes festgestellt: Wenn ein Pod auf unserem Knoten platziert wird und sein Bild f√ºr eine sehr lange Zeit abgepumpt wird, <b>beginnt</b> der andere Pod, der zum selben Knoten " <b>gelangt</b> " ist, einfach <b>nicht, das Bild des neuen Pods zu ziehen</b> .  Stattdessen wartet er darauf, dass das Bild des vorherigen Pods gezogen wird.  Infolgedessen wird ein bereits geplanter Pod, dessen Bild in nur einer Minute heruntergeladen werden kann, f√ºr lange Zeit in den Status <code>containerCreating</code> . <br><br>  In Veranstaltungen wird es so etwas geben: <br><br><pre> <code class="plaintext hljs">Normal Pulling 8m kubelet, ip-10-241-44-128.ap-northeast-1.compute.internal pulling image "registry.example.com/infra/openvpn/openvpn:master"</code> </pre> <br>  Es stellt sich heraus, dass <b>ein einzelnes Image aus der langsamen Registrierung die Bereitstellung</b> auf dem Knoten <b>blockieren kann</b> . <br><br>  Leider gibt es nicht so viele Auswege: <br><br><ol><li>  Versuchen Sie, Ihre Docker-Registrierung direkt im Cluster oder direkt mit dem Cluster zu verwenden (z. B. GitLab-Registrierung, Nexus usw.). </li><li>  Verwenden Sie Dienstprogramme wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kraken</a> . </li></ol><br><h2>  Verlauf 5. H√§ngende Knoten mit nicht gen√ºgend Speicher </h2><br>  W√§hrend des Betriebs verschiedener Anwendungen haben wir auch eine Situation erhalten, in der der Knoten nicht mehr zug√§nglich ist: SSH reagiert nicht, alle √úberwachungsd√§monen fallen ab und dann ist nichts (oder fast nichts) in den Protokollen abnormal. <br><br>  Ich werde Ihnen auf den Bildern am Beispiel eines Knotens sagen, auf dem MongoDB funktioniert hat. <br><br>  So sieht atop <b>vor dem</b> Absturz aus: <br><br><img src="https://habrastorage.org/webt/l5/ef/az/l5efazbhjmzsuxdv6puz1tlvbzs.png"><br><br>  Und so - <b>nach dem</b> Unfall: <br><br><img src="https://habrastorage.org/webt/wx/mh/q7/wxmhq71060dhvxsxk-dh8m--pas.png"><br><br>  Auch bei der √úberwachung gibt es einen scharfen Sprung, bei dem der Knoten nicht mehr zug√§nglich ist: <br><br><img src="https://habrastorage.org/webt/tp/fm/wf/tpfmwftsui_eoz91ojyy5rzektc.png"><br><br>  So zeigen die Screenshots, dass: <br><br><ol><li>  Der Arbeitsspeicher des Computers ist fast zu Ende. </li><li>  Es wird ein starker Anstieg des RAM-Verbrauchs beobachtet, wonach der Zugriff auf die gesamte Maschine stark deaktiviert wird. </li><li>  Bei Mongo kommt eine gro√üe Aufgabe an, die den DBMS-Prozess dazu zwingt, mehr Speicher zu verwenden und aktiv von der Festplatte zu lesen. </li></ol><br>  Es stellt sich heraus, dass, wenn Linux keinen freien Speicher mehr hat (Speicherdruck tritt auf) und kein Swap stattfindet, <b>bevor</b> der OOM-Killer eintrifft, ein Gleichgewicht zwischen dem Werfen von Seiten in den Seitencache und dem Zur√ºckschreiben auf die Festplatte auftreten kann.  Dies geschieht durch kswapd, das mutig so viele Seiten Speicher wie m√∂glich f√ºr die sp√§tere Verteilung freigibt. <br><br>  Leider wird <b>kswapd</b> bei einer gro√üen E / A-Last in Verbindung mit einer geringen Menge an freiem Speicher <b>zum Engpass des gesamten Systems</b> , da <b>alle</b> Seitenfehler der Speicherseiten im System damit verbunden sind.  Dies kann sehr lange dauern, wenn die Prozesse keinen Speicher mehr verwenden m√∂chten, sondern am √§u√üersten Rand des OOM-Killer-Abgrunds fixiert sind. <br><br>  Die logische Frage ist: Warum kommt der OOM-Killer so sp√§t?  In der aktuellen OOM-Iteration ist Killer extrem dumm: Er beendet den Prozess nur, wenn der Versuch, eine Speicherseite zuzuweisen, fehlschl√§gt, d. H.  wenn der Seitenfehler fehlschl√§gt.  Dies geschieht lange Zeit nicht, da kswapd mutig Seiten des Speichers freigibt, indem der Seitencache (tats√§chlich alle Festplatten-E / A im System) auf die Festplatte zur√ºckgesp√ºlt wird.  Ausf√ºhrlicher mit einer Beschreibung der Schritte, die erforderlich sind, um solche Probleme im Kernel zu beseitigen, k√∂nnen Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> lesen. <br><br>  Dieses Verhalten <a href="">sollte sich</a> mit dem Linux 4.6+ Kernel <a href="">verbessern</a> . <br><br><h2>  Story 6. Pods stehen noch aus </h2><br>  In einigen Clustern, in denen es wirklich viele Pods gibt, haben wir festgestellt, dass die meisten von ihnen im Status " <code>Pending</code> sehr lange h√§ngen bleiben, obwohl gleichzeitig die Docker-Container selbst bereits auf den Knoten ausgef√ºhrt werden und Sie manuell mit ihnen arbeiten k√∂nnen. <br><br>  Es ist nichts Falsches daran zu <code>describe</code> : <br><br><pre> <code class="plaintext hljs"> Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned sphinx-0 to ss-dev-kub07 Normal SuccessfulAttachVolume 1m attachdetach-controller AttachVolume.Attach succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "sphinx-config" Normal SuccessfulMountVolume 1m kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "default-token-fzcsf" Normal SuccessfulMountVolume 49s (x2 over 51s) kubelet, ss-dev-kub07 MountVolume.SetUp succeeded for volume "pvc-6aaad34f-ad10-11e8-a44c-52540035a73b" Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx-exporter/sphinx-indexer:v1" already present on machine Normal Created 43s kubelet, ss-dev-kub07 Created container Normal Started 43s kubelet, ss-dev-kub07 Started container Normal Pulled 43s kubelet, ss-dev-kub07 Container image "registry.example.com/infra/sphinx/sphinx:v1" already present on machine Normal Created 42s kubelet, ss-dev-kub07 Created container Normal Started 42s kubelet, ss-dev-kub07 Started container</code> </pre> <br>  Nach dem St√∂bern gingen wir davon aus, dass kubelet einfach keine Zeit hat, dem API-Server alle Informationen √ºber den Status der Pods, Lebendigkeit / Bereitschaftsproben zu senden. <br><br>  Nachdem wir die Hilfe studiert hatten, fanden wir die folgenden Parameter: <br><br><pre> <code class="plaintext hljs">--kube-api-qps - QPS to use while talking with kubernetes apiserver (default 5) --kube-api-burst - Burst to use while talking with kubernetes apiserver (default 10) --event-qps - If &gt; 0, limit event creations per second to this value. If 0, unlimited. (default 5) --event-burst - Maximum size of a bursty event records, temporarily allows event records to burst to this number, while still not exceeding event-qps. Only used if --event-qps &gt; 0 (default 10) --registry-qps - If &gt; 0, limit registry pull QPS to this value. --registry-burst - Maximum size of bursty pulls, temporarily allows pulls to burst to this number, while still not exceeding registry-qps. Only used if --registry-qps &gt; 0 (default 10)</code> </pre> <br>  Wie Sie sehen k√∂nnen, sind die <b>Standardwerte recht klein</b> und decken in 90% alle Anforderungen ab ... In unserem Fall war dies jedoch nicht ausreichend.  Deshalb setzen wir folgende Werte: <br><br><pre> <code class="plaintext hljs">--event-qps=30 --event-burst=40 --kube-api-burst=40 --kube-api-qps=30 --registry-qps=30 --registry-burst=40</code> </pre> <br><br>  ... und startete die Kubelets neu, woraufhin sie das folgende Bild in den Diagrammen des Zugriffs auf den API-Server sahen: <br><br><img src="https://habrastorage.org/webt/nq/-i/oq/nq-ioqoyt6_qudmacm5dwfe8hnk.png"><br><br>  ... und ja, alles begann zu fliegen! <br><br><h2>  PS </h2><br>  F√ºr die Hilfe beim Sammeln von Fehlern und bei der Vorbereitung des Artikels m√∂chte ich den zahlreichen Ingenieuren unseres Unternehmens und insbesondere Andrei Klimentyev (Kollege aus unserem Forschungs- und Entwicklungsteam) ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">zuzzas</a> ) meinen tiefen Dank <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">aussprechen</a> . <br><br><h2>  PPS </h2><br>  Lesen Sie auch in unserem Blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubectl-Debug-Plugin zum Debuggen in Kubernetes-Pods</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Monitoring und Kubernetes (Review und Video Report)</a> "; </li><li>  Kubernetes Tipps &amp; Tricks Zyklus: <ul><li>  ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úbertragung von Ressourcen, die in einem Cluster arbeiten, an das Helm 2-Management</a> ‚Äú; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úber die Zuweisung von Knoten und die Belastung der Webanwendung</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zugang zu Dev-Sites</a> "; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschleunigen Sie den Bootstrap gro√üer Datenbanken.</a> " </li></ul></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443458/">https://habr.com/ru/post/de443458/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443438/index.html">7 n√ºtzliche Firefox-Erweiterungen zum Englischlernen</a></li>
<li><a href="../de443440/index.html">PHP-Modul zum Arbeiten mit hierarchischen Daten in InterSystems IRIS</a></li>
<li><a href="../de443450/index.html">Warum die Armen nicht gesund sein k√∂nnen</a></li>
<li><a href="../de443452/index.html">Das russische Milit√§r wird ein eigenes geschlossenes Internet schaffen</a></li>
<li><a href="../de443456/index.html">Wir laden Sie f√ºr eine Woche zu Yandex NLP ein</a></li>
<li><a href="../de443460/index.html">11 Antworten zu Yandex.Directory</a></li>
<li><a href="../de443462/index.html">Hacking-Kameras: Angriffsmethoden, Tools zur Suche nach Sicherheitsl√ºcken und Anti-Tracking</a></li>
<li><a href="../de443464/index.html">Vollst√§ndige Anleitung zum Wechseln von Ausdr√ºcken in Java 12</a></li>
<li><a href="../de443466/index.html">Entwicklungsk√∂nig</a></li>
<li><a href="../de443468/index.html">Welche Netzwerk√ºberwachungstools sind in der Version von Gartner f√ºhrend geworden?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>