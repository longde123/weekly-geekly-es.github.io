<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçü§ù‚Äçüë®üèø üßùüèø ‚õëÔ∏è Cache ist der K√∂nig der Leistung: Brauchen Prozessoren ein viertes Caching-Level? üë∑üèª üîñ üòà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die L√ºcke zwischen der Geschwindigkeit der Prozessoren im allgemeinen Sinne und der Geschwindigkeit des Haupt-DRAM im allgemeinen Sinne war in den let...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cache ist der K√∂nig der Leistung: Brauchen Prozessoren ein viertes Caching-Level?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485374/"><img src="https://habrastorage.org/getpro/habr/post_images/2ac/73c/219/2ac73c219095e9b2a9f772e767ba9cb7.jpg"><br><br>  Die L√ºcke zwischen der Geschwindigkeit der Prozessoren im allgemeinen Sinne und der Geschwindigkeit des Haupt-DRAM im allgemeinen Sinne war in den letzten 30 Jahren ein Problem - in dieser Zeit begann sich die L√ºcke wirklich zu vergr√∂√üern.  Und es ist ehrlich zu sagen, dass die Ingenieure, die sowohl die Ausr√ºstung als auch die Programme entwickelt haben, die die Cache-Hierarchie erstellt haben, und die Software, die sie nutzen kann, einfach brillant waren.  Dies ist eine der schwierigsten Architekturen, die der Mensch je erdacht hat. <br><br>  Jetzt, da wir uns jedoch am Rande einer stetig wachsenden Speicherhierarchie befinden, erscheinen nichtfl√ºchtige Speicher wie Optane 3D XPoint (eine Speichervariante mit Phasenwechsel) in den Formaten DIMM und SSD sowie neue Protokolle (CXL, OpenCAPI, CCIX, NVLink und Gen-Z) stellt sich die Frage: Ist es an der Zeit, den Servern einen Cache der vierten Ebene hinzuzuf√ºgen?  Da die Arbeit einer solchen Anzahl von Ger√§ten vom CPU-Komplex abh√§ngt - von denen einige n√§her liegen, w√§hrend andere weiter entfernt sind - ist es logisch zu √ºberlegen, ob wir eine weitere Cache-Ebene ben√∂tigen, die die Verz√∂gerungen dieser anderen Speichertypen √ºberdeckt und den Durchsatz des gesamten Systems erh√∂ht. <br><a name="habracut"></a><br>  Um die M√∂glichkeiten vorzustellen, st√∂berten wir in unserem eigenen Ged√§chtnis und sprachen gleichzeitig mit Chip-Architektur-Entwicklern von IBM, Intel, AMD und Marvell, um zu verstehen, was sie √ºber die Verwendung des L4-Cache in Servern denken.  L4-Cache ist nat√ºrlich kein neues Wort f√ºr Geschwindigkeit, aber es ist in Systemarchitekturen nicht so verbreitet. <br><br>  Bevor wir jedoch auf die Geschichte des Problems eingehen sollten. <br><br>  Das Hinzuf√ºgen eines Cache der ersten Ebene zu Prozessoren, die zu diesem Zeitpunkt nur einen Kern hatten, war ein Kompromiss in den 1980er Jahren, der die Latenz von Speichersubsystemen erh√∂hte und gleichzeitig die durchschnittliche Latenz von Datenanforderungen und Anweisungen von Prozessoren verringerte.  L1-Caches befanden sich urspr√ºnglich im externen SRAM auf den Motherboards und waren mit dem CPU-Speicherkomplex verbunden.  Ein solcher L1-Cache war sowohl hinsichtlich der Taktfrequenz als auch des physischen Platzes auf der Platine sehr nahe am Prozessor und erm√∂glichte die Erh√∂hung der CPU-Last.  Dann wurden diese Caches aufgeteilt, so dass h√§ufig verwendete Daten in einem Block und beliebte Anweisungen im zweiten Block gespeichert werden konnten, was die Leistung leicht erh√∂hte.  Irgendwann bei der Erh√∂hung der Prozessortaktrate und der entsprechenden L√ºcke in der Geschwindigkeit von CPU und DRAM wurden mehr fette, aber auch langsamere L2-Caches hinzugef√ºgt (aber bandbreitenm√§√üig g√ºnstiger), die sich zun√§chst wieder au√üerhalb des CPU-Geh√§uses befanden, aber dann in sie integriert.  Und als mehr und mehr Kerne zur CPU hinzugef√ºgt wurden, sowie mehr und mehr DRAM-Controller, um sie zu laden, wurden der Hierarchie noch gr√∂√üere L3-Cache-Bl√∂cke hinzugef√ºgt. <br><br>  Ein solches System funktionierte gr√∂√ütenteils recht gut.  In einigen CPU-Schaltkreisen sehen wir sogar bestimmte praktische Regeln, die die Ebenen der Cache-Hierarchie widerspiegeln, wodurch wir die mit der vierten Ebene verbundenen M√∂glichkeiten absch√§tzen k√∂nnen. <br><br>  Chris Gianos, ein Chip-Ingenieur und Architekt bei Intel, der die Entwicklung vieler Generationen von Xeon-Prozessoren vorangetrieben hat, erkl√§rt dies folgenderma√üen: ‚ÄûMit jeder Cache-Ebene m√ºssen wir in der Regel stark genug aus der vorherigen Ebene herauswachsen, damit sich all dies lohnt Um die Systemleistung sp√ºrbar zu steigern, m√ºssen Sie eine interessante H√§ufigkeit erfolgreicher Anrufe erreichen.  Wenn Sie nur in wenigen Prozent der F√§lle auf zwischengespeicherte Daten zugreifen, ist dies nur schwer zu erkennen.  Alles andere verlangsamt Ihre Geschwindigkeit, und diese Steigerung wird nicht wahrnehmbar sein.  Daher sind relativ gro√üe Caches erforderlich, und wenn es um h√∂here Ebenen geht, sind wirklich gro√üe Caches erforderlich.  Heute wird L2 in Megabyte gemessen, L3 in Dutzenden oder Hunderten von Megabyte.  Wenn Sie also √ºber den L4-Cache nachdenken, sprechen wir von Hunderten von Megabyte, wenn nicht von Gigabyte.  Und eine solche Gr√∂√üe wird definitiv zu hohen Kosten f√ºhren.  Es ist notwendig, dass bestimmte Bedingungen vorliegen, damit diese Option interessant wird, und sie wird sicherlich nicht billig sein. ‚Äú <br><br>  Die von uns befragten AMD-Ingenieure wollten anonym bleiben, weil sie nicht den Eindruck erwecken wollten, dass das Unternehmen den L4-Cache in die Epyc-Prozessorlinie aufnehmen w√ºrde - und genau gesagt, AMD versprach so etwas nicht.  Das Unternehmen erkennt jedoch, dass dies der n√§chste naheliegende Schritt ist, den es in Betracht zu ziehen gilt, und ist genau wie bei Intel der Ansicht, dass alle Ingenieure √ºber die Implementierung des L4-Caches nachdenken.  Laut AMD wurden die mit Cache-Levels und Latenzen verbundenen Kompromisse sowohl in der Industrie als auch im akademischen Bereich eingehend untersucht. Mit jedem neuen Level, das gr√∂√üer und langsamer als das vorherige ist, ist ein Kompromiss bei der Erh√∂hung des Gesamtwegs zum DRAM verbunden.  Dies wird auch von Intel Gianos angegeben, die √ºber die Notwendigkeit sprechen, ein Gleichgewicht zwischen erfolgreichen Cache-Anforderungen und seinem Volumen zu finden. <br><br>  IBM hat in den 2000er Jahren einigen seiner X86-Chips√§tze L4-Cache hinzugef√ºgt und in den 2010er Jahren NUMA-Chips√§tzen ( <a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access" rel="nofollow">ungleichm√§√üiger Speicherzugriff</a> ) auf System z11-Mainframes L4 hinzugef√ºgt.  Der z11-Prozessor verf√ºgt √ºber vier Kerne, einen 64-KB-L1-Cache f√ºr Anweisungen und einen 128-KB-L1-Cache f√ºr Daten sowie einen 1,5-MB-L2-Cache f√ºr jeden Kern und einen gemeinsam genutzten 24-MB-L3-Cache f√ºr alle Kerne.  Der NUMA-Chipsatz f√ºr z10 verf√ºgte √ºber zwei B√§nke mit 96 MB L4-Cache, dh insgesamt 192 MB.  Durch die Freigabe von z12 reduzierte IBM die Gr√∂√üe des L1-Caches auf 98 KB pro Kern, erh√∂hte jedoch den L2-Cache auf 2 MB pro Kern und teilte ihn wie im Fall von L1 in zwei Teile f√ºr Anweisungen und Daten auf.  Sie hat auch die Gr√∂√üe des L3-Cache f√ºr sechs Kerne auf 48 MB verdoppelt und die Gr√∂√üe des L4-Cache f√ºr ein Paar Chips im Chipsatz auf 384 MB erh√∂ht.  Mit dem Wechsel der Generationen von System z-Prozessoren wuchsen die Cache-Gr√∂√üen. F√ºr die im September angek√ºndigten z15-Prozessoren wiegen zwei L1-Caches jeweils 128 KB, zwei L2-Caches jeweils 4 MB und ein gemeinsam genutzter L3-Cache f√ºr 256 Kerne hat eine Kapazit√§t von 256 MB.  Der L4-Cache in jedem Mainframe-Schacht ist 960 MB gro√ü, und das Gesamtvolumen f√ºr das gesamte System, das aus f√ºnf Sch√§chten besteht, betr√§gt 4,68 GB. <br><br>  Wie <a href="https://www.nextplatform.com/2018/08/28/ibm-power-chips-blur-the-lines-to-memory-and-accelerators/" rel="nofollow">bereits erw√§hnt</a> , haben Power8- und Power9-Prozessoren Arbeitsspeicher gepuffert, und IBM f√ºgte jedem Centaur-Puffer 16 MB L4-Cache hinzu, was 128 MB L4-Cache pro Socket f√ºr 32 Speichersteckpl√§tze entspricht.  Die billigsten Power9-Maschinen haben keinen Speicherpuffer und daher keinen L4-Cache.  Die Architekten, die das Power10-Diagramm entwickelten, waren mit der Entwicklung des Diagramms f√ºr Power11 besch√§ftigt und konnten daher unsere Fragen nicht beantworten. William Stark, der die Entwicklung von Power10 leitete, fand jedoch etwas Zeit f√ºr uns und stellte Folgendes fest: <br><br>  "Im Allgemeinen sind wir zu dem Schluss gekommen, dass Caches der letzten Ebene auf gro√üer Ebene n√ºtzlich sind, um die Geschwindigkeit industrieller Systeme zu erh√∂hen", erkl√§rte Stark uns per E-Mail.  "Die mit dem nichtfl√ºchtigen Speicher verbundene hohe Latenz, insbesondere mit dem Phasenzustandsspeicher, erzeugt eine Cache-Anforderung - m√∂glicherweise f√ºr einen Cache vom Typ L4 - in der Hierarchie des Speichers." <br><br>  Genau das dachten wir.  √úbrigens behaupten wir nicht, dass sich der L4-Cache notwendigerweise in unmittelbarer N√§he des gepufferten Speichers des zuk√ºnftigen DDR5-DIMM befindet.  Vielleicht ist es besser, es zwischen dem PCI-Express- und dem L3-Prozessor-Cache und noch besser in den Speicherpuffern und zwischen dem PCI-Express- und dem L3-Prozessor-Cache zu platzieren.  M√∂glicherweise muss es in der zuk√ºnftigen Serverarchitektur, die <a href="https://www.nextplatform.com/2018/12/13/intel-bets-heavily-on-chip-stacking-for-the-future-of-compute/" rel="nofollow">der Foveros-Technologie von Intel √§hnelt</a> , auf dem E / A-Controller und dem Speicher <a href="https://www.nextplatform.com/2018/12/13/intel-bets-heavily-on-chip-stacking-for-the-future-of-compute/" rel="nofollow">platziert werden</a> . <br><br>  Es ist m√∂glich, dies aus einer anderen Perspektive zu betrachten - IBM hatte beispielsweise die M√∂glichkeit, die Gr√∂√üe des Kristalls zu √§ndern, und die Ingenieure entschieden sich, den L4-Cache dem System-z-NUMA-Bus oder dem Power8- und Power9-Speicherpuffer-Chip hinzuzuf√ºgen, und zwar nicht aus eigenem Grund, sondern nur deswegen Sie hatten immer noch die M√∂glichkeit, Transistoren hinzuzuf√ºgen, nachdem alle erforderlichen Funktionen implementiert wurden.  Manchmal scheint es uns, dass die Anzahl der Kerne in Intel X86-Prozessoren von der Gr√∂√üe des L3-Caches abh√§ngt, den sie sich leisten k√∂nnen.  Es scheint manchmal, dass Intel einem Kristall die maximale Gr√∂√üe des L3-Caches zuweist, und danach werden Xeon-Kristalle in drei verschiedenen Gr√∂√üen einfach gem√§√ü diesen Spezifikationen hergestellt - in den neuesten Generationen haben sie 10, 18 oder 28 Kerne in einem 14-nm-Herstellungsprozess. <br><br>  All dies sind nat√ºrlich rein akademische Fragen, aber sie geben uns die potenzielle Motivation f√ºr IBM und andere Chipsatzhersteller, den L4-Cache hinzuzuf√ºgen.  Dies kann nicht nur in einigen F√§llen helfen, es ist auch eine ziemlich offensichtliche Sache.  Wir glauben, dass auf einem solchen E / A-Monster wie dem System z-Mainframe der L4-Cache ohne Fragen an seinem Platz ist und allen Kunden zugutekommt, indem er den Durchsatz dieser Computer erh√∂ht und ihnen erm√∂glicht, bei 98-99% Prozessorauslastung zu arbeiten, da wie viele Kerne vorhanden sind , und der Umfang von NUMA in Gro√ürechnern ist in letzter Zeit stark gewachsen. <br><br>  Es gibt keinen Grund, den L4-Cache ausschlie√ülich auf dem eingebauten DRAM (wie IBM es bei seinen Chips tut) oder auf der Basis eines viel teureren SRAM zu erstellen - daran erinnert uns Rabin Sugumar, Chip-Architekt von Cray Research, Sun Microsystems, Oracle, Broadcom , Cavium und Marvell: <br><br>  "Unsere L3-Caches sind bereits gro√ü genug", sagt Sugumar.  - L4 muss f√ºr den Fall, dass Sie interessiert sind, mit einer anderen Technologie ausgef√ºhrt werden.  Vielleicht eDRAM oder sogar HBM oder DRAM.  In diesem Zusammenhang scheint eine Implementierung eines HBM-basierten L4-Cache eine interessante Option zu sein, und dieser Cache l√∂st weniger das Latenzproblem als vielmehr die Bandbreite.  Da die HBM-Kapazit√§t begrenzt und die Bandbreite gro√ü ist, k√∂nnen wir eine gewisse Geschwindigkeitssteigerung erzielen - und in einigen Sonderf√§llen sehen wir tats√§chlich eine deutliche Erh√∂hung der Bandbreite. "  Sugumar f√ºgt hinzu, dass f√ºr eine relativ gro√üe Anzahl von Anwendungen eine relativ gro√üe Anzahl von Cache-Fehlern beobachtet wird.  Sie m√ºssen jedoch berechnen, ob sich das Hinzuf√ºgen der n√§chsten Cache-Ebene lohnt. <br><br>  Ein anderer m√∂glicher Anwendungsfall f√ºr so etwas wie den L4-Cache, sagt Sugumar, ist die Verwendung eines lokalen DRAM als Cache.  ‚ÄûWir f√ºhren keine derartigen Untersuchungen im Labor durch, nehmen jedoch an, dass auf dem Chip eine Schnittstelle mit hoher Bandbreite vorhanden ist, die mit einem gemeinsam genutzten Speicher in einem Abstand von 500 ns bis 1 Œºs am anderen Ende der Schleife verbunden ist.  Dann besteht einer der Anwendungsf√§lle darin, einen Cache zu erstellen, der diese Daten aus dem gemeinsam genutzten Speicher in den lokalen DRAM verschiebt.  Sie k√∂nnen sich die Arbeit der Zustandsmaschine vorstellen, die diesen Speicher verwaltet, so dass die Anrufe die meiste Zeit an den lokalen DRAM gehen und Sie die Anzahl der Anrufe an den allgemeinen verteilten DRAM minimieren k√∂nnen. <br><br>  Diese Option scheint uns eine sehr interessante Art von NUMA zu sein.  √úbrigens arbeitete Sugumar bereits vor dem Erscheinen des nichtfl√ºchtigen Speichers an verteiltem Speicher f√ºr parallele Hochgeschwindigkeitssysteme in Sun Microsystems.  Und eines der Probleme bei diesen verschiedenen Varianten der Speicherhierarchie war, dass die gesamte Maschine abst√ºrzt, wenn eine von ihnen aufgrund eines Netzwerk- oder Busausfalls verloren geht.  ‚ÄûAuf verteilten Speichersystemen m√ºssen Netzwerkfehler eleganter behandelt werden, was viele Designherausforderungen mit sich bringt.‚Äú <br><br>  Ein weiterer Punkt ist, dass jeder High-Level-Cache, nicht einmal L4, mit Hilfe von Eisen maximal und mit Hilfe von Software minimal realisiert werden soll.  Die Betriebssystemkerne und andere Software ben√∂tigen immer etwas Zeit, um mit der Hardware Schritt zu halten, unabh√§ngig davon, ob neue Kernel, L3- oder L4-Caches oder ein adressierbarer nichtfl√ºchtiger Speicher hinzugef√ºgt werden. <br><br>  "Irgendwann wird eine zus√§tzliche Cache-Ebene unvermeidlich", sagt Gianos.  - Wir haben die erste Cache-Ebene und irgendwann erschien die zweite.  Und dann haben wir endlich eine dritte hinzugef√ºgt.  Und eines Tages werden wir einen vierten haben.  Die Frage ist nur wann und warum.  Und es scheint mir, dass Ihre Beobachtungen in Bezug auf die F√§higkeiten dieses Caches ziemlich interessant sind.  Aber Intel hat noch nicht entschieden, wann oder warum solche Dinge ver√∂ffentlicht werden.  Andere Unternehmen besch√§ftigen sich ebenfalls mit diesem Thema.  es w√§re dumm, es nicht zu untersuchen.  Fr√ºher oder sp√§ter wird das passieren, aber bald wird es sein oder nicht sehr - es ist noch nicht klar. " </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485374/">https://habr.com/ru/post/de485374/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485358/index.html">Ist mobile Entwicklung einfach und langweilig? Yandex-Bericht</a></li>
<li><a href="../de485362/index.html">JavaScript ES2020 Innovationen mit einfachen Beispielen</a></li>
<li><a href="../de485364/index.html">So verschwenden Sie keine Zeit mehr mit technischen Schulden</a></li>
<li><a href="../de485370/index.html">Wie kann ein Entwickler einem Manager helfen, einen Deal abzuschlie√üen?</a></li>
<li><a href="../de485372/index.html">√úber die Unver√§nderlichkeit: Geschichte des 9. Platzes des russischen AI Cup 2019</a></li>
<li><a href="../de485376/index.html">Wie wird das Frontend dreimal schneller und wann werden Befehle anstelle von Repositorys angewendet? Video</a></li>
<li><a href="../de485378/index.html">Fallstudie: So werden Sie bei Google Play vorgestellt und passen ASO an verschiedene L√§nder an</a></li>
<li><a href="../de485380/index.html">Handwerk und IT-Erfolg</a></li>
<li><a href="../de485384/index.html">NeurIPS 2019: ML-Trends, die uns das n√§chste Jahrzehnt begleiten werden</a></li>
<li><a href="../de485386/index.html">Microbrowsers sind √ºberall. Aber was wissen wir √ºber sie?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>