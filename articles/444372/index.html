<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø üë©üèΩ‚Äçü§ù‚Äçüë©üèª üë≥üèø Orientaci√≥n a m√°quina a larga distancia utilizando aprendizaje reforzado üë©üèº‚Äçüé® üë©üèΩ‚Äçü§ù‚Äçüë®üèæ üßúüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Solo en los Estados Unidos, hay 3 millones de personas con discapacidad que no pueden abandonar sus hogares. Los robots auxiliares que pueden navegar ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Orientaci√≥n a m√°quina a larga distancia utilizando aprendizaje reforzado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444372/"> Solo en los Estados Unidos, hay 3 millones de personas con discapacidad que no pueden abandonar sus hogares.  Los robots auxiliares que pueden navegar autom√°ticamente largas distancias pueden hacer que estas personas sean m√°s independientes al llevarles alimentos, medicinas y paquetes.  Los estudios muestran que el aprendizaje profundo con refuerzo (OP) es muy adecuado para comparar datos de entrada sin procesar y acciones, por ejemplo, para aprender a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">capturar objetos</a> o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mover robots</a> , pero generalmente los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">agentes</a> OP carecen de la comprensi√≥n de los grandes espacios f√≠sicos necesarios para una orientaci√≥n segura a larga distancia distancias sin ayuda humana y adaptaci√≥n a un nuevo entorno. <br><a name="habracut"></a><br>  En tres trabajos recientes, " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Entrenamiento de orientaci√≥n desde cero con AOP</a> ", " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">PRM-RL: Implementaci√≥n de orientaci√≥n rob√≥tica a largas distancias utilizando una combinaci√≥n de aprendizaje por refuerzo y planificaci√≥n basada en patrones</a> " y " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Orientaci√≥n a</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">largo alcance con PRM-RL</a> ", Estudiamos robots aut√≥nomos que se adaptan f√°cilmente a un nuevo entorno, combinando OP profundo con planificaci√≥n a largo plazo.  Ense√±amos a los agentes de planificaci√≥n locales c√≥mo realizar las acciones b√°sicas necesarias para la orientaci√≥n y c√≥mo moverse distancias cortas sin colisiones con objetos en movimiento.  Los planificadores locales realizan observaciones ambientales ruidosas utilizando sensores como lidares unidimensionales que proporcionan distancia a un obst√°culo y proporcionan velocidades lineales y angulares para controlar el robot.  Capacitamos al planificador local en simulaciones utilizando el aprendizaje de refuerzo autom√°tico (AOP), un m√©todo que automatiza la b√∫squeda de recompensas para el OP y la arquitectura de la red neuronal.  A pesar del alcance limitado de 10-15 m, los planificadores locales se adaptan bien tanto para su uso en robots reales como para entornos nuevos y previamente desconocidos.  Esto le permite usarlos como bloques de construcci√≥n para orientarse en espacios grandes.  Luego construimos una hoja de ruta, un gr√°fico donde los nodos son secciones separadas, y los bordes conectan los nodos solo si los planificadores locales, imitando bien a los robots reales que usan sensores y controles ruidosos, pueden moverse entre ellos. <br><br><h2>  Aprendizaje autom√°tico de refuerzo (AOP) </h2><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">nuestro primer trabajo,</a> capacitamos a un planificador local en un peque√±o entorno est√°tico.  Sin embargo, cuando se aprende con el algoritmo est√°ndar de OP profunda, por ejemplo, el gradiente determinista profundo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DDPG</a> ), existen varios obst√°culos.  Por ejemplo, el objetivo real de los planificadores locales es lograr un objetivo determinado, por lo que reciben recompensas poco frecuentes.  En la pr√°ctica, esto requiere que los investigadores pasen un tiempo considerable en la implementaci√≥n paso a paso del algoritmo y el ajuste manual de los premios.  Los investigadores tambi√©n tienen que tomar decisiones sobre la arquitectura de las redes neuronales sin tener recetas claras y exitosas.  Finalmente, los algoritmos como DDPG aprenden de manera inestable y a menudo exhiben <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">olvidos catastr√≥ficos</a> . <br><br>  Para superar estos obst√°culos, automatizamos el aprendizaje profundo con refuerzo.  AOP es un envoltorio autom√°tico evolutivo alrededor de un OP profundo, que busca recompensas y arquitectura de red neuronal a trav√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">de la optimizaci√≥n de hiperpar√°metros a gran escala</a> .  Funciona en dos etapas, la b√∫squeda de recompensas y la b√∫squeda de arquitectura.  Durante la b√∫squeda de recompensas, AOP entrena simult√°neamente a la poblaci√≥n de agentes DDPG durante varias generaciones, y cada uno tiene su propia funci√≥n de recompensa ligeramente modificada, optimizada para la verdadera tarea del planificador local: llegar al punto final del camino.  Al final de la fase de b√∫squeda de recompensas, seleccionamos uno que con mayor frecuencia lleva a los agentes a la meta.  En la fase de b√∫squeda de la arquitectura de la red neuronal, repetimos este proceso, utilizando el premio seleccionado para esta carrera y ajustando las capas de la red, optimizando el premio acumulativo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP con la b√∫squeda de premios y arquitectura de la red neuronal</i> <br><br>  Sin embargo, este proceso paso a paso hace que AOP sea ineficaz en t√©rminos del n√∫mero de muestras.  ¬°El entrenamiento AOP con 10 generaciones de 100 agentes requiere 5 mil millones de muestras, lo que equivale a 32 a√±os de estudio!  La ventaja es que despu√©s del AOP, el proceso de aprendizaje manual est√° automatizado y DDPG no tiene olvidos catastr√≥ficos.  Lo que es m√°s importante, la calidad de las pol√≠ticas finales es mayor: son resistentes al ruido del sensor, la unidad y la localizaci√≥n, y est√°n bien generalizadas a nuevos entornos.  Nuestra mejor pol√≠tica es un 26% m√°s exitosa que otros m√©todos de orientaci√≥n en nuestros sitios de prueba. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Rojo - AOP tiene √©xito a distancias cortas (hasta 10 m) en varios edificios previamente desconocidos.</i>  <i>Comparaci√≥n con DDPG entrenado manualmente (rojo oscuro), campos de potencial artificial (azul), ventana din√°mica (azul) y clonaci√≥n de comportamiento (verde).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>La pol√≠tica del planificador de AOP local funciona bien con robots en entornos reales no estructurados</i> <br><br>  Y aunque estos pol√≠ticos solo son capaces de orientaci√≥n local, son resistentes a obst√°culos en movimiento y son bien tolerados por robots reales en entornos no estructurados.  Y aunque fueron entrenados en simulaciones con objetos est√°ticos, efectivamente hacen frente a los que se mueven.  El siguiente paso es combinar las pol√≠ticas de AOP con la planificaci√≥n basada en muestras para expandir su √°rea de trabajo y ense√±arles a navegar largas distancias. <br><br><h2>  Orientaci√≥n a larga distancia con PRM-RL </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los planificadores basados ‚Äã‚Äãen patrones</a> trabajan con orientaci√≥n de largo alcance, aproximando los movimientos del robot.  Por ejemplo, un robot construye <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">hojas de ruta probabil√≠sticas</a> (PRM) al dibujar rutas de transici√≥n entre secciones.  En nuestro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">segundo trabajo</a> , que gan√≥ el premio en la conferencia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICRA 2018</a> , combinamos PRM con programadores OP locales sintonizados manualmente (sin AOP) para entrenar robots localmente y luego adaptarlos a otros entornos. <br><br>  Primero, para cada robot, entrenamos la pol√≠tica del planificador local en una simulaci√≥n generalizada.  Luego creamos un PRM teniendo en cuenta esta pol√≠tica, el llamado PRM-RL, basado en un mapa del entorno donde se utilizar√°.  La misma tarjeta se puede usar para cualquier robot que deseamos usar en el edificio. <br><br>  Para crear un PRM-RL, combinamos nodos de muestras solo si el programador de OP local puede moverse de manera confiable y repetida entre ellos.  Esto se hace en una simulaci√≥n de Monte Carlo.  El mapa resultante se adapta a las capacidades y la geometr√≠a de un robot en particular.  Las tarjetas para robots con la misma geometr√≠a, pero con diferentes sensores y unidades, tendr√°n una conectividad diferente.  Dado que el agente puede girar alrededor de la esquina, los nodos que no est√°n en l√≠nea directa tambi√©n pueden activarse.  Sin embargo, los nodos adyacentes a las paredes y los obst√°culos tendr√°n menos probabilidades de ser incluidos en el mapa debido al ruido del sensor.  En tiempo de ejecuci√≥n, el agente de OP se mueve por el mapa de una secci√≥n a otra. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>Se crea un mapa con tres simulaciones de Monte Carlo para cada par de nodos seleccionados al azar</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>El mapa m√°s grande ten√≠a un tama√±o de 288x163 m y conten√≠a casi 700,000 bordes.</i>  <i>300 trabajadores lo recogieron durante 4 d√≠as, despu√©s de haber realizado 1.100 millones de controles de colisi√≥n.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El tercer trabajo</a> proporciona varias mejoras al PRM-RL original.  En primer lugar, estamos reemplazando el DDPG sintonizado manualmente con programadores locales de AOP, lo que mejora la orientaci√≥n a largas distancias.  En segundo lugar, se agregan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mapas de localizaci√≥n y marcado simult√°neo</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SLAM</a> ), que los robots usan en tiempo de ejecuci√≥n como fuente para construir mapas de ruta.  Las tarjetas SLAM est√°n sujetas a ruido, y esto cierra la "brecha entre el simulador y la realidad", un problema bien conocido en rob√≥tica, debido a que los agentes entrenados en simulaciones se comportan mucho peor en el mundo real.  Nuestro nivel de √©xito en la simulaci√≥n coincide con el nivel de √©xito de los robots reales.  Y finalmente, agregamos mapas de construcci√≥n distribuidos, para que podamos crear mapas muy grandes que contengan hasta 700,000 nodos. <br><br>  Evaluamos este m√©todo con la ayuda de nuestro agente de AOP, que cre√≥ mapas basados ‚Äã‚Äãen dibujos de edificios que exced√≠an el entorno de capacitaci√≥n 200 veces en el √°rea, incluidas solo costillas, que se completaron con √©xito en el 90% de los casos en 20 intentos.  Comparamos PRM-RL con varios m√©todos a distancias de hasta 100 m, que excedieron seriamente el alcance del planificador local.  PRM-RL logr√≥ el √©xito 2-3 veces m√°s a menudo que los m√©todos convencionales debido a la conexi√≥n correcta de los nodos, adecuados para las capacidades del robot. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>Tasa de √©xito en el movimiento de 100 m en diferentes edificios.</i>  <i>Azul: planificador local de AOP, primer trabajo;</i>  <i>rojo - PRM original;</i>  <i>amarillo - campos de potencial artificial;</i>  <i>el verde es el segundo trabajo;</i>  <i>rojo - el tercer trabajo, PRM con AOP.</i> <br><br>  Probamos PRM-RL en muchos robots reales en muchos edificios.  A continuaci√≥n se muestra una de las suites de prueba;  el robot se mueve de manera confiable en casi todos lados, excepto en los lugares m√°s desordenados y √°reas que van m√°s all√° de la tarjeta SLAM. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Conclusi√≥n </h2><br>  La orientaci√≥n de la m√°quina puede aumentar seriamente la independencia de las personas con discapacidades de movilidad.  Esto se puede lograr desarrollando robots aut√≥nomos que puedan adaptarse f√°cilmente al entorno y los m√©todos disponibles para la implementaci√≥n en el nuevo entorno basados ‚Äã‚Äãen la informaci√≥n existente.  Esto se puede hacer automatizando el entrenamiento de orientaci√≥n b√°sica para distancias cortas con AOP, y luego utilizando las habilidades adquiridas junto con las tarjetas SLAM para crear hojas de ruta.  Las hojas de ruta consisten en nodos conectados por costillas, sobre los cuales los robots pueden moverse de manera confiable.  Como resultado, se desarrolla una pol√≠tica de comportamiento del robot que, despu√©s de un entrenamiento, puede usarse en diferentes entornos y emitir hojas de ruta especialmente adaptadas para un robot en particular. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/444372/">https://habr.com/ru/post/444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444362/index.html">Unity y Havok trabajan en un nuevo motor de f√≠sica</a></li>
<li><a href="../444364/index.html">24 horas de juego Rust: experiencia de desarrollo personal</a></li>
<li><a href="../444366/index.html">Seminario "Requisitos de seguridad de la informaci√≥n: c√≥mo las empresas pueden vivir con ellos"</a></li>
<li><a href="../444368/index.html">Acabamos de imprimir el micr√≥fono en una impresora 3D en el laboratorio, y luego habr√° ciencia ficci√≥n completa</a></li>
<li><a href="../444370/index.html">¬øDe qu√© es capaz el formato Mini PCI-e?</a></li>
<li><a href="../444374/index.html">Efecto hipster: por qu√© los inconformistas a menudo se ven iguales</a></li>
<li><a href="../444376/index.html">La econom√≠a de la atenci√≥n est√° casi muerta.</a></li>
<li><a href="../444378/index.html">USPACE - Espacio √∫nico para aeronaves tripuladas y no tripuladas</a></li>
<li><a href="../444382/index.html">C√≥mo visitar la Universidad de Corea con el sistema de archivos de red</a></li>
<li><a href="../444384/index.html">Libro "An√°lisis de datos de texto aplicado en Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>