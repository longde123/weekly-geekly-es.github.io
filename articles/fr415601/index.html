<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🌱 🚢 👩🏼‍🏭 Cluster Kubernetes HA avec containerd. Ou y a-t-il une vie sans docker? 🐌 😋 🥣</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Déployer kubernetes HA avec containerd 



 Bonjour chers lecteurs de Habr! Le 24 mai 2018, un article intitulé Kubernetes Containerd Integration Goes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster Kubernetes HA avec containerd. Ou y a-t-il une vie sans docker?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd">  Déployer kubernetes HA avec containerd </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br>  Bonjour chers lecteurs de Habr!  Le 24 mai 2018, un article intitulé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kubernetes Containerd Integration Goes GA a</a> été publié sur le blog officiel de Kubernetes, qui indique que l'intégration de containerd avec Kubernetes est prête pour la production.  En outre, des gars de la société Flant ont publié une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traduction de l'article en russe sur</a> leur blog, ajoutant une petite clarification d'eux-mêmes.  Après avoir lu la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet sur github</a> , j'ai décidé d'essayer containerd sur "ma propre peau". </p><br><p>  Notre entreprise a plusieurs projets au stade de "encore très loin de la production".  Ils deviendront donc notre expérimental;  pour eux, nous avons décidé d'essayer de déployer un cluster de basculement de Kubernetes à l'aide de containerd et de voir s'il y a de la vie sans docker. </p><br><p>  Si vous êtes intéressé de voir comment nous l'avons fait et ce qui en est arrivé, bienvenue chez cat. </p><a name="habracut"></a><br><p>  <b>Description du schéma et du déploiement</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p>  Lors du déploiement d'un cluster, comme d'habitude, (j'ai écrit à ce sujet dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article précédent</a> </p><div class="spoiler">  <b class="spoiler_title">keepalived - implémentations de VRRP (Virtual Router Redundancy Protocol) pour Linux</b> <div class="spoiler_text">  Keepalived crée une IP virtuelle (VIRTIP) qui "pointe" (crée une sous-interface) vers l'IP de l'un des trois maîtres.  Le démon keepalived surveille la santé des machines et, en cas de panne, exclut le serveur défaillant de la liste des serveurs actifs en basculant VIRTIP vers l'IP d'un autre serveur, selon le "poids" spécifié lors de la configuration de keepalived sur chaque serveur. </div></div><br><p>  Les démons Keepalived communiquent via VRRP, s'envoyant des messages à l'adresse 224.0.0.18. </p><br><p>  Si le voisin n'a pas envoyé son message, après la période, il est considéré comme mort.  Dès que le serveur en panne commence à envoyer ses messages au réseau, tout revient à sa place </p><br><p>  Nous configurons le travail avec le serveur API sur les nœuds kubernetes comme suit. </p><br><p>  Après avoir installé le cluster, configurez kube-proxy, changez le port de 6443 en 16443 (détails ci-dessous).  Sur chacun des maîtres, nginx est déployé, qui fonctionne comme un équilibreur de charge, écoute sur le port 16443 et fait un amont des trois maîtres sur le port 6443 (détails ci-dessous). </p><br><p>  Ce schéma a permis d'augmenter la tolérance aux pannes en utilisant keepalived, ainsi qu'en utilisant nginx, un équilibrage entre les serveurs API sur les assistants a été atteint. <br></p><br><p>  Dans un article précédent, j'ai décrit le déploiement de nginx et etcd dans docker.  Mais dans ce cas, nous n'avons pas de docker, donc nginx et etcd fonctionneront localement sur les masternodes. </p><br><p>  Théoriquement, il serait possible de déployer nginx et etcd en utilisant containerd, mais en cas de problème, cette approche compliquerait le diagnostic, nous avons donc décidé de ne pas l'expérimenter et de l'exécuter localement. </p><br><p>  <b>Description des serveurs à déployer:</b> </p><br><table><thead><tr><th>  Nom </th><th>  IP </th><th>  Les services </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ </td></tr><tr><td>  kube-master01 </td><td>  172.26.133.161 </td><td>  kubeadm, kubelet, kubectl, etcd, containerd, nginx, keepalived </td></tr><tr><td>  kube-master02 </td><td>  172.26.133.162 </td><td>  kubeadm, kubelet, kubectl, etcd, containerd, nginx, keepalived </td></tr><tr><td>  kube-master03 </td><td>  172.26.133.163 </td><td>  kubeadm, kubelet, kubectl, etcd, containerd, nginx, keepalived </td></tr><tr><td>  kube-node01 </td><td>  172.26.133.164 </td><td>  kubeadm, kubelet, kubectl, containerd </td></tr><tr><td>  kube-node02 </td><td>  172.26.133.165 </td><td>  kubeadm, kubelet, kubectl, containerd </td></tr><tr><td>  kube-node03 </td><td>  172.26.133.166 </td><td>  kubeadm, kubelet, kubectl, containerd </td></tr></tbody></table><br><p>  <b>Installer kubeadm, kubelet, kubectl et les packages associés</b> </p><br><p>  Toutes les commandes s'exécutent à partir de root </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>Installer conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>Configuration des configurations containerd</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p>  Ajouter au fichier: </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  On démarre conteinerd on vérifie que tout va bien </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service └─10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>Installer et exécuter etcd</b> </p><br><p>  Remarque importante, j'ai installé la version 1.10 du cluster kubernetes.  Quelques jours plus tard, au moment de la rédaction de l'article, la version 1.11 était disponible. Si vous installez la version 1.11, définissez la variable ETCD_VERSION = "v3.2.17", si 1.10 alors ETCD_VERSION = "v3.1.12". </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p>  Copiez les configurations de gitahab. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p>  Configurez les variables dans le fichier de configuration. </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Description des variables du fichier create-config.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p>  paramètres sur la machine locale de chaque nœud (chaque nœud a le sien) <br>  <b>K8SHA_IPLOCAL</b> - Adresse IP du noeud sur lequel le script est configuré <br>  <b>K8SHA_ETCDNAME</b> - nom de la machine locale dans le cluster ETCD <br>  <b>K8SHA_KA_STATE</b> - rôle dans keepalived.  Un nœud MASTER, tous les autres BACKUP. <br>  <b>K8SHA_KA_PRIO</b> - priorité <b>persistante</b> , le maître a 102 pour les 101, 100 restants. Lorsque le maître avec le numéro 102 tombe, le nœud avec le numéro 101 prend sa place et ainsi de suite. <br>  <b>K8SHA_KA_INTF</b> - Interface réseau keepalived.  Le nom de l'interface que keepalived écoutera. </p><br><p>  Les paramètres généraux pour tous les masternodes sont les mêmes: </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160 - IP virtuelle du cluster. <br>  <b>K8SHA_IP1 ... K8SHA_IP3 -</b> Adresses <b>IP</b> des maîtres <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3</b> - noms d'hôte pour les masternodes.  Un point important, par ces noms, kubeadm générera des certificats. <br>  <b>K8SHA_KA_AUTH</b> - mot de passe pour keepalived.  Vous pouvez spécifier tout <br>  <b>K8SHA_TOKEN</b> - jeton de cluster.  Peut être généré avec la commande <b>kubeadm token generate</b> <br>  <b>K8SHA_CIDR</b> - adresse de sous-réseau pour les foyers.  J'utilise de la flanelle donc CIDR 0.244.0.0/16.  Assurez-vous de filtrer - dans la configuration devrait être K8SHA_CIDR = 10.244.0.0 \ / 16 </p></div></div><br><p>  Exécutez le script qui configurera nginx, keepalived, etcd et kubeadmin </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p>  Nous commençons etcd. </p><br>  etcd j'ai soulevé sans tls.  Si vous avez besoin de tls, alors dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle de kubernetes,</a> il est écrit en détail comment générer des certificats pour etcd. <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p>  Vérification de l'état </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Si tout va bien, passez à l'étape suivante. </p><br><p>  <b>Configurer kubeadmin</b> <br>  Si vous utilisez kubeadm version 1.11, vous pouvez ignorer cette étape <br>  Pour que kybernetes commence à fonctionner non pas avec docker, mais avec containerd, configurez la configuration kubeadmin </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  Après [Service], ajoutez une ligne au bloc </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">La configuration entière devrait ressembler à ceci:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p>  Si vous installez la version 1.11 et souhaitez tester CoreDNS au lieu de kube-dns et tester la configuration dynamique, décommentez le bloc suivant dans le fichier de configuration kubeadm-init.yaml: </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Redémarrez le kubelet </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>Initialisation du premier assistant</b> </p><br><p>  Avant de démarrer kubeadm, vous devez redémarrer keepalived et vérifier son état </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ├─4590 /usr/sbin/keepalived ├─4591 /usr/sbin/keepalived ├─4593 /usr/sbin/keepalived ├─5222 /usr/sbin/keepalived ├─5223 sh -c /etc/keepalived/check_apiserver.sh ├─5224 /bin/bash /etc/keepalived/check_apiserver.sh └─5231 sleep 5</code> </pre> <br><p>  vérifier si VIRTIP pings </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p>  Après cela, exécutez kubeadmin.  Assurez-vous d'inclure la ligne --skip-preflight-checks.  Kubeadmin recherche par défaut Docker et sans ignorer les vérifications échouera avec une erreur. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  Une fois que kubeadm a fonctionné, enregistrez la ligne générée.  Il sera nécessaire d'entrer des nœuds de travail dans le cluster. </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p>  Ensuite, indiquez où le fichier admin.conf est stocké <br>  Si nous travaillons en tant que root, alors: </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p>  Pour un utilisateur simple, suivez les instructions à l'écran. </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p>  Ajoutez 2 assistants supplémentaires au cluster.  Pour ce faire, copiez les certificats de kube-master01 vers kube-master02 et kube-master03 dans le répertoire / etc / kubernetes /.  Pour ce faire, j'ai configuré l'accès ssh pour root, et après avoir copié les fichiers, j'ai renvoyé les paramètres. </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p>  Après avoir copié vers kube-master02 et kube-master03, exécutez. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>Installer la flanelle CIDR</b> </p><br><p>  sur kube-master01 exécuter </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p>  La version actuelle de flanel se trouve dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation de kubernetes</a> . </p><br><p>  Nous attendons que tous les conteneurs soient créés. </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>Nous faisons la réplication de kube-dns aux trois maîtres</b> </p><br><p>  Sur kube-master01 exécuter </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>Installer et configurer nginx</b> </p><br><p>  Sur chaque nœud maître, installez nginx en tant qu'équilibreur pour l'API Kubernetes <br>  J'ai toutes les machines du cluster sur Debian.  Parmi les packages nginx, le module de flux ne prend pas en charge, alors ajoutez les référentiels nginx et installez-le à partir des référentiels nginx`a.  Si vous avez un système d'exploitation différent, consultez la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation nginx</a> . </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p>  Créer une configuration nginx (si ce n'est déjà fait) </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p>  user nginx; <br>  worker_processes auto; </p><br><p>  error_log /var/log/nginx/error.log warn; <br>  pid /var/run/nginx.pid; </p><br><p>  événements { <br>  travailleurs_connexions 1024; <br>  } </p><br><p>  http { <br>  inclure /etc/nginx/mime.types; <br>  application par défaut_type / flux d'octets; </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  } </p><br><p>  stream { <br>  apiserver en amont { <br>  serveur 172.26.133.161:6443 poids = 5 max_fails = 3 fail_timeout = 30s; <br>  serveur 172.26.133.162:6443 poids = 5 max_fails = 3 fail_timeout = 30s; <br>  serveur 172.26.133.163:6443 poids = 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  } </p></div></div><br><p>  Nous vérifions que tout va bien et appliquons la configuration </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ● nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ├─22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf └─22134 nginx: worker process</code> </pre> <br><p>  Testez l'équilibreur </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>Configurer kube-proxy pour fonctionner avec l'équilibreur</b> </p><br><p>  Une fois l'équilibreur configuré, modifiez le port dans les paramètres kubernetes. </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p>  Modifiez les paramètres du serveur en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://172.26.133.160:16443</a> <br>  Ensuite, vous devez configurer kube-proxy pour fonctionner avec le nouveau port </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p>  Nous supprimons tous les pods, après leur suppression, ils sont automatiquement recréés avec les nouveaux paramètres </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>Ajout de nœuds de travail au cluster</b> </p><br><p>  Sur chaque note fondamentale, exécutez la commande générée par kubeadm </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p>  Si la ligne est "perdue", vous devez générer un nouveau </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p>  Sur les nœuds de travail dans les fichiers /etc/kubernetes/bootstrap-kubelet.conf et /etc/kubernetes/kubelet.conf <br>  valeur variable du serveur à notre virtip </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p>  Et redémarrez containerd et kubernetes </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>Installation du tableau de bord</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p>  Créez un utilisateur avec des privilèges d'administrateur: </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p>  Nous obtenons le jeton d'entrée: </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p>  Configuration de l'accès au tableau de bord via NodePort sur VIRTIP </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p>  Nous remplaçons la valeur de type: ClusterIP par type: NodePort et dans la section port: ajoutez la valeur de nodePort: 30000 (ou le port dans la plage de 30000 à 32000 sur lequel vous souhaitez que le panneau soit accessible): </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p>  Le panel est désormais disponible sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https: // VIRTIP: 30000</a> </p><br><p>  <b>Heapster</b> </p><br><p>  Ensuite, installez Heapster, un outil pour obtenir des métriques des composants du cluster. </p><br><p>  Installation: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>Conclusions</b> </p><br><p>  Je n'ai remarqué aucun problème particulier lors de l'utilisation de containerd.  Une fois, il y a eu un problème incompréhensible avec un foyer après le retrait du déploiement.  Kubernetes croyait que under avait été supprimé, mais under était devenu un "zombie" si particulier. Il restait à exister sur le nœud, mais dans le statut étendu. </p><br><p>  Je crois que Containerd est plus orienté en tant que runtime de conteneur pour kubernetes.  Très probablement, à l'avenir, en tant qu'environnement de lancement de microservices dans Kubernetes, il sera possible et nécessaire d'utiliser différents environnements qui seront orientés pour différentes tâches, projets, etc. </p><br><p>  Le projet se développe très rapidement.  Alibaba Cloud a commencé à utiliser activement conatinerd et souligne qu'il s'agit de l'environnement idéal pour l'exécution de conteneurs. </p><br><p>  Selon les développeurs, l'intégration de containerd dans la plateforme cloud de Google Kubernetes équivaut désormais à l'intégration de Docker. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un bon exemple de l'utilitaire de console crictl</a> .  Je vais également donner quelques exemples du cluster créé: </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  La CLI Docker n'a pas les concepts de base de Kubernetes, par exemple, pod et espace de noms, tandis que crictl prend en charge ces concepts </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p>  Et si nécessaire, nous pouvons regarder les conteneurs au format habituel, comme docker </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p>  Nous pouvons voir les images qui se trouvent sur le nœud </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p>  En fait, la vie sans docker est :) </p><br><p>  Il est trop tôt pour parler de bugs et de bugs, le cluster travaille avec nous depuis environ une semaine.  Dans un avenir proche, le test lui sera transféré, et en cas de succès, très probablement le stand de développement de l'un des projets.  Il y a une idée à ce sujet pour écrire une série d'articles couvrant les processus DevOps, tels que: créer un cluster, configurer un contrôleur d'entrée et le déplacer vers des nœuds de cluster séparés, automatiser l'assemblage d'images, vérifier les images pour les vulnérabilités, le déploiement, etc.  En attendant, nous examinerons la stabilité, chercherons des bugs et développerons de nouveaux produits. </p><br><p>  En outre, ce manuel convient au déploiement d'un cluster de basculement avec docker, vous devez uniquement installer docker conformément aux instructions de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle de Kubernetes</a> et ignorer les étapes pour installer containerd et configurer la configuration kubeadm. </p><br><p>  Ou vous pouvez mettre containerd et docker simultanément sur le même hôte et, comme les développeurs le garantissent, ils fonctionneront parfaitement ensemble.  Containerd est l'environnement de lancement du conteneur konbernetes, et docker est comme docker))) </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br>  Le référentiel containerd possède <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un playbook</a> pour la configuration d'un cluster à assistant unique.  Mais il était plus intéressant pour moi de «soulever» le système de mes mains afin de comprendre plus en détail la configuration de chaque composant et de comprendre comment il fonctionne dans la pratique. <br><p>  Peut-être qu'un jour mes mains atteindront et j'écrirai mon livre de jeu pour déployer un cluster avec HA, car au cours des six derniers mois, j'en ai déployé plus d'une douzaine et il serait probablement temps d'automatiser le processus. </p><br><p>  De plus, lors de la rédaction de cet article, la version kubernetes 1.11 a été publiée.  Vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lire</a> les principales modifications <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le blog Flant</a> ou sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">blog officiel de kubernetes</a> .  Nous avons mis à jour les clusters de test vers la version 1.11 et remplacé kube-dns par CoreDNS.  De plus, nous avons inclus la fonction DynamicKubeletConfig pour tester les capacités de mise à jour dynamique des configurations. </p><br><p>  Matériaux utilisés: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'intégration de Kubernetes Containerd passe à GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'intégration Kubernetes Container remplace Docker prêt pour la production</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">containerd github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation de Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation NGINX</a> </li></ul><br><p>  Merci d'avoir lu jusqu'au bout. </p><br><p>  Étant donné que les informations sur kubernetes, en particulier sur les clusters fonctionnant en conditions réelles, sont très rares dans RuNet, les indications d'inexactitudes sont les bienvenues, tout comme les commentaires sur le schéma général de déploiement des clusters.  J'essaierai de les prendre en compte et d'apporter les corrections appropriées.  Et je suis toujours prêt à répondre aux questions dans les commentaires, sur githab et dans tous les réseaux sociaux indiqués dans mon profil. </p><br><p>  Cordialement, Eugene. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr415601/">https://habr.com/ru/post/fr415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr415591/index.html">Apple Engineers Trap Piège à clavier MacBook Pro</a></li>
<li><a href="../fr415593/index.html">6 ans plus tard, une nouvelle version de la légendaire distribution de crash Hiren's BootCD</a></li>
<li><a href="../fr415595/index.html">Méthodes pour augmenter la rétention des joueurs en utilisant les jeux SLOT comme exemple: Partie 1</a></li>
<li><a href="../fr415597/index.html">Postfix - amavisd-new sans localhost ni serveur de messagerie d'une nouvelle façon</a></li>
<li><a href="../fr415599/index.html">L'Inde veut également obtenir de l'hélium-3</a></li>
<li><a href="../fr415605/index.html">Comment nous avons économisé le traitement des cartes avec Exadata</a></li>
<li><a href="../fr415611/index.html">PKI: bibliothèques GCrypt et KSBA comme alternative à OpenSSL avec prise en charge de la cryptographie russe. Continuation</a></li>
<li><a href="../fr415613/index.html">Pourquoi vous ne devriez pas acheter de lustres à LED</a></li>
<li><a href="../fr415615/index.html">Interaction avec le serveur via l'API dans iOS sur Swift 3. Partie 2</a></li>
<li><a href="../fr415617/index.html">Utilisation de la bibliothèque FPC InternetPCools dans Delphi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>