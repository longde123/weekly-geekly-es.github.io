<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåç üõÄ ‚ôÄÔ∏è Verwenden von Consul zum Skalieren von Stateful Services ‚å®Ô∏è üßíüèΩ üòë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Am 22. September veranstalteten wir unsere erste nicht standardm√§√üige Mitap f√ºr Entwickler hoch belasteter Systeme. Es war sehr cool, viele positive R...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verwenden von Consul zum Skalieren von Stateful Services</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/pixonic/blog/424777/">  <i>Am 22. September veranstalteten wir unsere erste <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht standardm√§√üige Mitap</a> f√ºr Entwickler hoch belasteter Systeme.</i>  <i>Es war sehr cool, viele positive R√ºckmeldungen zu Berichten und entschied sich daher, diese nicht nur hochzuladen, sondern auch f√ºr Habr zu entschl√ºsseln.</i>  <i>Heute ver√∂ffentlichen wir eine Rede von Ivan Bubnov, DevOps von BIT.GAMES.</i>  <i>Er sprach √ºber die Implementierung des Consul Discovery Service in einem bereits funktionierenden Hochlastprojekt f√ºr die M√∂glichkeit einer schnellen Skalierung und eines Failovers von Stateful Services.</i>  <i>Au√üerdem geht es darum, einen flexiblen Namespace f√ºr Backend-Anwendungen und Fallstricke zu organisieren.</i>  <i>Nun ein Wort an Ivan.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/X4VYCrOCD3A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Ich verwalte die Produktionsinfrastruktur im BIT.GAMES-Studio und erz√§hle die Geschichte der Einf√ºhrung des Konsuls von Hashicorp in unserem Projekt ‚ÄûGuild of Heroes‚Äú - Fantasy-Rollenspiel mit asynchronem PvP f√ºr mobile Ger√§te.  Verf√ºgbar bei Google Play, App Store, Samsung, Amazon.  DAU etwa 100.000, online von 10 bis 13 Tausend.  Wir machen das Spiel in Unity, also schreiben wir den Client in C # und verwenden unsere eigene BHL-Skriptsprache f√ºr die Spielelogik.  Wir schreiben den Serverteil in Golang (von PHP darauf umgestellt).  Als n√§chstes folgt die schematische Architektur unseres Projekts. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/dd/-v/gu/dd-vgufl1o4g4sjqu8luww4f56k.png"><br>  <i>In der Tat gibt es viel mehr Dienste, es gibt nur die Grundlagen der Spielelogik.</i> <br><br>  Also was wir haben.  Von den staatenlosen Diensten sind dies: <br><br><ul><li>  nginx, das wir als Frontend- und Load-Balancer verwenden und Kunden nach Gewichtskoeffizienten an unsere Backends verteilen; </li><li>  Gamed - Backends, kompilierte Anwendungen von Go.  Dies ist die zentrale Achse unserer Architektur. Sie erledigen den L√∂wenanteil der Arbeit und kommunizieren mit allen anderen Backend-Diensten. </li></ul><br>  Von den Stateful-Diensten haben wir folgende: <br><br><ul><li>  Redis, mit dem wir hei√üe Informationen zwischenspeichern (wir organisieren damit auch Chat im Spiel und speichern Benachrichtigungen f√ºr unsere Spieler); </li><li>  Percona Server f√ºr MySQL ist ein Repository f√ºr persistente Informationen (wahrscheinlich das gr√∂√üte und langsamste in jeder Architektur).  Wir verwenden die Gabel von MySQL und werden hier heute ausf√ºhrlicher darauf eingehen. </li></ul><br>  W√§hrend des Designprozesses hofften wir (wie alle anderen auch), dass das Projekt erfolgreich sein und einen Sharding-Mechanismus vorsehen w√ºrde.  Es besteht aus zwei MAINDB-Datenbankentit√§ten und den Shards selbst. <br><br><img src="https://habrastorage.org/webt/y2/sj/h-/y2sjh-qfoxlosksflpkvxuq3cxk.png"><br><br>  MAINDB ist eine Art Inhaltsverzeichnis - es speichert Informationen dar√ºber, auf welchen bestimmten Shard-Daten √ºber den Fortschritt des Players gespeichert sind.  Die gesamte Kette des Informationsabrufs sieht also ungef√§hr so ‚Äã‚Äãaus: Der Client greift auf das Frontend zu, das es wiederum nach Gewicht an eines der Backends verteilt, das Backend geht an MAINDB, lokalisiert den Shard des Players und w√§hlt die Daten direkt aus dem Shard selbst aus. <br><br>  Aber als wir entwarfen, waren wir kein gro√ües Projekt, also beschlossen wir, Shards Shards nur nominell herzustellen.  Sie befanden sich alle auf demselben physischen Server und h√∂chstwahrscheinlich auf derselben Datenbankpartitionierung auf demselben Server. <br><br>  F√ºr die Sicherung haben wir die klassische Master-Slave-Replikation verwendet.  Es war keine sehr gute L√∂sung (ich werde etwas sp√§ter sagen, warum), aber der Hauptnachteil dieser Architektur war, dass alle unsere Backends √ºber andere Backend-Dienste ausschlie√ülich √ºber IP-Adressen Bescheid wussten.  Und im Falle eines weiteren l√§cherlichen Unfalls im Rechenzentrum vom Typ " <i>Entschuldigung, unser Techniker hat das Kabel Ihres Servers getroffen, w√§hrend er einen anderen gewartet hat, und wir haben sehr lange</i> gebraucht, <i>um herauszufinden, warum Ihr Server nicht in Kontakt tritt</i> ", waren erhebliche Bewegungen von uns erforderlich.  Erstens ist dies die Neuerstellung und Vorinstallation von Backends vom IP-Sicherungsserver f√ºr den Ort des fehlgeschlagenen.  Zweitens ist es nach dem Vorfall erforderlich, unseren Master aus der Sicherung aus der Reserve wiederherzustellen, da er sich in einem inkonsistenten Zustand befand, und ihn mit derselben Replikation in einen koordinierten Zustand zu versetzen.  Dann haben wir die Backends wieder zusammengesetzt und neu geladen.  All dies verursachte nat√ºrlich Ausfallzeiten. <br><br>  Es kam eine Zeit, in der unser technischer Direktor (f√ºr den ich ihm so sehr danke) sagte: "Leute, h√∂rt auf zu leiden, wir m√ºssen etwas √§ndern, lasst uns nach Auswegen suchen."  Zun√§chst wollten wir einen einfachen, verst√§ndlichen und vor allem einfach zu verwaltenden Prozess der Skalierung und Migration unserer Datenbanken von Ort zu Ort erreichen, falls erforderlich.  Dar√ºber hinaus wollten wir durch die Automatisierung von Failovers eine hohe Verf√ºgbarkeit erreichen. <br><br><img src="https://habrastorage.org/webt/lb/96/ys/lb96yszgjsbjtriury8zxqr0loa.png"><br><br>  Die zentrale Achse unserer Forschung ist Konsul aus Hashicorp geworden.  Erstens wurden wir beraten, und zweitens waren wir sehr angetan von seiner Einfachheit, Freundlichkeit und dem hervorragenden Technologie-Stack in einer Box: Discovery-Service mit Healthcheck, Schl√ºsselwertspeicherung und das Wichtigste, was wir verwenden wollten, war DNS, das an uns Adressen aus der Dom√§ne service.consul aufl√∂st. <br><br>  Consul bietet auch gro√üartige Web-Benutzeroberfl√§chen und REST-APIs f√ºr die Verwaltung all dessen. <br><br>  F√ºr die Hochverf√ºgbarkeit haben wir zwei Dienstprogramme f√ºr das automatische Failover ausgew√§hlt: <br><br><ul><li>  MHA f√ºr MySQL </li><li>  Redis-Sentinel </li></ul><br><img src="https://habrastorage.org/webt/bc/e5/dg/bce5dg_dxoi0irv9gic5e4d1jki.png"><br><br>  Im Fall von MHA f√ºr MySQL haben wir Agenten in Knoten mit Datenbanken gegossen, und diese haben ihren Status √ºberwacht.  Es gab eine gewisse Zeit√ºberschreitung beim Ausfall des Masters. Danach wurde ein Stopp-Slave erstellt, um die Konsistenz aufrechtzuerhalten, und unser Backup-Master vom angezeigten Master in einem inkonsistenten Zustand hat die Daten nicht erfasst.  Und wir haben diesen Agenten einen Web-Hook hinzugef√ºgt, der dort die neue IP des Backup-Masters in Consul selbst registriert hat, wonach es zur Ausgabe von DNS kam. <br><br>  Mit Redis-Sentinel ist alles noch einfacher.  Da er selbst den L√∂wenanteil der Arbeit ausf√ºhrt, mussten wir bei der Gesundheitspr√ºfung nur ber√ºcksichtigen, dass Redis-Sentinel ausschlie√ülich auf dem Hauptknoten stattfinden sollte. <br><br>  Anfangs funktionierte alles perfekt wie eine Uhr.  Wir hatten keine Probleme am Pr√ºfstand.  Es hat sich jedoch gelohnt, sich in die nat√ºrliche Umgebung der Daten√ºbertragung eines geladenen Rechenzentrums zu begeben, sich an einige OOM-Kills zu erinnern (dies ist nicht gen√ºgend Speicher, bei denen der Prozess vom Systemkern beendet wird) und den Dienst oder komplexere Dinge wiederherzustellen, die die Verf√ºgbarkeit des Dienstes beeintr√§chtigen. Wie sind wir sofort zu einem ernsthaften Risiko von Fehlalarmen oder gar keiner garantierten Reaktion gekommen (wenn Sie versuchen, einige √úberpr√ºfungen zu verdrehen, um Fehlalarmen zu entkommen)? <br><br><img src="https://habrastorage.org/webt/a9/8t/7i/a98t7i3cvbb18duz7mkcr8f4vqg.png"><br><br>  Zun√§chst h√§ngt alles von der Schwierigkeit ab, den richtigen Gesundheitscheck zu schreiben.  Es scheint, dass die Aufgabe ziemlich trivial ist - √ºberpr√ºfen Sie, ob der Dienst auf Ihrem Server und Pingani-Port ausgef√ºhrt wird.  Wie die nachfolgende Praxis gezeigt hat, ist das Schreiben eines Healthchecks bei der Implementierung von Consul ein √§u√üerst komplexer und zeitaufw√§ndiger Prozess.  Da so viele Faktoren, die die Verf√ºgbarkeit Ihres Dienstes im Rechenzentrum beeinflussen, nicht vorhersehbar sind, werden sie erst nach einer bestimmten Zeit erkannt. <br><br>  Dar√ºber hinaus ist das Rechenzentrum keine statische Struktur, mit der Sie √ºberflutet sind, und es funktioniert wie beabsichtigt.  Leider (oder zum Gl√ºck) haben wir dies erst sp√§ter erfahren, aber im Moment waren wir inspiriert und voller Zuversicht, dass wir alles in der Produktion umsetzen w√ºrden. <br><br><img src="https://habrastorage.org/webt/c3/kt/uz/c3ktuzba_fzzjqodfbfsdv6sj6k.png"><br><br>  Zur Skalierung m√∂chte ich kurz sagen: Wir haben versucht, ein fertiges Fahrrad zu finden, aber alle sind f√ºr bestimmte Architekturen konzipiert.  Und wie im Fall von Jetpants konnten wir die Bedingungen, die er der Architektur einer dauerhaften Speicherung von Informationen auferlegte, nicht erf√ºllen. <br><br>  Deshalb haben wir √ºber unsere eigene Skriptbindung nachgedacht und diese Frage verschoben.  Wir haben uns entschlossen, konsequent zu handeln und mit der Implementierung von Consul zu beginnen. <br><br><img src="https://habrastorage.org/webt/99/em/bk/99embkj_f7vdi-kta4rdme6ga6k.png"><br><br>  Consul ist ein dezentraler, verteilter Cluster, der auf der Grundlage des Klatschprotokolls und des Raft-Konsensalgorithmus arbeitet. <br><br>  Wir haben ein unabh√§ngiges √Ñquorum von f√ºnf Servern (f√ºnf, um die Split-Brain-Situation zu vermeiden).  F√ºr jeden Knoten wird der Consul-Agent im Agentenmodus versch√ºttet und der gesamte Healthcheck versch√ºttet (d. H. Es wurde kein Healthcheck auf einen bestimmten Server und andere auf bestimmte Server hochgeladen).  Healthcheck wurde so geschrieben, dass sie nur dort bestehen, wo es einen Service gibt. <br><br>  Wir haben auch ein anderes Dienstprogramm verwendet, damit wir unser Backend nicht lernen mussten, um Adressen aus einer bestimmten Dom√§ne an einem nicht standardm√§√üigen Port aufzul√∂sen.  Wir haben Dnsmasq verwendet - es bietet die M√∂glichkeit, die Adressen auf den ben√∂tigten Clusterknoten (die in der realen Welt sozusagen nicht existieren, sondern ausschlie√ülich innerhalb des Clusters existieren) vollst√§ndig transparent aufzul√∂sen.  Wir haben ein automatisches Skript zum F√ºllen von Ansible vorbereitet, alles in die Produktion hochgeladen, den Namespace optimiert und sichergestellt, dass alles vollst√§ndig ist.  Und dr√ºckten die Daumen und luden unsere Backends neu, auf die nicht √ºber IP-Adressen, sondern √ºber diese Namen aus der Dom√§ne server.consul zugegriffen wurde. <br><br>  Alles begann beim ersten Mal, unsere Freude kannte keine Grenzen.  Aber es war zu fr√ºh, um sich zu freuen, denn innerhalb einer Stunde stellten wir fest, dass auf allen Knoten, an denen sich unsere Backends befinden, der Lastdurchschnittsindikator von 0,7 auf 1,0 anstieg, was ein ziemlich fetter Indikator ist. <br><br><img src="https://habrastorage.org/webt/em/ua/v_/emuav_oozpbvjdoa7yg1ldfeag0.png"><br><br>  Ich stieg auf den Server, um zu sehen, was los war, und es wurde offensichtlich, dass die CPU Consul a√ü.  Hier haben wir angefangen, es herauszufinden, haben mit strace angefangen (ein Dienstprogramm f√ºr Unix-Systeme, mit dem Sie verfolgen k√∂nnen, welcher Systemaufruf der Prozess ausgef√ºhrt wird), Dnsmasq-Statistiken ausgegeben, um zu verstehen, was auf diesem Knoten vor sich geht, und es hat sich herausgestellt, dass wir einen sehr wichtigen Punkt √ºbersehen haben.  Bei der Planung der Integration haben wir das Zwischenspeichern von DNS-Eintr√§gen verpasst, und es stellte sich heraus, dass unser Backend Dnsmasq f√ºr jede seiner Bewegungen abgerufen hat und sich wiederum an Consul gewandt hat, was zu 940 DNS-Abfragen pro Sekunde f√ºhrte. <br><br>  Der Ausweg schien offensichtlich - drehen Sie einfach ttl und alles wird besser.  Aber hier war es unm√∂glich, fanatisch zu sein, weil wir diese Struktur implementieren wollten, um einen dynamischen, einfach zu kontrollierenden und sich schnell √§ndernden Namespace zu erhalten (daher konnten wir beispielsweise keine 20 Minuten festlegen).  Wir haben ttl auf die f√ºr uns maximal optimalen Werte gedreht und es geschafft, die Abfragerate pro Sekunde auf 540 zu reduzieren. Dies hatte jedoch keinen Einfluss auf die CPU-Verbrauchsanzeige. <br><br>  Dann haben wir uns entschlossen, auf knifflige Weise mit einer benutzerdefinierten Hosts-Datei herauszukommen. <br><br><img src="https://habrastorage.org/webt/0p/li/01/0pli01ivofxzndxm9pa9xrtvu6m.png"><br><br>  Es ist gut, dass wir alles daf√ºr hatten: ein exzellentes Vorlagensystem von Consul, das basierend auf dem Status des Clusters und dem Vorlagenskript eine Datei jeglicher Art generiert, jede Konfiguration ist alles, was Sie wollen.  Dar√ºber hinaus verf√ºgt Dnsmasq √ºber einen Konfigurationsparameter f√ºr addn-hosts, mit dem Sie eine Nicht-System-Hosts-Datei als dieselbe zus√§tzliche Hosts-Datei verwenden k√∂nnen. <br><br>  Was wir getan haben, hat das Skript in Ansible erneut vorbereitet, es in die Produktion hochgeladen und es sah ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/p6/qe/3i/p6qe3iaqlloehk1ld7ptisb_0dg.png"><br><br>  Es gab ein zus√§tzliches Element und eine statische Datei auf der Festplatte, die ziemlich schnell neu generiert wird.  Jetzt sah die Kette ganz einfach aus: Das Spiel wandte sich an Dnsmasq, und das wiederum (anstatt den Consula-Agenten zu ziehen, der die Server fragen w√ºrde, wo wir diesen oder jenen Knoten hatten) sah sich nur die Datei an.  Dies l√∂ste das Problem mit dem CPU-Verbrauch durch Consul. <br><br>  Jetzt sah alles so aus, wie wir es geplant hatten - absolut transparent f√ºr unsere Produktion, praktisch ohne Ressourcen zu verbrauchen. <br><br>  Wir waren an diesem Tag ziemlich gequ√§lt und gingen mit gro√üer Besorgnis nach Hause.  Sie hatten keine vergebliche Angst, denn nachts weckte mich ein Alarm der √úberwachung und informierte mich, dass wir einen ziemlich gro√üen (wenn auch kurzfristigen) Fehlerausbruch hatten. <br><br><img src="https://habrastorage.org/webt/yg/yj/z_/ygyjz_upz8khxqgriqbsye0_xma.png"><br><br>  Als ich mich morgens mit den Protokollen befasste, sah ich, dass alle Fehler vom gleichen Typ eines unbekannten Hosts waren.  Es war nicht klar, warum Dnsmasq den einen oder anderen Dienst aus einer Datei nicht verwenden konnte - es schien, als g√§be es ihn √ºberhaupt nicht.  Um zu verstehen, was los war, habe ich eine benutzerdefinierte Metrik hinzugef√ºgt, um die Datei neu zu generieren. Jetzt wusste ich genau, wann sie neu generiert werden w√ºrde.  Dar√ºber hinaus verf√ºgt die Consul-Vorlage selbst √ºber eine hervorragende Sicherungsoption, d. H.  Sie k√∂nnen den vorherigen Status der neu generierten Datei anzeigen. <br><br>  Tags√ºber wiederholte sich der Vorfall mehrmals und es wurde klar, dass zu einem bestimmten Zeitpunkt (obwohl er sporadisch und unsystematisch war) die Hosts-Datei ohne bestimmte Dienste erneut erbracht wurde.  Es stellte sich heraus, dass es in einem bestimmten Rechenzentrum (ich werde keine Anti-Werbung machen) ziemlich instabile Netzwerke gibt - aufgrund von Netzwerkausf√§llen haben wir absolut unvorhersehbar aufgeh√∂rt, Healthchecks zu bestehen, oder sogar die Knoten sind aus dem Cluster herausgefallen.  Es sah ungef√§hr so ‚Äã‚Äãaus: <br><br><img src="https://habrastorage.org/webt/3v/_q/ve/3v_qvengzywl6bqdchzxs2fshl4.png"><br><br>  Der Knoten fiel aus dem Cluster heraus, der Consul-Agent wurde sofort dar√ºber informiert, und die Consul-Vorlage hat die Hosts-Datei sofort ohne den von uns ben√∂tigten Dienst neu generiert.  Dies war im Allgemeinen nicht akzeptabel, da das Problem l√§cherlich ist: Wenn der Dienst einige Sekunden lang nicht verf√ºgbar ist, richten Sie Zeit√ºberschreitungen und Retrays ein (sie haben keine Verbindung hergestellt, aber das zweite Mal stellte sich heraus).  Wir haben jedoch eine neue Struktur im Verk√§ufer provoziert, als der Dienst einfach aus dem Blickfeld verschwand und es keine M√∂glichkeit gab, eine Verbindung zu ihm herzustellen. <br><br>  Wir begannen dar√ºber nachzudenken, was zu tun ist, und drehten den Timeout-Parameter in Consul. Danach wird er identifiziert, nachdem der Knoten herausgefallen ist.  Wir haben es geschafft, dieses Problem mit einem relativ kleinen Indikator zu l√∂sen. Die Knoten fielen nicht mehr aus, aber dies half nicht bei der √úberpr√ºfung des Gesundheitszustands. <br><br>  Wir begannen dar√ºber nachzudenken, verschiedene Parameter f√ºr den Gesundheitscheck auszuw√§hlen und irgendwie zu verstehen, wann und wie dies geschieht.  Aber aufgrund der Tatsache, dass alles sporadisch und unvorhersehbar passierte, konnten wir es nicht tun. <br><br>  Dann gingen wir zur Consul-Vorlage und beschlossen, eine Zeit√ºberschreitung daf√ºr vorzunehmen. Danach reagiert sie auf eine √Ñnderung des Status des Clusters.  Auch hier war es unm√∂glich, fanatisch zu sein, weil wir zu einer Situation kommen konnten, in der das Ergebnis nicht besser w√§re als das klassische DNS, wenn wir auf ein v√∂llig anderes abzielten. <br><br>  Und wieder kam unser technischer Direktor zur Rettung und sagte: ‚ÄûLeute, lasst uns versuchen, all diese Interaktivit√§t aufzugeben, wir sind alle in Produktion und es gibt keine Zeit f√ºr Forschung, wir m√ºssen dieses Problem l√∂sen.  Nutzen wir einfache und verst√§ndliche Dinge. ‚Äú  So kamen wir zu dem Konzept, Schl√ºsselwertspeicher als Quelle f√ºr die Generierung einer Hosts-Datei zu verwenden. <br><br><img src="https://habrastorage.org/webt/ar/qm/kx/arqmkxvbfzahdo6qrlom1htydq4.png"><br><br>  Wie es aussieht: Wir lehnen alle dynamischen Integrit√§tspr√ºfungen ab und schreiben unser Vorlagenskript so um, dass es eine Datei basierend auf den im Schl√ºsselwertspeicher aufgezeichneten Daten generiert.  Im Schl√ºsselwertspeicher beschreiben wir unsere gesamte Infrastruktur in Form des Schl√ºsselnamens (dies ist der Name des ben√∂tigten Dienstes) und des Schl√ºsselwerts (dies ist der Name des Knotens im Cluster).  Das hei√üt,  Wenn der Knoten im Cluster vorhanden ist, k√∂nnen wir seine IP-Adresse sehr einfach abrufen und in die Hosts-Datei schreiben. <br><br>  Wir haben alles getestet, es in der Produktion gef√ºllt und es wurde in einer bestimmten Situation zu einer Silberkugel.  Wieder qu√§lten wir uns den ganzen Tag ziemlich, gingen nach Hause, kehrten aber bereits ausgeruht und ermutigt zur√ºck, weil diese Probleme nicht wieder auftraten und im vergangenen Jahr nicht wieder auftraten.  Daraus schlie√üe ich pers√∂nlich, dass dies die richtige Entscheidung war (speziell f√ºr uns). <br><br>  Also.  Wir haben endlich das bekommen, was wir wollten und einen dynamischen Namespace f√ºr unsere Backends organisiert.  Weiter haben wir uns um eine hohe Verf√ºgbarkeit bem√ºht. <br><br><img src="https://habrastorage.org/webt/m9/k0/nu/m9k0nueo_w_79ywwhcco0tgnq5u.png"><br><br>  Tatsache ist jedoch, dass wir ziemlich Angst vor der Integration von Consul haben und aufgrund der Probleme, auf die wir gesto√üen sind, dachten und entschieden, dass die Einf√ºhrung von Auto-Failover keine so gute L√∂sung ist, weil wir wiederum falsch positive Ergebnisse riskieren oder Ausf√§lle.  Dieser Prozess ist undurchsichtig und unkontrollierbar. <br><br>  Aus diesem Grund haben wir einen einfacheren (oder komplexeren) Weg eingeschlagen: Wir haben beschlossen, das Failover dem Gewissen des diensthabenden Administrators zu √ºberlassen, ihm jedoch ein weiteres zus√§tzliches Tool zur Verf√ºgung gestellt.  Wir haben die Master-Slave-Replikation durch die Master-Master-Replikation im schreibgesch√ºtzten Modus ersetzt.  Dies beseitigt eine Menge Kopfschmerzen beim Failover'ov. Wenn Sie einen Assistenten erhalten, m√ºssen Sie lediglich den Wert im k / v-Speicher √ºber die Web-Benutzeroberfl√§che oder den Befehl in der API √§ndern und zuvor den schreibgesch√ºtzten Modus entfernen Backup-Master. <br><br>  Nachdem der Vorfall beendet ist, kontaktiert der Master und kommt automatisch zu einem koordinierten Zustand ohne unn√∂tige Aktionen.  Wir haben bei dieser Option angehalten und sie wie zuvor verwendet - f√ºr uns ist sie so bequem wie m√∂glich und vor allem so einfach, klar und kontrolliert wie m√∂glich. <br><br><img src="https://habrastorage.org/webt/mr/fn/tc/mrfntctgtpanvrkqhlqmbm9liue.png"><br>  <i>Consul Webinterface</i> <br><br>  Rechts ist der k / v-Speicher und unsere Dienste sind sichtbar, die wir im Spiel verwenden;  Wert ist der Name des Knotens. <br><br>  Die Skalierung wurde implementiert, als die Shards bereits auf einem Server √ºberf√ºllt waren, die Basen wuchsen, langsam wurden, die Anzahl der Spieler zunahm, wir tauschten und die Aufgabe hatten, alle Shards auf unsere verschiedenen separaten Server zu verteilen. <br><br><img src="https://habrastorage.org/webt/0b/yc/zg/0byczgl-2ugpi8z_dgwzytcmrw4.png"><br><br>  So sah es aus: Mit dem Dienstprogramm XtraBackup haben wir unser Backup auf einem neuen Serverpaar wiederhergestellt. Danach wurde der neue Master mit einem Slave auf dem alten Server aufgeh√§ngt.  Es wurde ein konsistenter Zustand erreicht. Wir haben den Schl√ºsselwert in k / v-storage vom Namen des Knotens des alten Masters in den Namen des Knotens des neuen Masters ge√§ndert.  Dann (als wir glaubten, dass alles korrekt lief und alle Spiele mit ihren Auswahlen, Aktualisierungen und Einf√ºgungen an den neuen Master gingen) mussten wir nur die Replikation beenden und die begehrte Drop-Datenbank f√ºr die Produktion erstellen, da wir alle gerne mit unn√∂tigen Datenbanken arbeiten. <br><br><img src="https://habrastorage.org/webt/1x/6k/pw/1x6kpwybgw6-wsgscd6r1kylk-u.png"><br><br>  Also haben wir die Scherben zerbrochen.  Der gesamte Bewegungsprozess dauerte von 40 Minuten bis zu einer Stunde und verursachte keine Ausfallzeiten. Er war f√ºr unsere Backends v√∂llig transparent und f√ºr die Spieler f√ºr sich genommen v√∂llig transparent (mit der Ausnahme, dass das Spielen f√ºr sie einfacher und angenehmer wurde, sobald sie sich bewegten). <br><br><img src="https://habrastorage.org/webt/gm/du/bj/gmdubjovhtlet9n2-bljkeqz7oq.png"><br><br>  Bei den Failover-Prozessen betr√§gt hier die Umschaltzeit 20 bis 40 Sekunden zuz√ºglich der Reaktionszeit des diensthabenden Systemadministrators.  So sieht es jetzt bei uns aus. <br><br>  Was ich abschlie√üend sagen m√∂chte - leider sind unsere Hoffnungen auf eine absolute, umfassende Automatisierung in die harte Realit√§t des Daten√ºbertragungsmediums in einem geladenen Rechenzentrum und in zuf√§llige Faktoren geraten, die wir nicht vorhersehen konnten. <br><br>  Zweitens hat es uns wieder einmal gelehrt, dass eine einfache und bew√§hrte Meise in den H√§nden Ihres Systemadministrators besser ist als ein neu reagierender, selbstreagierender, selbstskalierter Kran irgendwo hinter den Wolken, den Sie nicht einmal verstehen, ob er auseinander f√§llt oder wirklich begann zu skalieren. <br><br>  Die Einf√ºhrung einer Infrastruktur und Automatisierung in Ihrer Produktion sollte den Mitarbeitern, die sie bedienen, keine unn√∂tigen Kopfschmerzen bereiten.  Die Kosten f√ºr die Aufrechterhaltung der Infrastrukturproduktion sollten nicht wesentlich erh√∂ht werden. Die L√∂sung sollte einfach, klar, f√ºr Ihre Kunden transparent, bequem und kontrolliert sein. <br><br><h3>  Fragen aus dem Publikum </h3><br>  <b>Wie schreibt man k / v mit Servern - einem Skript oder patcht man es einfach?</b> <br><br> K/v-     Consul-   -  ,     http- RESTful API  Web UI. <br><br>     ,   -             ,     ,   . <br><br> <b>       ,     Redis?</b> <br><br>        ,    - . <br><br> -,          backend. -,      backend',       ‚Äî    .  Das hei√üt,       ,     MAINDB        ,   .        .                  -  ,       . <br><br>      - ,       inmemory key-value    -. <br><br> <b>   ?</b> <br><br>    MySQL ‚Äî Percona server. <br><br> <b>            ?      Maria,     MHA for MySQL,    Galera.</b> <br><br>      Galera.    -   ¬´ ¬ª       Galera       ,     .       ,       . <br><br>    ,     ‚Äî         ,         ,     -  ,    ,         ,   . <br><br><h3>    Pixonic DevGAMM Talks </h3><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">CICD:        </a> ( ,   Pixonic); </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">     -  Quake Champions</a> ( , backend- Saber Interactive); </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> -  - Tacticool</a> ( , Lead Software Engineer  PanzerDog); </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> ECS, C# Job System  SRP    </a> ( , Field Engineer  Unity); </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> KISS  </a> ( , Lead Game Programmer  1C Game Studios); </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">      </a> ( , Deputy Technical Officer  Pixonic). </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cucumber  :  BDD-    </a> ( , Technical Product Manager  ALICE Platform). </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de424777/">https://habr.com/ru/post/de424777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de424763/index.html">Ein Texteditor ist nicht Ihre h√∂chste Mathematik, hier m√ºssen Sie nachdenken</a></li>
<li><a href="../de424765/index.html">Zustandsverwaltung in Flatteranwendungen</a></li>
<li><a href="../de424767/index.html">Wir machen einen Kuchen aus Habr. Wieder</a></li>
<li><a href="../de424771/index.html">Pers√∂nliche Erfahrung: von einer Idee und einem leeren Blatt bis zu einer Entwurfsversion einer Website</a></li>
<li><a href="../de424773/index.html">Biopharma und numerische Modellierung: Amgen Erfahrung und Praxis</a></li>
<li><a href="../de424779/index.html">Mehrseitiges SPA in Python</a></li>
<li><a href="../de424781/index.html">Lehren und Testen neuronaler Netze auf PyTorch mit Ignite</a></li>
<li><a href="../de424787/index.html">Interview mit Aaron Patterson, Sprecher der RubyRussia-Konferenz 2018</a></li>
<li><a href="../de424789/index.html">So stellen Sie eine Ruby on Rails-Anwendung mit HAProxy Ingress, Unicorn / Puma und Web-Sockets bereit</a></li>
<li><a href="../de424791/index.html">Erweitern der Netzwerkfunktionen eines programmierbaren Relais mithilfe von WI-FI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>