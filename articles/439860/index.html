<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëΩ üßñüèø üë©‚Äçüë©‚Äçüë¶ Ubuntu 18.04 Root en ZFS ‚ô†Ô∏è üå•Ô∏è üôáüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El a√±o pasado necesitaba crear instrucciones para instalar el sistema operativo Ubuntu 18.04. Por cierto, no hay nada complicado en la instalaci√≥n de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ubuntu 18.04 Root en ZFS</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439860/"><p>  El a√±o pasado necesitaba crear instrucciones para instalar el sistema operativo Ubuntu 18.04.  Por cierto, no hay nada complicado en la instalaci√≥n de Ubuntu, pero hay un matiz: quer√≠a usar el sistema de archivos ZFS como base.  Por un lado, Ubuntu admite ZFS en el nivel del kernel, pero todav√≠a no hay un instalador, pero hay una instrucci√≥n, s√≠: </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/zfsonlinux/zfs/wiki/Ubuntu-18.04-Root-on-ZFS</a> </p><br><p>  La secuencia de acciones en este manual es generalmente correcta, pero algunos puntos requieren ajustes.  Entonces, lo que sigue no es una traducci√≥n directa de las instrucciones, sino gratuita, teniendo en cuenta las correcciones, mi experiencia con ZFS y otras cosas.  Tampoco considero problemas de cifrado de disco y uso el gestor de arranque MBR.  Mis instrucciones de instalaci√≥n se pueden obtener <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠.</a> </p><br><a name="habracut"></a><br><h2><blockquote>  0. Preparaci√≥n del servidor </blockquote></h2><br><p> Lo primero que falta en las instrucciones y que no se considera de ninguna manera es que ZFS no funciona muy bien con las matrices RAID de hardware, en particular, est√° conectado con la cach√© de escritura, lo cual es comprensible: el sistema de archivos ZFS est√° registrado y requiere un control completo sobre las operaciones de escritura.  Adem√°s, cuando se utiliza una matriz RAID de hardware ya preparada, las capacidades de ZFS se pierden en t√©rminos de cach√©, repuesto y m√°s.  Por lo tanto, todos los discos deben transferirse al modo HBA y, si esto no es posible, crear un RAID separado para cada disco y deshabilitar el controlador de cach√© de escritura. </p><br><p>  Adem√°s, cuando use la agregaci√≥n de puertos de red, puede deshabilitarlos en la etapa de instalaci√≥n, para no complicarlo (realizar√© todas las operaciones adicionales sin vinculaci√≥n). </p><br><h2>  1. Preparaci√≥n del entorno de instalaci√≥n. </h2><br><h3>  1.1.  Livecd </h3><br><p>  Como se mencion√≥ anteriormente, desafortunadamente, no hay un instalador de Ubuntu listo que use root en ZFS, por lo que la instalaci√≥n se lleva a cabo usando un disco LiveCD: </p><br><p>  Descargue desde aqu√≠: <a href="">http://releases.ubuntu.com/18.04/ubuntu-18.04.1-desktop-amd64.iso</a> </p><br><blockquote>  Al mismo tiempo, intent√© con mis colegas usar varias im√°genes de disco, ya que realmente no quer√≠a usar el shell gr√°fico, pero esto no condujo a nada bueno. </blockquote><br><p>  Arrancamos desde el LiveCD, seleccionamos Probar Ubuntu y abrimos el terminal (Ctrl + Alt + T). </p><br><h3>  1.2.  Actualizaci√≥n e instalaci√≥n de repositorios </h3>  '' <br><pre><code class="bash hljs">sudo apt-add-repository universe sudo apt update</code> </pre> <br><blockquote>  Aqu√≠ estamos esperando el primer problema si la configuraci√≥n de red del servidor no est√° determinada por DHCP.  La actualizaci√≥n de repositorios no funcionar√°, as√≠ que configuremos la red. </blockquote><br><p>  Observamos las interfaces de red y encontramos la que nos permitir√° conectarnos: </p><br><pre> <code class="bash hljs">sudo ip a</code> </pre> <br><p>  Configure la interfaz de red: </p><br><pre> <code class="bash hljs">sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"auto {{ NAME }}"</span></span> &gt;&gt; /etc/network/interfaces sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"iface {{ NAME }} inet static"</span></span> &gt;&gt; /etc/network/interfaces sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">" address {{ IP }}"</span></span> &gt;&gt; /etc/network/interfaces sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">" netmask {{ NETMASK }}"</span></span> &gt;&gt; /etc/network/interfaces sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">" gateway {{ GATEWAY }}"</span></span> &gt;&gt; /etc/network/interfaces sudo service networking restart</code> </pre><br><p>  Y resoluci√≥n de DNS: </p><br><pre> <code class="bash hljs">sudo <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'nameserver 8.8.8.8'</span></span> &gt;&gt; /etc/resolv.conf</code> </pre> <br><p>  Actualizaci√≥n de repositorios: </p><br><pre> <code class="bash hljs">sudo apt update</code> </pre> <br><h3>  1.3.  Servidor SSH (opcional) </h3><br><p>  Para facilitar la instalaci√≥n, puede elevar el servidor OpenSSH y realizar todas las operaciones adicionales a trav√©s del cliente SSH </p><br><p>  Establezca la contrase√±a para el usuario de ubuntu: </p><br><pre> <code class="bash hljs">passwd</code> </pre> <br><blockquote>  Esto es importante!  Dado que de lo contrario, el acceso a trav√©s de ssh se realizar√° sin una contrase√±a con derechos de sudo.  Sin embargo, no puede establecer una contrase√±a simple. </blockquote><br><p>  Instala y ejecuta OpenSSH: </p><br><pre> <code class="bash hljs">sudo apt install openssh-server sudo service ssh start</code> </pre> <br><p>  Y en la terminal de la estaci√≥n de trabajo: </p><br><pre> <code class="bash hljs">ssh ubuntu@{{ ip server }}</code> </pre> <br><h3>  1.4.  Hazte root </h3><br><pre> <code class="bash hljs">sudo -s</code> </pre> <br><h3>  1.5.  Instalaci√≥n de soporte ZFS en un entorno LiveCD </h3><br><pre> <code class="bash hljs">apt install --yes debootstrap gdisk zfs-initramfs</code> </pre> <br><h2>  2. Particionamiento y formateo de discos duros </h2><br><h3><blockquote>  2.0.  Definici√≥n de matrices de discos </blockquote></h3><br><p>  La instrucci√≥n principal no contiene un punto importante sobre c√≥mo determinar las matrices de discos. </p><br><p>  Por lo general, la cantidad de discos en los servidores es: </p><br><ul><li>  2 discos; </li><li>  4 discos; </li><li>  muchos discos; </li></ul><br><p>  No consideramos 1 disco porque generalmente es una anomal√≠a. </p><br><h4>  2.0.1.  2 discos </h4><br><p>  Aqu√≠ todo es simple, una matriz MIRROR (RAID1).  Si hay otra tercera unidad, puede colocarla en un repuesto din√°mico (REPUESTO) o ensamblar una matriz RAIDZ (RAID5).  Pero 3 discos en el servidor son muy raros. </p><br><h4>  2.0.2.  4 discos </h4><br><p>  Si todas las unidades son iguales, solo hay tres opciones (el cuarto RAID0 b√°sicamente no lo considero): </p><br><ul><li>  MIRROR + MIRROR es un an√°logo de RAID10, m√°s precisamente RAID01, ya que en ZFS es espejo + espejo.  50% del espacio disponible en disco; </li><li>  RAIDZ es un an√°logo de RAID5.  75% del espacio disponible en disco; </li><li>  RAIDZ2 es un an√°logo de RAID6.  50% del espacio disponible en disco; </li></ul><br><p>  En la pr√°ctica, uso la matriz MIRROR + MIRROR, aunque es obvio que la matriz RAIDZ es m√°s rentable, ya que proporciona m√°s espacio en disco, pero hay matices. </p><br><p>  En t√©rminos de tolerancia a fallas, los arreglos se organizan en este orden (de mejor a peor): </p><br><ul><li>  RAIDZ2: se pueden perder dos discos sin p√©rdida de datos; </li><li>  MIRROR + MIRROR: se puede perder un disco sin p√©rdida de datos, y con un 66% de probabilidad de que se pierda un segundo disco sin p√©rdida de datos; </li><li>  RAIDZ: solo se puede perder un disco sin p√©rdida de datos; </li></ul><br><p>  En t√©rminos de velocidad, las matrices se organizan en este orden: </p><br><ul><li>  ESPEJO + ESPEJO - tanto en t√©rminos de escritura como de lectura; </li><li>  RAIDZ: en t√©rminos de grabaci√≥n es m√°s lento, ya que adem√°s de la grabaci√≥n, se requiere calcular la suma de verificaci√≥n; </li><li>  RAIDZ2: en t√©rminos de escritura es a√∫n m√°s lento ya que requiere el c√°lculo de sumas de verificaci√≥n m√°s complejas; </li></ul><br><p>  En t√©rminos de la velocidad de la matriz durante la degradaci√≥n de un disco: </p><br><ul><li>  MIRROR + MIRROR: cuando una unidad se cae, esencialmente solo se pierde la lectura paralela de un espejo, el segundo espejo funciona sin degradaci√≥n del rendimiento; </li><li>  RAIDZ2: la degradaci√≥n en la degradaci√≥n del rendimiento es mayor, ya que requiere una asignaci√≥n hacia atr√°s del bloque desde la suma de comprobaci√≥n para 1/4 de la b√∫squeda de datos + bloque; </li><li>  RAIDZ: la degradaci√≥n es mucho mayor, ya que requiere un rec√°lculo del bloque desde la suma de comprobaci√≥n para 1/3 de la b√∫squeda de datos + bloque; </li></ul><br><p>  La comparaci√≥n de caracter√≠sticas es subjetiva, pero refleja suficientemente mi elecci√≥n como punto medio. </p><br><p>  Al mismo tiempo, debe comprender que "m√°s lento" e "a√∫n m√°s lento" no lo es a veces, pero solo el 10-20% en el peor de los casos, por lo tanto, si su base de datos o aplicaci√≥n para trabajar con discos no est√° optimizada, entonces disminuir√° la velocidad en principio, no te des cuenta.  El factor de velocidad de grabaci√≥n debe considerarse solo cuando realmente lo necesite. </p><br><h4>  2.0.2.  Muchos discos </h4><br><p>  El problema principal es que si tenemos muchos discos y queremos hacer una matriz com√∫n para todo, entonces tendremos que marcar cada disco con el sector de arranque o hacer un poco de finta con nuestros o√≠dos.  En la pr√°ctica, para plataformas multidisco, intento construir esta configuraci√≥n: </p><br><ul><li>  2 discos SSD: hacemos un espejo y como matriz de arranque principal con el sistema operativo y cach√© ZFS para la segunda matriz de discos; </li><li>  El resto est√° obstruido con discos SATA o SAS y sin marcado, recopilamos una matriz de discos ZFS; </li></ul><br><p>  Lo mismo se aplica a los servidores de 4 discos si queremos obtener una plataforma bastante universal; </p><br><p>  Si todos los discos son iguales, y no tiene sentido asignar dos discos para una matriz separada (por ejemplo, 6 discos de 8 Tb cada uno), entonces puede hacer discos de arranque del primer grupo de la matriz.  Es decir, si va a hacer una matriz como: MIRROR + MIRROR + MIRROR o RAIDZ + RAIDZ, marcamos el sector de arranque solo para el primer grupo.  En principio, es posible particionar solo una unidad, incluso MIRROR y RAIDZ, y sustituir el resto en forma cruda, ZFS har√° la matriz por el elemento m√°s peque√±o, pero en este caso, si la primera unidad falla, perder√° el √∫nico disco de arranque, por lo tanto Vale la pena hacer esto. </p><br><p>  Es importante comprender que en el sistema de archivos ZFS - stripe esto no es exactamente RAID0, y funciona de manera un poco diferente y no requiere los mismos tama√±os de disco, por lo que asignar un peque√±o espacio para el sector de arranque del clima no servir√° de mucho, lo principal es indicar en el BIOS el disco correcto desde el cual arrancar . </p><br><h3>  2.1.  Particionamiento y limpieza de disco </h3><br><p>  El paquete mdadm se usa para marcar el disco, p√≥ngalo: </p><br><pre> <code class="bash hljs">apt install --yes mdadm</code> </pre> <br><p>  Analizamos qu√© discos tenemos disponibles: </p><br><pre> <code class="bash hljs">lsblk</code> </pre> <br><p>  Y limpiarlos: </p><br><pre> <code class="bash hljs">sgdisk --zap-all /dev/{{ disk name }}</code> </pre> <br><h3>  2.2.  Dise√±o del disco </h3><br><p>  En realidad, la partici√≥n de arranque: </p><br><pre> <code class="bash hljs">sgdisk -a1 -n1:34:2047 -t1:EF02 /dev/{{ disk name }}</code> </pre> <br><p>  La seccion principal. </p><br><blockquote>  Aqu√≠ puede haber variaciones: si necesita asignar una partici√≥n adicional de discos SSD, por ejemplo, para ZFS Cache o para Aerospike, entonces crea la partici√≥n principal de volumen limitado: </blockquote><br><pre> <code class="bash hljs">sgdisk -n2:0:+100GB -t2:BF01 /dev/{{ disk name }} sgdisk -n3:0:0 -t2:BF01 /dev/{{ disk name }}</code> </pre><br><p>  Si usamos todo el espacio, simplemente cree una secci√≥n para el espacio restante: </p><br><pre> <code class="bash hljs">sgdisk -n2:0:0 -t2:BF01 /dev/{{ disk name }}</code> </pre> <br><p>  No olvides comprobar c√≥mo result√≥: </p><br><pre> <code class="bash hljs">lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 1.8T 0 disk ‚îú‚îÄsda1 8:1 0 1007K 0 part ‚îî‚îÄsda2 8:2 0 1.8T 0 part sdb 8:16 0 1.8T 0 disk ‚îú‚îÄsdb1 8:17 0 1007K 0 part ‚îî‚îÄsdb2 8:18 0 1.8T 0 part ...</code> </pre><br><h3>  2.3.  Crear una matriz ZFS </h3><br><pre> <code class="bash hljs">zpool create \ -o ashift=12 \ -O atime=off \ -O canmount=off \ -O compression=lz4 \ -O checksum=fletcher4 \ -O normalization=formD \ -m legacy \ -R /mnt \ -f \ tank \ mirror \ /dev/{{ disk a part 2}} \ /dev/{{ disk b part 2}}</code> </pre> <br><blockquote>  El primer rastrillo en el que uno de mis familiares administradores pis√≥ de inmediato, es que al crear una matriz ZFS, se requiere especificar no un disco sino una partici√≥n en el disco, si est√° especialmente creado para esto. </blockquote><br><p>  A continuaci√≥n, en orden: </p><br><ul><li>  ashift = 12 - use el tama√±o de bloque en 4K, en principio, todav√≠a no entiendo por qu√© a menudo en los sistemas operativos el tama√±o de bloque predeterminado es de 512 bytes cuando pr√°cticamente no hay tales discos; </li><li>  atime = off - deshabilita la fecha de actualizaci√≥n de acceso a los archivos, siempre lo apago ya que realmente nunca necesit√© esta informaci√≥n y no es necesario volver a cargar el kernel; </li><li>  canmount = off: deshabilita la capacidad de montar la partici√≥n ra√≠z; </li><li>  compresi√≥n = lz4 - habilita la compresi√≥n de datos con el algoritmo LZ4.  Se recomienda incluir este par√°metro no solo para ahorrar espacio en disco, sino tambi√©n para reducir la cantidad de operaciones de E / S.  Al mismo tiempo, para este agloritmo de compresi√≥n, utilizaci√≥n de CPU extremadamente baja; </li><li>  checksum = fletcher4: el algoritmo de suma de comprobaci√≥n predeterminado, por lo que vale la pena volver a comprobar fletcher4; </li><li>  normalizaci√≥n = formD: se usa para mejorar el trabajo con UTF-8, de hecho, limita la posibilidad de usar nombres de archivos que no sean UTF-8.  Aqu√≠ todos deciden por s√≠ mismos, en nuestro trabajo siempre usamos solo la codificaci√≥n UTF-8; </li><li>  xattr = sa: acelerar el trabajo con atributos extendidos.  No uso esta opci√≥n debido a que cuando se usa esta opci√≥n, la compatibilidad con otras implementaciones de OpenZFS est√° deshabilitada (por ejemplo: FreeBSD).  Y compatibilidad con Windows y, por cierto, lo necesito.  Adem√°s, esta opci√≥n se puede habilitar en la secci√≥n final; </li><li>  -m legacy: punto de montaje en ninguna parte, y no es necesario montar la partici√≥n ra√≠z; </li><li>  -R / mnt: prefijo de montaje de partici√≥n temporal para instalar el n√∫cleo; </li><li>  -f - fuerza de creaci√≥n de matriz.  Si la matriz ZFS se recopil√≥ en discos anteriormente, entonces el comando de creaci√≥n no funcionar√°, nunca se sabe, tal vez cometi√≥ un error y desea borrar datos importantes; </li></ul><br><blockquote><p>  Indico habitualmente el nombre de la matriz de discos del sistema ra√≠z como tanque, aunque en la actualidad prefieren usar el nombre rpool (grupo ra√≠z) en el entorno Linux.  En mi pr√°ctica, generalmente uso este nombre de matrices: </p><br><ul><li>  tanque: la matriz del sistema principal; </li><li>  store: una matriz adicional con discos grandes para el almacenamiento de datos; </li><li>  cach√©: una matriz adicional de discos SSD, si la partici√≥n principal no est√° en ellos; </li></ul><br><p>  En general, recomiendo desarrollar inmediatamente una pr√°ctica de nombrar algo que no se confunda. </p></blockquote><br><h2>  3. Instalaci√≥n del sistema </h2><br><h3>  3.1.  y 3.2.  Crear un sistema de archivos ra√≠z </h3><br><blockquote>  Combin√© espec√≠ficamente los p√°rrafos 3.1.  y 3.2.  ya que creo que especificar la partici√≥n ra√≠z en el tercer nivel es absolutamente redundante.  Eso es cierto, durante varios a√±os de trabajo con ZFS, nunca tuve que hacer ninguna manipulaci√≥n con la partici√≥n ra√≠z.  Adem√°s, hay im√°genes con las que puedes hacer puntos de control.  Por lo tanto, mi secci√≥n ra√≠z es tank / root: </blockquote><br><pre> <code class="bash hljs">zfs create -o mountpoint=/ tank/root</code> </pre> <br><blockquote>  Al mismo tiempo, se detecta el primer error fatal en la instrucci√≥n original, a saber, la falta de una partici√≥n de arranque para la matriz de discos: </blockquote><br><pre> <code class="bash hljs">zpool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> bootfs=tank/root tank</code> </pre> <br><h3>  3.3.  Crear particiones adicionales </h3><br><blockquote><p>  En esta parte de las instrucciones b√°sicas puedes tirar todo y olvidar.  Los muchachos obviamente lo exageraron con aplastamiento y opciones, por lo que, en el camino, tuve que arreglar algo.  Es cierto, no ayud√≥ mucho.  Como los problemas posteriores aparecen nuevamente y al final resulta que todo esto no funciona, por lo tanto, en el p√°rrafo 4.11.  Esto se corrige nuevamente. </p><br><p>  Separar una secci√≥n separada para / var / games parece bastante √©pico.  No me importa, pero esto es claramente demasiado. </p><br><p>  El hecho de que las particiones se creen simplemente en ZFS y admitan una jerarqu√≠a no significa que los directorios cl√°sicos deban abandonarse.  Un ejemplo simple: una vez tuve m√°s de 4K particiones ZFS en un grupo de servidores, fue necesario, pero el reinicio del servidor se ralentiz√≥ durante varios minutos debido al montaje de estas particiones. </p></blockquote><br><p>  Comencemos con una pizarra limpia. </p><br><p>  Hay particiones de archivo est√°ticas y din√°micas. </p><br><p>  Las secciones de archivos est√°ticos incluyen secciones con programas y sus configuraciones, se completan una vez y no cambian durante la operaci√≥n.  Al mismo tiempo, las particiones est√°ticas anteriores se dividieron en particiones del sistema y del usuario (/ usr), pero en este momento est√°n mezcladas en los sistemas operativos Linux y no tiene sentido separarlas, y no funcionar√°. </p><br><p>  Las secciones de archivos din√°micos incluyen secciones en las que se almacenan: </p><br><ul><li>  Datos temporales: ecuaci√≥n: tmp, swap; </li><li>  Registros de trabajo: ecuaci√≥n: var / log; </li><li>  Datos del usuario - eq .: inicio; </li><li>  Datos - ecuaci√≥n: var / db y qu√© suerte; </li><li>  Otros resultados del programa en forma de archivos; </li></ul><br><p>  En las familias de Linux, las particiones din√°micas incluyen / tmp y / var, pero esto no es preciso, ya que pueden entrar en / var / lib, programas y bibliotecas, en general, todo est√° mezclado, pero no obstante ... </p><br><p>  Primero debe decidir si crear la partici√≥n / tmp en el disco o en la memoria como tmpfs.  Si creamos en el disco, cree una partici√≥n separada para √©l: </p><br><pre> <code class="bash hljs">zfs create -o mountpoint=legacy tank/tmp</code> </pre> <br><blockquote>  Opciones com.sun: auto-snapshot = false setuid = off bien, no importa c√≥mo se haga el clima, no se complique.  Pero con SWAP lo haremos m√°s adelante en el paso 7. </blockquote><br><p>  Separe la secci√≥n var por separado: </p><br><pre> <code class="bash hljs">zfs create -o mountpoint=legacy tank/var</code> </pre> <br><p>  Y secciones de usuario: </p><br><pre> <code class="bash hljs">zfs create -o mountpoint=/home tank/home zfs create -o mountpoint=legacy tank/home/root</code> </pre> <br><blockquote>  Tiene sentido asignar particiones de usuario, ya que en la pr√°ctica se obstruyen peri√≥dicamente con diferentes artefactos y para que sea m√°s f√°cil monitorearlos, es mejor crear particiones separadas para ellos, as√≠ como el directorio de inicio del usuario ra√≠z (especialmente para aquellos a quienes les gusta trabajar como root).  El uso de cuotas en los directorios de usuarios no solo no ayuda a obstruir el espacio en disco, sino que tambi√©n interfiere, ya que en tales casos, los usuarios comienzan a dejar artefactos en cualquier lugar y puede ser bastante dif√≠cil encontrarlos m√°s tarde.  Esto no se trata, por lo que solo necesita controlar y golpear las manos. </blockquote><br><p>  mount point tank / home / root aparece como heredado, no como / root.  Esto es correcto, ya que el montaje de esta secci√≥n se realiza en la secci√≥n 4.11 </p><br><p>  Ahora necesitamos montar temporalmente nuestras particiones din√°micas en / mnt: </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /mnt/ mkdir var tmp root mount -t zfs tank/var /mnt/var/ mount -t zfs tank/tmp /mnt/tmp/ mount -t zfs tank/home/root /mnt/root/</code> </pre> <br><h3>  3.4 Instalando el n√∫cleo </h3><br><blockquote>  En la instrucci√≥n principal hay un par de comandos innecesarios m√°s, no prestamos atenci√≥n, aparentemente artefactos de experimentos: </blockquote><br><pre> <code class="bash hljs">debootstrap bionic /mnt</code> </pre> <br><p>  Como resultado, deber√≠a obtener algo como esto: </p><br><pre> <code class="bash hljs">zfs list NAME USED AVAIL REFER MOUNTPOINT tank 213M 1.76T 96K legacy tank/home 208K 1.76T 96K /mnt/home tank/home/root 112K 1.76T 112K legacy tank/root 147M 1.76T 147M /mnt tank/tmp 96K 1.76T 96K legacy tank/var 64.6M 1.76T 64.6M legacy</code> </pre><br><p>  El tama√±o de la partici√≥n vac√≠a de 96K, respectivamente, solo el tanque / tmp permaneci√≥ vac√≠o, y el resto se registr√≥ durante la instalaci√≥n del n√∫cleo, lo que significa que las particiones se montaron correctamente. </p><br><h2>  4. Configuraci√≥n del sistema </h2><br><h3>  4.1.  Configurar hosts y nombre de host </h3><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> HOSTNAME &gt; /mnt/etc/hostname <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> ‚Äú127.0.0.1 localhost‚Äù &gt; /mnt/etc/hosts <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> ‚Äú127.0.0.1 HOSTNAME‚Äù &gt;&gt; /mnt/etc/hosts</code> </pre> <br><h3>  4.2.  Configurar la interfaz de red </h3><br><blockquote>  Entonces s√≠, ya tenemos netplan aqu√≠: </blockquote><br><pre> <code class="bash hljs">nano /mnt/etc/netplan/setup.yaml network: version: 2 renderer: networkd ethernets: eno2: dhcp4: no dhcp6: no addresses: [ {{ IP }}/{{ netmask }}, ] gateway4: {{ gateway IP }} nameservers: addresses: [8.8.8.8]</code> </pre> <br><h3>  4.3.  Configurar repositorios apt </h3><br><pre> <code class="bash hljs">nano /mnt/etc/apt/sources.list deb http://archive.ubuntu.com/ubuntu/ bionic main restricted universe deb http://security.ubuntu.com/ubuntu/ bionic-security main restricted universe deb http://archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe</code> </pre> <br><blockquote>  src: no se necesita principalmente </blockquote><br><h3>  4.4.  Montamos secciones de archivos virtuales LiveCD y "vamos" al nuevo sistema </h3><br><pre> <code class="bash hljs">mount --rbind /dev /mnt/dev mount --rbind /proc /mnt/proc mount --rbind /sys /mnt/sys chroot /mnt /bin/bash --login</code> </pre><br><blockquote>  se requiere usar - rbind, pero no - bind </blockquote><br><p>  Ya estamos en el nuevo sistema ... </p><br><h3>  4.5.  Configurar el entorno base </h3><br><pre> <code class="bash hljs">ln -s /proc/self/mounts /etc/mtab chmod 1777 /tmp apt update</code> </pre> <br><p>  Lugar y hora: </p><br><pre> <code class="bash hljs">dpkg-reconfigure locales * en_US.UTF-8 * ru_RU.UTF-8 dpkg-reconfigure tzdata</code> </pre> <br><p>  Y editores adicionales que les gusta lo que: </p><br><pre> <code class="bash hljs">apt install --yes vim nano</code> </pre> <br><h3>  4.6.  Instalaci√≥n de soporte ZFS </h3><br><pre> <code class="bash hljs">apt install --yes --no-install-recommends linux-image-generic apt install --yes zfs-initramfs</code> </pre> <br><h3>  4.8.  Instalar el gestor de arranque </h3><br><p>  Como se dijo anteriormente, estoy usando un MBR desactualizado: </p><br><pre> <code class="bash hljs">apt install --yes grub-pc</code> </pre> <br><blockquote>  Durante la instalaci√≥n del gestor de arranque, es necesario seleccionar todos nuestros discos que identificamos como de arranque, mientras que el instalador jura sobre todos los dem√°s discos, excepto el primero, estamos de acuerdo y hacemos el paso 5 (no est√° claro por qu√© el resto se dej√≥ para m√°s adelante): </blockquote><br><h4>  4.8.1.  (5.1) Verifique que se reconozca el sistema de archivos ra√≠z: </h4><br><pre> <code class="bash hljs">grub-probe / zfs</code> </pre> <br><h4>  4.8.2.  (5.2) Actualizaci√≥n de initrd </h4><br><pre> <code class="bash hljs">update-initramfs -u -k al</code> </pre> <br><h4>  4.8.3.  (5.3) Simplifique la depuraci√≥n de GRUB </h4><br><pre> <code class="bash hljs">vi /etc/default/grub ... GRUB_CMDLINE_LINUX_DEFAULT=<span class="hljs-string"><span class="hljs-string">""</span></span> GRUB_CMDLINE_LINUX=<span class="hljs-string"><span class="hljs-string">"console"</span></span> ...</code> </pre><br><h4>  4.8.4.  (5.4.) Actualizaci√≥n de la configuraci√≥n del gestor de arranque </h4><br><pre> <code class="bash hljs">update-grub</code> </pre> <br><h4>  4.8.5.  (5.5.) Instale el gestor de arranque en cada disco marcado como de arranque </h4><br><pre> <code class="bash hljs">grub-install /dev/sda grub-install /dev/sdb ...</code> </pre><br><blockquote>  Es importante que estos comandos funcionen correctamente.  Para ser sincero, no pude obtener lo contrario al menos una vez, por lo que no s√© qu√© hacer, pero lo m√°s probable es que, si tiene un error, probablemente haya hecho algo mal al marcar el disco (Secci√≥n 2.2.). </blockquote><br><h4>  4.8.6.  (5.6.) Verifique que el m√≥dulo ZFS est√© instalado </h4><br><pre> <code class="bash hljs">ls /boot/grub/*/zfs.mod /boot/grub/i386-pc/zfs.mod</code> </pre> <br><h3>  4.10.  Establecer la contrase√±a de root (¬°dif√≠cil!) </h3><br><pre> <code class="bash hljs">passwd</code> </pre> <br><blockquote>  Y s√≠, instalaremos openssh de inmediato, de lo contrario obtendremos una sorpresa despu√©s del reinicio si trabajamos de forma remota: </blockquote><br><pre> <code class="bash hljs">apt install --yes openssh-server</code> </pre> <br><p>  No olvide corregir la configuraci√≥n de sshd: </p><br><pre> <code class="bash hljs">vi /etc/ssh/sshd_config ... PermitRootLogin yes ... PasswordAuthentication yes ...</code> </pre> <br><h3>  4.11.  Arreglar sistemas de archivos de montaje </h3><br><blockquote>  Aqu√≠ llegamos a lo m√°s interesante.  El hecho es que las particiones ZFS se montan despu√©s del inicio de algunos demonios (tambi√©n influimos en ZFS_INITRD_ADDITIONAL_DATASETS en / etc / default / zfs), que, a su vez, crean una estructura propia en / var y comienzan a completar los registros del sistema.  Cuando llega el momento de montar particiones ZFS, resulta que los puntos de montaje no est√°n vac√≠os y no se monta nada, los datos est√°n dispersos, todo est√° mal.  Por lo tanto, debe especificar puntos de montaje en / etc / fstab ya que systemd se enfoca principalmente en ellos al acceder a la carpeta: </blockquote><br><pre> <code class="bash hljs">vi /etc/fstab tank/var /var zfs noatime,nodev 0 0 tank/tmp /tmp zfs noatime,nodev 0 0 tank/home/root /root zfs noatime,nodev 0 0</code> </pre> <br><blockquote>  El resto depende de la cl√°usula 6.  ya hecho </blockquote><br><h2>  6. Primer reinicio </h2><br><h3>  6.1.  Tome una foto de la partici√≥n ra√≠z </h3><br><pre> <code class="bash hljs">zfs snapshot tank/root@setup</code> </pre> <br><blockquote>  No tiene sentido para √©l, en la pr√°ctica nunca he sacudido la partici√≥n ra√≠z del sistema y nunca he usado instant√°neas de esta partici√≥n, pero de todos modos dej√© que mienta, puede ser √∫til </blockquote><br><h3>  6.2.  Salir chroot </h3><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> <br><h3>  6.3.  Desmontar particiones de LiveCD y exportar matriz ZFS </h3><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> mount | grep -v zfs | tac | awk <span class="hljs-string"><span class="hljs-string">'/\/mnt/ {print $3}'</span></span> | xargs -i{} umount -lf {} umount /mnt/root umount /mnt/var umount /mnt/tmp zpool <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> tank</code> </pre> <br><blockquote>  Se requiere la exportaci√≥n de matriz de disco para borrar el cach√© zfs </blockquote><br><h3>  6.4 Reiniciar </h3><br><blockquote>  El reinicio se realiza mejor en el terminal LiveCD, ya que si trabaja a trav√©s de un cliente ssh, reiniciarlo puede provocar que el servidor se "congele". </blockquote><br><pre> <code class="bash hljs">reboot</code> </pre> <br><blockquote>  Sin embargo, si algo sali√≥ mal y el servidor no se reinici√≥, puede reiniciarlo de cualquier manera, ya que la matriz ZFS se exporta y es dif√≠cil da√±arla. </blockquote><br><h3>  6.5.  Estamos esperando un reinicio y vamos como root </h3><br><h3>  6.6.  Crea tu cuenta de usuario </h3><br><pre> <code class="bash hljs">zfs create tank/home/{{ LOGIN }} useradd -u {{ UID }} -G adm,sudo -d /home/{{ LOGIN }}/ -s /bin/bash {{ LOGIN }} cp -a /etc/skel/.[!.]* /home/{{ LOGIN }} chown -R {{ LOGIN }}:{{ LOGIN }} /home/{{ LOGIN }}</code> </pre> <br><p>  Agregue la clave ssh p√∫blica al usuario y configure la contrase√±a para √©l: </p><br><pre> <code class="bash hljs">su - {{ LOGIN }} mkdir .ssh chmod 0700 .ssh vi .ssh/authorized_keys <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span> passwd {{ LOGIN }}</code> </pre> <br><blockquote>  En OpenSSH eliminamos la posibilidad de iniciar sesi√≥n como autenticaci√≥n de root y contrase√±a: </blockquote><br><pre> <code class="bash hljs">vi /etc/ssh/sshd_config ... PermitRootLogin no ... PubkeyAuthentication yes ... PasswordAuthentication no ... service ssh restart</code> </pre> <br><h3>  6.7.  6.8.  Ya no es necesario </h3><br><h2>  7. Configuraci√≥n de intercambio </h2><br><h3>  7.1.  Crear una partici√≥n ZFS </h3><br><pre> <code class="bash hljs">zfs create \ -V 32G \ -b $(getconf PAGESIZE) \ -o compression=<span class="hljs-built_in"><span class="hljs-built_in">zle</span></span> \ -o logbias=throughput \ -o sync=always \ -o primarycache=metadata \ -o secondarycache=none \ tank/swap</code> </pre> <br><ul><li>  -V 32G: el tama√±o de nuestro SWAP, puede determinar el que realmente se necesita; </li><li>  -b $ (getconf PAGESIZE) - tama√±o de bloque (4K con ashift = 12); </li><li>  compresi√≥n = zle: elija el algoritmo de compresi√≥n que sea m√≠nimo en t√©rminos de consumo de recursos, ya que de hecho el tama√±o del bloque es 4K, la compresi√≥n como tal no permitir√° la utilizaci√≥n de entrada-salida, pero ser√° posible ahorrar en bloques cero; </li><li>  logbias = throughput - configurando el ancho de banda para optimizar las operaciones sincr√≥nicas; </li><li>  sync = always: siempre sincroniza el registro.  Esto reduce ligeramente el rendimiento, pero garantiza totalmente la fiabilidad de los datos; </li><li>  primarycache = metadata: solo metadatos de cach√©, ya que el intercambio no se usar√° para leer el mismo bloque varias veces; </li><li>  darycache = none: deshabilita la cach√© secundaria por completo por los motivos mencionados anteriormente; </li></ul><br><h3>  7.2.  Configurar la partici√≥n de intercambio </h3><br><pre> <code class="bash hljs">mkswap -f /dev/zvol/tank/swap <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> /dev/zvol/tank/swap none swap defaults 0 0 &gt;&gt; /etc/fstab <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> RESUME=none &gt; /etc/initramfs-tools/conf.d/resume</code> </pre><br><h3>  7.3.  Activar intercambio </h3><br><pre> <code class="bash hljs">swapon -av</code> </pre> <br><blockquote><p>  Adem√°s de las instrucciones, no hay mucho inter√©s, ya que depende en gran medida de las preferencias de administradores espec√≠ficos y las tareas del servidor en su conjunto, excepto por un punto, a saber: "Arranque de emergencia" </p><p>  Y no olvides poner Firewall </p></blockquote><br><h2>  R. Bota de emergencia </h2><br><p>  Preparamos el entorno de instalaci√≥n (elemento 1.) </p><br><p>  Durante la preparaci√≥n, se importa la matriz ZFS, por lo que debe volver a importarla, pero con el punto de montaje correcto: </p><br><pre> <code class="bash hljs">zpool <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> -a zpool import -N -R /mnt tank zfs mount -a</code> </pre> <br><blockquote> , ,      ,        fstab,    : </blockquote><br><pre> <code class="bash hljs">mount -t zfs tank/var /mnt/var/ mount -t zfs tank/tmp /mnt/tmp/ mount -t zfs tank/home/root /mnt/root/</code> </pre> <br><p> ,   ,   chroot   .4.4.,          . 6.3. </p><br><h2> D.   </h2><br><p>   3.3.             .      ,        : ,       /spool,      /data.       ZFS     . </p><br><h2>  Resumen </h2><br><ul><li>       ZFS  ,     ,     ; </li><li>        ZFS,                 ,     .    ZFS ‚Äî       ,   ; </li><li>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">.</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/439860/">https://habr.com/ru/post/439860/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../439848/index.html">Ni una sola VPN. Hoja de trucos sobre c√≥mo protegerse y proteger sus datos</a></li>
<li><a href="../439850/index.html">Detecci√≥n de emociones contextuales en conversaciones textuales usando redes neuronales</a></li>
<li><a href="../439852/index.html">Lanzamiento de la aplicaci√≥n de control remoto: Aspia 1.1.0</a></li>
<li><a href="../439854/index.html">Eh, una, una vez m√°s: qu√© hacer con un cliente en CRM despu√©s de comprar</a></li>
<li><a href="../439858/index.html">Prometheus + Grafana + Node Exporter + Docker en Azure con notificaciones en Telegram</a></li>
<li><a href="../439862/index.html">Eventos digitales en Mosc√∫ del 11 al 17 de febrero.</a></li>
<li><a href="../439864/index.html">Gesti√≥n del conocimiento, por qu√© y c√≥mo lo hicimos.</a></li>
<li><a href="../439866/index.html">Los principios de dise√±o de directorios de nomenclatura en 1C Enterprise Management 2 (ERP 2.4.6)</a></li>
<li><a href="../439868/index.html">La vida sin Facebook: opiniones menos radicales, buen humor, m√°s tiempo para los seres queridos. Ahora probado por la ciencia</a></li>
<li><a href="../439870/index.html">El video como motor de progreso: la evoluci√≥n de los sistemas de vigilancia</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>