<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëö ‚õé üéæ As redes neurais preferem texturas e como lidar com isso. üï¥üèø üéÆ üíö</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recentemente, houve v√°rios artigos criticando o ImageNet, talvez o conjunto de imagens mais famoso usado para treinar redes neurais. 


 No primeiro a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>As redes neurais preferem texturas e como lidar com isso.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  Recentemente, houve v√°rios artigos criticando o ImageNet, talvez o conjunto de imagens mais famoso usado para treinar redes neurais. </p><br><p>  No primeiro artigo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aproximando CNNs com modelos de saco de recursos locais funciona surpreendentemente bem no ImageNet, os</a> autores adotam um modelo semelhante ao saco de palavras e usam fragmentos da imagem como ‚Äúpalavras‚Äù.  Esses fragmentos podem ter at√© 9x9 pixels.  E, ao mesmo tempo, nesse modelo, onde qualquer informa√ß√£o sobre o arranjo espacial desses fragmentos est√° completamente ausente, os autores obt√™m uma precis√£o de 70 a 86% (por exemplo, a precis√£o de um ResNet-50 regular √© de ~ 93%). </p><br><p>  No segundo artigo das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CNNs treinadas pela ImageNet s√£o tendenciosas em rela√ß√£o √† textura, os</a> autores concluem que todo o conjunto de dados da ImageNet e a maneira como as pessoas e as redes neurais percebem as imagens s√£o falhas e sugerem o uso de um novo conjunto de dados - Stylized-ImageNet. </p><br><p>  Mais detalhadamente sobre o que as pessoas veem nas fotos e quais redes neurais <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  O conjunto de dados ImageNet come√ßou a ser criado em 2006 pelos esfor√ßos do professor Fei-Fei Li e continua a evoluir at√© hoje.  No momento, ele cont√©m cerca de 14 milh√µes de imagens pertencentes a mais de 20 mil categorias diferentes. </p><br><p>  Desde 2010, um subconjunto desse conjunto de dados, conhecido como ImageNet 1K com ~ 1 milh√£o de imagens e milhares de classes, tem sido usado no Desafio de reconhecimento visual do ImageNet em grande escala (ILSVRC).  Nesta competi√ß√£o, em 2012, a AlexNet, uma rede neural convolucional, alcan√ßou a precis√£o m√°xima de 1% em 60% e a faixa 5 em 80%. <br>  √â nesse subconjunto do conjunto de dados que as pessoas do ambiente acad√™mico medem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">seu SOTA</a> quando oferecem novas arquiteturas de rede. </p><br><p>  Um pouco sobre o processo de aprendizado neste conjunto de dados.  Falaremos sobre o protocolo de treinamento no ImageNet no ambiente acad√™mico.  Ou seja, quando nos s√£o mostrados os resultados de alguma rede SE, ResNeXt ou DenseNet, o processo se parece com o seguinte: a rede aprende por 90 eras, a velocidade de aprendizado diminui nas 30a e 60a, a cada 10 vezes, como um otimizador um SGD comum com uma pequena redu√ß√£o de peso √© selecionado, apenas RandomCrop e HorizontalFlip s√£o usados ‚Äã‚Äãem aprimoramentos, a imagem geralmente √© redimensionada para 224x224 pixels. </p><br><p>  Aqui est√° um exemplo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">script pytorch</a> para treinamento no ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Voltemos aos artigos mencionados anteriormente.  No primeiro deles, os autores queriam um modelo mais f√°cil de interpretar do que as redes profundas comuns.  Inspirados na id√©ia de modelos de bolsa de recursos, eles criam sua pr√≥pria fam√≠lia de modelos - BagNets.  Utilizando como base a rede ResNet-50 usual. </p><br><p>  Substituindo algumas convolu√ß√µes 3x3 por 1x1 no ResNet-50, elas garantem que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">campo receptivo dos</a> neur√¥nios na √∫ltima camada convolucional seja significativamente reduzido, at√© 9x9 pixels.  Assim, eles limitam as informa√ß√µes dispon√≠veis para um neur√¥nio individual a um fragmento muito pequeno de toda a imagem - um fragmento de v√°rios pixels.  Deve-se observar que, para o novo ResNet-50, o tamanho do campo receptivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√© superior a 400 pixels, o</a> que cobre completamente a imagem, que geralmente √© redimensionada para 224x224 pixels. </p><br><p>  Esse patch √© o fragmento <strong>m√°ximo</strong> da imagem da qual o modelo pode extrair dados espaciais.  No final do modelo, todos os dados foram simplesmente resumidos e o modelo n√£o conseguiu saber onde cada patch est√° localizado em rela√ß√£o a outros patches. <br>  No total, foram testadas tr√™s variantes de redes com campo receptivo 9x9, 17x17 e 33x33.  E, apesar da completa falta de informa√ß√µes espaciais, esses modelos foram capazes de obter boa precis√£o na classifica√ß√£o no ImageNet.  A precis√£o das 5 principais para os patches 9x9 foi de 70%, para 17x17 - 80%, para 33x33 - 86%.  Para compara√ß√£o, a precis√£o do top 5 da ResNet-50 √© de aproximadamente 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  A estrutura do modelo √© mostrada na figura acima.  Cada patch de qxqx3 pixels cortado da imagem √© transformado em um vetor 2048 pela rede.Em seguida, esse vetor √© alimentado √† entrada de um classificador linear, que produz pontua√ß√µes para cada uma das 1000 classes.  Ao coletar pontua√ß√µes de cada patch em uma matriz 2D, voc√™ pode obter um mapa de calor para cada classe e cada pixel da imagem original.  As pontua√ß√µes finais para a imagem foram obtidas somando o mapa de calor de cada classe. </p><br><p>  Exemplos de mapas de calor para algumas classes: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Heatmaps"><br>  Como voc√™ pode ver, a maior contribui√ß√£o para o benef√≠cio de uma classe espec√≠fica √© feita por patches localizados nas bordas dos objetos.  Patches do plano de fundo s√£o quase ignorados.  At√© agora, tudo est√° indo bem. </p><br><p>  Vejamos os patches mais informativos: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="Pacotes informativos"></p><br><p>  Por exemplo, os autores participaram de quatro aulas.  Para cada um deles, foram selecionados 2x7 patches mais significativos (ou seja, patches em que a pontua√ß√£o dessa classe foi mais alta).  A linha superior dos 7 patches √© obtida de imagens apenas da classe correspondente, a parte inferior - de toda a amostra de imagens. </p><br><p>  O que pode ser visto nessas fotos √© not√°vel.  Por exemplo, para a classe de tenca (tenca, peixe), os dedos s√£o uma caracter√≠stica.  Sim, dedos humanos comuns sobre um fundo verde.  E tudo porque h√° um pescador em quase todas as imagens dessa classe, que de fato segura esse peixe nas m√£os, exibindo um trof√©u. </p><br><div class="spoiler">  <b class="spoiler_title">Exemplos do ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Para computadores laptop, um recurso caracter√≠stico s√£o as teclas de letras.  As teclas da m√°quina de escrever tamb√©m contam para essa classe. </p><br><p>  Uma caracter√≠stica da capa de um livro s√£o as letras em um fundo colorido.  Que seja uma inscri√ß√£o em uma camiseta ou em uma bolsa. </p><br><p>  Parece que esse problema n√£o deve nos incomodar.  Uma vez que √© inerente apenas a uma classe estreita de redes com um campo receptivo muito limitado.  Al√©m disso, os autores calcularam a correla√ß√£o entre os logits (sa√≠das de rede antes do softmax final) atribu√≠dos a cada classe BagNet com campo receptivo diferente e os logits do VGG-16, que possui um campo receptivo suficientemente grande.  E eles a acharam bem alta. </p><br><div class="spoiler">  <b class="spoiler_title">Correla√ß√£o entre BagNets e VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Os autores se perguntaram se o BagNet cont√©m dicas sobre como outras redes tomam decis√µes. </p><br><p>  Para um dos testes, eles usaram uma t√©cnica como Image Scrambling.  Que consistia em usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um gerador de textura baseado em matrizes de grama</a> para compor uma imagem em que as texturas s√£o salvas, mas faltam informa√ß√µes espaciais. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p>  O VGG-16, treinado em imagens completas comuns, lidou muito bem com essas imagens embaralhadas.  Sua precis√£o no top 5 caiu de 90% para 80%.  Ou seja, mesmo redes com um campo receptivo bastante amplo ainda preferem lembrar texturas e ignorar informa√ß√µes espaciais.  Portanto, sua precis√£o n√£o caiu muito nas imagens embaralhadas. </p><br><p>  Os autores realizaram uma s√©rie de experimentos em que compararam quais partes das imagens s√£o mais significativas para BagNet e outras redes (VGG-16, ResNet-50, ResNet-152 e DenseNet-169).  Tudo indicava que outras redes, como a BagNet, confiam em pequenos fragmentos de imagens e cometem os mesmos erros ao tomar decis√µes.  Isso foi especialmente not√°vel para redes n√£o muito profundas, como a VGG. </p><br><p>  Essa tend√™ncia das redes de tomar decis√µes baseadas em texturas, diferentemente de n√≥s, pessoas que preferem a forma (veja a figura abaixo), levou os autores do segundo artigo a criar um novo conjunto de dados baseado no ImageNet. </p><br><h3 id="stylized-imagenet">  ImageNet estilizado </h3><br><p>  Antes de tudo, os autores do artigo, usando a transfer√™ncia de estilos, criaram um conjunto de imagens em que a forma (dados espaciais) e as texturas de uma imagem se contradiziam.  E comparamos os resultados de pessoas e redes profundas de convolu√ß√£o de diferentes arquiteturas em um conjunto de dados sintetizados de 16 classes. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  Na figura da extrema direita, as pessoas veem um gato, uma rede - um elefante. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Compara√ß√£o dos resultados de pessoas e redes neurais. </p><br><p>  Como voc√™ pode ver, as pessoas ao atribuir um objeto a uma classe espec√≠fica confiavam na forma dos objetos, nas redes neurais nas texturas.  Na figura acima, as pessoas viram um gato, uma rede - um elefante. </p><br><blockquote>  Sim, aqui voc√™ pode encontrar falhas no fato de que as redes tamb√©m est√£o certas e isso, por exemplo, pode ser um elefante fotografado de perto com uma tatuagem de um gato amado.  Mas o fato de as redes ao tomarem decis√µes se comportarem de maneira diferente das pessoas, os autores consideraram o problema e come√ßaram a procurar maneiras de resolv√™-lo. </blockquote><p>  Como mencionado acima, confiando apenas em texturas, a rede √© capaz de obter um bom resultado com uma precis√£o de 86% entre as 5 melhores.  E n√£o se trata de v√°rias classes, nas quais as texturas ajudam a classificar as imagens corretamente, mas da maioria das classes. </p><br><p>  O problema est√° no pr√≥prio ImageNet, pois ser√° mostrado mais adiante que a rede √© capaz de aprender o formul√°rio, mas n√£o o faz, porque as texturas s√£o suficientes para esse conjunto de dados e os neur√¥nios respons√°veis ‚Äã‚Äãpelas texturas est√£o em camadas rasas, que s√£o muito mais f√°ceis de treinar. </p><br><p>  Usando esse tempo, um mecanismo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">transfer√™ncia de estilo r√°pido AdaIN</a> ligeiramente diferente, os autores criaram um novo conjunto de dados - o Stylized ImageNet.  A forma dos objetos foi tirada do ImageNet e o conjunto de texturas dessa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">competi√ß√£o no Kaggle</a> .  O script para gera√ß√£o est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no link</a> . </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  Al√©m disso, por uma quest√£o de brevidade, o ImageNet ser√° referido como <strong>IN</strong> , o ImageNet estilizado como <strong>SIN</strong> . </p><br><p>  Os autores utilizaram o ResNet-50 e tr√™s BagNet com campo receptivo diferente e treinaram em um modelo separado para cada conjunto de dados. </p><br><p>  E aqui est√° o que eles fizeram: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  O que vemos aqui.  O ResNet-50 treinado em IN √© completamente incapacitado no SIN.  O que confirma parcialmente que, durante o treinamento em IN, a rede se adapta √†s texturas e ignora a forma dos objetos.  Ao mesmo tempo, o ResNet-50 treinado no SIN lida perfeitamente com o SIN e com o IN.  Ou seja, se √© privado de um caminho simples, a rede segue um caminho dif√≠cil - ensina a forma dos objetos. <br>  O BagNet finalmente come√ßou a se comportar como esperado, especialmente em pequenas corre√ß√µes, pois n√£o tem nada a que se agarrar - as informa√ß√µes de textura est√£o simplesmente ausentes do SIN. </p><br><p>  Nas dezesseis aulas mencionadas anteriormente, o ResNet-50, treinado no SIN, come√ßou a dar respostas mais semelhantes √†s que as pessoas d√£o: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  Al√©m de simplesmente treinar o ResNet-50 no SIN, os autores tentaram treinar a rede em um conjunto misto de SIN e IN, incluindo o ajuste fino separadamente em IN puro. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Como voc√™ pode ver, ao usar o SIN + IN para treinamento, os resultados melhoraram n√£o apenas na tarefa principal - classifica√ß√£o de imagens no ImageNet, mas tamb√©m na tarefa de detectar objetos no conjunto de dados PASCAL VOC 2007. </p><br><p>  Al√©m disso, as redes treinadas pelo SIN se tornaram mais resistentes a v√°rios ru√≠dos nos dados. </p><br><h3 id="zaklyuchenie">  Conclus√£o </h3><br><p>  Mesmo agora, em 2019, ap√≥s sete anos de sucesso com a AlexNet, quando as redes neurais s√£o amplamente usadas na vis√£o computacional, quando o ImageNet 1K de fato se tornou o padr√£o para avaliar o desempenho de modelos no ambiente acad√™mico, o mecanismo de como as redes neurais tomam decis√µes n√£o est√° totalmente claro. .  E como os conjuntos de dados nos quais essas redes foram treinadas influenciam isso. </p><br><p>  Os autores do primeiro artigo tentaram esclarecer como essas decis√µes s√£o tomadas em redes com arquitetura de saco de recursos com um campo receptivo limitado, mais f√°cil de interpretar.  E, comparando as respostas da BagNet e as redes neurais profundas usuais, chegamos √† conclus√£o de que os processos de tomada de decis√£o nelas s√£o bastante semelhantes. </p><br><p>  Os autores do segundo artigo compararam como as pessoas e as redes neurais percebem imagens em que formas e texturas se contradizem.  E eles sugeriram o uso de um novo conjunto de dados, o Stylized ImageNet, para reduzir as diferen√ßas de percep√ß√£o.  Tendo recebido como b√¥nus um aumento na precis√£o da classifica√ß√£o no ImageNet e detec√ß√£o em conjuntos de dados de terceiros. </p><br><p>  A principal conclus√£o pode ser feita da seguinte forma: redes que aprendem com imagens, com a capacidade de lembrar propriedades espaciais de objetos de n√≠vel superior, preferem uma maneira mais f√°cil de atingir a meta - superestimar as texturas.  Se o conjunto de dados no qual eles treinam permitir isso. </p><br><p>  Al√©m do interesse acad√™mico, o problema da adapta√ß√£o excessiva de texturas √© importante para todos n√≥s que usamos modelos pr√©-treinados para transferir o aprendizado em suas tarefas. <br>  Uma conseq√º√™ncia importante de tudo isso para n√≥s √© que voc√™ n√£o deve confiar no peso dos modelos que geralmente s√£o pr√©-treinados no ImageNet, j√° que para a maioria deles foram usados ‚Äã‚Äãaprimoramentos bastante simples, que de forma alguma contribuem para se livrar do excesso de ajuste.  E √© melhor, se poss√≠vel, ter modelos treinados com aprimoramentos mais graves ou estilizados ImageNet + ImageNet no ninho.  Sempre poder comparar qual √© o mais adequado para a nossa tarefa atual. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt453788/">https://habr.com/ru/post/pt453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt453776/index.html">Bombeamos designers da empresa: do j√∫nior ao diretor de arte</a></li>
<li><a href="../pt453778/index.html">Como criamos um banco on-line para empresas. Parte Um: Rebranding</a></li>
<li><a href="../pt453780/index.html">Como escolher um telefone SIP Grandstream - em geral e em particular?</a></li>
<li><a href="../pt453782/index.html">Infinito UIScrollView</a></li>
<li><a href="../pt453784/index.html">Como o especialista em DevOps √© v√≠tima de automa√ß√£o</a></li>
<li><a href="../pt453790/index.html">"O cliente se foi - √© para sempre?" Como contar a rotatividade de clientes em SaaS e o que h√° de errado nas m√©tricas b√°sicas</a></li>
<li><a href="../pt453792/index.html">Sistemas de recomenda√ß√£o: id√©ias, abordagens, tarefas</a></li>
<li><a href="../pt453796/index.html">As pessoas precisam de matem√°tica?</a></li>
<li><a href="../pt453800/index.html">Como resolver o "Campo Minado" (e torn√°-lo melhor)</a></li>
<li><a href="../pt453804/index.html">O livro "Competitividade e simultaneidade na plataforma .NET. Padr√µes de design eficazes ‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>