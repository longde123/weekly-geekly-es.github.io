<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👂🏾 ☮️ 🤴🏽 部署CEPH分布式存储并将其连接到Kubernetes 📑 ☃️ 🏴</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="第1部分我们部署用于微服务的环境。 第1部分在裸机上安装Kubernetes HA（Debian） 
 哈，亲爱的读者，您好！ 


 在上一篇文章中，我讨论了如何部署Kubernetes故障转移群集。 但是事实是，在Kubernetes中，部署不需要维护状态或使用数据的无状态应用程序很方便。 但是...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>部署CEPH分布式存储并将其连接到Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">第1部分我们部署用于微服务的环境。</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">第1部分在裸机上安装Kubernetes HA（Debian）</a> </p><br><h2> 哈，亲爱的读者，您好！ </h2><br><p> 在上一篇<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文章中，</a>我讨论了如何部署Kubernetes故障转移群集。 但是事实是，在Kubernetes中，部署不需要维护状态或使用数据的无状态应用程序很方便。 但是在大多数情况下，我们需要保存数据，并且在重新启动炉床时不会丢失数据。 <br>  Kubernetes将卷用于这些目的。 当我们使用Kubernetes云解决方案时，没有特别的问题。 我们只需要从Google，Amazon或其他云提供商订购所需的卷，然后在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">文档的</a>指导下将收到的卷连接到Pod。 <br> 当我们处理裸机时，事情要复杂一些。 今天，我想谈谈基于ceph的使用的解决方案之一。 </p><br><p> 在此出版物中，我将告诉您： </p><br><ul><li> 如何部署Ceph分布式存储 </li><li> 使用Kubernetes时如何使用Ceph <a name="habracut"></a></li></ul><br><h2> 引言 </h2><br><p> 首先，我想向您解释这篇文章对谁有用。 首先，对于那些根据我的第一个出版物部署了集群以继续构建微服务架构的读者。 其次，对于那些想要自己部署ceph集群并评估其性能的人。 </p><br><p> 在本出版物中，我不会针对任何需求来讨论集群规划的主题，我只会谈论一般的原理和概念。 我不会深入研究“调优”和深度调优，有关此主题的出版物很多，包括关于Habr的出版物。 本文将具有更多介绍性，但同时，它还使您能够获得可行的解决方案，以适应将来的需求。 </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">主机，主机资源，操作系统和软件版本的列表</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Ceph集群结构</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">安装前配置集群节点</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">安装ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">创建一个ceph集群</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">网络设置</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">安装ceph软件包</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">监视器的安装和初始化</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">添加OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">将Ceph连接到kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">创建数据池</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">创建客户机密</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">部署ceph rbd供应商</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">创建一个存储类</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes + Ceph韧带测试</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">物品准备中使用的材料清单</a> </li></ol><br><a name="vm"></a><br><h2> 主机列表和系统要求 </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>名称</b> </th><th>  <b>IP地址</b> </th><th>  <b>留言</b> </th></tr><tr><td>  ceph01测试 </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  ceph02测试 </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  ceph03测试 </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p> 在撰写文章时，我使用具有此配置的虚拟机 </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p> 每个都安装了Debian 9.5操作系统。 这些是测试机器，每个都有两个磁盘，第一个用于OS，第二个用于OSD cef。 </p><br><p> 我将通过ceph-deploy实用程序部署集群。 您可以在手动模式下部署ceph集群，所有步骤均在文档中进行了描述，但是本文的目的是告诉您可以多快部署ceph并开始在kubernetes中使用它。 <br>  Ceph的资源非常繁琐，尤其是RAM。 为了获得良好的速度，建议使用ssd驱动器。 </p><br><p> 您可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方ceph文档中</a>阅读有关要求的更多信息<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">。</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Ceph集群结构 </h2><br><p>  <strong>蒙</strong> <br>  <em>监视器是一个守护程序，它充当协调程序的开始，群集从该守护程序开始。</em>  <em>一旦我们至少有一个工作监视器，便有了一个Ceph集群。</em>  <em>监视器通过与其他监视器交换各种卡来存储有关群集的健康状况的信息。</em>  <em>客户端转向监视器，以了解向哪个OSD写入/读取数据。</em>  <em>部署新存储时，要做的第一件事是创建一个（或多个）监视器。</em>  <em>群集可以驻留在一个监视器上，但是建议制作3个或5个监视器，以避免整个系统因单个监视器的故障而损坏。</em>  <em>最主要的是，这些数目应该是奇数，以避免出现裂脑情况。</em>  <em>监视器按法定人数运行，因此，如果超过一半的监视器掉落，则会阻塞群集以防止数据不一致。</em> <br>  <strong>经理</strong> <br>  <em>Ceph Manager守护程序与Monitor守护程序一起使用以提供其他控制。</em> <em><br></em>  <em>从12.x版开始，ceph-mgr守护程序已成为正常操作所必需的。</em> <em><br></em>  <em>如果mgr守护程序未在运行，您将看到关于此的警告。</em> <br>  <strong>OSD（对象存储设备）</strong> <br>  <em>OSD是一个存储单元，用于存储数据本身并通过与其他OSD交换数据来处理客户端请求。</em>  <em>通常是磁盘。</em>  <em>通常，对于每个OSD，都有一个单独的OSD守护程序，该守护程序可以在安装了该磁盘的任何计算机上运行。</em> </p><br><p> 所有这三个守护程序都将在集群中的每台机器上运行。 因此，将监视和管理器守护程序作为服务，将OSD守护程序作为我们虚拟机的一个驱动器。 </p><br><a name="before"></a><br><h2> 安装前配置集群节点 </h2><br><p>  ceph文档指定以下工作流程： </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p> 我将从ceph01-test群集的第一个节点开始工作，它将是Admin Node，它还将包含ceph-deploy实用程序的配置文件。 为了使ceph-deploy实用程序正常运行，必须使用Admin节点通过ssh访问所有群集节点。 为了方便起见，我将在主机中写出集群的简称 </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p> 并将密钥复制到其他主机。 我将从root执行的所有命令。 </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">设定文件</a> </p><br><anchor> 头颅部署 </anchor><br><h2> 安装ceph-deploy </h2><br><p> 第一步是在ceph01-test机器上安装ceph-deploy </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p> 接下来，您需要选择要发布的版本。 但是这里有困难，目前用于Debian OS的ceph仅支持发光软件包。 <br> 如果要发布更新的版本，则必须使用镜像，例如 <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p> 在所有三个节点上添加具有模拟内容的存储库 </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p> 如果发光足以满足您的需求，那么您可以使用官方存储库 </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p> 我们还将在所有三个节点上安装NTP。 </p><br><div class="spoiler">  <b class="spoiler_title">因为此建议在ceph文档中</b> <div class="spoiler_text"><p> 我们建议在Ceph节点上（尤其是在Ceph Monitor节点上）安装NTP，以防止时钟漂移引起问题。 <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p> 确保启用NTP服务。 确保每个Ceph节点使用相同的NTP服务器。 您可以<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在此处</a>查看更多详细信息 </p><br><a name="ceph-install"></a><br><h2> 创建一个ceph集群 </h2><br><p> 为配置文件和ceph-deploy文件创建目录 </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p> 让我们创建一个新的集群配置，在创建时，指示集群中将有三个监视器 </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2> 网络设置 </h2><br><p> 现在重要的是，该谈论ceph的网络了。  Ceph使用两个公共网络和一个群集网络来工作 <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p> 从公共网络图中可以看到，这是用户和应用程序级别，而群集网络是通过其复制数据的网络。 <br> 非常需要将这两个网络彼此分开。 同样，网络速度群集网络至少需要10 Gb。 <br> 当然，您可以将所有内容保留在同一网络上。 但是，这充满了这样的事实：一旦OSD之间的复制量增加，例如，当新的OSD（磁盘）下降或添加时，网络负载就会非常大。 因此，基础架构的速度和稳定性将在很大程度上取决于ceph使用的网络。 <br> 不幸的是，我的虚拟化群集没有单独的网络，我将使用一个公共的网段。 <br> 集群的网络配置是通过配置文件完成的，该文件是我们使用上一个命令生成的。 </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p> 如我们所见，cef部署没有为我们创建默认的网络设置，因此我将public network = {public-network / netmask}参数添加到配置的global部分。 我的网络是10.73.0.0/16，所以添加我的配置后将如下所示 </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p> 如果要将群集网络与公共网络分开，请添加参数cluster network = {cluster-network / netmask} <br> 您<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">可以在文档中阅读</a>有关网络的更多信息<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">。</a> </p><br><a name="ceph-pack"></a><br><h2> 安装ceph软件包 </h2><br><p> 使用ceph-deploy，我们在三个节点上安装了所需的所有ceph软件包。 <br> 为此，请在ceph01-test上执行 <br> 如果版本是模仿的 </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p> 如果版本发光 </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p> 等到一切都建立好。 </p><br><a name="ceph-mon"></a><br><h2> 监视器的安装和初始化 </h2><br><p> 安装完所有软件包后，我们将创建并启动集群的监视器。 <br>  C ceph01-test执行以下操作 </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p> 在此过程中将创建监视器，将启动守护程序，而ceph-deploy将检查仲裁。 <br> 现在，将配置分散在群集节点上。 </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p> 并检查我们集群的状态，如果您正确执行了所有操作，则该状态应为 <br>  HEALTH_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p> 创建mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p> 并再次检查状态 </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p> 应该出现一行 </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p> 我们将配置写入集群中的所有主机 </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2> 添加OSD </h2><br><p> 目前，我们有一个正常工作的集群，但是它还没有用于存储信息的磁盘（在ceph术语中为osd）。 </p><br><p> 可以使用以下命令添加OSD（一般视图） </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p> 在我的测试平台上，disk / dev / sdb分配在osd下，因此在我的情况下，命令如下 </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p> 检查所有OSD是否都在工作。 </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p> 结论 </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p> 您也可以尝试一些有用的OSD命令。 </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p> 和 </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p> 如果一切正常，那么我们有一个正常工作的ceph集群。 在下一部分中，我将告诉您如何在kubernetes中使用ceph </p><br><a name="kubernetes"></a><br><h1> 将Ceph连接到kubernetes </h1><br><p> 不幸的是，我将无法在本文中详细描述Kubernetes卷的操作，因此，我将尝试将其放在一个段落中。 <br>  Kubernetes使用存储类来处理数据的数据量，每个存储类都有自己的配置程序，您可以将其视为处理不同数据存储量的一种“驱动程序”。 支持kubernetes的完整列表可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方文档中</a>找到。 <br>  Kubernetes本身也支持使用rbd，但是官方的kube-controller-manager映像未安装rbd客户端，因此您需要使用其他映像。 <br> 还应注意，以rbd创建的卷（pvc）只能是ReadWriteOnce（RWO），这意味着您只能将创建的卷挂载到一个炉床上。 </p><br><p> 为了使我们的集群能够使用ceph卷，我们需要： <br> 在Ceph集群中： </p><br><ul><li> 在ceph集群中创建数据池 </li><li> 创建一个客户端并访问数据池的密钥 </li><li> 获取ceph管理员机密 </li></ul><br><p> 为了使我们的集群能够使用ceph卷，我们需要： <br> 在Ceph集群中： </p><br><ul><li> 在ceph集群中创建数据池 </li><li> 创建一个客户端并访问数据池的密钥 </li><li> 获取ceph管理员机密 </li></ul><br><p> 在Kubernetes集群中： </p><br><ul><li> 创建ceph管理员密码和ceph客户端密钥 </li><li> 安装ceph rbd提供程序或将kube-controller-manager映像更改为支持rbd的映像 </li><li> 使用Ceph客户端密钥创建秘密 </li><li> 创建存储类 </li><li> 在kubernetes工人笔记上安装ceph-common </li></ul><br><a name="ceph-pool"></a><br><h2> 创建数据池 </h2><br><p> 在ceph集群中，为kubernetes卷创建一个池 </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p> 在这里，我将做一个小小的解释，最后的数字8 8是pg和pgs的数字。 这些值取决于ceph集群的大小。 有一些特殊的计算器可以计算pg和pg的数量，例如<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">ceph的</a>官方数据 <br> 首先，我建议默认情况下保留它，如果将来可以增加此数量（只能从Nautilus版本中减少）。 </p><br><a name="ceph-key"></a><br><h2> 为数据池创建客户端 </h2><br><p> 为新池创建一个客户端 </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p> 我们将为客户提供一个密钥，将来我们将需要它来创建一个秘密的kubernetes。 </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2> 获取管理员密钥 </h2><br><p> 并获得管理员密钥 </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>在ceph集群上，所有工作都已完成，现在我们需要去一台可以访问kubernetes集群的机器</strong> <br> 我将在第一篇出版物中使用我部署的集群的master01-test（10.73.71.25）。 </p><br><a name="kubernetes-secrets"></a><br><h2> 创建客户机密 </h2><br><p> 使用我们收到的客户令牌创建文件（不要忘记用您的令牌替换它） </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p> 并创建一个我们将来使用的秘密 </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2> 创建管理员机密 </h2><br><p> 使用管理员令牌创建文件（不要忘记将其替换为令牌） </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p> 之后，创建管理员密码 </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p> 检查机密是否已创建 </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2> 方法首先部署ceph rbd Provisioner </h2><br><p> 我们从github克隆了kubernetes-incubator / external-storage存储库，它拥有与ceph存储库成为kubernetes集群好友所需的一切。 </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p> 结论 </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2> 方法二：替换kube-controller-manager映像 </h2><br><p> 官方的kube-controller-manager映像中没有rbd支持，因此我们需要更改controller-manager映像。 <br> 为此，在每个Kubernetes向导上，您需要编辑kube-controller-manager.yaml文件，并将图像替换为gcr.io/google_containers/hyperkube:v1.15.2。 请注意与您的Kubernetes集群版本匹配的映像版本。 </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p> 之后，您将需要重新启动kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Pod应该自动更新，但是如果由于某种原因没有发生，可以通过删除手动重新创建。 </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p> 检查一切正常 </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  -- </p><br><a name="storage-class"></a><br><h2> 创建一个存储类 </h2><br><p>  <strong>方法一-如果使用预配器ceph.com/rbd</strong> <br> 创建一个带有我们存储类描述的yaml文件。 另外，下面使用的所有文件都可以下载<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">到我</a>在ceph目录中的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">存储库</a>中 </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p> 并将他嵌入我们的集群中 </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p> 检查一切正常 </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>方法二-如果您使用了预配器kubernetes.io/rbd</strong> <br> 创建存储类 </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p> 并将他嵌入我们的集群中 </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p> 检查一切正常 </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Kubernetes + Ceph韧带测试 </h2><br><p> 在测试ceph + kubernetes之前，必须在集群的每个工作代码上安装ceph-common软件包。 </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p> 创建一个Yaml文件PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p> 杀了他 </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p> 检查是否已创建PersistentVolumeClaim。 </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p> 并且还自动创建了PersistentVolume。 </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p> 让我们创建一个测试容器，在其中将创建的pvc包含在/ mnt目录中。 使用文本“ Hello World！”运行此文件/mnt/test.txt。 </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p> 我们将杀死他并确认他已完成任务 </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p> 让我们等待状态 </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p> 让我们创建另一个，将其连接到该卷，但已经在/ mnt / test中，然后确保由第一个卷创建的文件到位 </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p> 运行kubectl get po -w并等待直到pod运行 <br> 之后，让我们进入该目录并检查该卷是否已连接以及我们在/ mnt / test目录中的文件 </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p> 感谢您阅读到最后。 抱歉，发布延迟。 <br> 我已经准备好回答个人信息或个人资料中指示的社交网络中的所有问题。 <br> 在下一个小型出版物中，我将告诉您如何基于创建的ceph集群，然后根据第一个出版物中的计划来部署S3存储。 </p><br><a name="book"></a><br><p> 出版材料 </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">官方Ceph文档</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">在图片中介绍Ceph存储库</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Kubernetes官方文档</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">kubernetes培养箱/外部存储</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">带有示例文件的kubernetes-ceph-percona存储库</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN465399/">https://habr.com/ru/post/zh-CN465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN465379/index.html">自制无线独立式胰岛素泵控制</a></li>
<li><a href="../zh-CN465391/index.html">在俄罗斯斯卡拉运动的脚步。 第一部分</a></li>
<li><a href="../zh-CN465393/index.html">MySensors设备的电池电源</a></li>
<li><a href="../zh-CN465395/index.html">依赖注入和服务定位器之间的主要区别是什么？</a></li>
<li><a href="../zh-CN465397/index.html">Nitro转换器的外观如何，可帮助开发人员进行本地化和技术支持</a></li>
<li><a href="../zh-CN465401/index.html">在任何IT团队中加速解决问题的5项活动</a></li>
<li><a href="../zh-CN465403/index.html">啊！ 公路上的新相机或有关雷达和雷达探测器的最新信息</a></li>
<li><a href="../zh-CN465407/index.html">1.极限企业层交换机概述</a></li>
<li><a href="../zh-CN465409/index.html">Vue.js Web开发最佳实践</a></li>
<li><a href="../zh-CN465415/index.html">我们用一种易于理解的语言谈论DevOps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>