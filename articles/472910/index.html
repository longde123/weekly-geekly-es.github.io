<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👮 🍕 🐖 Encuentra texto en carteles y paquetes usando un teléfono inteligente 🕵🏽 👦🏾 ⛽️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El problema de la búsqueda automática de texto en imágenes existe desde hace mucho tiempo, al menos desde principios de los años noventa del siglo pas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Encuentra texto en carteles y paquetes usando un teléfono inteligente</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/472910/">  El problema de la búsqueda automática de texto en imágenes existe desde hace mucho tiempo, al menos desde principios de los años noventa del siglo pasado.  Los veteranos podrían recordarlos por la distribución generalizada de ABBYY FineReader, que puede traducir los escaneos de documentos a sus versiones editables. <br><br>  Los escáneres conectados a computadoras personales funcionan muy bien en las empresas, pero el progreso no se detiene y los dispositivos móviles se han apoderado del mundo.  La gama de tareas para trabajar con texto también ha cambiado.  Ahora es necesario buscar el texto, no en hojas A4 perfectamente rectas con texto negro sobre fondo blanco, sino en varias tarjetas de presentación, menús coloridos, letreros de tiendas y mucho más sobre lo que una persona puede encontrar en la jungla de una ciudad moderna. <br><br> <a href=""><img src="https://habrastorage.org/webt/br/xk/fe/brxkfes7mckw4fwiswow98w4ajy.png"></a> <br>  <i>Un verdadero ejemplo del trabajo de nuestra red neuronal.</i>  <i>Se puede hacer clic en la imagen.</i> <br><br><h2>  Requisitos básicos y limitaciones </h2><br>  Con tal variedad de condiciones para presentar texto, los algoritmos escritos a mano ya no pueden hacer frente.  Aquí, las redes neuronales con su capacidad de generalizar vienen al rescate.  En esta publicación, hablaremos sobre nuestro enfoque para crear una arquitectura de red neuronal que detecte texto en imágenes complejas con buena calidad y alta velocidad. <br><a name="habracut"></a><br>  Los dispositivos móviles imponen restricciones adicionales en la elección del enfoque: <br><br><ol><li>  Los usuarios no siempre tienen la oportunidad de usar una red móvil para comunicarse con el servidor debido al costoso tráfico móvil o problemas de privacidad.  Entonces, soluciones como Google Lens no ayudarán aquí. </li><li>  Dado que nos centramos en el procesamiento local de datos, sería bueno para nuestra solución: <br><ul><li>  Ocupaba poca memoria; </li><li>  Funcionó rápidamente utilizando las capacidades técnicas del teléfono inteligente. </li></ul></li><li>  El texto se puede girar y estar en un fondo aleatorio. </li><li>  Las palabras pueden ser muy largas.  En las redes neuronales convolucionales, el alcance del núcleo de convolución generalmente no cubre la palabra alargada en su conjunto, por lo que tomará algún truco para sortear esta restricción. <br><br><img src="https://habrastorage.org/webt/s5/qw/6h/s5qw6h1fmawausqg-sm_o-7fwm8.png" alt="imagen"><br></li><li> Los tamaños de texto en una foto pueden ser diferentes: <br><br><img src="https://habrastorage.org/webt/ia/a-/om/iaa-omk2j8qxwl-sbqmg1_v1rlc.png" alt="imagen"><br></li></ol><br><h2>  Solución </h2><br>  ¡La solución más simple para el problema de búsqueda de texto que viene a la mente es tomar la mejor red de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">concursos ICDAR</a> (Conferencia Internacional sobre Análisis y Reconocimiento de Documentos) especializados en esta tarea y negocio!  Desafortunadamente, tales redes logran calidad debido a su volumen y complejidad computacional, y solo son adecuadas como una solución en la nube, que no cumple con los párrafos 1 y 2 de nuestros requisitos.  Pero, ¿qué sucede si tomamos una red grande que funciona bien en los escenarios que necesitamos cubrir e intentamos reducirla?  Este enfoque ya es más interesante. <br><br>  Baoguang Shi et al. En su red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SegLink</a> [1] propuso lo siguiente: <br><br><ol><li>  Para encontrar no palabras completas a la vez (áreas verdes en la imagen <b>a</b> ), sino sus partes, llamadas segmentos, con la predicción de su rotación, inclinación y desplazamiento.  Tomemos prestada esta idea. </li><li>  Debe buscar segmentos de palabras en varias escalas a la vez para cumplir con el requisito 5. Los segmentos se muestran con rectángulos verdes en la imagen <b>b</b> . </li><li>  Para evitar que una persona invente cómo combinar estos segmentos, simplemente hacemos que la red neuronal prediga conexiones (enlaces) entre segmentos relacionados con la misma palabra <br><br>  a.  dentro de la misma escala (líneas rojas en la imagen <b>c</b> ) <br><br>  b.  y entre escalas (líneas rojas en la imagen <b>d</b> ), resolviendo el problema de la cláusula 4 de los requisitos. </li></ol><br>  Los cuadrados azules en la imagen a continuación muestran las áreas de visibilidad de los píxeles de las capas de salida de la red neuronal de diferentes escalas, que "ven" al menos parte de la palabra. <br><br><img src="https://habrastorage.org/webt/qx/eu/zs/qxeuzsrcszz4pxafddjyma3cwqq.png" alt="imagen"><br>  <i>Ejemplos de segmentos y enlaces</i> <br><br>  SegLink utiliza la conocida arquitectura VGG-16 como base.  La predicción de segmentos y enlaces se realiza en 6 escalas.  Como primer experimento, comenzamos con la implementación de la arquitectura original.  Resultó que la red contiene 23 millones de parámetros (pesos) que deben almacenarse en un archivo de 88 megabytes de tamaño.  Si crea una aplicación basada en VGG, será uno de los primeros candidatos para la eliminación si no hay suficiente espacio, y la búsqueda de texto en sí funcionará muy lentamente, por lo que la red debe perder peso con urgencia. <br><br><img src="https://habrastorage.org/webt/oj/lp/xo/ojlpxo224mtzrerf6o4urty-gfk.png" alt="imagen"><br>  <i>Arquitectura de red SegLink</i> <br><br><h2>  El secreto de nuestra dieta. </h2><br>  Puede reducir el tamaño de la red simplemente cambiando la cantidad de capas y canales en ella, o cambiando la propia circunvolución y las conexiones entre ellas.  Mark Sandler y sus asociados justo a tiempo recogieron la arquitectura en su red <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNetV2</a> [2] para que funcione rápidamente en dispositivos móviles, ocupe poco espacio y aún no se quede atrás del mismo VGG en calidad de trabajo.  El secreto para acelerar y reducir el consumo de memoria está en tres pasos principales: <br><br><ol><li>  El número de canales con mapas de características en la entrada del bloque se reduce por convolución de puntos a toda la profundidad (el llamado cuello de botella) sin una función de activación. </li><li>  La convolución clásica se reemplaza por una convolución separable por canal.  Tal convolución requiere menos peso y menos cómputo. </li><li>  Las cartas de personaje después del cuello de botella se envían a la entrada del siguiente bloque para sumar sin convoluciones adicionales. </li></ol><br><br><img src="https://habrastorage.org/webt/s8/uq/zb/s8uqzb_r7i6z508gyddsvwztsfc.png" alt="imagen"><br>  <i>Unidad base MobileNetV2</i> <i><br></i> <br><br><h2>  Red neuronal resultante </h2><br>  Usando los enfoques anteriores, llegamos a la siguiente estructura de red: <br><br><ul><li>  Usamos segmentos y enlaces de SegLink </li><li>  Reemplace VGG con un MobileNetV2 menos glotón </li><li>  Reduzca la cantidad de escalas de búsqueda de texto de 6 a 5 para mayor velocidad </li></ul><br><br><img src="https://habrastorage.org/webt/el/t6/8a/elt68a2truovh33paqx1omw1mjg.png" alt="imagen"><br>  <i>Red de resumen de búsqueda de texto</i> <i><br></i> <br><br><h3>  Descifrado de valores en bloques de arquitectura de red </h3><br>  El paso de zancada y el número base de canales en los canales se indican como s &lt;stride&gt; c &lt;channels&gt;, respectivamente.  Por ejemplo, s2c32 significa 32 canales con un desplazamiento de 2. El número real de canales en las capas de convolución se obtiene multiplicando su número base por un factor de escala α, que le permite simular rápidamente diferentes "espesores" de la red.  A continuación se muestra una tabla con el número de parámetros en la red dependiendo de α. <br><br><img src="https://habrastorage.org/webt/ay/k7/ih/ayk7ihjypgcp6vbw609jf9nmtfi.png" alt="imagen"><br><br>  Tipo de bloque: <br><br><ul><li>  Conv2D: una operación de convolución completa; </li><li>  D-wise Conv - convolución separable por canal; </li><li>  Bloques: un grupo de bloques MobileNetV2; </li><li>  Salida: convolución para obtener la capa de salida.  Los valores numéricos de tipo NxN indican el tamaño del campo receptivo del píxel. </li></ul><br>  Como función de activación, los bloques usan ReLU6. <br><br><img src="https://habrastorage.org/webt/a3/yb/ft/a3ybftdxyu7_vuw04wsx-vclcb4.png" alt="imagen" width="400"><br><br>  La capa de salida tiene 31 canales: <br><br><img src="https://habrastorage.org/webt/se/3i/xc/se3ixchyxmnjwp2mdlp0l0czwr4.png" alt="imagen"><br><br>  Los primeros dos canales de la capa de salida votan para que el píxel pertenezca al texto y no al texto.  Los siguientes cinco canales contienen información para reconstruir con precisión la geometría del segmento: desplazamientos verticales y horizontales en relación con la posición del píxel, factores de ancho y alto (ya que el segmento generalmente no es cuadrado) y el ángulo de rotación.  16 valores de enlaces intracanal indican si hay una conexión entre ocho píxeles adyacentes en la misma escala.  Los últimos 8 canales nos informan sobre la presencia de enlaces a cuatro píxeles de la escala anterior (la escala anterior siempre es 2 veces mayor).  Cada 2 valores de segmentos, los enlaces intra y de escala cruzada se normalizan mediante la función softmax.  El acceso a la primera escala no tiene enlaces de escala cruzada. <br><br><h2>  Asamblea de palabras </h2><br>  La red predice si un segmento particular y sus vecinos pertenecen al texto.  Queda por recopilarlos en palabras. <br><br>  Para comenzar, combine todos los segmentos que están vinculados por enlaces.  Para hacer esto, componimos un gráfico donde los vértices son todos los segmentos en todas las escalas, y los bordes son enlaces.  Luego encontramos los componentes conectados de la gráfica.  Para cada componente, ahora es posible calcular la palabra que encierra el rectángulo de la siguiente manera: <br><br><ol><li>  Calculamos el ángulo de rotación de la palabra θ <br><ul><li>  O como el valor promedio de las predicciones del ángulo de rotación de los segmentos, si hay muchos de ellos, </li><li>  O como el ángulo de rotación de la línea obtenida por regresión en los puntos de los centros de los segmentos, si hay pocos segmentos. </li></ul></li><li>  El centro de la palabra se selecciona como el centro de masa de los puntos centrales de los segmentos. </li><li>  Expanda todos los segmentos por -θ para organizarlos horizontalmente.  Encuentra los límites de la palabra. <br><ul><li>  Los límites izquierdo y derecho de la palabra se seleccionan como los límites de los segmentos más a la izquierda y a la derecha, respectivamente. </li><li>  Para obtener el límite superior de la palabra, los segmentos se ordenan por la altura del borde superior, se corta el 20% de los más altos y el valor del primer segmento se selecciona de la lista que queda después del filtrado. </li><li>  El límite inferior se obtiene de los segmentos más bajos con un límite del 20% del más bajo, por analogía con el límite superior. </li></ul></li><li>  Gira el rectángulo resultante de nuevo a θ. </li></ol><br>  La solución final se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>FaSTExt</b> : Extractor de texto rápido y pequeño</a> [3] <br><br><h2>  ¡Experimente el tiempo! </h2><br><h3>  Detalles de entrenamiento </h3><br>  La red misma y sus parámetros se seleccionaron para un buen trabajo en una muestra interna grande, que refleja el escenario principal del uso de la aplicación en el teléfono: apuntó la cámara a un objeto con texto y tomó una foto.  Resultó que una red grande con α = 1 omite en calidad la versión con α = 0.5 en solo un 2%.  Esta muestra no está disponible públicamente, por lo que para mayor claridad, tuve que entrenar a la red en la muestra pública <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICDAR2013</a> , en la que las condiciones de disparo son similares a las nuestras.  La muestra es muy pequeña, por lo que la red se entrenó previamente en una gran cantidad de datos sintéticos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SynthText en el Wild Dataset</a> .  El proceso de pre-entrenamiento tomó aproximadamente 20 días de cálculos para cada experimento en el GTX 1080 Ti, por lo que la operación de la red en datos públicos se verificó solo para las opciones α = 0.75, 1 y 2. <br><br>  Como optimizador, se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">utilizó la</a> versión <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AMSGrad</a> de Adam. <br><br>  Funciones de error: <br><br><ul><li>  Entropía cruzada para la clasificación de segmentos y enlaces; </li><li>  Función de pérdida de Huber para geometría de segmento. </li></ul><br><h2>  Resultados </h2><br>  En términos de la calidad del rendimiento de la red en el escenario objetivo, podemos decir que no está muy por detrás de la competencia en calidad, e incluso supera a algunos.  MS es una red de competidores multiescala pesada. <br><br><img src="https://habrastorage.org/webt/74/3y/h4/743yh4dgvv_4kjz_e0uecgqxtf0.png" alt="imagen" width="450"><br>  <i>* En el artículo sobre EAST no hubo resultados en la muestra que necesitábamos, por lo que realizamos el experimento nosotros mismos.</i> <br><br>  La imagen a continuación muestra un ejemplo de cómo funciona FaSTExt en imágenes de ICDAR2013.  La primera línea muestra que las letras iluminadas de la palabra ESPMOTO no estaban marcadas, pero la red pudo encontrarlas.  La versión menos espaciosa con α = 0,75 hizo frente a texto pequeño peor que las versiones más "gruesas".  La línea inferior muestra nuevamente defectos de marcado en la muestra con texto perdido en la reflexión.  FaSTExt al mismo tiempo ve dicho texto. <br><br><img src="https://habrastorage.org/webt/cc/dg/7f/ccdg7fs1wpqejmvk-hmorikeuwq.png" alt="imagen"><br><br>  Entonces, la red realiza sus tareas.  Queda por comprobar si realmente se puede usar en teléfonos?  Los modelos se lanzaron en imágenes en color de 512x512 en el Huawei P20 usando la CPU, y en el iPhone SE y iPhone XS usando la GPU, porque nuestro sistema de aprendizaje automático todavía permite usar la GPU solo en iOS.  Valores obtenidos promediando 100 comienzos.  En Android, logramos alcanzar una velocidad de 5 fotogramas por segundo aceptable para nuestra tarea.  El iPhone XS mostró un efecto interesante con una disminución en el tiempo promedio requerido para los cálculos al tiempo que complica la red.  Un iPhone moderno detecta texto con un retraso mínimo, lo que se puede llamar una victoria. <br><br><img src="https://habrastorage.org/webt/mx/zb/kq/mxzbkqrxlnjhqbwfj3hkv_bamby.png" alt="imagen"><br><br><h3>  Referencias </h3><br>  [1] B. Shi, X. Bai y S. Belongie, "Detección de texto orientado en imágenes naturales mediante la vinculación de segmentos", Hawaii, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [2] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov y L.-C.  Chen, "MobileNetV2: Residuos invertidos y cuellos de botella lineales", Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [3] A. Filonenko, K. Gudkov, A. Lebedev, N. Orlov e I. Zagaynov, "FaSTExt: Extractor de texto rápido y pequeño", en el 8º Taller internacional sobre análisis y reconocimiento de documentos basados ​​en cámara, Sydney, 2019 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [4] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu y X. Bai, "Detección de texto de orientación múltiple con redes totalmente convolucionales", Las Vegas, 2016. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [5] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He y J. Liang, "ESTE: un detector de texto de escena eficiente y preciso", en la Conferencia de informática IEEE 2017 Visión y patrón, Honolulu, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [6] M. Liao, Z. Zhu, B. Shi, G.-s.  Xia y X. Bai, "Regresión sensible a la rotación para la detección de texto de escenas orientadas", en la Conferencia IEEE / CVF 2018 sobre visión y patrón por computadora, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [7] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao y J. Yan, "Fots: localización rápida de texto orientado con una red unificada", en la Conferencia IEEE / CVF 2018 sobre visión por computadora y Patrón, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  <i>Grupo de visión por computadora</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472910/">https://habr.com/ru/post/472910/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472894/index.html">DartUP 2019: conferencia Dart and Flutter en San Petersburgo el 23 de noviembre</a></li>
<li><a href="../472896/index.html">Helicóptero desde una impresora: por primera vez, los científicos "imprimieron" una caja de gran tamaño de un motor de helicóptero</a></li>
<li><a href="../472902/index.html">Windows para IoT: soporte de hardware mejorado y nuevas características de dispositivos inteligentes</a></li>
<li><a href="../472904/index.html">Cómo Amazon convirtió las acciones en un juego</a></li>
<li><a href="../472908/index.html">Dagaz: Episodios (Parte 2)</a></li>
<li><a href="../472912/index.html">Base de datos ClickHouse para humanos, o Alien Technology</a></li>
<li><a href="../472914/index.html">Wolfram Function Repository: plataforma de acceso abierto para extensiones de lenguaje Wolfram</a></li>
<li><a href="../472916/index.html">Backend, machine learning y serverless son los más interesantes de la conferencia de julio Habr</a></li>
<li><a href="../472918/index.html">ZX Spectrum en Rusia y la CEI: cómo se transformó la búsqueda en línea fuera de línea</a></li>
<li><a href="../472922/index.html">Programador defensor más fuerte que la entropía</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>