<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÆ üçï üêñ Encuentra texto en carteles y paquetes usando un tel√©fono inteligente üïµüèΩ üë¶üèæ ‚õΩÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El problema de la b√∫squeda autom√°tica de texto en im√°genes existe desde hace mucho tiempo, al menos desde principios de los a√±os noventa del siglo pas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Encuentra texto en carteles y paquetes usando un tel√©fono inteligente</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/472910/">  El problema de la b√∫squeda autom√°tica de texto en im√°genes existe desde hace mucho tiempo, al menos desde principios de los a√±os noventa del siglo pasado.  Los veteranos podr√≠an recordarlos por la distribuci√≥n generalizada de ABBYY FineReader, que puede traducir los escaneos de documentos a sus versiones editables. <br><br>  Los esc√°neres conectados a computadoras personales funcionan muy bien en las empresas, pero el progreso no se detiene y los dispositivos m√≥viles se han apoderado del mundo.  La gama de tareas para trabajar con texto tambi√©n ha cambiado.  Ahora es necesario buscar el texto, no en hojas A4 perfectamente rectas con texto negro sobre fondo blanco, sino en varias tarjetas de presentaci√≥n, men√∫s coloridos, letreros de tiendas y mucho m√°s sobre lo que una persona puede encontrar en la jungla de una ciudad moderna. <br><br> <a href=""><img src="https://habrastorage.org/webt/br/xk/fe/brxkfes7mckw4fwiswow98w4ajy.png"></a> <br>  <i>Un verdadero ejemplo del trabajo de nuestra red neuronal.</i>  <i>Se puede hacer clic en la imagen.</i> <br><br><h2>  Requisitos b√°sicos y limitaciones </h2><br>  Con tal variedad de condiciones para presentar texto, los algoritmos escritos a mano ya no pueden hacer frente.  Aqu√≠, las redes neuronales con su capacidad de generalizar vienen al rescate.  En esta publicaci√≥n, hablaremos sobre nuestro enfoque para crear una arquitectura de red neuronal que detecte texto en im√°genes complejas con buena calidad y alta velocidad. <br><a name="habracut"></a><br>  Los dispositivos m√≥viles imponen restricciones adicionales en la elecci√≥n del enfoque: <br><br><ol><li>  Los usuarios no siempre tienen la oportunidad de usar una red m√≥vil para comunicarse con el servidor debido al costoso tr√°fico m√≥vil o problemas de privacidad.  Entonces, soluciones como Google Lens no ayudar√°n aqu√≠. </li><li>  Dado que nos centramos en el procesamiento local de datos, ser√≠a bueno para nuestra soluci√≥n: <br><ul><li>  Ocupaba poca memoria; </li><li>  Funcion√≥ r√°pidamente utilizando las capacidades t√©cnicas del tel√©fono inteligente. </li></ul></li><li>  El texto se puede girar y estar en un fondo aleatorio. </li><li>  Las palabras pueden ser muy largas.  En las redes neuronales convolucionales, el alcance del n√∫cleo de convoluci√≥n generalmente no cubre la palabra alargada en su conjunto, por lo que tomar√° alg√∫n truco para sortear esta restricci√≥n. <br><br><img src="https://habrastorage.org/webt/s5/qw/6h/s5qw6h1fmawausqg-sm_o-7fwm8.png" alt="imagen"><br></li><li> Los tama√±os de texto en una foto pueden ser diferentes: <br><br><img src="https://habrastorage.org/webt/ia/a-/om/iaa-omk2j8qxwl-sbqmg1_v1rlc.png" alt="imagen"><br></li></ol><br><h2>  Soluci√≥n </h2><br>  ¬°La soluci√≥n m√°s simple para el problema de b√∫squeda de texto que viene a la mente es tomar la mejor red de los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">concursos ICDAR</a> (Conferencia Internacional sobre An√°lisis y Reconocimiento de Documentos) especializados en esta tarea y negocio!  Desafortunadamente, tales redes logran calidad debido a su volumen y complejidad computacional, y solo son adecuadas como una soluci√≥n en la nube, que no cumple con los p√°rrafos 1 y 2 de nuestros requisitos.  Pero, ¬øqu√© sucede si tomamos una red grande que funciona bien en los escenarios que necesitamos cubrir e intentamos reducirla?  Este enfoque ya es m√°s interesante. <br><br>  Baoguang Shi et al. En su red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SegLink</a> [1] propuso lo siguiente: <br><br><ol><li>  Para encontrar no palabras completas a la vez (√°reas verdes en la imagen <b>a</b> ), sino sus partes, llamadas segmentos, con la predicci√≥n de su rotaci√≥n, inclinaci√≥n y desplazamiento.  Tomemos prestada esta idea. </li><li>  Debe buscar segmentos de palabras en varias escalas a la vez para cumplir con el requisito 5. Los segmentos se muestran con rect√°ngulos verdes en la imagen <b>b</b> . </li><li>  Para evitar que una persona invente c√≥mo combinar estos segmentos, simplemente hacemos que la red neuronal prediga conexiones (enlaces) entre segmentos relacionados con la misma palabra <br><br>  a.  dentro de la misma escala (l√≠neas rojas en la imagen <b>c</b> ) <br><br>  b.  y entre escalas (l√≠neas rojas en la imagen <b>d</b> ), resolviendo el problema de la cl√°usula 4 de los requisitos. </li></ol><br>  Los cuadrados azules en la imagen a continuaci√≥n muestran las √°reas de visibilidad de los p√≠xeles de las capas de salida de la red neuronal de diferentes escalas, que "ven" al menos parte de la palabra. <br><br><img src="https://habrastorage.org/webt/qx/eu/zs/qxeuzsrcszz4pxafddjyma3cwqq.png" alt="imagen"><br>  <i>Ejemplos de segmentos y enlaces</i> <br><br>  SegLink utiliza la conocida arquitectura VGG-16 como base.  La predicci√≥n de segmentos y enlaces se realiza en 6 escalas.  Como primer experimento, comenzamos con la implementaci√≥n de la arquitectura original.  Result√≥ que la red contiene 23 millones de par√°metros (pesos) que deben almacenarse en un archivo de 88 megabytes de tama√±o.  Si crea una aplicaci√≥n basada en VGG, ser√° uno de los primeros candidatos para la eliminaci√≥n si no hay suficiente espacio, y la b√∫squeda de texto en s√≠ funcionar√° muy lentamente, por lo que la red debe perder peso con urgencia. <br><br><img src="https://habrastorage.org/webt/oj/lp/xo/ojlpxo224mtzrerf6o4urty-gfk.png" alt="imagen"><br>  <i>Arquitectura de red SegLink</i> <br><br><h2>  El secreto de nuestra dieta. </h2><br>  Puede reducir el tama√±o de la red simplemente cambiando la cantidad de capas y canales en ella, o cambiando la propia circunvoluci√≥n y las conexiones entre ellas.  Mark Sandler y sus asociados justo a tiempo recogieron la arquitectura en su red <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MobileNetV2</a> [2] para que funcione r√°pidamente en dispositivos m√≥viles, ocupe poco espacio y a√∫n no se quede atr√°s del mismo VGG en calidad de trabajo.  El secreto para acelerar y reducir el consumo de memoria est√° en tres pasos principales: <br><br><ol><li>  El n√∫mero de canales con mapas de caracter√≠sticas en la entrada del bloque se reduce por convoluci√≥n de puntos a toda la profundidad (el llamado cuello de botella) sin una funci√≥n de activaci√≥n. </li><li>  La convoluci√≥n cl√°sica se reemplaza por una convoluci√≥n separable por canal.  Tal convoluci√≥n requiere menos peso y menos c√≥mputo. </li><li>  Las cartas de personaje despu√©s del cuello de botella se env√≠an a la entrada del siguiente bloque para sumar sin convoluciones adicionales. </li></ol><br><br><img src="https://habrastorage.org/webt/s8/uq/zb/s8uqzb_r7i6z508gyddsvwztsfc.png" alt="imagen"><br>  <i>Unidad base MobileNetV2</i> <i><br></i> <br><br><h2>  Red neuronal resultante </h2><br>  Usando los enfoques anteriores, llegamos a la siguiente estructura de red: <br><br><ul><li>  Usamos segmentos y enlaces de SegLink </li><li>  Reemplace VGG con un MobileNetV2 menos glot√≥n </li><li>  Reduzca la cantidad de escalas de b√∫squeda de texto de 6 a 5 para mayor velocidad </li></ul><br><br><img src="https://habrastorage.org/webt/el/t6/8a/elt68a2truovh33paqx1omw1mjg.png" alt="imagen"><br>  <i>Red de resumen de b√∫squeda de texto</i> <i><br></i> <br><br><h3>  Descifrado de valores en bloques de arquitectura de red </h3><br>  El paso de zancada y el n√∫mero base de canales en los canales se indican como s &lt;stride&gt; c &lt;channels&gt;, respectivamente.  Por ejemplo, s2c32 significa 32 canales con un desplazamiento de 2. El n√∫mero real de canales en las capas de convoluci√≥n se obtiene multiplicando su n√∫mero base por un factor de escala Œ±, que le permite simular r√°pidamente diferentes "espesores" de la red.  A continuaci√≥n se muestra una tabla con el n√∫mero de par√°metros en la red dependiendo de Œ±. <br><br><img src="https://habrastorage.org/webt/ay/k7/ih/ayk7ihjypgcp6vbw609jf9nmtfi.png" alt="imagen"><br><br>  Tipo de bloque: <br><br><ul><li>  Conv2D: una operaci√≥n de convoluci√≥n completa; </li><li>  D-wise Conv - convoluci√≥n separable por canal; </li><li>  Bloques: un grupo de bloques MobileNetV2; </li><li>  Salida: convoluci√≥n para obtener la capa de salida.  Los valores num√©ricos de tipo NxN indican el tama√±o del campo receptivo del p√≠xel. </li></ul><br>  Como funci√≥n de activaci√≥n, los bloques usan ReLU6. <br><br><img src="https://habrastorage.org/webt/a3/yb/ft/a3ybftdxyu7_vuw04wsx-vclcb4.png" alt="imagen" width="400"><br><br>  La capa de salida tiene 31 canales: <br><br><img src="https://habrastorage.org/webt/se/3i/xc/se3ixchyxmnjwp2mdlp0l0czwr4.png" alt="imagen"><br><br>  Los primeros dos canales de la capa de salida votan para que el p√≠xel pertenezca al texto y no al texto.  Los siguientes cinco canales contienen informaci√≥n para reconstruir con precisi√≥n la geometr√≠a del segmento: desplazamientos verticales y horizontales en relaci√≥n con la posici√≥n del p√≠xel, factores de ancho y alto (ya que el segmento generalmente no es cuadrado) y el √°ngulo de rotaci√≥n.  16 valores de enlaces intracanal indican si hay una conexi√≥n entre ocho p√≠xeles adyacentes en la misma escala.  Los √∫ltimos 8 canales nos informan sobre la presencia de enlaces a cuatro p√≠xeles de la escala anterior (la escala anterior siempre es 2 veces mayor).  Cada 2 valores de segmentos, los enlaces intra y de escala cruzada se normalizan mediante la funci√≥n softmax.  El acceso a la primera escala no tiene enlaces de escala cruzada. <br><br><h2>  Asamblea de palabras </h2><br>  La red predice si un segmento particular y sus vecinos pertenecen al texto.  Queda por recopilarlos en palabras. <br><br>  Para comenzar, combine todos los segmentos que est√°n vinculados por enlaces.  Para hacer esto, componimos un gr√°fico donde los v√©rtices son todos los segmentos en todas las escalas, y los bordes son enlaces.  Luego encontramos los componentes conectados de la gr√°fica.  Para cada componente, ahora es posible calcular la palabra que encierra el rect√°ngulo de la siguiente manera: <br><br><ol><li>  Calculamos el √°ngulo de rotaci√≥n de la palabra Œ∏ <br><ul><li>  O como el valor promedio de las predicciones del √°ngulo de rotaci√≥n de los segmentos, si hay muchos de ellos, </li><li>  O como el √°ngulo de rotaci√≥n de la l√≠nea obtenida por regresi√≥n en los puntos de los centros de los segmentos, si hay pocos segmentos. </li></ul></li><li>  El centro de la palabra se selecciona como el centro de masa de los puntos centrales de los segmentos. </li><li>  Expanda todos los segmentos por -Œ∏ para organizarlos horizontalmente.  Encuentra los l√≠mites de la palabra. <br><ul><li>  Los l√≠mites izquierdo y derecho de la palabra se seleccionan como los l√≠mites de los segmentos m√°s a la izquierda y a la derecha, respectivamente. </li><li>  Para obtener el l√≠mite superior de la palabra, los segmentos se ordenan por la altura del borde superior, se corta el 20% de los m√°s altos y el valor del primer segmento se selecciona de la lista que queda despu√©s del filtrado. </li><li>  El l√≠mite inferior se obtiene de los segmentos m√°s bajos con un l√≠mite del 20% del m√°s bajo, por analog√≠a con el l√≠mite superior. </li></ul></li><li>  Gira el rect√°ngulo resultante de nuevo a Œ∏. </li></ol><br>  La soluci√≥n final se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><b>FaSTExt</b> : Extractor de texto r√°pido y peque√±o</a> [3] <br><br><h2>  ¬°Experimente el tiempo! </h2><br><h3>  Detalles de entrenamiento </h3><br>  La red misma y sus par√°metros se seleccionaron para un buen trabajo en una muestra interna grande, que refleja el escenario principal del uso de la aplicaci√≥n en el tel√©fono: apunt√≥ la c√°mara a un objeto con texto y tom√≥ una foto.  Result√≥ que una red grande con Œ± = 1 omite en calidad la versi√≥n con Œ± = 0.5 en solo un 2%.  Esta muestra no est√° disponible p√∫blicamente, por lo que para mayor claridad, tuve que entrenar a la red en la muestra p√∫blica <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICDAR2013</a> , en la que las condiciones de disparo son similares a las nuestras.  La muestra es muy peque√±a, por lo que la red se entren√≥ previamente en una gran cantidad de datos sint√©ticos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SynthText en el Wild Dataset</a> .  El proceso de pre-entrenamiento tom√≥ aproximadamente 20 d√≠as de c√°lculos para cada experimento en el GTX 1080 Ti, por lo que la operaci√≥n de la red en datos p√∫blicos se verific√≥ solo para las opciones Œ± = 0.75, 1 y 2. <br><br>  Como optimizador, se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">utiliz√≥ la</a> versi√≥n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AMSGrad</a> de Adam. <br><br>  Funciones de error: <br><br><ul><li>  Entrop√≠a cruzada para la clasificaci√≥n de segmentos y enlaces; </li><li>  Funci√≥n de p√©rdida de Huber para geometr√≠a de segmento. </li></ul><br><h2>  Resultados </h2><br>  En t√©rminos de la calidad del rendimiento de la red en el escenario objetivo, podemos decir que no est√° muy por detr√°s de la competencia en calidad, e incluso supera a algunos.  MS es una red de competidores multiescala pesada. <br><br><img src="https://habrastorage.org/webt/74/3y/h4/743yh4dgvv_4kjz_e0uecgqxtf0.png" alt="imagen" width="450"><br>  <i>* En el art√≠culo sobre EAST no hubo resultados en la muestra que necesit√°bamos, por lo que realizamos el experimento nosotros mismos.</i> <br><br>  La imagen a continuaci√≥n muestra un ejemplo de c√≥mo funciona FaSTExt en im√°genes de ICDAR2013.  La primera l√≠nea muestra que las letras iluminadas de la palabra ESPMOTO no estaban marcadas, pero la red pudo encontrarlas.  La versi√≥n menos espaciosa con Œ± = 0,75 hizo frente a texto peque√±o peor que las versiones m√°s "gruesas".  La l√≠nea inferior muestra nuevamente defectos de marcado en la muestra con texto perdido en la reflexi√≥n.  FaSTExt al mismo tiempo ve dicho texto. <br><br><img src="https://habrastorage.org/webt/cc/dg/7f/ccdg7fs1wpqejmvk-hmorikeuwq.png" alt="imagen"><br><br>  Entonces, la red realiza sus tareas.  Queda por comprobar si realmente se puede usar en tel√©fonos?  Los modelos se lanzaron en im√°genes en color de 512x512 en el Huawei P20 usando la CPU, y en el iPhone SE y iPhone XS usando la GPU, porque nuestro sistema de aprendizaje autom√°tico todav√≠a permite usar la GPU solo en iOS.  Valores obtenidos promediando 100 comienzos.  En Android, logramos alcanzar una velocidad de 5 fotogramas por segundo aceptable para nuestra tarea.  El iPhone XS mostr√≥ un efecto interesante con una disminuci√≥n en el tiempo promedio requerido para los c√°lculos al tiempo que complica la red.  Un iPhone moderno detecta texto con un retraso m√≠nimo, lo que se puede llamar una victoria. <br><br><img src="https://habrastorage.org/webt/mx/zb/kq/mxzbkqrxlnjhqbwfj3hkv_bamby.png" alt="imagen"><br><br><h3>  Referencias </h3><br>  [1] B. Shi, X. Bai y S. Belongie, "Detecci√≥n de texto orientado en im√°genes naturales mediante la vinculaci√≥n de segmentos", Hawaii, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [2] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov y L.-C.  Chen, "MobileNetV2: Residuos invertidos y cuellos de botella lineales", Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [3] A. Filonenko, K. Gudkov, A. Lebedev, N. Orlov e I. Zagaynov, "FaSTExt: Extractor de texto r√°pido y peque√±o", en el 8¬∫ Taller internacional sobre an√°lisis y reconocimiento de documentos basados ‚Äã‚Äãen c√°mara, Sydney, 2019 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [4] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu y X. Bai, "Detecci√≥n de texto de orientaci√≥n m√∫ltiple con redes totalmente convolucionales", Las Vegas, 2016. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [5] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He y J. Liang, "ESTE: un detector de texto de escena eficiente y preciso", en la Conferencia de inform√°tica IEEE 2017 Visi√≥n y patr√≥n, Honolulu, 2017. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [6] M. Liao, Z. Zhu, B. Shi, G.-s.  Xia y X. Bai, "Regresi√≥n sensible a la rotaci√≥n para la detecci√≥n de texto de escenas orientadas", en la Conferencia IEEE / CVF 2018 sobre visi√≥n y patr√≥n por computadora, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  [7] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao y J. Yan, "Fots: localizaci√≥n r√°pida de texto orientado con una red unificada", en la Conferencia IEEE / CVF 2018 sobre visi√≥n por computadora y Patr√≥n, Salt Lake City, 2018. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">enlace</a> <br><br>  <i>Grupo de visi√≥n por computadora</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472910/">https://habr.com/ru/post/472910/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472894/index.html">DartUP 2019: conferencia Dart and Flutter en San Petersburgo el 23 de noviembre</a></li>
<li><a href="../472896/index.html">Helic√≥ptero desde una impresora: por primera vez, los cient√≠ficos "imprimieron" una caja de gran tama√±o de un motor de helic√≥ptero</a></li>
<li><a href="../472902/index.html">Windows para IoT: soporte de hardware mejorado y nuevas caracter√≠sticas de dispositivos inteligentes</a></li>
<li><a href="../472904/index.html">C√≥mo Amazon convirti√≥ las acciones en un juego</a></li>
<li><a href="../472908/index.html">Dagaz: Episodios (Parte 2)</a></li>
<li><a href="../472912/index.html">Base de datos ClickHouse para humanos, o Alien Technology</a></li>
<li><a href="../472914/index.html">Wolfram Function Repository: plataforma de acceso abierto para extensiones de lenguaje Wolfram</a></li>
<li><a href="../472916/index.html">Backend, machine learning y serverless son los m√°s interesantes de la conferencia de julio Habr</a></li>
<li><a href="../472918/index.html">ZX Spectrum en Rusia y la CEI: c√≥mo se transform√≥ la b√∫squeda en l√≠nea fuera de l√≠nea</a></li>
<li><a href="../472922/index.html">Programador defensor m√°s fuerte que la entrop√≠a</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>