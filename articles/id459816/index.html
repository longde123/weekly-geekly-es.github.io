<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎅🏾 🈚️ 🌾 Jaringan saraf dan pembelajaran mendalam, bab 3, bagian 2: mengapa regularisasi membantu mengurangi pelatihan ulang? 🙌🏽 ✴️ 🏑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Isi 

- Bab 1: menggunakan jaringan saraf untuk mengenali nomor tulisan tangan 
- Bab 2: cara kerja algoritma backpropagation 
- Bab 3: Bagian 1: meni...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Jaringan saraf dan pembelajaran mendalam, bab 3, bagian 2: mengapa regularisasi membantu mengurangi pelatihan ulang?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/459816/"><div class="spoiler">  <b class="spoiler_title">Isi</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 1: menggunakan jaringan saraf untuk mengenali nomor tulisan tangan</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 2: cara kerja algoritma backpropagation</a> </li><li>  Bab 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1: meningkatkan metode pelatihan jaringan saraf</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 2: Mengapa regularisasi membantu mengurangi pelatihan ulang?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 3: bagaimana memilih hyperparameters jaringan saraf?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 4: bukti visual bahwa jaringan saraf mampu menghitung fungsi apa pun</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 5: mengapa jaringan saraf yang dalam begitu sulit untuk dilatih?</a> </li><li>  Bab 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1: Pembelajaran Mendalam</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 2: kemajuan terbaru dalam pengenalan gambar</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kata penutup: apakah ada algoritma sederhana untuk menciptakan kecerdasan?</a> </li></ul></div></div><br>  Secara empiris, kita telah melihat bahwa regularisasi membantu mengurangi pelatihan ulang.  Ini menginspirasi - tetapi, sayangnya, tidak jelas mengapa regularisasi membantu.  Biasanya orang menjelaskannya dengan cara tertentu: dalam arti, bobot yang lebih kecil memiliki kompleksitas yang lebih sedikit, yang memberikan penjelasan data yang lebih sederhana dan lebih efisien, sehingga mereka harus lebih disukai.  Namun, ini penjelasan yang terlalu singkat, dan beberapa bagiannya mungkin tampak meragukan atau misterius.  Mari kita buka kisah ini dan memeriksanya dengan mata kritis.  Untuk melakukan ini, misalkan kita memiliki kumpulan data sederhana yang ingin kita buat model: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0f/h2/4p/0fh24p1sl8wmgoov1ewqmqbv900.png"></div><a name="habracut"></a><br>  Dalam hal makna, di sini kita mempelajari fenomena dunia nyata, dan x dan y menunjukkan data nyata.  Tujuan kami adalah untuk membangun model yang memungkinkan kami untuk memprediksi y sebagai fungsi x.  Kita dapat mencoba menggunakan jaringan saraf untuk membuat model seperti itu, tetapi saya menyarankan sesuatu yang lebih sederhana: Saya akan mencoba memodelkan y sebagai polinomial dalam x.  Saya akan melakukan ini alih-alih jaringan saraf, karena penggunaan polinomial membuat penjelasan lebih jelas.  Segera setelah kami menangani kasus polinomial, kami akan beralih ke Majelis Nasional.  Ada sepuluh poin pada grafik di atas, yang berarti bahwa kita dapat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menemukan polinomial</a> orde ke-9 y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... + a <sub>9</sub> yang persis sesuai dengan data.  Dan ini adalah grafik polinomial ini. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o6/lj/6j/o6lj6j82dn6rly8xnv-fmzvvwn0.png"></div><br>  Pukulan sempurna.  Tapi kita bisa mendapatkan perkiraan yang baik menggunakan model linier y = 2x <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-3/qk/sq/-3qksq4rmlcq54obwxcxy4_dvtm.png"></div><br>  Mana yang lebih baik?  Mana yang lebih mungkin benar?  Mana yang akan digeneralisasikan dengan lebih baik pada contoh-contoh lain dari fenomena yang sama dari dunia nyata? <br><br>  Pertanyaan sulit.  Dan mereka tidak dapat dijawab dengan tepat tanpa informasi tambahan tentang fenomena dunia nyata yang mendasarinya.  Namun, mari kita lihat dua kemungkinan: (1) model dengan polinomial orde ke-9 benar-benar menggambarkan fenomena dunia nyata, dan karenanya, menggeneralisasi dengan sempurna;  (2) model yang benar adalah y = 2x, tetapi kami memiliki noise tambahan yang terkait dengan kesalahan pengukuran, sehingga model tersebut tidak cocok dengan sempurna. <br><br>  A priori, seseorang tidak bisa mengatakan mana dari dua kemungkinan yang benar (atau tidak ada yang ketiga).  Logikanya, salah satu dari mereka mungkin ternyata benar.  Dan perbedaan di antara mereka adalah nontrivial.  Ya, berdasarkan data yang tersedia, dapat dikatakan bahwa hanya ada sedikit perbedaan antara model.  Tapi anggaplah kita ingin memprediksi nilai y yang sesuai dengan beberapa nilai x yang besar, jauh lebih besar daripada yang ditunjukkan dalam grafik.  Jika kita mencoba melakukan ini, maka perbedaan besar akan muncul antara prediksi kedua model, karena istilah x <sup>9</sup> mendominasi dalam polinomial orde ke-9, dan model linier tetap linier. <br><br>  Satu sudut pandang tentang apa yang terjadi adalah untuk menyatakan bahwa penjelasan yang lebih sederhana harus digunakan dalam sains, jika memungkinkan.  Ketika kami menemukan model sederhana yang menjelaskan banyak poin referensi, kami hanya ingin berteriak: "Eureka!"  Lagi pula, kecil kemungkinan bahwa penjelasan sederhana akan muncul murni secara kebetulan.  Kami menduga bahwa model tersebut harus menghasilkan beberapa kebenaran yang terkait dengan fenomena tersebut.  Dalam hal ini, model y = 2x + noise tampaknya jauh lebih sederhana daripada y = a <sub>0</sub> x <sup>9</sup> + a <sub>1</sub> x <sup>8</sup> + ... Akan mengejutkan jika kesederhanaan muncul secara kebetulan, jadi kami menduga bahwa y = 2x + noise mengekspresikan beberapa kebenaran yang mendasarinya.  Dari sudut pandang ini, model urutan ke-9 hanya mempelajari efek kebisingan lokal.  Meskipun model urutan ke-9 bekerja dengan sempurna untuk titik-titik referensi khusus ini, ia tidak dapat menggeneralisasi ke titik-titik lain, sehingga model linier dengan noise akan memiliki kemampuan prediksi yang lebih baik. <br><br>  Mari kita lihat apa arti sudut pandang ini untuk jaringan saraf.  Misalkan, di jaringan kami, terutama ada bobot yang rendah, seperti yang biasanya terjadi pada jaringan yang diatur.  Karena bobotnya yang kecil, perilaku jaringan tidak banyak berubah ketika beberapa input acak diubah di sana-sini.  Akibatnya, jaringan yang teregulasi sulit untuk mempelajari efek noise lokal yang ada dalam data.  Ini mirip dengan keinginan untuk memastikan bahwa bukti individual tidak sangat mempengaruhi hasil jaringan secara keseluruhan.  Alih-alih, jaringan yang diatur dilatih untuk menanggapi bukti yang sering ditemukan dalam data pelatihan.  Sebaliknya, jaringan dengan bobot besar dapat mengubah perilakunya dengan cukup kuat sebagai respons terhadap perubahan kecil pada data input.  Oleh karena itu, jaringan tidak teratur dapat menggunakan bobot besar untuk melatih model yang kompleks yang berisi banyak informasi gangguan dalam data pelatihan.  Singkatnya, keterbatasan jaringan yang diatur memungkinkan mereka untuk membuat model yang relatif sederhana berdasarkan pola yang sering ditemukan dalam data pelatihan, dan mereka tahan terhadap penyimpangan yang disebabkan oleh kebisingan dalam data pelatihan.  Ada harapan bahwa ini akan membuat jaringan kita mempelajari fenomena itu sendiri, dan lebih menggeneralisasi pengetahuan yang didapat. <br><br>  Dengan semua yang dikatakan, gagasan memberikan preferensi pada penjelasan yang lebih sederhana harus membuat Anda gugup.  Kadang-kadang orang menyebut ide ini "Occam's razor" dan dengan bersemangat menerapkannya, seolah-olah memiliki status prinsip ilmiah umum.  Tapi ini, tentu saja, bukan prinsip ilmiah umum.  Tidak ada alasan logis apriori untuk lebih memilih penjelasan sederhana daripada yang kompleks.  Terkadang penjelasan yang lebih rumit benar. <br><br>  Izinkan saya menjelaskan dua contoh bagaimana penjelasan yang lebih rumit ternyata benar.  Pada 1940-an, fisikawan Marcel Shane mengumumkan penemuan partikel baru.  Perusahaan tempat dia bekerja, General Electric, senang, dan secara luas mendistribusikan publikasi acara ini.  Namun, fisikawan Hans Bethe skeptis.  Bethe mengunjungi Shane dan mempelajari lemping-lemping itu dengan jejak partikel baru Shane.  Shane menunjukkan piring Beta setelah piring, tetapi Bete menemukan pada masing-masing dari mereka masalah yang menunjukkan perlunya menolak data ini.  Akhirnya, Shane menunjukkan Beta sebuah catatan yang tampak pas.  Bethe mengatakan itu mungkin hanya penyimpangan statistik.  Shane: "Ya, tetapi kemungkinan itu karena statistik, bahkan dengan formula Anda sendiri, adalah satu dari lima."  Bethe: "Namun, saya sudah melihat lima catatan."  Akhirnya, Shane berkata, "Tetapi Anda menjelaskan masing-masing catatan saya, setiap citra yang baik dengan beberapa teori lain, dan saya memiliki satu hipotesis yang menjelaskan semua catatan sekaligus, dari mana selanjutnya kita berbicara tentang partikel baru."  Bethe menjawab: “Satu-satunya perbedaan antara penjelasan saya dan Anda adalah bahwa Anda salah dan milik saya benar.  Penjelasan tunggal Anda salah, dan semua penjelasan saya benar. "  Selanjutnya, ternyata alam setuju dengan Bethe, dan partikel Shane menguap. <br><br>  Dalam contoh kedua, pada 1859, astronom Urbain Jean Joseph Le Verrier menemukan bahwa bentuk orbit Merkurius tidak sesuai dengan teori gravitasi universal Newton.  Ada sedikit penyimpangan dari teori ini, dan kemudian beberapa opsi untuk memecahkan masalah diusulkan, yang bermuara pada fakta bahwa teori Newton secara keseluruhan adalah benar, dan hanya memerlukan sedikit perubahan.  Dan pada tahun 1916, Einstein menunjukkan bahwa penyimpangan ini dapat dijelaskan dengan baik menggunakan teori relativitas umumnya, secara radikal berbeda dari gravitasi Newton dan didasarkan pada matematika yang jauh lebih kompleks.  Terlepas dari kompleksitas tambahan ini, secara umum diterima hari ini bahwa penjelasan Einstein benar, dan gravitasi Newton tidak benar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bahkan dalam bentuk yang dimodifikasi</a> .  Ini terjadi, khususnya, karena hari ini kita tahu bahwa teori Einstein menjelaskan banyak fenomena lain yang dengannya teori Newton mengalami kesulitan.  Selain itu, bahkan lebih menakjubkan, teori Einstein secara akurat memprediksi beberapa fenomena yang tidak diprediksi oleh gravitasi Newton.  Namun, kualitas yang mengesankan ini tidak terlihat jelas di masa lalu.  Menilai berdasarkan kesederhanaan belaka, maka beberapa bentuk modifikasi dari teori Newton akan terlihat lebih menarik. <br><br>  Tiga moralitas dapat ditarik dari kisah-kisah ini.  Pertama, kadang-kadang cukup sulit untuk memutuskan mana dari dua penjelasan yang akan "lebih mudah."  Kedua, bahkan jika kita membuat keputusan seperti itu, kesederhanaan harus dibimbing dengan sangat hati-hati!  Ketiga, tes sebenarnya dari model bukanlah kesederhanaan, tetapi seberapa baik ia memprediksi fenomena baru dalam kondisi perilaku yang baru. <br><br>  Mempertimbangkan semua ini dan berhati-hati, kami akan menerima fakta empiris - NS yang diatur biasanya lebih digeneralisasikan daripada yang tidak teratur.  Karena itu, nanti dalam buku kita akan sering menggunakan regularisasi.  Cerita-cerita yang disebutkan di atas hanya diperlukan untuk menjelaskan mengapa belum ada yang mengembangkan penjelasan teoretis yang sepenuhnya meyakinkan tentang mengapa regularisasi membantu jaringan secara umum.  Para peneliti terus menerbitkan karya-karya di mana mereka mencoba untuk mencoba pendekatan berbeda untuk regularisasi, membandingkannya, melihat mana yang terbaik, dan mencoba memahami mengapa pendekatan yang berbeda bekerja lebih buruk atau lebih baik.  Jadi regularisasi dapat diperlakukan seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">awan</a> .  Saat ini cukup membantu, kami tidak memiliki pemahaman sistemik yang sepenuhnya memuaskan tentang apa yang terjadi - hanya aturan heuristik dan praktis yang tidak lengkap. <br><br>  Di sinilah letak masalah yang lebih dalam yang masuk ke jantung ilmu pengetahuan.  Ini adalah masalah generalisasi.  Regularisasi dapat memberi kita tongkat ajaib komputasi yang membantu jaringan kita menggeneralisasikan data dengan lebih baik, tetapi itu tidak memberikan pemahaman dasar tentang bagaimana generalisasi bekerja, dan apa pendekatan terbaik untuk itu. <br><br>  Masalah-masalah ini kembali ke <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">masalah induksi</a> , interpretasi terkenal yang dilakukan oleh filsuf Skotlandia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">David Hume</a> dalam buku " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">A Study on Human Cognition</a> " (1748).  Masalah induksi adalah subjek dari " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">teorema tentang tidak adanya makanan gratis</a> " oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">David Walpert dan William Macredie</a> (1977). <br><br>  Dan ini sangat menjengkelkan, karena dalam kehidupan sehari-hari orang secara fenomenal mampu menggeneralisasikan data.  Perlihatkan beberapa gambar gajah kepada anak, dan ia akan segera belajar mengenali gajah lain.  Tentu saja, ia kadang-kadang dapat membuat kesalahan, misalnya, membingungkan badak dengan gajah, tetapi secara umum, proses ini bekerja dengan sangat akurat.  Sekarang, kami memiliki sistem - otak manusia - dengan sejumlah besar parameter gratis.  Dan setelah dia diperlihatkan satu atau lebih gambar pelatihan, sistem belajar untuk menggeneralisasikannya ke gambar lain.  Otak kita, dalam arti tertentu, sangat bagus dalam mengatur!  Tetapi bagaimana kita melakukan ini?  Saat ini, ini tidak kami ketahui.  Saya pikir bahwa di masa depan kita akan mengembangkan teknologi regularisasi yang lebih kuat dalam jaringan saraf tiruan, teknik yang pada akhirnya memungkinkan Majelis Nasional untuk menggeneralisasikan data berdasarkan pada set data yang lebih kecil. <br><br>  Bahkan, jaringan kami sudah menggeneralisasi jauh lebih baik daripada yang dapat diperkirakan secara apriori.  Jaringan dengan 100 neuron tersembunyi memiliki hampir 80.000 parameter.  Kami hanya memiliki 50.000 gambar dalam data pelatihan.  Ini sama dengan mencoba meregangkan polinomial dengan urutan 80.000 lebih dari 50.000 poin referensi.  Dengan semua indikasi, jaringan kami harus sangat berlatih.  Namun, seperti yang telah kita lihat, jaringan seperti itu sebenarnya sebenarnya cukup baik untuk digeneralisasi.  Mengapa ini terjadi?  Ini tidak sepenuhnya jelas.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dihipotesiskan</a> bahwa "dinamika pembelajaran dengan penurunan gradien dalam jaringan multilayer tunduk pada pengaturan diri."  Ini adalah kekayaan yang ekstrem, tetapi juga fakta yang agak mengganggu, karena kita tidak mengerti mengapa ini terjadi.  Sementara itu, kami akan mengambil pendekatan pragmatis, dan kami akan menggunakan regularisasi jika memungkinkan.  Ini akan bermanfaat bagi Majelis Nasional kita. <br><br>  Biarkan saya menyelesaikan bagian ini dengan kembali ke apa yang saya tidak jelaskan sebelumnya: bahwa regularisasi L2 tidak membatasi perpindahan.  Tentu, akan mudah untuk mengubah prosedur regularisasi sehingga mengatur perpindahan.  Tetapi secara empiris, ini seringkali tidak mengubah hasil dengan cara yang terlihat, oleh karena itu, sampai batas tertentu, untuk berurusan dengan regularisasi bias, atau tidak, adalah masalah kesepakatan.  Namun, perlu dicatat bahwa perpindahan besar tidak membuat neuron sensitif terhadap input seperti bobot besar.  Oleh karena itu, kita tidak perlu khawatir tentang offset besar yang memungkinkan jaringan kita untuk belajar suara dalam data pelatihan.  Pada saat yang sama, dengan memungkinkan perpindahan besar, kami membuat jaringan kami lebih fleksibel dalam perilaku mereka - khususnya, perpindahan besar memfasilitasi saturasi neuron, yang kami inginkan.  Untuk alasan ini, kami biasanya tidak memasukkan offset dalam regularisasi. <br><br><h2>  Teknik regularisasi lainnya </h2><br>  Ada banyak teknik regularisasi selain L2.  Sebenarnya, sudah banyak teknik yang dikembangkan sehingga, dengan semua keinginan, saya tidak dapat menjelaskan semuanya secara singkat.  Pada bagian ini, saya akan menjelaskan secara singkat tiga pendekatan lain untuk mengurangi pelatihan ulang: mengatur L1, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">keluar</a> , dan secara artifisial meningkatkan set pelatihan.  Kami tidak akan mempelajarinya sedalam topik sebelumnya.  Sebagai gantinya, kita hanya mengenal mereka, dan pada saat yang sama menghargai beragam teknik regularisasi yang ada. <br><br><h3>  Regularisasi L1 </h3><br>  Dalam pendekatan ini, kami memodifikasi fungsi biaya tidak teratur dengan menambahkan jumlah nilai absolut dari bobot: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>95</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="40.447ex" height="2.66ex" viewBox="0 -832 17414.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-3D" x="1038" y="0"></use><g transform="translate(2094,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-30" x="1011" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2B" x="3486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="4736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="5287" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="5738" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="6268" y="0"></use><g transform="translate(6701,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="10140" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-73" x="10991" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-75" x="11460" y="0"></use><g transform="translate(12033,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-7C" x="13518" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="13797" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-7C" x="14513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="15042" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="15403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="15933" y="0"></use><g transform="translate(16413,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-35" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>C</mi><mo>=</mo><msub><mi>C</mi><mn>0</mn></msub><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msub><mi>m</mi><mi>w</mi></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>w</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>95</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> C = C_0 + \ frac {\ lambda} {n} \ sum_w | w | \ tag {95} </script></p><br><br>  Secara intuitif, ini mirip dengan regularisasi L2, yang dikenakan untuk bobot besar dan membuat jaringan lebih memilih bobot rendah.  Tentu saja, istilah regularisasi L1 tidak seperti istilah regularisasi L2, jadi Anda seharusnya tidak mengharapkan perilaku yang persis sama.  Mari kita mencoba memahami bagaimana perilaku jaringan yang dilatih dengan regularisasi L1 berbeda dari jaringan yang dilatih dengan regularisasi L2. <br><br>  Untuk melakukan ini, lihat turunan parsial dari fungsi biaya.  Membedakan (95), kami memperoleh: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mspace width=&quot;thinmathspace&quot; /><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>96</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="86.162ex" height="2.66ex" viewBox="0 -832 37097.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="3269" y="0"></use></g><g transform="translate(6245,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-3D" x="10508" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="11815" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="12365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="12817" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="13346" y="0"></use><g transform="translate(13780,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(18218,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2B" x="22427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="23677" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="24228" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="24679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="25209" y="0"></use><g transform="translate(25642,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-62" x="1956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-64" x="2386" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2909" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="29081" y="0"></use><g transform="translate(29849,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="701" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-73" x="1580" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="2049" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="2530" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-28" x="32979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="33369" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-29" x="34085" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="34725" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="35086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="35616" y="0"></use><g transform="translate(36096,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-36" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>C</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mspace width="thinmathspace"></mspace><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>r</mi><mi>m</mi><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>96</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> \ frac {\ partial C} {\ partial w} = \ frac {\ partial C_0} {\ partial w} + \ frac {\ lambda} {n} \, {\ rm sgn} (w) \ tag {96 } </script></p><br><br>  di mana sgn (w) adalah tanda w, yaitu, +1 jika w positif, dan -1 jika w negatif.  Menggunakan ungkapan ini, kami sedikit memodifikasi propagasi belakang sehingga melakukan penurunan gradien stokastik menggunakan regularisasi L1.  Aturan pembaruan terakhir untuk jaringan yang diatur L1: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>97</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="92.249ex" height="2.78ex" viewBox="0 -883.9 39718.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2212" x="9100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="10901" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="11353" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="11882" y="0"></use><g transform="translate(12316,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="17362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="18213" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-62" x="19091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6F" x="19521" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-78" x="20006" y="0"></use><g transform="translate(20579,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-73" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="469" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="950" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-28" x="22129" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="22519" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-29" x="23235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2212" x="23847" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-65" x="25098" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="25564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="25926" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="26705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="27256" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="27707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="28237" y="0"></use><g transform="translate(28670,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33109,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="37345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="37707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="38236" y="0"></use><g transform="translate(38717,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-37" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>′</mo></msup><mo>=</mo><mi>w</mi><mo>−</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>m</mi><mi>b</mi><mi>o</mi><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>s</mi><mi>g</mi><mi>n</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>−</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>97</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \ rightarrow w '= w- \ frac {\ eta \ lambda} {n} \ mbox {sgn} (w) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {97} </script></p><br><br>  di mana, seperti biasa, ∂C / ∂w dapat diperkirakan secara opsional menggunakan nilai rata-rata paket-mini.  Bandingkan ini dengan aturan pembaruan regularisasi L2 (93): <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>w</mi><mtext>&amp;#xA0;</mtext><mi>k</mi><mi>i</mi><mi>r</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mn>1</mn><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>n</mi></mrow><mtext>&amp;#xA0;</mtext><mi>k</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>98</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="93.557ex" height="2.78ex" viewBox="0 -883.9 40281.2 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="966" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="1418" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="1763" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-68" x="2244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="2820" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="3182" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="3711" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="4163" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6F" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="5100" y="0"></use><g transform="translate(5816,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-3D" x="7105" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="8161" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6B" x="9128" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="9649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="9995" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="10446" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-28" x="10792" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-31" x="11181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2212" x="11904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="13155" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="13705" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="14157" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="14686" y="0"></use><g transform="translate(15120,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="1857" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6D" x="2685" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-62" x="3564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-64" x="3993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="4517" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="20166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6B" x="21017" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="21538" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="22068" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="22668" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6E" x="23198" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-29" x="23798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2212" x="24410" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-65" x="25661" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="26127" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="26489" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-66" x="27268" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="27819" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="28270" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-63" x="28800" y="0"></use><g transform="translate(29233,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><g transform="translate(3269,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-30" x="1011" y="-213"></use></g></g><g transform="translate(33672,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-72" x="1283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1734" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-69" x="2096" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="2441" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="2971" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-77" x="3269" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="37908" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="38270" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="38799" y="0"></use><g transform="translate(39280,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>w</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><msup><mi>w</mi><mo>′</mo></msup><mo>=</mo><mi>w</mi><mtext>&nbsp;</mtext><mi>k</mi><mi>i</mi><mi>r</mi><mi>i</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>l</mi><mi>a</mi><mi>m</mi><mi>b</mi><mi>d</mi><mi>a</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>n</mi></mrow><mtext>&nbsp;</mtext><mi>k</mi><mi>a</mi><mi>n</mi><mi>a</mi><mi>n</mi><mo stretchy="false">)</mo><mo>−</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>t</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><msub><mi>C</mi><mn>0</mn></msub></mrow><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>a</mi><mi>l</mi><mi>w</mi></mrow><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>98</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> w \ rightarrow w '= w \ kiri (1 - \ frac {\ eta \ lambda} {n} \ kanan) - \ eta \ frac {\ partial C_0} {\ partial w} \ tag {98} </script></p><br><br>  Dalam kedua ungkapan, efek regularisasi adalah mengurangi bobot.  Ini bertepatan dengan gagasan intuitif bahwa kedua jenis regularisasi menghukum bobot yang besar.  Namun, bobot dikurangi dengan cara yang berbeda.  Dalam regularisasi L1, bobot berkurang dengan nilai konstan, cenderung ke 0. Dalam regularisasi L2, bobot berkurang dengan nilai yang sebanding dengan w.  Oleh karena itu, ketika beberapa berat memiliki nilai besar | w |, regularisasi L1 mengurangi bobot tidak sebanyak L2.  Begitu juga sebaliknya, saat | w |  kecil, regularisasi L1 mengurangi berat badan jauh lebih banyak daripada regularisasi L2.  Akibatnya, regularisasi L1 cenderung memusatkan bobot jaringan dalam jumlah yang relatif kecil dari ikatan penting, sedangkan bobot lainnya cenderung nol. <br><br>  Saya sedikit merapikan satu masalah dalam diskusi sebelumnya - turunan parsial ∂C / ∂w tidak didefinisikan ketika w = 0.  Ini karena fungsi | w |  ada "ketegaran" akut pada titik w = 0, oleh karena itu, tidak dapat dibedakan di sana.  Tapi ini tidak menakutkan.  Kami hanya menerapkan aturan biasa, tidak teratur untuk penurunan gradien stokastik ketika w = 0.  Secara intuitif, tidak ada yang salah dengan itu - regularisasi harus mengurangi bobot, dan jelas itu tidak dapat mengurangi bobot yang sudah sama dengan 0. Lebih tepatnya, kita akan menggunakan persamaan (96) dan (97) dengan kondisi sgn (0) = 0.  Ini akan memberi kita aturan yang nyaman dan kompak untuk penurunan gradien stokastik dengan regularisasi L1. <br><br><h3>  Pengecualian [putus sekolah] </h3><br>  Pengecualian adalah teknik regularisasi yang sama sekali berbeda.  Berbeda dengan regularisasi L1 dan L2, pengecualian tidak berurusan dengan perubahan fungsi biaya.  Sebaliknya, kami mengubah jaringan itu sendiri.  Izinkan saya menjelaskan mekanika dasar pengoperasian pengecualian sebelum mempelajari topik mengapa ia bekerja dan dengan hasil apa. <br><br>  Misalkan kita mencoba untuk melatih jaringan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ae5/671/838/ae5671838ebc48f82eb01c1a839b60a7.png"></div><br>  Secara khusus, katakanlah kita memiliki input pelatihan x dan output yang diinginkan y.  Biasanya, kita akan melatihnya dengan secara langsung mendistribusikan x melalui jaringan, dan kemudian kembali merambat untuk menentukan kontribusi gradien.  Pengecualian mengubah proses ini.  Kami mulai dengan secara acak dan sementara menghapus setengah dari neuron tersembunyi di jaringan, meninggalkan neuron input dan output tidak berubah.  Setelah itu, kita akan memiliki kira-kira jaringan seperti itu.  Perhatikan bahwa neuron yang dikecualikan, yang dikeluarkan sementara, masih ditandai dalam diagram: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec3/51c/b6a/ec351cb6a877c8f0688718e0d5980088.png"></div><br>  Kami melewati x dengan distribusi langsung melalui jaringan yang diubah, dan kemudian kembali mendistribusikan hasilnya, juga melalui jaringan yang diubah.  Setelah kami melakukan ini dengan paket-mini contoh, kami memperbarui bobot dan offset yang sesuai.  Kemudian kami ulangi proses ini, pertama mengembalikan neuron yang dikecualikan, kemudian memilih subset acak baru dari neuron tersembunyi untuk dihapus, mengevaluasi gradien untuk paket-mini lainnya, dan memperbarui bobot dan offset jaringan. <br><br>  Mengulangi proses ini berulang-ulang, kami mendapatkan jaringan yang telah mempelajari beberapa bobot dan perpindahan.  Secara alami, bobot dan perpindahan ini dipelajari dalam kondisi di mana setengah dari neuron yang tersembunyi dikeluarkan.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dan ketika kita meluncurkan jaringan secara penuh, kita akan memiliki dua kali lipat neuron tersembunyi aktif. </font><font style="vertical-align: inherit;">Untuk mengimbangi ini, kami membagi dua bobot yang berasal dari neuron tersembunyi.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Prosedur pengecualian mungkin tampak aneh dan sewenang-wenang. Mengapa dia harus membantu dengan regularisasi? Untuk menjelaskan apa yang terjadi, saya ingin Anda melupakan pengecualian untuk sementara waktu dan memberikan pelatihan Majelis Nasional dengan cara standar. Secara khusus, bayangkan bahwa kami melatih beberapa NS yang berbeda menggunakan data pelatihan yang sama. Tentu saja, jaringan dapat bervariasi pada awalnya, dan kadang-kadang pelatihan dapat memberikan hasil yang berbeda. Dalam kasus seperti itu, kita dapat menerapkan semacam skema rata-rata atau pemungutan suara untuk memutuskan output mana yang akan diterima. Misalnya, jika kita melatih lima jaringan, dan tiga di antaranya mengklasifikasikan angka tersebut sebagai "3," maka ini mungkin tiga yang benar. Dan dua jaringan lainnya mungkin salah. Skema rata-rata seperti itu seringkali merupakan cara yang berguna (walaupun mahal) untuk mengurangi pelatihan ulang. Alasannya adalahbahwa jaringan yang berbeda dapat berlatih kembali dengan cara yang berbeda, dan rata-rata dapat membantu menghilangkan pelatihan ulang tersebut.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bagaimana semua ini berhubungan dengan pengecualian? Secara heuristik, ketika kami mengecualikan set neutron yang berbeda, seolah-olah kami melatih NS yang berbeda. Oleh karena itu, prosedur pengecualian mirip dengan efek rata-rata pada sejumlah besar jaringan yang berbeda. Jaringan yang berbeda dilatih ulang dengan cara yang berbeda, sehingga diharapkan efek rata-rata pengecualian akan mengurangi pelatihan ulang. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Penjelasan heuristik terkait tentang manfaat pengecualian diberikan dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">salah satu karya paling awal</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">menggunakan teknik ini: “Teknik ini mengurangi adaptasi sendi yang kompleks dari neuron, karena neuron tidak dapat bergantung pada kehadiran tetangga tertentu. Pada akhirnya, ia harus mempelajari sifat-sifat yang lebih andal yang dapat berguna dalam bekerja bersama dengan banyak subset neuron acak yang berbeda. ” Dengan kata lain, jika kita membayangkan Majelis Nasional kita sebagai suatu model yang membuat prediksi, maka pengecualian akan menjadi cara untuk menjamin stabilitas model tersebut dengan hilangnya bagian-bagian bukti individual. Dalam hal ini, tekniknya menyerupai regularisasi L1 dan L2, yang berusaha mengurangi bobot, dan dengan cara ini membuat jaringan lebih tahan terhadap kehilangan koneksi individu dalam jaringan. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secara alami, ukuran sebenarnya dari manfaat pengecualian adalah keberhasilannya yang luar biasa dalam meningkatkan efisiensi jaringan saraf. Dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">karya aslinya</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">di mana metode ini diperkenalkan, itu diterapkan pada banyak tugas berbeda. </font><font style="vertical-align: inherit;">Kami sangat tertarik pada kenyataan bahwa penulis menerapkan pengecualian pada klasifikasi angka dari MNIST, menggunakan jaringan distribusi langsung sederhana mirip dengan yang kami periksa. </font><font style="vertical-align: inherit;">Makalah ini mencatat bahwa sampai saat itu, hasil terbaik untuk arsitektur seperti itu adalah akurasi 98,4%. </font><font style="vertical-align: inherit;">Mereka meningkatkannya menjadi 98,7% menggunakan kombinasi pengecualian dan bentuk modifikasi dari regularisasi L2. </font><font style="vertical-align: inherit;">Hasil yang sama mengesankannya diperoleh untuk banyak tugas lain, termasuk pengenalan pola dan ucapan, dan pemrosesan bahasa alami. </font><font style="vertical-align: inherit;">Pengecualian ini sangat berguna dalam pelatihan jaringan dalam yang besar, di mana masalah pelatihan ulang sering muncul.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Set data pelatihan yang diperluas secara artifisial </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami melihat sebelumnya bahwa akurasi klasifikasi MNIST kami turun menjadi 80 persen, ketika kami hanya menggunakan 1.000 gambar pelatihan. Dan tidak heran - dengan lebih sedikit data, jaringan kami akan memenuhi lebih sedikit opsi untuk menulis angka oleh orang-orang. Mari kita coba latih jaringan kami dari 30 neuron tersembunyi, menggunakan volume pelatihan yang berbeda untuk melihat perubahan efisiensi. Kami berlatih menggunakan ukuran paket mini 10, kecepatan belajar η = 0,5, parameter regularisasi λ = 5.0, dan fungsi biaya dengan cross entropy. Kami akan melatih jaringan 30 era menggunakan set data lengkap, dan meningkatkan jumlah era secara proporsional dengan penurunan volume data pelatihan. Untuk menjamin faktor penurunan berat badan yang sama untuk set data pelatihan yang berbeda, kami akan menggunakan parameter regularisasi λ = 5,0 dengan set pelatihan lengkap, dan menguranginya secara proporsional dengan penurunan volume data.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d6/a9f/6db/7d6a9f6db97493f1d716450344dd877f.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dapat dilihat bahwa akurasi klasifikasi tumbuh secara signifikan dengan meningkatnya data pelatihan. Pertumbuhan ini kemungkinan akan berlanjut dengan peningkatan volume lebih lanjut. Tentu saja, dilihat dari grafik di atas, kita sedang mendekati saturasi. Namun, misalkan kita mengulang grafik ini menjadi ketergantungan logaritmik pada jumlah data pelatihan:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dapat dilihat bahwa pada akhirnya grafik masih cenderung naik. Ini menunjukkan bahwa jika kita mengambil jumlah data yang jauh lebih besar - misalnya, jutaan atau bahkan miliaran contoh tulisan tangan, daripada 50.000 - maka kita mungkin akan mendapatkan jaringan kerja yang jauh lebih baik bahkan dari ukuran sekecil itu. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mendapatkan lebih banyak data pelatihan adalah ide bagus. Sayangnya, ini bisa mahal, jadi dalam praktiknya tidak selalu mungkin. Namun, ada ide lain yang dapat bekerja hampir sama - secara artifisial meningkatkan kumpulan data. Sebagai contoh, misalkan kita mengambil gambar lima dari MNIST, dan memutarnya sedikit, derajat 15:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcf/555/69f/bcf55569f0ecfda9357d6c1ce1f3e9fb.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/408/bcc/32a/408bcc32a8b3b03094eb4f2b7fdea833.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ini jelas angka yang sama. Tetapi pada tingkat piksel, ini sangat berbeda dari gambar yang tersedia di database MNIST. Masuk akal untuk mengasumsikan bahwa menambahkan gambar ini ke dataset pelatihan dapat membantu jaringan kami mempelajari lebih lanjut tentang klasifikasi gambar. Selain itu, kami jelas tidak terbatas pada kemampuan untuk menambahkan hanya satu gambar. Kami dapat memperluas data pelatihan kami dengan membuat beberapa belokan kecil dari semua gambar pelatihan dari MNIST, dan kemudian menggunakan rangkaian data pelatihan yang diperluas untuk meningkatkan efisiensi jaringan. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ide ini sangat kuat, dan digunakan secara luas. Mari kita lihat hasil dari </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">karya ilmiah</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yang menerapkan beberapa variasi ide ini ke MNIST. Salah satu arsitektur jaringan yang dipertimbangkan oleh mereka mirip dengan yang kami gunakan - jaringan distribusi langsung dengan 800 neuron tersembunyi, menggunakan fungsi biaya dengan cross entropy. Dengan meluncurkan jaringan ini dengan set pelatihan MNIST standar, mereka memperoleh akurasi klasifikasi 98,4%. Tapi kemudian mereka memperluas data pelatihan, tidak hanya menggunakan rotasi yang saya jelaskan di atas, tetapi juga transfer dan distorsi gambar. Setelah melatih jaringan pada data tingkat lanjut, mereka meningkatkan keakuratannya menjadi 98,9%. Mereka juga bereksperimen dengan apa yang disebut "Elastic distortion", jenis khusus dari distorsi gambar, dirancang untuk menghilangkan getaran acak dari otot-otot tangan. Menggunakan distorsi elastis untuk memperluas data, mereka mencapai akurasi 99,3%. Intinya, mereka memperluas pengalaman jaringan mereka,memberinya berbagai variasi tulisan tangan yang ditemukan dalam tulisan tangan asli.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Varian dari ide ini dapat digunakan untuk meningkatkan kinerja banyak tugas belajar, tidak hanya untuk pengenalan tulisan tangan. Prinsip umum adalah memperluas data pelatihan dengan menerapkan operasi pada mereka yang mencerminkan variasi yang ditemui dalam kenyataan. Variasi seperti itu mudah dibuat. Misalkan kita membuat NS untuk pengenalan ucapan. Orang-orang dapat mengenali pembicaraan bahkan dengan distorsi seperti kebisingan latar belakang. Oleh karena itu, Anda dapat memperluas data dengan menambahkan noise latar belakang. Kami juga mampu mengenali pembicaraan yang dipercepat dan lambat. Ini adalah cara lain untuk memperluas data pelatihan. Teknik-teknik ini tidak selalu digunakan - misalnya, alih-alih memperluas set pelatihan dengan menambahkan noise, mungkin lebih efisien untuk membersihkan input dengan menerapkan filter noise pada mereka. Namun, ada baiknya mengingat gagasan memperluas set pelatihan,dan mencari cara untuk menggunakannya.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Latihan </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seperti yang kita diskusikan di atas, salah satu cara untuk memperluas data pelatihan dari MNIST adalah dengan menggunakan rotasi kecil dari gambar-gambar pelatihan. </font><font style="vertical-align: inherit;">Masalah apa yang bisa muncul jika kita mengizinkan rotasi gambar di sudut mana pun?</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Penyimpangan data besar dan arti membandingkan akurasi klasifikasi </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mari kita lihat lagi bagaimana keakuratan NS kami bervariasi tergantung pada ukuran set pelatihan: </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b7a/774/4e0/b7a7744e077b9522d251333f647318b2.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Misalkan alih-alih menggunakan NS kita akan menggunakan teknologi pembelajaran mesin lain untuk mengklasifikasikan angka. Misalnya, mari kita coba menggunakan metode mesin vektor dukungan (SVM), yang kami temui secara singkat di bab 1. Saat itu, jangan khawatir jika Anda tidak terbiasa dengan SVM, kami tidak perlu memahami detailnya. Kami akan menggunakan SVM melalui perpustakaan scikit-learn. Berikut adalah bagaimana efektivitas SVM bervariasi dengan ukuran set pelatihan. Sebagai perbandingan, saya memasukkan jadwal dan hasil Majelis Nasional.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a6/21b/a21/9a621ba21ebb087e64fc7c68d3238ef5.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mungkin hal pertama yang menarik perhatian Anda - NS melampaui SVM dalam ukuran pelatihan apa pun. Ini bagus, meskipun tidak layak menarik kesimpulan yang jauh dari ini, karena saya menggunakan pengaturan scikit-belajar yang telah ditentukan, dan kami bekerja cukup serius di NS kami. Fakta yang kurang jelas, tetapi lebih menarik, yang mengikuti dari grafik, adalah bahwa jika kita melatih SVM kita menggunakan 50.000 gambar, itu akan bekerja lebih baik (akurasi 94,48%) daripada NS kami dilatih dengan 5000 gambar ( 93,24%). Dengan kata lain, peningkatan volume data pelatihan kadang-kadang mengimbangi perbedaan dalam algoritma MO.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sesuatu yang lebih menarik mungkin terjadi. Misalkan kita sedang mencoba untuk menyelesaikan masalah menggunakan dua algoritma MO, A dan B. Kadang-kadang terjadi bahwa algoritma A berada di depan algoritma B pada satu set data pelatihan, dan algoritma B berada di depan algoritma A pada kumpulan data pelatihan lainnya. Kami tidak melihat ini di atas - maka grafik akan berpotongan - tetapi </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ini terjadi</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Jawaban yang benar untuk pertanyaan: "Apakah algoritma A lebih unggul dari algoritma B?" sebenarnya, ini: "Apa dataset pelatihan yang Anda gunakan?"</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Semua ini harus diperhitungkan, baik selama pengembangan dan saat membaca karya ilmiah. Banyak karya berkonsentrasi pada menemukan trik baru untuk memeras hasil yang lebih baik pada set data pengukuran standar. "Teknologi makanan super kami memberi kami peningkatan X% pada set komparatif standar Y" - formulir aplikasi kanonik dalam penelitian tersebut. Terkadang pernyataan seperti itu sebenarnya menarik, tetapi perlu dipahami bahwa pernyataan itu hanya berlaku dalam konteks rangkaian pelatihan tertentu. Bayangkan sebuah cerita alternatif di mana orang-orang yang awalnya membuat set komparatif menerima dana penelitian yang lebih besar. Mereka dapat menggunakan uang ekstra untuk mengumpulkan data tambahan. Ada kemungkinan bahwa "peningkatan" teknologi super-duper akan hilang pada dataset yang lebih besar. Dengan kata lain,esensi dari perbaikan mungkin hanya kecelakaan. Dari ini, moralitas berikut harus dibawa ke bidang aplikasi praktis: kita membutuhkan algoritma yang ditingkatkan dan data pelatihan yang ditingkatkan. Tidak ada yang salah dengan mencari algoritma yang ditingkatkan, tetapi pastikan Anda tidak berkonsentrasi pada hal ini, mengabaikan cara yang lebih mudah untuk menang dengan meningkatkan volume atau kualitas data pelatihan.</font></font><br><br><h3>  Tantangan </h3><br><ul><li>  .              ?                 .        –  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">   </a> ,  ,   ,      .   ,             .        -   ?   ,      . </li></ul><br><h3>  Ringkasan </h3><br>  Kami telah menyelesaikan pencelupan kami dalam pelatihan ulang dan regularisasi.  Tentu saja, kami akan kembali ke masalah ini.  Seperti yang telah saya sebutkan beberapa kali, pelatihan ulang adalah masalah besar di bidang NS, terutama karena komputer menjadi lebih kuat dan kita dapat melatih jaringan yang lebih besar.  Akibatnya, ada kebutuhan mendesak untuk mengembangkan teknik regularisasi yang efektif untuk mengurangi pelatihan ulang, sehingga bidang ini sangat aktif saat ini. <br><br><h2>  Inisialisasi Berat </h2><br>  Ketika kita membuat NS kita, kita perlu membuat pilihan nilai awal bobot dan offset.  Sejauh ini, kami telah memilihnya sesuai dengan pedoman yang dijelaskan secara singkat di Bab 1. Saya ingat bahwa kami memilih bobot dan offset berdasarkan distribusi Gaussian independen dengan ekspektasi matematis 0 dan standar deviasi 1. Pendekatan ini bekerja dengan baik, tetapi tampaknya agak sewenang-wenang, jadi itu sangat berharga, merevisinya dan berpikir apakah mungkin untuk menemukan cara yang lebih baik untuk menetapkan bobot dan perpindahan awal, dan, mungkin, membantu NS kami belajar lebih cepat. <br><br>  Ternyata proses inisialisasi dapat ditingkatkan secara serius dibandingkan dengan distribusi Gaussian yang dinormalisasi.  Untuk memahami ini, katakanlah kita bekerja dengan jaringan dengan sejumlah besar neuron input, katakanlah, dari 1000. Dan katakanlah kita menggunakan distribusi Gaussian yang dinormalisasi untuk menginisialisasi bobot yang terhubung ke lapisan tersembunyi pertama.  Sejauh ini, saya hanya akan fokus pada skala yang menghubungkan neuron input ke neuron pertama di lapisan tersembunyi, dan mengabaikan sisa jaringan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac3/c9d/62f/ac3c9d62f301e85234b7cd4bbcbd0e0d.png"></div><br>  Untuk kesederhanaan, mari kita bayangkan bahwa kita mencoba untuk melatih jaringan dengan input x, di mana setengah dari neuron input dihidupkan, yaitu, mereka memiliki nilai 1, dan setengahnya dimatikan, yaitu, mereka memiliki nilai 0. Argumen berikutnya bekerja dalam kasus yang lebih umum, tetapi lebih mudah bagi Anda akan mengerti dia tentang contoh khusus ini.  Pertimbangkan jumlah tertimbang z = wjwjxj + b input untuk neuron tersembunyi.  500 anggota dari jumlah menghilang karena xj yang sesuai adalah 0. Oleh karena itu, z adalah jumlah dari 501 variabel acak Gaussian yang dinormalisasi, 500 bobot, dan 1 offset tambahan.  Oleh karena itu, nilai z itu sendiri memiliki distribusi Gaussian dengan ekspektasi matematika 0 dan standar deviasi √501 ≈ 22,4.  Artinya, z memiliki distribusi Gaussian yang cukup luas, tanpa puncak yang tajam: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/an/fj/_3/anfj_3qej8-fcxrnig7beewu3qm.png"></div><br>  Secara khusus, grafik ini menunjukkan bahwa | z | cenderung cukup besar, yaitu, z ≫ 1 atau z ≫ -1.  Dalam hal ini, output dari neuron tersembunyi σ (z) akan sangat dekat dengan 1 atau 0. Ini berarti bahwa neuron tersembunyi kita akan jenuh.  Dan ketika ini terjadi, seperti yang sudah kita ketahui, perubahan kecil dalam bobot akan menghasilkan perubahan kecil dalam aktivasi neuron tersembunyi.  Perubahan kecil ini, pada gilirannya, praktis tidak akan mempengaruhi neutron yang tersisa dalam jaringan, dan kita akan melihat perubahan kecil terkait fungsi biaya.  Akibatnya, bobot ini akan dilatih sangat lambat ketika kita menggunakan algoritma gradient descent.  Ini mirip dengan tugas yang telah kita bahas dalam bab ini, di mana neuron keluaran jenuh dengan nilai-nilai yang salah menyebabkan pembelajaran melambat.  Kami biasa mengatasi masalah ini dengan secara cerdik memilih fungsi biaya.  Sayangnya, meskipun ini membantu dengan neuron output jenuh, itu tidak membantu sama sekali dengan saturasi neuron tersembunyi. <br><br>  Sekarang saya berbicara tentang skala yang masuk dari lapisan tersembunyi pertama.  Tentu saja, argumen yang sama berlaku untuk lapisan tersembunyi berikut: jika bobot dalam lapisan tersembunyi kemudian diinisialisasi menggunakan distribusi Gaussian yang dinormalisasi, aktivasi mereka akan sering mendekati 0 atau 1, dan pelatihan akan berjalan sangat lambat. <br><br>  Apakah ada cara untuk memilih opsi inisialisasi terbaik untuk bobot dan offset, sehingga kita tidak mendapatkan kejenuhan seperti itu, dan dapat menghindari belajar keterlambatan?  Misalkan kita memiliki neuron dengan jumlah bobot masuk n.  Maka kita perlu menginisialisasi bobot ini dengan distribusi Gaussian acak dengan ekspektasi matematika 0 dan deviasi standar 1 / √ n <sub>in</sub> .  Artinya, kita menekan Gaussians, dan mengurangi kemungkinan saturasi neuron.  Kemudian kita akan memilih distribusi Gaussian untuk perpindahan dengan ekspektasi matematis 0 dan standar deviasi 1, dengan alasan bahwa saya akan kembali lagi nanti.  Setelah membuat pilihan ini, kita kembali menemukan bahwa z = w <sub>jw j</sub> x <sub>j</sub> + b akan menjadi variabel acak dengan distribusi Gaussian dengan ekspektasi matematis 0, tetapi dengan puncak yang jauh lebih jelas daripada sebelumnya.  Misalkan, seperti sebelumnya, bahwa 500 input adalah 0 dan 500 adalah 1. Maka mudah untuk menunjukkan (lihat latihan di bawah ini) bahwa z memiliki distribusi Gaussian dengan ekspektasi matematis 0 dan standar deviasi √ (3/2) = 1.22 ... Grafik ini memiliki puncak yang jauh lebih tajam, sedemikian rupa sehingga bahkan dalam gambar di bawah situasinya sedikit dikecilkan, karena saya harus mengubah skala sumbu vertikal dibandingkan dengan grafik sebelumnya: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5e/ee/xk/5eeexk-kyxhmi3pdoslo9w_al44.png"></div><br>  Neuron semacam itu akan jenuh dengan probabilitas yang jauh lebih rendah, dan, karenanya, kemungkinan kecil akan mengalami penurunan dalam pembelajaran. <br><br><h3>  Latihan </h3><br><ul><li>  Konfirmasikan bahwa standar deviasi z = ∑ <sub>jw</sub> <sub>j</sub> x <sub>j</sub> + b dari paragraf sebelumnya adalah √ (3/2).  Pertimbangan yang mendukung ini: varians dari jumlah variabel acak independen sama dengan jumlah varian variabel acak individu;  variansnya sama dengan kuadrat dari deviasi standar. </li></ul><br>  Saya sebutkan di atas bahwa kita akan terus menginisialisasi perpindahan, seperti sebelumnya, berdasarkan pada distribusi Gaussian yang independen dengan ekspektasi matematis 0 dan standar deviasi 1. Dan ini normal, karena tidak banyak meningkatkan kemungkinan saturasi neuron kita.  Sebenarnya, inisialisasi offset tidak terlalu menjadi masalah jika kita berhasil menghindari masalah saturasi.  Beberapa bahkan mencoba untuk menginisialisasi semua offset ke nol, dan mengandalkan fakta bahwa gradient descent dapat mempelajari offset yang sesuai.  Tetapi karena kemungkinan ini akan mempengaruhi sesuatu yang kecil, kami akan terus menggunakan prosedur inisialisasi yang sama seperti sebelumnya. <br><br>  Mari kita bandingkan hasil dari pendekatan lama dan baru untuk menginisialisasi bobot menggunakan tugas mengklasifikasikan angka dari MNIST.  Seperti sebelumnya, kita akan menggunakan 30 neuron tersembunyi, paket mini ukuran 10, parameter regularisasi &amp; lambda = 5.0, dan fungsi biaya dengan cross entropy.  Kami akan secara bertahap mengurangi kecepatan belajar dari η = 0,5 menjadi 0,1, karena dengan cara ini hasilnya akan sedikit lebih baik terlihat pada grafik.  Anda dapat belajar menggunakan metode inisialisasi berat yang lama: <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.large_weight_initializer() &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Anda juga dapat belajar menggunakan pendekatan baru untuk menginisialisasi bobot.  Ini bahkan lebih sederhana, karena secara default network2 menginisialisasi bobot menggunakan pendekatan baru.  Ini berarti bahwa kami dapat menghilangkan panggilan net.large_weight_initializer () sebelumnya: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Kami memplot (menggunakan program weight_initialization.py): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/640/4ab/59d6404ab3c5e01946106b26743ae130.png"></div><br>  Dalam kedua kasus, akurasi klasifikasi 96% diperoleh.  Akurasi yang dihasilkan hampir sama dalam kedua kasus.  Tetapi teknik inisialisasi baru mencapai titik ini jauh lebih cepat.  Pada akhir era terakhir pelatihan, pendekatan lama untuk menginisialisasi bobot mencapai akurasi 87%, dan pendekatan baru sudah mendekati 93%.  Rupanya, pendekatan baru untuk menginisialisasi bobot dimulai dari posisi yang jauh lebih baik, sehingga kami mendapatkan hasil yang baik lebih cepat.  Fenomena yang sama diamati jika kita membangun hasil untuk jaringan dengan 100 neuron: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/72d/8ef/ccd72d8effddc7815bd526dcb1434acd.png"></div><br>  Dalam hal ini, dua kurva tidak terjadi.  Namun, percobaan saya mengatakan bahwa jika Anda menambahkan sedikit lebih banyak era, maka keakuratannya mulai hampir bersamaan.  Oleh karena itu, berdasarkan percobaan ini, kita dapat mengatakan bahwa meningkatkan inisialisasi bobot hanya mempercepat pelatihan, tetapi tidak mengubah efisiensi jaringan secara keseluruhan.  Namun, dalam bab 4 kita akan melihat contoh NS di mana efisiensi jangka panjang meningkat secara signifikan sebagai akibat dari inisialisasi bobot melalui 1 / √ n <sub>in</sub> .  Oleh karena itu, tidak hanya meningkatkan kecepatan belajar, tetapi kadang-kadang efektivitas yang dihasilkan. <br><br>  Pendekatan untuk menginisialisasi bobot melalui 1 / √n <sub>dalam</sub> membantu meningkatkan pelatihan jaringan saraf.  Teknik-teknik lain untuk menginisialisasi bobot telah diusulkan, banyak di antaranya didasarkan pada ide dasar ini.  Saya tidak akan mempertimbangkannya di sini, karena 1 / inn bekerja dengan baik untuk tujuan kita.  Jika Anda tertarik, saya sarankan membaca diskusi di halaman 14 dan 15 di sebuah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">makalah 2012 oleh</a> Yoshua Benggio. <br><br><h3>  Tantangan </h3><br><ul><li>  Kombinasi regularisasi dan metode inisialisasi berat badan ditingkatkan.  Kadang-kadang regularisasi L2 secara otomatis memberi kita hasil yang mirip dengan metode baru inisialisasi bobot.  Katakanlah kita menggunakan pendekatan lama untuk menginisialisasi bobot.  Buat garis besar argumen heuristik yang membuktikan bahwa: (1) jika λ tidak terlalu kecil, maka pada zaman pelatihan pertama, pelemahan bobot akan mendominasi hampir sepenuhnya;  (2) jika ηλ ≪ n, maka bobot akan melemah e <sup>−ηλ / m</sup> kali dalam zaman;  (3) jika λ tidak terlalu besar, melemahnya bobot akan melambat ketika bobot turun menjadi sekitar 1 / √ n, di mana n adalah jumlah total bobot dalam jaringan.  Buktikan bahwa syarat-syarat ini dipenuhi dalam contoh-contoh yang grafiknya dibangun di bagian ini. </li></ul><br><br><h2>  Kembali ke pengenalan tulisan tangan: kode </h2><br>  Mari menerapkan ide-ide yang dijelaskan dalam bab ini.  Kami akan mengembangkan program baru, network2.py, versi yang lebih baik dari program network.py yang kami buat di bab 1. Jika Anda belum melihat kodenya untuk waktu yang lama, Anda mungkin perlu dengan cepat membahasnya.  Ini hanya 74 baris kode, dan mudah dimengerti. <br><br>  Seperti halnya network.py, bintang dari network2.py adalah kelas Jaringan, yang kami gunakan untuk mewakili NS kami.  Kami menginisialisasi instance kelas dengan daftar ukuran lapisan jaringan yang sesuai, dan dengan pilihan fungsi biaya, secara default akan menjadi lintas entropi: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Network</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, sizes, cost=CrossEntropyCost)</span></span></span><span class="hljs-function">:</span></span> self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost</code> </pre> <br>  Beberapa baris pertama dari metode __init__ sama dengan network.py, dan dipahami sendiri.  Dua baris berikutnya adalah baru, dan kita perlu memahami secara rinci apa yang mereka lakukan. <br><br>  Mari kita mulai dengan metode default_weight_initializer.  Dia menggunakan pendekatan baru yang ditingkatkan untuk menginisialisasi bobot.  Seperti yang telah kita lihat, dalam pendekatan ini, bobot memasuki neuron diinisialisasi berdasarkan distribusi Gaussian independen dengan ekspektasi matematis 0 dan deviasi standar 1 dibagi dengan akar kuadrat dari jumlah tautan yang masuk ke neuron.  Juga, metode ini akan menginisialisasi offset menggunakan distribusi Gaussian dengan rata-rata 0 dan standar deviasi 1. Berikut adalah kodenya: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">default_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Untuk memahaminya, Anda harus ingat bahwa np adalah perpustakaan Numpy yang berurusan dengan aljabar linier.  Kami mengimpornya di awal program.  Perhatikan juga bahwa kami tidak menginisialisasi perpindahan di lapisan pertama neuron.  Lapisan pertama adalah inbound, jadi offset tidak digunakan.  Yang sama adalah network.py. <br><br>  Selain metode default_weight_initializer, kami akan membuat metode large_weight_initializer.  Ini menginisialisasi bobot dan offset menggunakan pendekatan lama dari Bab 1, di mana bobot dan offset diinisialisasi berdasarkan distribusi Gaussian independen dengan ekspektasi matematis 0 dan standar deviasi 1. Kode ini, tentu saja, tidak jauh berbeda dari default_weight_initializer: <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">large_weight_initializer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.biases = [np.random.randn(y, <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:]] self.weights = [np.random.randn(y, x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(self.sizes[:<span class="hljs-number"><span class="hljs-number">-1</span></span>], self.sizes[<span class="hljs-number"><span class="hljs-number">1</span></span>:])]</code> </pre> <br>  Saya memasukkan metode ini terutama karena lebih mudah bagi kami untuk membandingkan hasil dari bab dan bab 1. Saya tidak bisa membayangkan opsi nyata yang saya sarankan untuk menggunakannya! <br><br>  Kebaruan kedua dari metode __init__ akan menjadi inisialisasi atribut biaya.  Untuk memahami cara kerjanya, mari kita lihat kelas yang kita gunakan untuk merepresentasikan fungsi biaya lintas-entropi (direktif @staticmethod memberi tahu juru bahasa bahwa metode ini tidak tergantung pada objek, sehingga parameter mandiri tidak diteruskan ke metode fn dan delta). <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropyCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.sum(np.nan_to_num(-y*np.log(a)-(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*np.log(<span class="hljs-number"><span class="hljs-number">1</span></span>-a))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay)</code> </pre> <br>  Mari kita cari tahu.  Hal pertama yang dapat dilihat di sini adalah bahwa, meskipun cross entropy adalah fungsi dari sudut pandang matematika, kami mengimplementasikannya sebagai kelas python, bukan fungsi python.  Mengapa saya memutuskan untuk melakukan ini?  Di jaringan kami, nilai memainkan dua peran yang berbeda.  Jelas - ini adalah ukuran seberapa baik aktivasi output sesuai dengan output yang diinginkan y.  Peran ini disediakan oleh metode CrossEntropyCost.fn.  (Omong-omong, perhatikan bahwa memanggil np.nan_to_num di dalam CrossEntropyCost.fn memastikan bahwa Numpy memproses logaritma angka yang mendekati nol dengan benar).  Namun, fungsi biaya digunakan dalam jaringan kami dengan cara kedua.  Kita ingat dari Bab 2 bahwa ketika memulai algoritma backpropagation, kita perlu mempertimbangkan kesalahan output dari jaringan δ <sup>L.</sup>  Bentuk kesalahan output tergantung pada fungsi biaya: fungsi biaya yang berbeda akan memiliki bentuk kesalahan output yang berbeda.  Untuk cross entropy, kesalahan output, seperti yang berikut dari persamaan (66), akan sama dengan: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>&amp;#x2212;</mo><mi>y</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>99</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.761ex" height="2.901ex" viewBox="0 -987.6 9799.8 1249" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-65" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-6C" x="1240" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="1538" y="0"></use><g transform="translate(1900,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-3D" x="3289" y="0"></use><g transform="translate(4345,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-4C" x="748" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-2212" x="5679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-79" x="6679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-74" x="7427" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-61" x="7788" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMATHI-67" x="8318" y="0"></use><g transform="translate(8798,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/459816/&amp;usg=ALkJrhgHFEQetXDReq1V3mK23JpTRrmvTQ#MJMAIN-39" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>d</mi><mi>e</mi><mi>l</mi><mi>t</mi><msup><mi>a</mi><mi>L</mi></msup><mo>=</mo><msup><mi>a</mi><mi>L</mi></msup><mo>−</mo><mi>y</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>99</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> \ delta ^ L = a ^ L-y \ tag {99} </script></p><br><br>  Oleh karena itu, saya mendefinisikan metode kedua, CrossEntropyCost.delta, yang tujuannya adalah untuk menjelaskan kepada jaringan cara menghitung kesalahan output.  Dan kemudian kami menggabungkan kedua metode ini ke dalam satu kelas yang berisi segala sesuatu yang perlu diketahui jaringan kami tentang fungsi biaya. <br><br>  Untuk alasan yang sama, network2.py berisi kelas yang mewakili fungsi biaya kuadratik.  Termasuk ini untuk perbandingan dengan hasil Bab 1, karena di masa depan kita terutama akan menggunakan cross entropy.  Kode di bawah.  Metode QuadraticCost.fn adalah perhitungan sederhana biaya kuadratik yang terkait dengan output a dan output y yang diinginkan.  Nilai yang dikembalikan oleh QuadraticCost.delta didasarkan pada ekspresi (30) untuk kesalahan output dari nilai kuadratik, yang kami peroleh pada Bab 2. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">QuadraticCost</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fn</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*np.linalg.norm(ay)**<span class="hljs-number"><span class="hljs-number">2</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delta</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z, a, y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (ay) * sigmoid_prime(z)</code> </pre> <br>  Sekarang kami telah menemukan perbedaan utama antara network2.py dan network2.py.  Semuanya sangat sederhana.  Ada perubahan kecil lain yang akan saya jelaskan di bawah ini, termasuk implementasi regularisasi L2.  Sebelum itu, mari kita lihat kode network2.py lengkap.  Tidak perlu mempelajarinya secara rinci, tetapi perlu memahami struktur dasar, khususnya, membaca komentar untuk memahami apa yang masing-masing bagian dari program lakukan.  Tentu saja, saya tidak melarang mempelajari pertanyaan ini sebanyak yang Anda suka!  Jika Anda tersesat, coba baca teks setelah program, dan kembali ke kode lagi.  Secara umum, ini dia: <br><br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"""network2.py ~~~~~~~~~~~~~~   network.py,            .   –      , ,   .     ,    .   ,       . """</span></span> <span class="hljs-comment"><span class="hljs-comment">####  #  import json import random import sys #  import numpy as np ####   ,      class QuadraticCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. """ return 0.5*np.linalg.norm(ay)**2 @staticmethod def delta(z, a, y): """  delta   .""" return (ay) * sigmoid_prime(z) class CrossEntropyCost(object): @staticmethod def fn(a, y): """ ,    ``a``    ``y``. np.nan_to_num    .  ,   ``a``  ``y``      1.0,   (1-y)*np.log(1-a)  nan. np.nan_to_num ,       (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """  delta   .  ``z``    ,          delta     . """ return (ay) ####   Network class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """  sizes      .  ,      Network      ,     ,     ,    ,  [2, 3, 1].       ,   ``self.default_weight_initializer`` (.  ). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """            0    1,       ,       .          0    1.    ,         ,           . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """          0    1.          0    1.    ,         ,           .         1,    .       . """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """  ,  ``a``  .""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """     -    . ``training_data`` –   ``(x, y)``,       .       ,    ``lmbda``.    ``evaluation_data``,     ,   .         ,     ,   .      :   ,    ,   ,              .  ,      30 ,        30 ,        .     ,   . """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: {}".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: {} / {}".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: {}".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: {} / {}".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """    ,          -. ``mini_batch`` –    ``(x, y)``, ``eta`` –  , ``lmbda`` -  , ``n`` -     .""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """  ``(nabla_b, nabla_w)``,      C_x. ``nabla_b``  ``nabla_w`` -    numpy,   ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] #   activation = x activations = [x] #      zs = [] #     z- for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) """  l      ,      . l = 1    , l = 2 – ,   .    ,   python      . """ for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """    ``data``,      .   –        .  ``convert``  False,    –    ( )  True,   .    - ,  ``y`` -     .  ,       .           .          ?     –       ,      .   ,      .       mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """      ``data``.  ``convert``   False,   –  (),   True,   –   . .    ,      ``accuracy``, . """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """    ``filename``.""" data = {"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)} f = open(filename, "w") json.dump(data, f) f.close() ####  Network def load(filename): """    ``filename``.    Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net ####   def vectorized_result(j): """  10-    1.0   j     .      (0..9)     . """ e = np.zeros((10, 1)) e[j] = 1.0 return e def sigmoid(z): """.""" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): """ .""" return sigmoid(z)*(1-sigmoid(z))</span></span></code> </pre> <br>  Di antara perubahan yang lebih menarik adalah dimasukkannya regularisasi L2.  Meskipun ini adalah perubahan konseptual yang besar, sangat mudah untuk diterapkan sehingga Anda mungkin tidak melihatnya dalam kode.  Sebagian besar, ini hanya melewatkan parameter lmbda ke metode yang berbeda, terutama Network.SGD.  Semua pekerjaan dilakukan dalam satu baris program, yang keempat dari akhir dalam metode Network.update_mini_batch.  Di sana kami mengubah aturan pembaruan gradient descent untuk memasukkan pengurangan berat.  Perubahannya kecil, tetapi sangat memengaruhi hasil! <br><br>  Omong-omong, ini sering terjadi ketika menerapkan teknik baru dalam jaringan saraf.  Kami menghabiskan ribuan kata untuk membahas regularisasi.  Secara konseptual, ini adalah hal yang cukup halus dan sulit dipahami.  Namun, ini dapat ditambahkan secara sepele ke dalam program!  Tanpa diduga, teknik kompleks dapat diimplementasikan dengan perubahan kode kecil. <br><br>  Perubahan kecil tapi penting lainnya dalam kode adalah penambahan beberapa flag opsional pada metode gradient descent stochastic Network.SGD.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bendera ini memungkinkan untuk melacak biaya dan akurasi pada training_data atau evaluation_data, yang dapat ditransmisikan ke Network.SGD. </font><font style="vertical-align: inherit;">Sebelumnya di bab ini, kami sering menggunakan flag-flag ini, tetapi izinkan saya memberi contoh penggunaannya, hanya sebagai pengingat:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], cost=network2.CrossEntropyCost) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami menetapkan evaluation_data melalui validation_data. Namun, kami dapat melacak kinerja pada test_data dan kumpulan data lainnya. Kami juga memiliki empat flag yang menentukan kebutuhan untuk melacak biaya dan keakuratan baik pada evaluation_data dan training_data. Bendera ini disetel ke False secara default, namun mereka disertakan di sini untuk melacak efektivitas Jaringan. Selain itu, metode Network.SGD dari network2.py mengembalikan tupel empat elemen yang mewakili hasil pelacakan. Anda bisa menggunakannya seperti ini:</font></font><br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>evaluation_cost, evaluation_accuracy, ... training_cost, training_accuracy = net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, ... lmbda = <span class="hljs-number"><span class="hljs-number">5.0</span></span>, ... evaluation_data=validation_data, ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_evaluation_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ... monitor_training_cost=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, misalnya, evaluation_cost akan menjadi daftar 30 elemen yang berisi biaya perkiraan data pada akhir setiap era. Informasi tersebut sangat berguna untuk memahami perilaku jaringan saraf. Informasi tersebut sangat berguna untuk memahami perilaku jaringan. Misalnya, dapat digunakan untuk menggambar grafik pembelajaran jaringan dari waktu ke waktu. Begitulah cara saya membuat semua grafik dari bab ini. Namun, jika salah satu flag tidak diatur, elemen tuple yang sesuai akan menjadi daftar kosong.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambahan kode lain termasuk metode Network.save, yang menyimpan objek Network ke disk, dan fungsi memuatnya ke dalam memori. Menyimpan dan memuat dilakukan melalui JSON, bukan modul acar Python atau cPickle, yang biasanya digunakan untuk menyimpan ke disk dan memuat dalam python. Menggunakan JSON membutuhkan lebih banyak kode daripada yang diperlukan untuk acar atau cPickle. Untuk memahami mengapa saya memilih JSON, bayangkan bahwa di beberapa titik di masa depan kami memutuskan untuk mengubah kelas Jaringan kami sehingga ada lebih dari neuron sigmoid. Untuk menerapkan perubahan ini, kami kemungkinan besar akan mengubah atribut yang didefinisikan dalam metode Network .__ init__. Dan jika kita hanya menggunakan acar untuk menghemat, fungsi beban kita tidak akan berfungsi. Menggunakan JSON dengan serialisasi eksplisit membuatnya mudah bagi kami untuk menjaminbahwa versi yang lebih lama dari objek Jaringan dapat diunduh.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ada banyak perubahan kecil dalam kode, tetapi ini hanyalah variasi kecil dari network.py. </font><font style="vertical-align: inherit;">Hasil akhirnya adalah perpanjangan dari program kami dari 74 baris ke program yang jauh lebih fungsional dari 152 baris.</font></font><br><br><h3>  Tantangan </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubah kode di bawah ini dengan memperkenalkan regularisasi L1, dan gunakan untuk mengklasifikasikan digit MNIST oleh jaringan dengan 30 neuron tersembunyi. </font><font style="vertical-align: inherit;">Dapatkah Anda memilih parameter regularisasi yang memungkinkan Anda untuk meningkatkan hasil dibandingkan dengan jaringan tanpa regularisasi?</font></font></li><li>    Network.cost_derivative method  network.py.      .        ?     ,       ?  network2.py      Network.cost_derivative,     CrossEntropyCost.delta.      ? </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id459816/">https://habr.com/ru/post/id459816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id459802/index.html">Habr Weekly # 9 / Burnout di masa muda, antarmuka Jepang, jaringan saraf Battle.net, game, dan kekejaman</a></li>
<li><a href="../id459804/index.html">Buat kartu bantuan crowdsourcing di WordPress + shMapper</a></li>
<li><a href="../id459806/index.html">Cara kami merawat kucing Lapuna</a></li>
<li><a href="../id459810/index.html">Layanan mikro atau monolit: mencari solusi</a></li>
<li><a href="../id459814/index.html">Apa yang kamu, Rendering Engine? Atau cara modul tampilan browser bekerja</a></li>
<li><a href="../id459820/index.html">Geser kartu saja: bagaimana OS / 2 digunakan di subway New York</a></li>
<li><a href="../id459822/index.html">Contoh jaringan saraf sederhana, sebagai hasilnya, mencari tahu apa itu</a></li>
<li><a href="../id459824/index.html">Daftar periksa untuk menulis ekstensi Visual Studio yang hebat</a></li>
<li><a href="../id459828/index.html">Berita mingguan: Harga tiket Hyperloop di Rusia, penambangan arus utama komputer Apollo, bot AI di StarCraft II</a></li>
<li><a href="../id459830/index.html">Tentu saja, mereka memberi kekuatan dan garis dari senapan mesin. Kanker dan banyak lagi ... pengalaman dengan obat-obatan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>