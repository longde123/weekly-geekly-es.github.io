<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üö∂üèø üëµüèø ‚å®Ô∏è Python + OpenCV + Keras: haga reconocimiento de texto en media hora üêò ‚úçüèø ü§µüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola habr 

 Despu√©s de experimentar con una conocida base de 60,000 n√∫meros escritos a mano, MNIST, surgi√≥ la pregunta l√≥gica de si hab√≠a algo simila...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python + OpenCV + Keras: haga reconocimiento de texto en media hora</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/466565/">  Hola habr <br><br>  Despu√©s de experimentar con una conocida base de 60,000 n√∫meros escritos a mano, MNIST, surgi√≥ la pregunta l√≥gica de si hab√≠a algo similar, pero con soporte no solo para los n√∫meros, sino tambi√©n para las letras.  Al final result√≥ que existe, y se llama tal base, como se puede adivinar, Extended MNIST (EMNIST). <br><br>  Si alguien est√° interesado en c√≥mo usar esta base de datos puede hacer un simple reconocimiento de texto, bienvenido a cat. <br><br><img src="https://habrastorage.org/webt/kq/bl/4r/kqbl4rtgmtvz1xl50tzbulmdmlw.png"><br><a name="habracut"></a><br>  <i>Nota</i> : este ejemplo es experimental y educativo, solo estaba interesado en ver qu√© sucede.  No plane√© y no planeo hacer el segundo FineReader, por lo que muchas cosas aqu√≠, por supuesto, no est√°n implementadas.  Por lo tanto, no se aceptan reclamos en el estilo de "por qu√©", "ya es mejor", etc.  Probablemente ya haya bibliotecas de OCR listas para Python, pero fue interesante hacerlo usted mismo.  Por cierto, para aquellos que quieren ver c√≥mo se hizo el verdadero FineReader, hay dos art√≠culos en su blog sobre Habr en 2014: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">1</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">2</a> (pero, por supuesto, sin c√≥digos fuente y detalles, como en cualquier blog corporativo).  Bueno, comencemos, todo est√° abierto aqu√≠ y todo es de c√≥digo abierto. <br><br>  Por ejemplo, tomaremos el texto sin formato.  Aqu√≠ hay uno: <br><br><h3>  HOLA MUNDO </h3><br>  Y veamos qu√© se puede hacer con √©l. <br><br><h2>  Romper texto en letras </h2><br>  El primer paso es dividir el texto en letras separadas.  OpenCV es √∫til para esto, m√°s precisamente su funci√≥n findContours. <br><br>  Abra la imagen (cv2.imread), traduzca a b / w (cv2.cvtColor + cv2.threshold), aumente ligeramente (cv2.erode) y encuentre los contornos. <br><br><pre><code class="python hljs">image_file = <span class="hljs-string"><span class="hljs-string">"text.png"</span></span> img = cv2.imread(image_file) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ret, thresh = cv2.threshold(gray, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">255</span></span>, cv2.THRESH_BINARY) img_erode = cv2.erode(thresh, np.ones((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), np.uint8), iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Get contours contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) output = img.copy() for idx, contour in enumerate(contours): (x, y, w, h) = cv2.boundingRect(contour) # print("R", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx]) # hierarchy[i][0]: the index of the next contour of the same level # hierarchy[i][1]: the index of the previous contour of the same level # hierarchy[i][2]: the index of the first child # hierarchy[i][3]: the index of the parent if hierarchy[0][idx][3] == 0: cv2.rectangle(output, (x, y), (x + w, y + h), (70, 0, 0), 1) cv2.imshow("Input", img) cv2.imshow("Enlarged", img_erode) cv2.imshow("Output", output) cv2.waitKey(0)</span></span></code> </pre> <br>  Obtenemos un √°rbol jer√°rquico de contornos (par√°metro cv2.RETR_TREE).  Primero viene el contorno general de la imagen, luego los contornos de las letras, luego los contornos internos.  Solo necesitamos el esquema de las letras, as√≠ que verifico que el "esquema" es el esquema general.  Este es un enfoque simplificado, y para escaneos reales esto puede no funcionar, aunque no es cr√≠tico reconocer capturas de pantalla. <br><br>  Resultado: <br><br><img src="https://habrastorage.org/webt/7j/zi/pg/7jzipgqvc9ebvxgu2j7c_ubr-rw.png"><br><br>  El siguiente paso es guardar cada letra, habi√©ndola escalado previamente a un cuadrado de 28x28 (es en este formato que se almacena la base de datos MNIST).  OpenCV se basa en numpy, por lo que podemos usar las funciones de trabajar con matrices para recortar y escalar. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">letters_extract</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image_file: str, out_size=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">28</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; List[Any]:</span></span> img = cv2.imread(image_file) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ret, thresh = cv2.threshold(gray, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">255</span></span>, cv2.THRESH_BINARY) img_erode = cv2.erode(thresh, np.ones((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), np.uint8), iterations=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Get contours contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) output = img.copy() letters = [] for idx, contour in enumerate(contours): (x, y, w, h) = cv2.boundingRect(contour) # print("R", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx]) # hierarchy[i][0]: the index of the next contour of the same level # hierarchy[i][1]: the index of the previous contour of the same level # hierarchy[i][2]: the index of the first child # hierarchy[i][3]: the index of the parent if hierarchy[0][idx][3] == 0: cv2.rectangle(output, (x, y), (x + w, y + h), (70, 0, 0), 1) letter_crop = gray[y:y + h, x:x + w] # print(letter_crop.shape) # Resize letter canvas to square size_max = max(w, h) letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8) if w &gt; h: # Enlarge image top-bottom # ------ # ====== # ------ y_pos = size_max//2 - h//2 letter_square[y_pos:y_pos + h, 0:w] = letter_crop elif w &lt; h: # Enlarge image left-right # --||-- x_pos = size_max//2 - w//2 letter_square[0:h, x_pos:x_pos + w] = letter_crop else: letter_square = letter_crop # Resize letter to 28x28 and add letter and its X-coordinate letters.append((x, w, cv2.resize(letter_square, (out_size, out_size), interpolation=cv2.INTER_AREA))) # Sort array in place by X-coordinate letters.sort(key=lambda x: x[0], reverse=False) return letters</span></span></code> </pre><br>  Al final, clasificamos las letras por la coordenada X, tal como puede ver, guardamos los resultados en forma de tupla (x, w, letra), para que se puedan seleccionar espacios entre los espacios entre las letras. <br><br>  Aseg√∫rate de que todo funcione: <br><br><pre> <code class="python hljs">cv2.imshow(<span class="hljs-string"><span class="hljs-string">"0"</span></span>, letters[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>]) cv2.imshow(<span class="hljs-string"><span class="hljs-string">"1"</span></span>, letters[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>]) cv2.imshow(<span class="hljs-string"><span class="hljs-string">"2"</span></span>, letters[<span class="hljs-number"><span class="hljs-number">2</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>]) cv2.imshow(<span class="hljs-string"><span class="hljs-string">"3"</span></span>, letters[<span class="hljs-number"><span class="hljs-number">3</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>]) cv2.imshow(<span class="hljs-string"><span class="hljs-string">"4"</span></span>, letters[<span class="hljs-number"><span class="hljs-number">4</span></span>][<span class="hljs-number"><span class="hljs-number">2</span></span>]) cv2.waitKey(<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j-/uw/yh/j-uwyhhh8l0yrapth5u6fy9na0u.png"><br><br>  Las cartas est√°n listas para el reconocimiento, las reconoceremos utilizando una red convolucional; este tipo de red es muy adecuado para tales tareas. <br><br><h2>  Red neuronal (CNN) para reconocimiento </h2><br>  El conjunto de datos EMNIST de origen tiene 62 caracteres diferentes (A..Z, 0..9, etc.): <br><br><pre> <code class="python hljs">emnist_labels = [<span class="hljs-number"><span class="hljs-number">48</span></span>, <span class="hljs-number"><span class="hljs-number">49</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">51</span></span>, <span class="hljs-number"><span class="hljs-number">52</span></span>, <span class="hljs-number"><span class="hljs-number">53</span></span>, <span class="hljs-number"><span class="hljs-number">54</span></span>, <span class="hljs-number"><span class="hljs-number">55</span></span>, <span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">57</span></span>, <span class="hljs-number"><span class="hljs-number">65</span></span>, <span class="hljs-number"><span class="hljs-number">66</span></span>, <span class="hljs-number"><span class="hljs-number">67</span></span>, <span class="hljs-number"><span class="hljs-number">68</span></span>, <span class="hljs-number"><span class="hljs-number">69</span></span>, <span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">71</span></span>, <span class="hljs-number"><span class="hljs-number">72</span></span>, <span class="hljs-number"><span class="hljs-number">73</span></span>, <span class="hljs-number"><span class="hljs-number">74</span></span>, <span class="hljs-number"><span class="hljs-number">75</span></span>, <span class="hljs-number"><span class="hljs-number">76</span></span>, <span class="hljs-number"><span class="hljs-number">77</span></span>, <span class="hljs-number"><span class="hljs-number">78</span></span>, <span class="hljs-number"><span class="hljs-number">79</span></span>, <span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">81</span></span>, <span class="hljs-number"><span class="hljs-number">82</span></span>, <span class="hljs-number"><span class="hljs-number">83</span></span>, <span class="hljs-number"><span class="hljs-number">84</span></span>, <span class="hljs-number"><span class="hljs-number">85</span></span>, <span class="hljs-number"><span class="hljs-number">86</span></span>, <span class="hljs-number"><span class="hljs-number">87</span></span>, <span class="hljs-number"><span class="hljs-number">88</span></span>, <span class="hljs-number"><span class="hljs-number">89</span></span>, <span class="hljs-number"><span class="hljs-number">90</span></span>, <span class="hljs-number"><span class="hljs-number">97</span></span>, <span class="hljs-number"><span class="hljs-number">98</span></span>, <span class="hljs-number"><span class="hljs-number">99</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">101</span></span>, <span class="hljs-number"><span class="hljs-number">102</span></span>, <span class="hljs-number"><span class="hljs-number">103</span></span>, <span class="hljs-number"><span class="hljs-number">104</span></span>, <span class="hljs-number"><span class="hljs-number">105</span></span>, <span class="hljs-number"><span class="hljs-number">106</span></span>, <span class="hljs-number"><span class="hljs-number">107</span></span>, <span class="hljs-number"><span class="hljs-number">108</span></span>, <span class="hljs-number"><span class="hljs-number">109</span></span>, <span class="hljs-number"><span class="hljs-number">110</span></span>, <span class="hljs-number"><span class="hljs-number">111</span></span>, <span class="hljs-number"><span class="hljs-number">112</span></span>, <span class="hljs-number"><span class="hljs-number">113</span></span>, <span class="hljs-number"><span class="hljs-number">114</span></span>, <span class="hljs-number"><span class="hljs-number">115</span></span>, <span class="hljs-number"><span class="hljs-number">116</span></span>, <span class="hljs-number"><span class="hljs-number">117</span></span>, <span class="hljs-number"><span class="hljs-number">118</span></span>, <span class="hljs-number"><span class="hljs-number">119</span></span>, <span class="hljs-number"><span class="hljs-number">120</span></span>, <span class="hljs-number"><span class="hljs-number">121</span></span>, <span class="hljs-number"><span class="hljs-number">122</span></span>]</code> </pre> <br>  Una red neuronal, en consecuencia, tiene 62 salidas, en la entrada recibir√° im√°genes de 28x28, despu√©s del reconocimiento "1" estar√° en la salida de red correspondiente. <br><br>  Crea un modelo de red. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optimizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM, BatchNormalization <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD, RMSprop, Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.constraints <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> maxnorm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">emnist_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Convolution2D(filters=<span class="hljs-number"><span class="hljs-number">32</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'valid'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Convolution2D(filters=<span class="hljs-number"><span class="hljs-number">64</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(len(emnist_labels), activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adadelta'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br>  Como puede ver, esta es una red convolucional cl√°sica que resalta ciertas caracter√≠sticas de la imagen (el n√∫mero de filtros 32 y 64), a cuya "salida" est√° conectada la red MLP "lineal", que forma el resultado final. <br><br><h2>  Entrenamiento de redes neuronales </h2><br>  Pasamos a la etapa m√°s larga: capacitaci√≥n en red.  Para hacer esto, tomamos la base de datos EMNIST, que se puede descargar <a href="">desde el enlace</a> (tama√±o de archivo 536Mb). <br><br>  Para leer la base de datos, use la biblioteca idx2numpy.  Prepararemos datos para capacitaci√≥n y validaci√≥n. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> idx2numpy emnist_path = <span class="hljs-string"><span class="hljs-string">'/home/Documents/TestApps/keras/emnist/'</span></span> X_train = idx2numpy.convert_from_file(emnist_path + <span class="hljs-string"><span class="hljs-string">'emnist-byclass-train-images-idx3-ubyte'</span></span>) y_train = idx2numpy.convert_from_file(emnist_path + <span class="hljs-string"><span class="hljs-string">'emnist-byclass-train-labels-idx1-ubyte'</span></span>) X_test = idx2numpy.convert_from_file(emnist_path + <span class="hljs-string"><span class="hljs-string">'emnist-byclass-test-images-idx3-ubyte'</span></span>) y_test = idx2numpy.convert_from_file(emnist_path + <span class="hljs-string"><span class="hljs-string">'emnist-byclass-test-labels-idx1-ubyte'</span></span>) X_train = np.reshape(X_train, (X_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) X_test = np.reshape(X_test, (X_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(emnist_labels)) k = <span class="hljs-number"><span class="hljs-number">10</span></span> X_train = X_train[:X_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] // k] y_train = y_train[:y_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] // k] X_test = X_test[:X_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] // k] y_test = y_test[:y_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] // k] <span class="hljs-comment"><span class="hljs-comment"># Normalize X_train = X_train.astype(np.float32) X_train /= 255.0 X_test = X_test.astype(np.float32) X_test /= 255.0 x_train_cat = keras.utils.to_categorical(y_train, len(emnist_labels)) y_test_cat = keras.utils.to_categorical(y_test, len(emnist_labels))</span></span></code> </pre><br>  Hemos preparado dos juegos para entrenamiento y validaci√≥n.  Los caracteres en s√≠ mismos son matrices comunes que son f√°ciles de mostrar: <br><br><img src="https://habrastorage.org/webt/lb/uj/yt/lbujytoizk2gxviahqxz5emvgay.png"><br><br>  Tambi√©n usamos solo 1/10 del conjunto de datos para el entrenamiento (par√°metro k), de lo contrario, el proceso tomar√° al menos 10 horas. <br><br>  Comenzamos el entrenamiento en red, al final del proceso guardamos el modelo entrenado en el disco. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Set a learning rate reduction learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) # Required for learning_rate_reduction: keras.backend.get_session().run(tf.global_variables_initializer()) model.fit(X_train, x_train_cat, validation_data=(X_test, y_test_cat), callbacks=[learning_rate_reduction], batch_size=64, epochs=30) model.save('emnist_letters.h5')</span></span></code> </pre> <br>  El proceso de aprendizaje en s√≠ toma aproximadamente media hora: <br><br><img src="https://habrastorage.org/webt/vu/xv/_s/vuxv_s6hsxg1q0gxcqapd0o3bv8.png"><br><br>  Esto debe hacerse solo una vez, luego usaremos el archivo de modelo ya guardado.  Cuando finaliza el entrenamiento, todo est√° listo, puede reconocer el texto. <br><br><h2>  Reconocimiento </h2><br>  Para el reconocimiento, cargamos el modelo y llamamos a la funci√≥n predict_classes. <br><br><pre> <code class="python hljs">model = keras.models.load_model(<span class="hljs-string"><span class="hljs-string">'emnist_letters.h5'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">emnist_predict_img</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, img)</span></span></span><span class="hljs-function">:</span></span> img_arr = np.expand_dims(img, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) img_arr = <span class="hljs-number"><span class="hljs-number">1</span></span> - img_arr/<span class="hljs-number"><span class="hljs-number">255.0</span></span> img_arr[<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.rot90(img_arr[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">3</span></span>) img_arr[<span class="hljs-number"><span class="hljs-number">0</span></span>] = np.fliplr(img_arr[<span class="hljs-number"><span class="hljs-number">0</span></span>]) img_arr = img_arr.reshape((<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) result = model.predict_classes([img_arr]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> chr(emnist_labels[result[<span class="hljs-number"><span class="hljs-number">0</span></span>]])</code> </pre> <br>  Al final result√≥ que, las im√°genes en el conjunto de datos se rotaron inicialmente, por lo que tenemos que rotar la imagen antes del reconocimiento. <br><br>  La funci√≥n final, que recibe un archivo con una imagen en la entrada y da una l√≠nea en la salida, ocupa solo 10 l√≠neas de c√≥digo: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">img_to_str</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model: Any, image_file: str)</span></span></span><span class="hljs-function">:</span></span> letters = letters_extract(image_file) s_out = <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(letters)): dn = letters[i+<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] - letters[i][<span class="hljs-number"><span class="hljs-number">0</span></span>] - letters[i][<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i &lt; len(letters) - <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> s_out += emnist_predict_img(model, letters[i][<span class="hljs-number"><span class="hljs-number">2</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (dn &gt; letters[i][<span class="hljs-number"><span class="hljs-number">1</span></span>]/<span class="hljs-number"><span class="hljs-number">4</span></span>): s_out += <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> s_out</code> </pre> <br>  Aqu√≠ usamos el ancho de caracteres previamente guardado para agregar espacios si el espacio entre letras es m√°s de 1/4 del car√°cter. <br><br>  Ejemplo de uso: <br><pre> <code class="python hljs">model = keras.models.load_model(<span class="hljs-string"><span class="hljs-string">'emnist_letters.h5'</span></span>) s_out = img_to_str(model, <span class="hljs-string"><span class="hljs-string">"hello_world.png"</span></span>) print(s_out)</code> </pre><br>  Resultado: <br><img src="https://habrastorage.org/webt/ck/r5/iq/ckr5iqlfoiokza60cleu3w1x-hg.png"><br><br>  Una caracter√≠stica divertida es que la red neuronal "confundi√≥" la letra "O" y el n√∫mero "0", lo cual, sin embargo, no es sorprendente ya que  El conjunto original de EMNIST contiene letras y n√∫meros <i>escritos a mano</i> que no son exactamente como los impresos.  Idealmente, para reconocer textos de pantalla, debe preparar un conjunto separado basado en las fuentes de pantalla, y ya entrenar una red neuronal en √©l. <br><br><h2>  Conclusi√≥n </h2><br>  Como puede ver, no son los dioses los que queman las ollas, y lo que una vez pareci√≥ ser "m√°gico" con la ayuda de las bibliotecas modernas es bastante simple. <br><br>  Dado que Python es multiplataforma, el c√≥digo funcionar√° en todas partes, en Windows, Linux y OSX.  Al igual que Keras est√° portado a iOS / Android, te√≥ricamente, el modelo entrenado tambi√©n se puede usar en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">dispositivos m√≥viles</a> . <br><br>  Para aquellos que quieran experimentar por su cuenta, el c√≥digo fuente est√° bajo el spoiler. <br><br><div class="spoiler">  <b class="spoiler_title">keras_emnist.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Code source: dmitryelj@gmail.com import os # Force CPU # os.environ["CUDA_VISIBLE_DEVICES"] = "-1" # Debug messages # 0 = all messages are logged (default behavior) # 1 = INFO messages are not printed # 2 = INFO and WARNING messages are not printed # 3 = INFO, WARNING, and ERROR messages are not printed os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' import cv2 import imghdr import numpy as np import pathlib from tensorflow import keras from keras.models import Sequential from keras import optimizers from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM, BatchNormalization from keras.optimizers import SGD, RMSprop, Adam from keras import backend as K from keras.constraints import maxnorm import tensorflow as tf from scipy import io as spio import idx2numpy # sudo pip3 install idx2numpy from matplotlib import pyplot as plt from typing import * import time # Dataset: # https://www.nist.gov/node/1298471/emnist-dataset # https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip def cnn_print_digit(d): print(d.shape) for x in range(28): s = "" for y in range(28): s += "{0:.1f} ".format(d[28*y + x]) print(s) def cnn_print_digit_2d(d): print(d.shape) for y in range(d.shape[0]): s = "" for x in range(d.shape[1]): s += "{0:.1f} ".format(d[x][y]) print(s) emnist_labels = [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122] def emnist_model(): model = Sequential() model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='valid', input_shape=(28, 28, 1), activation='relu')) model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(len(emnist_labels), activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) return model def emnist_model2(): model = Sequential() # In Keras there are two options for padding: same or valid. Same means we pad with the number on the edge and valid means no padding. model.add(Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(28, 28, 1))) model.add(MaxPooling2D((2, 2))) model.add(Convolution2D(64, (3, 3), activation='relu', padding='same')) model.add(MaxPooling2D((2, 2))) model.add(Convolution2D(128, (3, 3), activation='relu', padding='same')) model.add(MaxPooling2D((2, 2))) # model.add(Conv2D(128, (3, 3), activation='relu', padding='same')) # model.add(MaxPooling2D((2, 2))) ## model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(len(emnist_labels), activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']) return model def emnist_model3(): model = Sequential() model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', input_shape=(28, 28, 1), activation='relu')) model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')) model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512, activation="relu")) model.add(Dropout(0.5)) model.add(Dense(len(emnist_labels), activation="softmax")) model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0), metrics=['accuracy']) return model def emnist_train(model): t_start = time.time() emnist_path = 'D:\\Temp\\1\\' X_train = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-train-images-idx3-ubyte') y_train = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-train-labels-idx1-ubyte') X_test = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-test-images-idx3-ubyte') y_test = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-test-labels-idx1-ubyte') X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1)) X_test = np.reshape(X_test, (X_test.shape[0], 28, 28, 1)) print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(emnist_labels)) # Test: k = 10 X_train = X_train[:X_train.shape[0] // k] y_train = y_train[:y_train.shape[0] // k] X_test = X_test[:X_test.shape[0] // k] y_test = y_test[:y_test.shape[0] // k] # Normalize X_train = X_train.astype(np.float32) X_train /= 255.0 X_test = X_test.astype(np.float32) X_test /= 255.0 x_train_cat = keras.utils.to_categorical(y_train, len(emnist_labels)) y_test_cat = keras.utils.to_categorical(y_test, len(emnist_labels)) # Set a learning rate reduction learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001) # Required for learning_rate_reduction: keras.backend.get_session().run(tf.global_variables_initializer()) model.fit(X_train, x_train_cat, validation_data=(X_test, y_test_cat), callbacks=[learning_rate_reduction], batch_size=64, epochs=30) print("Training done, dT:", time.time() - t_start) def emnist_predict(model, image_file): img = keras.preprocessing.image.load_img(image_file, target_size=(28, 28), color_mode='grayscale') emnist_predict_img(model, img) def emnist_predict_img(model, img): img_arr = np.expand_dims(img, axis=0) img_arr = 1 - img_arr/255.0 img_arr[0] = np.rot90(img_arr[0], 3) img_arr[0] = np.fliplr(img_arr[0]) img_arr = img_arr.reshape((1, 28, 28, 1)) result = model.predict_classes([img_arr]) return chr(emnist_labels[result[0]]) def letters_extract(image_file: str, out_size=28): img = cv2.imread(image_file) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY) img_erode = cv2.erode(thresh, np.ones((3, 3), np.uint8), iterations=1) # Get contours contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) output = img.copy() letters = [] for idx, contour in enumerate(contours): (x, y, w, h) = cv2.boundingRect(contour) # print("R", idx, x, y, w, h, cv2.contourArea(contour), hierarchy[0][idx]) # hierarchy[i][0]: the index of the next contour of the same level # hierarchy[i][1]: the index of the previous contour of the same level # hierarchy[i][2]: the index of the first child # hierarchy[i][3]: the index of the parent if hierarchy[0][idx][3] == 0: cv2.rectangle(output, (x, y), (x + w, y + h), (70, 0, 0), 1) letter_crop = gray[y:y + h, x:x + w] # print(letter_crop.shape) # Resize letter canvas to square size_max = max(w, h) letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8) if w &gt; h: # Enlarge image top-bottom # ------ # ====== # ------ y_pos = size_max//2 - h//2 letter_square[y_pos:y_pos + h, 0:w] = letter_crop elif w &lt; h: # Enlarge image left-right # --||-- x_pos = size_max//2 - w//2 letter_square[0:h, x_pos:x_pos + w] = letter_crop else: letter_square = letter_crop # Resize letter to 28x28 and add letter and its X-coordinate letters.append((x, w, cv2.resize(letter_square, (out_size, out_size), interpolation=cv2.INTER_AREA))) # Sort array in place by X-coordinate letters.sort(key=lambda x: x[0], reverse=False) # cv2.imshow("Input", img) # # cv2.imshow("Gray", thresh) # cv2.imshow("Enlarged", img_erode) # cv2.imshow("Output", output) # cv2.imshow("0", letters[0][2]) # cv2.imshow("1", letters[1][2]) # cv2.imshow("2", letters[2][2]) # cv2.imshow("3", letters[3][2]) # cv2.imshow("4", letters[4][2]) # cv2.waitKey(0) return letters def img_to_str(model: Any, image_file: str): letters = letters_extract(image_file) s_out = "" for i in range(len(letters)): dn = letters[i+1][0] - letters[i][0] - letters[i][1] if i &lt; len(letters) - 1 else 0 s_out += emnist_predict_img(model, letters[i][2]) if (dn &gt; letters[i][1]/4): s_out += ' ' return s_out if __name__ == "__main__": # model = emnist_model() # emnist_train(model) # model.save('emnist_letters.h5') model = keras.models.load_model('emnist_letters.h5') s_out = img_to_str(model, "hello_world.png") print(s_out)</span></span></code> </pre><br></div></div><br>  Como de costumbre, todos los experimentos exitosos. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/466565/">https://habr.com/ru/post/466565/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../466555/index.html">6 lecciones aprendidas al encontrar una soluci√≥n a un problema masivo en gitlab.com. Parte 1</a></li>
<li><a href="../466557/index.html">C√≥mo crear un dise√±o para el sitio y no permanecer extremo</a></li>
<li><a href="../466559/index.html">Let es el nuevo Var</a></li>
<li><a href="../466561/index.html">¬øNecesita opciones absolutamente transparentes? - los tengo</a></li>
<li><a href="../466563/index.html">KOST: lo que se incluye en la nueva pila de tecnolog√≠a para desarrollar aplicaciones en la nube</a></li>
<li><a href="../466567/index.html">Kit de herramientas para el proveedor: seminarios web tem√°ticos sobre sistemas para trabajar con tr√°fico y su configuraci√≥n</a></li>
<li><a href="../466569/index.html">OPI en la Bolsa de Mosc√∫: por qu√© es necesaria, qui√©n la realiza y c√≥mo comprar acciones</a></li>
<li><a href="../466571/index.html">Consejos de OCR de Tesseract: cree su propio vocabulario para mejorar el rendimiento de OCR</a></li>
<li><a href="../466573/index.html">Preguntas al futuro empleador</a></li>
<li><a href="../466575/index.html">Pasar listas bidimensionales de Python a DLL</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>