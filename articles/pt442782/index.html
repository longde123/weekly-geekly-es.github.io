<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôéüèø üë®üèª‚Äçüåæ üôéüèø VShard - escala horizontal em Tarantool üìí üíµ üë≤üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√°, meu nome √© Vladislav e sou membro da equipe de desenvolvimento do Tarantool . Tarantool √© um DBMS e um servidor de aplicativos, tudo em um. Hoje ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - escala horizontal em Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442782/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Ol√°, meu nome √© Vladislav e sou membro da equipe de desenvolvimento do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarantool</a> .  Tarantool √© um DBMS e um servidor de aplicativos, tudo em um.  Hoje vou contar a hist√≥ria de como implementamos a escala horizontal no Tarantool por meio do m√≥dulo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VShard</a> . <br><br>  Algum conhecimento b√°sico primeiro. <br><br>  Existem dois tipos de escala: horizontal e vertical.  E existem dois tipos de dimensionamento horizontal: replica√ß√£o e sharding.  A replica√ß√£o garante o dimensionamento computacional, enquanto o sharding √© usado para o dimensionamento de dados. <br><br>  O sharding tamb√©m √© subdividido em dois tipos: sharding baseado em intervalo e sharding baseado em hash. <br><br>  O sharding baseado em intervalo implica que alguma chave de shard √© computada para cada registro de cluster.  As chaves de fragmentos s√£o projetadas em uma linha reta, separada em intervalos e alocada para diferentes n√≥s f√≠sicos. <br><br>  O sharding baseado em hash √© menos complicado: uma fun√ß√£o de hash √© calculada para cada registro em um cluster;  registros com a mesma fun√ß√£o de hash s√£o alocados para o mesmo n√≥ f√≠sico. <br><br>  Vou me concentrar no dimensionamento horizontal usando sharding baseado em hash. <br><a name="habracut"></a><br><h2>  Implementa√ß√£o mais antiga </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Tarantool Shard</a> foi nosso m√≥dulo original para dimensionamento horizontal.  Utilizou sharding simples baseado em hash e calculou chaves shard por chave prim√°ria para todos os registros em um cluster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Mas, eventualmente, o Tarantool Shard se tornou incapaz de realizar novas tarefas. <br><br>  Primeiro, um de nossos eventuais requisitos tornou-se a <b>localidade</b> garantida <b>de dados relacionados logicamente</b> .  Em outras palavras, quando temos dados logicamente relacionados, sempre queremos armazen√°-los em um √∫nico n√≥ f√≠sico, independentemente da topologia do cluster e das altera√ß√µes de balanceamento.  O Tarantool Shard n√£o pode garantir isso.  Ele calculava hashes apenas com chaves prim√°rias e, portanto, o reequil√≠brio poderia causar a separa√ß√£o tempor√°ria de registros com o mesmo hash, porque as altera√ß√µes n√£o s√£o realizadas atomicamente. <br><br>  Essa falta de localidade dos dados foi o principal problema para n√≥s.  Aqui est√° um exemplo.  Digamos que exista um banco em que um cliente tenha aberto uma conta.  As informa√ß√µes sobre a conta e o cliente devem ser armazenadas fisicamente juntas para que possam ser recuperadas em uma √∫nica solicita√ß√£o ou alteradas em uma √∫nica transa√ß√£o, por exemplo, durante uma transfer√™ncia de dinheiro.  Se usarmos o sharding tradicional do Tarantool Shard, haver√° diferentes valores de fun√ß√£o de hash para contas e clientes.  Os dados podem acabar em n√≥s f√≠sicos separados.  Isso realmente complica a leitura e a transa√ß√£o com os dados de um cliente. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  No exemplo acima, os campos de identifica√ß√£o das contas e do cliente podem ser inconsistentes.  Eles s√£o conectados pelo campo customer_id da conta e pelo campo id do cliente.  O mesmo campo de ID violaria a restri√ß√£o de exclusividade da chave prim√°ria da conta.  E o Shard n√£o pode realizar sharding de nenhuma outra maneira. <br><br>  Outro problema foi <b>o compartilhamento compartilhado lento</b> , que √© o problema fundamental de todos os shards de hash.  A conclus√£o √© que, ao alterar os componentes do cluster, a fun√ß√£o shard muda porque geralmente depende do n√∫mero de n√≥s.  Portanto, quando a fun√ß√£o muda, √© necess√°rio passar por todos os registros no cluster e recalcular a fun√ß√£o.  Tamb√©m pode ser necess√°rio transferir alguns registros.  E durante a transfer√™ncia de dados, nem sabemos se o registro necess√°rio?  Na solicita√ß√£o, os dados j√° foram transferidos ou est√£o sendo transferidos no momento.  Assim, durante o novo compartilhamento, √© necess√°rio fazer solicita√ß√µes de leitura com as fun√ß√µes de shard antigas e novas.  As solicita√ß√µes s√£o tratadas duas vezes mais devagar, e isso √© inaceit√°vel. <br><br>  Outro problema com o Tarantool Shard foi a baixa disponibilidade de leituras no caso de falha do n√≥ em um conjunto de r√©plicas. <br><br><h2>  Nova solu√ß√£o </h2><br>  Criamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarantool VShard</a> para resolver os tr√™s problemas mencionados acima.  Sua principal diferen√ßa √© que seu n√≠vel de armazenamento de dados √© virtualizado, ou seja, os armazenamentos f√≠sicos hospedam os armazenamentos virtuais e os registros de dados s√£o alocados pelos virtuais.  Esses armazenamentos s√£o chamados de <i>baldes</i> .  O usu√°rio n√£o precisa se preocupar com o que est√° localizado em um determinado n√≥ f√≠sico.  Um balde √© uma unidade de dados indivis√≠vel at√¥mica, como uma tupla no sharding tradicional.  O VShard sempre armazena um dep√≥sito inteiro em um n√≥ f√≠sico e, durante o novo compartilhamento, migra todos os dados de um dep√≥sito atomicamente.  Este m√©todo garante a localiza√ß√£o dos dados.  Apenas colocamos os dados em um √∫nico balde e sempre podemos ter certeza de que eles n√£o ser√£o separados durante as altera√ß√µes do cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Como colocamos dados em um balde?  Vamos adicionar um novo campo de ID do dep√≥sito √† tabela para nosso cliente banc√°rio.  Se esse valor do campo for o mesmo para dados relacionados, todos os registros estar√£o em um intervalo.  A vantagem √© que podemos armazenar registros com o mesmo ID de bucket em espa√ßos diferentes e at√© em mecanismos diferentes.  A localidade dos dados com base no ID do bucket √© garantida, independentemente do m√©todo de armazenamento. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  Por que isso √© t√£o importante?  Ao usar o sharding tradicional, os dados estendem-se a v√°rios armazenamentos f√≠sicos existentes.  Para o nosso exemplo banc√°rio, ter√≠amos que entrar em contato com cada n√≥ ao solicitar todas as contas para um determinado cliente.  Portanto, obtemos uma complexidade de leitura O (N), onde N √© o n√∫mero de armazenamentos f√≠sicos.  √â assustadoramente lento. <br><br>  O uso de buckets e a localidade pelo ID do bucket tornam poss√≠vel ler os dados necess√°rios de um n√≥ usando uma solicita√ß√£o - independentemente do tamanho do cluster. <br><br><img src="https://habrastorage.org/webt/t7/_r/fm/t7_rfmxoroosmaoqbe8cskpsr0k.png"><br><br>  No VShard, voc√™ calcula seu ID de bucket e o atribui.  Para algumas pessoas, isso √© uma vantagem, enquanto outros a consideram uma desvantagem.  Acredito que a capacidade de escolher sua pr√≥pria fun√ß√£o para o c√°lculo da identifica√ß√£o do bucket seja uma vantagem. <br><br>  Qual √© a principal diferen√ßa entre o sharding tradicional e o sharding virtual com buckets? <br><br>  No primeiro caso, quando alteramos os componentes do cluster, temos dois estados: o atual (antigo) e o novo a ser implementado.  No processo de transi√ß√£o, √© necess√°rio n√£o apenas migrar dados, mas tamb√©m recalcular a fun√ß√£o de hash para cada registro.  Isso n√£o √© muito conveniente, porque em um dado momento n√£o sabemos se os dados necess√°rios j√° foram migrados ou n√£o.  Al√©m disso, esse m√©todo n√£o √© confi√°vel e as altera√ß√µes n√£o s√£o at√¥micas, pois a migra√ß√£o at√¥mica do conjunto de registros com o mesmo valor de fun√ß√£o de hash exigiria um armazenamento persistente do estado de migra√ß√£o, caso a recupera√ß√£o fosse necess√°ria.  Como resultado, existem conflitos e erros, e a opera√ß√£o deve ser reiniciada v√°rias vezes. <br><br>  O sharding virtual √© muito mais simples.  N√£o temos dois estados de cluster diferentes;  n√≥s s√≥ temos estado de bucket.  O cluster √© mais flex√≠vel, move-se suavemente de um estado para outro.  Existem mais de dois estados agora?  (claro).  Com a transi√ß√£o suave, √© poss√≠vel alterar o balanceamento em tempo real ou remover os novos armazenamentos adicionados.  Ou seja, o controle de balanceamento aumentou bastante e se tornou mais granular. <br><br><h2>  Uso </h2><br>  Digamos que selecionamos uma fun√ß√£o para nosso ID de bucket e carregamos tantos dados no cluster que n√£o resta espa√ßo.  Agora, gostar√≠amos de adicionar alguns n√≥s e mover dados automaticamente para eles.  √â assim que fazemos no VShard: primeiro, come√ßamos novos n√≥s e rodamos o Tarantool l√°, depois atualizamos nossa configura√ß√£o do VShard.  Ele cont√©m informa√ß√µes sobre todos os componentes do cluster, todas as r√©plicas, conjuntos de r√©plicas, mestres, URIs atribu√≠dos e muito mais.  Agora, adicionamos nossos novos n√≥s ao arquivo de configura√ß√£o e aplicamos a todos os n√≥s do cluster usando VShard.storage.cfg. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Como voc√™ deve se lembrar, ao alterar o n√∫mero de n√≥s no sharding tradicional, a pr√≥pria fun√ß√£o shard tamb√©m muda.  Isso n√£o acontece no VShard.  Aqui temos um n√∫mero fixo de armazenamentos virtuais, ou buckets.  Essa √© uma constante que voc√™ escolhe ao iniciar o cluster.  Pode parecer que a escalabilidade esteja restrita, mas na verdade n√£o √©.  Voc√™ pode especificar um grande n√∫mero de baldes, dezenas e centenas de milhares.  O importante √© saber que deve haver pelo menos duas ordens de magnitude a mais do que o n√∫mero m√°ximo de conjuntos de r√©plicas que voc√™ jamais ter√° no cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  Como o n√∫mero de armazenamentos virtuais n√£o muda e a fun√ß√£o shard depende apenas desse valor, podemos adicionar quantos armazenamentos f√≠sicos desejarmos sem recalcular a fun√ß√£o shard. <br><br>  Ent√£o, como os baldes s√£o alocados para armazenamentos f√≠sicos?  Se VShard.storage.cfg for chamado, um processo de rebalanceador ser√° ativado em um dos n√≥s.  Este √© um processo anal√≠tico que calcula o equil√≠brio perfeito para o cluster.  O processo vai para cada n√≥ f√≠sico e recupera seu n√∫mero de buckets e cria rotas de seus movimentos para equilibrar a aloca√ß√£o.  Em seguida, o rebalanceador envia as rotas para os armazenamentos sobrecarregados, que por sua vez come√ßam a enviar baldes.  Um pouco mais tarde, o cluster est√° equilibrado. <br><br>  Em projetos do mundo real, um equil√≠brio perfeito pode n√£o ser alcan√ßado com tanta facilidade.  Por exemplo, um conjunto de r√©plicas pode conter menos dados que o outro porque possui menos capacidade de armazenamento.  Nesse caso, o VShard pode pensar que tudo est√° equilibrado, mas na verdade o primeiro armazenamento est√° prestes a sobrecarregar.  Para combater isso, fornecemos um mecanismo para corrigir as regras de balanceamento por meio de pesos.  Um peso pode ser atribu√≠do a qualquer conjunto de r√©plicas ou armazenamento.  Quando o rebalanceador decide quantos baldes devem ser enviados e para onde, considera os <b>relacionamentos</b> de todos os pares de pesos. <br><br>  Por exemplo, se um armazenamento pesa 100 e o outro 200, o segundo armazenar√° duas vezes mais baldes do que o primeiro.  Observe que estou falando especificamente sobre <b>relacionamentos de</b> peso.  Valores absolutos n√£o t√™m nenhuma influ√™ncia.  Voc√™ escolhe pesos com base na distribui√ß√£o de 100% em um cluster: portanto, 30% para um armazenamento renderia 70% para o outro.  Voc√™ pode considerar a capacidade de armazenamento em gigabytes ou medir o peso no n√∫mero de buckets.  O mais importante √© manter a propor√ß√£o necess√°ria. <br><br><img src="https://habrastorage.org/webt/sz/0v/gi/sz0vgicyunfvpamx3ic8enwsl58.png"><br><br>  Esse m√©todo tem um efeito colateral interessante: se um peso zero for atribu√≠do a um armazenamento, o rebalanceador far√° esse armazenamento redistribuir todos os seus buckets.  Depois disso, voc√™ pode remover todo o conjunto de r√©plicas da configura√ß√£o. <br><br><h2>  Migra√ß√£o de bucket at√¥mica </h2><br>  N√≥s temos um balde;  ele aceita algumas leituras e grava√ß√µes e, em um determinado momento, o rebalanceador solicita sua migra√ß√£o para outro armazenamento.  O bucket deixa de aceitar solicita√ß√µes de grava√ß√£o, caso contr√°rio, ele seria atualizado durante a migra√ß√£o, depois atualizado novamente durante a migra√ß√£o da atualiza√ß√£o, a atualiza√ß√£o seria atualizada e assim por diante.  Portanto, as solicita√ß√µes de grava√ß√£o s√£o bloqueadas, mas a leitura do dep√≥sito ainda √© poss√≠vel.  Os dados agora est√£o sendo migrados para o novo local.  Quando a migra√ß√£o √© conclu√≠da, o bucket come√ßa a aceitar solicita√ß√µes novamente.  Ainda existe no local antigo, mas est√° marcado como lixo e, posteriormente, o coletor de lixo o exclui pe√ßa por pe√ßa. <br><br>  Existem alguns metadados armazenados fisicamente no disco associado a cada bloco.  Todas as etapas descritas acima s√£o armazenadas no disco e, n√£o importa o que aconte√ßa com o armazenamento, o estado do bucket ser√° restaurado automaticamente. <br><br>  Voc√™ pode ter algumas das seguintes perguntas: <br><br><ul><li>  <b>O que acontece com as solicita√ß√µes que est√£o trabalhando com o bucket quando a migra√ß√£o √© iniciada?</b> <br><br>  Existem dois tipos de refer√™ncias nos metadados de cada bloco: RO e RW.  Quando um usu√°rio faz uma solicita√ß√£o para um bucket, ele indica se o trabalho deve estar no modo somente leitura ou no modo leitura / grava√ß√£o.  Para cada solicita√ß√£o, o contador de refer√™ncia correspondente √© aumentado. <br><br>  Por que precisamos de contadores de refer√™ncia para solicita√ß√µes de grava√ß√£o?  Digamos que um balde esteja sendo migrado e, de repente, o coletor de lixo deseja exclu√≠-lo.  O coletor de lixo reconhece que o contador de refer√™ncia est√° acima de zero e, portanto, o balde n√£o ser√° exclu√≠do.  Quando todas as solicita√ß√µes s√£o conclu√≠das, o coletor de lixo pode fazer seu trabalho. <br><br>  O contador de refer√™ncia para grava√ß√µes tamb√©m garante que a migra√ß√£o do bucket n√£o seja iniciada se houver pelo menos uma solicita√ß√£o de grava√ß√£o em processo.  Por√©m, novamente, as solicita√ß√µes de grava√ß√£o podem ser recebidas uma ap√≥s a outra, e o bucket nunca ser√° migrado.  Portanto, se o rebalanceador desejar mover o bucket, o sistema bloquear√° novas solicita√ß√µes de grava√ß√£o enquanto aguarda a conclus√£o das solicita√ß√µes atuais durante um determinado per√≠odo de tempo limite.  Se as solicita√ß√µes n√£o forem conclu√≠das dentro do tempo limite especificado, o sistema come√ßar√° a aceitar novas solicita√ß√µes de grava√ß√£o novamente ao adiar a migra√ß√£o do bucket.  Dessa forma, o rebalanceador tentar√° migrar o bucket at√© que a migra√ß√£o seja bem-sucedida. <br><br>  O VShard possui uma API bucket_ref de baixo n√≠vel, caso voc√™ precise mais do que apenas recursos de alto n√≠vel.  Se voc√™ realmente deseja fazer alguma coisa, consulte esta API. </li><li>  <b>√â poss√≠vel deixar os registros desbloqueados?</b> <br><br>  N√£o.  Se o bucket contiver dados cr√≠ticos e exigir acesso permanente de grava√ß√£o, voc√™ dever√° bloquear completamente sua migra√ß√£o.  Temos uma fun√ß√£o bucket_pin para fazer exatamente isso.  Ele fixa o balde ao conjunto de r√©plicas atual para que o rebalanceador n√£o possa migrar o balde.  Nesse caso, os buckets adjacentes poder√£o se mover sem restri√ß√µes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Um bloqueio de conjunto de r√©plicas √© uma ferramenta ainda mais forte que o bucket_pin.  Isso n√£o √© mais feito no c√≥digo, mas na configura√ß√£o.  Um bloqueio do conjunto de r√©plicas desativa a migra√ß√£o de qualquer intervalo de entrada / sa√≠da do conjunto de r√©plicas.  Portanto, todos os dados estar√£o permanentemente dispon√≠veis para grava√ß√µes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h2>  VShard.router </h2><br>  O VShard consiste em dois subm√≥dulos: VShard.storage e VShard.router.  Podemos criar e dimensionar esses itens independentemente em uma √∫nica inst√¢ncia.  Ao solicitar um cluster, n√£o sabemos onde um determinado dep√≥sito est√° localizado, e o VShard.router pesquisar√° por ID do dep√≥sito. <br><br>  Vamos olhar para o nosso exemplo, o cluster banc√°rio com contas de clientes.  Gostaria de poder obter todas as contas de um determinado cliente do cluster.  Isso requer uma fun√ß√£o padr√£o para pesquisa local: <br><br><img src="https://habrastorage.org/webt/q4/om/pp/q4omppscsww5c-cshatc6bnvgky.png"><br><br>  Ele procura todas as contas do cliente por seu ID.  Agora tenho que decidir onde devo executar a fun√ß√£o.  Para esse prop√≥sito, calculo o ID do bucket pelo identificador do cliente em minha solicita√ß√£o e solicito ao VShard.router para chamar a fun√ß√£o no armazenamento em que o bucket com o ID do bucket de destino est√° localizado.  O subm√≥dulo possui uma tabela de roteamento que descreve os locais dos buckets nos conjuntos de r√©plicas.  O VShard.router redireciona minha solicita√ß√£o. <br><br>  Certamente pode acontecer que o sharding comece nesse exato momento e que os baldes comecem a se mover.  O roteador em segundo plano atualiza gradualmente a tabela em grandes partes solicitando as tabelas de bucket atuais dos armazenamentos. <br><br>  Podemos at√© solicitar um bucket migrado recentemente, pelo qual o roteador ainda n√£o atualizou sua tabela de roteamento.  Nesse caso, ele solicitar√° o armazenamento antigo, que redirecionar√° o roteador para outro armazenamento ou responder√° simplesmente que n√£o possui os dados necess√°rios.  Em seguida, o roteador passar√° por cada armazenamento em busca do balde necess√°rio.  E nem perceberemos um erro na tabela de roteamento. <br><br><h2>  Ler failover </h2><br>  Vamos relembrar nossos problemas iniciais: <br><br><ul><li>  Sem localidade de dados.  Resolvido por meio de baldes. </li><li>  Processo de re-compartilhamento atolando e retendo tudo.  Implementamos a transfer√™ncia de dados at√¥micos por meio de baldes e nos livramos do rec√°lculo da fun√ß√£o shard. </li><li>  Leia o failover. </li></ul><br>  O √∫ltimo problema foi solucionado pelo VShard.router, suportado pelo subsistema de failover de leitura autom√°tica. <br><br>  De tempos em tempos, o roteador faz ping nos armazenamentos especificados na configura√ß√£o.  Digamos, por exemplo, o roteador n√£o pode executar ping em um deles.  O roteador tem uma conex√£o de backup quente para cada r√©plica; portanto, se a r√©plica atual n√£o estiver respondendo, ela apenas alterna para outra.  As solicita√ß√µes de leitura ser√£o processadas normalmente porque podemos ler as r√©plicas (mas n√£o gravar).  E podemos especificar a prioridade das r√©plicas como um fator para o roteador escolher o failover para leituras.  Isso √© feito por meio de zoneamento. <br><br><img src="https://habrastorage.org/webt/aw/iz/ry/awizryylhzk9h2rct_kxo1-jvpc.png"><br><br>  Atribu√≠mos um n√∫mero de zona a cada r√©plica e a cada roteador e especificamos uma tabela na qual indicamos a dist√¢ncia entre cada par de zonas.  Quando o roteador decide para onde deve enviar uma solicita√ß√£o de leitura, ele seleciona uma r√©plica na zona mais pr√≥xima. <br><br>  Isto √© o que parece na configura√ß√£o: <br><br><img src="https://habrastorage.org/webt/2w/jx/cu/2wjxcuidtcghobukd3mxu02ms2y.png"><br><br>  Geralmente, voc√™ pode solicitar qualquer r√©plica, mas se o cluster for grande, complexo e altamente distribu√≠do, o zoneamento poder√° ser muito √∫til.  Racks de servidor diferentes podem ser selecionados como zonas para que a rede n√£o seja sobrecarregada pelo tr√°fego.  Como alternativa, pontos geograficamente isolados podem ser selecionados. <br><br>  O zoneamento tamb√©m ajuda quando as r√©plicas demonstram comportamentos diferentes.  Por exemplo, cada conjunto de r√©plicas possui uma r√©plica de backup que n√£o deve aceitar solicita√ß√µes, mas deve armazenar apenas uma c√≥pia dos dados.  Nesse caso, a colocamos em uma zona distante de todos os roteadores da tabela, para que o roteador n√£o endere√ßa essa r√©plica, a menos que seja absolutamente necess√°rio. <br><br><h2>  Failover de grava√ß√£o </h2><br>  J√° falamos sobre failover de leitura.  E o failover de grava√ß√£o ao alterar o mestre?  No VShard, a imagem n√£o √© t√£o otimista quanto era antes: a sele√ß√£o principal n√£o √© implementada, portanto teremos que fazer isso sozinhos.  Quando de alguma forma designamos um mestre, a inst√¢ncia designada deve agora assumir o controle de mestre.  Em seguida, atualizamos a configura√ß√£o especificando master = false para o antigo mestre e master = true para o novo, aplicamos a configura√ß√£o por meio do VShard.storage.cfg e as compartilhamos com cada armazenamento.  Tudo o resto √© feito automaticamente.  O mestre antigo para de aceitar solicita√ß√µes de grava√ß√£o e inicia a sincroniza√ß√£o com o novo, porque pode haver dados que j√° foram aplicados no mestre antigo, mas n√£o no novo.  Depois disso, o novo mestre √© respons√°vel e come√ßa a aceitar solicita√ß√µes, e o antigo mestre √© uma r√©plica.  √â assim que o failover de grava√ß√£o funciona no VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><br><h2>  Como rastreamos esses v√°rios eventos? </h2><br>  VShard.storage.info e VShard.router.info s√£o suficientes. <br><br>  VShard.storage.info exibe informa√ß√µes em v√°rias se√ß√µes. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  A primeira se√ß√£o √© para replica√ß√£o.  Aqui voc√™ pode ver o status do conjunto de r√©plicas em que a fun√ß√£o est√° sendo chamada: seu atraso de replica√ß√£o, suas conex√µes dispon√≠veis e indispon√≠veis, sua configura√ß√£o principal etc. <br><br>  Na se√ß√£o bucket, voc√™ pode ver em tempo real o n√∫mero de buckets que est√£o sendo migrados para / do conjunto de r√©plicas atual, o n√∫mero de buckets trabalhando no modo regular, o n√∫mero de buckets marcados como lixo e o n√∫mero de buckets fixados. <br><br>  A se√ß√£o Alertas exibe os problemas que o VShard conseguiu determinar: "o mestre n√£o est√° configurado", "h√° um n√≠vel de redund√¢ncia insuficiente", "o mestre est√° l√°, mas todas as r√©plicas falharam" etc. <br><br>  E a √∫ltima se√ß√£o (q: √© esse "status"?) √â uma luz que fica vermelha quando tudo d√° errado.  √â um n√∫mero de zero a tr√™s, pelo qual um n√∫mero maior √© pior. <br><br>  VShard.router.info possui as mesmas se√ß√µes, mas seu significado √© um pouco diferente. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  A primeira se√ß√£o √© para replica√ß√£o, embora n√£o contenha informa√ß√µes sobre atrasos na replica√ß√£o, mas informa√ß√µes sobre disponibilidade: conex√µes do roteador a um conjunto de r√©plicas;  conex√£o quente e conex√£o de backup caso o mestre falhe;  o mestre selecionado;  e o n√∫mero de ca√ßambas RW e RO dispon√≠veis em cada conjunto de r√©plicas. <br><br>  A se√ß√£o bucket exibe o n√∫mero total de buckets de leitura e grava√ß√£o e somente leitura atualmente dispon√≠veis para este roteador;  o n√∫mero de baldes com um local desconhecido;  e o n√∫mero de buckets com um local conhecido, mas sem uma conex√£o com o conjunto de r√©plicas necess√°rio. <br><br>  A se√ß√£o de alertas descreve principalmente conex√µes, eventos de failover e buckets n√£o identificados. <br><br>  Finalmente, h√° tamb√©m o status simples?  Indicador de zero a tr√™s. <br><br><h2>  O que voc√™ precisa para usar o VShard? </h2><br>  Primeiro, voc√™ deve selecionar um n√∫mero constante de buckets.  Por que n√£o apenas configur√°-lo para int32_max?  Como os metadados s√£o armazenados junto com cada intervalo, 30 bytes no armazenamento e 16 bytes no roteador.  Quanto mais baldes voc√™ tiver, mais espa√ßo ser√° ocupado pelos metadados.  Mas, ao mesmo tempo, o tamanho do bucket ser√° menor, o que significa maior granularidade do cluster e maior velocidade de migra√ß√£o por bucket.  Portanto, voc√™ deve escolher o que √© mais importante para voc√™ e o n√≠vel de escalabilidade necess√°rio. <br><br>  Segundo, voc√™ precisa selecionar uma fun√ß√£o de fragmento para calcular a identifica√ß√£o do intervalo.  As regras s√£o as mesmas que ao selecionar uma fun√ß√£o de fragmento no sharding tradicional, j√° que um bucket aqui √© igual ao n√∫mero fixo de armazenamentos no sharding tradicional.  A fun√ß√£o deve distribuir uniformemente os valores de sa√≠da, caso contr√°rio, o crescimento do tamanho do balde n√£o ser√° equilibrado, e o VShard opera apenas com o n√∫mero de baldes.  Se voc√™ n√£o equilibrar sua fun√ß√£o de shard, ser√° necess√°rio migrar os dados de um bucket para outro e alterar a fun√ß√£o de shard.  Assim, voc√™ deve escolher com cuidado. <br><br><h2>  Sum√°rio </h2><br>  O VShard garante: <br><br><ul><li>  localidade dos dados </li><li>  compartilhamento at√¥mico </li><li>  maior flexibilidade de cluster </li><li>  failover de leitura autom√°tica </li><li>  v√°rios controladores de balde. </li></ul><br>  VShard est√° em desenvolvimento ativo.  Algumas tarefas planejadas j√° est√£o sendo implementadas.  A primeira tarefa √© <b>o balanceamento de carga do roteador</b> .  Se houver solicita√ß√µes de leitura intensas, nem sempre √© recomend√°vel endere√ß√°-las ao mestre.  O roteador deve equilibrar solicita√ß√µes para diferentes r√©plicas de leitura por conta pr√≥pria. <br><br>  A segunda tarefa √© a <b>migra√ß√£o de bucket sem bloqueio</b> .  Um algoritmo j√° foi implementado que ajuda a manter os buckets desbloqueados, mesmo durante a migra√ß√£o.  O bucket ser√° bloqueado apenas no final para documentar a pr√≥pria migra√ß√£o. <br><br>  A terceira tarefa √© <b>a aplica√ß√£o at√¥mica da configura√ß√£o</b> .  N√£o √© conveniente ou at√¥mico aplicar a configura√ß√£o separadamente, pois algum armazenamento pode estar indispon√≠vel e, se a configura√ß√£o n√£o for aplicada, o que faremos a seguir?  √â por isso que estamos trabalhando em um mecanismo para transfer√™ncia autom√°tica de configura√ß√£o. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt442782/">https://habr.com/ru/post/pt442782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt442770/index.html">Vis√£o geral dos scanners de c√≥digo de barras JavaScript</a></li>
<li><a href="../pt442772/index.html">Matem√°tica para cientista de dados: se√ß√µes necess√°rias</a></li>
<li><a href="../pt442776/index.html">√çndices no PostgreSQL - 3 (Hash)</a></li>
<li><a href="../pt442778/index.html">Learning Go: uma sele√ß√£o de relat√≥rios de v√≠deo</a></li>
<li><a href="../pt442780/index.html">Equ√≠vocos mais comuns na f√≠sica popular</a></li>
<li><a href="../pt442784/index.html">Seq√ºestro de BGP adicionando a v√≠tima AS ao AS-SET do atacante</a></li>
<li><a href="../pt442786/index.html">7 dicas √∫teis para usar o quarto</a></li>
<li><a href="../pt442788/index.html">Por que precisamos de um sistema de monitoramento em um chip</a></li>
<li><a href="../pt442790/index.html">As inscri√ß√µes est√£o abertas para o Allure Server Meetup em S√£o Petersburgo</a></li>
<li><a href="../pt442794/index.html">Convidamos voc√™ para a confer√™ncia ‚Äúarquiteto (TI) em projetos e organiza√ß√µes de TI‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>