<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê∫ üéì üôáüèΩ NLP. Die Grundlagen. Techniken. Selbstentwicklung. Teil 1 üç† üë¶üèæ üôá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inhalt   NLP. Die Grundlagen. Techniken. Selbstentwicklung. Teil 2: NER 
 
 Hallo! Mein Name ist Ivan Smurov und ich leite die NLP-Forschungsgruppe be...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NLP. Die Grundlagen. Techniken. Selbstentwicklung. Teil 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/437008/"><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/nf/p0/ws/nfp0wsz4wap5qwx33ulwimmaid8.png" alt="Bild"></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NLP.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Grundlagen.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Techniken.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Selbstentwicklung.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: NER</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a> </div></div><br>  Hallo!  Mein Name ist Ivan Smurov und ich leite die NLP-Forschungsgruppe bei ABBYY.  Lesen Sie hier, was unsere Gruppe tut.  Ich habe k√ºrzlich einen Vortrag √ºber die Verarbeitung nat√ºrlicher Sprache (NLP) an der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">School of Deep Learning gehalten</a> - dies ist eine Gruppe an der PhysTech School f√ºr Angewandte Mathematik und Informatik am MIPT f√ºr √§ltere Studenten, die sich f√ºr Programmierung und Mathematik interessieren.  Vielleicht sind die Thesen meines Vortrags f√ºr jemanden n√ºtzlich, also werde ich sie mit Habr teilen. <br><br>  Da nicht alles auf einmal erfasst werden kann, werden wir den Artikel in zwei Teile teilen.  Heute werde ich dar√ºber sprechen, wie neuronale Netze (oder Deep Learning) in NLP verwendet werden.  Im zweiten Teil des Artikels konzentrieren wir uns auf eine der h√§ufigsten NLP-Aufgaben - das Extrahieren benannter Entit√§ten (Named-Entity-Erkennung, NER) und die detaillierte Analyse der Architektur ihrer L√∂sungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cn/5o/dx/cn5odxj0tvmrhaf4jstaz7ptars.png"></div><br><a name="habracut"></a><br><h2>  Was ist NLP? <br></h2><br>  Dies ist eine breite Palette von Aufgaben zum Verarbeiten von Texten in einer nat√ºrlichen Sprache (d. H. Der Sprache, die Menschen sprechen und schreiben).  Es gibt eine Reihe klassischer NLP-Aufgaben, deren L√∂sung von praktischem Nutzen ist. <br><br><ul><li>  Die erste und historisch wichtigste Aufgabe ist die maschinelle √úbersetzung.  Es wurde sehr lange praktiziert und es gibt enorme Fortschritte.  Die Aufgabe, eine vollautomatische √úbersetzung von hoher Qualit√§t (FAHQMT) zu erhalten, bleibt jedoch ungel√∂st.  In gewisser Weise ist dies die NLP-Engine, eine der gr√∂√üten Aufgaben, die Sie ausf√ºhren k√∂nnen. <br><br><img src="https://habrastorage.org/webt/bj/r0/og/bjr0ogf9-tarmd10sntfy-c_mnq.png" alt="Bild"><br></li><li>  Die zweite Aufgabe ist die Klassifizierung von Texten.  Es wird eine Reihe von Texten angegeben, deren Aufgabe es ist, diese Texte in Kategorien einzuteilen.  Welches?  Dies ist eine Frage an das Korps. <br><br>  Die erste und eine der praktischsten M√∂glichkeiten, sie aus praktischer Sicht anzuwenden, ist die Einteilung von Buchstaben in Spam und Boor (nicht Spam). <br><br>  Eine weitere klassische Option ist die Klassifizierung von Nachrichten in mehrere Klassen in Kategorien (Rubrik) - Au√üenpolitik, Sport, Big Top usw. Oder Sie erhalten beispielsweise Briefe und m√∂chten Bestellungen aus dem Online-Shop von Flugtickets und Hotelreservierungen trennen. <br><br>  Die dritte klassische Anwendung des Textklassifizierungsproblems ist die sentimentale Analyse.  Zum Beispiel die Einstufung von Bewertungen als positiv, negativ und neutral. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/c1/co/myc1cop8u7adcvhhh0qgw-dmtse.png"></div><br>  Da es so viele m√∂gliche Kategorien gibt, in die Sie Texte unterteilen k√∂nnen, ist die Textklassifizierung eine der beliebtesten praktischen Aufgaben von NLP. </li><li>  Die dritte Aufgabe besteht darin, benannte Entit√§ten, NER, abzurufen.  Wir w√§hlen in den Textabschnitten aus, die einer vorgew√§hlten Gruppe von Entit√§ten entsprechen. Beispielsweise m√ºssen Sie alle Standorte, Personen und Organisationen im Text finden.  Im Text ‚ÄûOstap Bender - Direktor des B√ºros‚Äû H√∂rner und Hufe ‚Äú‚Äú sollten Sie verstehen, dass Ostap Bender eine Person und ‚ÄûH√∂rner und Hufe‚Äú eine Organisation ist.  Warum diese Aufgabe in der Praxis ben√∂tigt wird und wie sie gel√∂st werden kann, werden wir im zweiten Teil unseres Artikels besprechen. </li><li><img src="https://habrastorage.org/webt/op/d2/lv/opd2lvphnvm4j1e7o6s6-u34pie.png" align="right">  Die vierte Aufgabe ist mit der dritten verbunden - die Aufgabe, Fakten und Beziehungen zu extrahieren (Beziehungsextraktion).  Zum Beispiel gibt es eine Arbeitseinstellung (Beruf).  Aus dem Text ‚ÄûOstap Bender - Direktor des B√ºros‚Äû H√∂rner und Hufe ‚Äú‚Äú geht hervor, dass unser Held mit den beruflichen Beziehungen zu ‚ÄûH√∂rner und Hufe‚Äú verbunden ist.  Das Gleiche kann auf viele andere Arten gesagt werden: "Das B√ºro von Ostap Bender wird vom B√ºro" Horns and Hooves "geleitet, oder" Ostap Bender ist vom einfachen Sohn von Leutnant Schmidt zum Leiter des B√ºros "Horns and Hooves" gewechselt. "  Diese S√§tze unterscheiden sich nicht nur im Pr√§dikat, sondern auch in der Struktur. <br><br>  Beispiele f√ºr andere Beziehungen, die h√§ufig hervorgehoben werden, sind Kauf und Verkauf, Eigentum, die Tatsache der Geburt mit Attributen wie Datum, Ort usw. (Geburt) und einige andere. <br><br>  Die Aufgabe scheint keine offensichtliche praktische Anwendung zu haben, wird jedoch bei der Strukturierung unstrukturierter Informationen verwendet.  Dar√ºber hinaus ist es in Frage-Antwort- und Dialogsystemen, in Suchmaschinen wichtig - immer dann, wenn Sie eine Frage analysieren und verstehen m√ºssen, auf welchen Typ sie sich bezieht und welche Einschr√§nkungen die Antwort enth√§lt. <br><br><img src="https://habrastorage.org/webt/eh/y7/tl/ehy7tl9hbhhxlsy57j21q8xpa4w.png" alt="Bild"><br></li><li>  Die n√§chsten beiden Aufgaben sind wahrscheinlich der gr√∂√üte Hype.  Dies sind Frage-Antwort- und Dialogsysteme (Chat-Bots).  Amazon Alexa, Alice sind klassische Beispiele f√ºr Konversationssysteme.  Damit sie ordnungsgem√§√ü funktionieren, m√ºssen viele NLP-Aufgaben gel√∂st werden.  Mithilfe der Textklassifizierung k√∂nnen Sie beispielsweise feststellen, ob wir in eines der zielorientierten Chatbot-Szenarien fallen.  Angenommen, "die Frage der Wechselkurse".  Das Extrahieren von Beziehungen ist erforderlich, um Platzhalter f√ºr die Skriptvorlage zu identifizieren, und die Aufgabe, einen Dialog √ºber allgemeine Themen (‚ÄûSprecher‚Äú) zu f√ºhren, hilft uns in einer Situation, in der wir in keines der Szenarien geraten sind. <br><br>  Frage-Antwort-Systeme sind ebenfalls verst√§ndlich und n√ºtzlich.  Wenn Sie einem Auto eine Frage stellen, sucht das Auto in einer Datenbank oder einem Textk√∂rper nach einer Antwort darauf.  Beispiele f√ºr solche Systeme sind IBM Watson oder Wolfram Alpha. </li><li>  Ein weiteres Beispiel f√ºr das klassische NLP-Problem ist die Sammarisierung.  Die Erkl√§rung des Problems ist einfach: Das Eingabesystem akzeptiert gro√üen Text, und die Ausgabe ist ein kleinerer Text, der irgendwie den Inhalt eines gro√üen Textes widerspiegelt.  Beispielsweise muss eine Maschine eine Nacherz√§hlung eines Textes, seines Namens oder einer Anmerkung generieren. <br><br><img src="https://habrastorage.org/webt/m-/mj/uq/m-mjuqawoktinjvjcrkcmlnt5xi.png" alt="Bild"><br></li><li>  Eine weitere beliebte Aufgabe ist das Argumentation Mining, die Suche nach Rechtfertigung im Text.  Sie erhalten eine Tatsache und einen Text, Sie m√ºssen eine Rechtfertigung f√ºr diese Tatsache im Text finden. </li></ul><br>  Dies ist keineswegs die gesamte Liste der NLP-Aufgaben.  Es gibt Dutzende von ihnen.  Im Gro√üen und Ganzen kann alles, was mit Text in einer nat√ºrlichen Sprache getan werden kann, den Aufgaben von NLP zugeordnet werden. Nur die aufgef√ºhrten Themen sind nach Geh√∂r und sie haben die offensichtlichsten praktischen Anwendungen. <br><br><h2>  Warum ist es schwierig, NLP-Aufgaben zu l√∂sen? <br></h2><br>  Der Wortlaut der Aufgaben ist nicht sehr kompliziert, aber die Aufgaben selbst sind √ºberhaupt nicht einfach, weil wir mit der nat√ºrlichen Sprache arbeiten.  Die Ph√§nomene Polysemie (polysemische W√∂rter haben eine gemeinsame Anfangsbedeutung) und Homonymie (W√∂rter mit unterschiedlichen Bedeutungen werden gleich ausgesprochen und geschrieben) sind charakteristisch f√ºr jede nat√ºrliche Sprache.  Und wenn ein russischer Muttersprachler gut versteht, dass <i>warmer Empfang</i> wenig mit <i>Kampftechnik</i> einerseits und <i>warmem Bier</i> andererseits zu tun hat, muss das automatische System dies lange lernen.  Warum es besser ist, " <i>Leertaste dr√ºcken,</i> <i>um fortzufahren</i> " in langweiliges " <i>Dr√ºcken Sie die Leertaste,</i> <i>um fortzufahren</i> " zu √ºbersetzen <i>,</i> als "Die <i>Leertaste</i> <i>wird weiterhin funktionieren</i> ". <br><br><ul><li>  Polysemie: Stopp (Prozess oder Geb√§ude), Tisch (Organisation oder Objekt), Specht (Vogel oder Person). </li><li>  Homonymie: Schl√ºssel, Bogen, Schloss, Herd. <br><br><img src="https://habrastorage.org/webt/5i/-h/c1/5i-hc1p3hrtjf1duq5zxwajegyy.png" alt="Bild"><br></li><li>  Ein weiteres klassisches Beispiel f√ºr Sprachkomplexit√§t ist das Pronomen anaphora.  Nehmen wir zum Beispiel den Text " <i>Hausmeister zwei Stunden Schnee, er war unzufrieden</i> ."  Das Pronomen "er" kann sich sowohl auf den Hausmeister als auch auf den Schnee beziehen.  Im Zusammenhang verstehen wir leicht, dass er ein Hausmeister ist, kein Schnee.  Aber zu erreichen, dass der Computer dies auch leicht verstand, ist nicht einfach.  Das Problem des Pronomen Anaphora ist immer noch nicht sehr gut gel√∂st, und es werden weiterhin aktive Versuche unternommen, die Qualit√§t der Entscheidungen zu verbessern. </li><li>  Eine weitere zus√§tzliche Komplexit√§t ist die Ellipse.  Zum Beispiel: " <i>Petja a√ü einen gr√ºnen Apfel und Mascha a√ü einen roten</i> ."  Wir verstehen, dass Mascha einen roten Apfel gegessen hat.  Es ist jedoch nicht einfach, die Maschine dazu zu bringen, dies zu verstehen.  Jetzt wird die Aufgabe der Wiederherstellung der Ellipse in winzigen F√§llen (mehrere hundert S√§tze) gel√∂st, und bei ihnen ist die Qualit√§t der vollst√§ndigen Wiederherstellung offen gesagt schwach (in der Gr√∂√üenordnung von 0,5).  Es ist klar, dass f√ºr praktische Anwendungen eine solche Qualit√§t nicht gut ist. </li></ul><br>  √úbrigens werden dieses Jahr auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dialogkonferenz</a> Tracks sowohl zu Anaphoren als auch zu L√ºcken (eine Art Ellipse) f√ºr die russische Sprache gehalten.  F√ºr beide Aufgaben wurden F√§lle mit einem Volumen zusammengestellt, das um ein Vielfaches gr√∂√üer ist als das Volumen der derzeit vorhandenen Geb√§ude (au√üerdem ist das Volumen des Falles f√ºr L√ºcken um eine Gr√∂√üenordnung gr√∂√üer als das Volumen der F√§lle, nicht nur f√ºr Russisch, sondern im Allgemeinen f√ºr alle Sprachen).  Wenn Sie an Wettbewerben in diesen Geb√§uden teilnehmen m√∂chten, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">klicken Sie hier (mit Registrierung, jedoch ohne SMS)</a> . <br><br><h2>  Wie NLP-Aufgaben gel√∂st werden <br></h2><br>  Im Gegensatz zur Bildverarbeitung finden Sie auf NLP immer noch Artikel, die L√∂sungen beschreiben, die klassische Algorithmen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SVM</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Xgboost verwenden</a> , keine neuronalen Netze, und die Ergebnisse zeigen, die den L√∂sungen auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neuesten</a> Stand der Technik nicht zu unterlegen sind. <br><br>  Vor einigen Jahren begannen neuronale Netze jedoch, klassische Modelle zu besiegen.  Es ist wichtig anzumerken, dass f√ºr die meisten Aufgaben die auf klassischen Methoden basierenden L√∂sungen in der Regel einzigartig waren und nicht mit der L√∂sung anderer Probleme sowohl in der Architektur als auch in der Art und Weise, wie die Erfassung und Verarbeitung von Attributen erfolgt, vergleichbar waren. <br><br>  Neuronale Netzwerkarchitekturen sind jedoch viel allgemeiner.  Die Architektur des Netzwerks selbst ist h√∂chstwahrscheinlich ebenfalls unterschiedlich, aber viel kleiner, es besteht eine Tendenz zur vollst√§ndigen Universalisierung.  Mit welchen Funktionen und wie genau wir arbeiten, ist es jedoch f√ºr die meisten NLP-Aufgaben bereits fast gleich.  Nur die letzten Schichten neuronaler Netze unterscheiden sich.  Wir k√∂nnen also davon ausgehen, dass eine einzelne NLP-Pipeline gebildet wurde.  √úber die Anordnung werden wir Ihnen jetzt mehr erz√§hlen. <br><br><h2>  Pipeline nlp </h2><br>  Diese Art der Arbeit mit Zeichen ist f√ºr alle Aufgaben mehr oder weniger gleich. <br><br>  Wenn es um Sprache geht, ist die Grundeinheit, mit der wir arbeiten, das Wort.  Oder formeller ein "Token".  Wir verwenden diesen Begriff, weil nicht sehr klar ist, was 2128506 ist - ist das ein Wort oder nicht?  Die Antwort ist nicht offensichtlich.  Das Token wird normalerweise durch Leerzeichen oder Satzzeichen von anderen Token getrennt.  Und wie Sie anhand der oben beschriebenen Schwierigkeiten verstehen k√∂nnen, ist der Kontext jedes Tokens sehr wichtig.  Es gibt verschiedene Ans√§tze, aber in 95% der F√§lle ist der Kontext, der bei der Arbeit des Modells ber√ºcksichtigt wird, ein Vorschlag, der das erste Token enth√§lt. <br><br>  Viele Aufgaben werden in der Regel auf Vorschlagsebene gel√∂st.  Zum Beispiel maschinelle √úbersetzung.  Meistens √ºbersetzen wir einfach einen Satz und verwenden √ºberhaupt keinen breiteren Kontext.  Es gibt Aufgaben, bei denen dies nicht der Fall ist, beispielsweise Dialogsysteme.  Es ist wichtig, sich daran zu erinnern, wor√ºber das System zuvor gefragt wurde, damit es Fragen beantworten kann.  Das Angebot ist jedoch auch die Haupteinheit, mit der wir arbeiten. <br><br>  Daher sind die ersten beiden Schritte der Pipeline, die ausgef√ºhrt werden, um fast jede Aufgabe zu l√∂sen, Segmentierung (Teilen von Text in S√§tze) und Tokenisierung (Teilen von S√§tzen in Token, dh einzelne W√∂rter).  Dies geschieht mit einfachen Algorithmen. <br><br>  Als n√§chstes m√ºssen Sie die Eigenschaften jedes Tokens berechnen.  Dies geschieht in der Regel in zwei Schritten.  Die erste besteht darin, kontextunabh√§ngige Tokenattribute zu berechnen.  Dies ist eine Reihe von Zeichen, die in keiner Weise von anderen Worten abh√§ngen, die unser Zeichen umgeben.  Gemeinsame kontextunabh√§ngige Attribute sind: <br><br><ul><li>  Einbettungen </li><li>  symbolische Zeichen </li><li>  zus√§tzliche Funktionen, die f√ºr eine bestimmte Aufgabe oder Sprache spezifisch sind </li></ul><br>  Wir werden im Folgenden ausf√ºhrlicher √ºber Einbettungen und symbolische Zeichen sprechen (√ºber symbolische Zeichen - nicht heute, sondern im zweiten Teil unseres Artikels), aber lassen Sie uns zun√§chst m√∂gliche Beispiele f√ºr zus√§tzliche Zeichen geben. <br><br>  Eine der am h√§ufigsten verwendeten Funktionen ist der Teil der Sprache oder das POS-Tag (Teil der Sprache).  Solche Funktionen k√∂nnen wichtig sein, um viele Probleme zu l√∂sen, z. B. Parsing-Aufgaben.  F√ºr Sprachen mit komplexer Morphologie wie die russische Sprache sind auch morphologische Zeichen wichtig: In welchem ‚Äã‚ÄãFall ist das Substantiv, welche Art von Adjektiv.  Daraus k√∂nnen wir unterschiedliche Schlussfolgerungen √ºber die Struktur des Vorschlags ziehen.  Au√üerdem wird Morphologie f√ºr die Lemmatisierung (Reduktion von W√∂rtern auf Anfangsformen) ben√∂tigt, mit deren Hilfe wir die Dimension des Attributraums reduzieren k√∂nnen, und daher wird die morphologische Analyse f√ºr die meisten NLP-Probleme aktiv verwendet. <br><br>  Wenn wir ein Problem l√∂sen, bei dem die Interaktion zwischen verschiedenen Objekten wichtig ist (z. B. bei der Beziehungsextraktionsaufgabe oder beim Erstellen eines Frage-Antwort-Systems), m√ºssen wir viel √ºber die Struktur des Vorschlags wissen.  Dies erfordert eine Analyse.  In der Schule hat jeder einen Satz f√ºr ein Thema, ein Pr√§dikat, eine Addition usw. analysiert. Die syntaktische Analyse ist etwas in diesem Sinne, aber komplizierter. <br><br>  Ein weiteres Beispiel f√ºr eine zus√§tzliche Funktion ist die Position des Tokens im Text.  Wir k√∂nnen a priori wissen, dass eine Entit√§t h√§ufiger am Anfang des Textes zu finden ist oder umgekehrt am Ende. <br><br>  Alles zusammen - Einbettungen, symbolische und zus√§tzliche Zeichen - bilden einen Vektor von Token-Zeichen, der nicht vom Kontext abh√§ngt. <br><br><h2>  Kontextsensitive Funktionen </h2><br>  Kontextsensitive Token-Zeichen sind eine Reihe von Zeichen, die nicht nur Informationen √ºber das Token selbst, sondern auch √ºber seine Nachbarn enthalten.  Es gibt verschiedene M√∂glichkeiten, diese Symptome zu berechnen.  Bei klassischen Algorithmen gingen die Leute oft einfach am ‚ÄûFenster‚Äú vorbei: Sie nahmen mehrere (zum Beispiel drei) Token zum Original und mehrere Token danach und berechneten dann alle Zeichen in einem solchen Fenster.  Dieser Ansatz ist unzuverl√§ssig, da wichtige Informationen f√ºr die Analyse m√∂glicherweise in einem gr√∂√üeren Abstand als das Fenster liegen und wir m√∂glicherweise etwas √ºbersehen. <br><br>  Daher werden jetzt alle kontextsensitiven Funktionen auf Vorschlagsebene auf standardm√§√üige Weise berechnet: unter Verwendung von in zwei Richtungen wiederkehrenden neuronalen Netzen LSTM oder GRU.  Um kontextsensitive Tokenattribute aus kontextunabh√§ngigen, kontextunabh√§ngigen Attributen aller Angebotstoken zu erhalten, werden diese an das bidirektionale RNN (ein- oder mehrschichtig) gesendet.  Die Ausgabe des bidirektionalen RNN zum i-ten Zeitpunkt ist ein kontextsensitives Zeichen des i-ten Tokens, das Informationen zu beiden vorherigen Token (da diese Informationen im i-ten Wert des direkten RNN enthalten sind) und zum nachfolgenden Token enth√§lt Diese Informationen sind im entsprechenden Wert des inversen RNN enthalten. <br><br>  Au√üerdem machen wir f√ºr jede einzelne Aufgabe etwas anderes, aber die ersten Schichten - bis hin zur bidirektionalen RNN - k√∂nnen f√ºr fast jede Aufgabe verwendet werden. <br><br>  Diese Methode zum Abrufen von Features wird als NLP-Pipeline bezeichnet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mn/sz/0h/mnsz0hwhtiects6ksgru_y9hw2o.png"></div><br><br>  Es ist erw√§hnenswert, dass Forscher in den letzten 2 Jahren aktiv versucht haben, die NLP-Pipeline zu verbessern - sowohl in Bezug auf die Geschwindigkeit (z. B. Transformator - eine auf Selbstaufmerksamkeit basierende Architektur, die kein RNN enth√§lt und daher schneller lernen und anwenden kann) als auch mit Sicht der verwendeten Zeichen (jetzt verwenden sie aktiv Zeichen, die auf vorab trainierten Sprachmodellen basieren, beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ELMo</a> , oder sie verwenden die ersten Schichten des vorab trainierten Sprachmodells und trainieren sie in dem f√ºr die Aufgabe verf√ºgbaren Fall - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ULMFit</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT</a> ). <br><br><h2>  Einbettungen in Wortform <br></h2><br>  Schauen wir uns die Einbettung genauer an.  Grob gesagt ist das Einbetten eine pr√§gnante Darstellung des Kontextes eines Wortes.  Warum ist es wichtig, den Kontext eines Wortes zu kennen?  Weil wir an eine Verteilungshypothese glauben - dass W√∂rter mit √§hnlicher Bedeutung in √§hnlichen Kontexten verwendet werden. <br><br>  Versuchen wir nun, die Einbettung genau zu definieren.  Das Einbetten ist eine Abbildung von einem diskreten Vektor kategorialer Merkmale in einen kontinuierlichen Vektor mit einer vorbestimmten Dimension. <br><br>  Ein kanonisches Beispiel f√ºr das Einbetten ist das Einbetten von W√∂rtern (Einbetten in Wortform). <br><br>  Was wirkt normalerweise als diskreter Merkmalsvektor?  Ein boolescher Vektor, der allen m√∂glichen Werten einer bestimmten Kategorie entspricht (z. B. alle m√∂glichen Wortarten oder alle m√∂glichen W√∂rter aus einem begrenzten W√∂rterbuch). <br><br>  Bei Einbettungen in Wortform ist diese Kategorie normalerweise der Index des Wortes im W√∂rterbuch.  Angenommen, es gibt ein W√∂rterbuch mit einer Gr√∂√üe von 100.000.  Dementsprechend hat jedes Wort einen diskreten Merkmalsvektor - einen Booleschen Vektor der Dimension 100.000, wobei an einer Stelle (der Index des Wortes in unserem W√∂rterbuch) eins ist und der Rest Nullen sind. <br><br>  Warum m√∂chten wir unsere diskreten Merkmalsvektoren auf kontinuierliche gegebene Dimensionen abbilden?  Weil Vektoren mit einer Dimension von 100.000 nicht sehr bequem f√ºr Berechnungen zu verwenden sind, sind Vektoren von ganzen Zahlen mit den Dimensionen 100, 200 oder beispielsweise 300 viel bequemer. <br><br>  Grunds√§tzlich d√ºrfen wir nicht versuchen, einer solchen Zuordnung zus√§tzliche Einschr√§nkungen aufzuerlegen.  Da wir jedoch eine solche Zuordnung erstellen, sollten wir sicherstellen, dass die Vektoren √§hnlich aussagekr√§ftiger W√∂rter auch in gewissem Sinne nahe beieinander liegen.  Dies erfolgt unter Verwendung eines einfachen neuronalen Feed-Forward-Netzwerks. <br><br><h2>  Einbettungstraining <br></h2><br>  Wie werden Einbettungen trainiert?  Wir versuchen, das Problem der Wiederherstellung eines Wortes nach Kontext zu l√∂sen (oder umgekehrt, einen Kontext nach Wort wiederherzustellen).  Im einfachsten Fall erhalten wir den Index im W√∂rterbuch des vorherigen Wortes (den Booleschen Vektor der W√∂rterbuchdimension) als Eingabe und versuchen, den Index im W√∂rterbuch unseres Wortes zu bestimmen.  Dies geschieht mithilfe eines Rasters mit einer √§u√üerst einfachen Architektur: zwei vollst√§ndig verbundenen Schichten.  Zuerst kommt eine vollst√§ndig verbundene Schicht vom Booleschen Vektor der Dimension des W√∂rterbuchs zur verborgenen Schicht der Dimension der Einbettung (d. H. Nur Multiplizieren des Booleschen Vektors mit der Matrix der gew√ºnschten Dimension).  Und umgekehrt, eine vollst√§ndig verbundene Ebene mit Softmax aus einer verborgenen Dimensionsebene, die in einen W√∂rterbuch-Dimensionsvektor eingebettet ist.  Dank der Softmax-Aktivierungsfunktion erhalten wir die Wahrscheinlichkeitsverteilung unseres Wortes und k√∂nnen die wahrscheinlichste Option ausw√§hlen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vn/fu/ld/vnfuldlmtgik5rihivtpcivmnla.png"></div><br>  <i>Die Einbettung des i-ten Wortes ist einfach die i-te Zeile in der √úbergangsmatrix W.</i> <br><br>  In den in der Praxis verwendeten Modellen ist die Architektur komplexer, aber nicht viel.  Der Hauptunterschied besteht darin, dass wir nicht einen Vektor aus dem Kontext verwenden, um unser Wort zu definieren, sondern mehrere (zum Beispiel alles in einem Fenster der Gr√∂√üe 3).  Eine etwas popul√§rere Option ist, wenn wir versuchen, nicht ein Wort nach Kontext, sondern einen Kontext nach Wort vorherzusagen.  Dieser Ansatz wird Skip-Gramm genannt. <br><br>  Lassen Sie uns ein Beispiel f√ºr die Anwendung einer Aufgabe geben, die w√§hrend des Trainings von Einbettungen gel√∂st wird (in der CBOW-Variante Wortvorhersagen nach Kontext).  Angenommen, ein Token-Kontext besteht aus zwei vorherigen W√∂rtern.                ‚Äú ‚Äù, ,  ,       ‚Äú‚Äù. <br><br>   ,          (    ),       ,      . <br><br>      ,    ,      ,        (,        ,        ).   ‚Äî      . <br><br>  ,  ,         .      ,      ,    ,   . <br><br>         ,   ‚Äî ELMo, ULMFit, BERT.       ,         (  , , ,    ). <br><br><h2>   ? <br></h2><br>    ,     2  . <br><br><ul><li> -,     ,           ,   -   100 .     ‚Äì   :    ,    ,     . </li><li> -,      .      -.        .        .    ,        ,    .  ,    ,    .          .         ,       ,    . </li></ul><br><img src="https://habrastorage.org/webt/dh/w6/w2/dhw6w2s41xbc8y08jgszsupa-z8.png" alt="Bild"><br><br>    ,      .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,    ,  ,        ,    ,       .       ,     ,    ,     ,     . <br><br>          NER.    ,    ,            .     ,        ,       ,    ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437008/">https://habr.com/ru/post/de437008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436998/index.html">Schutz von Mikrochips vor Reverse Engineering und unbefugtem Betreten</a></li>
<li><a href="../de437000/index.html">Wie man Menschen den Umgang mit Git beibringt</a></li>
<li><a href="../de437002/index.html">G√ºltiger ASP.NET Core</a></li>
<li><a href="../de437004/index.html">Tr√§umen YML-Programmierer von ansiblen Tests?</a></li>
<li><a href="../de437006/index.html">Wanhao Duplicator 10 3D-Drucker Bewertung</a></li>
<li><a href="../de437010/index.html">Echos der Vergangenheit: Young's Erfahrung auf der Basis der neuen R√∂ntgenspektroskopie-Methode</a></li>
<li><a href="../de437014/index.html">Die Aufgabe von N K√∂rpern oder wie man eine Galaxie in die Luft jagt, ohne die K√ºche zu verlassen</a></li>
<li><a href="../de437018/index.html">Einige Fallstricke bei der statischen Eingabe in Python</a></li>
<li><a href="../de437020/index.html">Was ist falsch an Reinforcement Learning?</a></li>
<li><a href="../de437022/index.html">Noise Security Bit 0x22 (Fehlerinjektionsangriffe, 35C3 und Wallet.fail)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>