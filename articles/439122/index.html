<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💓 ⚽️ 💎 La simplicidad y complejidad de las primitivas o cómo determinar el preprocesamiento innecesario para una red neuronal 📉 🧑🏾‍🤝‍🧑🏾 🧑🏻‍🤝‍🧑🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este es el tercer artículo sobre el análisis y estudio de elipses, triángulos y otras formas geométricas. 
 Los artículos anteriores plantearon alguna...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La simplicidad y complejidad de las primitivas o cómo determinar el preprocesamiento innecesario para una red neuronal</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439122/">  Este es el tercer artículo sobre el análisis y estudio de elipses, triángulos y otras formas geométricas. <br>  Los artículos anteriores plantearon algunas preguntas muy interesantes entre los lectores, en particular, sobre la complejidad o simplicidad de ciertas secuencias de entrenamiento.  Las preguntas son realmente muy interesantes, por ejemplo, ¿cuánto más difícil es aprender un triángulo que un cuadrángulo u otro polígono? <br><br><img src="https://habrastorage.org/webt/3p/l8/bo/3pl8bouhofjpesjyyzbmosjerxw.jpeg"><br><br>  Intentemos comparar, y para comparar tenemos una gran idea, probada por generaciones de estudiantes, la idea: cuanto más corta es la hoja de trucos, más fácil es el examen. <br><br>  Este artículo también es simplemente el resultado de la curiosidad y el interés ocioso, nada de eso se encuentra en la práctica y para tareas prácticas hay un par de grandes ideas, pero no hay casi nada para copiar y pegar.  Este es un pequeño estudio de la complejidad de las secuencias de entrenamiento: se presentan el razonamiento y el código del autor, puede verificar / complementar / cambiar todo usted mismo. <br><br>  Entonces, intentemos averiguar qué figura geométrica es más complicada o más simple para la segmentación, qué curso de conferencias para IA es más comprensible y mejor absorbido. <a name="habracut"></a><br><br>  Hay muchas formas geométricas diferentes, pero solo compararemos triángulos, cuadrángulos y estrellas de cinco puntas.  Utilizaremos un método simple para construir una secuencia de tren: dividiremos las imágenes monocromas de 128x128 en cuatro partes y colocaremos al azar una elipse y, por ejemplo, un triángulo en estos cuartos.  Detectaremos un triángulo del mismo color que la elipse.  Es decir  la tarea es entrenar la red para distinguir, por ejemplo, un polígono cuadrangular de una elipse pintada del mismo color.  Aquí hay ejemplos de imágenes que estudiaremos. <br><br><img src="https://habrastorage.org/webt/nu/qo/8i/nuqo8io482lnoa3ukyvjorfrlyo.png"><br><br><img src="https://habrastorage.org/webt/3h/rf/n6/3hrfn6wwnthkepnuqdrjezoyaas.png"><br><br><img src="https://habrastorage.org/webt/fi/_l/zw/fi_lzwaortnx2k4fbb-50l2_8rs.png"><br><br>  No detectaremos un triángulo y un cuadrángulo en una imagen, los detectaremos por separado, en diferentes trenes, contra el fondo de interferencia en forma de elipse. <br><br>  Tomemos la clásica red U y tres tipos de secuencias de entrenamiento con triángulos, cuadrángulos y estrellas para la investigación. <br><br>  Entonces, dado: <br><br><ul><li>  tres secuencias de entrenamiento de pares de imágenes / máscaras; </li><li>  la red  Red U ordinaria, que se usa ampliamente para la segmentación. </li></ul><br>  Idea para probar: <br><br><ul><li>  determinar cuál de las secuencias de entrenamiento es "más difícil" de aprender; </li><li>  cómo algunas técnicas de preprocesamiento afectan el aprendizaje </li></ul><br>  Comencemos, seleccione 10,000 pares de imágenes de cuadrángulos con elipses y máscaras y considérelos cuidadosamente.  Estamos interesados ​​en lo corto que resultará la cuna y de qué longitud depende. <br><br><div class="spoiler">  <b class="spoiler_title">Cargamos bibliotecas, determinamos los tamaños de una serie de imágenes.</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.draw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ellipse, polygon <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input,Conv2D,Conv2DTranspose,MaxPooling2D,concatenate <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization,Activation,Add,Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.losses <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> binary_crossentropy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> keras w_size = <span class="hljs-number"><span class="hljs-number">128</span></span> train_num = <span class="hljs-number"><span class="hljs-number">10000</span></span> radius_min = <span class="hljs-number"><span class="hljs-number">10</span></span> radius_max = <span class="hljs-number"><span class="hljs-number">20</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">determinar las funciones de pérdida y precisión</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_coef</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> y_true_f = K.flatten(y_true) y_pred = K.cast(y_pred, <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) y_pred_f = K.cast(K.greater(K.flatten(y_pred), <span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) intersection = y_true_f * y_pred_f score = <span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> smooth = <span class="hljs-number"><span class="hljs-number">1.</span></span> y_true_f = K.flatten(y_true) y_pred_f = K.flatten(y_pred) intersection = y_true_f * y_pred_f score = (<span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> - score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bce_dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_iou_vector</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, B)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Numpy version batch_size = A.shape[0] metric = 0.0 for batch in range(batch_size): t, p = A[batch], B[batch] true = np.sum(t) pred = np.sum(p) # deal with empty mask first if true == 0: metric += (pred == 0) continue # non empty mask case. Union is never empty # hence it is safe to divide by its number of pixels intersection = np.sum(t * p) union = true + pred - intersection iou = intersection / union # iou metrric is a stepwise approximation of the real iou over 0.5 iou = np.floor(max(0, (iou - 0.45)*20)) / 10 metric += iou # teake the average over all images in batch metric /= batch_size return metric def my_iou_metric(label, pred): # Tensorflow version return tf.py_func(get_iou_vector, [label, pred &gt; 0.5], tf.float64) from keras.utils.generic_utils import get_custom_objects get_custom_objects().update({'bce_dice_loss': bce_dice_loss }) get_custom_objects().update({'dice_loss': dice_loss }) get_custom_objects().update({'dice_coef': dice_coef }) get_custom_objects().update({'my_iou_metric': my_iou_metric })</span></span></code> </pre><br></div></div><br>  Usaremos la métrica del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">primer artículo</a> .  Permítanme recordarles a los lectores que vamos a predecir la máscara del píxel: este es el "fondo" o "cuadrángulo" y evaluar la verdad o la falsedad de la predicción.  Es decir  Son posibles las siguientes cuatro opciones: predijimos correctamente que un píxel es un fondo, predijimos correctamente que un píxel es un cuadrángulo o cometimos un error al predecir un "fondo" o "cuadrángulo".  Entonces, para todas las imágenes y todos los píxeles, estimamos el número de las cuatro opciones y calculamos el resultado; este será el resultado de la red.  Y cuanto menos predicciones erróneas y más verdaderas, cuanto más preciso sea el resultado y mejor será la red. <br><br>  Examinamos la red como un "recuadro negro", no veremos lo que está sucediendo con la red interna, cómo cambian los pesos y cómo se eligen los gradientes; más adelante analizaremos las entrañas de la red cuando comparemos las redes. <br><br><div class="spoiler">  <b class="spoiler_title">U-net simple</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_layer, start_neurons)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># 128 -&gt; 64 conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(input_layer) conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(conv1) pool1 = MaxPooling2D((2, 2))(conv1) pool1 = Dropout(0.25)(pool1) # 64 -&gt; 32 conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(pool1) conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(conv2) pool2 = MaxPooling2D((2, 2))(conv2) pool2 = Dropout(0.5)(pool2) # 32 -&gt; 16 conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(pool2) conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(conv3) pool3 = MaxPooling2D((2, 2))(conv3) pool3 = Dropout(0.5)(pool3) # 16 -&gt; 8 conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(pool3) conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(conv4) pool4 = MaxPooling2D((2, 2))(conv4) pool4 = Dropout(0.5)(pool4) # Middle convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(pool4) convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(convm) # 8 -&gt; 16 deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(convm) uconv4 = concatenate([deconv4, conv4]) uconv4 = Dropout(0.5)(uconv4) uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4) uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4) # 16 -&gt; 32 deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(uconv4) uconv3 = concatenate([deconv3, conv3]) uconv3 = Dropout(0.5)(uconv3) uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3) uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3) # 32 -&gt; 64 deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding="same")(uconv3) uconv2 = concatenate([deconv2, conv2]) uconv2 = Dropout(0.5)(uconv2) uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2) uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2) # 64 -&gt; 128 deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding="same")(uconv2) uconv1 = concatenate([deconv1, conv1]) uconv1 = Dropout(0.5)(uconv1) uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1) uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1) uncov1 = Dropout(0.5)(uconv1) output_layer = Conv2D(1, (1,1), padding="same", activation="sigmoid")(uconv1) return output_layer # model input_layer = Input((w_size, w_size, 1)) output_layer = build_model(input_layer, 26) model = Model(input_layer, output_layer) model.compile(loss=bce_dice_loss, optimizer=Adam(lr=1e-4), metrics=[my_iou_metric]) model.summary()</span></span></code> </pre><br></div></div><br>  La función de generar pares de imagen / máscara.  En una imagen en blanco y negro de 128x128 llena de ruido aleatorio con una selección aleatoria de dos rangos, o 0.0 ... 0.75 o 0.25..1.0.  Seleccione aleatoriamente un cuarto en la imagen y coloque una elipse orientada al azar y en el otro cuarto colocamos un cuadrilátero e igualmente color con ruido aleatorio. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_pair</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> img_l = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">1</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img_h = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">1</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span> + <span class="hljs-number"><span class="hljs-number">0.25</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img = np.zeros((w_size, w_size, <span class="hljs-number"><span class="hljs-number">2</span></span>), dtype=<span class="hljs-string"><span class="hljs-string">'float'</span></span>) i0_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) i1_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> i0_qua == i1_qua: i1_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) _qua = np.int(w_size/<span class="hljs-number"><span class="hljs-number">4</span></span>) qua = np.array([[_qua,_qua],[_qua,_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>],[_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>,_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>],[_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>,_qua]]) p = np.random.sample() - <span class="hljs-number"><span class="hljs-number">0.5</span></span> r = qua[i0_qua,<span class="hljs-number"><span class="hljs-number">0</span></span>] c = qua[i0_qua,<span class="hljs-number"><span class="hljs-number">1</span></span>] r_radius = np.random.sample()*(radius_max-radius_min) + radius_min c_radius = np.random.sample()*(radius_max-radius_min) + radius_min rot = np.random.sample()*<span class="hljs-number"><span class="hljs-number">360</span></span> rr, cc = ellipse( r, c, r_radius, c_radius, rotation=np.deg2rad(rot), shape=img_l.shape ) p0 = np.rint(np.random.sample()*(radius_max-radius_min) + radius_min) p1 = qua[i1_qua,<span class="hljs-number"><span class="hljs-number">0</span></span>] - (radius_max-radius_min) p2 = qua[i1_qua,<span class="hljs-number"><span class="hljs-number">1</span></span>] - (radius_max-radius_min) p3 = np.rint(np.random.sample()*radius_min) p4 = np.rint(np.random.sample()*radius_min) p5 = np.rint(np.random.sample()*radius_min) p6 = np.rint(np.random.sample()*radius_min) p7 = np.rint(np.random.sample()*radius_min) p8 = np.rint(np.random.sample()*radius_min) poly = np.array(( (p1, p2), (p1+p3, p2+p4+p0), (p1+p5+p0, p2+p6+p0), (p1+p7+p0, p2+p8), (p1, p2), )) rr_p, cc_p = polygon(poly[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], poly[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], img_l.shape) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> p &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h[rr_p, cc_p] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l[rr_p, cc_p] img[:,:,<span class="hljs-number"><span class="hljs-number">1</span></span>] = <span class="hljs-number"><span class="hljs-number">0.</span></span> img[rr_p, cc_p,<span class="hljs-number"><span class="hljs-number">1</span></span>] = <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> img</code> </pre><br>  Creemos una secuencia de entrenamiento de pares, ver al azar 10. Permítanme recordarles que las imágenes son monocromas, en escala de grises. <br><br><pre> <code class="python hljs">_txy = [next_pair() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(train_num)] f_imgs = np.array(_txy)[:,:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) f_msks = np.array(_txy)[:,:,:,<span class="hljs-number"><span class="hljs-number">1</span></span>:].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_txy) <span class="hljs-comment"><span class="hljs-comment">#    10   fig, axes = plt.subplots(2, 10, figsize=(20, 5)) for k in range(10): kk = np.random.randint(train_num) axes[0,k].set_axis_off() axes[0,k].imshow(f_imgs[kk]) axes[1,k].set_axis_off() axes[1,k].imshow(f_msks[kk].squeeze())</span></span></code> </pre><br><img src="https://habrastorage.org/webt/nu/qo/8i/nuqo8io482lnoa3ukyvjorfrlyo.png"><br><br><h3>  Primer paso  Entrenamos en el set inicial mínimo </h3><br>  El primer paso de nuestro experimento es simple, estamos tratando de entrenar a la red para predecir solo 11 primeras imágenes. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">10</span></span> val_len = <span class="hljs-number"><span class="hljs-number">11</span></span> precision = <span class="hljs-number"><span class="hljs-number">0.85</span></span> m0_select = np.zeros((f_imgs.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]), dtype=<span class="hljs-string"><span class="hljs-string">'int'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(val_len): m0_select[k] = <span class="hljs-number"><span class="hljs-number">1</span></span> t = tqdm() <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: fit = model.fit(f_imgs[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], f_msks[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], batch_size=batch_size, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span> ) current_accu = fit.history[<span class="hljs-string"><span class="hljs-string">'my_iou_metric'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] current_loss = fit.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] t.set_description(<span class="hljs-string"><span class="hljs-string">"accuracy {0:6.4f} loss {1:6.4f} "</span></span>.\ format(current_accu, current_loss)) t.update(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> current_accu &gt; precision: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> t.close()</code> </pre> <br> <code>accuracy 0.8545 loss 0.0674 lenght 11 : : 793it [00:58, 14.79it/s]</code> <br> <br>  Seleccionamos los primeros 11 de la secuencia inicial y capacitamos a la red en ellos.  Ahora no importa si la red memoriza estas imágenes específicamente o resume, lo principal es que puede reconocer estas 11 imágenes de la manera que necesitamos.  Dependiendo del conjunto de datos y la precisión seleccionados, la capacitación en red puede durar mucho, mucho tiempo.  Pero solo tenemos unas pocas iteraciones.  Repito que ahora no es importante para nosotros cómo y qué aprendió o aprendió la red, lo principal es que ha alcanzado la precisión establecida de predicción. <br><br><h3>  Ahora comienza el experimento principal </h3><br>  Construiremos la hoja de trucos, construiremos dichas hojas de trucos por separado para las tres secuencias de entrenamiento y compararemos su longitud.  Tomaremos nuevos pares de imagen / máscara de la secuencia construida e intentaremos predecirlos por la red entrenada en la secuencia ya seleccionada.  Al principio, son solo 11 pares de imagen / máscara y la red está entrenada, quizás no muy correctamente.  Si en un nuevo par se predice la máscara de la imagen con una precisión aceptable, entonces descartamos este par, no tiene información nueva para la red, ya lo sabe y puede calcular la máscara a partir de esta imagen.  Si la precisión de la predicción es insuficiente, entonces agregamos esta imagen con una máscara a nuestra secuencia y comenzamos a entrenar la red hasta que se obtenga un resultado de precisión aceptable en la secuencia seleccionada.  Es decir  Esta imagen contiene información nueva y la agregamos a nuestra secuencia de entrenamiento y extraemos la información contenida en ella mediante entrenamiento. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">50</span></span> t_batch_size = <span class="hljs-number"><span class="hljs-number">1024</span></span> raw_len = val_len t = tqdm(<span class="hljs-number"><span class="hljs-number">-1</span></span>) id_train = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment">#id_select = 1 while True: t.set_description("Accuracy {0:6.4f} loss {1:6.4f}\ selected img {2:5d} tested img {3:5d} ". format(current_accu, current_loss, val_len, raw_len)) t.update(1) if id_train == 1: fit = model.fit(f_imgs[m0_select&gt;0], f_msks[m0_select&gt;0], batch_size=batch_size, epochs=1, verbose=0 ) current_accu = fit.history['my_iou_metric'][0] current_loss = fit.history['loss'][0] if current_accu &gt; precision: id_train = 0 else: t_pred = model.predict( f_imgs[raw_len: min(raw_len+t_batch_size,f_imgs.shape[0])], batch_size=batch_size ) for kk in range(t_pred.shape[0]): val_iou = get_iou_vector( f_msks[raw_len+kk].reshape(1,w_size,w_size,1), t_pred[kk].reshape(1,w_size,w_size,1) &gt; 0.5) if val_iou &lt; precision*0.95: new_img_test = 1 m0_select[raw_len+kk] = 1 val_len += 1 break raw_len += (kk+1) id_train = 1 if raw_len &gt;= train_num: break t.close()</span></span></code> </pre><br><pre> <code class="bash hljs">Accuracy 0.9338 loss 0.0266 selected img 1007 tested img 9985 : : 4291it [49:52, 1.73s/it]</code> </pre> <br>  Aquí la precisión se usa en el sentido de "precisión", y no como la métrica estándar de keras, y la subrutina "my_iou_metric" se usa para calcular la precisión. <br><br>  Ahora compare el funcionamiento de la misma red con los mismos parámetros en una secuencia diferente, en triángulos <br><br><img src="https://habrastorage.org/webt/3h/rf/n6/3hrfn6wwnthkepnuqdrjezoyaas.png"><br><br>  Y obtenemos un resultado completamente diferente <br><br><pre> <code class="bash hljs">Accuracy 0.9823 loss 0.0108 selected img 1913 tested img 9995 : : 6343it [2:11:36, 3.03s/it]</code> </pre> <br>  La red seleccionó imágenes de 1913 con información "nueva", es decir  ¡El contenido de las imágenes con triángulos es la mitad que con los cuadrángulos! <br><br>  Verifiquemos lo mismo en las estrellas y ejecutemos la red en la tercera secuencia <br><br><img src="https://habrastorage.org/webt/fi/_l/zw/fi_lzwaortnx2k4fbb-50l2_8rs.png"><br><br>  tenemos <br><br><pre> <code class="bash hljs">Accuracy 0.8985 loss 0.0478 selected img 476 tested img 9985 : : 2188it [16:13, 1.16it/s]</code> </pre> <br>  Como puede ver, las estrellas resultaron ser las más informativas, solo 476 imágenes en una hoja de trucos. <br><br>  Teníamos razones para juzgar la complejidad de las formas geométricas para la percepción por su red neuronal.  La más simple es la estrella, con solo 476 imágenes en la hoja de trucos, luego el cuadrángulo con su 1007 y el más complejo resultó ser un triángulo: para el entrenamiento necesitas 1913 imágenes. <br><br>  Tenga en cuenta que esto es para nosotros, para las personas es una imagen, pero para la red es un curso de lectura sobre reconocimiento y el curso sobre triángulos resultó ser el más difícil. <br><br><h3>  Ahora sobre lo serio </h3><br>  A primera vista, todas estas elipses y triángulos parecen mimos, tortas de arena y lego.  Pero aquí hay una pregunta específica y seria: si aplicamos algún tipo de preprocesamiento, filtro a la secuencia inicial, ¿cómo cambiará la complejidad de la secuencia?  Por ejemplo, tomamos las mismas elipses y cuadrángulos y les aplicamos dicho preprocesamiento <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.ndimage <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gaussian_filter _tmp = [gaussian_filter(idx, sigma = <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f_imgs] f1_imgs = np.array(_tmp)[:,:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_tmp) fig, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>): kk = np.random.randint(train_num) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].imshow(f1_imgs[kk].squeeze(), cmap=<span class="hljs-string"><span class="hljs-string">"gray"</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].imshow(f_msks[kk].squeeze(), cmap=<span class="hljs-string"><span class="hljs-string">"gray"</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/76/mb/0f/76mb0fpk1weknaahb8fyxn6cevk.png"><br><br>  A primera vista, todo es lo mismo, las mismas elipses, los mismos polígonos, pero la red comenzó a funcionar de una manera completamente diferente: <br><br><pre> <code class="bash hljs">Accuracy 1.0575 loss 0.0011 selected img 7963 tested img 9999 : : 17765it [29:02:00, 12.40s/it]</code> </pre> <br>  Aquí se necesita una pequeña explicación, no utilizamos el aumento, porque  La forma del polígono y la forma de la elipse se seleccionan inicialmente al azar.  Por lo tanto, el aumento no dará nueva información y no tiene sentido en este caso. <br><br>  Pero, como se puede ver en el resultado del trabajo, un simple gaussian_filter creó muchos problemas para la red, generó mucha información nueva y probablemente superflua. <br><br>  Bueno, para los amantes de la simplicidad en su forma más pura, tomamos las mismas elipses con polígonos, pero sin ninguna aleatoriedad en el color. <br><br><img src="https://habrastorage.org/webt/8x/7b/vd/8x7bvdqpavgkjuubnk-2ug-kjt4.png"><br><br>  El resultado sugiere que el color aleatorio no es una simple adición. <br><br><pre> <code class="bash hljs">Accuracy 0.9004 loss 0.0315 selected img 251 tested img 9832 : : 1000it [06:46, 1.33it/s]</code> </pre><br>  La red valió completamente la información extraída de 251 imágenes, casi cuatro veces menos que la de muchas imágenes pintadas con ruido. <br><br>  El propósito del artículo es mostrar algunas herramientas y ejemplos de su trabajo en ejemplos frívolos, el lego en el sandbox.  Tenemos una herramienta para comparar dos secuencias de entrenamiento, podemos evaluar cuánto complica nuestro preprocesamiento o simplifica la secuencia de entrenamiento, cómo esta o aquella primitiva en la secuencia de entrenamiento es fácil de detectar. <br><br>  La posibilidad de aplicar este ejemplo de Lego en casos reales es obvia, pero los entrenamientos reales y las redes de lectores dependen de los propios lectores. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/439122/">https://habr.com/ru/post/439122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../439112/index.html">Creador de Kate Mobile arrestado por pedófilo usando su servicio</a></li>
<li><a href="../439114/index.html">Del caos al orden, o "crear una estructura de proyecto en Unity y no solo ..."</a></li>
<li><a href="../439116/index.html">Juego de Andrey: miedo a la crisis tecnológica</a></li>
<li><a href="../439118/index.html">Nuevo en los navegadores: Firefox 66 bloquea el video y el sonido de forma predeterminada, Chromium limita el presupuesto de la página</a></li>
<li><a href="../439120/index.html">Solicitudes de funciones y requisitos del producto</a></li>
<li><a href="../439124/index.html">Si el software se crea con dinero público, el código debe estar abierto</a></li>
<li><a href="../439126/index.html">Solicitudes de usuario y requisitos de producto</a></li>
<li><a href="../439128/index.html">Cómo organizar el trabajo de QA. Una forma práctica</a></li>
<li><a href="../439130/index.html">13 tendencias del mercado de ciberseguridad y seguridad de la información 2019-2020</a></li>
<li><a href="../439132/index.html">Vejez inolvidable</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>