<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👼🏽 😚 👨🏿‍🚀 Guia Kubernetes, Parte 2: Criando e trabalhando com um cluster 💅🏽 ⏳ 👩‍🍳</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Na última vez, examinamos duas abordagens para trabalhar com microsserviços. Em particular, um deles envolve o uso de contêineres Docker, nos quais vo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Guia Kubernetes, Parte 2: Criando e trabalhando com um cluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ruvds/blog/438984/">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Na última</a> vez, examinamos duas abordagens para trabalhar com microsserviços.  Em particular, um deles envolve o uso de contêineres Docker, nos quais você pode executar o código de microsserviços e programas auxiliares.  Hoje, usando nossas imagens de contêiner existentes, trabalharemos com o Kubernetes. <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/webt/13/lv/dr/13lvdrwhhap-ouchegvweul0fg0.jpeg"></a> <br><a name="habracut"></a><br><h2>  <font color="#3AC1EF">Apresentando o Kubernetes</font> </h2><br>  Eu prometo e não exagero quando você lê este artigo, pergunte-se: "Por que os Kubernetes não são chamados de Supernetes?" <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d76/9f9/e76/d769f9e7670a725759dd7415949177a0.png"></div><br>  <i><font color="#999999">Supernetes</font></i> <br><br>  Se você leu a parte anterior deste material, sabe que examinamos muitas coisas relacionadas à preparação de aplicativos para contêiner e ao trabalho com contêineres do Docker.  Pode parecer para você que a coisa mais difícil espera por você agora, mas, de fato, o que falaremos aqui é muito mais simples do que o que já descobrimos.  A única razão pela qual aprender Kubernetes pode parecer uma tarefa assustadora para alguém é a quantidade de informações extras necessárias para entender o Kubernetes e usá-lo de forma eficaz.  Já discutimos todas as "informações adicionais" necessárias para o desenvolvimento bem-sucedido do Kubernetes. <br><br><h3>  <font color="#3AC1EF">▍O que é o Kubernetes?</font> </h3><br>  Na primeira parte deste artigo, após o lançamento de microsserviços em contêineres, você foi solicitado a pensar no problema de dimensionar aplicativos em contêiner. <br>  Sugiro refletir juntos sobre isso, no formato de perguntas e respostas: <br><br>  <b>Pergunta:</b> Como os aplicativos em contêiner são dimensionados? <br>  <b>Resposta:</b> Lance contêineres adicionais. <br><br>  <b>Pergunta:</b> E como a carga é distribuída entre eles?  E se um determinado servidor já estiver usado ao máximo e o contêiner precisar ser implantado em outro servidor?  Como encontrar a maneira mais eficiente de usar hardware? <br>  <b>Resposta:</b> Então ... vou procurar na Internet ... <br><br>  <b>Pergunta:</b> Como atualizar programas sem interromper o sistema?  E, se a atualização contiver um erro, como retornar à versão funcional do aplicativo? <br><br>  De fato, é a tecnologia Kubernetes que fornece respostas dignas para essas e muitas outras perguntas.  Tentarei restringir a definição de Kubernetes a uma frase: "Kubernetes é um sistema de gerenciamento de contêineres que abstrai a infraestrutura subjacente (o ambiente em que os contêineres são executados)". <br><br>  Acredito que agora você não está particularmente claro sobre o conceito de "gerenciamento de contêineres", embora já tenhamos mencionado isso.  Abaixo, consideraremos essa tecnologia na prática.  No entanto, o conceito de "abstrair a infraestrutura básica" é encontrado pela primeira vez.  Portanto, agora vamos considerar isso. <br><br><h3>  <font color="#3AC1EF">StrAbstração da infraestrutura básica</font> </h3><br>  O Kubernetes permite que os aplicativos se afastem da infraestrutura, fornecendo uma API simples para a qual você pode enviar solicitações.  O Kubernetes tenta atender a essas solicitações usando todos os seus recursos.  Por exemplo, em um idioma comum, uma solicitação semelhante pode ser descrita da seguinte forma: “Kubernetes, expanda 4 contêineres de imagens X”.  Após receber o comando, o Kubernetes encontrará nós que não estão muito ocupados (eles também são chamados de "nós" - do "nó" inglês), nos quais você pode implantar novos contêineres. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/735/b88/a2a/735b88a2a717e9c01bfc197f3c1b20fd.png"></div><br>  <i><font color="#999999">Solicitação do servidor de API</font></i> <br><br>  O que isso significa para o desenvolvedor?  Isso significa que ele não precisa se preocupar com o número de nós, exatamente onde os contêineres são lançados ou como eles interagem.  Ele não precisa lidar com a otimização de hardware ou se preocupar com os nós que podem estar com defeito (e algo semelhante, de acordo com a lei de Murphy, certamente acontecerá), pois, se necessário, novos nós podem ser adicionados ao cluster Kubernetes.  Se algo estiver errado com alguns nós existentes, o Kubernetes implantará contêineres nos nós que ainda estão em um estado íntegro. <br><br>  Muito do que é mostrado na figura anterior já é familiar para você.  Mas há também algo novo: <br><br><ul><li>  Servidor API  Fazer chamadas para este servidor é a única maneira de interagir com o cluster que temos, se estamos falando sobre iniciar ou parar contêineres, verificar o status do sistema, trabalhar com logs ou executar outras ações. </li><li>  Kubelet.  Este é um agente que monitora os contêineres dentro do nó e interage com o nó principal. </li></ul><br>  Observe que em algumas frases anteriores usamos o termo "contêiner", mas aqui seria mais correto usar o termo "pod".  Essas entidades são freqüentemente chamadas de "vagens" em publicações em russo e, às vezes - "vagens", na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação</a> , esclarecendo o conceito de "vagem", elas falam de um "bando de baleias" (vagem de baleias) ou de um "vagem de ervilha" mas ninguém os chama de "bandos" ou "vagens".  Falando deles, usaremos a palavra "abaixo".  Agora você pode considerá-los contêineres, falaremos mais sobre os pods abaixo. <br><br>  Pararemos com isso por enquanto, pois podemos falar sobre tudo isso ainda mais, e, além disso, existem muitos materiais bons sobre a teoria de Kubernetes.  Por exemplo, essa é uma documentação oficial, embora não seja fácil de ler, ou livros como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este</a> . <br><br><h3>  <font color="#3AC1EF">▍ Padronização do trabalho com provedores de serviços em nuvem</font> </h3><br>  Outro ponto forte do Kubernetes reside no fato de que essa tecnologia contribui para a padronização do trabalho com provedores de serviços em nuvem (Cloud Service Provider, CSP).  Esta é uma afirmação ousada.  Considere o seguinte exemplo.  Um especialista que conhece bem o Azure ou o Google Cloud Platform precisa trabalhar em um projeto projetado para um ambiente em nuvem completamente novo para ele, com o qual não está familiarizado.  Nesta situação, muita coisa pode dar errado.  Por exemplo, os prazos para a entrega do projeto podem ser atrasados, a empresa cliente do projeto pode precisar alugar mais recursos de nuvem do que o planejado, e assim por diante. <br><br>  Ao usar o Kubernetes, esse problema simplesmente não pode surgir, pois, independentemente de qual provedor de serviços em nuvem específico, estamos trabalhando, o trabalho com o Kubernetes sempre parece o mesmo.  O desenvolvedor, em estilo declarativo, diz ao servidor da API o que ele precisa e o Kubernetes trabalha com os recursos do sistema, permitindo que o desenvolvedor ignore os detalhes da implementação desse sistema. <br><br>  Demore um pouco nessa idéia, já que esta é uma oportunidade muito poderosa para o Kubernetes.  Para as empresas, isso significa que suas decisões não estão vinculadas a um CSP específico.  Se uma empresa encontrar uma oferta melhor no mercado de serviços em nuvem, poderá tirar proveito dessa oferta livremente mudando para um novo provedor.  Além disso, a experiência adquirida pelos especialistas da empresa não se perde em lugar algum. <br><br>  Agora vamos falar sobre o uso prático do Kubernetes <br><br><h2>  <font color="#3AC1EF">Prática de Kubernetes: Pods</font> </h2><br>  Configuramos o lançamento de microsserviços em contêineres, o processo de instalação foi bastante tedioso, mas conseguimos acessar um sistema em funcionamento.  Além disso, como já mencionado, nossa solução não é bem dimensionada e não é resistente a falhas.  Vamos resolver esses problemas com o Kubernetes.  Em seguida, traremos nosso sistema para um formulário correspondente ao esquema a seguir.  Ou seja, os contêineres serão gerenciados pelo Kubernetes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br>  <i><font color="#999999">Os microsserviços funcionam em um cluster gerenciado pelo Kubernetes</font></i> <br><br>  Aqui, usaremos o Minikube para implantação local do cluster e para testar os recursos do Kubernetes, embora tudo o que faremos aqui possa ser feito usando plataformas em nuvem como o Azure ou o Google Cloud Platform. <br><br><h3>  <font color="#3AC1EF">▍Instalação e início do Minikube</font> </h3><br>  Para instalar o Minikube, siga as instruções encontradas na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documentação</a> .  Durante a instalação do Minikube, você também instalará o Kubectl.  Este é um cliente que permite que solicitações sejam feitas ao servidor da API Kubernetes. <br><br>  Para iniciar o Minikube, execute o comando <code>minikube start</code> e, após a conclusão, execute o comando <code>kubectl get nodes</code> .  Como resultado, você deve ver algo como o seguinte: <br><br><pre> <code class="plaintext hljs">kubectl get nodes NAME       STATUS ROLES     AGE VERSION minikube   Ready &lt;none&gt;    11m v1.9.0</code> </pre> <br>  O Minikube coloca à nossa disposição um cluster que consiste em apenas um nó.  É verdade que isso nos convém muito bem.  Quem trabalha com o Kubernetes não precisa se preocupar exatamente com quantos nós existem no cluster, pois o Kubernetes permite abstrair esses detalhes. <br><br>  Agora vamos falar sobre vagens. <br><br><h3>  <font color="#3AC1EF">▍Pods</font> </h3><br>  Eu realmente gosto de contêineres, e você provavelmente também gosta deles agora.  Por que o Kubernetes nos oferece o uso de pods, entidades que são as unidades de computação mínimas implantáveis ​​neste sistema?  Em que funções ele executa?  O fato é que a lareira pode incluir um ou mais contêineres que compartilham o mesmo tempo de execução. <br><br>  Mas é necessário realizar, por exemplo, dois contêineres em uma lareira?  Como dizer ... Normalmente, há apenas um contêiner por contêiner, e é isso que vamos fazer.  Mas para os casos em que, por exemplo, dois contêineres precisam de acesso compartilhado ao mesmo data warehouse, ou se estão conectados usando a técnica de comunicação entre processos, ou se estão intimamente conectados por algum outro motivo, tudo isso pode ser realizado executando-os em uma lareira.  Outra possibilidade em que os pods diferem é que eles não precisam usar contêineres do Docker.  Se necessário, aqui você pode aplicar outras tecnologias para conteinerização de aplicativos, por exemplo - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Rkt</a> . <br><br>  O diagrama a seguir mostra as propriedades numeradas da lareira. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f6/2a4/bb1/4f62a4bb18bddccc49a9d224a4aa919d.png"></div><br>  <i><font color="#999999">Propriedades da lareira</font></i> <br><br>  Considere essas propriedades. <br><br><ol><li>  Cada pod em um cluster Kubernetes possui um endereço IP exclusivo. </li><li>  Uma lareira pode conter muitos recipientes.  Eles compartilham números de portas disponíveis, ou seja, por exemplo, eles podem trocar informações entre si via <code>localhost</code> (naturalmente, eles não podem usar as mesmas portas).  A interação com contêineres localizados em outros pods é organizada usando os endereços IP desses pods. </li><li>  Os contêineres nos pods compartilham volumes de armazenamento de dados, endereço IP, números de porta e espaço para nome do IPC. </li></ol><br>  Deve-se observar que os contêineres têm seus próprios sistemas de arquivos isolados, mas eles podem compartilhar dados usando o recurso Kubernetes chamado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Volume</a> . <br><br>  Para nós, o que já foi dito sobre as lareiras é suficiente para continuar a dominar os Kubernetes.  Leia mais sobre eles <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br><h3>  <font color="#3AC1EF">▍ Descrição da lareira</font> </h3><br>  A seguir, um arquivo de manifesto para o aplicativo <code>sa-frontend</code> . <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                            # 1 metadata: name: sa-frontend                                  # 2 spec:                                                # 3 containers:   - image: rinormaloku/sentiment-analysis-frontend # 4     name: sa-frontend                              # 5     ports:       - containerPort: 80</code> </pre> <br>  Vamos explicar alguns dos parâmetros especificados nele. <br><br><ol><li>  <code>Kind</code> : especifica o tipo de recurso do Kubernetes que queremos criar.  No nosso caso, este é o <code>Pod</code> . </li><li>  <code>Name</code> : nome do recurso.  Chamamos isso <code>sa-frontend</code> . </li><li>  <code>Spec</code> : um objeto que descreve o estado desejado do recurso.  A propriedade mais importante aqui é a matriz de contêineres. </li><li>  <code>Image</code> : a imagem do contêiner que queremos executar neste pod. </li><li>  <code>Name</code> : um nome exclusivo para o contêiner abaixo. </li><li>  <code>ContainerPort</code> : a porta em que o contêiner está escutando.  Este parâmetro pode ser considerado uma indicação para quem lê este arquivo (se você omitir este parâmetro, isso não limitará o acesso à porta). </li></ol><br><h3>  <font color="#3AC1EF">▍Criando um SA-Frontend da lareira</font> </h3><br>  O arquivo de descrição do pod de que falamos pode ser encontrado em <code>resource-manifests/sa-frontend-pod.yaml</code> .  Você deve ir para esta pasta usando as ferramentas do terminal ou, quando chamar o comando apropriado, especificar o caminho completo para o arquivo.  Aqui está este comando e um exemplo de uma reação do sistema a ele: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod.yaml pod "sa-frontend" created</code> </pre> <br>  Para descobrir se funciona, execute o seguinte comando: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s</code> </pre> <br>  Se o estado da lareira durante a execução deste comando for <code>ContainerCreating</code> , você poderá executar o mesmo comando com a <code>--watch</code> .  Por esse motivo, quando a lareira estiver no estado <code>Running</code> , informações sobre isso serão exibidas automaticamente. <br><br><h3>  <font color="#3AC1EF">CessAcesso ao aplicativo de fora</font> </h3><br>  Para organizar o acesso ao aplicativo de fora, será correto criar um recurso Kubernetes do tipo Serviço, sobre o qual falaremos abaixo, mas aqui, por questões de brevidade, usaremos um encaminhamento de porta simples: <br><br><pre> <code class="plaintext hljs">kubectl port-forward sa-frontend 88:80 Forwarding from 127.0.0.1:88 -&gt; 80</code> </pre> <br>  Se você agora acessa um navegador em <code>127.0.0.1:88</code> , pode ver a página do aplicativo React. <br><br><h3>  <font color="#3AC1EF">Approach Abordagem de escala incorreta</font> </h3><br>  Já dissemos que um dos recursos do Kubernetes é o dimensionamento de aplicativos.  Para aproveitar esta oportunidade, abordaremos outra.  Crie uma descrição de outro recurso de <code>Pod</code> , colocando o seguinte código no arquivo <code>sa-frontend-pod2.yaml</code> : <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Pod                                           metadata: name: sa-frontend2      #   spec:                                                containers:   - image: rinormaloku/sentiment-analysis-frontend     name: sa-frontend                                  ports:       - containerPort: 80</code> </pre> <br>  Como você pode ver, se você comparar essa descrição com o que examinamos acima, a única alteração será o valor da propriedade <code>Name</code> . <br><br>  Crie um novo em: <br><br><pre> <code class="plaintext hljs">kubectl create -f sa-frontend-pod2.yaml pod "sa-frontend2" created</code> </pre> <br>  Verifique se está em execução: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                          READY STATUS RESTARTS AGE sa-frontend                   1/1 Running 0 7s sa-frontend2                  1/1 Running 0 7s</code> </pre> <br>  Agora temos duas lareiras!  É verdade que não há nada de especial para desfrutar aqui.  Observe que a solução para o problema de dimensionamento de aplicativos mostrado aqui tem muitas desvantagens.  Falaremos sobre como fazer isso corretamente na seção em outro recurso do Kubernetes chamado Deployment. <br><br>  Agora considere o que obtivemos após o lançamento de duas lareiras idênticas.  Ou seja, o servidor da web Nginx agora roda em dois pods diferentes.  Nesse sentido, podemos fazer duas perguntas: <br><br><ol><li>  Como dar acesso a esses servidores de fora, por URL? </li><li>  Como organizar o balanceamento de carga entre eles? </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ff/3a9/6f4/1ff3a96f4b930fe55727d1063b3c117b.png"></div><br>  <i><font color="#999999">Abordagem de escala incorreta</font></i> <br><br>  Entre as ferramentas do Kubernetes, existem recursos do formulário Serviço.  Vamos conversar sobre eles. <br><br><h2>  <font color="#3AC1EF">Kubernetes Practice: Serviços</font> </h2><br>  Os serviços Kubernetes atuam como pontos de acesso a conjuntos de fornos que fornecem a mesma funcionalidade que esses fornos.  Os serviços executam a solução de tarefas difíceis de trabalhar com lareiras e equilibrar a carga entre elas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bbd/95f/bd8/bbd95fbd8562bed4a09ab4930a20f98d.png"></div><br>  <i><font color="#999999">O serviço Kubernetes atende endereços IP</font></i> <br><br>  No nosso cluster Kubernetes, haverá pods que implementam funções diferentes.  Este é um aplicativo front-end, um aplicativo Web Spring e um aplicativo Flask escrito em Python.  Isso levanta a questão de como o serviço precisa entender com quais pods ele precisa trabalhar, ou seja, como descobrir com base em quais informações o sistema deve gerar uma lista de pontos de extremidade para os pods. <br><br>  Isso é feito com outra abstração do Kubernetes chamada Label.  O trabalho com tags consiste em dois estágios: <br><br><ol><li>  A atribuição de etiqueta fornecerá o serviço para trabalhar. </li><li>  Aplicando um "seletor" ao serviço, que determina quais pods às quais etiquetas são atribuídas, o serviço funcionará. </li></ol><br>  Talvez seja mais fácil imaginar isso como ilustração do que descrever. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb9/fcf/f0c/bb9fcff0cded591f1a5ab8a0b825245a.png"></div><br>  <i><font color="#999999">Pods rotulados e seus arquivos de manifesto</font></i> <br><br>  Vemos aqui dois lares que, usando o <code>app: sa-frontend</code> construção <code>app: sa-frontend</code> , recebem os mesmos rótulos.  O serviço está interessado em pods com essas marcas. <br><br><h3>  <font color="#3AC1EF">▍Tags</font> </h3><br>  Os rótulos oferecem aos desenvolvedores uma maneira simples de organizar os recursos do Kubernetes.  Eles são pares de valores-chave; você pode atribuí-los a qualquer recurso.  Modifique os arquivos de descrição da lareira do aplicativo frontend e traga-os para a exibição mostrada na figura anterior.  Depois disso, salve esses arquivos e execute os seguintes comandos: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-pod.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend" configured kubectl apply -f sa-frontend-pod2.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply pod "sa-frontend2" configured</code> </pre> <br>  Quando esses comandos são executados, o sistema emitirá avisos (não nos convém usar o <code>apply</code> vez de <code>create</code> , entendemos isso), mas, após um aviso, ele informa que os pods correspondentes estão configurados.  Podemos verificar se os rótulos foram atribuídos, filtrando os logs para os quais queremos exibir informações: <br><br><pre> <code class="plaintext hljs">kubectl get pod -l app=sa-frontend NAME           READY STATUS    RESTARTS AGE sa-frontend    1/1 Running   0 2h sa-frontend2   1/1 Running   0 2h</code> </pre> <br>  Outra maneira de verificar se as etiquetas foram realmente atribuídas é anexar a chave <code>--show-labels</code> ao comando anterior.  Devido a isso, as informações sobre seus pods também incluirão dados em suas marcas. <br><br>  Agora as tags foram atribuídas e estamos prontos para configurar o serviço para trabalhar com elas.  Portanto, assumiremos a descrição de um serviço como o <code>LoadBalancer</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e0f/081/7e9/e0f0817e9fc090628aa1d0d89577ce80.gif"></div><br>  <i><font color="#999999">Balanceamento de carga usando um serviço como o LoadBalancer</font></i> <br><br><h3>  <font color="#3AC1EF">▍ Descrição do serviço</font> </h3><br>  Aqui está uma descrição do YAML de um serviço como o <code>LoadBalancer</code> : <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service              # 1 metadata: name: sa-frontend-lb spec: type: LoadBalancer       # 2 ports: - port: 80               # 3   protocol: TCP          # 4   targetPort: 80         # 5 selector:                # 6   app: sa-frontend       # 7</code> </pre> <br>  Explique este texto: <br><br><ol><li>  <code>Kind</code> : criamos um serviço, um recurso de <code>Service</code> . </li><li>  <code>Type</code> : o tipo de recurso indicado em sua especificação.  Escolhemos o tipo <code>LoadBalancer</code> , porque com este serviço queremos resolver o problema de equilibrar a carga entre as lareiras. </li><li>  <code>Port</code> : porta na qual o serviço aceita solicitações. </li><li>  <code>Protocol</code> : o protocolo usado pelo serviço. </li><li>  <code>TargetPort</code> : porta para a qual as solicitações recebidas são redirecionadas. </li><li>  <code>Selector</code> : um objeto que contém informações sobre com quais pods o serviço deve funcionar. </li><li>  <code>app: sa-frontend</code> : essa propriedade indica com quais pods o serviço trabalhará.  Nomeadamente, esses são os pods aos quais o rótulo <code>app: sa-frontend</code> foi atribuído. </li></ol><br>  Para criar um serviço, você precisa executar o seguinte comando: <br><br><pre> <code class="plaintext hljs">kubectl create -f service-sa-frontend-lb.yaml service "sa-frontend-lb" created</code> </pre> <br>  Você pode verificar o status do serviço da seguinte maneira: <br><br><pre> <code class="plaintext hljs">kubectl get svc NAME             TYPE CLUSTER-IP      EXTERNAL-IP PORT(S) AGE sa-frontend-lb   LoadBalancer 10.101.244.40   &lt;pending&gt; 80:30708/TCP 7m</code> </pre> <br>  Aqui você pode ver que a propriedade <code>EXTERNAL-IP</code> está no estado <code>&lt;pending&gt;</code> , mas não pode esperar que ela mude.  Isso se deve ao fato de usarmos o Minikube.  Se criarmos um serviço semelhante ao trabalhar com um determinado provedor de serviços em nuvem, como o Azure ou o Google Cloud Platform, o serviço terá um endereço IP público que tornará possível acessá-lo pela Internet. <br><br>  Apesar disso, o Minikube não nos permitirá mexer, dando-nos um comando útil para a depuração local do sistema: <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb Opening kubernetes service default/sa-frontend-lb in default browser...</code> </pre> <br>  Graças a este comando, será lançado um navegador que acessará o serviço.  Depois que o serviço receber a solicitação, ele será redirecionado para um dos lares (não importa em qual deles estará).  Essa abstração nos permite perceber um grupo de lares como uma única entidade e trabalhar com eles, usando o serviço como um único ponto de acesso a eles. <br><br>  Nesta seção, falamos sobre como atribuir rótulos a recursos, como usá-los ao configurar serviços como seletores.  Aqui nós descrevemos e criamos um serviço como o <code>LoadBalancer</code> .  Graças a isso, resolvemos o problema de dimensionar o aplicativo (o dimensionamento consiste em adicionar novos lares com as etiquetas correspondentes ao cluster) e organizar o balanceamento de carga entre os lares, usando o serviço como ponto de entrada. <br><br><h2>  <font color="#3AC1EF">Prática Kubernetes: implantações</font> </h2><br>  A implantação é uma abstração do Kubernetes que nos permite controlar o que está sempre presente no ciclo de vida do aplicativo.  É sobre o gerenciamento de alterações de aplicativos.  Aplicativos que não mudam são, por assim dizer, aplicativos "mortos".  Se o aplicativo "permanecer", você poderá encontrar o fato de que seus requisitos mudam periodicamente, seu código se expande, esse código é empacotado e implantado.  Além disso, erros podem ser cometidos em todas as etapas do processo. <br><br>  Um recurso do tipo Implantação permite automatizar o processo de transição de uma versão de um aplicativo para outra.  Isso é feito sem interromper o sistema e, se ocorrer um erro durante esse processo, teremos a oportunidade de retornar rapidamente à versão de trabalho anterior do aplicativo. <br><br><h3>  <font color="#3AC1EF">SeUso de implantações</font> </h3><br>  Agora, o cluster tem duas lareiras e um serviço que dá acesso a elas de fora e equilibra a carga nelas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/426/651/0c4/4266510c40a1faa6086178e5db23d20c.png"></div><br>  <i><font color="#999999">Status atual do cluster</font></i> <br><br>  Falamos sobre o fato de que administrar dois lares diferentes com a mesma funcionalidade não é uma boa ideia.  Ao usar esse esquema, precisamos trabalhar com cada lareira individualmente, criando, atualizando e excluindo cada lareira específica, observando seu estado.  Com essa abordagem, não é necessário falar sobre uma atualização rápida do sistema ou a reversão rápida de uma atualização malsucedida.  Não estamos satisfeitos com esse estado de coisas, portanto, vamos recorrer à possibilidade de recurso de implantação, que visa solucionar os problemas acima. <br><br>  Antes de continuarmos o trabalho, vamos formular seus objetivos, que nos fornecerão diretrizes que serão úteis ao analisar o arquivo de manifesto de implantação.  Então, aqui está o que precisamos: <br><br><ol><li>  Queremos ser capazes de criar dois lares com base em um contêiner <code>rinormaloku/sentiment-analysis-frontend</code> . </li><li>  Precisamos de um sistema de implantação de aplicativos que permita que ele funcione sem interrupções quando for atualizado. </li><li>  Queremos que o rótulo do <code>app: sa-frontend</code> seja atribuído <code>app: sa-frontend</code> , que permitirá que o serviço <code>sa-frontend-lb</code> detecte esses pods. </li></ol><br>  Agora, expressaremos esses requisitos como uma descrição do recurso Deployment. <br><br><h3>  <font color="#3AC1EF">▍ Descrição da implantação</font> </h3><br>  Aqui está uma descrição do YAML de um recurso do tipo Implantação, criado tendo em conta os requisitos de sistema acima: <br><br><pre> <code class="plaintext hljs">apiVersion: extensions/v1beta1 kind: Deployment                                          # 1 metadata: name: sa-frontend spec: replicas: 2                                             # 2 minReadySeconds: 15 strategy:   type: RollingUpdate                                   # 3   rollingUpdate:     maxUnavailable: 1                                   # 4     maxSurge: 1                                         # 5 template:                                               # 6   metadata:     labels:       app: sa-frontend                                  # 7   spec:     containers:       - image: rinormaloku/sentiment-analysis-frontend         imagePullPolicy: Always                         # 8         name: sa-frontend         ports:           - containerPort: 80</code> </pre> <br>  Vamos analisar esta descrição: <br><br><ol><li>  <code>Kind</code> : diz aqui que estamos descrevendo um recurso da visualização <code>Deployment</code> . </li><li>  <code>Replicas</code> : uma propriedade do objeto de especificação de implantação que define quantas instâncias (réplicas) de lares serão executadas. </li><li>  <code>Type</code> : descreve a estratégia usada nesta implantação ao alternar da versão atual para uma nova.  <code>RollingUpdate</code> estratégia <code>RollingUpdate</code> fornece zero tempo de inatividade do sistema durante as atualizações. </li><li>  <code>MaxUnavailable</code> : esta é uma propriedade do objeto <code>RollingUpdate</code> , que define o número máximo de lares indisponíveis (comparado ao número desejado de lares) ao executar uma atualização seqüencial do sistema.  Em nossa implantação, que implica a presença de duas réplicas, o valor dessa propriedade indica que após a conclusão de um pod, outro pod será executado, o que torna o aplicativo disponível durante a atualização. </li><li>  <code>MaxSurge</code> : esta é uma propriedade do objeto <code>RollingUpdate</code> que descreve o número máximo de fornos que podem ser adicionados a uma implantação (em comparação com um determinado número de fornos).  No nosso caso, seu valor 1 significa que, ao mudar para uma nova versão do programa, podemos adicionar outra sub para o cluster, o que levará ao fato de que até três lareiras podem ser lançadas simultaneamente. </li><li>  <code>Template</code> : este objeto define o modelo de lareira que o recurso de <code>Deployment</code> descrito utilizará para criar novas lareiras.  Você provavelmente encontrará essa configuração familiar. </li><li>  <code>app: sa-frontend</code> : etiqueta para lareiras criadas de acordo com um determinado padrão. </li><li>  <code>ImagePullPolicy</code> : define a ordem do trabalho com imagens.  No nosso caso, essa propriedade é configurada como <code>Always</code> , ou seja, durante cada implantação, a imagem correspondente será baixada do repositório. </li></ol><br>  Tendo examinado tudo isso, vamos continuar praticando.  Execute a implantação: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml deployment "sa-frontend" created</code> </pre> <br>  Verifique o status do sistema: <br><br><pre> <code class="plaintext hljs">kubectl get pods NAME                           READY STATUS RESTARTS AGE sa-frontend                    1/1 Running 0 2d sa-frontend-5d5987746c-ml6m4   1/1 Running 0 1m sa-frontend-5d5987746c-mzsgg   1/1 Running 0 1m sa-frontend2                   1/1 Running 0 2d</code> </pre> <br>  Como você pode ver, agora temos 4 pods.  Dois deles foram criados usando o recurso Deployment, outros dois são aqueles que nós mesmos criamos.  Agora você pode remover os pods criados por nós usando comandos do seguinte tipo: <br><br><pre> <code class="plaintext hljs">kubectl delete pod &lt;pod-name&gt;</code> </pre> <br>  A propósito, aqui está uma tarefa para um trabalho independente.  Exclua uma das lareiras criadas usando o recurso Deployment e monitore o sistema.  Pense nas razões do que está acontecendo antes de ler mais. <br><br>  Ao excluir uma lareira, o recurso de Implantação descobre que o estado atual do sistema (1 sub) é diferente do desejado (2 sub), portanto, outra sub é iniciada. <br><br>  Qual é a utilização dos recursos de implantação, além do fato de que, quando usado, o sistema é mantido no estado correto?  Considere os pontos fortes desses recursos. <br><br><h3>  <font color="#3AC1EF">▍ Executando implantações com zero tempo de inatividade do sistema</font> </h3><br>  Suponha que um gerente de produto chegue até nós e relate que o cliente para quem criamos este produto deseja um botão verde no aplicativo cliente.  Os desenvolvedores implementam esse requisito e nos dão a única coisa que precisamos deles - um contêiner de imagem chamado <code>rinormaloku/sentiment-analysis-frontend:green</code> .  Agora chega a nossa hora.  Nós, a equipe do DevOps, precisamos implantar o sistema atualizado e garantir zero tempo de inatividade.  Agora vamos ver se os esforços para desenvolver e configurar o recurso Deployment são justificados. <br><br>  Edite o arquivo <code>sa-frontend-deployment.yaml</code> , substituindo o nome do contêiner de imagem por um novo, com <code>rinormaloku/sentiment-analysis-frontend:green</code> , salve esse arquivo como <code>sa-frontend-deployment-green.yaml</code> e execute o seguinte comando: <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment-green.yaml --record deployment "sa-frontend" configured</code> </pre> <br>  Verifique o status do sistema com o seguinte comando: <br><br><pre> <code class="plaintext hljs">kubectl rollout status deployment sa-frontend Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 of 2 updated replicas are available... deployment "sa-frontend" successfully rolled out</code> </pre> <br>  De acordo com os dados exibidos em resposta a este comando, podemos concluir que a implantação da atualização foi bem-sucedida.  Durante a atualização, as réplicas antigas, uma de cada vez, foram substituídas por novas.  ,   ,    ,   .     ,    ,    . <br><br><h4>   </h4><br>      ,     ,     : <br><br><pre> <code class="plaintext hljs">minikube service sa-frontend-lb</code> </pre> <br>       ,      . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/777/489/902/777489902694f46438ceae41ce59db9b.png"></div><br> <i><font color="#999999"> </font></i> <br><br>  ,     ,  —    . <br><br><h4>      RollingUpdate </h4><br>  ,     <code>kubectl apply -f sa-frontend-deployment-green.yaml --record</code> , Kubernetes   ,     ,    .         ,       ,    <code>rinormaloku/sentiment-analysis-frontend:green</code> .       ,    ,   . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/465/7ee/290/4657ee29097dc99e4fa2a0ebd9180e7a.png"></div><br> <i><font color="#999999">     </font></i> <br><br>  <code>RollingUpdate</code>       ,  ,     <code>maxUnavailable: 1</code>  <code>maxSurge: 1</code> .  ,   Deployment ,     ,    ,     .  ,    ,    ,         . <br><br>         Deployment.     ,   .      . <br><br><h3> <font color="#3AC1EF">▍    </font> </h3><br>   ,   ,   . «!  !    !», —  .        . ,   ,      : <br><br><pre> <code class="plaintext hljs">kubectl rollout history deployment sa-frontend deployments "sa-frontend" REVISION  CHANGE-CAUSE 1         &lt;none&gt;    2         kubectl.exe apply --filename=sa-frontend-deployment-green.yaml --record=true</code> </pre> <br>          : «,    ,    ?». <br><br> «.  ,   ?», —   . <br><br>  ,         ,     : <br><br><pre> <code class="plaintext hljs">kubectl rollout undo deployment sa-frontend --to-revision=1 deployment "sa-frontend" rolled back</code> </pre> <br>      .   ,      . <br><br>       . <br><br>       . <br><br> ! <br><br>   ,  .   Kubernetes         ,  ,      . ,   ! <br><br>           . ,       .  <code>CHANGE-CAUSE</code>      <code>&lt;none&gt;</code> ,    — <code>kubectl.exe apply –filename=sa-frontend-deployment-green.yaml –record=true</code> ? <br><br>   ,         -- <code>record</code>     ,    . <br><br>       ,   ,  ,      . <br><br><h2> <font color="#3AC1EF">   Kubernetes:    </font> </h2><br>      Kubernetes,    ,     .      ,     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7cb/af4/880/7cbaf4880d435df50761d22508f61e83.png"></div><br> <i><font color="#999999">  </font></i> <br><br>       . <br><br><h3> <font color="#3AC1EF">▍  sa-logic</font> </h3><br>        <code>resource-manifests</code>    : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-logic-deployment.yaml --record deployment "sa-logic" created</code> </pre> <br>  <code>sa-logic</code>   .     Python-.    <code>app: sa-logic</code> .          <code>sa-logic</code> ,   .   <code>sa-logic-deployment.yaml</code>     . <br><br>  -,        ,      —  <code>sa-logic</code> . <br><br><h3> <font color="#3AC1EF">▍ sa-logic</font> </h3><br>   ,       Service.   ,   Java-,        <code>sa-webapp</code> ,      ,  Python-.  ,    ,       ,     Python-,   .     ,  ,  ,  . <br><br>      , ,    ,       ,   .  ,      <code>sa-logic</code>   ,       <code>sa-logic</code> . <br><br>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-logic.yaml service "sa-logic" created</code> </pre> <br>    ,        . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/693/79b/3e3/69379b3e373ad1bc728242db341411ab.png"></div><br> <i><font color="#999999">  </font></i> <br><br>   <code>sa-logic</code> ,   <code>sa-webapp</code> ,    ,    . <br><br>   <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍  sa-webapp</font> </h3><br>      ,          Deployment    - . ,     <code>sa-web-app-deployment.yaml</code> ,      : <br><br><pre> <code class="plaintext hljs">- image: rinormaloku/sentiment-analysis-web-app imagePullPolicy: Always name: sa-web-app env:   - name: SA_LOGIC_API_URL     value: "http://sa-logic" ports:   - containerPort: 8080</code> </pre> <br>     <code>env</code> ?  ,   ,  ,   <code>SA_LOGIC_API_URL</code>   <code>http://sa-logic</code> .   ,    ,       .    ? <br><br>             kube-dns. <br><br><h3> <font color="#3AC1EF">▍DNS-  Kubernetes</font> </h3><br>  Kubernetes   ,   <code>kube-dns</code> .        DNS-.     <code>kube-dns</code>   ,     DNS-    . <br><br>  ,      <code>sa-logic</code> ,   IP-.  <code>kube-dns</code>        IP- .        <code>http://sa-logic</code>  IP-. <br><br>      Deployment <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍  sa-webapp</font> </h3><br>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-web-app-deployment.yaml --record deployment "sa-web-app" created</code> </pre> <br>         <code>sa-webapp</code>   ,   .   React-    ,       <code>sa-webapp</code> . <br><br><h3> <font color="#3AC1EF">▍ sa-webapp</font> </h3><br>     <code>service-sa-web-app-lb.yaml</code> ,  ,  ,    ,   . ,   ,   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f service-sa-web-app-lb.yaml service "sa-web-app-lb" created</code> </pre> <br>    . ,     ,      . ,     <code>sa-frontend</code> ,        Java- <code>sa-webapp</code> ,    <code>http://localhost:8080/sentiment</code> .      ,       ,   <code>sa-webapp</code> ,    React-  ,     Java-. <br><br>           ,     . ,          —  ,    ,     . <br><br>  ,       : <br><br><ol><li>  IP-   <code>sa-webapp</code> ,   : <br><br> <code>minikube service list <br> |-------------|----------------------|-----------------------------| <br> |  NAMESPACE  | NAME         | URL       | <br> |-------------|----------------------|-----------------------------| <br> | default     | kubernetes         | No node port       | <br> | default     | sa-frontend-lb       | http://192.168.99.100:30708 | <br> | default     | sa-logic         | No node port       | <br> | default     | sa-web-app-lb        | http://192.168.99.100:31691 | <br> | kube-system | kube-dns             | No node port | <br> | kube-system | kubernetes-dashboard | http://192.168.99.100:30000 | <br> |-------------|----------------------|-----------------------------|</code> </li> <li>   IP-   <code>sa-frontend/src/App.js</code> .   ,     : <br><br><pre> <code class="plaintext hljs">analyzeSentence() {       fetch('http://192.168.99.100:31691/sentiment', { /*    */})           .then(response =&gt; response.json())           .then(data =&gt; this.setState(data));   }</code> </pre> </li><li>  React-,       <code>sa-frontend</code>    <code>npm run build</code> . </li><li>   : <br><br><pre> <code class="plaintext hljs">docker build -f Dockerfile -t $DOCKER_USER_ID/sentiment-analysis-frontend:minikube.</code> </pre> </li><li>     Docker Hub: <br><br><pre> <code class="plaintext hljs">docker push $DOCKER_USER_ID/sentiment-analysis-frontend:minikube</code> </pre> </li><li>   <code>sa-frontend-deployment.yaml</code> ,       . </li><li>   : <br><br><pre> <code class="plaintext hljs">kubectl apply -f sa-frontend-deployment.yaml</code> </pre> </li></ol><br>     ,   , ,      ,    <code>minikube service sa-frontend-lb</code> .  ,   - . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/53d/19c/3ba/53d19c3bac2f8cdd66213c9b34e7b05b.png"></div><br> <i><font color="#999999">  </font></i> <br><br><h2>  <font color="#3AC1EF">Sumário</font> </h2><br>   Kubernetes     ,        ,   ,   ,     .  Kubernetes   ,     ,           .    Kubernetes  Supernetes. <br><br>    ,   : <br><br><ul><li> ,    ,   React, Java  Python. </li><li>    Docker,  ,        <code>Dockerfile</code> . </li><li>    ,  ,  Docker Hub. </li></ul><br>  ,     Kubernetes: <br><br><ul><li>  </li><li>  </li><li>  </li><li>           </li><li>   </li></ul><br>      ,   ,   Kubernetes. <br><br>  <b>Caros leitores!</b>    Kubernetes? <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><img src="https://habrastorage.org/files/1ba/550/d25/1ba550d25e8846ce8805de564da6aa63.png"></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt438984/">https://habr.com/ru/post/pt438984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt438974/index.html">Realidade virtual ajuda a lidar com transtornos mentais</a></li>
<li><a href="../pt438976/index.html">O livro "Primavera. Todos os padrões de design »</a></li>
<li><a href="../pt438978/index.html">Aprendendo sempre e em qualquer lugar! Podcasts para desenvolvedores em inglês</a></li>
<li><a href="../pt438980/index.html">Spring Boot 2: o que há de novo?</a></li>
<li><a href="../pt438982/index.html">Guia Kubernetes, Parte 1: Aplicativos, Microsserviços e Contêineres</a></li>
<li><a href="../pt438986/index.html">Tutorial Reagir Parte 14: Workshop sobre componentes baseados em classe, status do componente</a></li>
<li><a href="../pt438988/index.html">Tutorial Reagir Parte 15: Workshops sobre o estado dos componentes</a></li>
<li><a href="../pt438992/index.html">Diário do desenvolvedor ou decisões incorretas</a></li>
<li><a href="../pt438994/index.html">Intel Xeon W-3175X, um baterista quente. Teste</a></li>
<li><a href="../pt438996/index.html">Rede da empresa e MitM. Parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>