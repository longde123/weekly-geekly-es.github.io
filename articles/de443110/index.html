<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëåüèΩ üë©üèø‚Äçüíª üçô Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit GlusterFS & MetalLB. Teil 2/3 üòï üê• üë®üèª‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Teil 1/3 hier 
 Teil 3/3 hier 


 Hallo und willkommen zur√ºck! Dies ist der zweite Teil des Artikels zum Einrichten eines Kubernetes-Clusters auf Bare...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Konfigurieren Sie den Kubernetes HA-Cluster auf Bare Metal mit GlusterFS & MetalLB. Teil 2/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Teil 1/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> <br>  <strong>Teil 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>hier</strong></a> </p><br><p>  Hallo und willkommen zur√ºck!  Dies ist der zweite Teil des Artikels zum Einrichten eines Kubernetes-Clusters auf Bare-Metal.  Zuvor haben wir den Kubernetes HA-Cluster mithilfe von externem etcd, Master-Master und Lastausgleich konfiguriert.  Nun ist es an der Zeit, eine zus√§tzliche Umgebung und Dienstprogramme einzurichten, um den Cluster n√ºtzlicher und so nah wie m√∂glich am Arbeitszustand zu machen. </p><br><p>  In diesem Teil des Artikels konzentrieren wir uns auf die Konfiguration des internen Load Balancers von Clusterdiensten - dies ist MetalLB.  Wir werden auch den verteilten Dateispeicher zwischen unseren Arbeitsknoten installieren und konfigurieren.  Wir werden GlusterFS f√ºr persistente Volumes verwenden, die in Kubernetes verf√ºgbar sind. <br>  Nachdem Sie alle Schritte ausgef√ºhrt haben, sieht unser Clusterdiagramm folgenderma√üen aus: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Richten Sie MetalLB als internen Load Balancer ein. </h3><br><p>  Ein paar Worte zu MetalLB direkt von der Dokumentseite: </p><br><blockquote> MetalLB ist eine Load-Balancer-Implementierung f√ºr Kubernetes Bare-Metal-Cluster mit Standard-Routing-Protokollen. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes</a> bietet keine Implementierung von Netzwerklastenausgleichern ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Diensttyp LoadBalancer</a> ) f√ºr Bare Metal an.  Alle Implementierungsoptionen von Network LB, mit denen Kubernetes geliefert wird, sind Middleware und greifen auf verschiedene IaaS-Plattformen (GCP, AWS, Azure usw.) zu.  Wenn Sie nicht auf einer von IaaS unterst√ºtzten Plattform (GCP, AWS, Azure usw.) arbeiten, bleibt der LoadBalancer bei der Erstellung auf unbestimmte Zeit im Standby-Status. <br><br>  BM-Serverbetreiber verf√ºgen √ºber zwei weniger effektive Tools zum Eingeben des Benutzerverkehrs in ihre Cluster: NodePort- und externalIPs-Dienste.  Beide Optionen weisen erhebliche Produktionsm√§ngel auf, wodurch BM-Cluster zu B√ºrgern zweiter Klasse im Kubernetes-√ñkosystem werden. <br><br>  MetalLB versucht, dieses Ungleichgewicht zu korrigieren, indem es eine Network LB-Implementierung anbietet, die in Standard-Netzwerkger√§te integriert ist, sodass externe Dienste in BM-Clustern auch ‚Äûnur mit maximaler Geschwindigkeit funktionieren‚Äú. </blockquote><p>  Mit diesem Tool starten wir daher Dienste im Kubernetes-Cluster mithilfe eines Load Balancers, wof√ºr wir uns beim MetalLB-Team bedanken.  Der Einrichtungsprozess ist sehr einfach und unkompliziert. </p><br><p>  Zu Beginn des Beispiels haben wir das Subnetz 192.168.0.0/24 f√ºr die Anforderungen unseres Clusters ausgew√§hlt.  Nehmen Sie nun einen Teil dieses Subnetzes f√ºr den zuk√ºnftigen Load Balancer. </p><br><p>  Wir betreten das Maschinensystem mit dem konfigurierten <strong>kubectl-</strong> Dienstprogramm und f√ºhren <strong>Folgendes</strong> aus: </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  Dadurch wird MetalLB im Cluster im <code>metallb-system</code> .  Stellen Sie sicher, dass alle MetalLB-Komponenten ordnungsgem√§√ü funktionieren: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Konfigurieren Sie nun MetalLB mit configmap.  In diesem Beispiel verwenden wir die Layer 2-Anpassung. Informationen zu anderen Anpassungsoptionen finden Sie in der MetalLB-Dokumentation. </p><br><p>  Erstellen Sie die <strong>Datei metallb-config.yaml</strong> in einem beliebigen Verzeichnis innerhalb des ausgew√§hlten IP-Bereichs des Subnetzes unseres Clusters: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p>  Und wenden Sie diese Einstellung an: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  √úberpr√ºfen und √§ndern Sie die Konfigurationskarte sp√§ter, falls erforderlich: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  Jetzt haben wir unseren eigenen konfigurierten lokalen Load Balancer.  Lassen Sie uns am Beispiel des Nginx-Dienstes sehen, wie es funktioniert. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Erstellen Sie dann eine Testbereitstellung und einen Nginx-Dienst: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  Und jetzt - √ºberpr√ºfen Sie das Ergebnis: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Es wurden 3 Nginx-Pods erstellt, wie in der vorherigen Bereitstellung angegeben.  Der Nginx-Dienst leitet den Verkehr gem√§√ü dem zyklischen Ausgleichsschema zu all diesen Pods.  Sie k√∂nnen auch die externe IP sehen, die von unserem MetalLB-Load-Balancer empfangen wurde. </p><br><p>  Versuchen Sie nun, auf die IP-Adresse 192.168.0.240 aufzurollen, und Sie sehen die Seite Nginx index.html.  Denken Sie daran, die Testbereitstellung und den Nginx-Dienst zu entfernen. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Nun, das ist alles mit MetalLB. Fahren wir fort. Wir konfigurieren GlusterFS f√ºr Kubernetes-Volumes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Konfigurieren von GlusterFS mit Heketi auf Arbeitsknoten. </h3><br><p>  Tats√§chlich kann der Kubernetes-Cluster nicht ohne darin enthaltene Volumes verwendet werden.  Wie Sie wissen, sind Herde kurzlebig, d.h.  Sie k√∂nnen jederzeit erstellt und gel√∂scht werden.  Alle darin enthaltenen Daten gehen verloren.  In einem realen Cluster ist daher verteilter Speicher erforderlich, um den Austausch von Einstellungen und Daten zwischen Knoten und Anwendungen in diesem Cluster sicherzustellen. </p><br><p>  In Kubernetes stehen Volumes auf verschiedene Arten zur Verf√ºgung. W√§hlen Sie die gew√ºnschten aus.  In diesem Beispiel werde ich zeigen, wie GlusterFS-Speicher f√ºr interne Anwendungen erstellt wird. Dies √§hnelt persistenten Volumes.  Fr√ºher habe ich daf√ºr die Systeminstallation von GlusterFS auf allen Kubernetes-Arbeitsknoten verwendet und dann einfach Volumes wie hostPath in GlusterFS-Verzeichnissen erstellt. </p><br><p>  Jetzt haben wir ein neues handliches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><strong>Heketi-</strong></a> Tool. </p><br><p>  Ein paar Worte aus der Heketi-Dokumentation: </p><br><blockquote>  RESTful Volume Management-Infrastruktur f√ºr GlusterFS. <br><br>  Heketi bietet eine RESTful-Verwaltungsschnittstelle, mit der der Lebenszyklus von GlusterFS-Volumes verwaltet werden kann.  Dank Heketi k√∂nnen Cloud-Dienste wie OpenStack Manila, Kubernetes und OpenShift GlusterFS-Volumes dynamisch mit jeder unterst√ºtzten Zuverl√§ssigkeit bereitstellen.  Heketi ermittelt automatisch die Position von Bl√∂cken in einem Cluster und gibt die Position von Bl√∂cken und deren Replikaten in verschiedenen Fehlerbereichen an.  Heketi unterst√ºtzt auch eine beliebige Anzahl von GlusterFS-Clustern, sodass Cloud-Dienste Online-Dateispeicherung anbieten k√∂nnen, nicht nur einen einzelnen GlusterFS-Cluster. </blockquote><p>  Es h√∂rt sich gut an und au√üerdem wird dieses Tool unseren VM-Cluster n√§her an die gro√üen Cloud-Cluster von Kubernetes bringen.  Am Ende k√∂nnen Sie <strong>PersistentVolumeClaims</strong> erstellen, die automatisch generiert werden, und vieles mehr. </p><br><p>  Sie k√∂nnen zus√§tzliche Systemfestplatten verwenden, um GlusterFS zu konfigurieren, oder einfach einige Dummy-Block-Ger√§te erstellen.  In diesem Beispiel werde ich die zweite Methode verwenden. </p><br><p>  Erstellen Sie Dummy-Block-Ger√§te auf allen drei Arbeitsknoten: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  Sie erhalten eine Datei mit einer Gr√∂√üe von ca. 10 GB.  Verwenden Sie dann <strong>losetup</strong> , um es diesen Knoten als Loopback-Ger√§t hinzuzuf√ºgen: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Bitte beachten Sie: Wenn Sie bereits eine Art Loopback-Ger√§t 0 haben, m√ºssen Sie eine andere Nummer ausw√§hlen.</em> </blockquote><p>  Ich nahm mir Zeit und fand heraus, warum Heketi nicht richtig arbeiten will.  Um Probleme in zuk√ºnftigen Konfigurationen zu vermeiden, stellen Sie zun√§chst sicher, dass wir das Kernelmodul <strong>dm_thin_pool</strong> geladen und das Paket <strong>glusterfs-client</strong> auf allen Arbeitsknoten installiert haben. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Nun m√ºssen Sie die Datei <strong>/ home / gluster / image</strong> und das Ger√§t <strong>/ dev / loop0</strong> auf allen Arbeitsknoten haben.  Denken Sie daran, einen systemd-Dienst zu erstellen, der bei jedem Start dieser Server automatisch <strong>losetup</strong> und <strong>modprobe startet</strong> . </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  Und schalten Sie es ein: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  Die Vorbereitungsarbeiten sind abgeschlossen und wir sind bereit, GlusterFS und Heketi in unserem Cluster bereitzustellen.  Daf√ºr werde ich diese coole <a href="">Anleitung verwenden</a> .  Die meisten Befehle werden von einem externen Steuercomputer gestartet, und sehr kleine Befehle werden von jedem Masterknoten innerhalb des Clusters gestartet. </p><br><p>  Kopieren Sie zun√§chst das Repository und erstellen Sie DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Markieren wir nun unsere drei Arbeitsknoten f√ºr GlusterFS.  Nach dem Beschriften werden GlusterFS-Pods erstellt: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Erstellen Sie jetzt ein Heketi-Dienstkonto: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  Wir bieten f√ºr dieses Servicekonto die M√∂glichkeit, Gluster-Pods zu verwalten.  Erstellen Sie dazu eine Clusterfunktion, die f√ºr unser neu erstelltes Dienstkonto erforderlich ist: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Erstellen wir nun einen geheimen Kubernetes-Schl√ºssel, der die Konfiguration unserer Heketi-Instanz blockiert: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Erstellen Sie unter Heketi die erste Quelle, die wir f√ºr die ersten Setup-Vorg√§nge verwenden, und l√∂schen Sie anschlie√üend: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  Nach dem Erstellen und Starten des Bootstrap Heketi-Dienstes m√ºssen wir zu einem unserer Hauptknoten wechseln. Dort werden mehrere Befehle ausgef√ºhrt, da sich unser externer Steuerknoten nicht in unserem Cluster befindet und wir nicht auf die Arbeits-Pods und das interne Netzwerk des Clusters zugreifen k√∂nnen. </p><br><p>  Laden Sie zun√§chst das Dienstprogramm heketi-client herunter und kopieren Sie es in den Ordner bin system: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Suchen Sie nun die IP-Adresse des Heketi-Pods und exportieren Sie sie als Systemvariable: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Lassen Sie uns nun Heketi Informationen √ºber den GlusterFS-Cluster bereitstellen, den es verwalten soll.  Wir stellen es √ºber eine Topologiedatei zur Verf√ºgung.  Eine Topologie ist ein JSON-Manifest mit einer Liste aller von GlusterFS verwendeten Knoten, Festplatten und Cluster. </p><br><blockquote>  HINWEIS  <code>kubectl get node</code> Sie sicher, dass <code>hostnames/manage</code> den genauen Namen angibt, wie im Abschnitt <code>kubectl get node</code> , und dass <code>hostnames/storage</code> die IP-Adresse der Speicherknoten ist. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  Dann laden Sie diese Datei herunter: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Als N√§chstes verwenden wir Heketi, um Volumes zum Speichern der Datenbank bereitzustellen.  Der Teamname ist etwas seltsam, aber alles ist in Ordnung.  Erstellen Sie auch ein Heketi-Repository: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  Dies sind alle Befehle, die Sie vom Masterknoten ausf√ºhren m√ºssen.  Kehren wir zum Steuerknoten zur√ºck und fahren von dort fort.  Stellen Sie zun√§chst sicher, dass der zuletzt ausgef√ºhrte Befehl erfolgreich ausgef√ºhrt wurde: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  Und der Heketi-Storage-Copy-Job-Job ist erledigt. </p><br><blockquote>  Wenn auf Ihren Arbeitsknoten derzeit kein <strong>glusterfs-client-</strong> Paket installiert ist, tritt ein Fehler auf. </blockquote><p>  Es ist Zeit, die Heketi Bootstrap-Installationsdatei zu entfernen und eine kleine Bereinigung durchzuf√ºhren: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  In der letzten Phase m√ºssen wir eine langfristige Kopie von Heketi erstellen: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  Wenn auf Ihren Arbeitsknoten derzeit kein glusterfs-client-Paket installiert ist, tritt ein Fehler auf.  Und wir sind fast fertig, jetzt ist die Heketi-Datenbank im GlusterFS-Volume gespeichert und wird nicht jedes Mal zur√ºckgesetzt, wenn der Heketi-Herd neu gestartet wird. </p><br><p>  Um den GlusterFS-Cluster mit dynamischer Ressourcenzuweisung verwenden zu k√∂nnen, m√ºssen Sie eine StorageClass erstellen. </p><br><p>  Lassen Sie uns zun√§chst den Gluster-Speicherendpunkt suchen, der als Parameter an die StorageClass √ºbergeben wird (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Erstellen Sie nun einige Dateien: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Verwenden Sie diese Dateien, um Klasse und PVC zu erstellen: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  Wir k√∂nnen auch das PV-Volumen anzeigen: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Wir haben jetzt ein dynamisch erstelltes GlusterFS-Volume, das <strong>PersistentVolumeClaim zugeordnet ist</strong> , und wir k√∂nnen diese Anweisung in jedem Unterplot verwenden. </p><br><p>  Erstellen Sie eine einfache unter Nginx und testen Sie sie: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Durchsuchen Sie unter (warten Sie einige Minuten, m√∂glicherweise m√ºssen Sie das Bild herunterladen, falls es noch nicht vorhanden ist): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Gehen Sie nun in den Container und erstellen Sie die Datei index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  Sie m√ºssen die interne IP-Adresse des Herdes finden und sich von jedem Masterknoten aus darin einrollen: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  Dabei testen wir einfach unser neues best√§ndiges Volume. </p><br><blockquote>  Einige n√ºtzliche Befehle zum Auschecken des neuen GlusterFS-Clusters sind: <code>heketi-cli cluster list</code> <code>heketi-cli volume list</code> und <code>heketi-cli volume list</code> .  Sie k√∂nnen auf Ihrem Computer ausgef√ºhrt werden, wenn <strong>heketi-cli installiert ist</strong> .  In diesem Beispiel ist dies der Knoten <strong>master1</strong> . </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  Zu diesem Zeitpunkt haben wir erfolgreich einen internen Load Balancer mit Dateispeicher eingerichtet, und unser Cluster befindet sich jetzt n√§her am Betriebszustand. </p><br><p>  Im n√§chsten Teil des Artikels konzentrieren wir uns auf die Erstellung eines Cluster√ºberwachungssystems und starten darin ein Testprojekt, um alle von uns konfigurierten Ressourcen zu nutzen. </p><br><p>  Bleiben Sie in Kontakt und alles Gute! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de443110/">https://habr.com/ru/post/de443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de443098/index.html">Daten werden mit Magneten und Lasern auf die Disc geschrieben</a></li>
<li><a href="../de443100/index.html">Fehler im Windows-Rechner z√§hlen</a></li>
<li><a href="../de443102/index.html">Verhaltens√§nderung als Produkt: Warum sammelt Marie Kondo mit Sequoia Capital eine 40-Millionen-Dollar-Runde?</a></li>
<li><a href="../de443104/index.html">Berechnen Sie symbolische Ausdr√ºcke mit unscharfen Dreieckszahlen in Python</a></li>
<li><a href="../de443106/index.html">Angek√ºndigt USB4: Was ist √ºber den Standard bekannt</a></li>
<li><a href="../de443112/index.html">Sind Sie sicher, dass Sie Ihrem VPN vertrauen k√∂nnen?</a></li>
<li><a href="../de443114/index.html">DevProject Award: Meine Rede auf der DeveloperWeek 2019</a></li>
<li><a href="../de443120/index.html">Die Staatsduma wird den Kampf gegen den illegalen Verkauf von SIM-Karten fortsetzen</a></li>
<li><a href="../de443122/index.html">Verlust von 809 Millionen E-Mail-Adressen des Verifications.io-Dienstes aufgrund √∂ffentlich ge√∂ffneter MongoDB</a></li>
<li><a href="../de443124/index.html">React.lazy? Aber was ist, wenn Sie keine Komponente haben?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>