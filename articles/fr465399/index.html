<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïé ü§∑üèø üÖæÔ∏è D√©ployez le stockage distribu√© CEPH et connectez-le √† Kubernetes ü§≥üèº ‚ö°Ô∏è ‚è∞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Partie 1 Nous d√©ployons l'environnement pour travailler avec des microservices. Partie 1 Installer Kubernetes HA sur du m√©tal nu (Debian) 
 Bonjour, c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>D√©ployez le stockage distribu√© CEPH et connectez-le √† Kubernetes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465399/"><p><img src="https://habrastorage.org/webt/w-/ha/t9/w-hat9zskv2ab6vdpumzdxllw8w.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1 Nous d√©ployons l'environnement pour travailler avec des microservices.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 1 Installer Kubernetes HA sur du m√©tal nu (Debian)</a> </p><br><h2>  Bonjour, chers lecteurs de Habr! </h2><br><p>  Dans un article pr√©c√©dent <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">,</a> j'ai expliqu√© comment d√©ployer un cluster de basculement Kubernetes.  Mais le fait est que dans Kubernetes, il est pratique de d√©ployer des applications sans √©tat qui n'ont pas besoin de conserver leur √©tat ou de travailler avec des donn√©es.  Mais dans la plupart des cas, nous devons enregistrer les donn√©es et ne pas les perdre lors du red√©marrage des foyers. <br>  Kubernetes utilise des volumes √† ces fins.  Lorsque nous travaillons avec des solutions cloud Kubernetes, il n'y a pas de probl√®mes particuliers.  Il nous suffit de commander le volume requis aupr√®s de Google, Amazon ou d'un autre fournisseur de cloud et, guid√© par la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> , de connecter les volumes re√ßus aux pods. <br>  Lorsque nous traitons de m√©tal nu, les choses sont un peu plus compliqu√©es.  Aujourd'hui, je veux parler d'une des solutions bas√©es sur l'utilisation de ceph. </p><br><p>  Dans cette publication, je dirai: </p><br><ul><li>  comment d√©ployer le stockage distribu√© ceph </li><li>  Comment utiliser Ceph lorsque vous travaillez avec Kubernetes <a name="habracut"></a></li></ul><br><h2>  Pr√©sentation </h2><br><p>  Pour commencer, je voudrais expliquer √† qui cet article sera utile.  Tout d'abord, pour les lecteurs qui ont d√©ploy√© le cluster selon ma premi√®re publication afin de continuer √† construire une architecture de microservices.  Deuxi√®mement, pour les personnes qui souhaitent essayer de d√©ployer un cluster ceph par leurs propres moyens et d'√©valuer ses performances. </p><br><p>  Dans cette publication, je n'aborderai pas le sujet de la planification de cluster pour tous les besoins, je ne parlerai que des principes et concepts g√©n√©raux.  Je ne m'attarderai pas sur le "tuning" et le deep tuning, il existe de nombreuses publications sur ce sujet, notamment sur le Habr.  L'article sera de nature plus introductive, mais en m√™me temps, il vous permettra d'obtenir une solution de travail que vous pourrez adapter √† vos besoins √† l'avenir. </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Liste des h√¥tes, des ressources h√¥tes, des versions du syst√®me d'exploitation et des logiciels</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Structure du cluster Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Configurer les n≈ìuds de cluster avant l'installation</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installer ceph-deploy</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cr√©ation d'un cluster ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Configuration du r√©seau</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installer les packages ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installation et initialisation de moniteurs</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ajout d'OSD</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Connectez ceph √† kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cr√©ation d'un pool de donn√©es</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cr√©ation d'un secret client</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">D√©ployer ceph rbd provisioner</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cr√©ation d'une classe de stockage</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Test du ligament de Kubernetes + ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Liste des mat√©riaux utilis√©s dans la pr√©paration de l'article</a> </li></ol><br><a name="vm"></a><br><h2>  Liste d'h√¥tes et configuration syst√®me requise </h2><br><div class="scrollable-table"><table><tbody><tr><th>  <b>Nom</b> </th><th>  <b>Adresse IP</b> </th><th>  <b>Commentaire</b> </th></tr><tr><td>  ceph01-test </td><td>  10.73.88.52 </td><td>  ceph-node01 </td></tr><tr><td>  test ceph02 </td><td>  10.73.88.53 </td><td>  ceph-node02 </td></tr><tr><td>  test ceph03 </td><td>  10.73.88.54 </td><td>  ceph-node03 </td></tr></tbody></table></div><br><p>  Lors de la r√©daction d'un article, j'utilise des machines virtuelles avec cette configuration </p><br><p><img src="https://habrastorage.org/webt/kd/-z/bd/kd-zbdwb_76g1vvvgmtojoirseo.png"></p><br><p>  Chacun a un OS Debian 9.5 install√©.  Ce sont des machines de test, chacune avec deux disques, le premier pour l'OS, le second pour l'OSD cef. </p><br><p>  Je d√©ploierai le cluster via l'utilitaire ceph-deploy.  Vous pouvez d√©ployer un cluster ceph en mode manuel, toutes les √©tapes sont d√©crites dans la documentation, mais le but de cet article est de dire √† quelle vitesse vous pouvez d√©ployer ceph et commencer √† l'utiliser dans kubernetes. <br>  Ceph est assez gourmand pour les ressources, en particulier la RAM.  Pour une bonne vitesse, il est conseill√© d'utiliser des disques SSD. </p><br><p>  Vous pouvez en savoir plus sur les exigences dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle de ceph.</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><br></a> </p><br><a name="ceph"></a><br><h2>  Structure du cluster Ceph </h2><br><p>  <strong>LUN</strong> <br>  <em>Un moniteur est un d√©mon qui agit en tant que coordinateur √† partir duquel le cluster commence.</em>  <em>D√®s que nous avons au moins un moniteur fonctionnel, nous avons un cluster Ceph.</em>  <em>Le moniteur stocke des informations sur la sant√© et l'√©tat du cluster en √©changeant diverses cartes avec d'autres moniteurs.</em>  <em>Les clients se tournent vers les moniteurs pour savoir sur quel OSD √©crire / lire les donn√©es.</em>  <em>Lorsque vous d√©ployez un nouveau stockage, la premi√®re chose que vous faites est de cr√©er un moniteur (ou plusieurs).</em>  <em>Le cluster peut vivre sur un seul moniteur, mais il est recommand√© de r√©aliser 3 ou 5 moniteurs afin d'√©viter la chute de l'ensemble du syst√®me due √† la chute d'un seul moniteur.</em>  <em>L'essentiel est que le nombre de ceux-ci soit impair afin d'√©viter les situations de cerveau divis√©.</em>  <em>Les moniteurs fonctionnent dans un quorum, donc si plus de la moiti√© des moniteurs tombent, le cluster sera bloqu√© pour √©viter l'incoh√©rence des donn√©es.</em> <br>  <strong>Mgr</strong> <br>  <em>Le d√©mon Ceph Manager fonctionne avec le d√©mon moniteur pour fournir un contr√¥le suppl√©mentaire.</em> <em><br></em>  <em>Depuis la version 12.x, le d√©mon ceph-mgr est devenu n√©cessaire pour un fonctionnement normal.</em> <em><br></em>  <em>Si le d√©mon mgr n'est pas en cours d'ex√©cution, vous verrez un avertissement √† ce sujet.</em> <br>  <strong>OSD (p√©riph√©rique de stockage d'objets)</strong> <br>  <em>OSD est une unit√© de stockage qui stocke les donn√©es elles-m√™mes et traite les demandes des clients en √©changeant des donn√©es avec d'autres OSD.</em>  <em>Il s'agit g√©n√©ralement d'un disque.</em>  <em>Et g√©n√©ralement, pour chaque OSD, il existe un d√©mon OSD distinct qui peut s'ex√©cuter sur n'importe quelle machine sur laquelle ce disque est install√©.</em> </p><br><p>  Les trois d√©mons fonctionneront sur chaque machine de notre cluster.  En cons√©quence, surveillez et g√©rez les d√©mons en tant que d√©mons de service et OSD pour un lecteur de notre machine virtuelle. </p><br><a name="before"></a><br><h2>  Configurer les n≈ìuds de cluster avant l'installation </h2><br><p>  La documentation de ceph sp√©cifie le workflow suivant: </p><br><p><img src="https://habrastorage.org/webt/wz/9i/gu/wz9igu71hiezyom4zvtbxp6ye30.png"></p><br><p>  Je travaillerai √† partir du premier n≈ìud du cluster ceph01-test, ce sera Admin Node, il contiendra √©galement des fichiers de configuration pour l'utilitaire ceph-deploy.  Pour que l'utilitaire ceph-deploy fonctionne correctement, tous les n≈ìuds de cluster doivent √™tre accessibles via ssh avec le n≈ìud Admin.  Pour plus de commodit√©, j'√©crirai dans les h√¥tes des noms courts pour le cluster </p><br><pre><code class="plaintext hljs">10.73.88.52 ceph01-test 10.73.88.53 ceph02-test 10.73.88.54 ceph03-tset</code> </pre> <br><p>  Et copiez les cl√©s sur les autres h√¥tes.  Toutes les commandes que j'ex√©cuterai depuis root. </p><br><pre> <code class="plaintext hljs">ssh-copy-id ceph02-test ssh-copy-id ceph03-test</code> </pre> <br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation d'installation</a> </p><br><anchor>  ceph-deploy </anchor><br><h2>  Installer ceph-deploy </h2><br><p>  La premi√®re √©tape consiste √† installer ceph-deploy sur la machine de test ceph01 </p><br><pre> <code class="plaintext hljs">wget -q -O- 'https://download.ceph.com/keys/release.asc' | apt-key add -</code> </pre> <br><p>  Ensuite, vous devez choisir la version que vous souhaitez mettre.  Mais ici il y a des difficult√©s, actuellement ceph pour Debian OS ne supporte que les paquets lumineux. <br>  Si vous voulez mettre une version plus r√©cente, vous devrez utiliser un miroir, par exemple <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://mirror.croit.io/debian-mimic/dists/</a> </p><br><p>  Ajouter un r√©f√©rentiel avec mimic sur les trois n≈ìuds </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-deploy</code> </pre> <br><p>  Si lumineux vous suffit, alors vous pouvez utiliser les d√©p√¥ts officiels </p><br><pre> <code class="plaintext hljs">echo deb https://download.ceph.com/debian-luminous/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list apt-transport-https apt update apt install ceph-deploy</code> </pre> <br><p>  Nous installons √©galement NTP sur les trois n≈ìuds. </p><br><div class="spoiler">  <b class="spoiler_title">puisque cette recommandation est dans la documentation de ceph</b> <div class="spoiler_text"><p>  Nous recommandons d'installer NTP sur les n≈ìuds Ceph (en particulier sur les n≈ìuds Ceph Monitor) pour √©viter les probl√®mes li√©s √† la d√©rive de l'horloge. <br></p></div></div><br><pre> <code class="plaintext hljs">apt install ntp</code> </pre> <br><p>  Assurez-vous d'activer le service NTP.  Assurez-vous que chaque n≈ìud Ceph utilise le m√™me serveur NTP.  Vous pouvez voir plus de d√©tails <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> </p><br><a name="ceph-install"></a><br><h2>  Cr√©ation d'un cluster ceph </h2><br><p>  Cr√©er un r√©pertoire pour les fichiers de configuration et les fichiers ceph-deploy </p><br><pre> <code class="plaintext hljs">mkdir my-cluster cd my-cluster</code> </pre> <br><p>  Cr√©ons une nouvelle configuration de cluster, lors de la cr√©ation, indiquons qu'il y aura trois moniteurs dans notre cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy new ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-network"></a><br><h2>  Configuration du r√©seau </h2><br><p>  Maintenant, le point important, il est temps de parler du r√©seau pour ceph.  Ceph utilise deux r√©seaux publics et un r√©seau de clusters pour fonctionner <br><img src="https://habrastorage.org/webt/jz/dp/wz/jzdpwzxrzpdqr7k9v4u53hmc05s.png"></p><br><p>  Comme vous pouvez le voir sur le diagramme du r√©seau public, il s'agit du niveau utilisateur et application, et le r√©seau de cluster est le r√©seau √† travers lequel les donn√©es sont r√©pliqu√©es. <br>  Il est hautement souhaitable de s√©parer ces deux r√©seaux l'un de l'autre.  En outre, le r√©seau de cluster de vitesse de r√©seau est souhaitable d'au moins 10 Go. <br>  Bien s√ªr, vous pouvez tout garder sur le m√™me r√©seau.  Mais cela se heurte au fait que d√®s que le volume de r√©plication entre les OSD augmente, par exemple, lorsque de nouveaux OSD (disques) tombent ou sont ajout√©s, la charge du r√©seau augmentera TR√àS.  La vitesse et la stabilit√© de votre infrastructure d√©pendront donc grandement du r√©seau utilis√© par ceph. <br>  Malheureusement, mon cluster de virtualisation n'a pas de r√©seau s√©par√© et j'utiliserai un segment de r√©seau commun. <br>  La configuration r√©seau du cluster se fait via le fichier de configuration, que nous avons g√©n√©r√© avec la commande pr√©c√©dente. </p><br><pre> <code class="plaintext hljs">/my-cluster# cat ceph.conf [global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Comme nous pouvons le voir, le d√©ploiement cef n'a pas cr√©√© les param√®tres r√©seau par d√©faut pour nous, donc j'ajouterai le param√®tre public network = {public-network / netmask} √† la section globale de la configuration.  Mon r√©seau est 10.73.0.0/16, donc apr√®s avoir ajout√© ma configuration ressemblera √† ceci </p><br><pre> <code class="plaintext hljs">[global] fsid = 2e0d92b0-e803-475e-9060-0871b63b6e7f mon_initial_members = ceph01-test, ceph02-test, ceph03-test mon_host = 10.73.88.52,10.73.88.53,10.73.88.54 public network = 10.73.0.0/16 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx</code> </pre> <br><p>  Si vous souhaitez s√©parer le r√©seau de clusters du public, ajoutez le param√®tre cluster network = {cluster-network / netmask} <br>  Vous <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pouvez en savoir</a> plus sur les r√©seaux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans la documentation.</a> </p><br><a name="ceph-pack"></a><br><h2>  Installer les packages ceph </h2><br><p>  En utilisant ceph-deploy, nous installons tous les packages ceph dont nous avons besoin sur nos trois n≈ìuds. <br>  Pour ce faire, sur ceph01-test, ex√©cutez <br>  Si la version est mim√©tique, </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release mimic ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Si la version est lumineuse alors </p><br><pre> <code class="plaintext hljs">ceph-deploy install --release luminous ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Et attendez que tout soit √©tabli. </p><br><a name="ceph-mon"></a><br><h2>  Installation et initialisation de moniteurs </h2><br><p>  Une fois tous les packages install√©s, nous allons cr√©er et lancer les moniteurs de notre cluster. <br>  C ceph01-test, proc√©dez comme suit </p><br><pre> <code class="plaintext hljs">ceph-deploy mon create-initial</code> </pre> <br><p>  Des moniteurs seront cr√©√©s au cours du processus, des d√©mons seront lanc√©s et ceph-deploy v√©rifiera le quorum. <br>  Maintenant, dispersez les configurations sur les n≈ìuds du cluster. </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Et v√©rifiez l'√©tat de notre cluster, si vous avez tout fait correctement, alors l'√©tat devrait √™tre <br>  HEALTH_OK </p><br><pre> <code class="plaintext hljs">~/my-cluster# ceph status cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs:</code> </pre> <br><p>  Cr√©er mgr </p><br><pre> <code class="plaintext hljs">ceph-deploy mgr create ceph01-test ceph02-test ceph03-test</code> </pre> <br><p>  Et v√©rifiez √† nouveau le statut </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Une ligne devrait appara√Ætre </p><br><pre> <code class="plaintext hljs">mgr: ceph01-test(active), standbys: ceph02-test, ceph03-test</code> </pre> <br><p>  Nous √©crivons la configuration √† tous les h√¥tes du cluster </p><br><pre> <code class="plaintext hljs">ceph-deploy admin ceph01-test ceph02-test ceph03-test</code> </pre> <br><a name="ceph-osd"></a><br><h2>  Ajout d'OSD </h2><br><p>  Nous avons actuellement un cluster fonctionnel, mais il n'a pas encore de disques (osd dans la terminologie ceph) pour stocker les informations. </p><br><p>  L'OSD peut √™tre ajout√© avec la commande suivante (vue g√©n√©rale) </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data {device} {ceph-node}</code> </pre> <br><p>  Dans mon banc d'essai, disk / dev / sdb est allou√© sous osd, donc dans mon cas, les commandes seront les suivantes </p><br><pre> <code class="plaintext hljs">ceph-deploy osd create --data /dev/sdb ceph01-test ceph-deploy osd create --data /dev/sdb ceph02-test ceph-deploy osd create --data /dev/sdb ceph03-test</code> </pre> <br><p>  V√©rifiez que tous les OSD fonctionnent. </p><br><pre> <code class="plaintext hljs">ceph -s</code> </pre> <br><p>  Conclusion </p><br><pre> <code class="plaintext hljs"> cluster: id: 2e0d92b0-e803-475e-9060-0871b63b6e7f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01-test,ceph02-test,ceph03-test mgr: ceph01-test(active) osd: 3 osds: 3 up, 3 in</code> </pre> <br><p>  Vous pouvez √©galement essayer quelques commandes utiles pour l'OSD. </p><br><pre> <code class="plaintext hljs">ceph osd df ID CLASS WEIGHT REWEIGHT SIZE USE AVAIL %USE VAR PGS 0 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 1 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 2 hdd 0.00490 1.00000 5.0 GiB 1.0 GiB 4.0 GiB 20.05 1.00 0 TOTAL 15 GiB 3.0 GiB 12 GiB 20.05</code> </pre> <br><p>  et </p><br><pre> <code class="plaintext hljs">ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.01469 root default -3 0.00490 host ceph01-test 0 hdd 0.00490 osd.0 up 1.00000 1.00000 -5 0.00490 host ceph02-test 1 hdd 0.00490 osd.1 up 1.00000 1.00000 -7 0.00490 host ceph03-test 2 hdd 0.00490 osd.2 up 1.00000 1.00000</code> </pre><br><p>  Si tout va bien, alors nous avons un cluster ceph fonctionnel.  Dans la partie suivante, je dirai comment utiliser ceph avec kubernetes </p><br><a name="kubernetes"></a><br><h1>  Connectez ceph √† kubernetes </h1><br><p>  Malheureusement, je ne serai pas en mesure de d√©crire en d√©tail le fonctionnement des volumes Kubernetes dans cet article, je vais donc essayer de tenir dans un paragraphe. <br>  Kubernetes utilise des classes de stockage pour travailler avec des volumes de donn√©es, chaque classe de stockage a son propre provisionneur, vous pouvez le consid√©rer comme une sorte de ¬´pilote¬ª pour travailler avec diff√©rents volumes de stockage de donn√©es.  La liste compl√®te qui prend en charge kubernetes se trouve dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation officielle</a> . <br>  Kubernetes lui-m√™me prend √©galement en charge le travail avec rbd, mais l'image officielle de kube-controller-manager n'a pas de client rbd install√©, vous devez donc utiliser une image diff√©rente. <br>  Il convient √©galement de noter que les volumes (pvc) cr√©√©s en tant que rbd ne peuvent √™tre que ReadWriteOnce (RWO) et, ce qui signifie que vous pouvez monter le volume cr√©√© UNIQUEMENT sur un seul foyer. </p><br><p>  Pour que notre cluster puisse fonctionner avec des volumes ceph, nous avons besoin de: <br>  dans un cluster Ceph: </p><br><ul><li>  cr√©er un pool de donn√©es dans le cluster ceph </li><li>  cr√©er un client et une cl√© d'acc√®s au pool de donn√©es </li><li>  obtenir ceph admin secret </li></ul><br><p>  Pour que notre cluster puisse fonctionner avec des volumes ceph, nous avons besoin de: <br>  dans un cluster Ceph: </p><br><ul><li>  cr√©er un pool de donn√©es dans le cluster ceph </li><li>  cr√©er un client et une cl√© d'acc√®s au pool de donn√©es </li><li>  obtenir ceph admin secret </li></ul><br><p>  dans le cluster Kubernetes: </p><br><ul><li>  cr√©er le secret administrateur ceph et la cl√© client ceph </li><li>  installez ceph rbd provisioner ou remplacez l'image kube-controller-manager par une image qui prend en charge rbd </li><li>  cr√©er un secret avec la cl√© client ceph </li><li>  cr√©er une classe de stockage </li><li>  installer ceph-common sur les notes de travail de kubernetes </li></ul><br><a name="ceph-pool"></a><br><h2>  Cr√©ation d'un pool de donn√©es </h2><br><p>  Dans le cluster ceph, cr√©ez un pool pour les volumes kubernetes </p><br><pre> <code class="plaintext hljs">ceph osd pool create kube 8 8</code> </pre> <br><p>  Ici, je vais faire une petite explication, les chiffres 8 8 √† la fin sont les nombres de pg et pgs.  Ces valeurs d√©pendent de la taille de votre cluster ceph.  Il existe des calculatrices sp√©ciales qui calculent la quantit√© de pg et de pgs, par exemple, officielle de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ceph</a> <br>  Pour commencer, je recommande de le laisser par d√©faut, si √† l'avenir ce montant peut √™tre augment√© (il ne peut √™tre r√©duit qu'√† partir de la version Nautilus). </p><br><a name="ceph-key"></a><br><h2>  Cr√©ation d'un client pour un pool de donn√©es </h2><br><p>  Cr√©er un client pour le nouveau pool </p><br><pre> <code class="plaintext hljs">ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=kube'</code> </pre> <br><p>  Nous recevrons une cl√© pour le client, √† l'avenir nous en aurons besoin pour cr√©er un kubernetes secret </p><br><pre> <code class="plaintext hljs">ceph auth get-key client.kube AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg==</code> </pre> <br><h2>  Obtenir la cl√© d'administration </h2><br><p>  Et obtenez la cl√© d'administration </p><br><pre> <code class="plaintext hljs">ceph auth get client.admin 2&gt;&amp;1 |grep "key = " |awk '{print $3'} AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA==</code> </pre> <br><p>  <strong>Sur le cluster ceph, tout le travail est termin√© et maintenant nous devons aller sur une machine qui a acc√®s au cluster kubernetes</strong> <br>  Je vais travailler avec le master01-test (10.73.71.25) du cluster d√©ploy√© par moi dans la premi√®re publication. </p><br><a name="kubernetes-secrets"></a><br><h2>  Cr√©ation d'un secret client </h2><br><p>  Cr√©ez un fichier avec le token client que nous avons re√ßu (n'oubliez pas de le remplacer par votre token) </p><br><pre> <code class="plaintext hljs">echo AQDd5aldka5KJRAAkpWTQYUMQi+5dfGDqSyxkg== &gt; /tmp/key.client</code> </pre> <br><p>  Et cr√©er un secret que nous utiliserons √† l'avenir </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-secret --from-file=/tmp/key.client --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><h2>  Cr√©er un secret administrateur </h2><br><p>  Cr√©ez un fichier avec un token administrateur (n'oubliez pas de le remplacer par votre token) </p><br><pre> <code class="plaintext hljs">echo AQAv+Itdx4DwKBAAKVhWRS3+eEPqV3Xrnlg9KA== &gt; /tmp/key.admin</code> </pre> <br><p>  Apr√®s cela, cr√©ez un secret administrateur </p><br><pre> <code class="plaintext hljs">kubectl create secret generic ceph-admin-secret --from-file=/tmp/key.admin --namespace=kube-system --type=kubernetes.io/rbd</code> </pre> <br><p>  V√©rifiez que des secrets ont √©t√© cr√©√©s </p><br><pre> <code class="plaintext hljs">kubectl get secret -n kube-system | grep ceph ceph-admin-secret kubernetes.io/rbd 1 8m31s ceph-secret kubernetes.io/rbd 1 7m32s</code> </pre> <br><a name="kubernetes-provisioner"></a><br><h2>  M√©thode d√©ployant d'abord ceph rbd provisioner </h2><br><p>  Nous clonons le r√©f√©rentiel kubernetes-incubator / external-storage de github, il a tout ce dont vous avez besoin pour faire des amis du cluster kubernetes avec le r√©f√©rentiel ceph. </p><br><pre> <code class="plaintext hljs">git clone https://github.com/kubernetes-incubator/external-storage.git cd external-storage/ceph/rbd/deploy/ NAMESPACE=kube-system sed -r -i "s/namespace: [^ ]+/namespace: $NAMESPACE/g" ./rbac/clusterrolebinding.yaml ./rbac/rolebinding.yaml</code> </pre> <br><pre> <code class="plaintext hljs">kubectl -n $NAMESPACE apply -f ./rbac</code> </pre> <br><p>  Conclusion </p><br><pre> <code class="plaintext hljs">clusterrole.rbac.authorization.k8s.io/rbd-provisioner created clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created deployment.extensions/rbd-provisioner created role.rbac.authorization.k8s.io/rbd-provisioner created rolebinding.rbac.authorization.k8s.io/rbd-provisioner created serviceaccount/rbd-provisioner created</code> </pre> <br><h2>  M√©thode deux: remplacer l'image kube-controller-manager </h2><br><p>  Il n'y a pas de support rbd dans l'image officielle de kube-controller-manager, nous devrons donc changer l'image de controller-manager. <br>  Pour ce faire, sur chacun des assistants Kubernetes, vous devez modifier le fichier kube-controller-manager.yaml et remplacer l'image par gcr.io/google_containers/hyperkube:v1.15.2.  Faites attention √† la version de l'image qui doit correspondre √† votre version du cluster Kubernetes. </p><br><pre> <code class="plaintext hljs">vim /etc/kubernetes/manifests/kube-controller-manager.yaml</code> </pre> <br><p>  Apr√®s cela, vous devrez red√©marrer kube-controller-manager </p><br><pre> <code class="plaintext hljs">ubectl get pods -A | grep manager kube-system kube-controller-manager-master01-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master02-test 1/1 Running 0 5m54s kube-system kube-controller-manager-master03-test 1/1 Running 9111 103d</code> </pre> <br><p>  Les pods doivent √™tre mis √† jour automatiquement, mais si pour une raison quelconque cela ne s'est pas produit, vous pouvez les recr√©er manuellement, par suppression. </p><br><pre> <code class="plaintext hljs">kubectl delete pod -n kube-system kube-controller-manager-master01-test kubectl delete pod -n kube-system kube-controller-manager-master02-test kubectl delete pod -n kube-system kube-controller-manager-master03-test</code> </pre> <br><p>  V√©rifiez que tout va bien </p><br><pre> <code class="plaintext hljs">kubectl describe pod -n kube-system kube-controller-manager-master02-test | grep Image: Image: gcr.io/google_containers/hyperkube:v1.15.2</code> </pre> <br><p>  - </p><br><a name="storage-class"></a><br><h2>  Cr√©ation d'une classe de stockage </h2><br><p>  <strong>Premi√®re m√©thode - si vous avez utilis√© le provisionneur ceph.com/rbd</strong> <br>  Cr√©ez un fichier yaml avec une description de notre classe de stockage.  De plus, tous les fichiers utilis√©s ci-dessous peuvent √™tre t√©l√©charg√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans mon r√©f√©rentiel</a> dans le r√©pertoire ceph </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: ceph.com/rbd parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Et l'inclure dans notre cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class.yaml</code> </pre> <br><p>  V√©rifiez que tout va bien </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd ceph.com/rbd 7s</code> </pre> <br><p>  <strong>Deuxi√®me m√©thode - si vous avez utilis√© le provisionneur kubernetes.io/rbd</strong> <br>  Cr√©er une classe de stockage </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./storage-class-hyperkube.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-rbd provisioner: kubernetes.io/rbd allowVolumeExpansion: true parameters: monitors: 10.73.88.52:6789, 10.73.88.53:6789, 10.73.88.54:6789 pool: kube adminId: admin adminSecretNamespace: kube-system adminSecretName: ceph-admin-secret userId: kube userSecretNamespace: kube-system userSecretName: ceph-secret imageFormat: "2" imageFeatures: layering EOF</code> </pre> <br><p>  Et l'inclure dans notre cluster </p><br><pre> <code class="plaintext hljs">kubectl apply -f storage-class-hyperkube.yaml storageclass.storage.k8s.io/ceph-rbd created</code> </pre> <br><p>  V√©rifiez que tout va bien </p><br><pre> <code class="plaintext hljs">kubectl get sc NAME PROVISIONER AGE ceph-rbd kubernetes.io/rbd 107s</code> </pre> <br><a name="test"></a><br><h2>  Test du ligament de Kubernetes + ceph </h2><br><p>  Avant de tester ceph + kubernetes, vous devez installer le package ceph-common sur CHAQUE code de travail du cluster. </p><br><pre> <code class="plaintext hljs">apt install curl apt-transport-https -y curl https://mirror.croit.io/keys/release.gpg &gt; /usr/share/keyrings/croit-signing-key.gpg echo 'deb [signed-by=/usr/share/keyrings/croit-signing-key.gpg] https://mirror.croit.io/debian-mimic/ stretch main' &gt; /etc/apt/sources.list.d/croit-ceph.list apt update apt install ceph-common</code> </pre> <br><p>  Cr√©er un fichier yaml PersistentVolumeClaim </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./claim.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: claim1 spec: accessModes: - ReadWriteOnce storageClassName: ceph-rbd resources: requests: storage: 1Gi EOF</code> </pre> <br><p>  Tuez-le </p><br><pre> <code class="plaintext hljs">kubectl apply -f claim.yaml</code> </pre> <br><p>  V√©rifiez que PersistentVolumeClaim est cr√©√©. </p><br><pre> <code class="plaintext hljs">bectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO ceph-rbd 44m</code> </pre> <br><p>  Et √©galement cr√©√© automatiquement PersistentVolume. </p><br><pre> <code class="plaintext hljs">kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-d1e47825-289c-4201-acb8-033e62a3fe81 1Gi RWO Delete Bound default/claim1 ceph-rbd 37m</code> </pre> <br><p>  Cr√©ons un pod de test dans lequel nous incluons le pvc cr√©√© dans le r√©pertoire / mnt.  Ex√©cutez ce fichier /mnt/test.txt avec le texte "Hello World!" </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./create-file-pod.yaml kind: Pod apiVersion: v1 metadata: name: create-file-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "echo Hello world! &gt; /mnt/test.txt &amp;&amp; exit 0 || exit 1" volumeMounts: - name: pvc mountPath: "/mnt" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Nous allons le tuer et v√©rifier qu'il a termin√© sa t√¢che </p><br><pre> <code class="plaintext hljs">kubectl apply -f create-file-pod.yaml kubectl get pods -w</code> </pre> <br><p>  Attendons le statut </p><br><pre> <code class="plaintext hljs">create-file-pod 0/1 Completed 0 16s</code> </pre> <br><p>  Cr√©ons-en un autre, connectons notre volume √† celui-ci mais d√©j√† dans / mnt / test, et apr√®s cela assurez-vous que le fichier cr√©√© par le premier volume est en place </p><br><pre> <code class="plaintext hljs">cat &lt;&lt;EOF &gt;./test-pod.yaml kind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: gcr.io/google_containers/busybox:1.24 command: - "/bin/sh" args: - "-c" - "sleep 600" volumeMounts: - name: pvc mountPath: "/mnt/test" restartPolicy: "Never" volumes: - name: pvc persistentVolumeClaim: claimName: claim1 EOF</code> </pre> <br><p>  Ex√©cutez kubectl get po -w et attendez que le pod soit en cours d'ex√©cution <br>  Apr√®s cela, allons-y et v√©rifions que le volume est connect√© et notre fichier dans le r√©pertoire / mnt / test </p><br><pre> <code class="plaintext hljs">kubectl exec test-pod -ti sh cat /mnt/test/test.txt Helo world!</code> </pre> <br><p>  Merci d'avoir lu jusqu'au bout.  D√©sol√© pour le retard dans la publication. <br>  Je suis pr√™t √† r√©pondre √† toutes les questions dans des messages personnels ou dans les r√©seaux sociaux qui sont indiqu√©s sur mon profil. <br>  Dans la prochaine petite publication, je vous dirai comment d√©ployer le stockage S3 en fonction du cluster ceph cr√©√©, puis selon le plan de la premi√®re publication. </p><br><a name="book"></a><br><p>  Mat√©riaux utilis√©s pour la publication </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation officielle Ceph</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pr√©sentation du r√©f√©rentiel Ceph en images</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation officielle de Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">incubateur kubernetes / √† stockage externe</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©f√©rentiel kubernetes-ceph-percona avec des exemples de fichiers</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr465399/">https://habr.com/ru/post/fr465399/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr465379/index.html">Commande autonome sans fil autonome de pompe √† insuline</a></li>
<li><a href="../fr465391/index.html">Sur les traces du mouvement russe Scala. Partie 1</a></li>
<li><a href="../fr465393/index.html">Alimentation par batterie pour les appareils MySensors</a></li>
<li><a href="../fr465395/index.html">Quelle est la principale diff√©rence entre l'injection de d√©pendance et le localisateur de service?</a></li>
<li><a href="../fr465397/index.html">Comment le traducteur Nitro est-il apparu, qui aide les d√©veloppeurs dans la localisation et le support technique</a></li>
<li><a href="../fr465401/index.html">5 activit√©s pour acc√©l√©rer la r√©solution de probl√®mes dans n'importe quelle √©quipe informatique</a></li>
<li><a href="../fr465403/index.html">Achtung! Nouvelles cam√©ras sur la route ou informations √† jour sur les radars et d√©tecteurs de radar</a></li>
<li><a href="../fr465407/index.html">1. Pr√©sentation des commutateurs Extreme Enterprise Layer</a></li>
<li><a href="../fr465409/index.html">Meilleures pratiques Vue.js pour le d√©veloppement Web</a></li>
<li><a href="../fr465415/index.html">Nous parlons de DevOps dans un langage compr√©hensible</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>