<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë∑üèº üèîÔ∏è üë©üèΩ‚ÄçüöÄ Como transformar um jornalista em uma rede neural ou "Segredos de reduzir o texto em Habr√© sem uma palavra" üóª üé® üëêüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="S√≥ n√£o se surpreenda, mas o segundo cabe√ßalho deste post gerou uma rede neural, ou melhor, o algoritmo de sammariza√ß√£o. E o que √© sammariza√ß√£o? 

 Ess...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como transformar um jornalista em uma rede neural ou "Segredos de reduzir o texto em Habr√© sem uma palavra"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/abbyy/blog/479400/"><img src="https://habrastorage.org/webt/iq/f_/fg/iqf_fgkfyqb1rbtwvba0p4gbsiq.png" align="left">  S√≥ n√£o se surpreenda, mas o segundo cabe√ßalho deste post gerou uma rede neural, ou melhor, o algoritmo de sammariza√ß√£o.  E o que √© sammariza√ß√£o? <br><br>  Esse √© um dos principais e cl√°ssicos <a href="https://habr.com/ru/company/abbyy/blog/437008/">desafios do Processamento de linguagem natural (PNL)</a> .  Consiste na cria√ß√£o de um algoritmo que recebe o texto como entrada e gera uma vers√£o resumida dele.  Al√©m disso, a estrutura correta (correspondente √†s normas da l√≠ngua) √© preservada e a id√©ia principal do texto √© transmitida corretamente. <br><br>  Tais algoritmos s√£o amplamente utilizados na ind√∫stria.  Por exemplo, eles s√£o √∫teis para os mecanismos de pesquisa: usando a redu√ß√£o de texto, voc√™ pode entender facilmente se a ideia principal de um site ou documento est√° relacionada a uma consulta de pesquisa.  Eles s√£o usados ‚Äã‚Äãpara procurar informa√ß√µes relevantes em um grande fluxo de dados de m√≠dia e para filtrar o lixo de informa√ß√µes.  A redu√ß√£o de texto ajuda na pesquisa financeira, na an√°lise de contratos legais, anota√ß√£o de artigos cient√≠ficos e muito mais.  A prop√≥sito, o algoritmo de sammariza√ß√£o gerou todos os subt√≠tulos para este post. <br><br>  Para minha surpresa, em Habr√© havia muito poucos artigos sobre sammariza√ß√£o, ent√£o decidi compartilhar minhas pesquisas e resultados nessa dire√ß√£o.  Este ano, participei da pista de corridas da confer√™ncia <a href="http://www.dialog-21.ru/">Dialogue</a> e experimentei geradores de manchetes para not√≠cias e poemas usando redes neurais.  Neste post, irei primeiro abordar brevemente a parte te√≥rica da sammariza√ß√£o e, em seguida, darei exemplos com a gera√ß√£o de t√≠tulos, explicarei quais dificuldades os modelos t√™m ao reduzir o texto e como esses modelos podem ser aprimorados para obter melhores t√≠tulos. <br><a name="habracut"></a><br>  Abaixo est√° um exemplo de uma not√≠cia e seu t√≠tulo de refer√™ncia original.  Os modelos sobre os quais falarei treinam para gerar cabe√ßalhos com este exemplo: <br><br><img src="https://habrastorage.org/webt/8n/yy/gc/8nyygcx_ymy38dwrqpbqdiylvsu.png" alt="imagem"><br><br><h2>  Segredos para cortar a arquitetura seq2seq de texto </h2><br>  Existem dois tipos de m√©todos de redu√ß√£o de texto: <br><br><ol><li>  <b>Extrativista</b> .  Consiste em encontrar as partes mais informativas do texto e construir a partir delas a anota√ß√£o correta para o idioma especificado.  Este grupo de m√©todos usa apenas as palavras que est√£o no texto de origem. </li><li>  <b>Resumo</b>  Consiste na extra√ß√£o de links sem√¢nticos do texto, levando em considera√ß√£o as depend√™ncias do idioma.  Com a sammariza√ß√£o abstrata, as palavras de anota√ß√£o n√£o s√£o selecionadas no texto abreviado, mas no dicion√°rio (a lista de palavras para um determinado idioma) - reformulando assim a id√©ia principal. </li></ol><br>  A segunda abordagem implica que o algoritmo deve levar em conta as depend√™ncias da linguagem, reformular e generalizar.  Ele tamb√©m quer ter algum conhecimento do mundo real para evitar erros de fato.  Por um longo tempo, isso foi considerado uma tarefa dif√≠cil, e os pesquisadores n√£o conseguiram obter uma solu√ß√£o de alta qualidade - um texto gramaticalmente correto, preservando a id√©ia principal.  √â por isso que, no passado, a maioria dos algoritmos era baseada em uma abordagem de extra√ß√£o, pois a sele√ß√£o de partes inteiras do texto e a transfer√™ncia delas para o resultado permitem manter o mesmo n√≠vel de alfabetiza√ß√£o da fonte. <br><br>  Mas isso foi antes do boom das redes neurais e de sua penetra√ß√£o iminente na PNL.  Em 2014, a arquitetura <b>seq2seq</b> foi <b>introduzida com um mecanismo de aten√ß√£o</b> que pode ler algumas seq√º√™ncias de texto e gerar outras (que depende do que o modelo aprendeu a produzir) ( <a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">artigo</a> de Sutskever et al.).  Em 2016, essa arquitetura foi aplicada diretamente √† solu√ß√£o do problema de sammariza√ß√£o, realizando uma abordagem abstrata e obtendo um resultado compar√°vel ao que uma pessoa competente poderia escrever ( <a href="https://arxiv.org/pdf/1602.06023.pdf">artigo</a> de Nallapati et al., 2016; <a href="https://arxiv.org/pdf/1509.00685.pdf">artigo</a> de Rush et al., 2015; )  Como essa arquitetura funciona? <br><br><img src="https://habrastorage.org/webt/px/4x/wi/px4xwin577dd65z-lahsx7fvexs.png" alt="imagem"><br><br>  Seq2Seq consiste em duas partes: <br><br><ol><li>  <b>Codificador</b> (Encoder) - um RNN bidirecional, usado para ler a sequ√™ncia de entrada, ou seja, processa sequencialmente os elementos de entrada simultaneamente da esquerda para a direita e da direita para a esquerda para melhor considerar o contexto. </li><li>  <b>decodificador</b> (decodificador) - RNN unidirecional, que sequencialmente e por elementos produz uma sequ√™ncia de sa√≠da. </li></ol><br>  Primeiro, a sequ√™ncia de entrada √© traduzida em uma sequ√™ncia de incorpora√ß√£o (em resumo, incorpora√ß√£o √© uma representa√ß√£o concisa de uma palavra como um vetor).  Os casamentos passam pela rede recursiva do codificador.  Portanto, para cada palavra, obtemos os estados ocultos do codificador ( <i>indicados por ret√¢ngulos vermelhos no diagrama</i> ), e eles cont√™m informa√ß√µes sobre o pr√≥prio token e seu contexto, permitindo que levemos em considera√ß√£o as conex√µes de idioma entre as palavras. <br><br>  Ap√≥s processar a entrada, o codificador transfere seu √∫ltimo estado oculto (que cont√©m informa√ß√µes compactadas sobre o texto inteiro) para o decodificador, que recebe um token especial <img src="https://habrastorage.org/webt/qn/ud/38/qnud38u15vpvzie4zkne-doze7k.png" alt="imagem">  e cria a primeira palavra da sequ√™ncia de sa√≠da ( <i>na imagem √© "Alemanha"</i> ).  Em seguida, ele pega ciclicamente sua sa√≠da anterior, alimenta-a e exibe novamente o pr√≥ximo elemento de sa√≠da ( <i>assim, depois que "Alemanha" vier "bater" e depois de "bater" vir a palavra seguinte, etc.</i> ).  Isso √© repetido at√© que um token especial seja emitido <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="imagem">  .  Isso significa o fim da gera√ß√£o. <br><br>  Para exibir o pr√≥ximo elemento, o decodificador, assim como o codificador, converte o token de entrada em incorpora√ß√£o, d√° um passo na rede recursiva e recebe o pr√≥ximo estado oculto do decodificador ( <i>ret√¢ngulos amarelos no diagrama</i> ).  Em seguida, usando uma camada totalmente conectada, √© obtida uma distribui√ß√£o de probabilidade para todas as palavras de um dicion√°rio de modelo pr√©-compilado.  As palavras mais prov√°veis ‚Äã‚Äãser√£o deduzidas pelo modelo. <br><br>  A adi√ß√£o de <b>um mecanismo de aten√ß√£o</b> ajuda o decodificador a utilizar melhor as informa√ß√µes de entrada.  O mecanismo em cada etapa da gera√ß√£o determina a chamada <b>distribui√ß√£o de aten√ß√£o</b> (os <i>ret√¢ngulos azuis na figura s√£o o conjunto de pesos correspondente aos elementos da sequ√™ncia original, a soma dos pesos √© 1, todos os pesos&gt; = 0</i> ) e dele recebe a soma ponderada de todos os estados ocultos do codificador, formando assim vetor de contexto ( <i>o diagrama mostra um ret√¢ngulo vermelho com um tra√ßo azul</i> ).  Esse vetor concatena com a incorpora√ß√£o da palavra de entrada do decodificador no est√°gio de c√°lculo do estado latente e com o pr√≥prio estado latente no est√°gio de determina√ß√£o da pr√≥xima palavra.  Portanto, em cada etapa da sa√≠da, o modelo pode determinar quais estados do codificador s√£o mais importantes para ele no momento.  Em outras palavras, ele decide o contexto em que as palavras de entrada devem ser levadas em considera√ß√£o ao m√°ximo (por exemplo, na figura, exibindo a palavra "batida", o mecanismo de aten√ß√£o atribui grandes pesos aos tokens "vitorioso" e "ganha", e o restante √© quase zero). <br><br>  Como a gera√ß√£o de cabe√ßalhos tamb√©m √© uma das tarefas de sammariza√ß√£o, apenas com a sa√≠da m√≠nima poss√≠vel (1-12 palavras), decidi aplicar o <b>seq2seq com o mecanismo de aten√ß√£o</b> para o nosso caso.  N√≥s treinamos esse sistema em textos com t√≠tulos, por exemplo, nas not√≠cias.  Al√©m disso, √© aconselh√°vel, no est√°gio de treinamento, submeter ao decodificador n√£o sua pr√≥pria produ√ß√£o, mas as palavras do cabe√ßalho real (for√ßar o professor), facilitando a vida para ele e o modelo.  Como uma fun√ß√£o de erro, usamos a fun√ß√£o de perda de entropia cruzada padr√£o, mostrando qu√£o pr√≥ximas as distribui√ß√µes de probabilidade da palavra de sa√≠da e da palavra do cabe√ßalho real s√£o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3v/co/b9/3vcob9esvdhgsydjcnjfpzv2wbi.png"></div><br>  Ao usar o modelo treinado, usamos a pesquisa de raios para encontrar uma sequ√™ncia de palavras mais prov√°vel do que o algoritmo ganancioso.  Para fazer isso, em cada etapa da gera√ß√£o, derivamos n√£o a palavra mais prov√°vel, mas ao mesmo tempo observamos o tamanho do feixe das seq√º√™ncias de palavras mais prov√°veis.  Quando eles terminam (cada um termina em <img src="https://habrastorage.org/webt/vw/kv/4g/vwkv4gs-ul7vlvnkjwxag8njicw.png" alt="imagem">  ), derivamos a sequ√™ncia mais prov√°vel. <br><br><img src="https://habrastorage.org/webt/t2/dt/ni/t2dtnicefjxn0hwd0elaeduekbk.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/ln/q-/yxlnq-7z2-lwuyylmz83pferopm.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ey/81/9u/ey819uvi_jhx4wbwchr-wsixvgs.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/q1/tn/zu/q1tnzuopjtdslrltsmn8euup-kq.png"></div><br><br><h2>  Evolu√ß√£o do modelo </h2><br>  Um dos problemas do modelo no seq2seq √© a incapacidade de citar palavras que n√£o est√£o no dicion√°rio.  Por exemplo, o modelo n√£o tem chance de deduzir "obamacare" do artigo acima.  O mesmo vale para: <br><br><ul><li>  sobrenomes e nomes raros </li><li>  novos termos </li><li>  palavras em outras l√≠nguas, </li><li>  diferentes pares de palavras conectados por um h√≠fen (como um "senador republicano") </li><li>  e outros projetos. </li></ul><br>  Obviamente, voc√™ pode expandir o dicion√°rio, mas isso aumenta muito o n√∫mero de par√¢metros treinados.  Al√©m disso, √© necess√°rio fornecer um grande n√∫mero de documentos nos quais essas palavras raras s√£o encontradas, para que o gerador aprenda a utiliz√°-las de maneira qualitativa. <br><br>  Outra solu√ß√£o mais elegante para esse problema foi apresentada em um artigo de 2017 - ‚Äú <a href="https://arxiv.org/pdf/1704.04368.pdf">V√° direto ao ponto: resumo com redes de geradores de ponteiros</a> ‚Äù (Abigail See et al.).  Ela adiciona um novo mecanismo ao nosso modelo - <b>um mecanismo de</b> ponteiro, que pode selecionar palavras do texto de origem e inserir diretamente na sequ√™ncia gerada.  Se o texto contiver OOV ( <i>fora do vocabul√°rio - uma palavra que n√£o est√° no dicion√°rio</i> ), o modelo, se considerar necess√°rio, poder√° isolar o OOV e inseri-lo na sa√≠da.  Esse sistema √© chamado de <b>‚Äú</b> gerador de ponteiro‚Äù (gerador de ponteiro ou pg) e √© uma s√≠ntese de duas abordagens para a sammariza√ß√£o.  Ela mesma pode decidir em que etapa deve ser abstrata e em que etapa - extrair.  Como ela faz isso, vamos descobrir agora. <br><br><img src="https://habrastorage.org/webt/f-/9y/xw/f-9yxwborbgpjpalzwd5e_f74xi.png" alt="imagem"><br><br>  A principal diferen√ßa do modelo seq2seq usual √© a a√ß√£o adicional sobre a qual p <sub>gen</sub> √© calculado - a probabilidade de gera√ß√£o.  Isso √© feito usando o estado oculto do decodificador e o vetor de contexto.  O significado da a√ß√£o adicional √© simples.  Quanto mais pr√≥ximo de <sub>gen √©</sub> 1, maior a probabilidade de o modelo emitir uma palavra do dicion√°rio usando gera√ß√£o abstrata.  Quanto mais pr√≥ximo o p <sub>gen for</sub> de 0, maior a probabilidade de o gerador extrair a palavra do texto, guiada pela distribui√ß√£o da aten√ß√£o obtida anteriormente.  A distribui√ß√£o de probabilidade final dos resultados da palavra √© a soma da distribui√ß√£o de probabilidade gerada das palavras (na qual n√£o h√° OOV) multiplicada por p <sub>gen</sub> e a distribui√ß√£o da aten√ß√£o (na qual OOV √©, por exemplo, ‚Äú2-0‚Äù na figura) multiplicada por (1 - p <sub>gen</sub> ). <br><br><img src="https://habrastorage.org/webt/iv/8f/8q/iv8f8qvol1j3bbyx79vt5zo7-oq.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/if/ew/s3/ifews3gqgojf5mclbxd0lpkz2q8.png"></div><br>  Al√©m do mecanismo de apontar, o artigo apresenta <b>um mecanismo de cobertura</b> , que ajuda a evitar a repeti√ß√£o de palavras.  Tamb√©m experimentei, mas n√£o percebi melhorias significativas na qualidade dos t√≠tulos - n√£o √© realmente necess√°rio.  Provavelmente, isso se deve √†s especificidades da tarefa: como √© necess√°rio gerar um pequeno n√∫mero de palavras, o gerador simplesmente n√£o tem tempo para se repetir.  Mas para outras tarefas de sammariza√ß√£o, por exemplo, anota√ß√£o, pode ser √∫til.  Se estiver interessado, voc√™ pode ler sobre isso no <a href="https://arxiv.org/pdf/1704.04368.pdf">artigo</a> original. <br><br><img src="https://habrastorage.org/webt/4q/bx/op/4qbxopqt862cphikmbezyaoadlk.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/--/3m/mz/--3mmzahhdk8kelwoqyh9rdjg8e.png"></div><br><h2>  Grande variedade de palavras em russo </h2><br>  Outra maneira de melhorar a qualidade dos cabe√ßalhos de sa√≠da √© pr√©-processar adequadamente a sequ√™ncia de entrada.  Al√©m da disposi√ß√£o √≥bvia de caracteres mai√∫sculos, tamb√©m tentei converter palavras do texto de origem em pares de estilos e inflex√µes (ou seja, funda√ß√µes e finais).  Para dividir, use o Porter Stemmer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/j8/-y/jyj8-yonzpmvpyoye6mbygrlcjg.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/_k/__/vy/_k__vyhouqm_ctxpsp8ukcselwe.png"></div><br>  Marcamos todas as inflex√µes com o s√≠mbolo ‚Äú+‚Äù no in√≠cio para distingui-las de outros tokens.  Consideramos cada t√≥pico e inflex√£o como uma palavra separada e aprendemos com eles da mesma maneira que nas palavras.  Ou seja, obtemos incorpora√ß√£o deles e derivamos uma sequ√™ncia (tamb√©m dividida em funda√ß√µes e finais) que pode ser facilmente transformada em palavras. <br><br>  Essa convers√£o √© muito √∫til quando se trabalha com idiomas morfologicamente ricos como o russo.  Em vez de compilar dicion√°rios enormes com uma grande variedade de formas de palavras em russo, voc√™ pode limitar-se a um grande n√∫mero de hastes dessas palavras (elas s√£o v√°rias vezes menores que o n√∫mero de formas de palavras) e a um conjunto muito pequeno de termina√ß√µes (recebi muitas 450 inflex√µes).  Assim, facilitamos o trabalho do modelo com essa ‚Äúriqueza‚Äù e, ao mesmo tempo, n√£o aumentamos a complexidade da arquitetura e o n√∫mero de par√¢metros. <br><br><img src="https://habrastorage.org/webt/a4/es/s5/a4ess5qr3vaxprksv7avun3obl8.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3d/rk/6h/3drk6hntzcow2i3soi-heul7ckq.png"></div><br>  Eu tamb√©m tentei usar a transforma√ß√£o de lema + grama.  Ou seja, a partir de cada palavra antes do processamento, voc√™ pode obter sua forma inicial e significado gramatical usando o pacote pymorphy (por exemplo, "was" <img src="https://habrastorage.org/webt/95/wb/-a/95wb-aopfalycfvrshqfynxnwd4.png" alt="imagem">  "Ser" e "VERBO | impf | passado | cantar | femn").  Assim, obtive um par de sequ√™ncias paralelas (em uma - as formas iniciais, na outra - valores gramaticais).  Para cada tipo de sequ√™ncia, compilei minhas incorpora√ß√µes, que concatenaram e submeteram ao pipeline descrito anteriormente.  Nele, o decodificador n√£o aprendeu a dar uma palavra, mas um lema e gram√°ticas.  Mas esse sistema n√£o trouxe melhorias vis√≠veis em compara√ß√£o com a p√°gina sobre o assunto.  Talvez fosse uma arquitetura excessivamente simples para trabalhar com valores gramaticais e valesse a pena criar um classificador separado para cada categoria gramatical na sa√≠da.  Mas n√£o experimentei modelos assim ou mais complexos. <br><br>  Eu experimentei outra adi√ß√£o √† arquitetura original do gerador de ponteiros, que, no entanto, n√£o se aplica ao pr√©-processamento.  Isso √© um aumento no n√∫mero de camadas (at√© 3) das redes recursivas do codificador e decodificador.  Aumentar a profundidade da rede recorrente pode melhorar a qualidade da sa√≠da, pois o estado oculto das √∫ltimas camadas pode conter informa√ß√µes sobre uma subsequ√™ncia de entrada muito mais longa do que o estado oculto de uma RNN de camada √∫nica.  Isso ajuda a levar em considera√ß√£o as conex√µes sem√¢nticas estendidas complexas entre os elementos da sequ√™ncia de entrada.  √â verdade que isso custa um aumento significativo no n√∫mero de par√¢metros do modelo e complica o aprendizado. <br><br><img src="https://habrastorage.org/webt/cf/7g/ej/cf7gejf-hxd5pbscvqgnvuuviqy.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4g/ww/zy/4gwwzytsrt506_xllgy5xcis6vk.png"></div><br><h2>  Experi√™ncias do gerador de cabe√ßalho </h2><br>  Todas as minhas experi√™ncias com geradores de t√≠tulos podem ser divididas em dois tipos: experi√™ncias com artigos e vers√≠culos.  Vou falar sobre eles em ordem. <br><br><h3>  Not√≠cias Experi√™ncias </h3><br>  Ao trabalhar com not√≠cias, usei modelos como seq2seq, pg, pg com hastes e inflex√µes - camada √∫nica e tr√™s camadas.  Tamb√©m considerei modelos que funcionam com gramas, mas tudo o que queria contar sobre eles j√° descrevi acima.  Devo dizer imediatamente que todas as p√°ginas descritas nesta se√ß√£o usaram o mecanismo de revestimento, embora sua influ√™ncia sobre o resultado seja duvidosa (pois sem ele n√£o foi muito pior). <br><br>  Treinei no conjunto de dados RIA Novosti, fornecido pela ag√™ncia de not√≠cias Rossiya Segodnya, para conduzir uma faixa de gera√ß√£o de manchetes na confer√™ncia Dialog.  O conjunto de dados cont√©m 1.003.869 not√≠cias publicadas de janeiro de 2010 a dezembro de 2014. <br><br>  Todos os modelos estudados usaram os mesmos embeddings (128), vocabul√°rio (100k) e estados latentes (256) e treinados para o mesmo n√∫mero de √©pocas.  Portanto, apenas altera√ß√µes qualitativas na arquitetura ou no pr√©-processamento podem afetar o resultado. <br><br>  Modelos adaptados para trabalhar com texto pr√©-processado oferecem melhores resultados do que modelos que trabalham com palavras.  Uma p√°gina de tr√™s camadas que usa informa√ß√µes sobre t√≥picos e inflex√µes funciona melhor.  Ao usar qualquer p√°gina, a melhoria esperada na qualidade dos cabe√ßalhos em compara√ß√£o com seq2seq tamb√©m aparece, o que sugere o uso preferencial do ponteiro ao gerar cabe√ßalhos.  Aqui est√° um exemplo da opera√ß√£o de todos os modelos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6w/4u/0r/6w4u0ruudngxtt2szek31w0achq.png"></div><br>  Observando os cabe√ßalhos gerados, podemos distinguir os seguintes problemas dos modelos em estudo: <br><br><ol><li>  Os modelos costumam usar formas irregulares de palavras.  Modelos com hastes (como no exemplo acima) s√£o mais aliviados dessa desvantagem; </li><li>  Todos os modelos, exceto aqueles que trabalham com temas, podem produzir cabe√ßalhos que parecem incompletos ou designs estranhos que n√£o est√£o no idioma (como no exemplo acima); </li><li>  Todos os modelos estudados freq√ºentemente confundem as pessoas descritas, substituem datas incorretas ou usam palavras n√£o muito adequadas. </li></ol><br><img src="https://habrastorage.org/webt/9c/-b/ot/9c-bote3bwomvfu_3bwaixhqnks.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e-/3c/px/e-3cpxuknykthyu-xvf-cp-rdsi.png"></div><br><img src="https://habrastorage.org/webt/lf/ry/ep/lfryepksl4kmcqxttb14ikgx4wg.png" alt="imagem"><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b3/4m/fw/b34mfwpsffud2kaa93tccjvyja0.png"></div><br><h3>  Experimentos com vers√≠culos </h3><br>  Como a p√°gina de tr√™s camadas com os temas tem as menos imprecis√µes nos cabe√ßalhos gerados, esse √© o modelo que eu escolhi para experimentos com versos.  Eu a ensinei sobre o caso, composto por 6 milh√µes de poemas russos do site "stihi.ru".  Eles incluem amor (cerca de metade dos versos s√£o dedicados a este t√≥pico), c√≠vica (cerca de um quarto), poesia urbana e paisag√≠stica.  Per√≠odo de reda√ß√£o: janeiro de 2014 a maio de 2019. Vou dar exemplos de t√≠tulos gerados para os vers√≠culos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/mo/da/yvmodarkck7cgra-ymxigffwhik.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3u/mb/fn/3umbfnpzo1hwhqy0_glxyhxynu4.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tp/tk/5e/tptk5eoq9wa1xrnj9w0gewjeayk.png"></div><br><br>  O modelo acabou extraindo principalmente: quase todos os cabe√ßalhos s√£o de uma √∫nica linha, geralmente extra√≠dos da primeira ou da √∫ltima estrofe.  Em casos excepcionais, o modelo pode gerar palavras que n√£o est√£o no poema.  Isso se deve ao fato de um grande n√∫mero de textos no caso ter realmente uma das linhas como nome. <br><br>    ,  -,         ,     <a href="https://vk.com/wall-177402111_31"> </a>             ¬´¬ª.     ‚Äî ABBYY,         Natural Language Processing. <br><br> ,    :    ,  ,      . <br><br> <i>,   NLP Group  ABBYY</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt479400/">https://habr.com/ru/post/pt479400/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt479384/index.html">O que o Big Data faz no MegaFon e como chegar l√°?</a></li>
<li><a href="../pt479388/index.html">Caracter√≠sticas da constru√ß√£o de data centers nacionais, Mikhalych</a></li>
<li><a href="../pt479392/index.html">Pinebook Pro: n√£o √© mais o Chromebook</a></li>
<li><a href="../pt479394/index.html">Como procurei o helpdesk entre 15 solu√ß√µes e ... n√£o encontrei</a></li>
<li><a href="../pt479398/index.html">Trazemos a equa√ß√£o de regress√£o linear para a forma matricial</a></li>
<li><a href="../pt479402/index.html">Como pagar oficialmente por servi√ßos freelancers no exterior, pagar 0% de impostos e n√£o alimentar sistemas de pagamento</a></li>
<li><a href="../pt479404/index.html">Pessoal para o Papai Noel</a></li>
<li><a href="../pt479406/index.html">16 dicas de desenvolvimento para o Android no Kotlin. Parte 1</a></li>
<li><a href="../pt479414/index.html">Maneiras de encontrar o objetivo. O papel do acaso</a></li>
<li><a href="../pt479416/index.html">Veja para onde est√° indo (vis√£o perif√©rica versus carga cognitiva)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>