<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÇüèæ üç∏ üßôüèΩ Como o concurso retro dos cossacos decidiu üç§ ü•É üö¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nesta primavera, foi realizado um importante concurso OpenAI Retro, dedicado ao aprendizado por refor√ßo, ao aprendizado por meta e, √© claro, ao Sonic....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Como o concurso retro dos cossacos decidiu</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/421585/"><p>  Nesta primavera, foi realizado um importante concurso OpenAI Retro, dedicado ao aprendizado por refor√ßo, ao aprendizado por meta e, √© claro, ao Sonic.  Nossa equipe ficou em 4¬∫ lugar entre mais de 900 equipes.  O campo de treinamento com refor√ßo √© um pouco diferente do aprendizado de m√°quina padr√£o, e esse concurso era diferente de uma competi√ß√£o t√≠pica de RL.  Pe√ßo detalhes sob gato. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sx/pt/0r/sxpt0rdvj5g3xn6eohh6vmusj1o.jpeg" alt="imagem"></div><br><hr><a name="habracut"></a><br><h2 id="tldr">  TL; DR </h2><br><p>  Uma linha de base devidamente ajustada n√£o precisa de truques adicionais ... praticamente. </p><br><h2 id="intro-v-obuchenie-s-podkrepleniem">  Introdu√ß√£o ao treinamento de refor√ßo </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/nb/vn/yxnbvndwcscdelo4q_m7zxxhfsu.png" alt="imagem"></div><br><p>  O aprendizado refor√ßado √© uma √°rea que combina a teoria do controle ideal, teoria dos jogos, psicologia e neurobiologia.  Na pr√°tica, o aprendizado por refor√ßo √© usado para resolver problemas de tomada de decis√£o e procurar estrat√©gias comportamentais ideais ou pol√≠ticas muito complexas para a programa√ß√£o "direta".  Nesse caso, o agente √© treinado no hist√≥rico de intera√ß√µes com o meio ambiente.  O ambiente, por sua vez, avaliando as a√ß√µes do agente, fornece a ele uma recompensa (escalar) - quanto melhor o comportamento do agente, maior a recompensa.  Como resultado, a melhor pol√≠tica √© aprendida com o agente que aprendeu a maximizar a recompensa total por todo o tempo de intera√ß√£o com o ambiente. </p><br><p>  Como um exemplo simples, voc√™ pode jogar BreakOut.  Neste bom e velho jogo da s√©rie Atari, uma pessoa / agente precisa controlar a plataforma horizontal inferior, bater na bola e gradualmente quebrar todos os blocos superiores com ela.  Quanto mais derrubado, maior a recompensa.  Consequentemente, o que uma pessoa / agente v√™ √© uma imagem da tela e √© necess√°rio tomar uma decis√£o em que dire√ß√£o mover a plataforma inferior. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/an/ke/wiankeye_8wb0cxtrhpsfdf5sdm.gif" alt="imagem"></div><br><p>  Se voc√™ estiver interessado no t√≥pico de treinamento por refor√ßo, aconselho voc√™ em um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">curso introdut√≥rio</a> interessante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do HSE</a> , bem como em seu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">colega de c√≥digo aberto</a> mais detalhado.  Se voc√™ quer algo que possa ler, mas com exemplos - um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">livro</a> inspirado nesses dois cursos.  Revisei / completei / ajudei a criar todos esses cursos e, portanto, sei por experi√™ncia pr√≥pria que eles fornecem uma excelente base. </p><br><h2 id="pro-zadachu">  Sobre a tarefa </h2><br><p>  O principal objetivo dessa competi√ß√£o era conseguir um agente que pudesse jogar bem na s√©rie de jogos SEGA - Sonic The Hedgehog.  A OpenAI estava come√ßando a importar jogos da SEGA para sua plataforma de treinamento de agentes da RL, e decidiu promover um pouco esse momento.  At√© o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo foi</a> lan√ßado com o dispositivo de tudo e uma descri√ß√£o detalhada dos m√©todos b√°sicos. </p><br><p>  Todos os tr√™s jogos do Sonic foram suportados, cada um com 9 n√≠veis, nos quais, ao abrir uma l√°grima, voc√™ pode at√© jogar, lembrando a sua inf√¢ncia (depois de compr√°-los no Steam primeiro). </p><br><p>  O estado do ambiente (o que o agente viu) foi a imagem do simulador - uma imagem RGB e, como a√ß√£o, o agente foi solicitado a escolher qual bot√£o do joystick virtual pressionar - pular / esquerda / direita e assim por diante.  O agente recebeu pontos de recompensa, bem como no jogo original, ou seja,  para coletar an√©is, bem como para a velocidade de passar de n√≠vel.  De fato, t√≠nhamos um som original √† nossa frente, mas era necess√°rio passar por ele com a ajuda de nosso agente. </p><br><p>  A competi√ß√£o foi realizada de 5 de abril a 5 de junho, ou seja,  apenas 2 meses, o que parece bastante pequeno.  Nossa equipe conseguiu se unir e participar da competi√ß√£o apenas em maio, o que nos fez aprender muito em qualquer lugar. </p><br><h2 id="baselines">  Linhas de base </h2><br><p>  Como linhas de base, foram fornecidos guias de treinamento completos para o treinamento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Rainbow</a> (abordagem DQN) e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">PPO</a> (abordagem Gradiente de Pol√≠tica) em um dos n√≠veis poss√≠veis no Sonic e o envio do agente resultante. </p><br><p>  A vers√£o Rainbow foi baseada no pouco conhecido projeto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">anyrl</a> , mas o PPO usou as boas e antigas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">linhas</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">base</a> do OpenAI e nos pareceu muito mais prefer√≠vel. </p><br><p>  As linhas de base publicadas diferiam das abordagens descritas no artigo por sua maior simplicidade e conjuntos menores de "hacks" para acelerar o aprendizado.  Assim, os organizadores lan√ßaram id√©ias e definiram a dire√ß√£o, mas a decis√£o sobre o uso e a implementa√ß√£o dessas id√©ias foi deixada para o participante da competi√ß√£o. </p><br><p>  Em rela√ß√£o √†s id√©ias, gostaria de agradecer √† OpenAI pela abertura e, individualmente, a John Schulman pelos conselhos, id√©ias e sugest√µes que ele expressou no in√≠cio desta competi√ß√£o.  N√≥s, como muitos participantes (e ainda mais novos no mundo da RL), isso nos permitiu focar melhor no objetivo principal da competi√ß√£o - o aprendizado de meta e a melhoria da generaliza√ß√£o de agentes, sobre os quais falaremos agora. </p><br><h2 id="osobennosti-ocenivaniya-resheniy">  Caracter√≠sticas da avalia√ß√£o da decis√£o </h2><br><p>  A coisa mais interessante come√ßou no momento da avalia√ß√£o dos agentes.  Em competi√ß√µes / benchmarks t√≠picos de RL, os algoritmos s√£o testados no mesmo ambiente em que foram treinados, o que contribui para algoritmos que s√£o bons em lembrar e t√™m muitos hiperpar√¢metros.  No mesmo concurso, o teste do algoritmo foi realizado nos novos n√≠veis do Sonic (que nunca foram mostrados a ningu√©m), desenvolvidos pela equipe OpenAI especificamente para este concurso.  A cereja do bolo foi o fato de que, no processo de teste, o agente tamb√©m recebeu uma recompensa durante a passagem do n√≠vel, o que possibilitou a reciclagem direta no processo de teste.  No entanto, neste caso, vale a pena lembrar que os testes eram limitados em tempo - 24 horas e em ticks de jogo - 1 milh√£o.  Ao mesmo tempo, a OpenAI apoiou fortemente a cria√ß√£o de agentes que poderiam rapidamente treinar para novos n√≠veis.  Como j√° mencionado, obter e estudar essas solu√ß√µes foi o principal objetivo da OpenAI durante esta competi√ß√£o. </p><br><p>  No ambiente acad√™mico, a dire√ß√£o do estudo de pol√≠ticas que podem se adaptar rapidamente a novas condi√ß√µes √© chamada meta-aprendizagem e, nos √∫ltimos anos, vem se desenvolvendo ativamente. </p><br><p>  Al√©m disso, em contraste com as competi√ß√µes normais do kaggle, em que todo o envio se resume ao envio do seu arquivo de resposta, nesta competi√ß√£o (e de fato nas competi√ß√µes de RL) a equipe era obrigada a agrupar sua solu√ß√£o em um cont√™iner de docker com a API especificada, colet√°-la e envi√°-la imagem do docker.  Isso aumentou o limiar para entrar na competi√ß√£o, mas tornou o processo de decis√£o muito mais honesto - os recursos e o tempo para a imagem do docker eram limitados, respectivamente, algoritmos muito pesados ‚Äã‚Äãe / ou lentos simplesmente n√£o passaram na sele√ß√£o.  Parece-me que essa abordagem de avalia√ß√£o √© muito mais prefer√≠vel, pois permite que pesquisadores sem um ‚Äúcluster dom√©stico de DGX e AWS‚Äù competam em p√© de igualdade com os amantes dos modelos 50000 de vidro.  Espero ver mais desse tipo de competi√ß√£o no futuro. </p><br><h2 id="komanda">  A equipe </h2><br><p>  Kolesnikov Sergey ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">scitator</a> ) <br>  Entusiasta da RL.  Na √©poca da competi√ß√£o, um estudante do Instituto de F√≠sica e Tecnologia de Moscou, MIPT, escreveu e defendeu um diploma do NIPS do ano passado: Aprendendo a Executar a competi√ß√£o (um artigo sobre o qual tamb√©m deveria ser escrito). <br>  Senior Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dbrain</a> - Apresentamos concursos prontos para produ√ß√£o com docker e recursos limitados para o mundo real. </p><br><p>  Pavlov Mikhail ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">fgvbrt</a> ) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Desenvolvedor de</a> pesquisa s√™nior <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">DiphakLab</a> .  Participou e ganhou repetidamente pr√™mios em hackathons e competi√ß√µes de treinamento refor√ßadas. </p><br><p>  Sergeev Ilya ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sergeevii123</a> ) <br>  Entusiasta da RL.  Eu bati em um dos hackathons RL do Deephack e tudo come√ßou.  Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Avito.ru</a> - vis√£o computacional para v√°rios projetos internos. </p><br><p>  Sorokin Ivan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">l√≠tico</a> ) <br>  Envolvido no reconhecimento de fala em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">speechpro.ru</a> . </p><br><h2 id="podhody-i-reshenie">  Abordagens e Solu√ß√£o </h2><br><p>  Ap√≥s testes r√°pidos das linhas de base propostas, nossa escolha recaiu sobre a abordagem OpenAI - PPO, como uma op√ß√£o mais formada e interessante para o desenvolvimento de nossa solu√ß√£o.  Al√©m disso, a julgar pelo artigo desta competi√ß√£o, o agente da OPP lidou com a tarefa um pouco melhor.  Do mesmo artigo, nasceram as primeiras melhorias que usamos em nossa solu√ß√£o, mas as primeiras coisas primeiro: </p><br><ol><li><p>  Treinamento colaborativo de PPO em todos os n√≠veis dispon√≠veis </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/oc/lx/iaoclxlyzkgmdgiqkkksiwzcoli.png" alt="imagem"></div><br><p>  A linha de base estabelecida foi treinada em apenas um dos 27 n√≠veis Sonic dispon√≠veis.  No entanto, com a ajuda de pequenas modifica√ß√µes, foi poss√≠vel paralelizar o treinamento de uma s√≥ vez para todos os 27 n√≠veis.  Devido √† maior diversidade no treinamento, o agente resultante teve uma generaliza√ß√£o muito maior e uma melhor compreens√£o do dispositivo do mundo Sonic e, portanto, lidou melhor com uma ordem de magnitude. </p><br></li><li><p>  Treinamento adicional durante o teste <br>  Voltando √† id√©ia principal da competi√ß√£o, o meta-aprendizado, foi necess√°rio encontrar uma abordagem que tivesse a m√°xima generaliza√ß√£o e pudesse se adaptar facilmente a novos ambientes.  E para a adapta√ß√£o, foi necess√°rio treinar novamente o agente existente para o ambiente de teste, o que, de fato, foi realizado (em cada n√≠vel de teste, o agente executava 1 milh√£o de etapas, o que era suficiente para se adaptar a um n√≠vel espec√≠fico).  No final de cada um dos jogos de teste, o agente avaliou o pr√™mio recebido e otimizou sua pol√≠tica usando a hist√≥ria rec√©m-recebida.  √â importante observar aqui que, com essa abordagem, √© importante n√£o esquecer toda a sua experi√™ncia anterior e n√£o se degradar em condi√ß√µes espec√≠ficas, o que, em ess√™ncia, √© o principal interesse do meta-aprendizado, pois esse agente perde imediatamente toda a sua capacidade de generaliza√ß√£o. </p><br></li><li><p>  B√¥nus de explora√ß√£o <br>  Ao aprofundar-se nas condi√ß√µes de remunera√ß√£o de um n√≠vel, o agente recebeu uma recompensa por seguir adiante na coordenada x, respectivamente, ele poderia ficar preso em alguns n√≠veis, quando era necess√°rio avan√ßar e voltar.  Foi decidido adicionar uma recompensa ao agente, a chamada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">explora√ß√£o baseada em contagem</a> , quando o agente receberia uma pequena recompensa se estivesse em um estado em que ainda n√£o estava.  Dois tipos de b√¥nus de explora√ß√£o foram implementados: com base na figura e com a coordenada x do agente.  Uma recompensa baseada em uma imagem foi calculada da seguinte forma: para cada local de pixel na imagem, era contada quantas vezes cada valor ocorreu para um epis√≥dio, a recompensa era inversamente proporcional ao produto em todos os locais de pixel de quantas vezes os valores nesses locais foram encontrados para um epis√≥dio.  A recompensa baseada na coordenada x foi considerada de maneira semelhante: para cada coordenada x (com uma certa precis√£o) era contada quantas vezes o agente estava nessa coordenada para o epis√≥dio, a recompensa √© inversamente proporcional a esse valor para a coordenada x atual. </p><br></li><li><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Experimentos de mistura</a> <br>  Recentemente, ao ‚Äúensinar com um professor‚Äù um m√©todo simples mas eficaz de aumento de dados, o chamado  confus√£o.  A id√©ia √© muito simples: a adi√ß√£o de duas imagens de entrada arbitr√°rias √© feita e uma soma ponderada dos r√≥tulos correspondentes √© atribu√≠da a essa nova imagem (por exemplo, 0,7 <em>c√£o + 0,3</em> gato).  Em tarefas como classifica√ß√£o de imagem e reconhecimento de fala, a mixagem mostra bons resultados.  Portanto, foi interessante testar esse m√©todo para RL.  O aumento foi feito em todos os lotes grandes, consistindo em v√°rios epis√≥dios.  As imagens de entrada eram misturadas em pixels, mas com tags nem tudo era t√£o simples.  Os valores retornos, valores e neglogpacs foram combinados por uma soma ponderada, mas a a√ß√£o (a√ß√µes) foi escolhida no exemplo com o coeficiente m√°ximo.  Essa solu√ß√£o n√£o mostrou um aumento tang√≠vel (embora, ao que parece, devesse ter havido um aumento na generaliza√ß√£o), mas n√£o piorou a linha de base.  Os gr√°ficos abaixo comparam o algoritmo PPO com confus√£o (vermelho) e sem confus√£o (azul): na parte superior √© a recompensa durante o treinamento, na parte inferior √© a dura√ß√£o do epis√≥dio. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bk/h-/jb/bkh-jbhhzowcl3gpazkbn8mun2a.jpeg" alt="imagem"></div><br></li><li><p>  Sele√ß√£o da melhor pol√≠tica inicial <br>  Essa melhoria foi uma das √∫ltimas e deu uma contribui√ß√£o muito significativa para o resultado final.  No n√≠vel do treinamento, v√°rias pol√≠ticas diferentes com diferentes hiperpar√¢metros foram treinadas.  No n√≠vel do teste, nos primeiros epis√≥dios, cada um deles foi testado e, para treinamento adicional, foi escolhida a pol√≠tica que dava a recompensa m√°xima do teste pelo epis√≥dio. </p><br></li></ol><br><h2 id="bloopers">  Choque </h2><br><p>  E agora sobre a quest√£o do que foi tentado, mas "n√£o voou".  Afinal, este n√£o √© um novo artigo da SOTA para ocultar algo. </p><br><ol><li>  Mudan√ßa na arquitetura da rede: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ativa√ß√£o SELU</a> , aten√ß√£o pr√≥pria, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">blocos SE</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Neuroevolu√ß√£o</a> </li><li>  Criando seus pr√≥prios n√≠veis de Sonic - tudo foi preparado, mas n√£o havia tempo suficiente </li><li>  Meta-treinamento atrav√©s de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MAML</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">REPTILE</a> </li><li>  Conjunto de v√°rios modelos e treinamento adicional durante o teste de cada modelo usando amostragem importante </li></ol><br><h2 id="itogi">  Sum√°rio </h2><br><p>  Ap√≥s tr√™s semanas do final da competi√ß√£o, a OpenAI divulgou os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">resultados</a> .  Em 11 n√≠veis adicionais criados adicionalmente, nossa equipe recebeu um 4¬∫ lugar honroso, depois de saltar do 8¬∫ em um teste p√∫blico e ultrapassar as linhas de base obscurecidas da OpenAI. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/om/iw/yvomiwj1jmffvmlicjmhe7vjp9y.png" alt="imagem"></div><br><p>  Os principais pontos distintivos que "voaram" nos primeiros 3ki: </p><br><ol><li>  Sistema de a√ß√µes aprimorado (veio com seus pr√≥prios bot√µes, removeu outros); </li><li>  Investiga√ß√£o de estados atrav√©s de hash a partir da imagem de entrada; </li><li>  Mais n√≠veis de treinamento; </li></ol><br><p>  Al√©m disso, quero observar que, neste concurso, al√©m de vencer, a descri√ß√£o de suas decis√µes, bem como materiais que ajudaram outros participantes foram ativamente encorajados - houve tamb√©m uma indica√ß√£o separada para isso.  O que, novamente, aumentou o concurso de l√¢mpadas. </p><br><h2 id="posleslovie">  Posf√°cio </h2><br><p>  Pessoalmente, gostei muito desta competi√ß√£o, bem como do tema de meta-aprendizado.  Durante a participa√ß√£o, eu me familiarizei com uma grande lista de artigos (eu nem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esqueci</a> alguns deles) e aprendi um grande n√∫mero de diferentes abordagens que espero aplicar no futuro. </p><br><p>  Na melhor tradi√ß√£o de participar da competi√ß√£o, todo o c√≥digo est√° dispon√≠vel e publicado no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">github</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt421585/">https://habr.com/ru/post/pt421585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt421573/index.html">Food Design Digest agosto 2018</a></li>
<li><a href="../pt421575/index.html">O confronto entre Yandex e Roskomnadzor est√° se formando; em um dia o mecanismo de pesquisa poder√° estar parcialmente bloqueado</a></li>
<li><a href="../pt421577/index.html">Uma explora√ß√£o de uma vulnerabilidade n√£o fechada no Agendador de tarefas do Windows √© publicada (tradu√ß√£o)</a></li>
<li><a href="../pt421579/index.html">Organiza√ß√£o de intera√ß√£o eficaz de microsservi√ßos</a></li>
<li><a href="../pt421583/index.html">Onde ir para a faculdade para estudar para um especialista em TI? + pesquisa</a></li>
<li><a href="../pt421587/index.html">[Yekaterinburg, an√∫ncio] Java Mitap - JUG.EKB</a></li>
<li><a href="../pt421589/index.html">Metamorfoses: programa√ß√£o molecular da forma</a></li>
<li><a href="../pt421591/index.html">Sistema de or√ßamento para vigil√¢ncia por v√≠deo sem fio (Wi-Fi) aut√¥noma (a partir da bateria)</a></li>
<li><a href="../pt421593/index.html">SandboxEscaper / PoC-LPE: o que h√° dentro?</a></li>
<li><a href="../pt421595/index.html">Como as pessoas de TI encontram emprego nos EUA e na UE: 9 melhores recursos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>