<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñïüèΩ ‚ö±Ô∏è üßñüèΩ Mudando de Redshift para ClickHouse üôéüèº üè¥ üîé</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Por um longo tempo, o iFunny usou o Redshift como um banco de dados para eventos que ocorrem em servi√ßos de back-end e aplicativos m√≥veis. Foi escolhi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Mudando de Redshift para ClickHouse</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/funcorp/blog/433346/"><img src="https://habrastorage.org/webt/s8/xo/0d/s8xo0dnodxojhff6jufnruyg660.jpeg"><br><br>  Por um longo tempo, o iFunny usou o Redshift como um banco de dados para eventos que ocorrem em servi√ßos de back-end e aplicativos m√≥veis.  Foi escolhido porque, no momento da implementa√ß√£o, em geral, n√£o havia alternativas compar√°veis ‚Äã‚Äãem custo e conveni√™ncia. <br><br>  No entanto, tudo mudou ap√≥s o lan√ßamento p√∫blico do ClickHouse.  Estudamos por um longo tempo, comparamos o custo, estimamos a arquitetura aproximada e, finalmente, neste ver√£o, decidimos ver o quanto √© √∫til para n√≥s.  Neste artigo, voc√™ aprender√° sobre o problema que o Redshift nos ajudou a resolver e como mudamos essa solu√ß√£o para o ClickHouse. <br><a name="habracut"></a><br><h2>  O problema </h2><br>  O iFunny precisava de um servi√ßo semelhante ao Yandex.Metrica, mas exclusivamente para consumo dom√©stico.  Eu vou explicar o porqu√™. <br><br>  Clientes externos gravam eventos.  Pode ser aplicativos m√≥veis, sites ou servi√ßos internos de back-end.  √â muito dif√≠cil para esses clientes explicar que o servi√ßo de recep√ß√£o de eventos n√£o est√° dispon√≠vel no momento, "tente envi√°-lo em 15 minutos ou em uma hora".  Existem muitos clientes, eles desejam enviar eventos o tempo todo e mal podem esperar. <br><br>  Em contraste com eles, existem servi√ßos e usu√°rios internos que s√£o bastante tolerantes a esse respeito: eles podem funcionar corretamente, mesmo com um servi√ßo de an√°lise inacess√≠vel.  E a maioria das m√©tricas de produtos e os resultados dos testes A / B geralmente fazem sentido assistir apenas uma vez por dia, ou talvez com menos frequ√™ncia.  Portanto, os requisitos de leitura s√£o bastante baixos.  No caso de um acidente ou atualiza√ß√£o, podemos nos permitir ficar inacess√≠veis ou inconsistentes na leitura por v√°rias horas ou at√© dias (em um caso particularmente negligenciado). <br><br>  Se falamos de n√∫meros, precisamos realizar cerca de cinco bilh√µes de eventos (300 GB de dados compactados) por dia, enquanto os armazenamos por tr√™s meses em um formul√°rio "quente" dispon√≠vel para consultas SQL e no "frio" por dois anos ou mais, mas para que, dentro de alguns dias, possamos transform√°-los em "quentes". <br><br>  Basicamente, os dados s√£o uma cole√ß√£o de eventos ordenados por tempo.  Existem cerca de trezentos tipos de eventos, cada um com seu pr√≥prio conjunto de propriedades.  Ainda existem alguns dados de fontes de terceiros que devem ser sincronizados com o banco de dados de an√°lise: por exemplo, uma cole√ß√£o de instala√ß√µes de aplicativos do MongoDB ou um servi√ßo externo do AppsFlyer. <br><br>  Acontece que para o banco de dados precisamos de cerca de 40 TB de disco e, para armazenamento "frio" - cerca de 250 TB a mais. <br><br><h2>  Solu√ß√£o Redshift </h2><br><img src="https://habrastorage.org/webt/f0/nq/dl/f0nqdl7cvriq9ygc3jlhelfdlqi.png"><br><br>  Portanto, existem clientes m√≥veis e servi√ßos de back-end dos quais voc√™ precisa para receber eventos.  O servi√ßo HTTP aceita os dados, realiza a valida√ß√£o m√≠nima, coleta eventos no disco local em arquivos agrupados por um minuto, os compacta imediatamente e os envia para o bucket S3.  A disponibilidade desse servi√ßo depende da disponibilidade dos servidores com o aplicativo e o AWS S3.  Os aplicativos n√£o armazenam estado, portanto, s√£o facilmente equilibrados, dimensionados e intercambiados.  O S3 √© um servi√ßo de armazenamento de arquivos relativamente simples, com boa reputa√ß√£o e disponibilidade, para que voc√™ possa confiar nele. <br><br>  Em seguida, voc√™ precisa fornecer os dados para o Redshift.  Tudo √© bem simples aqui: o Redshift possui um importador S3 embutido, que √© a maneira recomendada de carregar dados.  Portanto, a cada 10 minutos, inicia um script que se conecta ao Redshift e solicita que ele baixe os dados usando o prefixo <code>s3://events-bucket/main/year=2018/month=10/day=14/10_3*</code> <br><br>  Para monitorar o status da tarefa de download, usamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Apache Airflow</a> : permite repetir a opera√ß√£o em caso de erros e ter um hist√≥rico de execu√ß√£o claro, o que √© importante para um grande n√∫mero de tarefas.  E em caso de problemas, voc√™ pode repetir o download por alguns intervalos de tempo ou baixar os dados "frios" do S3 h√° um ano. <br><br>  No mesmo fluxo de ar, da mesma maneira, de acordo com a programa√ß√£o, os scripts funcionam que se conectam ao banco de dados e executam downloads peri√≥dicos de reposit√≥rios externos, ou constroem agrega√ß√µes sobre eventos na forma de <code>INSERT INTO ... SELECT ...</code> <br><br>  O desvio para o vermelho tem fracas garantias de disponibilidade.  Uma vez por semana, por at√© meia hora (a janela de tempo √© especificada nas configura√ß√µes) A ‚Äã‚ÄãAWS pode impedir que o cluster atualize ou qualquer outro trabalho agendado.  No caso de uma falha em um n√≥, o cluster tamb√©m fica indispon√≠vel at√© que o host seja restaurado.  Isso geralmente leva cerca de 15 minutos e acontece uma vez a cada seis meses.  No sistema atual, isso n√£o √© um problema, ele foi originalmente projetado para que a base fique periodicamente indispon√≠vel. <br><br>  No Redshift, foram usadas 4 inst√¢ncias ds2.8xlarge (36 CPU, 16 TB HDD), o que totaliza 64 TB de espa√ßo em disco. <br><br>  O √∫ltimo ponto √© o backup.  O agendamento de backup pode ser especificado nas configura√ß√µes do cluster e funciona bem. <br><br><h2>  Motiva√ß√£o de transi√ß√£o da ClickHouse </h2><br>  Obviamente, se n√£o houvesse problemas, ningu√©m teria pensado em migrar para o ClickHouse.  No entanto, eles eram. <br><br>  Se voc√™ observar o esquema de armazenamento ClickHouse com o mecanismo MergeTree e Redshift, poder√° ver que a ideologia deles √© muito semelhante.  Ambos os bancos de dados s√£o colunares, funcionam bem com um grande n√∫mero de colunas e compactam muito bem os dados no disco (e no Redshift voc√™ pode configurar os tipos de compacta√ß√£o para cada coluna individual).  At√© os dados s√£o armazenados da mesma maneira: eles s√£o classificados por chave prim√°ria, o que permite ler apenas blocos espec√≠ficos e n√£o manter √≠ndices individuais na mem√≥ria, e isso √© importante ao trabalhar com grandes quantidades de dados. <br><br>  A diferen√ßa essencial, como sempre, est√° nos detalhes. <br><br><h3>  Mesa di√°ria </h3><br>  A classifica√ß√£o dos dados no disco e a exclus√£o deles no Redshift ocorre quando voc√™ faz: <pre> <code class="xml hljs">VACUUM <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tablename</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre>  Nesse caso, o processo de v√°cuo trabalha com todos os dados nesta tabela.  Se voc√™ armazenar dados dos tr√™s meses em uma tabela, esse processo levar√° um tempo indecente e ser√° necess√°rio execut√°-lo pelo menos diariamente, porque os dados antigos s√£o exclu√≠dos e os novos s√£o adicionados.  Eu tive que criar tabelas separadas para cada dia e combin√°-las atrav√©s da Visualiza√ß√£o, e isso n√£o √© apenas a dificuldade de rotacionar e dar suporte a essa Visualiza√ß√£o, mas tamb√©m a desacelera√ß√£o das consultas.  Mediante solicita√ß√£o, julgando por explica√ß√£o, todas as tabelas foram digitalizadas.  E, embora a varredura de uma tabela leve menos de um segundo, com uma quantidade de 90 partes, qualquer consulta leva pelo menos um minuto.  Isso n√£o √© muito conveniente. <br><br><h3>  Duplicatas </h3><br>  O pr√≥ximo problema √© duplicado.  De uma forma ou de outra, ao transmitir dados pela rede, existem duas op√ß√µes: perder dados ou receber duplicatas.  Como n√£o pudemos perder mensagens, simplesmente nos reconciliamos com o fato de que uma pequena porcentagem de eventos seria duplicada.  Voc√™ pode excluir duplicatas por dia criando uma nova tabela, inserindo dados da antiga, onde as linhas da fun√ß√£o de janela com identifica√ß√£o duplicada s√£o exclu√≠das, a tabela antiga √© exclu√≠da e a nova √© renomeada.  Como havia uma exibi√ß√£o em cima das tabelas di√°rias, era necess√°rio n√£o esquec√™-la e exclu√≠-la para a hora de renomear as tabelas.  Nesse caso, tamb√©m era necess√°rio monitorar os bloqueios; caso contr√°rio, no caso de uma consulta que bloqueasse a exibi√ß√£o ou uma das tabelas, esse processo poderia ser arrastado por um longo tempo. <br><br><h3>  Monitoramento e manuten√ß√£o </h3><br>  Nem uma √∫nica solicita√ß√£o no Redshift leva menos de alguns segundos.  Mesmo se voc√™ quiser apenas adicionar um usu√°rio ou ver uma lista de solicita√ß√µes ativas, precisar√° aguardar algumas dezenas de segundos.  Obviamente, voc√™ pode tolerar e, para essa classe de bancos de dados, isso √© aceit√°vel, mas no final isso se traduz em um monte de tempo perdido. <br><br><h3>  Custo </h3><br>  De acordo com nossos c√°lculos, a implanta√ß√£o de ClickHouse nas inst√¢ncias da AWS com exatamente os mesmos recursos √© exatamente metade do pre√ßo.  Obviamente, deve ser assim, porque, usando o Redshift, voc√™ obt√©m um banco de dados pronto para conectar-se a qualquer cliente PostgreSQL logo ap√≥s clicar em alguns bot√µes no console da AWS, e a AWS far√° o resto por voc√™.  Mas vale a pena?  J√° temos a infraestrutura, parecemos capazes de fazer backups, monitoramento e configura√ß√£o, e fazemos isso para v√°rios servi√ßos internos.  Por que n√£o enfrentar o suporte ao ClickHouse? <br><br><h2>  Processo de transi√ß√£o </h2><br>  Primeiro, criamos uma pequena instala√ß√£o ClickHouse em uma m√°quina, onde come√ßamos periodicamente, usando as ferramentas internas, a baixar dados do S3.  Assim, pudemos testar nossas suposi√ß√µes sobre a velocidade e os recursos do ClickHouse. <br><br>  Ap√≥s algumas semanas de teste em uma c√≥pia pequena dos dados, ficou claro que, para substituir o Redshift pelo Clickhouse, v√°rios problemas precisariam ser resolvidos: <br><br><ul><li>  em que tipos de inst√¢ncias e discos implantar; </li><li>  usar replica√ß√£o? </li><li>  como instalar, configurar e executar; </li><li>  como fazer monitoramento; </li><li>  que tipo de esquema ser√°; </li><li>  como entregar dados do S3; </li><li>  Como reescrever todas as consultas do SQL padr√£o para n√£o padr√£o? </li></ul><br>  <b>Tipos de inst√¢ncias e discos</b> .  No n√∫mero de processadores, disco e mem√≥ria, eles decidiram desenvolver a instala√ß√£o atual do Redshift.  Havia v√°rias op√ß√µes, incluindo inst√¢ncias i3 com discos NVMe locais, mas decidiu parar em r5.4xlarge e armazenamento na forma de 8T ST1 EBS para cada inst√¢ncia.  Segundo estimativas, isso deveria ter proporcionado desempenho compar√°vel ao Redshift pela metade do custo.  Ao mesmo tempo, devido ao uso de discos EBS, obtemos backups simples e recupera√ß√£o atrav√©s de instant√¢neos de discos, quase como no Redshift. <br><br>  <b>Replica√ß√£o</b> .  Desde que come√ßamos do que j√° est√° no Redshift, decidimos n√£o usar a replica√ß√£o.  Al√©m disso, isso n√£o nos obriga a estudar imediatamente o ZooKeeper, que ainda n√£o est√° na infraestrutura, mas √© √≥timo que agora seja poss√≠vel fazer replica√ß√£o sob demanda. <br><br>  <b>Instala√ß√£o</b>  Esta √© a parte mais f√°cil.  Uma fun√ß√£o suficientemente pequena para o Ansible, que instala pacotes RPM prontos e faz a mesma configura√ß√£o em cada host. <br><br>  <b>Monitoramento</b>  Para monitorar todos os servi√ßos, o Prometheus √© usado junto com o Telegraf e o Grafana; portanto, eles simplesmente colocam os agentes do Telegraf nos hosts do ClickHouse, coletam um painel no Grafana, que mostra a carga atual do servidor por processador, mem√≥ria e discos.  Por meio do plug-in da Grafana, trouxemos para esse painel as solicita√ß√µes ativas atuais para o cluster, o status das importa√ß√µes do S3 e outras coisas √∫teis.  Ficou ainda melhor e mais informativo (e significativamente mais r√°pido) do que o painel que dava ao console da AWS. <br><br>  <b>Esquema</b> .  Um dos nossos principais erros no Redshift foi colocar apenas os campos do evento principal em colunas separadas e adicionar os campos que raramente s√£o usados ‚Äã‚Äãpara adicionar <br>  em uma grande coluna de propriedades.  Por um lado, isso nos deu flexibilidade para alterar os campos nos est√°gios iniciais, quando n√£o havia um entendimento completo de exatamente quais eventos coletar√≠amos, com quais propriedades, al√©m disso, eles mudavam 5 vezes por dia.  Por outro lado, os pedidos de uma grande coluna de propriedades levavam cada vez mais tempo.  No ClickHouse, decidimos fazer a coisa certa imediatamente, ent√£o coletamos todas as colunas poss√≠veis e inserimos o tipo ideal para elas.  O resultado √© uma tabela com aproximadamente duzentas colunas. <br><br>  A pr√≥xima tarefa foi escolher o mecanismo certo para armazenamento e particionamento. <br>  Eles n√£o pensaram em particionar novamente, mas fizeram o mesmo que no Redshift - uma parti√ß√£o para cada dia, mas agora todas as parti√ß√µes s√£o uma tabela, que <br>  acelera significativamente as solicita√ß√µes e simplifica a manuten√ß√£o.  O mecanismo de armazenamento foi utilizado pelo ReplacingMergeTree, pois permite remover duplicatas de uma parti√ß√£o espec√≠fica, simplesmente executando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">OPTIMIZE ... FINAL</a> .  Al√©m disso, o esquema de particionamento di√°rio permite, em caso de erros ou acidentes, trabalhar apenas com dados de um dia, n√£o de um m√™s, o que √© muito mais r√°pido. <br><br>  <b>Entrega de dados de s3 para ClickHouse</b> .  Esse foi um dos processos mais longos.  Simplesmente n√£o funcionou fazendo o carregamento pelas ferramentas internas do ClickHouse, porque os dados no S3 est√£o em JSON, cada campo precisa ser extra√≠do em seu pr√≥prio jsonpath, como fizemos no Redshift, e √†s vezes tamb√©m precisamos usar a transforma√ß√£o: por exemplo, o UUID de uma mensagem de um registro padr√£o no formato <code>DD96C92F-3F4D-44C6-BCD3-E25EB26389E9</code> converte em bytes e insere o tipo FixedString (16). <br><br>  Eu queria ter um servi√ßo especial semelhante ao que t√≠nhamos no Redshift como um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">comando COPY</a> .  Eles n√£o encontraram nada pronto, ent√£o eu tive que faz√™-lo.  Voc√™ pode escrever um artigo separado sobre como ele funciona, mas, em resumo, este √© um servi√ßo HTTP implantado em todos os hosts do ClickHouse.  Voc√™ pode se referir a qualquer um deles.  Os par√¢metros de solicita√ß√£o especificam o prefixo S3 do qual os arquivos s√£o obtidos, a lista jsonpath para convers√£o de JSON em um conjunto de colunas, bem como um conjunto de convers√µes para cada coluna.  O servidor para o qual a solicita√ß√£o veio come√ßa a varredura de arquivos no S3 e a distribui√ß√£o do trabalho de an√°lise para outros hosts.  Ao mesmo tempo, √© importante para n√≥s que as linhas que n√£o puderam ser importadas, juntamente com o erro, sejam adicionadas a uma tabela ClickHouse separada.  Isso ajuda muito a investigar problemas e bugs no servi√ßo de recebimento de eventos e nos clientes que geram esses eventos.  Com a coloca√ß√£o do importador diretamente nos hosts do banco de dados, utilizamos esses recursos, que, em regra, est√£o ociosos, porque solicita√ß√µes complexas n√£o funcionam 24 horas por dia.  Obviamente, se houver mais solicita√ß√µes, voc√™ sempre poder√° levar o servi√ßo do importador para separar hosts. <br><br>  N√£o houve grandes problemas com a importa√ß√£o de dados de fontes externas.  Nesses scripts, eles apenas mudaram o destino de Redshift para ClickHouse. <br><br>  Havia uma op√ß√£o para conectar o MongoDB na forma de um dicion√°rio, e n√£o fazer c√≥pias di√°rias.  Infelizmente, n√£o se encaixou, porque o dicion√°rio deve ser colocado na mem√≥ria e o tamanho da maioria das cole√ß√µes no MongoDB n√£o permite isso.  Mas os dicion√°rios tamb√©m nos foram √∫teis: us√°-los √© muito conveniente para conectar bancos de dados GeoIP do MaxMind e usar em consultas.  Para isso, usamos os arquivos ip_trie e CSV de layout fornecidos pelo servi√ßo.  Por exemplo, a configura√ß√£o do dicion√°rio geoip_asn_blocks_ipv4 √© assim: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>geoip_asn_blocks_ipv4<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span>GeoLite2-ASN-Blocks-IPv4.csv<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span>CSVWithNames<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">\</span></span></span><span class="hljs-tag">/</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span>300<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">ip_trie</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>prefix<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_number<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>UInt32<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>0<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_organization<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>?<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br>  Basta colocar essa configura√ß√£o em <code>/etc/clickhouse-server/geoip_asn_blocks_ipv4_dictionary.xml</code> , ap√≥s o qual voc√™ pode fazer consultas no dicion√°rio para obter o nome do provedor pelo endere√ßo IP: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dictGetString(<span class="hljs-string"><span class="hljs-string">'geoip_asn_blocks_ipv4'</span></span>, <span class="hljs-string"><span class="hljs-string">'autonomous_system_organization'</span></span>, tuple(IPv4StringToNum(<span class="hljs-string"><span class="hljs-string">'192.168.1.1'</span></span>)));</code> </pre><br>  <b>Alterar esquema de dados</b> .  Como mencionado acima, decidimos n√£o usar a replica√ß√£o ainda, j√° que agora podemos nos tornar inacess√≠veis em caso de acidente ou trabalho planejado, e uma c√≥pia dos dados j√° est√° no s3 e podemos transferi-los para o ClickHouse em um per√≠odo de tempo razo√°vel.  Se n√£o houver replica√ß√£o, eles n√£o expandiram o ZooKeeper e a aus√™ncia do ZooKeeper tamb√©m leva √† incapacidade de usar a express√£o ON CLUSTER nas consultas DDL.  Esse problema foi resolvido por um pequeno script python que se conecta a cada host ClickHouse (existem apenas oito deles at√© agora) e executa a consulta SQL especificada. <br><br>  <b>Suporte SQL incompleto no ClickHouse</b> .  O processo de transfer√™ncia de solicita√ß√µes da sintaxe Redshift para a sintaxe ClickHouse foi paralelo ao desenvolvimento do importador, e foi tratado principalmente por uma equipe de analistas.  Curiosamente, mas o assunto n√£o estava nem no JOIN, mas nas fun√ß√µes da janela.  Para entender como eles podem ser feitos por meio de matrizes e fun√ß√µes lambda, levou v√°rios dias.  √â bom que esse problema seja frequentemente abordado em relat√≥rios sobre o ClickHouse, dos quais h√° um grande n√∫mero, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">events.yandex.ru/lib/talks/5420</a> .  Nesse momento, os dados j√° estavam gravados de uma s√≥ vez em dois locais: no Redshift e no novo ClickHouse; portanto, quando transferimos as solicita√ß√µes, comparamos os resultados.  Foi problem√°tico comparar a velocidade, uma vez que removemos uma grande coluna de propriedades, e a maioria das consultas come√ßou a funcionar apenas com as colunas necess√°rias, o que, √© claro, deu um aumento significativo, mas as consultas em que a coluna de propriedades n√£o participou, funcionaram da mesma maneira ou um pouco mais r√°pido. <br><br>  Como resultado, obtivemos o seguinte esquema: <br><br><img src="https://habrastorage.org/webt/tj/h8/ka/tjh8kagqccdbjgnswbmbm9wkloc.png"><br><br><h2>  Resultados </h2><br>  Resumindo, obtivemos os seguintes benef√≠cios: <br><br><ul><li>  Uma mesa em vez de 90 </li><li>  Solicita√ß√µes de servi√ßo s√£o executadas em milissegundos </li><li>  O custo caiu pela metade </li><li>  Remo√ß√£o f√°cil de eventos duplicados </li></ul><br>  Tamb√©m h√° desvantagens para as quais estamos prontos: <br><br><ul><li>  Em caso de acidente, voc√™ ter√° que reparar o cluster sozinho </li><li>  As altera√ß√µes de esquema agora precisam ser feitas em cada host separadamente </li><li>  A atualiza√ß√£o para novas vers√µes ter√° que fazer voc√™ mesmo </li></ul><br>  N√£o podemos comparar a velocidade das solicita√ß√µes de frente, pois o esquema de dados mudou significativamente.  Muitas consultas se tornaram mais r√°pidas, simplesmente porque eles l√™em menos dados do disco.  De uma maneira boa, essa altera√ß√£o teve que ser feita novamente no Redshift, mas foi decidido combin√°-la com a migra√ß√£o para o ClickHouse. <br><br>  Toda a migra√ß√£o, juntamente com a prepara√ß√£o, levou cerca de tr√™s meses.  Ela caminhou do in√≠cio de julho at√© o final de setembro e exigiu a participa√ß√£o de duas pessoas.  Em 27 de setembro, desligamos o Redshift e, desde ent√£o, trabalhamos apenas no ClickHouse.  Acontece, j√° um pouco mais de dois meses.  O termo √© curto, mas at√© agora nunca houve perda de dados ou bug cr√≠tico, por causa do qual todo o cluster seria ativado.  √Ä nossa frente, aguardamos atualiza√ß√µes em novas vers√µes! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt433346/">https://habr.com/ru/post/pt433346/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt433336/index.html">A implementa√ß√£o da biblioteca babil√¥nica</a></li>
<li><a href="../pt433338/index.html">Vis√£o geral do fabricante da impressora 3D Creality</a></li>
<li><a href="../pt433340/index.html">Dispositivos sem fio Xiaomi no ioBroker smart home</a></li>
<li><a href="../pt433342/index.html">Outro processador verilog simples</a></li>
<li><a href="../pt433344/index.html">Dois sucessos de espa√ßo privado</a></li>
<li><a href="../pt433348/index.html">DSL digitado no TypeScript de JSX</a></li>
<li><a href="../pt433350/index.html">Eventos digitais em Moscou, de 17 a 23 de dezembro</a></li>
<li><a href="../pt433352/index.html">O resumo de materiais frescos do mundo do front-end da √∫ltima semana n ¬∞ 343 (10 a 16 de dezembro de 2018)</a></li>
<li><a href="../pt433354/index.html">Not√≠cias do mundo do OpenStreetMap n¬∫ 438 (04/04/2018 - 10/12/2018)</a></li>
<li><a href="../pt433356/index.html">Os invasores aprenderam a ignorar a autentica√ß√£o de dois fatores Yahoo Mail e Gmail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>