<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∂üèΩ üîÜ üëºüèΩ VShard - mise √† l'√©chelle horizontale dans Tarantool üë©‚Äç‚ù§Ô∏è‚Äçüë® ü§üüèª üóùÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, je m'appelle Vladislav et je fais partie de l'√©quipe de d√©veloppement de Tarantool . Tarantool est √† la fois un SGBD et un serveur d'applicat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>VShard - mise √† l'√©chelle horizontale dans Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/442782/"><img src="https://habrastorage.org/webt/4p/e8/fo/4pe8foryc_t_l5joliydwpislhm.png"><br><br>  Bonjour, je m'appelle Vladislav et je fais partie de l'√©quipe de d√©veloppement de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool</a> .  Tarantool est √† la fois un SGBD et un serveur d'applications.  Aujourd'hui, je vais raconter comment nous avons mis en ≈ìuvre la mise √† l'√©chelle horizontale dans Tarantool au moyen du module <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VShard</a> . <br><br>  Quelques connaissances de base en premier. <br><br>  Il existe deux types de mise √† l'√©chelle: horizontale et verticale.  Et il existe deux types de mise √† l'√©chelle horizontale: la r√©plication et le partitionnement.  La r√©plication assure la mise √† l'√©chelle du calcul tandis que le partage est utilis√© pour la mise √† l'√©chelle des donn√©es. <br><br>  Le partage est √©galement subdivis√© en deux types: le partage bas√© sur la plage et le partage bas√© sur le hachage. <br><br>  Le partage bas√© sur la plage implique qu'une cl√© de fragment est calcul√©e pour chaque enregistrement de cluster.  Les cl√©s de partition sont projet√©es sur une ligne droite qui est s√©par√©e en plages et affect√©e √† diff√©rents n≈ìuds physiques. <br><br>  Le partage bas√© sur le hachage est moins compliqu√©: une fonction de hachage est calcul√©e pour chaque enregistrement d'un cluster;  les enregistrements avec la m√™me fonction de hachage sont allou√©s au m√™me n≈ìud physique. <br><br>  Je vais me concentrer sur la mise √† l'√©chelle horizontale √† l'aide du partage bas√© sur le hachage. <br><a name="habracut"></a><br><h2>  Ancienne impl√©mentation </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool Shard</a> √©tait notre module d'origine pour la mise √† l'√©chelle horizontale.  Il a utilis√© une partition bas√©e sur le hachage simple et des cl√©s de partition calcul√©es par cl√© primaire pour tous les enregistrements d'un cluster. <br><br><pre><code class="plaintext hljs">function shard_function(primary_key) return guava(crc32(primary_key), shard_count) end</code> </pre> <br>  Mais finalement Tarantool Shard est devenu incapable de s'attaquer √† de nouvelles t√¢ches. <br><br>  Premi√®rement, l'une de nos √©ventuelles exigences est devenue la <b>localisation</b> garantie <b>des donn√©es li√©es logiquement</b> .  En d'autres termes, lorsque nous avons des donn√©es li√©es de mani√®re logique, nous voulons toujours les stocker sur un seul n≈ìud physique, ind√©pendamment de la topologie du cluster et des modifications d'√©quilibrage.  Tarantool Shard ne peut pas garantir cela.  Il a calcul√© les hachages uniquement avec les cl√©s primaires, et donc le r√©√©quilibrage pourrait entra√Æner la s√©paration temporaire des enregistrements avec le m√™me hachage car les modifications ne sont pas effectu√©es de mani√®re atomique. <br><br>  Ce manque de localisation des donn√©es √©tait le principal probl√®me pour nous.  Voici un exemple.  Disons qu'il y a une banque o√π un client a ouvert un compte.  Les informations sur le compte et le client doivent √™tre stock√©es physiquement ensemble afin de pouvoir √™tre r√©cup√©r√©es en une seule demande ou modifi√©es en une seule transaction, par exemple lors d'un transfert d'argent.  Si nous utilisons le partage traditionnel de Tarantool Shard, il y aura diff√©rentes valeurs de fonction de hachage pour les comptes et les clients.  Les donn√©es pourraient se retrouver sur des n≈ìuds physiques distincts.  Cela complique vraiment la lecture et les transactions avec les donn√©es d'un client. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  Dans l'exemple ci-dessus, les champs id des comptes et du client peuvent √™tre incoh√©rents.  Ils sont connect√©s par le champ customer_id du compte et le champ id du client.  Le m√™me champ id violerait la contrainte d'unicit√© de la cl√© primaire du compte.  Et Shard ne peut pas effectuer le partage d'une autre mani√®re. <br><br>  Un autre probl√®me √©tait <b>le r√©√©chantillonnage lent</b> , qui est le probl√®me fondamental de tous les fragments de hachage.  L'essentiel est que lors du changement de composants de cluster, la fonction de partition change car elle d√©pend g√©n√©ralement du nombre de n≈ìuds.  Ainsi, lorsque la fonction change, il est n√©cessaire de parcourir tous les enregistrements du cluster et de recalculer la fonction.  Il peut √©galement √™tre n√©cessaire de transf√©rer certains enregistrements.  Et pendant le transfert de donn√©es, nous ne savons m√™me pas si l'enregistrement requis?  Dans la demande, les donn√©es ont d√©j√† √©t√© transf√©r√©es ou sont en cours de transfert.  Ainsi, lors du nouveau partage, il est n√©cessaire de faire des demandes de lecture avec les anciennes et les nouvelles fonctions de partition.  Les demandes sont trait√©es deux fois plus lentement, ce qui est inacceptable. <br><br>  Un autre probl√®me avec Tarantool Shard √©tait la faible disponibilit√© des lectures en cas de d√©faillance d'un n≈ìud dans un jeu de r√©plicas. <br><br><h2>  Nouvelle solution </h2><br>  Nous avons cr√©√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool VShard</a> pour r√©soudre les trois probl√®mes mentionn√©s ci-dessus.  Sa principale diff√©rence est que son niveau de stockage de donn√©es est virtualis√©, c'est-√†-dire que les stockages physiques h√©bergent des stockages virtuels et que les enregistrements de donn√©es sont allou√©s sur les virtuels.  Ces stockages sont appel√©s <i>seaux</i> .  L'utilisateur n'a pas √† se soucier de ce qui se trouve sur un n≈ìud physique donn√©.  Un compartiment est une unit√© de donn√©es atomique indivisible, comme un tuple dans le partage traditionnel.  VShard stocke toujours un compartiment entier sur un n≈ìud physique et lors du nouveau partage, il migre toutes les donn√©es d'un compartiment de mani√®re atomique.  Cette m√©thode garantit la localisation des donn√©es.  Nous pla√ßons simplement les donn√©es dans un seul compartiment, et nous pouvons toujours √™tre s√ªrs qu'elles ne seront pas s√©par√©es lors des changements de cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/a4f/87b/42ea4f87b5c0f0b05bdf0e0c75b356fe.png"><br><br>  Comment regrouper les donn√©es dans un seul compartiment?  Ajoutons un nouveau champ d'ID de compartiment √† la table pour notre client bancaire.  Si cette valeur de champ est la m√™me pour les donn√©es li√©es, tous les enregistrements seront dans un seul compartiment.  L'avantage est que nous pouvons stocker des enregistrements avec le m√™me identifiant de compartiment dans diff√©rents espaces, et m√™me dans diff√©rents moteurs.  La localisation des donn√©es bas√©e sur l'id du compartiment est garantie quelle que soit la m√©thode de stockage. <br><br><pre> <code class="plaintext hljs">format = {{'id', 'unsigned'}, {'email', 'string'}, {'bucket_id', 'unsigned'}} box.schema.create_space('customer', {format = format}) format = {{'id', 'unsigned'}, {'customer_id', 'unsigned'}, {'balance', 'number'}, {'bucket_id', 'unsigned'}} box.schema.create_space('account', {format = format})</code> </pre> <br>  Pourquoi est-ce si important?  Lors de l'utilisation du partitionnement traditionnel, les donn√©es s'√©tendraient √† divers stockages physiques existants.  Pour notre exemple de banque, nous devons contacter chaque n≈ìud lors de la demande de tous les comptes d'un client donn√©.  On obtient donc une complexit√© de lecture O (N), o√π N est le nombre de stockages physiques.  C'est incroyablement lent. <br><br>  L'utilisation de compartiments et de la localit√© par identifiant de compartiment permet de lire les donn√©es n√©cessaires √† partir d'un n≈ìud √† l'aide d'une seule demande, quelle que soit la taille du cluster. <br><br><img src="https://habrastorage.org/webt/t7/_r/fm/t7_rfmxoroosmaoqbe8cskpsr0k.png"><br><br>  Dans VShard, vous calculez votre identifiant de compartiment et vous l'affectez.  Pour certaines personnes, c'est un avantage, tandis que d'autres le consid√®rent comme un inconv√©nient.  Je crois que la possibilit√© de choisir votre propre fonction pour le calcul de l'identifiant du compartiment est un avantage. <br><br>  Quelle est la principale diff√©rence entre le sharding traditionnel et le sharding virtuel avec des godets? <br><br>  Dans le premier cas, lorsque nous modifions les composants d'un cluster, nous avons deux √©tats: l'ancien (l'ancien) et le nouveau √† impl√©menter.  Dans le processus de transition, il est n√©cessaire non seulement de migrer les donn√©es, mais √©galement de recalculer la fonction de hachage pour chaque enregistrement.  Ce n'est pas tr√®s pratique car √† un moment donn√©, nous ne savons pas si les donn√©es requises ont d√©j√† √©t√© migr√©es ou non.  En outre, cette m√©thode n'est pas fiable et les modifications ne sont pas atomiques, car la migration atomique de l'ensemble d'enregistrements avec la m√™me valeur de fonction de hachage n√©cessiterait un stockage persistant de l'√©tat de migration au cas o√π une r√©cup√©ration serait n√©cessaire.  En cons√©quence, il y a des conflits et des erreurs, et l'op√©ration doit √™tre red√©marr√©e plusieurs fois. <br><br>  Le partage virtuel est beaucoup plus simple.  Nous n'avons pas deux √©tats de cluster diff√©rents;  nous n'avons que l'√©tat du seau.  Le cluster est plus flexible, il passe en douceur d'un √©tat √† un autre.  Il y a maintenant plus de deux √âtats?  (peu clair).  Avec la transition en douceur, il est possible de modifier l'√©quilibrage √† la vol√©e ou de supprimer les nouveaux stockages ajout√©s.  Autrement dit, le contr√¥le d'√©quilibrage a consid√©rablement augment√© et est devenu plus granulaire. <br><br><h2>  Utilisation </h2><br>  Supposons que nous avons s√©lectionn√© une fonction pour notre identifiant de compartiment et que nous ayons t√©l√©charg√© tant de donn√©es dans le cluster qu'il ne reste plus d'espace.  Maintenant, nous aimerions ajouter des n≈ìuds et y d√©placer automatiquement les donn√©es.  C'est ainsi que nous proc√©dons dans VShard: d'abord, nous d√©marrons de nouveaux n≈ìuds et y ex√©cutons Tarantool, puis nous mettons √† jour notre configuration VShard.  Il contient des informations sur chaque composant de cluster, chaque r√©plique, jeux de r√©plicas, ma√Ætres, URI attribu√©s et bien plus encore.  Maintenant, nous ajoutons nos nouveaux n≈ìuds dans le fichier de configuration et l'appliquons √† tous les n≈ìuds de cluster √† l'aide de VShard.storage.cfg. <br><br><pre> <code class="plaintext hljs">function create_user(email) local customer_id = next_id() local bucket_id = crc32(customer_id) box.space.customer:insert(customer_id, email, bucket_id) end function add_account(customer_id) local id = next_id() local bucket_id = crc32(customer_id) box.space.account:insert(id, customer_id, 0, bucket_id) end</code> </pre> <br>  Comme vous vous en souvenez peut-√™tre, lorsque vous modifiez le nombre de n≈ìuds dans le partage traditionnel, la fonction de partition elle-m√™me change √©galement.  Cela ne se produit pas dans VShard.  Ici, nous avons un nombre fixe de stockages virtuels ou de compartiments.  Il s'agit d'une constante que vous choisissez lors du d√©marrage du cluster.  Il peut sembler que l'√©volutivit√© est donc limit√©e, mais ce n'est vraiment pas le cas.  Vous pouvez sp√©cifier un grand nombre de compartiments, des dizaines et des centaines de milliers.  La chose importante √† savoir est qu'il devrait y avoir au moins deux ordres de grandeur de plus de compartiments que le nombre maximum de jeux de r√©plicas que vous aurez jamais dans le cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/422/499/979/422499979e5b8c5728c3df2b967cf599.gif"><br><br>  √âtant donn√© que le nombre de stockages virtuels ne change pas et que la fonction de partition ne d√©pend que de cette valeur, nous pouvons ajouter autant de stockages physiques que nous le souhaitons sans recalculer la fonction de partition. <br><br>  Alors, comment les compartiments sont-ils attribu√©s aux stockages physiques?  Si VShard.storage.cfg est appel√©, un processus de r√©√©quilibrage se r√©veille sur l'un des n≈ìuds.  Il s'agit d'un processus analytique qui calcule l'√©quilibre parfait pour le cluster.  Le processus va √† chaque n≈ìud physique et r√©cup√®re son nombre de compartiments, puis construit des itin√©raires de leurs mouvements afin d'√©quilibrer l'allocation.  Le r√©√©quilibreur envoie ensuite les routes vers les stockages surcharg√©s, qui √† leur tour commencent √† envoyer des compartiments.  Un peu plus tard, le cluster est √©quilibr√©. <br><br>  Dans les projets du monde r√©el, un √©quilibre parfait peut ne pas √™tre atteint aussi facilement.  Par exemple, un jeu de r√©pliques peut contenir moins de donn√©es que l'autre car il a moins de capacit√© de stockage.  Dans ce cas, VShard peut penser que tout est √©quilibr√© mais en fait le premier stockage est sur le point de surcharger.  Pour contrer cela, nous avons fourni un m√©canisme pour corriger les r√®gles d'√©quilibrage au moyen de pond√©rations.  Un poids peut √™tre attribu√© √† n'importe quel jeu de r√©pliques ou stockage.  Lorsque le r√©√©quilibreur d√©cide combien de seaux doivent √™tre envoy√©s et o√π, il prend en compte les <b>relations</b> de toutes les paires de poids. <br><br>  Par exemple, si un stockage p√®se 100 et l'autre 200, le second stockera deux fois plus de seaux que le premier.  Veuillez noter que je parle sp√©cifiquement des <b>relations de</b> poids.  Les valeurs absolues n'ont aucune influence.  Vous choisissez des pond√©rations bas√©es sur une distribution √† 100% dans un cluster: donc 30% pour un stockage donnerait 70% pour l'autre.  Vous pouvez prendre la capacit√© de stockage en gigaoctets comme base ou mesurer le poids du nombre de compartiments.  Le plus important est de garder le ratio n√©cessaire. <br><br><img src="https://habrastorage.org/webt/sz/0v/gi/sz0vgicyunfvpamx3ic8enwsl58.png"><br><br>  Cette m√©thode a un effet secondaire int√©ressant: si un stockage se voit attribuer un poids nul, le r√©√©quilibreur fera redistribuer ce stockage √† tous ses compartiments.  Par la suite, vous pouvez supprimer l'ensemble de r√©plicas complet de la configuration. <br><br><h2>  Migration du compartiment atomique </h2><br>  Nous avons un seau;  il accepte certaines lectures et √©critures, et √† un moment donn√©, le r√©√©quilibreur demande sa migration vers un autre stockage.  Le compartiment cesse d'accepter les demandes d'√©criture, sinon il serait mis √† jour pendant la migration, puis √† nouveau mis √† jour pendant la migration de mise √† jour, puis la mise √† jour serait mise √† jour, etc.  Par cons√©quent, les demandes d'√©criture sont bloqu√©es, mais la lecture √† partir du compartiment est toujours possible.  Les donn√©es sont d√©sormais migr√©es vers le nouvel emplacement.  Une fois la migration termin√©e, le compartiment recommence √† accepter les demandes.  Il existe toujours dans l'ancien emplacement, mais il est marqu√© comme poubelle, et plus tard le garbage collector le supprime pi√®ce par pi√®ce. <br><br>  Certaines m√©tadonn√©es sont physiquement stock√©es sur le disque associ√© √† chaque compartiment.  Toutes les √©tapes d√©crites ci-dessus sont stock√©es sur le disque, et quoi qu'il arrive au stockage, l'√©tat du compartiment sera automatiquement restaur√©. <br><br>  Vous pouvez avoir quelques questions suivantes: <br><br><ul><li>  <b>Qu'arrive-t-il aux demandes qui fonctionnent avec le compartiment au d√©but de la migration?</b> <br><br>  Il existe deux types de r√©f√©rences dans les m√©tadonn√©es de chaque compartiment: RO et RW.  Lorsqu'un utilisateur fait une demande √† un compartiment, il indique si le travail doit √™tre en lecture seule ou en lecture-√©criture.  Pour chaque demande, le compteur de r√©f√©rence correspondant est augment√©. <br><br>  Pourquoi avons-nous besoin de compteurs de r√©f√©rence pour les demandes d'√©criture?  Supposons qu'un compartiment soit en cours de migration et que, soudain, le garbage collector souhaite le supprimer.  Le garbage collector reconna√Æt que le compteur de r√©f√©rence est sup√©rieur √† z√©ro et donc le compartiment ne sera pas supprim√©.  Lorsque toutes les demandes sont termin√©es, le garbage collector peut faire son travail. <br><br>  Le compteur de r√©f√©rence pour les √©critures garantit √©galement que la migration du compartiment ne d√©marrera pas s'il y a au moins une demande d'√©criture en cours.  Mais l√† encore, les demandes d'√©criture pouvaient arriver l'une apr√®s l'autre et le compartiment ne serait jamais migr√©.  Donc, si le r√©√©quilibreur souhaite d√©placer le compartiment, le syst√®me bloque les nouvelles demandes d'√©criture en attendant que les demandes en cours soient termin√©es pendant un certain d√©lai.  Si les demandes ne sont pas termin√©es dans le d√©lai sp√©cifi√©, le syst√®me recommence √† accepter de nouvelles demandes d'√©criture tout en diff√©rant la migration du compartiment.  De cette fa√ßon, le r√©√©quilibreur tentera de migrer le compartiment jusqu'√† ce que la migration r√©ussisse. <br><br>  VShard dispose d'une API bucket_ref de bas niveau au cas o√π vous auriez besoin de plus que de simples capacit√©s de haut niveau.  Si vous voulez vraiment faire quelque chose vous-m√™me, veuillez vous r√©f√©rer √† cette API. </li><li>  <b>Est-il possible de laisser les enregistrements non bloqu√©s?</b> <br><br>  Non.  Si le compartiment contient des donn√©es critiques et n√©cessite un acc√®s permanent en √©criture, vous devrez bloquer compl√®tement sa migration.  Nous avons une fonction bucket_pin pour faire exactement cela.  Il √©pingle le compartiment au jeu de r√©plicas actuel afin que le r√©√©quilibreur ne puisse pas migrer le compartiment.  Dans ce cas, les godets adjacents pourront cependant se d√©placer sans contraintes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6a/848/fa7/b6a848fa775b0066ac6f69b73d97ed76.png"><br><br>  Un verrou de jeu de r√©pliques est un outil encore plus puissant que bucket_pin.  Cela ne se fait plus dans le code mais plut√¥t dans la configuration.  Un verrou de jeu de r√©plicas d√©sactive la migration de tout compartiment entrant / sortant du jeu de r√©plicas.  Ainsi, toutes les donn√©es seront disponibles en permanence pour les √©critures. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/65b/744/39c/65b74439c5b5743eda1168bdb320f8f4.png"></li></ul><br><h2>  VShard.router </h2><br>  VShard se compose de deux sous-modules: VShard.storage et VShard.router.  Nous pouvons les cr√©er et les mettre √† l'√©chelle ind√©pendamment sur une seule instance.  Lors de la demande d'un cluster, nous ne savons pas o√π se trouve un compartiment donn√©, et VShard.router le recherchera par identifiant de compartiment pour nous. <br><br>  Revenons √† notre exemple, le cluster bancaire avec les comptes clients.  Je voudrais pouvoir obtenir tous les comptes d'un certain client du cluster.  Cela n√©cessite une fonction standard pour la recherche locale: <br><br><img src="https://habrastorage.org/webt/q4/om/pp/q4omppscsww5c-cshatc6bnvgky.png"><br><br>  Il recherche tous les comptes du client par son identifiant.  Maintenant, je dois d√©cider o√π je dois ex√©cuter la fonction.  Dans ce but, je calcule l'id du bucket par identifiant client dans ma demande et demande √† VShard.router d'appeler la fonction dans le stockage o√π se trouve le bucket avec l'id du bucket cible.  Le sous-module poss√®de une table de routage qui d√©crit les emplacements des compartiments dans les jeux de r√©plicas.  VShard.router redirige ma demande. <br><br>  Il peut certainement arriver que l'√©clatement commence √† ce moment pr√©cis et que les godets commencent √† bouger.  Le routeur en arri√®re-plan met progressivement √† jour la table en gros morceaux en demandant les tables de compartiment actuelles aux stockages. <br><br>  Nous pouvons m√™me demander un compartiment r√©cemment migr√©, par lequel le routeur n'a pas encore mis √† jour sa table de routage.  Dans ce cas, il demandera l'ancien stockage, qui redirigera le routeur vers un autre stockage, ou r√©pondra simplement qu'il ne dispose pas des donn√©es n√©cessaires.  Ensuite, le routeur passera par chaque stockage √† la recherche du compartiment requis.  Et nous ne remarquerons m√™me pas une erreur dans la table de routage. <br><br><h2>  Lire le basculement </h2><br>  Rappelons nos probl√®mes initiaux: <br><br><ul><li>  Aucune localit√© de donn√©es.  R√©solu au moyen de seaux. </li><li>  Processus de resharding s'enlisant et retenant tout.  Nous avons impl√©ment√© le transfert de donn√©es atomiques au moyen de seaux et nous sommes d√©barrass√©s du recalcul de la fonction des fragments. </li><li>  Lire le basculement. </li></ul><br>  Le dernier probl√®me est r√©solu par VShard.router, pris en charge par le sous-syst√®me de basculement de lecture automatique. <br><br>  De temps en temps, le routeur envoie un ping aux stockages sp√©cifi√©s dans la configuration.  Supposons par exemple que le routeur ne puisse pas envoyer une requ√™te ping √† l'un d'eux.  Le routeur dispose d'une connexion de sauvegarde √† chaud √† chaque r√©plique, donc si la r√©plique actuelle ne r√©pond pas, elle passe simplement √† une autre.  Les demandes de lecture seront trait√©es normalement car nous pouvons lire sur les r√©pliques (mais pas √©crire).  Et nous pouvons sp√©cifier la priorit√© des r√©pliques comme facteur pour que le routeur choisisse le basculement pour les lectures.  Cela se fait par zonage. <br><br><img src="https://habrastorage.org/webt/aw/iz/ry/awizryylhzk9h2rct_kxo1-jvpc.png"><br><br>  Nous attribuons un num√©ro de zone √† chaque r√©plique et √† chaque routeur et sp√©cifions une table o√π nous indiquons la distance entre chaque paire de zones.  Lorsque le routeur d√©cide o√π envoyer une demande de lecture, il s√©lectionne une r√©plique dans la zone la plus proche. <br><br>  Voici √† quoi cela ressemble dans la configuration: <br><br><img src="https://habrastorage.org/webt/2w/jx/cu/2wjxcuidtcghobukd3mxu02ms2y.png"><br><br>  En g√©n√©ral, vous pouvez demander n'importe quelle r√©plique, mais si le cluster est volumineux, complexe et hautement distribu√©, le zonage peut √™tre tr√®s utile.  Diff√©rents racks de serveurs peuvent √™tre s√©lectionn√©s comme zones afin que le r√©seau ne soit pas surcharg√© par le trafic.  Alternativement, des points g√©ographiquement isol√©s peuvent √™tre s√©lectionn√©s. <br><br>  Le zonage est √©galement utile lorsque les r√©pliques pr√©sentent des comportements diff√©rents.  Par exemple, chaque jeu de r√©plicas poss√®de un r√©plica de sauvegarde qui ne doit pas accepter les demandes mais ne doit stocker qu'une copie des donn√©es.  Dans ce cas, nous le pla√ßons dans une zone √©loign√©e de tous les routeurs du tableau afin que le routeur n'adresse pas cette r√©plique √† moins que cela ne soit absolument n√©cessaire. <br><br><h2>  √âcriture de basculement </h2><br>  Nous avons d√©j√† parl√© du basculement en lecture.  Qu'en est-il du basculement en √©criture lors du changement de ma√Ætre?  Dans VShard, l'image n'est pas aussi rose qu'auparavant: la s√©lection principale n'est pas impl√©ment√©e, nous devrons donc la faire nous-m√™mes.  Lorsque nous avons en quelque sorte d√©sign√© un ma√Ætre, l'instance d√©sign√©e devrait maintenant prendre le relais en tant que ma√Ætre.  Ensuite, nous mettons √† jour la configuration en sp√©cifiant master = false pour l'ancien master et master = true pour le nouveau, appliquons la configuration au moyen de VShard.storage.cfg et la partageons avec chaque stockage.  Tout le reste se fait automatiquement.  L'ancien ma√Ætre cesse d'accepter les demandes d'√©criture et d√©marre la synchronisation avec le nouveau, car il se peut que des donn√©es aient d√©j√† √©t√© appliqu√©es sur l'ancien ma√Ætre mais pas sur le nouveau.  Apr√®s cela, le nouveau ma√Ætre est en charge et commence √† accepter les demandes, et l'ancien ma√Ætre est une r√©plique.  Voici comment fonctionne le basculement en √©criture dans VShard. <br><br><pre> <code class="plaintext hljs">replicas = new_cfg.sharding[uud].replicas replicas[old_master_uuid].master = false replicas[new_master_uuid].master = true vshard.storage.cfg(new_cfg)</code> </pre> <br><br><h2>  Comment suivons-nous ces diff√©rents √©v√©nements? </h2><br>  VShard.storage.info et VShard.router.info suffisent. <br><br>  VShard.storage.info affiche des informations dans plusieurs sections. <br><br><pre> <code class="plaintext hljs">vshard.storage.info() --- - replicasets: &lt;replicaset_2&gt;: uuid: &lt;replicaset_2&gt; master: uri: storage@127.0.0.1:3303 &lt;replicaset_1&gt;: uuid: &lt;replicaset_1&gt; master: missing bucket: receiving: 0 active: 0 total: 0 garbage: 0 pinned: 0 sending: 0 status: 2 replication: status: slave Alerts: - ['MISSING_MASTER', 'Master is not configured for ''replicaset &lt;replicaset_1&gt;']</code> </pre> <br>  La premi√®re section concerne la r√©plication.  Vous pouvez voir ici l'√©tat du jeu de r√©plicas o√π la fonction est appel√©e: son d√©calage de r√©plication, ses connexions disponibles et indisponibles, sa configuration principale, etc. <br><br>  Dans la section des compartiments, vous pouvez voir en temps r√©el le nombre de compartiments migr√©s vers / depuis le jeu de r√©plicas actuel, le nombre de compartiments fonctionnant en mode normal, le nombre de compartiments marqu√©s comme des ordures et le nombre de compartiments √©pingl√©s. <br><br>  La section Alertes affiche les probl√®mes que VShard a pu d√©terminer lui-m√™me: "le ma√Ætre n'est pas configur√©", "le niveau de redondance est insuffisant", "le ma√Ætre est l√†, mais toutes les r√©pliques ont √©chou√©", etc. <br><br>  Et la derni√®re section (q: est-ce "statut"?) Est une lumi√®re qui devient rouge quand tout va mal.  C'est un nombre de z√©ro √† trois, un nombre plus √©lev√© √©tant pire. <br><br>  VShard.router.info a les m√™mes sections, mais leur signification est quelque peu diff√©rente. <br><br><pre> <code class="plaintext hljs">vshard.router.info() --- - replicasets: &lt;replicaset_2&gt;: replica: &amp;0 status: available uri: storage@127.0.0.1:3303 uuid: 1e02ae8a-afc0-4e91-ba34-843a356b8ed7 bucket: available_rw: 500 uuid: &lt;replicaset_2&gt; master: *0 &lt;replicaset_1&gt;: replica: &amp;1 status: available uri: storage@127.0.0.1:3301 uuid: 8a274925-a26d-47fc-9e1b-af88ce939412 bucket: available_rw: 400 uuid: &lt;replicaset_1&gt; master: *1 bucket: unreachable: 0 available_ro: 800 unknown: 200 available_rw: 700 status: 1 alerts: - ['UNKNOWN_BUCKETS', '200 buckets are not discovered']</code> </pre> <br>  La premi√®re section concerne la r√©plication, bien qu'elle ne contienne pas d'informations sur les retards de r√©plication, mais plut√¥t des informations sur la disponibilit√©: connexions du routeur √† un jeu de r√©pliques;  connexion √† chaud et connexion de sauvegarde en cas de d√©faillance du ma√Ætre;  le ma√Ætre s√©lectionn√©;  et le nombre de godets RW et de godets RO disponibles sur chaque jeu de r√©pliques. <br><br>  La section de compartiment affiche le nombre total de compartiments en lecture-√©criture et en lecture seule actuellement disponibles pour ce routeur;  le nombre de seaux avec un emplacement inconnu;  et le nombre de compartiments avec un emplacement connu mais sans connexion au jeu de r√©plicas n√©cessaire. <br><br>  La section des alertes d√©crit principalement les connexions, les √©v√©nements de basculement et les compartiments non identifi√©s. <br><br>  Enfin, il y a aussi le statut simple?  Indicateur de z√©ro √† trois. <br><br><h2>  De quoi avez-vous besoin pour utiliser VShard? </h2><br>  Vous devez d'abord s√©lectionner un nombre constant de compartiments.  Pourquoi ne pas simplement le d√©finir sur int32_max?  Parce que les m√©tadonn√©es sont stock√©es avec chaque compartiment, 30 octets en stockage et 16 octets sur le routeur.  Plus vous disposez de compartiments, plus les m√©tadonn√©es prendront de la place.  Mais en m√™me temps, la taille du compartiment sera plus petite, ce qui signifie une granularit√© de cluster plus √©lev√©e et une vitesse de migration plus √©lev√©e par compartiment.  Vous devez donc choisir ce qui est le plus important pour vous et le niveau d'√©volutivit√© n√©cessaire. <br><br>  Deuxi√®mement, vous devez s√©lectionner une fonction de partition pour calculer l'ID du compartiment.  Les r√®gles sont les m√™mes que lors de la s√©lection d'une fonction de partition dans le partage traditionnel, car un compartiment ici est le m√™me que le nombre fixe de stockages dans le partage traditionnel.  La fonction doit r√©partir uniform√©ment les valeurs de sortie, sinon la croissance de la taille du compartiment ne sera pas √©quilibr√©e et VShard ne fonctionne qu'avec le nombre de compartiments.  Si vous n'√©quilibrez pas votre fonction de partition, vous devrez migrer les donn√©es d'un compartiment √† un autre et modifier la fonction de partition.  Ainsi, vous devez choisir avec soin. <br><br><h2>  R√©sum√© </h2><br>  VShard assure: <br><br><ul><li>  localit√© de donn√©es </li><li>  resharding atomique </li><li>  flexibilit√© de cluster plus √©lev√©e </li><li>  basculement en lecture automatique </li><li>  plusieurs contr√¥leurs de godet. </li></ul><br>  VShard est en d√©veloppement actif.  Certaines t√¢ches planifi√©es sont d√©j√† en cours d'ex√©cution.  La premi√®re t√¢che est <b>l'√©quilibrage de charge du routeur</b> .  En cas de demandes de lecture importantes, il n'est pas toujours recommand√© de les adresser au ma√Ætre.  Le routeur doit √©quilibrer les demandes de r√©plicas de lecture diff√©rents par lui-m√™me. <br><br>  La deuxi√®me t√¢che est <b>la migration du compartiment sans verrouillage</b> .  Un algorithme a d√©j√† √©t√© impl√©ment√© qui permet de garder les compartiments d√©bloqu√©s m√™me pendant la migration.  Le compartiment ne sera bloqu√© qu'√† la fin pour documenter la migration elle-m√™me. <br><br>  La troisi√®me t√¢che est <b>l'application atomique de la configuration</b> .  Il n'est pas commode ou atomique d'appliquer la configuration s√©par√©ment car une partie du stockage peut √™tre indisponible, et si la configuration n'est pas appliqu√©e, que faisons-nous ensuite?  C'est pourquoi nous travaillons sur un m√©canisme de transfert automatique de configuration. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr442782/">https://habr.com/ru/post/fr442782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr442770/index.html">Pr√©sentation des scanners de codes-barres JavaScript</a></li>
<li><a href="../fr442772/index.html">Math√©matiques pour Data Scientist: Sections n√©cessaires</a></li>
<li><a href="../fr442776/index.html">Index dans PostgreSQL - 3 (Hash)</a></li>
<li><a href="../fr442778/index.html">Learning Go: une s√©lection de reportages vid√©o</a></li>
<li><a href="../fr442780/index.html">Id√©es fausses les plus courantes en physique populaire</a></li>
<li><a href="../fr442784/index.html">D√©tournement BGP en ajoutant AS victime √† AS-SET de l'attaquant</a></li>
<li><a href="../fr442786/index.html">7 conseils utiles pour utiliser la pi√®ce</a></li>
<li><a href="../fr442788/index.html">Pourquoi avons-nous besoin d'un syst√®me de surveillance sur une puce</a></li>
<li><a href="../fr442790/index.html">L'inscription est ouverte pour Allure Server Meetup √† Saint-P√©tersbourg</a></li>
<li><a href="../fr442794/index.html">Nous vous invitons √† la conf√©rence ¬´Architecte (IT) dans les projets et organisations IT¬ª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>