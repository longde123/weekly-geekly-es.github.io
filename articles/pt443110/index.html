<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßìüèº üîª üë©üèø‚Äçüî¨ Configure o cluster de alta disponibilidade do Kubernetes no bare metal com GlusterFS e MetalLB. Parte 2/3 üë©üèΩ‚Äçüíª üïé üì•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1/3 aqui 
 Parte 3/3 aqui 


 Ol√° e bem vindo de volta! Esta √© a segunda parte do artigo sobre como configurar um cluster Kubernetes no bare met...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configure o cluster de alta disponibilidade do Kubernetes no bare metal com GlusterFS e MetalLB. Parte 2/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Parte 1/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>aqui</strong></a> <br>  <strong>Parte 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>aqui</strong></a> </p><br><p>  Ol√° e bem vindo de volta!  Esta √© a segunda parte do artigo sobre como configurar um cluster Kubernetes no bare metal.  Anteriormente, configuramos o cluster de HA do Kubernetes usando etcd externo, master-master e balanceamento de carga.  Bem, agora √© hora de configurar um ambiente e utilit√°rios adicionais para tornar o cluster mais √∫til e o mais pr√≥ximo poss√≠vel do estado de trabalho. </p><br><p>  Nesta parte do artigo, focaremos na configura√ß√£o do balanceador de carga interno dos servi√ßos de cluster - o MetalLB.  Tamb√©m instalaremos e configuramos o armazenamento de arquivos distribu√≠dos entre nossos n√≥s de trabalho.  Usaremos o GlusterFS para volumes persistentes dispon√≠veis no Kubernetes. <br>  Depois de concluir todas as etapas, nosso diagrama de cluster ficar√° assim: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Configure o MetalLB como um balanceador de carga interno. </h3><br><p>  Algumas palavras sobre o MetalLB, diretamente da p√°gina do documento: </p><br><blockquote> O MetalLB √© uma implementa√ß√£o do balanceador de carga para clusters bare metal do Kubernetes com protocolos de roteamento padr√£o. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">O Kubernetes</a> n√£o oferece a implementa√ß√£o de balanceadores de carga de rede ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tipo de servi√ßo LoadBalancer</a> ) para bare metal.  Todas as op√ß√µes de implementa√ß√£o de Network LB fornecidas com o Kubernetes s√£o middleware e acessam v√°rias plataformas de IaaS (GCP, AWS, Azure, etc.).  Se voc√™ n√£o estiver trabalhando em uma plataforma suportada por IaaS (GCP, AWS, Azure etc.), o LoadBalancer permanecer√° no estado "em espera" por um per√≠odo indeterminado ap√≥s a cria√ß√£o. <br><br>  Os operadores de servidor BM t√™m duas ferramentas menos eficazes para inserir o tr√°fego do usu√°rio em seus clusters, os servi√ßos NodePort e externalIPs.  Ambas as op√ß√µes apresentam falhas de produ√ß√£o significativas, o que transforma os clusters de BM em cidad√£os de segunda classe no ecossistema de Kubernetes. <br><br>  O MetalLB procura corrigir esse desequil√≠brio oferecendo uma implementa√ß√£o de Network LB que se integra ao equipamento de rede padr√£o, para que os servi√ßos externos nos clusters de BM tamb√©m ‚Äúfuncionem‚Äù na velocidade m√°xima. </blockquote><p>  Assim, usando essa ferramenta, lan√ßamos servi√ßos no cluster Kubernetes usando um balanceador de carga, pelo qual muito obrigado √† equipe do MetalLB.  O processo de instala√ß√£o √© realmente simples e direto. </p><br><p>  No in√≠cio do exemplo, selecionamos a sub-rede 192.168.0.0/24 para as necessidades do nosso cluster.  Agora pegue um pouco dessa sub-rede para o futuro balanceador de carga. </p><br><p>  Entramos no sistema da m√°quina com o utilit√°rio <strong>kubectl</strong> configurado e executamos: </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  Isso implantar√° o MetalLB no cluster, no <code>metallb-system</code> do <code>metallb-system</code> .  Verifique se todos os componentes do MetalLB est√£o funcionando corretamente: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Agora configure o MetalLB usando o configmap.  Neste exemplo, estamos usando a personaliza√ß√£o da Camada 2. Para obter informa√ß√µes sobre outras op√ß√µes de personaliza√ß√£o, consulte a documenta√ß√£o do MetalLB. </p><br><p>  Crie o <strong>arquivo metallb-config.yaml</strong> em qualquer diret√≥rio dentro do intervalo de IP selecionado da sub-rede do nosso cluster: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p>  E aplique esta configura√ß√£o: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  Verifique e modifique o configmap posteriormente, se necess√°rio: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  Agora, temos nosso pr√≥prio balanceador de carga local configurado.  Vamos ver como funciona, usando o servi√ßo Nginx como exemplo. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Em seguida, crie uma implanta√ß√£o de teste e um servi√ßo Nginx: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  E agora - verifique o resultado: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Criamos 3 pods Nginx, conforme indicado na implanta√ß√£o anteriormente.  O servi√ßo Nginx direcionar√° o tr√°fego para todos esses pods de acordo com o esquema de balanceamento c√≠clico.  E voc√™ tamb√©m pode ver o IP externo recebido do nosso balanceador de carga MetalLB. </p><br><p>  Agora tente acessar o endere√ßo IP 192.168.0.240 e voc√™ ver√° a p√°gina index.html do Nginx.  Lembre-se de remover a implanta√ß√£o de teste e o servi√ßo Nginx. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Bem, isso √© tudo com o MetalLB, vamos seguir em frente - vamos configurar os volumes GlusterFS para Kubernetes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Configurando o GlusterFS com Heketi nos n√≥s de trabalho. </h3><br><p>  De fato, o cluster Kubernetes n√£o pode ser usado sem volumes dentro dele.  Como voc√™ sabe, os lares s√£o ef√™meros, ou seja,  eles podem ser criados e exclu√≠dos a qualquer momento.  Todos os dados dentro deles ser√£o perdidos.  Portanto, em um cluster real, √© necess√°rio armazenamento distribu√≠do para garantir a troca de configura√ß√µes e dados entre n√≥s e aplicativos dentro dele. </p><br><p>  No Kubernetes, os volumes est√£o dispon√≠veis de v√°rias maneiras; escolha os que voc√™ deseja.  Neste exemplo, demonstrarei como criar o armazenamento GlusterFS para qualquer aplicativo interno, como volumes persistentes.  Anteriormente, usei a instala√ß√£o "sistema" do GlusterFS para todos os n√≥s de trabalho do Kubernetes para isso e simplesmente criei volumes hostPath nos diret√≥rios do GlusterFS. </p><br><p>  Agora temos uma nova ferramenta √∫til da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><strong>Heketi</strong></a> . </p><br><p>  Algumas palavras da documenta√ß√£o da Heketi: </p><br><blockquote>  Infra-estrutura de gerenciamento de volume RESTful para GlusterFS. <br><br>  A Heketi oferece uma interface de gerenciamento RESTful que pode ser usada para gerenciar o ciclo de vida dos volumes GlusterFS.  Gra√ßas √† Heketi, servi√ßos em nuvem como o OpenStack Manila, Kubernetes e OpenShift podem fornecer dinamicamente os volumes GlusterFS com qualquer tipo de confiabilidade suportada.  O Heketi determina automaticamente a localiza√ß√£o dos blocos em um cluster, fornecendo a localiza√ß√£o dos blocos e suas r√©plicas em diferentes √°reas de falha.  A Heketi tamb√©m suporta qualquer n√∫mero de clusters GlusterFS, permitindo que os servi√ßos em nuvem ofere√ßam armazenamento de arquivos on-line, n√£o apenas um √∫nico cluster GlusterFS. </blockquote><p>  Parece bom e, al√©m disso, essa ferramenta aproximar√° nosso cluster de VM dos grandes clusters de nuvem do Kubernetes.  No final, voc√™ poder√° criar <strong>PersistentVolumeClaims</strong> , que ser√£o gerados automaticamente e muito mais. </p><br><p>  Voc√™ pode usar discos r√≠gidos adicionais do sistema para configurar o GlusterFS ou apenas criar alguns dispositivos de bloqueio fict√≠cio.  Neste exemplo, usarei o segundo m√©todo. </p><br><p>  Crie dispositivos de bloco fict√≠cio nos tr√™s n√≥s de trabalho: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  Voc√™ receber√° um arquivo com aproximadamente 10 GB de tamanho.  Em seguida, use <strong>losetup</strong> - para adicion√°-lo a esses n√≥s, como um dispositivo de loopback: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Observe: se voc√™ j√° possui algum tipo de dispositivo de loopback 0, precisar√° escolher qualquer outro n√∫mero.</em> </blockquote><p>  Aproveitei o tempo e descobri por que a Heketi n√£o quer funcionar corretamente.  Portanto, para evitar problemas em configura√ß√µes futuras, primeiro verifique se carregamos o <strong>m√≥dulo do</strong> kernel <strong>dm_thin_pool</strong> e instalamos o pacote <strong>glusterfs-client</strong> em todos os n√≥s em funcionamento. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Bem, agora voc√™ precisa que o arquivo <strong>/ home / gluster / image</strong> e o dispositivo <strong>/ dev / loop0</strong> estejam presentes em todos os n√≥s em funcionamento.  Lembre-se de criar um servi√ßo systemd que iniciar√° automaticamente o <strong>losetup</strong> e o <strong>modprobe</strong> toda vez que esses servidores <strong>forem</strong> inicializados. </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  E ative: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  O trabalho preparat√≥rio est√° conclu√≠do e estamos prontos para implantar o GlusterFS e o Heketi em nosso cluster.  Para isso, usarei este <a href="">guia</a> legal.  A maioria dos comandos √© iniciada a partir de um computador de controle externo e comandos muito pequenos s√£o iniciados a partir de qualquer n√≥ principal dentro do cluster. </p><br><p>  Primeiro, copie o reposit√≥rio e crie DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Agora vamos marcar nossos tr√™s n√≥s de trabalho para o GlusterFS;  ap√≥s rotul√°-los, os pods do GlusterFS ser√£o criados: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Agora crie uma conta de servi√ßo Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  Fornecemos a essa conta de servi√ßo a capacidade de gerenciar pods de gluster.  Para fazer isso, crie uma fun√ß√£o de cluster necess√°ria para nossa conta de servi√ßo rec√©m-criada: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Agora vamos criar uma chave secreta do Kubernetes que bloqueia a configura√ß√£o da nossa inst√¢ncia Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Crie a primeira fonte em Heketi, que usamos nas primeiras opera√ß√µes de configura√ß√£o e, em seguida, exclua: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  Depois de criar e iniciar o servi√ßo Bootstrap Heketi, precisaremos mudar para um de nossos n√≥s principais, onde executaremos v√°rios comandos, j√° que nosso n√≥ de controle externo n√£o est√° dentro do cluster, portanto, n√£o podemos acessar os pods de trabalho e a rede interna do cluster. </p><br><p>  Primeiro, vamos baixar o utilit√°rio heketi-client e copi√°-lo para a pasta bin system: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Agora encontre o endere√ßo IP do pod heketi e exporte-o como uma vari√°vel do sistema: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Agora vamos fornecer √† Heketi informa√ß√µes sobre o cluster GlusterFS que ele deve gerenciar.  N√≥s o fornecemos atrav√©s de um arquivo de topologia.  Uma topologia √© um manifesto JSON com uma lista de todos os n√≥s, discos e clusters usados ‚Äã‚Äãpelo GlusterFS. </p><br><blockquote>  NOTA  Certifique-se de que <code>hostnames/manage</code> indiquem o nome exato, como na se√ß√£o <code>kubectl get node</code> , e que <code>hostnames/storage</code> sejam o endere√ßo IP dos n√≥s de armazenamento. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  Fa√ßa o download deste arquivo: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Em seguida, usamos o Heketi para fornecer volumes para armazenar o banco de dados.  O nome da equipe √© um pouco estranho, mas est√° tudo em ordem.  Crie tamb√©m um reposit√≥rio heketi: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  Esses s√£o todos os comandos que voc√™ precisa executar no n√≥ principal.  Vamos voltar ao n√≥ de controle e continuar a partir da√≠;  Antes de tudo, verifique se o √∫ltimo comando em execu√ß√£o foi executado com sucesso: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  E o trabalho de heketi-storage-copy-job est√° conclu√≠do. </p><br><blockquote>  Se atualmente n√£o houver nenhum pacote <strong>glusterfs-client</strong> instalado em seus n√≥s de trabalho, ocorrer√° um erro. </blockquote><p>  √â hora de remover o arquivo de instala√ß√£o do Heketi Bootstrap e fazer uma pequena limpeza: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  No √∫ltimo est√°gio, precisamos criar uma c√≥pia a longo prazo do Heketi: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  Se atualmente n√£o houver nenhum pacote glusterfs-client instalado em seus n√≥s de trabalho, ocorrer√° um erro.  E estamos quase terminando, agora o banco de dados Heketi √© armazenado no volume GlusterFS e n√£o √© redefinido toda vez que o cora√ß√£o da Heketi √© reiniciado. </p><br><p>  Para come√ßar a usar o cluster GlusterFS com aloca√ß√£o din√¢mica de recursos, precisamos criar uma StorageClass. </p><br><p>  Primeiro, vamos encontrar o ponto de extremidade de armazenamento Gluster, que ser√° passado para o StorageClass como um par√¢metro (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Agora crie alguns arquivos: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Use estes arquivos para criar classe e pvc: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  Tamb√©m podemos ver o volume fotovoltaico: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Agora temos um volume GlusterFS criado dinamicamente associado ao <strong>PersistentVolumeClaim</strong> , e podemos usar essa instru√ß√£o em qualquer subtrama. </p><br><p>  Crie um simples no Nginx e teste-o: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Navegue em (aguarde alguns minutos, pode ser necess√°rio fazer o download da imagem, se ela ainda n√£o existir): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Agora entre no cont√™iner e crie o arquivo index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  Voc√™ precisar√° encontrar o endere√ßo IP interno da lareira e se enrolar nele a partir de qualquer n√≥ principal: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  Ao fazer isso, simplesmente testamos nosso novo volume persistente. </p><br><blockquote>  Alguns comandos √∫teis para verificar o novo cluster GlusterFS s√£o: <code>heketi-cli cluster list</code> <code>heketi-cli volume list</code> .  Eles podem ser executados no seu computador se o <strong>heketi-cli estiver instalado</strong> .  Neste exemplo, este √© o n√≥ <strong>master1</strong> . </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  Nesse ponto, configuramos com √™xito um balanceador de carga interno com armazenamento de arquivos e nosso cluster agora est√° mais pr√≥ximo do estado operacional. </p><br><p>  Na pr√≥xima parte do artigo, focaremos na cria√ß√£o de um sistema de monitoramento de cluster e tamb√©m lan√ßaremos um projeto de teste para usar todos os recursos que configuramos. </p><br><p>  Fique em contato e tudo de bom! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt443110/">https://habr.com/ru/post/pt443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt443098/index.html">Os dados s√£o gravados no disco usando √≠m√£s e lasers</a></li>
<li><a href="../pt443100/index.html">Contando bugs na calculadora do Windows</a></li>
<li><a href="../pt443102/index.html">Mudan√ßa comportamental como produto: Por que Marie Kondo est√° levantando uma rodada de US $ 40 milh√µes com a Sequoia Capital?</a></li>
<li><a href="../pt443104/index.html">Calcular express√µes simb√≥licas com n√∫meros triangulares difusos em python</a></li>
<li><a href="../pt443106/index.html">Anunciado USB4: o que se sabe sobre o padr√£o</a></li>
<li><a href="../pt443112/index.html">Tem certeza de que pode confiar na sua VPN?</a></li>
<li><a href="../pt443114/index.html">Pr√™mio DevProject: Meu discurso na DeveloperWeek 2019</a></li>
<li><a href="../pt443120/index.html">A Duma do Estado continuar√° a luta contra a venda ilegal de cart√µes SIM</a></li>
<li><a href="../pt443122/index.html">Vazamento de 809 milh√µes de endere√ßos de e-mail do servi√ßo Verifications.io devido √† abertura p√∫blica do MongoDB</a></li>
<li><a href="../pt443124/index.html">React.lazy? Mas e se voc√™ n√£o tiver um componente?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>