<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïñ ü¶é ‚ñ™Ô∏è Die Sicherheit von Algorithmen f√ºr maschinelles Lernen. Sch√ºtzen und Testen von Modellen mit Python üëåüèΩ üßúüèæ üë©üèæ‚Äçüîß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im vorherigen Artikel haben wir √ºber ein solches Problem des maschinellen Lernens wie gegnerische Beispiele und einige Arten von Angriffen gesprochen,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Sicherheit von Algorithmen f√ºr maschinelles Lernen. Sch√ºtzen und Testen von Modellen mit Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dsec/blog/438644/"><p><img src="https://habrastorage.org/webt/wo/o_/u2/woo_u2i8ll_fqrqvt3o-typrlue.jpeg" alt="Bild"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Im vorherigen Artikel haben</a> wir √ºber ein solches Problem des maschinellen Lernens wie gegnerische Beispiele und einige Arten von Angriffen gesprochen, mit denen sie generiert werden k√∂nnen.  Dieser Artikel konzentriert sich auf Schutzalgorithmen vor dieser Art von Effekten und Empfehlungen zum Testen von Modellen. </p><a name="habracut"></a><br><h2 id="zaschita">  Schutz </h2><br><p>  Lassen Sie uns zun√§chst einen Punkt erkl√§ren: Es ist unm√∂glich, sich vollst√§ndig gegen einen solchen Effekt zu verteidigen, und das ist ganz nat√ºrlich.  Wenn wir das Problem der gegnerischen Beispiele vollst√§ndig l√∂sen w√ºrden, w√ºrden wir gleichzeitig das Problem der Konstruktion einer idealen Hyperebene l√∂sen, was nat√ºrlich ohne einen allgemeinen Datensatz nicht m√∂glich ist. </p><br><p>  Die Verteidigung eines maschinellen Lernmodells besteht aus zwei Phasen: </p><br><p>  <strong>Lernen</strong> - Wir bringen unserem Algorithmus bei, korrekt auf gegnerische Beispiele zu reagieren. </p><br><p>  <strong>Operation</strong> - Wir versuchen, w√§hrend der Operationsphase des Modells ein kontr√§res Beispiel zu erkennen. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erw√§hnenswert</a> ist, dass Sie mit den in diesem Artikel vorgestellten Schutzmethoden mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adversarial Robustness Toolbox</a> von IBM arbeiten k√∂nnen. </p><br><h3 id="adversarial-training">  Widerspr√ºchliches Training </h3><br><p><img src="https://habrastorage.org/webt/4w/t_/lm/4wt_lmm-cbcdye9rabryki0jj70.png" alt="Bild"></p><br><p> Wenn Sie eine Person, die sich gerade mit dem Problem des Gegners vertraut gemacht hat, anhand von Beispielen die Frage stellen: ‚ÄûWie k√∂nnen Sie sich vor diesem Effekt sch√ºtzen?‚Äú, Sagen 9 von 10 Personen auf jeden Fall: ‚ÄûF√ºgen wir die generierten Objekte zum Trainingssatz hinzu.‚Äú  Dieser Ansatz wurde bereits 2013 in dem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faszinierende Eigenschaften neuronaler Netze</a> vorgeschlagen.  In diesem Artikel wurde dieses Problem zuerst beschrieben und der L-BFGS-Angriff, der das Empfangen von gegnerischen Beispielen erm√∂glicht. </p><br><p>  Diese Methode ist sehr einfach.  Wir generieren gegnerische Beispiele unter Verwendung verschiedener Arten von Angriffen und f√ºgen sie bei jeder Iteration dem Trainingssatz hinzu, wodurch der ‚ÄûWiderstand‚Äú des gegnerischen Modells gegen√ºber den Beispielen erh√∂ht wird. </p><br><p>  Der Nachteil dieser Methode liegt auf der Hand: Bei jeder Trainingsiteration k√∂nnen wir f√ºr jedes Beispiel eine sehr gro√üe Anzahl von Beispielen generieren, und die Zeit zum Modellieren des Trainings nimmt um ein Vielfaches zu. </p><br><p>  Sie k√∂nnen diese Methode mithilfe der ART-IBM-Bibliothek wie folgt anwenden. </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.adversarial_trainer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AdversarialTrainer trainer = AdversarialTrainer(model, attacks) trainer.fit(x_train, y_train)</code> </pre> <br><h3 id="gaussian-data-augmentation">  Gau√üsche Datenerweiterung </h3><br><p><img src="https://habrastorage.org/webt/jf/9d/ko/jf9dkoia9fom1rtqkvgwe-7aulo.png" alt="Bild"></p><br><p>  Die folgende Methode, die im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Effiziente Verteidigung gegen gegnerische Angriffe</a> beschrieben wird, verwendet eine √§hnliche Logik: Sie schl√§gt auch vor, dem Trainingssatz zus√§tzliche Objekte hinzuzuf√ºgen. Im Gegensatz zum gegnerischen Training sind diese Objekte jedoch keine gegnerischen Beispiele, sondern leicht verrauschte Trainingssatzobjekte (Gau√ü wird als Rauschen verwendet Rauschen, daher der Name der Methode).  Dies scheint in der Tat sehr logisch zu sein, da das Hauptproblem der Modelle genau ihre schlechte St√∂rfestigkeit ist. </p><br><p>  Diese Methode zeigt √§hnliche Ergebnisse wie das gegnerische Training, w√§hrend viel weniger Zeit f√ºr das Generieren von Objekten f√ºr das Training aufgewendet wird. </p><br><p>  Sie k√∂nnen diese Methode mit der GaussianAugmentation-Klasse in ART-IBM anwenden </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.gaussian_augmentation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianAugmentation GDA = GaussianAugmentation() new_x = GDA(x_train)</code> </pre> <br><h3 id="label-smoothing">  Etikettengl√§ttung </h3><br><p>  Die Label Smoothing-Methode ist sehr einfach zu implementieren, hat jedoch eine gro√üe Wahrscheinlichkeitsbedeutung.  Wir werden nicht n√§her auf die probabilistische Interpretation dieser Methode eingehen. Sie finden sie im Originalartikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úberdenken der Inception-Architektur f√ºr Computer Vision</a> .  Kurz gesagt, Label Smoothing ist eine zus√§tzliche Art der Regularisierung des Modells im Klassifizierungsproblem, wodurch es widerstandsf√§higer gegen Rauschen wird. </p><br><p>  Tats√§chlich gl√§ttet diese Methode Klassenbeschriftungen.  Machen Sie sie sagen, nicht 1, sondern 0,9.  Trainingsmodelle werden daher f√ºr ein viel gr√∂√üeres "Vertrauen" in das Etikett f√ºr ein bestimmtes Objekt bestraft. </p><br><p>  Die Anwendung dieser Methode in Python ist unten zu sehen. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.label_smoothing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelSmoothing LS = LabelSmoothing() new_x, new_y = LS(train_x, train_y)</code> </pre> <br><h3 id="bounded-relu">  Begrenzte relu </h3><br><p><img src="https://habrastorage.org/webt/dw/mw/sz/dwmwszowk1t9l6byacxcscvmvh4.png" alt="Bild"></p><br><p>  Wenn wir √ºber Angriffe sprachen, konnten viele feststellen, dass einige Angriffe (JSMA, OnePixel) davon abh√§ngen, wie stark der Gradient an der einen oder anderen Stelle im Eingabebild ist.  Die einfache und "billige" Methode (in Bezug auf Rechenaufwand und Zeitaufwand) von Bounded ReLU versucht, dieses Problem zu l√∂sen. </p><br><p>  Das Wesentliche der Methode ist wie folgt.  Ersetzen wir die Aktivierungsfunktion von ReLU in einem neuronalen Netzwerk durch dieselbe, die nicht nur von unten, sondern auch von oben begrenzt ist, wodurch Gradientenkarten gegl√§ttet werden. An bestimmten Stellen ist es nicht m√∂glich, einen Splash zu erhalten, der es Ihnen nicht erm√∂glicht, den Algorithmus durch √Ñndern eines Pixels des Bildes zu t√§uschen. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"></script></p><br>  \ begin {Gleichung *} f (x) = <br>  \ begin {F√§lle} <br>  0, x &lt;0 <br>  \\ <br>  x, 0 \ leq x \ leq t <br>  \\ <br>  t, x&gt; t <br>  \ end {F√§lle} <br>  \ end {Gleichung *} <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"></script></p><br><p>  Diese Methode wurde auch im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Effiziente Abwehr gegen gegnerische Angriffe beschrieben</a> </p><br><h3 id="postroenie-ansambley-modeley">  Erstellen von Modellensembles </h3><br><p><img src="https://habrastorage.org/webt/cq/2i/pg/cq2ipgeru_vavdrnk-usoydtlxw.png" alt="Bild"><br>  Es ist nicht schwer, ein trainiertes Modell zu t√§uschen.  Noch schwieriger ist es, zwei Modelle gleichzeitig mit einem Objekt zu t√§uschen.  Und wenn es N solche Modelle gibt?  Darauf basiert die Ensemble-Methode der Modelle.  Wir bauen einfach N verschiedene Modelle und fassen ihre Ausgabe zu einer einzigen Antwort zusammen.  Wenn die Modelle auch durch unterschiedliche Algorithmen dargestellt werden, ist es √§u√üerst schwierig, ein solches System zu t√§uschen, aber es ist √§u√üerst schwierig! </p><br><p>  Es ist ganz nat√ºrlich, dass die Implementierung von Modellensembles ein rein architektonischer Ansatz ist, der viele Fragen aufwirft (Welche Grundmodelle sind zu verwenden? Wie werden die Ergebnisse von Grundmodellen aggregiert? Gibt es eine Beziehung zwischen Modellen? Und so weiter).  Aus diesem Grund ist dieser Ansatz in ART-IBM nicht implementiert </p><br><h3 id="feature-squeezing">  Feature quetschen </h3><br><p><img src="https://habrastorage.org/webt/sn/wy/wp/snwywpuqqae7pun4njrlmluseeg.png" alt="Bild"><br>  Diese Methode, die unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feature Squeezing: Erkennen widerspr√ºchlicher Beispiele in tiefen neuronalen Netzen</a> beschrieben wird, funktioniert w√§hrend der Betriebsphase des Modells.  Sie k√∂nnen damit kontr√§re Beispiele erkennen. </p><br><p>  Die Idee hinter dieser Methode ist die folgende: Wenn Sie n Modelle mit denselben Daten, aber mit unterschiedlichen Komprimierungsverh√§ltnissen trainieren, sind die Ergebnisse ihrer Arbeit immer noch √§hnlich.  Gleichzeitig wird das Adversarial-Beispiel, das im Quellnetzwerk funktioniert, h√∂chstwahrscheinlich in zus√§tzlichen Netzwerken fehlschlagen.  Nachdem wir den paarweisen Unterschied zwischen den Ausg√§ngen des anf√§nglichen neuronalen Netzwerks und den zus√§tzlichen Ausg√§ngen ber√ºcksichtigt, das Maximum daraus ausgew√§hlt und mit einem vorgew√§hlten Schwellenwert verglichen haben, k√∂nnen wir feststellen, dass das Eingabeobjekt entweder kontrovers oder absolut g√ºltig ist. </p><br><p>  Das Folgende ist eine Methode zum Abrufen komprimierter Objekte mit ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.feature_squeezing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FeatureSqueezing FS = FeatureSqueezing() new_x = FS(train_x)</code> </pre> <br><p>  Wir werden mit Schutzmethoden enden.  Aber es w√§re falsch, einen wichtigen Punkt nicht zu erfassen.  Wenn ein Angreifer keinen Zugriff auf die Modelleingabe und -ausgabe hat, versteht er vor der Eingabe des Modells nicht, wie die Rohdaten in Ihrem System verarbeitet werden.  Dann und nur dann werden alle seine Angriffe darauf reduziert, die Eingabewerte zuf√§llig zu sortieren, was nat√ºrlich unwahrscheinlich ist, dass das gew√ºnschte Ergebnis erzielt wird. </p><br><h2 id="testirovanie">  Testen </h2><br><p>  Lassen Sie uns nun √ºber das Testen von Algorithmen sprechen, um den gegnerischen Beispielen entgegenzuwirken.  Hier ist zun√§chst zu verstehen, wie wir unser Modell testen werden.  Wenn wir davon ausgehen, dass ein Angreifer in irgendeiner Weise vollen Zugriff auf das gesamte Modell erhalten kann, muss unser Modell mithilfe von WhiteBox-Angriffsmethoden getestet werden. <br><img src="https://habrastorage.org/webt/vj/pm/-w/vjpm-wuwle8c5sngov5ksw5iahq.png" alt="Bild"></p><br><p>  In einem anderen Fall gehen wir davon aus, dass ein Angreifer niemals Zugriff auf die "Innenseiten" unseres Modells erh√§lt. Er kann jedoch, wenn auch indirekt, die Eingabedaten beeinflussen und das Ergebnis des Modells sehen.  Dann sollten Sie die Methoden von BlackBox-Angriffen anwenden. <br><img src="https://habrastorage.org/webt/xc/h9/wo/xch9wo0pweqhlf33pzgrgdiihqm.png" alt="Bild"></p><br><p>  Der allgemeine Testalgorithmus kann anhand des folgenden Beispiels beschrieben werden: </p><br><p><img src="https://habrastorage.org/webt/1d/p_/lx/1dp_lxdocm0zkd2ssmbg_fmnvba.jpeg" alt="Bild"></p><br><p>  Es soll ein trainiertes neuronales Netzwerk in TensorFlow (TF NN) geschrieben sein.  Wir behaupten fachm√§nnisch, dass unser Netzwerk in die H√§nde eines Angreifers fallen kann, indem es in das System eindringt, in dem sich das Modell befindet.  In diesem Fall m√ºssen wir WhiteBox-Angriffe ausf√ºhren.  Zu diesem Zweck definieren wir einen Angriffspool und Frameworks (FoolBox - FB, CleverHans - CH, Toolbox f√ºr kontr√§re Robustheit - ART), mit denen diese Angriffe implementiert werden k√∂nnen.  Wenn wir z√§hlen, wie viele Angriffe erfolgreich waren, berechnen wir die Succes Rate (SR).  Wenn SR zu uns passt, beenden wir den Test, andernfalls verwenden wir eine der Schutzmethoden, die beispielsweise in ART-IBM implementiert sind.  Andererseits f√ºhren wir Angriffe durch und betrachten SR.  Wir machen diesen Vorgang zyklisch, bis SR zu uns passt. </p><br><h2 id="vyvody">  Schlussfolgerungen </h2><br><p>  Ich m√∂chte hier mit allgemeinen Informationen zu Angriffen, Abwehrma√ünahmen und zum Testen von Modellen f√ºr maschinelles Lernen enden.  Zusammenfassend k√∂nnen wir die beiden Artikel zusammenfassen: </p><br><ol><li>  Glauben Sie nicht an maschinelles Lernen als eine Art Wunder, das all Ihre Probleme l√∂sen kann. </li><li>  Denken Sie beim Anwenden von Algorithmen f√ºr maschinelles Lernen in Ihren Aufgaben daran, wie widerstandsf√§hig dieser Algorithmus gegen Bedrohungen wie Beispiele f√ºr Gegner ist. </li><li>  Sie k√∂nnen den Algorithmus sowohl von der Seite des maschinellen Lernens als auch von der Seite des Systems, in dem dieses Modell betrieben wird, sch√ºtzen. </li><li>  Testen Sie Ihre Modelle, insbesondere in F√§llen, in denen das Ergebnis des Modells die Entscheidung direkt beeinflusst </li><li>  Bibliotheken wie FoolBox, CleverHans und ART-IBM bieten eine praktische Schnittstelle zum Angreifen und Verteidigen von Modellen f√ºr maschinelles Lernen. </li></ol><br><p>  Auch in diesem Artikel m√∂chte ich die Arbeit mit den Bibliotheken FoolBox, CleverHans und ART-IBM zusammenfassen: </p><br><p>  FoolBox ist eine einfache und verst√§ndliche Bibliothek zum Angriff auf neuronale Netze, die viele verschiedene Frameworks unterst√ºtzt. </p><br><p>  CleverHans ist eine Bibliothek, mit der Sie Angriffe ausf√ºhren k√∂nnen, indem Sie viele Parameter des Angriffs √§ndern. Diese ist etwas komplizierter als FoolBox und unterst√ºtzt weniger Frameworks. </p><br><p>  ART-IBM ist die einzige Bibliothek der oben genannten Art, mit der Sie mit Sicherheitsmethoden arbeiten k√∂nnen. Bisher werden nur TensorFlow und Keras unterst√ºtzt, sie entwickeln sich jedoch schneller als andere. </p><br><p>  An dieser Stelle gibt es eine weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bibliothek</a> f√ºr die Arbeit mit gegnerischen Beispielen aus Baidu, die jedoch leider nur f√ºr Personen geeignet ist, die Chinesisch sprechen. </p><br><p>  Im n√§chsten Artikel zu diesem Thema werden wir einen Teil der Aufgabe analysieren, die w√§hrend des ZeroNights HackQuest 2018 gel√∂st werden soll, indem ein typisches neuronales Netzwerk mithilfe der FoolBox-Bibliothek get√§uscht wird. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de438644/">https://habr.com/ru/post/de438644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de438634/index.html">In B√ºros ist es entweder zu hei√ü oder zu kalt: Gibt es eine bessere M√∂glichkeit, die Temperatur anzupassen?</a></li>
<li><a href="../de438636/index.html">Fehlerhafte Einbettung von Funktionen in Go</a></li>
<li><a href="../de438638/index.html">Wir analysieren das Protokoll der Pager-Nachrichten POCSAG, Teil 2</a></li>
<li><a href="../de438640/index.html">Offene elektronische Hochgeschwindigkeitsw√§hrung</a></li>
<li><a href="../de438642/index.html">Die Grundlagen der reaktiven Programmierung mit RxJS</a></li>
<li><a href="../de438646/index.html">Informationen zum Erstellen von Budget-Stereobildern auf Fingern (Stereogramm, Anaglyphe, Stereoskop)</a></li>
<li><a href="../de438648/index.html">Vergleich von BI-Systemen (Tableau, Power BI, Oracle, Qlik)</a></li>
<li><a href="../de438650/index.html">Rakete 9M729. Ein paar Worte zum ‚ÄûVersto√ü‚Äú gegen den INF-Vertrag</a></li>
<li><a href="../de438652/index.html">Portabelization IDA</a></li>
<li><a href="../de438654/index.html">OpenSceneGraph: Integration mit dem Qt Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>