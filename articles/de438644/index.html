<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕖 🦎 ▪️ Die Sicherheit von Algorithmen für maschinelles Lernen. Schützen und Testen von Modellen mit Python 👌🏽 🧜🏾 👩🏾‍🔧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im vorherigen Artikel haben wir über ein solches Problem des maschinellen Lernens wie gegnerische Beispiele und einige Arten von Angriffen gesprochen,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Sicherheit von Algorithmen für maschinelles Lernen. Schützen und Testen von Modellen mit Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dsec/blog/438644/"><p><img src="https://habrastorage.org/webt/wo/o_/u2/woo_u2i8ll_fqrqvt3o-typrlue.jpeg" alt="Bild"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Im vorherigen Artikel haben</a> wir über ein solches Problem des maschinellen Lernens wie gegnerische Beispiele und einige Arten von Angriffen gesprochen, mit denen sie generiert werden können.  Dieser Artikel konzentriert sich auf Schutzalgorithmen vor dieser Art von Effekten und Empfehlungen zum Testen von Modellen. </p><a name="habracut"></a><br><h2 id="zaschita">  Schutz </h2><br><p>  Lassen Sie uns zunächst einen Punkt erklären: Es ist unmöglich, sich vollständig gegen einen solchen Effekt zu verteidigen, und das ist ganz natürlich.  Wenn wir das Problem der gegnerischen Beispiele vollständig lösen würden, würden wir gleichzeitig das Problem der Konstruktion einer idealen Hyperebene lösen, was natürlich ohne einen allgemeinen Datensatz nicht möglich ist. </p><br><p>  Die Verteidigung eines maschinellen Lernmodells besteht aus zwei Phasen: </p><br><p>  <strong>Lernen</strong> - Wir bringen unserem Algorithmus bei, korrekt auf gegnerische Beispiele zu reagieren. </p><br><p>  <strong>Operation</strong> - Wir versuchen, während der Operationsphase des Modells ein konträres Beispiel zu erkennen. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erwähnenswert</a> ist, dass Sie mit den in diesem Artikel vorgestellten Schutzmethoden mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Adversarial Robustness Toolbox</a> von IBM arbeiten können. </p><br><h3 id="adversarial-training">  Widersprüchliches Training </h3><br><p><img src="https://habrastorage.org/webt/4w/t_/lm/4wt_lmm-cbcdye9rabryki0jj70.png" alt="Bild"></p><br><p> Wenn Sie eine Person, die sich gerade mit dem Problem des Gegners vertraut gemacht hat, anhand von Beispielen die Frage stellen: „Wie können Sie sich vor diesem Effekt schützen?“, Sagen 9 von 10 Personen auf jeden Fall: „Fügen wir die generierten Objekte zum Trainingssatz hinzu.“  Dieser Ansatz wurde bereits 2013 in dem Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faszinierende Eigenschaften neuronaler Netze</a> vorgeschlagen.  In diesem Artikel wurde dieses Problem zuerst beschrieben und der L-BFGS-Angriff, der das Empfangen von gegnerischen Beispielen ermöglicht. </p><br><p>  Diese Methode ist sehr einfach.  Wir generieren gegnerische Beispiele unter Verwendung verschiedener Arten von Angriffen und fügen sie bei jeder Iteration dem Trainingssatz hinzu, wodurch der „Widerstand“ des gegnerischen Modells gegenüber den Beispielen erhöht wird. </p><br><p>  Der Nachteil dieser Methode liegt auf der Hand: Bei jeder Trainingsiteration können wir für jedes Beispiel eine sehr große Anzahl von Beispielen generieren, und die Zeit zum Modellieren des Trainings nimmt um ein Vielfaches zu. </p><br><p>  Sie können diese Methode mithilfe der ART-IBM-Bibliothek wie folgt anwenden. </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.adversarial_trainer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AdversarialTrainer trainer = AdversarialTrainer(model, attacks) trainer.fit(x_train, y_train)</code> </pre> <br><h3 id="gaussian-data-augmentation">  Gaußsche Datenerweiterung </h3><br><p><img src="https://habrastorage.org/webt/jf/9d/ko/jf9dkoia9fom1rtqkvgwe-7aulo.png" alt="Bild"></p><br><p>  Die folgende Methode, die im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Effiziente Verteidigung gegen gegnerische Angriffe</a> beschrieben wird, verwendet eine ähnliche Logik: Sie schlägt auch vor, dem Trainingssatz zusätzliche Objekte hinzuzufügen. Im Gegensatz zum gegnerischen Training sind diese Objekte jedoch keine gegnerischen Beispiele, sondern leicht verrauschte Trainingssatzobjekte (Gauß wird als Rauschen verwendet Rauschen, daher der Name der Methode).  Dies scheint in der Tat sehr logisch zu sein, da das Hauptproblem der Modelle genau ihre schlechte Störfestigkeit ist. </p><br><p>  Diese Methode zeigt ähnliche Ergebnisse wie das gegnerische Training, während viel weniger Zeit für das Generieren von Objekten für das Training aufgewendet wird. </p><br><p>  Sie können diese Methode mit der GaussianAugmentation-Klasse in ART-IBM anwenden </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.gaussian_augmentation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianAugmentation GDA = GaussianAugmentation() new_x = GDA(x_train)</code> </pre> <br><h3 id="label-smoothing">  Etikettenglättung </h3><br><p>  Die Label Smoothing-Methode ist sehr einfach zu implementieren, hat jedoch eine große Wahrscheinlichkeitsbedeutung.  Wir werden nicht näher auf die probabilistische Interpretation dieser Methode eingehen. Sie finden sie im Originalartikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Überdenken der Inception-Architektur für Computer Vision</a> .  Kurz gesagt, Label Smoothing ist eine zusätzliche Art der Regularisierung des Modells im Klassifizierungsproblem, wodurch es widerstandsfähiger gegen Rauschen wird. </p><br><p>  Tatsächlich glättet diese Methode Klassenbeschriftungen.  Machen Sie sie sagen, nicht 1, sondern 0,9.  Trainingsmodelle werden daher für ein viel größeres "Vertrauen" in das Etikett für ein bestimmtes Objekt bestraft. </p><br><p>  Die Anwendung dieser Methode in Python ist unten zu sehen. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.label_smoothing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelSmoothing LS = LabelSmoothing() new_x, new_y = LS(train_x, train_y)</code> </pre> <br><h3 id="bounded-relu">  Begrenzte relu </h3><br><p><img src="https://habrastorage.org/webt/dw/mw/sz/dwmwszowk1t9l6byacxcscvmvh4.png" alt="Bild"></p><br><p>  Wenn wir über Angriffe sprachen, konnten viele feststellen, dass einige Angriffe (JSMA, OnePixel) davon abhängen, wie stark der Gradient an der einen oder anderen Stelle im Eingabebild ist.  Die einfache und "billige" Methode (in Bezug auf Rechenaufwand und Zeitaufwand) von Bounded ReLU versucht, dieses Problem zu lösen. </p><br><p>  Das Wesentliche der Methode ist wie folgt.  Ersetzen wir die Aktivierungsfunktion von ReLU in einem neuronalen Netzwerk durch dieselbe, die nicht nur von unten, sondern auch von oben begrenzt ist, wodurch Gradientenkarten geglättet werden. An bestimmten Stellen ist es nicht möglich, einen Splash zu erhalten, der es Ihnen nicht ermöglicht, den Algorithmus durch Ändern eines Pixels des Bildes zu täuschen. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"></script></p><br>  \ begin {Gleichung *} f (x) = <br>  \ begin {Fälle} <br>  0, x &lt;0 <br>  \\ <br>  x, 0 \ leq x \ leq t <br>  \\ <br>  t, x&gt; t <br>  \ end {Fälle} <br>  \ end {Gleichung *} <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"></script></p><br><p>  Diese Methode wurde auch im Artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Effiziente Abwehr gegen gegnerische Angriffe beschrieben</a> </p><br><h3 id="postroenie-ansambley-modeley">  Erstellen von Modellensembles </h3><br><p><img src="https://habrastorage.org/webt/cq/2i/pg/cq2ipgeru_vavdrnk-usoydtlxw.png" alt="Bild"><br>  Es ist nicht schwer, ein trainiertes Modell zu täuschen.  Noch schwieriger ist es, zwei Modelle gleichzeitig mit einem Objekt zu täuschen.  Und wenn es N solche Modelle gibt?  Darauf basiert die Ensemble-Methode der Modelle.  Wir bauen einfach N verschiedene Modelle und fassen ihre Ausgabe zu einer einzigen Antwort zusammen.  Wenn die Modelle auch durch unterschiedliche Algorithmen dargestellt werden, ist es äußerst schwierig, ein solches System zu täuschen, aber es ist äußerst schwierig! </p><br><p>  Es ist ganz natürlich, dass die Implementierung von Modellensembles ein rein architektonischer Ansatz ist, der viele Fragen aufwirft (Welche Grundmodelle sind zu verwenden? Wie werden die Ergebnisse von Grundmodellen aggregiert? Gibt es eine Beziehung zwischen Modellen? Und so weiter).  Aus diesem Grund ist dieser Ansatz in ART-IBM nicht implementiert </p><br><h3 id="feature-squeezing">  Feature quetschen </h3><br><p><img src="https://habrastorage.org/webt/sn/wy/wp/snwywpuqqae7pun4njrlmluseeg.png" alt="Bild"><br>  Diese Methode, die unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Feature Squeezing: Erkennen widersprüchlicher Beispiele in tiefen neuronalen Netzen</a> beschrieben wird, funktioniert während der Betriebsphase des Modells.  Sie können damit konträre Beispiele erkennen. </p><br><p>  Die Idee hinter dieser Methode ist die folgende: Wenn Sie n Modelle mit denselben Daten, aber mit unterschiedlichen Komprimierungsverhältnissen trainieren, sind die Ergebnisse ihrer Arbeit immer noch ähnlich.  Gleichzeitig wird das Adversarial-Beispiel, das im Quellnetzwerk funktioniert, höchstwahrscheinlich in zusätzlichen Netzwerken fehlschlagen.  Nachdem wir den paarweisen Unterschied zwischen den Ausgängen des anfänglichen neuronalen Netzwerks und den zusätzlichen Ausgängen berücksichtigt, das Maximum daraus ausgewählt und mit einem vorgewählten Schwellenwert verglichen haben, können wir feststellen, dass das Eingabeobjekt entweder kontrovers oder absolut gültig ist. </p><br><p>  Das Folgende ist eine Methode zum Abrufen komprimierter Objekte mit ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.feature_squeezing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FeatureSqueezing FS = FeatureSqueezing() new_x = FS(train_x)</code> </pre> <br><p>  Wir werden mit Schutzmethoden enden.  Aber es wäre falsch, einen wichtigen Punkt nicht zu erfassen.  Wenn ein Angreifer keinen Zugriff auf die Modelleingabe und -ausgabe hat, versteht er vor der Eingabe des Modells nicht, wie die Rohdaten in Ihrem System verarbeitet werden.  Dann und nur dann werden alle seine Angriffe darauf reduziert, die Eingabewerte zufällig zu sortieren, was natürlich unwahrscheinlich ist, dass das gewünschte Ergebnis erzielt wird. </p><br><h2 id="testirovanie">  Testen </h2><br><p>  Lassen Sie uns nun über das Testen von Algorithmen sprechen, um den gegnerischen Beispielen entgegenzuwirken.  Hier ist zunächst zu verstehen, wie wir unser Modell testen werden.  Wenn wir davon ausgehen, dass ein Angreifer in irgendeiner Weise vollen Zugriff auf das gesamte Modell erhalten kann, muss unser Modell mithilfe von WhiteBox-Angriffsmethoden getestet werden. <br><img src="https://habrastorage.org/webt/vj/pm/-w/vjpm-wuwle8c5sngov5ksw5iahq.png" alt="Bild"></p><br><p>  In einem anderen Fall gehen wir davon aus, dass ein Angreifer niemals Zugriff auf die "Innenseiten" unseres Modells erhält. Er kann jedoch, wenn auch indirekt, die Eingabedaten beeinflussen und das Ergebnis des Modells sehen.  Dann sollten Sie die Methoden von BlackBox-Angriffen anwenden. <br><img src="https://habrastorage.org/webt/xc/h9/wo/xch9wo0pweqhlf33pzgrgdiihqm.png" alt="Bild"></p><br><p>  Der allgemeine Testalgorithmus kann anhand des folgenden Beispiels beschrieben werden: </p><br><p><img src="https://habrastorage.org/webt/1d/p_/lx/1dp_lxdocm0zkd2ssmbg_fmnvba.jpeg" alt="Bild"></p><br><p>  Es soll ein trainiertes neuronales Netzwerk in TensorFlow (TF NN) geschrieben sein.  Wir behaupten fachmännisch, dass unser Netzwerk in die Hände eines Angreifers fallen kann, indem es in das System eindringt, in dem sich das Modell befindet.  In diesem Fall müssen wir WhiteBox-Angriffe ausführen.  Zu diesem Zweck definieren wir einen Angriffspool und Frameworks (FoolBox - FB, CleverHans - CH, Toolbox für konträre Robustheit - ART), mit denen diese Angriffe implementiert werden können.  Wenn wir zählen, wie viele Angriffe erfolgreich waren, berechnen wir die Succes Rate (SR).  Wenn SR zu uns passt, beenden wir den Test, andernfalls verwenden wir eine der Schutzmethoden, die beispielsweise in ART-IBM implementiert sind.  Andererseits führen wir Angriffe durch und betrachten SR.  Wir machen diesen Vorgang zyklisch, bis SR zu uns passt. </p><br><h2 id="vyvody">  Schlussfolgerungen </h2><br><p>  Ich möchte hier mit allgemeinen Informationen zu Angriffen, Abwehrmaßnahmen und zum Testen von Modellen für maschinelles Lernen enden.  Zusammenfassend können wir die beiden Artikel zusammenfassen: </p><br><ol><li>  Glauben Sie nicht an maschinelles Lernen als eine Art Wunder, das all Ihre Probleme lösen kann. </li><li>  Denken Sie beim Anwenden von Algorithmen für maschinelles Lernen in Ihren Aufgaben daran, wie widerstandsfähig dieser Algorithmus gegen Bedrohungen wie Beispiele für Gegner ist. </li><li>  Sie können den Algorithmus sowohl von der Seite des maschinellen Lernens als auch von der Seite des Systems, in dem dieses Modell betrieben wird, schützen. </li><li>  Testen Sie Ihre Modelle, insbesondere in Fällen, in denen das Ergebnis des Modells die Entscheidung direkt beeinflusst </li><li>  Bibliotheken wie FoolBox, CleverHans und ART-IBM bieten eine praktische Schnittstelle zum Angreifen und Verteidigen von Modellen für maschinelles Lernen. </li></ol><br><p>  Auch in diesem Artikel möchte ich die Arbeit mit den Bibliotheken FoolBox, CleverHans und ART-IBM zusammenfassen: </p><br><p>  FoolBox ist eine einfache und verständliche Bibliothek zum Angriff auf neuronale Netze, die viele verschiedene Frameworks unterstützt. </p><br><p>  CleverHans ist eine Bibliothek, mit der Sie Angriffe ausführen können, indem Sie viele Parameter des Angriffs ändern. Diese ist etwas komplizierter als FoolBox und unterstützt weniger Frameworks. </p><br><p>  ART-IBM ist die einzige Bibliothek der oben genannten Art, mit der Sie mit Sicherheitsmethoden arbeiten können. Bisher werden nur TensorFlow und Keras unterstützt, sie entwickeln sich jedoch schneller als andere. </p><br><p>  An dieser Stelle gibt es eine weitere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bibliothek</a> für die Arbeit mit gegnerischen Beispielen aus Baidu, die jedoch leider nur für Personen geeignet ist, die Chinesisch sprechen. </p><br><p>  Im nächsten Artikel zu diesem Thema werden wir einen Teil der Aufgabe analysieren, die während des ZeroNights HackQuest 2018 gelöst werden soll, indem ein typisches neuronales Netzwerk mithilfe der FoolBox-Bibliothek getäuscht wird. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de438644/">https://habr.com/ru/post/de438644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de438634/index.html">In Büros ist es entweder zu heiß oder zu kalt: Gibt es eine bessere Möglichkeit, die Temperatur anzupassen?</a></li>
<li><a href="../de438636/index.html">Fehlerhafte Einbettung von Funktionen in Go</a></li>
<li><a href="../de438638/index.html">Wir analysieren das Protokoll der Pager-Nachrichten POCSAG, Teil 2</a></li>
<li><a href="../de438640/index.html">Offene elektronische Hochgeschwindigkeitswährung</a></li>
<li><a href="../de438642/index.html">Die Grundlagen der reaktiven Programmierung mit RxJS</a></li>
<li><a href="../de438646/index.html">Informationen zum Erstellen von Budget-Stereobildern auf Fingern (Stereogramm, Anaglyphe, Stereoskop)</a></li>
<li><a href="../de438648/index.html">Vergleich von BI-Systemen (Tableau, Power BI, Oracle, Qlik)</a></li>
<li><a href="../de438650/index.html">Rakete 9M729. Ein paar Worte zum „Verstoß“ gegen den INF-Vertrag</a></li>
<li><a href="../de438652/index.html">Portabelization IDA</a></li>
<li><a href="../de438654/index.html">OpenSceneGraph: Integration mit dem Qt Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>