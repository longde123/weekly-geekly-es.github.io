<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ñ üï£ üõéÔ∏è Wahngenerator: Erstellen Sie Texte in einer beliebigen Sprache mithilfe eines neuronalen Netzwerks üòÜ üèÇüèø üå©Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr. 

 Dieser Artikel wird ein bisschen wie ein "Freitag" -Format sein, heute werden wir uns mit NLP befassen. Nicht das NLP, √ºber das B√ºcher ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wahngenerator: Erstellen Sie Texte in einer beliebigen Sprache mithilfe eines neuronalen Netzwerks</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470035/">  Hallo Habr. <br><br>  Dieser Artikel wird ein bisschen wie ein "Freitag" -Format sein, heute werden wir uns mit NLP befassen.  Nicht das NLP, √ºber das B√ºcher in Unterf√ºhrungen verkauft werden, sondern das, bei dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Natural Language Processing</a> nat√ºrliche Sprachen verarbeitet.  Als Beispiel f√ºr eine solche Verarbeitung wird die Texterzeugung unter Verwendung eines neuronalen Netzwerks verwendet.  Wir k√∂nnen Texte in jeder Sprache erstellen, von Russisch oder Englisch bis C ++.  Die Ergebnisse sind sehr interessant, k√∂nnen Sie wahrscheinlich aus dem Bild erraten. <br><br><img src="https://habrastorage.org/webt/vc/cy/we/vccywe4c6r0vbryvvx3qiale_j8.jpeg"><br><br>  F√ºr diejenigen, die daran interessiert sind, was passiert, sind die Ergebnisse und der Quellcode unter dem Schnitt. <br><a name="habracut"></a><br><h2>  Datenaufbereitung </h2><br>  F√ºr die Verarbeitung verwenden wir eine spezielle Klasse neuronaler Netze - die sogenannten wiederkehrenden neuronalen Netze (RNN).  Dieses Netzwerk unterscheidet sich von dem √ºblichen dadurch, dass es zus√§tzlich zu den √ºblichen Zellen Speicherzellen hat.  Dies erm√∂glicht es uns, Daten einer komplexeren Struktur und tats√§chlich n√§her am menschlichen Ged√§chtnis zu analysieren, da wir auch nicht jeden Gedanken ‚Äûvon vorne‚Äú beginnen.  Zum Schreiben von Code verwenden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LSTM</a> -Netzwerke (Long Short-Term Memory), da diese bereits von Keras unterst√ºtzt werden. <br><br><img src="https://habrastorage.org/webt/ay/ox/ou/ayoxourylcbidznetfkphgdv5ni.jpeg"><br><br>  Das n√§chste Problem, das gel√∂st werden muss, ist die Arbeit mit Text.  Und hier gibt es zwei Ans√§tze - entweder Symbole oder die ganzen W√∂rter an die Eingabe zu senden.  Das Prinzip des ersten Ansatzes ist einfach: Der Text ist in kurze Bl√∂cke unterteilt, wobei die "Eingaben" ein Fragment des Textes sind und die "Ausgabe" das n√§chste Zeichen ist.  F√ºr den letzten Satz ist "Eingaben sind beispielsweise ein Textst√ºck": <br><br> <code>input:    output: "" <br> input:    : output: "" <br> input:    : output:"" <br> input:    : output: "" <br> input:    : output: "". <br></code> <br>  Usw.  Somit empf√§ngt das neuronale Netzwerk Textfragmente an der Eingabe und an der Ausgabe die Zeichen, die es bilden soll. <br><br>  Der zweite Ansatz ist im Grunde der gleiche, es werden nur ganze W√∂rter anstelle von W√∂rtern verwendet.  Zun√§chst wird ein W√∂rterbuch mit W√∂rtern zusammengestellt und am Netzwerkeingang Zahlen anstelle von W√∂rtern eingegeben. <br><br>  Dies ist nat√ºrlich eine ziemlich vereinfachte Beschreibung.  Keras hat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">bereits</a> Beispiele f√ºr die Texterzeugung, aber erstens werden sie nicht so detailliert beschrieben, und zweitens verwenden alle englischsprachigen Tutorials eher abstrakte Texte wie Shakespeare, die f√ºr den Muttersprachler schwer zu verstehen sind.  Nun, wir testen ein neuronales Netzwerk an unserem gro√üen und leistungsstarken, was nat√ºrlich klarer und verst√§ndlicher sein wird. <br><br><h2>  Netzwerktraining </h2><br>  Als Eingabetext habe ich ... Habrs Kommentare verwendet, die Gr√∂√üe der Quelldatei betr√§gt 1 MB (es gibt nat√ºrlich mehr Kommentare, aber ich musste nur einen Teil verwenden, sonst w√§re das neuronale Netzwerk eine Woche lang trainiert worden und die Leser h√§tten diesen Text bis Freitag nicht gesehen).  Ich m√∂chte Sie daran erinnern, dass nur Buchstaben an die Eingabe eines neuronalen Netzwerks gesendet werden. Das Netzwerk ‚Äûwei√ü‚Äú nichts √ºber die Sprache oder ihre Struktur.  Beginnen wir mit dem Netzwerktraining. <br><br>  <b>5 Minuten Training:</b> <br><br>  Bisher ist nichts klar, aber Sie k√∂nnen bereits einige erkennbare Buchstabenkombinationen sehen: <br><br> <code>                          .                   .                                                     .          ¬´                    <br></code> <br>  <b>15 Minuten Training:</b> <br><br>  Das Ergebnis ist bereits sp√ºrbar besser: <br><br> <code>                                                                                                                                 <br></code> <br>  <b>1 Stunde Training:</b> <br><br> <code>                                                                  ¬´ ¬ª ‚Äî                                                            ¬´     ¬ª ¬ª ‚Äî             </code> <br> <br>  Aus irgendeinem Grund erwiesen sich alle Texte als ohne Punkte und ohne Gro√übuchstaben. Vielleicht wurde die Verarbeitung von utf-8 nicht korrekt durchgef√ºhrt.  Aber insgesamt ist das beeindruckend.  Durch die Analyse und Speicherung nur von Symbolcodes lernte das Programm tats√§chlich ‚Äûunabh√§ngig‚Äú russische W√∂rter und kann einen glaubw√ºrdig aussehenden Text erzeugen. <br><br>  Nicht weniger interessant ist die Tatsache, dass sich das Programm den Textstil recht gut merkt.  Im folgenden Beispiel wurde der Text eines Gesetzes als Lehre verwendet.  Netzwerktrainingszeit 5 Minuten. <br><br> <code>  ""  ,  ,  ,  ,  ,  , ,  ,                 <br></code> <br>  Und hier wurden medizinische Anmerkungen f√ºr Medikamente als Eingabesatz verwendet.  Netzwerktrainingszeit 5 Minuten. <br><br> <code>  <br>      <br>                                          ,    ,             <br></code> <br>  Hier sehen wir fast ganze S√§tze.  Dies liegt an der Tatsache, dass der Originaltext kurz ist und das neuronale Netzwerk tats√§chlich einige Phrasen als Ganzes "auswendig gelernt" hat.  Dieser Effekt wird als ‚ÄûUmschulung‚Äú bezeichnet und sollte vermieden werden.  Idealerweise m√ºssen Sie ein neuronales Netzwerk an gro√üen Datenmengen testen, aber das Training kann in diesem Fall viele Stunden dauern, und leider habe ich keinen zus√§tzlichen Supercomputer. <br><br>  Ein unterhaltsames Beispiel f√ºr die Verwendung eines solchen Netzwerks ist die Namensgenerierung.  Nachdem ich eine Liste mit m√§nnlichen und weiblichen Namen in die Datei hochgeladen hatte, erhielt ich interessante neue Optionen, die f√ºr einen Science-Fiction-Roman gut geeignet w√§ren: Rlar, Laaa, Aria, Arera, Aelia, Ninran, Air.  Etwas in ihnen f√ºhlt den Stil von Efremov und dem Andromeda-Nebel ... <br><br><h2>  C ++ </h2><br>  Das Interessante ist, dass ein neuronales Netzwerk im Gro√üen und Ganzen wie das Erinnern ist.  Der n√§chste Schritt bestand darin, zu √ºberpr√ºfen, wie das Programm mit dem Quellcode umgeht.  Als Test habe ich verschiedene C ++ - Quellen genommen und sie zu einer Textdatei zusammengefasst. <br><br>  Ehrlich gesagt √ºberraschte das Ergebnis noch mehr als im Fall der russischen Sprache. <br><br>  <b>5 Minuten Training</b> <br><br>  Verdammt, es ist fast echtes C ++. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( snd_pcm_state_channels = <span class="hljs-number"><span class="hljs-number">0</span></span> ) { errortext_ = <span class="hljs-string"><span class="hljs-string">"rtapialsa::probedeviceopen: esror stream_.buffer stream!"</span></span>; errortext_ = errorstream_.str(); <span class="hljs-keyword"><span class="hljs-keyword">goto</span></span> unlock; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.mode == input &amp;&amp; stream_.mode == output || <span class="hljs-literal"><span class="hljs-literal">false</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ( stream_.state == stream_stopped ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;stream_.nuserbuffer[i] ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (j=<span class="hljs-number"><span class="hljs-number">0</span></span>; j&lt;info.channels; } } }</code> </pre><br>  <b>30 Minuten Training</b> <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> maxirecorder::stopstream() { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; ainchannels; i++ ) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> input=(stream, null; conternallock( pthread_cond_wate); <span class="hljs-comment"><span class="hljs-comment">// this should not ported to the device is a structure are is and the stream the buffer of the interleaved as and the probe the arauliored format for the the stream os and the software and the supported of the is so aa is set the directsound of allocates the reset and real the read the setup the buffer // this is the setup to the the devices. // asio will open the supported by the supported buffer are asioned structure. // this should not have to setup the counting and the proper in the position we mast it alw the aftware configuration. // this is the the stream and probe the supported be audio for sched/ond changed to this the sample rate it in the counting the fres // mecorend. windevoppart( null, 0, &amp;phase ); possint=&gt;buffersize( 1, bufferlist); } } } ///********************************************* void maxirecorder::abortdevice( device ) { if (cout &lt;&lt; aindeff( thread_cont_start); for (int i = 0; i &lt; sample_rates[0]) { if (count == int) { std::cout &lt;&lt; "checksing of convernion allocation handler (" &lt;&lt; device &lt;&lt; ")."; errortext_ = errorstream_.str(); return failure; } } } if ( stream_.userbuffer[i] ) { errortext_ = "rtapicore::probedeviceopen: system error (" &lt;&lt; geterrorcode( result ) &lt;&lt; ") getting stream configuration info state."; error( rterror::warning ); return; } if ( stream_.mode == output || stream_.mode == duplex ) { callbackinfo *info = (const char *) callback( stream_.userbuffer[1], stream_.convertinfo[0] ); return info; } // setup buffer callback for conversion on do any the supported be audio bet that al alinums be of support of the supported buffer // the buffer size a stream and buffers. info.name = dsdevices[ device ]; return 0; }</span></span></code> </pre> <br>  Wie Sie sehen k√∂nnen, hat das Programm "gelernt", ganze Funktionen zu schreiben.  Gleichzeitig wurden die Funktionen durch einen Kommentar mit Sternchen vollst√§ndig ‚Äûmenschlich‚Äú getrennt, Kommentare in den Code eingef√ºgt und so weiter.  Ich m√∂chte eine neue Programmiersprache mit einer solchen Geschwindigkeit lernen ... Nat√ºrlich gibt es Fehler im Code, und nat√ºrlich wird er nicht kompiliert.  √úbrigens habe ich den Code nicht formatiert, das Programm hat auch gelernt, Klammern und Einr√ºckungen "selbst" zu setzen. <br><br>  Nat√ºrlich haben diese Programme nicht die Hauptsache - <i>Bedeutung</i> und sehen daher surreal aus, als ob sie in einem Traum geschrieben wurden oder nicht von einer v√∂llig gesunden Person geschrieben wurden.  Trotzdem sind die Ergebnisse beeindruckend.  Und vielleicht hilft eine eingehendere Untersuchung der Generierung verschiedener Texte, einige der psychischen Erkrankungen realer Patienten besser zu verstehen.  √úbrigens gibt es, wie in den Kommentaren angedeutet, eine solche Geisteskrankheit, bei der eine Person in einem grammatikalisch verwandten, aber v√∂llig bedeutungslosen Text spricht ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schizophasie</a> ). <br><br><h2>  Fazit </h2><br>  Neuronale Freizeitnetze gelten als sehr vielversprechend, und dies ist in der Tat ein gro√üer Fortschritt im Vergleich zu ‚Äûnormalen‚Äú Netzen wie MLP, die keinen Speicher haben.  In der Tat sind die F√§higkeiten neuronaler Netze zum Speichern und Verarbeiten ziemlich komplexer Strukturen beeindruckend.  Nach diesen Tests dachte ich zum ersten Mal, dass Ilon Mask wahrscheinlich in etwas Recht hatte, als ich schrieb, dass KI in Zukunft ‚Äûdas gr√∂√üte Risiko f√ºr die Menschheit‚Äú sein k√∂nnte - selbst wenn sich ein einfaches neuronales Netzwerk leicht erinnern und reproduzieren kann ziemlich komplexe Muster, was kann ein Netzwerk von Milliarden von Komponenten tun?  Vergessen Sie andererseits nicht, dass unser neuronales Netzwerk nicht <i>denken kann</i> , sondern sich im Wesentlichen nur mechanisch an Zeichenfolgen erinnert und deren Bedeutung nicht versteht.  Dies ist ein wichtiger Punkt - selbst wenn Sie ein neuronales Netzwerk auf einem Supercomputer und einem riesigen Datensatz trainieren, wird es bestenfalls lernen, wie man grammatikalisch 100% korrekte, aber v√∂llig bedeutungslose S√§tze erzeugt. <br><br>  Aber es wird nicht in der Philosophie entfernt, der Artikel ist noch mehr f√ºr Praktiker.  F√ºr diejenigen, die alleine experimentieren m√∂chten, befindet sich der <b>Quellcode</b> in Python 3.7 unter dem Spoiler.  Dieser Code ist eine Zusammenstellung aus verschiedenen Github-Projekten und kein Beispiel f√ºr den besten Code, scheint aber seine Aufgabe zu erf√ºllen. <br><br>  Die Verwendung des Programms erfordert keine Programmierkenntnisse. Sie m√ºssen lediglich wissen, wie Python installiert wird.  Beispiele f√ºr das Starten √ºber die Befehlszeile: <br>  - Erstellen und Trainieren von Modellen und Textgenerierung: <br>  Python. \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000 <br>  - Nur Texterstellung ohne Modelltraining: <br>  python. \ keras_textgen.py --text = text_habr.txt --epochs = 10 --out_len = 4000 --generate <br><br><div class="spoiler">  <b class="spoiler_title">keras_textgen.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-comment"><span class="hljs-comment"># Force CPU os.environ["CUDA_VISIBLE_DEVICES"] = "-1" os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 0 = all messages are logged, 3 - INFO, WARNING, and ERROR messages are not printed from keras.callbacks import LambdaCallback from keras.models import Sequential from keras.layers import Dense, Dropout, Embedding, LSTM, TimeDistributed from keras.optimizers import RMSprop from keras.utils.data_utils import get_file import keras from collections import Counter import pickle import numpy as np import random import sys import time import io import re import argparse # Transforms text to vectors of integer numbers representing in text tokens and back. Handles word and character level tokenization. class Vectorizer: def __init__(self, text, word_tokens, pristine_input, pristine_output): self.word_tokens = word_tokens self._pristine_input = pristine_input self._pristine_output = pristine_output tokens = self._tokenize(text) # print('corpus length:', len(tokens)) token_counts = Counter(tokens) # Sort so most common tokens come first in our vocabulary tokens = [x[0] for x in token_counts.most_common()] self._token_indices = {x: i for i, x in enumerate(tokens)} self._indices_token = {i: x for i, x in enumerate(tokens)} self.vocab_size = len(tokens) print('Vocab size:', self.vocab_size) def _tokenize(self, text): if not self._pristine_input: text = text.lower() if self.word_tokens: if self._pristine_input: return text.split() return Vectorizer.word_tokenize(text) return text def _detokenize(self, tokens): if self.word_tokens: if self._pristine_output: return ' '.join(tokens) return Vectorizer.word_detokenize(tokens) return ''.join(tokens) def vectorize(self, text): """Transforms text to a vector of integers""" tokens = self._tokenize(text) indices = [] for token in tokens: if token in self._token_indices: indices.append(self._token_indices[token]) else: print('Ignoring unrecognized token:', token) return np.array(indices, dtype=np.int32) def unvectorize(self, vector): """Transforms a vector of integers back to text""" tokens = [self._indices_token[index] for index in vector] return self._detokenize(tokens) @staticmethod def word_detokenize(tokens): # A heuristic attempt to undo the Penn Treebank tokenization above. Pass the # --pristine-output flag if no attempt at detokenizing is desired. regexes = [ # Newlines (re.compile(r'[ ]?\\n[ ]?'), r'\n'), # Contractions (re.compile(r"\b(can)\s(not)\b"), r'\1\2'), (re.compile(r"\b(d)\s('ye)\b"), r'\1\2'), (re.compile(r"\b(gim)\s(me)\b"), r'\1\2'), (re.compile(r"\b(gon)\s(na)\b"), r'\1\2'), (re.compile(r"\b(got)\s(ta)\b"), r'\1\2'), (re.compile(r"\b(lem)\s(me)\b"), r'\1\2'), (re.compile(r"\b(mor)\s('n)\b"), r'\1\2'), (re.compile(r"\b(wan)\s(na)\b"), r'\1\2'), # Ending quotes (re.compile(r"([^' ]) ('ll|'re|'ve|n't)\b"), r"\1\2"), (re.compile(r"([^' ]) ('s|'m|'d)\b"), r"\1\2"), (re.compile(r'[ ]?‚Äù'), r'"'), # Double dashes (re.compile(r'[ ]?--[ ]?'), r'--'), # Parens and brackets (re.compile(r'([\[\(\{\&lt;]) '), r'\1'), (re.compile(r' ([\]\)\}\&gt;])'), r'\1'), (re.compile(r'([\]\)\}\&gt;]) ([:;,.])'), r'\1\2'), # Punctuation (re.compile(r"([^']) ' "), r"\1' "), (re.compile(r' ([?!\.])'), r'\1'), (re.compile(r'([^\.])\s(\.)([\]\)}&gt;"\']*)\s*$'), r'\1\2\3'), (re.compile(r'([#$]) '), r'\1'), (re.compile(r' ([;%:,])'), r'\1'), # Starting quotes (re.compile(r'(‚Äú)[ ]?'), r'"') ] text = ' '.join(tokens) for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.strip() @staticmethod def word_tokenize(text): # Basic word tokenizer based on the Penn Treebank tokenization script, but # setup to handle multiple sentences. Newline aware, ie newlines are # replaced with a specific token. You may want to consider using a more robust # tokenizer as a preprocessing step, and using the --pristine-input flag. regexes = [ # Starting quotes (re.compile(r'(\s)"'), r'\1 ‚Äú '), (re.compile(r'([ (\[{&lt;])"'), r'\1 ‚Äú '), # Punctuation (re.compile(r'([:,])([^\d])'), r' \1 \2'), (re.compile(r'([:,])$'), r' \1 '), (re.compile(r'\.\.\.'), r' ... '), (re.compile(r'([;@#$%&amp;])'), r' \1 '), (re.compile(r'([?!\.])'), r' \1 '), (re.compile(r"([^'])' "), r"\1 ' "), # Parens and brackets (re.compile(r'([\]\[\(\)\{\}\&lt;\&gt;])'), r' \1 '), # Double dashes (re.compile(r'--'), r' -- '), # Ending quotes (re.compile(r'"'), r' ‚Äù '), (re.compile(r"([^' ])('s|'m|'d) "), r"\1 \2 "), (re.compile(r"([^' ])('ll|'re|'ve|n't) "), r"\1 \2 "), # Contractions (re.compile(r"\b(can)(not)\b"), r' \1 \2 '), (re.compile(r"\b(d)('ye)\b"), r' \1 \2 '), (re.compile(r"\b(gim)(me)\b"), r' \1 \2 '), (re.compile(r"\b(gon)(na)\b"), r' \1 \2 '), (re.compile(r"\b(got)(ta)\b"), r' \1 \2 '), (re.compile(r"\b(lem)(me)\b"), r' \1 \2 '), (re.compile(r"\b(mor)('n)\b"), r' \1 \2 '), (re.compile(r"\b(wan)(na)\b"), r' \1 \2 '), # Newlines (re.compile(r'\n'), r' \\n ') ] text = " " + text + " " for regexp, substitution in regexes: text = regexp.sub(substitution, text) return text.split() def _create_sequences(vector, seq_length, seq_step): # Take strips of our vector at seq_step intervals up to our seq_length # and cut those strips into seq_length sequences passes = [] for offset in range(0, seq_length, seq_step): pass_samples = vector[offset:] num_pass_samples = pass_samples.size // seq_length pass_samples = np.resize(pass_samples, (num_pass_samples, seq_length)) passes.append(pass_samples) # Stack our sequences together. This will technically leave a few "breaks" # in our sequence chain where we've looped over are entire dataset and # return to the start, but with large datasets this should be neglegable return np.concatenate(passes) def shape_for_stateful_rnn(data, batch_size, seq_length, seq_step): """ Reformat our data vector into input and target sequences to feed into our RNN. Tricky with stateful RNNs. """ # Our target sequences are simply one timestep ahead of our input sequences. # eg with an input vector "wherefore"... # targets: herefore # predicts ^ ^ ^ ^ ^ ^ ^ ^ # inputs: wherefor inputs = data[:-1] targets = data[1:] # We split our long vectors into semi-redundant seq_length sequences inputs = _create_sequences(inputs, seq_length, seq_step) targets = _create_sequences(targets, seq_length, seq_step) # Make sure our sequences line up across batches for stateful RNNs inputs = _batch_sort_for_stateful_rnn(inputs, batch_size) targets = _batch_sort_for_stateful_rnn(targets, batch_size) # Our target data needs an extra axis to work with the sparse categorical # crossentropy loss function targets = targets[:, :, np.newaxis] return inputs, targets def _batch_sort_for_stateful_rnn(sequences, batch_size): # Now the tricky part, we need to reformat our data so the first # sequence in the nth batch picks up exactly where the first sequence # in the (n - 1)th batch left off, as the RNN cell state will not be # reset between batches in the stateful model. num_batches = sequences.shape[0] // batch_size num_samples = num_batches * batch_size reshuffled = np.zeros((num_samples, sequences.shape[1]), dtype=np.int32) for batch_index in range(batch_size): # Take a slice of num_batches consecutive samples slice_start = batch_index * num_batches slice_end = slice_start + num_batches index_slice = sequences[slice_start:slice_end, :] # Spread it across each of our batches in the same index position reshuffled[batch_index::batch_size, :] = index_slice return reshuffled def load_data(data_file, word_tokens, pristine_input, pristine_output, batch_size, seq_length=50, seq_step=25): global vectorizer try: with open(data_file, encoding='utf-8') as input_file: text = input_file.read() except FileNotFoundError: print("No input.txt in data_dir") sys.exit(1) skip_validate = True # try: # with open(os.path.join(data_dir, 'validate.txt'), encoding='utf-8') as validate_file: # text_val = validate_file.read() # skip_validate = False # except FileNotFoundError: # pass # Validation text optional # Find some good default seed string in our source text. # self.seeds = find_random_seeds(text) # Include our validation texts with our vectorizer all_text = text if skip_validate else '\n'.join([text, text_val]) vectorizer = Vectorizer(all_text, word_tokens, pristine_input, pristine_output) data = vectorizer.vectorize(text) x, y = shape_for_stateful_rnn(data, batch_size, seq_length, seq_step) print("Word_tokens:", word_tokens) print('x.shape:', x.shape) print('y.shape:', y.shape) if skip_validate: return x, y, None, None, vectorizer data_val = vectorizer.vectorize(text_val) x_val, y_val = shape_for_stateful_rnn(data_val, batch_size, seq_length, seq_step) print('x_val.shape:', x_val.shape) print('y_val.shape:', y_val.shape) return x, y, x_val, y_val, vectorizer def make_model(batch_size, vocab_size, embedding_size=64, rnn_size=128, num_layers=2): # Conversely if your data is large (more than about 2MB), feel confident to increase rnn_size and train a bigger model (see details of training below). # It will work significantly better. For example with 6MB you can easily go up to rnn_size 300 or even more. model = Sequential() model.add(Embedding(vocab_size, embedding_size, batch_input_shape=(batch_size, None))) for layer in range(num_layers): model.add(LSTM(rnn_size, stateful=True, return_sequences=True)) model.add(Dropout(0.2)) model.add(TimeDistributed(Dense(vocab_size, activation='softmax'))) model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) return model def train(model, x, y, x_val, y_val, batch_size, num_epochs): print('Training...') # print("Shape:", x.shape, y.shape) # print(num_epochs, batch_size, x[0], y[0]) train_start = time.time() validation_data = (x_val, y_val) if (x_val is not None) else None callbacks = None model.fit(x, y, validation_data=validation_data, batch_size=batch_size, shuffle=False, epochs=num_epochs, verbose=1, callbacks=callbacks) # self.update_sample_model_weights() train_end = time.time() print('Training time', train_end - train_start) def sample_preds(preds, temperature=1.0): """ Samples an unnormalized array of probabilities. Use temperature to flatten/amplify the probabilities. """ preds = np.asarray(preds).astype(np.float64) # Add a tiny positive number to avoid invalid log(0) preds += np.finfo(np.float64).tiny preds = np.log(preds) / temperature exp_preds = np.exp(preds) preds = exp_preds / np.sum(exp_preds) probas = np.random.multinomial(1, preds, 1) return np.argmax(probas) def generate(model, vectorizer, seed, length=100, diversity=0.5): seed_vector = vectorizer.vectorize(seed) # Feed in seed string print("Seed:", seed, end=' ' if vectorizer.word_tokens else '') model.reset_states() preds = None for char_index in np.nditer(seed_vector): preds = model.predict(np.array([[char_index]]), verbose=0) sampled_indices = [] # np.array([], dtype=np.int32) # Sample the model one token at a time for i in range(length): char_index = 0 if preds is not None: char_index = sample_preds(preds[0][0], diversity) sampled_indices.append(char_index) # = np.append(sampled_indices, char_index) preds = model.predict(np.array([[char_index]]), verbose=0) sample = vectorizer.unvectorize(sampled_indices) return sample if __name__ == "__main__": batch_size = 32 # Batch size for each train num_epochs = 10 # Number of epochs of training out_len = 200 # Length of the output phrase seq_length = 50 # 50 # Determines, how long phrases will be used for training use_words = False # Use words instead of characters (slower speed, bigger vocabulary) data_file = "text_habr.txt" # Source text file seed = "A" # Initial symbol of the text parser = argparse.ArgumentParser() parser.add_argument("-t", "--text", action="store", required=False, dest="text", help="Input text file") parser.add_argument("-e", "--epochs", action="store", required=False, dest="epochs", help="Number of training epochs") parser.add_argument("-p", "--phrase_len", action="store", required=False, dest="phrase_len", help="Phrase analyse length") parser.add_argument("-o", "--out_len", action="store", required=False, dest="out_len", help="Output text length") parser.add_argument("-g", "--generate", action="store_true", required=False, dest='generate', help="Generate output only without training") args = parser.parse_args() if args.text is not None: data_file = args.text if args.epochs is not None: num_epochs = int(args.epochs) if args.phrase_len is not None: seq_length = int(args.phrase_len) if args.out_len is not None: out_len = int(args.out_len) # Load text data pristine_input, pristine_output = False, False x, y, x_val, y_val, vectorizer = load_data(data_file, use_words, pristine_input, pristine_output, batch_size, seq_length) model_file = data_file.lower().replace('.txt', '.h5') if args.generate is False: # Make model model = make_model(batch_size, vectorizer.vocab_size) # Train model train(model, x, y, x_val, y_val, batch_size, num_epochs) # Save model to file model.save(filepath=model_file) model = keras.models.load_model(model_file) predict_model = make_model(1, vectorizer.vocab_size) predict_model.set_weights(model.get_weights()) # Generate phrases res = generate(predict_model, vectorizer, seed=seed, length=out_len) print(res)</span></span></code> </pre><br></div></div><br>  Ich denke, es hat sich als sehr <s>funky</s> funktionierender Textgenerator herausgestellt, <s>der n√ºtzlich ist, um Artikel √ºber Habr zu schreiben</s> .  Besonders interessant ist das Testen gro√üer Texte und einer gro√üen Anzahl von Trainingsiterationen. Wenn jemand Zugang zu schnellen Computern hat, w√§re es interessant, die Ergebnisse zu sehen. <br><br>  Wenn jemand das Thema genauer untersuchen m√∂chte, finden Sie eine gute Beschreibung der Verwendung von RNN mit detaillierten Beispielen unter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a> . <br><br>  PS: Und zum Schluss noch ein paar Verse;) Es ist interessant festzustellen, dass nicht ich die Formatierung des Textes oder sogar das Hinzuf√ºgen von Sternen vorgenommen habe. "Ich bin es selbst."  Der n√§chste Schritt ist interessant, um die M√∂glichkeit zu pr√ºfen, Bilder zu zeichnen und Musik zu komponieren.  Ich denke, neuronale Netze sind hier sehr vielversprechend. <br><br>  <i>xxx</i> <i><br><br></i>  <i>f√ºr einige in Keksen gefangen zu sein - alles in viel Gl√ºck auf einem Brotplatz.</i> <i><br></i>  <i>und unter dem Abend von Tamaki</i> <i><br></i>  <i>unter einer Kerze einen Berg nehmen.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>bald S√∂hne Mons in Petachas in der Stra√üenbahn</i> <i><br></i>  <i>Das unsichtbare Licht riecht nach Freude</i> <i><br></i>  <i>deshalb klopfe ich zusammen w√§chst</i> <i><br></i>  <i>Sie werden wegen eines Unbekannten nicht krank sein.</i> <i><br><br></i>  <i>Herz in gestaffelten Ogora zu zupfen,</i> <i><br></i>  <i>es ist nicht so alt, dass das Getreide isst,</i> <i><br></i>  <i>Ich bewache die Br√ºcke zum Ball, um zu stehlen.</i> <i><br><br></i>  <i>auf die gleiche Weise Darina in Doba,</i> <i><br></i>  <i>Ich h√∂re in meinem Herzen Schnee auf meiner Hand.</i> <i><br></i>  <i>unsere singen wei√ü wie viele sanfte dumina</i> <i><br></i>  <i>Ich wandte mich von der Erzbestie ab.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>Tierarzt Kreuziger mit einem Zauber</i> <i><br></i>  <i>und unter den Vergessenen versch√ºttet.</i> <i><br></i>  <i>und du, wie bei den Zweigen Kubas</i> <i><br></i>  <i>darin gl√§nzen.</i> <i><br></i>  <i>o Spa√ü bei Zakoto</i> <i><br></i>  <i>mit dem Flug der Milch.</i> <i><br><br></i>  <i>Oh, du bist eine Rose, Licht</i> <i><br></i>  <i>Wolkenlicht zur Hand:</i> <i><br></i>  <i>und rollte im Morgengrauen</i> <i><br></i>  <i>Wie geht es dir, mein Reiter?</i> <i><br><br></i>  <i>Er dient abends, nicht bis auf die Knochen.</i> <i><br></i>  <i>Nachts in Tanya das blaue Licht</i> <i><br></i>  <i>wie eine Art Traurigkeit.</i> <br><br>  Und die letzten Verse zum Lernen im Wortmodus.  Hier verschwand der Reim, aber es erschien eine Bedeutung (?). <br><br>  <i>und du, von der Flamme</i> <i><br></i>  <i>die Sterne.</i> <i><br></i>  <i>sprach mit entfernten Personen.</i> <i><br><br></i>  <i>Sorgen Sie rus ,, Sie ,, in morgen.</i> <i><br></i>  <i>"Taubenregen,</i> <i><br></i>  <i>und Heimat der M√∂rder,</i> <i><br></i>  <i>f√ºr das Prinzessinnenm√§dchen</i> <i><br></i>  <i>sein Gesicht.</i> <i><br><br></i>  <i>xxx</i> <i><br><br></i>  <i>Oh Hirte, winke den Kammern zu</i> <i><br></i>  <i>auf einem Hain im Fr√ºhjahr.</i> <i><br><br></i>  <i>Ich gehe durch das Herz des Hauses zum Teich,</i> <i><br></i>  <i>und M√§use frech</i> <i><br></i>  <i>Nischni Nowgorod Glocken.</i> <i><br><br></i>  <i>aber f√ºrchte dich nicht, der Morgenwind,</i> <i><br></i>  <i>mit einem Weg, mit einem eisernen Kn√ºppel,</i> <i><br></i>  <i>und dachte mit der Auster</i> <i><br></i>  <i>auf einem Teich beherbergt</i> <i><br></i>  <i>in verarmtem Rakit.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470035/">https://habr.com/ru/post/de470035/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470021/index.html">An√§misches und reichhaltiges Modell im Kontext von GRASP-Vorlagen</a></li>
<li><a href="../de470023/index.html">Wir schreiben eine Zahlung f√ºr einen Telegrammbot in Python mit der Telebot-Bibliothek Teil 3</a></li>
<li><a href="../de470027/index.html">VK Hackathon 2019 (wie es war)</a></li>
<li><a href="../de470029/index.html">Extreme P√§dagogik: ‚ÄûWir wissen‚Äú √ºber die Langzeitbehandlung mit Kindern</a></li>
<li><a href="../de470033/index.html">F # 2: FSI-Umgebung</a></li>
<li><a href="../de470043/index.html">Die Wissenschaft dahinter, wie unser Gehirn am besten funktioniert und wie Technologie und unsere Umwelt helfen k√∂nnen</a></li>
<li><a href="../de470045/index.html">Visual Studio f√ºr Mac: Top-Funktionen des neuen Editors</a></li>
<li><a href="../de470047/index.html">Ich schaute zur√ºck, um zu sehen, ob sie zur√ºckblickte - 2 oder √ºber AWS zu meinem eigenen Rechenzentrum</a></li>
<li><a href="../de470049/index.html">Einf√ºhrung in NuGet Package Management auf L√∂sungsebene in Visual Studio f√ºr Mac</a></li>
<li><a href="../de470051/index.html">Neurowissenschaften: Wenn unser Gehirn am besten funktioniert und wie Technologie helfen kann</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>