<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå∏ üõÉ üè∞ An√°lise da colora√ß√£o emocional das cr√≠ticas do Kinopoisk ‚òùüèø üëãüèø ‚óªÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entrada 
 O Processamento de linguagem natural (PNL) √© uma √°rea popular e importante do aprendizado de m√°quina. Neste hub, descreverei meu primeiro pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lise da colora√ß√£o emocional das cr√≠ticas do Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Entrada </h3><br>  O Processamento de linguagem natural (PNL) √© uma √°rea popular e importante do aprendizado de m√°quina.  Neste hub, descreverei meu primeiro projeto relacionado √† an√°lise da colora√ß√£o emocional de resenhas de filmes escritas em Python.  A tarefa da an√°lise sentimental √© bastante comum entre aqueles que desejam dominar os conceitos b√°sicos da PNL e pode se tornar um an√°logo do 'Hello world' nesta √°rea. <br><br>  Neste artigo, abordaremos todas as principais etapas do processo de ci√™ncia de dados: desde a cria√ß√£o de seu pr√≥prio conjunto de dados, processamento e extra√ß√£o de recursos usando a biblioteca NLTK e finalmente aprendendo e ajustando o modelo usando o scikit-learn.  A tarefa em si √© classificar as revis√µes em tr√™s classes: negativa, neutra e positiva. <br><a name="habracut"></a><br><h3>  Forma√ß√£o de Corpus de Dados </h3><br>  Para resolver esse problema, pode-se usar um corpo de dados j√° anotado e com an√°lises do IMDB, dos quais existem muitos no GitHub.  Mas foi decidido criar o seu pr√≥prio com coment√°rios em russo, retirados do Kinopoisk.  Para n√£o copi√°-los manualmente, escreveremos um analisador da web.  <i>Usarei a</i> biblioteca de <i>solicita√ß√µes</i> para enviar <i>solicita√ß√µes</i> HTTP e o <i>BeautifulSoup</i> para processar arquivos html.  Primeiro, vamos definir uma fun√ß√£o que ter√° um link para as cr√≠ticas de filmes e as recuperar√°.  Para que o Kinopoisk n√£o reconhe√ßa o bot em n√≥s, voc√™ precisa especificar o argumento <i>headers</i> na fun√ß√£o orders.get, que simular√° o navegador.  √â necess√°rio passar um dicion√°rio para ele com as chaves User-Agent, Accept-language e Accept, cujos valores podem ser encontrados nas ferramentas do desenvolvedor do navegador.  Em seguida, um analisador √© criado e as revis√µes s√£o armazenadas na p√°gina, que s√£o armazenadas na classe de marca√ß√£o _reachbanner_ html. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  N√≥s nos livramos da marca√ß√£o html, no entanto, nossos coment√°rios ainda s√£o objetos <i>BeautifulSoup</i> , mas precisamos convert√™-los em strings.  A fun√ß√£o de <i>convers√£o</i> faz exatamente isso.  Tamb√©m escreveremos uma fun√ß√£o que recupera o nome do filme, que mais tarde ser√° usada para salvar cr√≠ticas. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  A √∫ltima fun√ß√£o do analisador ter√° um link para a p√°gina principal do filme, uma classe de cr√≠tica e uma maneira de salvar cr√≠ticas.  A fun√ß√£o tamb√©m define <i>atrasos</i> entre solicita√ß√µes necess√°rias para evitar uma proibi√ß√£o.  A fun√ß√£o cont√©m um loop que recupera e armazena revis√µes a partir da primeira p√°gina, at√© encontrar uma p√°gina inexistente da qual a fun√ß√£o <i>load_data</i> extrair√° uma lista vazia e o loop ser√° interrompido. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Em seguida, usando o ciclo a seguir, voc√™ pode extrair resenhas de filmes que est√£o na lista de <i>URLs</i> .  Uma lista de filmes precisar√° ser criada manualmente.  Seria poss√≠vel, por exemplo, obter uma lista de links para filmes escrevendo uma fun√ß√£o que os extra√≠sse dos 250 principais filmes de busca de filmes, para n√£o fazer manualmente, mas 15 a 20 filmes seriam suficientes para formar um pequeno conjunto de dados de mil cr√≠ticas para cada classe.  Al√©m disso, se voc√™ receber uma proibi√ß√£o, o programa exibir√° em qual filme e classe o analisador parou para continuar do mesmo local ap√≥s a aprova√ß√£o. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Pr√©-tratamento </h3><br>  Depois de escrever um analisador, relembrando filmes aleat√≥rios para ele e v√°rias proibi√ß√µes de uma pesquisa de filmes, misturei as resenhas em pastas e selecionei 900 resenhas de cada classe para treinamento e o restante para o grupo de controle.  Agora √© necess√°rio pr√©-processar o alojamento, ou seja, tokenizar e normaliz√°-lo.  Tokenizar significa dividir o texto em componentes, neste caso em palavras, pois usaremos a representa√ß√£o de um conjunto de palavras.  E a normaliza√ß√£o consiste em converter palavras em min√∫sculas, remover palavras de parada e excesso de ru√≠do, golpes e outros truques que ajudam a reduzir o espa√ßo dos sinais. <br><br>  Importamos as bibliotecas necess√°rias. <br><br><div class="spoiler">  <b class="spoiler_title">Texto oculto</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Come√ßamos definindo algumas pequenas fun√ß√µes para o pr√©-processamento de texto.  O primeiro, chamado <i>lower_pos_tag,</i> far√° uma lista com palavras, as converter√° em min√∫sculas e salvar√° cada token em uma tupla com sua parte do discurso.  A opera√ß√£o de adi√ß√£o de parte do discurso a uma palavra √© chamada de marca√ß√£o Parte do discurso (POS) e √© frequentemente usada na PNL para extrair entidades.  No nosso caso, usaremos partes do discurso na fun√ß√£o a seguir para filtrar palavras. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Os textos cont√™m um grande n√∫mero de palavras que muitas vezes s√£o √∫teis para o modelo (as chamadas palavras de parada).  Basicamente, s√£o preposi√ß√µes, conjun√ß√µes, pronomes pelos quais √© imposs√≠vel determinar a que lembran√ßa de classe se refere.  A fun√ß√£o <i>clean</i> deixa apenas substantivos, adjetivos, verbos e adv√©rbios.  Observe que ele remove partes do discurso, pois n√£o s√£o necess√°rias para o modelo em si.  Voc√™ tamb√©m pode perceber que essa fun√ß√£o usa stamming, cuja ess√™ncia √© eliminar sufixos e prefixos de palavras.  Isso permite reduzir a dimens√£o dos sinais, pois as palavras com diferentes g√™neros e casos ser√£o reduzidas para o mesmo token.  Existe um an√°logo mais poderoso de stamming - lematization, que permite restaurar a forma inicial da palavra.  No entanto, funciona mais devagar do que stamming e, al√©m disso, o NLTK n√£o possui um lematizador russo. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  Em seguida, escrevemos a fun√ß√£o final que receber√° o r√≥tulo da classe e recuperar√° todas as revis√µes com essa classe.  Para ler o caso, usaremos o m√©todo <i>bruto</i> do objeto <i>PlaintextCorpusReader</i> , que permite extrair texto do arquivo especificado.  Em seguida, a tokeniza√ß√£o √© usada RegexpTokenizer, trabalhando com base em uma express√£o regular.  Al√©m das palavras individuais, adicionei ao modelo os bigrams, que s√£o combina√ß√µes de todas as palavras vizinhas.  Essa fun√ß√£o tamb√©m usa o objeto <i>FreqDist</i> , que retorna a frequ√™ncia de ocorr√™ncia de palavras.  √â usado aqui para remover palavras que aparecem em todas as revis√µes de uma classe espec√≠fica apenas uma vez (elas tamb√©m s√£o chamadas de hapaks).  Assim, a fun√ß√£o retornar√° um dicion√°rio contendo documentos apresentados como um conjunto de palavras e uma lista de todas as palavras para uma classe espec√≠fica. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  O est√°gio de pr√©-processamento √© o mais longo, por isso faz sentido paralelizar o processamento do nosso caso.  Isso pode ser feito usando o m√≥dulo de <i>multiprocessamento</i> .  Na pr√≥xima parte do c√≥digo do programa, inicio tr√™s processos que processar√£o simultaneamente tr√™s pastas com classes diferentes.  Em seguida, os resultados ser√£o coletados em um dicion√°rio.  Esse pr√©-processamento est√° conclu√≠do. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vetoriza√ß√£o </h3><br>  Depois de pr√©-processarmos o caso, temos um dicion√°rio em que, para cada r√≥tulo de classe, h√° uma lista com revis√µes que simbolizamos, normalizamos e enriquecemos com bigrams, al√©m de uma lista de palavras de todas as revis√µes desta classe.  Como o modelo n√£o pode perceber a linguagem natural como n√≥s, a tarefa agora √© apresentar nossas revis√µes em forma num√©rica.  Para fazer isso, criaremos um vocabul√°rio comum, composto por tokens exclusivos, e com ele vetorizaremos cada revis√£o. <br><br>  Para come√ßar, criamos uma lista que cont√©m revis√µes de todas as classes, juntamente com seus r√≥tulos.  Em seguida, criamos um vocabul√°rio comum, tirando de cada classe 10.000 das palavras mais comuns usando o m√©todo <i>most_common</i> do mesmo <i>FreqDist</i> .  Como resultado, obtive um vocabul√°rio composto por cerca de 17.000 palavras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Existem v√°rias maneiras de vetorizar o texto.  O mais popular deles: TF-IDF, codifica√ß√£o direta e de frequ√™ncia.  Usei a codifica√ß√£o de frequ√™ncia, cuja ess√™ncia √© apresentar cada revis√£o como um vetor, cujos elementos s√£o o n√∫mero de ocorr√™ncias de cada palavra do vocabul√°rio.  <i>O NLTK</i> possui seus pr√≥prios classificadores, voc√™ pode us√°-los, mas eles funcionam mais lentamente que os colegas do <i>scikit-learn</i> e t√™m menos configura√ß√µes.  Abaixo est√° o c√≥digo para codifica√ß√£o do <i>NLTK</i> .  No entanto, usarei o modelo Naive Bayes do <i>scikit-learn</i> e codificarei as revis√µes, armazenando os atributos em uma matriz esparsa do <i>SciPy</i> e os r√≥tulos da classe em uma matriz <i>NumPy</i> separada. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  Como no conjunto de dados, as revis√µes com determinadas tags passam uma ap√≥s a outra, ou seja, primeiro todas neutras, depois todas negativas e assim por diante, √© necess√°rio mistur√°-las.  Para fazer isso, voc√™ pode usar a fun√ß√£o <i>aleat√≥ria</i> no <i>scikit-learn</i> .  √â adequado apenas para situa√ß√µes em que sinais e r√≥tulos de classe est√£o em matrizes diferentes, porque permite misturar duas matrizes em un√≠ssono. <br><br><h3>  Modelo de treinamento </h3><br>  Agora resta treinar o modelo e verificar sua precis√£o no grupo de controle.  Como modelo, usaremos o modelo do classificador Naive Bayes.  <i>O Scikit-learn</i> possui tr√™s modelos do Naive Bayes, dependendo da distribui√ß√£o dos dados: bin√°rio, discreto e cont√≠nuo.  Como a distribui√ß√£o de nossos recursos √© discreta, escolhemos <i>MultinomialNB</i> . <br><br>  O classificador bayesiano possui o hiper <i>par√¢metro alfa</i> , respons√°vel por suavizar o modelo.  Naive Bayes calcula as probabilidades de cada revis√£o pertencente a todas as classes, multiplicando as probabilidades condicionais da apar√™ncia de todas as palavras de revis√£o, desde que elas perten√ßam a uma classe espec√≠fica.  Por√©m, se alguma palavra de revis√£o n√£o foi encontrada no conjunto de dados de treinamento, sua probabilidade condicional √© igual a zero, o que anula a probabilidade de a revis√£o pertencer a qualquer classe.  Para evitar isso, por padr√£o, uma unidade √© adicionada a todas as probabilidades de palavras condicionais, ou seja, <i>alfa</i> √© igual a uma.  No entanto, esse valor pode n√£o ser o ideal.  Voc√™ pode tentar selecionar <i>alfa</i> usando a pesquisa em grade e a valida√ß√£o cruzada. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  No meu caso, o cora√ß√£o da grade fornece o valor ideal do hiperpar√¢metro igual a 0 com uma precis√£o de 0,965.  No entanto, esse valor obviamente n√£o ser√° o ideal para o conjunto de dados de controle, pois haver√° um grande n√∫mero de palavras que n√£o foram encontradas anteriormente no conjunto de treinamento.  Para um conjunto de dados de refer√™ncia, este modelo tem uma precis√£o de 0,598.  No entanto, se voc√™ aumentar <i>alfa</i> para 0,1, a precis√£o nos dados de treinamento cair√° para 0,82, e nos dados de controle, aumentar√° para 0,62.  Provavelmente, em um conjunto de dados maior, a diferen√ßa ser√° mais significativa. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Conclus√£o </h3><br>  Sup√µe-se que o modelo deva ser usado para prever revis√µes cujas palavras n√£o foram usadas para formar um vocabul√°rio.  Portanto, a qualidade do modelo pode ser avaliada por sua precis√£o na parte de controle dos dados, que √© 0,62.  Isso √© quase duas vezes melhor do que adivinhar, mas a precis√£o ainda √© bastante baixa. <br><br>  De acordo com o relat√≥rio de classifica√ß√£o, √© claro que o modelo apresenta o pior desempenho com avalia√ß√µes de cor neutra (precis√£o 0,47 versus 0,68 para positivo e 0,76 para negativo).  De fato, as revis√µes neutras cont√™m palavras que s√£o caracter√≠sticas das cr√≠ticas positivas e negativas.  Provavelmente, a precis√£o do modelo pode ser aprimorada aumentando o volume do conjunto de dados, j√° que o conjunto de tr√™s mil√©simos de dados √© bastante modesto.  Al√©m disso, seria poss√≠vel reduzir o problema a uma classifica√ß√£o bin√°ria de an√°lises em positiva e negativa, o que tamb√©m aumentaria a precis√£o. <br><br>  Obrigado pela leitura. <br><br>  PS Se voc√™ quiser praticar, meu conjunto de dados pode ser baixado abaixo do link. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Link para o conjunto de dados</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt467081/">https://habr.com/ru/post/pt467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt467061/index.html">Linha da Babil√¥nia: 5 quest√µes de seguran√ßa no setor de constru√ß√£o</a></li>
<li><a href="../pt467063/index.html">Monitoramento de combust√≠vel para geradores a diesel de data center - como fazer e por que √© t√£o importante?</a></li>
<li><a href="../pt467065/index.html">Arquivo de problemas das olimp√≠adas em f√≠sica para crian√ßas em idade escolar</a></li>
<li><a href="../pt467073/index.html">‚ÄúNo Ocidente, n√£o h√° diretores de arte com menos de 40 anos. Conosco, pode ser de at√© 30 ". Como √© ser designer em TI</a></li>
<li><a href="../pt467079/index.html">Carrossel de formigas CSS e Javascript</a></li>
<li><a href="../pt467083/index.html">Como a estranha instru√ß√£o popcount √© usada nos processadores modernos</a></li>
<li><a href="../pt467085/index.html">Descompilar C, C ++ e DotNet s√£o os princ√≠pios b√°sicos da revers√£o. Resolvendo problemas para reverter com r0ot-mi. Parte 1</a></li>
<li><a href="../pt467087/index.html">Como me preparei e passei na Certifica√ß√£o SQL do Banco de Dados Oracle (1Z0-071)</a></li>
<li><a href="../pt467089/index.html">Patched Exim - fa√ßa o patch novamente. Execu√ß√£o fresca de comando remoto no Exim 4.92 em uma solicita√ß√£o</a></li>
<li><a href="../pt467091/index.html">Uma r√°pida introdu√ß√£o ao Svelte do ponto de vista do desenvolvedor Angular</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>