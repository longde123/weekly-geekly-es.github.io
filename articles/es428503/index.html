<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüè≠ üë®üèΩ‚Äçüî¨ üëßüèΩ Descripci√≥n general del m√©todo de vector de referencia de algoritmo de aprendizaje autom√°tico (SVM) üëçüèø üåª ‚ù§Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pr√≥logo 


 En este art√≠culo, exploraremos varios aspectos de SVM: 



- componente te√≥rico de SVM; 
- c√≥mo funciona el algoritmo en muestras que no s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Descripci√≥n general del m√©todo de vector de referencia de algoritmo de aprendizaje autom√°tico (SVM)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428503/"><h2>  Pr√≥logo </h2><br><img src="https://habrastorage.org/webt/eg/fk/rq/egfkrqshjcqqzkc7ktnnyyx9iuy.jpeg"><br><br>  En este art√≠culo, exploraremos varios aspectos de SVM: <br><br><ul><li>  componente te√≥rico de SVM; </li><li>  c√≥mo funciona el algoritmo en muestras que no se pueden dividir en clases linealmente; </li><li>  Ejemplo de Python e implementaci√≥n del algoritmo en la biblioteca SciKit Learn. </li></ul><a name="habracut"></a><br>  En los siguientes art√≠culos, tratar√© de hablar sobre el componente matem√°tico de este algoritmo. <br><br>  Como sabe, las tareas de aprendizaje autom√°tico se dividen en dos categor√≠as principales: clasificaci√≥n y regresi√≥n.  Dependiendo de a cu√°l de estas tareas nos enfrentamos y qu√© conjunto de datos tenemos para esta tarea, elegimos qu√© algoritmo usar. <br><br>  El m√©todo de m√°quinas de vectores de soporte o SVM (del ingl√©s Support Vector Machines) es un algoritmo lineal utilizado en problemas de clasificaci√≥n y regresi√≥n.  Este algoritmo se usa ampliamente en la pr√°ctica y puede resolver problemas tanto lineales como no lineales.  La esencia de las "m√°quinas" de los vectores de soporte es simple: el algoritmo crea una l√≠nea o hiperplano que divide los datos en clases. <br><br><h4>  Teor√≠a </h4><br>  La tarea principal del algoritmo es encontrar la l√≠nea o hiperplano m√°s correcta, dividiendo los datos en dos clases.  SVM es un algoritmo que recibe datos en la entrada y devuelve dicha l√≠nea divisoria. <br><br>  Considere el siguiente ejemplo.  Supongamos que tenemos un conjunto de datos y queremos clasificar y separar los cuadrados rojos de los c√≠rculos azules (digamos positivo y negativo).  El objetivo principal en esta tarea ser√° encontrar la l√≠nea "ideal" que separar√° estas dos clases. <br><br><img src="https://habrastorage.org/webt/lj/e4/oy/lje4oybbp_pbe_slxkvhm6yqaoy.png"><br><br>  Encuentre la l√≠nea perfecta, o hiperplano, que divide el conjunto de datos en clases azul y rojo. <br><br>  A primera vista, no es tan dif√≠cil, ¬øverdad? <br><br>  Pero, como puede ver, no hay una l√≠nea √∫nica que resuelva ese problema.  Podemos recoger un n√∫mero infinito de l√≠neas que pueden separar estas dos clases.  ¬øC√≥mo exactamente SVM encuentra la l√≠nea "ideal" y qu√© es "ideal" en su comprensi√≥n? <br><br>  Eche un vistazo al siguiente ejemplo y piense cu√°l de las dos l√≠neas (amarilla o verde) separa mejor las dos clases y se ajusta a la descripci√≥n de "ideal". <br><br><img src="https://habrastorage.org/webt/w4/_f/kz/w4_fkz5krspejxz1o73l1yjnidy.png"><br><br>  ¬øQu√© l√≠nea separa mejor el conjunto de datos en su opini√≥n? <br><br>  Si elige la l√≠nea amarilla, lo felicito: esta es la l√≠nea que elegir√≠a el algoritmo.  En este ejemplo, podemos entender intuitivamente que la l√≠nea amarilla se separa y, en consecuencia, clasifica las dos clases mejor que la verde. <br><br>  En el caso de la l√≠nea verde, se encuentra demasiado cerca de la clase roja.  A pesar de que clasific√≥ correctamente todos los objetos del conjunto de datos actual, dicha l√≠nea no se generalizar√°, no se comportar√° tan bien con un conjunto de datos desconocido.  La tarea de encontrar una separaci√≥n generalizada de dos clases es una de las tareas principales en el aprendizaje autom√°tico. <br><br><h4>  C√≥mo SVM encuentra la mejor l√≠nea </h4><br>  El algoritmo SVM est√° dise√±ado de tal manera que busca puntos en el gr√°fico que se encuentran directamente en la l√≠nea de separaci√≥n m√°s cercana.  Estos puntos se denominan vectores de referencia.  Luego, el algoritmo calcula la distancia entre los vectores de soporte y el plano divisorio.  Esta es la distancia llamada brecha.  El objetivo principal del algoritmo es maximizar la distancia de separaci√≥n.  Se considera que el mejor hiperplano es un hiperplano para el cual esta brecha es lo m√°s grande posible. <br><br><img src="https://habrastorage.org/webt/ps/iy/he/psiyhexemtrhnqukbvmvaqzafvi.png"><br><br>  Bastante simple, ¬øverdad?  Considere el siguiente ejemplo, con un conjunto de datos m√°s complejo que no se puede dividir linealmente. <br><br><img src="https://habrastorage.org/webt/jh/5v/bx/jh5vbxwn7vfzyzeuxibxpleejyk.png"><br><br>  Obviamente, este conjunto de datos no se puede dividir linealmente.  No podemos dibujar una l√≠nea recta que clasifique estos datos.  Pero, este conjunto de datos se puede dividir linealmente agregando una dimensi√≥n adicional, que llamaremos eje Z. Imagine que las coordenadas en el eje Z est√°n reguladas por la siguiente restricci√≥n: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">z</span><span class="MJXp-mo" id="MJXp-Span-3" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">x</span><span class="MJXp-mrow" id="MJXp-Span-5"><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span><span class="MJXp-mo" id="MJXp-Span-7" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">y</span><span class="MJXp-mrow" id="MJXp-Span-9"><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> z = x¬≤ + y¬≤ </script></p><br>  Por lo tanto, la ordenada Z se representa desde el cuadrado de la distancia del punto hasta el comienzo del eje. <br>  A continuaci√≥n se muestra una visualizaci√≥n del mismo conjunto de datos en el eje Z. <br><br><img src="https://habrastorage.org/webt/vd/nj/ce/vdnjce7p5csbhfp12tkaj6t-4-s.png"><br><br>  Ahora los datos se pueden dividir linealmente.  Suponga que la l√≠nea magenta que separa los datos z = k, donde k es una constante.  Si <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-11"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12">z</span><span class="MJXp-mo" id="MJXp-Span-13" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">x</span><span class="MJXp-mrow" id="MJXp-Span-15"><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span><span class="MJXp-mo" id="MJXp-Span-17" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">y</span><span class="MJXp-mrow" id="MJXp-Span-19"><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> z = x¬≤ + y¬≤ </script></p>  entonces <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-21"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22">k</span><span class="MJXp-mo" id="MJXp-Span-23" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24">x</span><span class="MJXp-mrow" id="MJXp-Span-25"><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span><span class="MJXp-mo" id="MJXp-Span-27" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28">y</span><span class="MJXp-mrow" id="MJXp-Span-29"><span class="MJXp-mo" id="MJXp-Span-30" style="margin-left: 0em; margin-right: 0em;">¬≤</span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.84ex" height="2.419ex" viewBox="0 -780.1 4667.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-6B" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-3D" x="799" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-78" x="1855" y="0"></use><g transform="translate(2428,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMAIN-2B" x="2909" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/428503/&amp;usg=ALkJrhi0bQJQhvmB1vyTUGASjg7x7o5N8A#MJMATHI-79" x="3910" y="0"></use><g transform="translate(4407,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> k = x¬≤ + y¬≤ </script></p>  - f√≥rmula circular.  Por lo tanto, podemos proyectar nuestro divisor lineal, de vuelta al n√∫mero original de dimensiones de muestra, utilizando esta transformaci√≥n. <br><br><img src="https://habrastorage.org/webt/we/nu/zn/wenuznqe4e7n4isscmunomrwzfw.png"><br><br>  Como resultado, podemos clasificar un conjunto de datos no lineal al agregarle una dimensi√≥n adicional y luego devolverlo a su forma original mediante transformaci√≥n matem√°tica.  Sin embargo, no con todos los conjuntos de datos, es igual de f√°cil acelerar tal transformaci√≥n.  Afortunadamente, la implementaci√≥n de este algoritmo en la biblioteca sklearn nos resuelve este problema. <br><br><h4>  Hiperplano </h4><br>  Ahora que nos hemos familiarizado con la l√≥gica del algoritmo, pasamos a la definici√≥n formal de un hiperplano <br><br>  Un hiperplano es un subplano n-1 dimensional en un espacio euclidiano n-dimensional que divide el espacio en dos partes separadas. <br><br>  Por ejemplo, imagine que nuestra l√≠nea se representa como un espacio euclidiano unidimensional (es decir, nuestro conjunto de datos se encuentra en una l√≠nea recta).  Seleccione un punto en esta l√≠nea.  Este punto dividir√° el conjunto de datos, en nuestro caso la l√≠nea, en dos partes.  La l√≠nea tiene una medida y el punto tiene 0 medidas.  Por lo tanto, un punto es un hiperplano de una l√≠nea. <br><br>  Para el conjunto de datos bidimensionales que conocimos anteriormente, la l√≠nea divisoria era el mismo hiperplano.  En pocas palabras, para un espacio n-dimensional hay un hiperplano n-1 dimensional que divide este espacio en dos partes. <br><br>  C√ìDIGO <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np X = np.array([[<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]) y = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])</code> </pre> <br>  Los puntos se representan como una matriz de X, y las clases a las que pertenecen como una matriz de y. <br>  Ahora entrenaremos a nuestro modelo con esta muestra.  Para este ejemplo, configuro el par√°metro lineal del "n√∫cleo" del clasificador (n√∫cleo). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SVC clf = SVC(kernel=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>) clf = SVC.fit(X, y)</code> </pre><br>  Predicci√≥n de clase de un nuevo objeto. <br><br><pre> <code class="python hljs">prediction = clf.predict([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>]])</code> </pre> <br><h4>  Ajuste de par√°metros </h4><br>  Los par√°metros son los argumentos que pasa al crear el clasificador.  A continuaci√≥n, proporcion√© algunas de las configuraciones SVM personalizadas m√°s importantes: <br><br>  <b>"C"</b> <br><br>  Este par√°metro ayuda a ajustar esa l√≠nea fina entre la "suavidad" y la precisi√≥n de la clasificaci√≥n de los objetos en la muestra de entrenamiento.  Cuanto mayor sea el valor de "C", m√°s objetos en el conjunto de entrenamiento se clasificar√°n correctamente. <br><br><img src="https://habrastorage.org/webt/rq/01/6s/rq016soikp4qfockp86li66s5mq.png"><br><br>  En este ejemplo, hay varios umbrales de decisi√≥n que podemos definir para esta muestra en particular.  Preste atenci√≥n al umbral de decisi√≥n directa (presentado en el cuadro como una l√≠nea verde).  Es bastante simple, y por esta raz√≥n, varios objetos se clasificaron incorrectamente.  Estos puntos que se han clasificado incorrectamente se denominan valores at√≠picos en los datos. <br><br>  Tambi√©n podemos ajustar los par√°metros de tal manera que al final obtengamos una l√≠nea m√°s curva (umbral de decisi√≥n azul claro), que clasificar√° absolutamente todos los datos de muestra de entrenamiento correctamente.  Por supuesto, en este caso, las posibilidades de que nuestro modelo pueda generalizar y mostrar resultados igualmente buenos en nuevos datos son catastr√≥ficamente peque√±as.  Por lo tanto, si est√° tratando de lograr la precisi√≥n al entrenar el modelo, debe apuntar a algo m√°s uniforme, directo.  Cuanto mayor sea el n√∫mero "C", m√°s enredado estar√° el hiperplano en su modelo, pero mayor ser√° el n√∫mero de objetos clasificados correctamente en el conjunto de entrenamiento.  Por lo tanto, es importante "torcer" los par√°metros del modelo para un conjunto de datos espec√≠fico con el fin de evitar el reentrenamiento pero, al mismo tiempo, lograr una alta precisi√≥n. <br><br>  <b>Gamma</b> <br><br>  En la documentaci√≥n oficial, la biblioteca SciKit Learn dice que la gamma determina hasta qu√© punto cada uno de los elementos del conjunto de datos influye en la determinaci√≥n de la "l√≠nea ideal".  Cuanto m√°s baja sea la gama, m√°s elementos, incluso aquellos que est√°n lo suficientemente lejos de la l√≠nea divisoria, participan en el proceso de elegir esta misma l√≠nea.  Si la gamma es alta, entonces el algoritmo "depender√°" solo de aquellos elementos que est√©n m√°s cerca de la l√≠nea misma. <br>  Si el nivel gamma se establece demasiado alto, solo los elementos m√°s cercanos a la l√≠nea participar√°n en el proceso de toma de decisiones sobre la ubicaci√≥n de la l√≠nea.  Esto ayudar√° a ignorar los valores at√≠picos en los datos.  El algoritmo SVM est√° dise√±ado para que los puntos ubicados m√°s cerca uno del otro tengan m√°s peso al tomar una decisi√≥n.  Sin embargo, con la configuraci√≥n correcta de "C" y "gamma", se puede lograr un resultado √≥ptimo que construir√° un hiperplano m√°s lineal que ignore los valores at√≠picos y, por lo tanto, sea m√°s generalizable. <br><br><h4>  Conclusi√≥n </h4><br>  Espero sinceramente que este art√≠culo le haya ayudado a comprender la esencia del trabajo de SVM o el M√©todo de vector de referencia.  Espero de usted cualquier comentario y consejo.  En publicaciones posteriores, hablar√© sobre el componente matem√°tico de SVM y los problemas de optimizaci√≥n. <br><br>  Fuentes: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Documentaci√≥n oficial de SVM en SciKit Learn</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://towardsdatascience.com/">Blog de TowardsDataScience</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Siraj Raval: M√°quinas de vectores de soporte</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Introducci√≥n a Machine Learning Udacity SVM: video del curso Gamma</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Wikipedia: SVM</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428503/">https://habr.com/ru/post/es428503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428493/index.html">Git subrepo</a></li>
<li><a href="../es428495/index.html">Como hice un simulador de f√∫tbol durante 13 a√±os</a></li>
<li><a href="../es428497/index.html">Atenuador inal√°mbrico personalizado Noolite SUF-1-300</a></li>
<li><a href="../es428499/index.html">Espeluznantes gigantes azules pueden revelar los secretos de la evoluci√≥n de las estrellas</a></li>
<li><a href="../es428501/index.html">DartUP: la primera conferencia en ruso sobre Dart y Flutter el 1 de diciembre en San Petersburgo</a></li>
<li><a href="../es428505/index.html">Obtener enlaces a audio sin VKApi</a></li>
<li><a href="../es428507/index.html">Escribimos un chat bot para VKontakte en python usando longpoll</a></li>
<li><a href="../es428509/index.html">C√≥mo H&M est√° tratando de salvarse con IA y Big Data</a></li>
<li><a href="../es428511/index.html">Energ√≠a de hidr√≥geno: el comienzo de un largo camino</a></li>
<li><a href="../es428513/index.html">500 punteros l√°ser en un solo lugar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>