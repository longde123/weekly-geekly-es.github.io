<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🏫 🤽🏼 🙅🏿 Membuat templat Dataflow untuk mengalirkan data dari Pub / Sub ke BigQuery berdasarkan GCP menggunakan Apache Beam SDK dan Python 👬 👩🏾‍🤝‍👨🏽 🛀🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Saat ini, saya terlibat dalam tugas streaming (dan mengkonversi) data. Di beberapa kalangan 
 proses seperti itu dikenal sebagai ETL , mis. ekstraksi,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Membuat templat Dataflow untuk mengalirkan data dari Pub / Sub ke BigQuery berdasarkan GCP menggunakan Apache Beam SDK dan Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441892/"><p><img src="https://habrastorage.org/webt/is/qf/7j/isqf7j7patfl6lgirlqfrfbz-k8.png" alt="gambar"></p><br><p>  Saat ini, saya terlibat dalam tugas streaming (dan mengkonversi) data.  Di beberapa kalangan <br>  proses seperti itu dikenal sebagai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ETL</a> , mis.  ekstraksi, konversi, dan pemuatan informasi. </p><br><p>  Seluruh proses mencakup partisipasi layanan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Google Cloud Platform</a> berikut: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pub / Sub</a> -layanan untuk streaming data realtime </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dataflow</a> - layanan untuk mengonversi data (bisa <br>  bekerja baik dalam mode waktu nyata maupun dalam mode batch) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BigQuery</a> - layanan untuk menyimpan data dalam bentuk tabel <br>  (mendukung SQL) </li></ul><a name="habracut"></a><br><h5 id="0-tekuschee-polozhenie-del">  0. Status saat ini </h5><br><p> Saat ini, ada versi streaming yang berfungsi pada layanan di atas, namun pada <br>  Sebagai templat, salah satu yang <a href="">standar digunakan</a> . </p><br><p>  Masalahnya adalah bahwa templat ini menyediakan transfer data 1 banding 1, mis.  pada <br>  di pintu masuk ke Pub / Sub kami memiliki string format JSON, pada output kami memiliki tabel BigQuery dengan bidang, <br>  yang sesuai dengan kunci objek di tingkat atas input JSON. </p><br><h5 id="1-postanovka-zadachi">  1. Pernyataan masalah </h5><br><p> Buat templat Dataflow yang memungkinkan Anda untuk mendapatkan tabel atau tabel di output <br>  sesuai dengan kondisi yang diberikan.  Misalnya, kami ingin membuat tabel terpisah untuk masing-masing <br>  nilai kunci JSON input tertentu.  Perlu untuk mempertimbangkan fakta bahwa beberapa <br>  Objek input JSON dapat berisi JSON bersarang sebagai nilai, mis.  diperlukan <br>  dapat membuat tabel BigQuery dengan bidang tipe <code>RECORD</code> untuk menyimpan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bersarang</a> <br>  data. </p><br><h5 id="2-podgotovka-k-resheniyu">  2. Persiapan untuk keputusan </h5><br><p>  Untuk membuat templat Dataflow, gunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Beam</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SDK</a> , yang, pada gilirannya, <br>  mendukung Java dan Python sebagai bahasa pemrograman.  Saya harus mengatakan itu <br>  hanya versi Python 2.7.x yang didukung, yang sedikit mengejutkan saya.  Apalagi dukungan <br>  Jawa agak lebih luas, karena  untuk Python, misalnya, beberapa fungsi tidak tersedia dan banyak lagi <br>  Daftar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konektor bawaan yang sederhana</a> .  Omong-omong, Anda dapat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menulis</a> konektor Anda sendiri. </p><br><p>  Namun, karena saya tidak terbiasa dengan Java, saya menggunakan Python. </p><br><p>  Sebelum Anda mulai membuat templat, Anda harus memiliki yang berikut ini: </p><br><ol><li>  masukan format JSON dan seharusnya tidak berubah dalam waktu </li><li>  skema atau skema tabel BigQuery di mana data akan dialirkan </li><li>  jumlah tabel di mana aliran data output akan di-stream </li></ol><br><p>  Perhatikan bahwa setelah membuat template dan memulai Pekerjaan Dataflow berdasarkan itu, parameter ini bisa <br>  ubah hanya dengan membuat templat baru. </p><br><p>  Katakan saja beberapa kata tentang batasan ini.  Mereka semua berasal dari kenyataan bahwa tidak ada kemungkinan <br>  buat template dinamis yang bisa mengambil string apa pun sebagai input, parsing <br>  menurut logika internal dan kemudian mengisi tabel yang dibuat secara dinamis dengan dinamis <br>  dibuat oleh sirkuit.  Sangat mungkin bahwa kemungkinan ini ada, tetapi di dalam data <br>  Saya tidak berhasil menerapkan skema seperti itu.  Sejauh yang saya mengerti keseluruhan <br>  pipeline dibangun sebelum menjalankannya dalam runtime dan oleh karena itu tidak ada cara untuk mengubahnya <br>  terbang.  Mungkin seseorang akan membagikan keputusan mereka. </p><br><h5 id="3-reshenie">  3. Keputusan </h5><br><p>  Untuk pemahaman yang lebih lengkap tentang proses, ada baiknya membawa diagram yang disebut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pipa</a> <br>  dari dokumentasi Apache Beam. </p><br><p><img src="https://habrastorage.org/webt/yq/yi/3z/yqyi3zjiwpmjf4i6qp7x4znqv2c.png" alt="gambar"></p><br><p>  Dalam kasus kami (kami akan menggunakan divisi menjadi beberapa tabel): </p><br><ul><li>  input - data berasal dari PubSub di Dataflow Job </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Transform</a> # 1 - data dikonversi dari string ke kamus Python, kita mendapatkan output <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PCollection</a> # 1 </li><li>  Transform # 2 - data diberi tag, untuk pemisahan lebih lanjut menurut tabel terpisah, menjadi <br>  outputnya adalah PCollection # 2 (sebenarnya tuple PCollection) </li><li>  Transform # 3 - data dari PCollection # 2 ditulis ke tabel menggunakan skema <br>  meja </li></ul><br><p>  Dalam proses menulis templat saya sendiri, saya secara aktif terinspirasi oleh contoh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">-</a> contoh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ini</a> . </p><br><div class="spoiler">  <b class="spoiler_title">Kode templat dengan komentar (meninggalkan komentar dengan cara yang sama dari penulis sebelumnya):</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding=utf-8 from __future__ import absolute_import import logging import json import os import apache_beam as beam from apache_beam.pvalue import TaggedOutput from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions from apache_beam.options.pipeline_options import StandardOptions from apache_beam.io.gcp.bigquery import parse_table_schema_from_json #  GCP  gcp_project = '' #  Pub/Sub  topic_name = '' # Pub/Sub    'projects/_GCP_/topics/_' input_topic = 'projects/%s/topics/%s' % (gcp_project, topic_name) #  BigQuery  bq_dataset = 'segment_eu_test' #       schema_dir = './' class TransformToBigQuery(beam.DoFn): #          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #   yield body class TagDataWithReqType(beam.DoFn): #      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element def run(): #       _.json   schema_dir,  #         ()  schema_dct = {} for schema_file in os.listdir(schema_dir): filename_list = schema_file.split('.') if filename_list[-1] == 'json': with open('%s/%s' % (schema_dir, schema_file)) as f: schema_json = f.read() schema_dct[filename_list[0]] = json.dumps({'fields': json.loads(schema_json)}) # We use the save_main_session option because one or more DoFn's in this # workflow rely on global context (eg, a module imported at module level). pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = True pipeline_options.view_as(StandardOptions).streaming = True # Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic) # Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery()) # Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()). with_outputs(*schema_dct.keys(), main='default') # Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), ) # Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), ) result = p.run() result.wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) logger = logging.getLogger(__name__) run()</span></span></code> </pre> </div></div><br><p>  Sekarang kita akan membaca kode dan memberikan penjelasan, tetapi pertama-tama layak untuk mengatakan bahwa utama <br>  kesulitan dalam menulis template ini adalah untuk berpikir dalam hal "aliran data", dan <br>  bukan pesan tertentu.  Juga penting untuk memahami bahwa Pub / Sub beroperasi dengan pesan dan <br>  dari merekalah kami akan menerima informasi untuk menandai aliran. </p><br><pre> <code class="python hljs">pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> pipeline_options.view_as(StandardOptions).streaming = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span></code> </pre> <br><p>  Karena  Konektor Apache Beam Pub / Sub IO hanya digunakan dalam mode streaming yang diperlukan <br>  tambahkan PipelineOptions () (walaupun sebenarnya opsi tidak digunakan); jika tidak, buat templat <br>  jatuh dengan pengecualian.  Harus dikatakan tentang opsi untuk meluncurkan template.  Mereka bisa <br>  statis dan disebut "runtime".  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Berikut ini</a> tautan ke dokumentasi tentang topik ini.  Opsi memungkinkan Anda untuk membuat templat tanpa menentukan parameter di muka, tetapi meneruskannya saat Anda memulai Pekerjaan Dataflow dari templat, tapi saya masih tidak bisa mengimplementasikannya, mungkin karena fakta bahwa konektor ini tidak mendukung <code>RuntimeValueProvider</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic)</span></span></code> </pre> <br><p>  Semuanya jelas dari komentar, kami membaca utas dari topik.  Perlu ditambahkan bahwa Anda dapat mengikuti arus <br>  baik dari topik dan dari berlangganan (berlangganan).  Jika topik ditentukan sebagai input, maka <br>  langganan sementara untuk topik ini akan dibuat secara otomatis.  Sintaksnya juga cantik <br>  jelas, input stream data <code>beam.io.ReadFromPubSub(input_topic)</code> dikirim ke kami <br>  pipa <code>p</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery())</span></span></code> </pre> <br><p>  Di sinilah Transform # 1 terjadi dan input kami dikonversi dari string python ke <br>  python dict, dan dalam output kita mendapatkan PCollection # 1.  <code>&gt;&gt;</code> muncul di sintaks.  Aktif <br>  sebenarnya, teks dalam tanda kutip adalah nama aliran (harus unik), serta komentar, <br>  yang akan ditambahkan ke blok pada grafik di antarmuka web GCP Dataflow.  Mari kita pertimbangkan lebih detail <br>  kelas utama <code>TransformToBigQuery</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TransformToBigQuery</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #  ,      python dict yield body</span></span></code> </pre> <br><p>  Variabel <code>element</code> akan berisi satu pesan dari langganan PubSub.  Seperti yang terlihat dari <br>  kode, dalam kasus kami harus JSON yang valid.  Harus di dalam kelas <br>  metode <code>process</code> didefinisikan ulang, di mana transformasi yang diperlukan harus dilakukan <br>  jalur input untuk mendapatkan output yang sesuai dengan rangkaian <br>  tabel tempat data ini akan dimuat.  Karena  aliran kami dalam hal ini adalah <br>  terus menerus, tidak <code>unbounded</code> dalam hal Apache Beam, Anda harus mengembalikannya menggunakan <br>  <code>yield</code> , bukan <code>return</code> , seperti untuk aliran data akhir.  Dalam hal aliran akhir, Anda bisa <br>  (dan perlu) tambahan mengkonfigurasi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><code>windowing</code></a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><code>triggers</code></a> </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()).with_outputs(*schema_dct.keys(), main='default')</span></span></code> </pre> <br><p>  Kode ini mengarahkan PCollection # 1 ke Transform # 2 di mana penandaan akan dilakukan <br>  (pemisahan) dari aliran data.  Dalam <code>schema_dct</code> variabel <code>schema_dct</code> dalam hal ini, kamus, di mana kuncinya adalah nama file skema tanpa ekstensi, ini akan menjadi tag, dan nilainya JSON valid dari skema <br>  Tabel BigQuery untuk tag ini.  Perlu dicatat bahwa skema tersebut harus ditransmisikan secara tepat kepada <br>  lihat <code>{'fields': }</code> mana <code></code> adalah skema dari tabel BigQuery dalam bentuk JSON (Anda bisa <br>  ekspor dari antarmuka web). </p><br><p>  <code>main='default'</code> adalah nama dari tag thread yang akan mereka tuju <br>  Semua pesan yang tidak tunduk pada kondisi pemberian tag.  Pertimbangkan kelasnya <br>  <code>TagDataWithReqType</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TagDataWithReqType</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element</span></span></code> </pre> <br><p>  Seperti yang Anda lihat, kelas <code>process</code> juga diganti di sini.  Variabel <code>types</code> berisi nama <br>  tag dan mereka harus mencocokkan nomor dan nama dengan nomor dan nama kunci kamus <br>  <code>schema_dct</code> .  Meskipun metode <code>process</code> memiliki kemampuan untuk menerima argumen, saya tidak pernah <br>  Saya bisa melewati mereka.  Saya belum menemukan alasannya. </p><br><p>  Pada output, kita mendapatkan tupel utas dalam jumlah tag, yaitu jumlah kita <br>  tag yang telah ditentukan + utas default yang gagal untuk menandai. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), )</span></span></code> </pre> <br><p>  Transform # ... (pada kenyataannya, ini bukan pada diagram, ini adalah "cabang") - kami menulis aliran default <br>  ke tabel default. </p><br><p>  <code>tagged_stream.default</code> - aliran dengan tag <code>default</code> diambil, sintaks alternatif adalah <code>tagged_stream['default']</code> </p><br><p>  <code>schema=parse_table_schema_from_json(schema_dct.get('default'))</code> - di sini skema didefinisikan <br>  meja.  Harap perhatikan bahwa file <code>default.json</code> dengan skema tabel BigQuery yang valid <br>  harus ada di <code>schema_dir = './'</code> . </p><br><p>  Aliran akan pergi ke tabel yang disebut <code>default</code> . </p><br><p>  Jika tabel dengan nama ini (dalam dataset yang diberikan proyek ini) tidak ada, maka itu <br>  akan secara otomatis dibuat dari skema berkat pengaturan default <br> <code>create_disposition=BigQueryDisposition.CREATE_IF_NEEDED</code> </p> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), )</span></span></code> </pre> <br><p>  Transform # 3, semuanya harus jelas bagi mereka yang membaca artikel dari awal dan sendiri <br>  sintaksis python  Kami memisahkan stream tuple dengan loop dan menulis setiap stream dengan tabelnya sendiri <br>  rencananya.  Harus diingat bahwa nama aliran harus unik - <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> . </p><br><p>  Sekarang harus jelas bagaimana ini bekerja dan Anda dapat membuat templat.  Untuk ini Anda perlu <br>  jalankan di konsol (jangan lupa untuk mengaktifkan venv jika tersedia) atau dari IDE: </p><br><pre> <code class="bash hljs">python _.py / --runner DataflowRunner / --project dreamdata-test / --staging_location gs://STORAGE_NAME/STAGING_DIR / --temp_location gs://STORAGE_NAME/TEMP_DIR / --template_location gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> </pre> <br><p>  Pada saat yang sama, akses ke Akun Google harus diatur, misalnya, melalui ekspor <br>  <code>GOOGLE_APPLICATION_CREDENTIALS</code> lingkungan <code>GOOGLE_APPLICATION_CREDENTIALS</code> atau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">cara</a> lain. </p><br><p>  Beberapa kata tentang - <code>--runner</code> .  Dalam hal ini, <code>DataflowRunner</code> mengatakan bahwa kode ini <br>  akan berjalan sebagai templat untuk Pekerjaan Dataflow.  Masih mungkin untuk menentukan <br>  <code>DirectRunner</code> , ini akan digunakan secara default jika tidak ada opsi - <code>--runner</code> dan kode <br>  akan bekerja sebagai Pekerjaan Dataflow, tetapi secara lokal, yang sangat nyaman untuk debugging. </p><br><p>  Jika tidak ada kesalahan terjadi, maka <code>gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> akan menjadi <br>  template yang dibuat.  Layak dikatakan bahwa dalam <code>gs://STORAGE_NAME/STAGING_DIR</code> juga akan ditulis <br>  file layanan yang diperlukan untuk keberhasilan operasi Datafow Job yang dibuat berdasarkan <br>  templat dan Anda tidak perlu menghapusnya. </p><br><p>  Selanjutnya, Anda perlu membuat Pekerjaan Dataflow menggunakan templat ini, secara manual atau oleh siapa saja <br>  dengan cara lain (CI misalnya). </p><br><h5 id="4-vyvody">  4. Kesimpulan </h5><br><p>  Jadi, kami berhasil melakukan streaming aliran dari PubSub ke BigQuery menggunakan <br>  diperlukan transformasi data untuk tujuan penyimpanan lebih lanjut, transformasi dan <br>  penggunaan data. </p><br><h2 id="osnovnye-ssylki">  Tautan utama </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Apache Beam SDK</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwjHvcGT5svgAhV7wsQBHSDWDEoQFjAAegQIABAC%26url%3D">Google dataflow</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Google bigquery</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Artikel James Moore di media</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Contoh kode python untuk Apache Beam</a> </li></ul><br><p>  Dalam artikel ini, ketidakakuratan dan bahkan kesalahan mungkin terjadi, saya akan berterima kasih atas konstruktifnya <br>  kritik.  Pada akhirnya, saya ingin menambahkan bahwa pada kenyataannya, tidak semua digunakan di sini <br>  fitur dari Apache Beam SDK, tetapi bukan itu tujuannya. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id441892/">https://habr.com/ru/post/id441892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id441878/index.html">Anda seperti yang Anda inginkan, tetapi saya lakukan</a></li>
<li><a href="../id441882/index.html">VMware NSX untuk yang terkecil. Bagian 3. Konfigurasi DHCP</a></li>
<li><a href="../id441886/index.html">Selama 12 tahun terakhir, saya tidak pernah menunjukkan resume</a></li>
<li><a href="../id441888/index.html">SIP dari Megaphone dengan tarif rumah</a></li>
<li><a href="../id441890/index.html">Semua yang perlu Anda ketahui tentang Ekstensi Aplikasi iOS</a></li>
<li><a href="../id441896/index.html">Pelajari Taktik Adversarial, Teknik & Pengetahuan Umum (ATT @ CK). Taktik Perusahaan. Bagian 9</a></li>
<li><a href="../id441898/index.html">Sketch + Node.js: menghasilkan ikon untuk banyak platform dan merek</a></li>
<li><a href="../id441900/index.html">Satya Nadella berbicara tentang kerja sama dengan Pentagon</a></li>
<li><a href="../id441902/index.html">Bagaimana teknologi menciptakan realitas baru</a></li>
<li><a href="../id441904/index.html">Memasang layar IPS di Thinkpad T430S</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>