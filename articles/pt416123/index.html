<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ûüèΩ üëÄ üë©üèª‚Äçüç≥ Reconhecimento de mercadorias nas prateleiras usando redes neurais usando as tecnologias Keras e Tensorflow Object Detection API üï∫üèΩ üé• ‚óÄÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No artigo, falaremos sobre o uso de redes neurais convolucionais para resolver uma tarefa pr√°tica de neg√≥cios de restaurar um realograma a partir de f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconhecimento de mercadorias nas prateleiras usando redes neurais usando as tecnologias Keras e Tensorflow Object Detection API</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/416123/">  No artigo, falaremos sobre o uso de redes neurais convolucionais para resolver uma tarefa pr√°tica de neg√≥cios de restaurar um realograma a partir de fotografias de prateleiras com mercadorias.  Usando a API de detec√ß√£o de objetos do Tensorflow, treinaremos o modelo de pesquisa / localiza√ß√£o.  Melhoraremos a qualidade da pesquisa de pequenos produtos em fotografias de alta resolu√ß√£o usando uma janela flutuante e um algoritmo de supress√£o n√£o m√°ximo.  Na Keras, estamos implementando um classificador de produtos por marca.  Paralelamente, compararemos abordagens e resultados com decis√µes de quatro anos atr√°s.  Todos os dados usados ‚Äã‚Äãno artigo est√£o dispon√≠veis para download e o c√≥digo totalmente funcional est√° no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GitHub</a> e foi desenvolvido como um tutorial. <br><br><img src="https://habrastorage.org/webt/aw/27/ar/aw27argaeseqsgos5cbpwdwt89s.jpeg"><br><a name="habracut"></a><br><h3>  1. Introdu√ß√£o </h3><br>  O que √© um planograma?  O diagrama de layout da exibi√ß√£o de mercadorias no equipamento comercial de concreto da loja. <br><br>  O que √© um realograma?  O layout das mercadorias em um equipamento comercial espec√≠fico existente na loja aqui e agora. <br><br>  Planograma - como deveria, realograma - o que temos. <br><br><img src="https://habrastorage.org/webt/ym/oo/ew/ymooewlirqpaosatarrenno8caw.jpeg"><br><br>  At√© agora, em muitas lojas, gerenciar o restante dos produtos em prateleiras, prateleiras, balc√µes e prateleiras √© exclusivamente trabalho manual.  Milhares de funcion√°rios verificam a disponibilidade dos produtos manualmente, calculam o saldo, verificam a localiza√ß√£o com os requisitos.  √â caro, e os erros s√£o muito prov√°veis.  A exibi√ß√£o incorreta ou a falta de mercadorias levam a vendas mais baixas. <br><br>  Al√©m disso, muitos fabricantes celebram acordos com varejistas para exibir seus produtos.  E como existem muitos fabricantes, entre eles come√ßa a luta pelo melhor lugar na prateleira.  Todo mundo quer que seu produto esteja no centro oposto aos olhos do comprador e ocupe a maior √°rea poss√≠vel.  H√° necessidade de auditoria cont√≠nua. <br><br>  Milhares de comerciantes se deslocam de loja em loja para garantir que os produtos de sua empresa estejam na prateleira e apresentados de acordo com o contrato.  √Äs vezes s√£o pregui√ßosos: √© muito mais agrad√°vel compilar um relat√≥rio sem sair de casa do que ir a um ponto de venda.  √â necess√°rio uma auditoria permanente dos auditores. <br><br>  Naturalmente, a tarefa de automa√ß√£o e simplifica√ß√£o desse processo foi resolvida por um longo tempo.  Uma das partes mais dif√≠ceis foi o processamento de imagens: encontrar e reconhecer produtos.  E apenas recentemente, essa tarefa foi simplificada tanto que, para um caso espec√≠fico e de forma simplificada, sua solu√ß√£o completa pode ser descrita em um artigo.  √â isso que faremos. <br><br>  O artigo cont√©m um m√≠nimo de c√≥digo (apenas nos casos em que o c√≥digo √© mais claro que o texto).  A solu√ß√£o completa est√° dispon√≠vel como um tutorial ilustrado em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">notebooks jupyter</a> .  O artigo n√£o cont√©m uma descri√ß√£o da arquitetura das redes neurais, os princ√≠pios dos neur√¥nios, as f√≥rmulas matem√°ticas.  No artigo, n√≥s os usamos como uma ferramenta de engenharia, sem entrar muito nos detalhes do seu dispositivo. <br><br><h3>  Dados e Abordagem </h3><br>  Como em qualquer abordagem orientada a dados, as solu√ß√µes de redes neurais exigem dados.  Voc√™ tamb√©m pode mont√°-los manualmente: para capturar v√°rias centenas de contadores e marc√°-los usando, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LabelImg</a> .  Voc√™ pode solicitar a marca√ß√£o, por exemplo, no Yandex.Tolok. <br><br><img src="https://habrastorage.org/webt/ji/he/lv/jihelvxh9vkmixjzsxink1nknya.jpeg"><br><br>  N√£o podemos divulgar os detalhes de um projeto real; portanto, explicaremos a tecnologia em dados abertos.  Fazer compras e tirar fotos era muito pregui√ßoso (e n√£o ter√≠amos sido compreendidos por l√°), e o desejo de fazer a marca√ß√£o das fotos encontradas na Internet por n√≥s mesmos terminou ap√≥s o cent√©simo objeto classificado.  Felizmente, por acaso me deparei com o arquivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Grocery Dataset</a> . <br><br>  Em 2014, os funcion√°rios da Idea Teknoloji, Istambul, Turquia carregaram 354 fotos de 40 lojas feitas em 4 c√¢meras.  Em cada uma dessas fotografias, destacaram com ret√¢ngulos um total de v√°rios milhares de objetos, alguns dos quais foram classificados em 10 categorias. <br><br>  S√£o fotos de ma√ßos de cigarro.  N√£o promovemos ou promovemos o tabagismo.  Simplesmente n√£o havia nada mais neutro.  Prometemos que em todo o artigo, onde a situa√ß√£o permitir, usaremos fotografias de gatos. <br><br><img src="https://habrastorage.org/webt/04/8x/ek/048xekrylrnspiwd7vefbux_kgu.jpeg"><br><br>  Al√©m das fotos marcadas das prateleiras, eles escreveram um artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Em dire√ß√£o ao reconhecimento de produtos de varejo nas prateleiras dos supermercados,</a> com uma solu√ß√£o para o problema de localiza√ß√£o e classifica√ß√£o.  Isso definiu um tipo de ponto de refer√™ncia: nossa solu√ß√£o, usando novas abordagens, deve se tornar mais simples e precisa, caso contr√°rio, n√£o √© interessante.  Sua abordagem consiste em uma combina√ß√£o de algoritmos: <br><br><img src="https://habrastorage.org/webt/j5/p4/t_/j5p4t_ul9ungv_luvlzxa5aa1gi.jpeg"><br><br>  Recentemente, as redes neurais convolucionais (CNNs) revolucionaram o campo da vis√£o computacional e mudaram completamente a abordagem para resolver esses problemas.  Nos √∫ltimos anos, essas tecnologias tornaram-se dispon√≠veis para uma ampla gama de desenvolvedores, e APIs de alto n√≠vel como Keras reduziram significativamente seu limite de entrada.  Agora, quase todo desenvolvedor pode usar todo o poder das redes neurais convolucionais ap√≥s apenas alguns dias de namoro.  O artigo descreve o uso dessas tecnologias usando um exemplo, mostrando como uma cascata inteira de algoritmos pode ser facilmente substitu√≠da por apenas duas redes neurais sem perda de precis√£o. <br><br>  Vamos resolver o problema em etapas: <br><br><ul><li>  Prepara√ß√£o de dados.  N√≥s extra√≠mos os arquivos e o transformamos em uma visualiza√ß√£o conveniente para o trabalho. </li><li>  Classifica√ß√£o da marca.  Resolvemos o problema de classifica√ß√£o usando uma rede neural. </li><li>  Pesquise produtos na foto.  Treinamos a rede neural para procurar mercadorias. </li><li>  Pesquisa de implementa√ß√£o.  Melhoraremos a qualidade da detec√ß√£o usando uma janela flutuante e um algoritmo para suprimir valores n√£o m√°ximos. </li><li>  Conclus√£o  Explique brevemente por que a vida real √© muito mais complicada do que este exemplo. </li></ul><br><h3>  Tecnologia </h3><br>  As principais tecnologias que usaremos: Tensorflow, Keras, API de detec√ß√£o de objetos de Tensorflow, OpenCV.  Embora o Windows e o Mac OS sejam adequados para trabalhar com o Tensorflow, ainda recomendamos o uso do Ubuntu.  Mesmo que voc√™ nunca tenha trabalhado com este sistema operacional antes, us√°-lo economizar√° muito tempo.  Instalar o Tensorflow para funcionar com a GPU √© um t√≥pico que merece um artigo separado.  Felizmente, esses artigos j√° existem.  Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Instalando o TensorFlow no Ubuntu 16.04 com uma GPU Nvidia</a> .  Algumas instru√ß√µes podem estar desatualizadas. <br><br>  <b>Etapa 1. Preparando os dados ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link do github</a> )</b> <br><br>  Esta etapa, via de regra, leva muito mais tempo do que a simula√ß√£o em si.  Felizmente, usamos dados prontos, que convertemos para o formul√°rio que precisamos. <br><br>  Voc√™ pode baixar e descompactar desta maneira: <br><br><pre><code class="bash hljs">wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz tar -xvzf GroceryDataset_part1.tar.gz tar -xvzf GroceryDataset_part2.tar.gz</code> </pre> <br>  Temos a seguinte estrutura de pastas: <br><br><img src="https://habrastorage.org/webt/n1/yi/8n/n1yi8n3faxzmxia70bxsee-b69u.jpeg"><br><br>  Usaremos as informa√ß√µes dos diret√≥rios ShelfImages e ProductImagesFromShelves. <br>  ShelfImages cont√©m imagens das pr√≥prias prateleiras.  No nome, o identificador do rack com o identificador da imagem √© codificado.  Pode haver v√°rias fotos de um rack.  Por exemplo, uma fotografia na sua totalidade e 5 fotografias em partes com interse√ß√µes. <br><br>  Arquivo C1_P01_N1_S2_2.JPG (rack C1_P01, instant√¢neo N1_S2_2): <br><br><img src="https://habrastorage.org/webt/nv/jd/or/nvjdorlqwj1qc7asuzktk7dqccs.jpeg"><br><br>  Examinamos todos os arquivos e coletamos informa√ß√µes no quadro de dados do pandas photos_df: <br><br><img src="https://habrastorage.org/webt/us/zq/zq/uszqzqw3haortnmdvscxp0sq1cq.png"><br>  ProductImagesFromShelves cont√©m fotos recortadas de mercadorias das prateleiras em 11 subdiret√≥rios: 0 - n√£o classificado, 1 - Marlboro, 2 - Kent, etc.  Para n√£o divulg√°-los, usaremos apenas n√∫meros de categoria sem especificar nomes.  Os arquivos nos nomes cont√™m informa√ß√µes sobre o rack, a posi√ß√£o e o tamanho da embalagem. <br><br>  Arquivo C1_P01_N1_S3_1.JPG_1276_1828_276_448.png do diret√≥rio 1 (categoria 1, rack C1_P01, imagem N1_S3_1, coordenadas do canto superior esquerdo (1276, 1828), largura 276, altura 448): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/-x/pc/f0-xpc4nqkymbj-ycs1my_sldhi.jpeg"></div><br>  N√£o precisamos das fotografias dos pacotes individuais (iremos cort√°-los das fotos das prateleiras) e coletamos informa√ß√µes sobre sua categoria e posi√ß√£o no quadro de dados do pandas products_df: <br><br><img src="https://habrastorage.org/webt/0x/nh/rv/0xnhrvv5nibludwh7svn54hj_ba.png"><br>  Na mesma etapa, dividimos todas as nossas informa√ß√µes em duas se√ß√µes: treinamento para treinamento e valida√ß√£o para o monitoramento do treinamento.  Claro, isso n√£o vale a pena fazer em projetos reais.  E tamb√©m n√£o confie em quem faz isso.  Voc√™ deve pelo menos alocar outro teste para o teste final.  Mas mesmo com essa abordagem n√£o muito honesta, √© importante n√£o nos enganarmos muito. <br><br>  Como j√° observamos, pode haver v√°rias fotos de um rack.  Consequentemente, o mesmo pacote pode cair em v√°rias fotos.  Portanto, recomendamos que voc√™ divida n√£o por fotos e, mais ainda, por pacotes, mas por racks.  Isso √© necess√°rio para que n√£o aconte√ßa que o mesmo objeto, retirado de √¢ngulos diferentes, acabe no trem e na valida√ß√£o. <br><br>  Fazemos uma divis√£o 70/30 (30% dos racks s√£o validados e o restante para treinamento): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get distinct shelves shelves = list(set(photos_df['shelf_id'].values)) # use train_test_split from sklearn shelves_train, shelves_validation, _, _ = train_test_split(   shelves, shelves, test_size=0.3, random_state=6) # mark all records in data frames with is_train flag def is_train(shelf_id): return shelf_id in shelves_train photos_df['is_train'] = photos_df.shelf_id.apply(is_train) products_df['is_train'] = products_df.shelf_id.apply(is_train)</span></span></code> </pre> <br>  Garantiremos que, ao nos separarmos, haja representantes suficientes de cada classe para treinamento e valida√ß√£o: <br><img src="https://habrastorage.org/webt/9n/w_/xj/9nw_xjw1qiqc21sbi5q3s0pv3_q.jpeg"><br>  A cor azul mostra o n√∫mero de produtos na categoria para valida√ß√£o e laranja para treinamento.  A situa√ß√£o n√£o √© muito boa com a categoria 3 para valida√ß√£o, mas em princ√≠pio existem poucos de seus representantes. <br><br>  Na fase de prepara√ß√£o dos dados, √© importante n√£o cometer erros, pois todo o trabalho posterior se baseia em seus resultados.  Ainda cometemos um erro e passamos muitas horas felizes tentando entender por que a qualidade dos modelos √© muito med√≠ocre.  J√° parecia um perdedor das tecnologias da ‚Äúvelha escola‚Äù, at√© que voc√™ acidentalmente notou que algumas das fotos originais foram giradas 90 graus e outras foram feitas de cabe√ßa para baixo. <br><br>  Ao mesmo tempo, a marca√ß√£o √© feita como se as fotos fossem orientadas corretamente.  Depois de uma solu√ß√£o r√°pida, as coisas ficaram muito mais divertidas. <br><br>  Salvaremos nossos dados em arquivos pkl para uso nas etapas a seguir.  Total, temos: <br><br><ul><li>  Um diret√≥rio de fotografias de racks e suas pe√ßas com feixes, </li><li>  Um quadro de dados com uma descri√ß√£o de cada rack com uma nota sobre se ele se destina ao treinamento, </li><li>  Um quadro de dados com informa√ß√µes sobre todos os produtos nas prateleiras, indicando sua posi√ß√£o, tamanho, categoria e marcando se s√£o destinados ao treinamento. </li></ul><br>  Para verifica√ß√£o, exibimos um rack de acordo com nossos dados: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function to display shelf photo with rectangled products def draw_shelf_photo(file):   file_products_df = products_df[products_df.file == file]   coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values   im = cv2.imread(f'{shelf_images}{file}')   im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)      for xmin, ymin, xmax, ymax in coordinates:       cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)   plt.imshow(im) # draw one photo to check our data fig = plt.gcf() fig.set_size_inches(18.5, 10.5) draw_shelf_photo('C3_P07_N1_S6_1.JPG')</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/1m/7a/xr/1m7axrc3gcdvgt1sg0sc9-dbs0y.png"><br><br>  <b>Etapa 2. Classifica√ß√£o por marca ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link no github</a> )</b> <br><br>  A classifica√ß√£o de imagens √© a principal tarefa no campo da vis√£o computacional.  O problema √© a ‚Äúlacuna sem√¢ntica‚Äù: a fotografia √© apenas uma grande matriz de n√∫meros [0, 255].  Por exemplo, 800x600x3 (3 canais RGB). <br><br><img src="https://habrastorage.org/webt/0w/mi/rx/0wmirxot0tx0_dl_b-m_gpdunse.jpeg"><br><br>  Por que essa tarefa √© dif√≠cil: <br><br><img src="https://habrastorage.org/webt/gx/il/fh/gxilfhn6woijdjngbgrbgfzgjmo.png"><br><br>  Como j√° dissemos, os autores dos dados que usamos identificaram 10 marcas.  Essa √© uma tarefa extremamente simplificada, pois h√° muito mais marcas de cigarros nas prateleiras.  Mas tudo o que n√£o se enquadra nessas 10 categorias foi enviado para 0 - n√£o classificado: <br><br><img src="https://habrastorage.org/webt/wv/-j/hm/wv-jhmn18kt8ta1zhsai4fzdxbq.png">  " <br><br>  O artigo deles oferece um algoritmo de classifica√ß√£o com uma precis√£o total de 92%: <br><img src="https://habrastorage.org/webt/vv/d3/ub/vvd3ubvr_tvjxasrwvs7jap6h7o.jpeg"><br>  O que faremos: <br><br><ul><li>  Vamos preparar os dados para treinamento, </li><li>  Treinamos uma rede neural convolucional com a arquitetura ResNet v1, </li><li>  Verifique as fotos para valida√ß√£o. </li></ul><br>  Parece "volumoso", mas apenas usamos o exemplo de Keras " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Treina um ResNet no conjunto de dados CIFAR10</a> ", retirando dele a fun√ß√£o de criar o ResNet v1. <br><br>  Para iniciar o processo de treinamento, voc√™ precisa preparar duas matrizes: x - fotos de pacotes com uma dimens√£o (n√∫mero de pacotes, altura, largura, 3) e y - suas categorias com uma dimens√£o (n√∫mero de pacotes, 10).  A matriz y cont√©m os chamados vetores 1-quentes.  Se a categoria de um pacote de treinamento tiver o n√∫mero 2 (de 0 a 9), isso corresponder√° ao vetor [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. <br><br>  Uma quest√£o importante √© o que fazer com a largura e a altura, porque todas as fotos foram tiradas com diferentes resolu√ß√µes e dist√¢ncias diferentes.  Precisamos escolher um tamanho fixo, para o qual possamos trazer todas as nossas fotos dos pacotes.  Esse tamanho fixo √© um meta-par√¢metro que determina como nossa rede neural treinar√° e funcionar√°. <br><br>  Por um lado, quero aumentar esse tamanho o m√°ximo poss√≠vel para que nem um √∫nico detalhe da imagem passe despercebido.  Por outro lado, com nossa escassa quantidade de dados de treinamento, isso pode levar a um novo treinamento r√°pido: o modelo funcionar√° perfeitamente nos dados de treinamento, mas mal nos dados de valida√ß√£o.  Escolhemos o tamanho 120x80, talvez em um tamanho diferente tenhamos um resultado melhor.  Fun√ß√£o de zoom: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT def resize_pack(pack):   fx_ratio = SHAPE_WIDTH / pack.shape[1]   fy_ratio = SHAPE_HEIGHT / pack.shape[0]      pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)   return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]</span></span></code> </pre> <br>  Escale e exiba um pacote para verifica√ß√£o.  O nome da marca √© dif√≠cil de ser lido por uma pessoa, vamos ver como a rede neural lidar√° com a tarefa de classifica√ß√£o: <br><br><img src="https://habrastorage.org/webt/z_/8p/0f/z_8p0f5kuxx27mneryilqv81ols.png"><br><br>  Depois de preparar de acordo com a flag obtida na etapa anterior, dividimos as matrizes xey em x_train / x_validation e y_train / y_validation, obtemos: <br><br><pre> <code class="bash hljs">x_train shape: (1969, 120, 80, 3) y_train shape: (1969, 10) 1969 train samples 775 validation samples</code> </pre><br>  Os dados s√£o preparados, copiamos a fun√ß√£o do construtor de redes neurais da arquitetura ResNet v1 do exemplo de Keras: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">resnet_v1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape, depth, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   ‚Ä¶</code> </pre> <br>  Constru√≠mos um modelo: <br><br><pre> <code class="python hljs">model = resnet_v1(input_shape=x_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:], depth=depth, num_classes=num_classes) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>,             optimizer=Adam(lr=lr_schedule(<span class="hljs-number"><span class="hljs-number">0</span></span>)), metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Temos um conjunto de dados bastante limitado.  Portanto, para impedir que o modelo veja a mesma foto todas as vezes durante o treinamento, usamos o aumento: mude a imagem aleatoriamente e gire-a um pouco.  Keras fornece este conjunto de op√ß√µes para isso: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator(   featurewise_center=False,  # set input mean to 0 over the dataset   samplewise_center=False,  # set each sample mean to 0   featurewise_std_normalization=False,  # divide inputs by std of the dataset   samplewise_std_normalization=False,  # divide each input by its std   zca_whitening=False,  # apply ZCA whitening   rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)   width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)   height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)   horizontal_flip=False,  # randomly flip images   vertical_flip=False)  # randomly flip images datagen.fit(x_train)</span></span></code> </pre> <br>  Iniciamos o processo de treinamento. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's run training process, 20 epochs is enough batch_size = 50 epochs = 15 model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),                   validation_data=(x_validation, y_validation),                   epochs=epochs, verbose=1, workers=4,                   callbacks=[LearningRateScheduler(lr_schedule)])</span></span></code> </pre> <br>  Ap√≥s o treinamento e a avalia√ß√£o, obtemos precis√£o na regi√£o de 92%.  Voc√™ pode ter uma precis√£o diferente: h√° muito poucos dados; portanto, a precis√£o depende muito do sucesso da parti√ß√£o.  Nesta parti√ß√£o, n√£o obtivemos precis√£o significativamente maior do que a indicada no artigo, mas praticamente n√£o fizemos nada e escrevemos pouco c√≥digo.  Al√©m disso, podemos adicionar facilmente uma nova categoria, e a precis√£o deve (em teoria) aumentar significativamente se prepararmos mais dados. <br><br>  Por interesse, compare matrizes de confus√£o: <br><img src="https://habrastorage.org/webt/ee/ns/tm/eenstmjcconudxyjjkmtpzcdmow.jpeg"><br>  Quase todas as categorias que nossa rede neural define melhor, exceto as categorias 4 e 7. Tamb√©m √© √∫til observar os representantes mais brilhantes de cada c√©lula da matriz de confus√£o: <br><img src="https://habrastorage.org/webt/qv/tm/lw/qvtmlwxgqvbdhut73zgsbbtfqqo.jpeg"><br>  Voc√™ tamb√©m pode entender por que o Parlamento foi confundido com Camel, mas por que Winston foi confundido com Lucky Strike √© completamente incompreens√≠vel, mas eles n√£o t√™m nada em comum.  Esse √© o principal problema das redes neurais - a total opacidade do que est√° acontecendo l√° dentro.  √â claro que voc√™ pode visualizar algumas camadas, mas para n√≥s essa visualiza√ß√£o √© assim: <br><br><img src="https://habrastorage.org/webt/w1/tn/et/w1tnetor61yz-uwvmjlserwvh3m.jpeg"><br><br>  Uma oportunidade √≥bvia para melhorar a qualidade do reconhecimento em nossas condi√ß√µes √© adicionar mais fotos. <br><br>  Ent√£o, o classificador est√° pronto.  V√° para o detector. <br><br>  <b>Etapa 3. Pesquise produtos na foto ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link no github</a> )</b> <br><br>  As seguintes tarefas importantes no campo da vis√£o por computador s√£o segmenta√ß√£o sem√¢ntica, localiza√ß√£o, pesquisa de objeto e segmenta√ß√£o de inst√¢ncia. <br><br><img src="https://habrastorage.org/webt/z0/y9/bf/z0y9bfe7f6qp143c3wubj8rvnd8.jpeg"><br><br>  Nossa tarefa precisa de detec√ß√£o de objetos.  O artigo de 2014 oferece uma abordagem baseada no m√©todo Viola-Jones e HOG com precis√£o visual: <br><br><img src="https://habrastorage.org/webt/11/jt/8j/11jt8jcggyutkrxyelswr8psm8s.jpeg"><br><br>  Gra√ßas ao uso de restri√ß√µes estat√≠sticas adicionais, sua precis√£o √© muito boa: <br><br><img src="https://habrastorage.org/webt/od/nb/yl/odnbylwn0yo-k92q_nu6mazz8fq.jpeg"><br><br>  Agora, a tarefa de reconhecimento de objetos √© resolvida com sucesso com a ajuda de redes neurais.  Usaremos o sistema API Tensorflow Object Detection e treinaremos uma rede neural com a arquitetura SSD Mobilenet V1.  O treinamento desse modelo a partir do zero requer muitos dados e pode levar dias. Portanto, usamos um modelo treinado em dados COCO de acordo com o princ√≠pio da transfer√™ncia de aprendizado. <br><br>  O conceito chave dessa abordagem √© esse.  Por que uma crian√ßa n√£o precisa mostrar milh√µes de objetos para aprender a encontrar e distinguir uma bola de um cubo?  Porque a crian√ßa tem 500 milh√µes de anos de desenvolvimento do c√≥rtex visual.  A evolu√ß√£o fez da vis√£o o maior sistema sensorial.  Quase 50% (mas isso n√£o √© exato) dos neur√¥nios do c√©rebro humano s√£o respons√°veis ‚Äã‚Äãpelo processamento da imagem.  Os pais s√≥ podem mostrar a bola e o cubo e, em seguida, corrigir a crian√ßa v√°rias vezes, para que ela encontre e distinga perfeitamente uma da outra. <br><br>  Do ponto de vista filos√≥fico (com diferen√ßas t√©cnicas mais que gerais), a transfer√™ncia de aprendizado em redes neurais funciona de maneira semelhante.  As redes neurais convolucionais consistem em n√≠veis, cada um dos quais define formas cada vez mais complexas: identifica pontos-chave, combina-os em linhas, que por sua vez se combinam em figuras.  E somente no √∫ltimo n√≠vel da totalidade dos sinais encontrados determina o objeto. <br><br>  Objetos do mundo real t√™m muito em comum.  Ao transferir o aprendizado, usamos os n√≠veis de defini√ß√£o de recursos b√°sicos j√° treinados e treinamos apenas as camadas respons√°veis ‚Äã‚Äãpela identifica√ß√£o dos objetos.  Para fazer isso, bastam algumas centenas de fotos e algumas horas de opera√ß√£o de uma GPU comum.  A rede foi treinada originalmente no conjunto de dados COCO (Microsoft Common Objects in Context), que possui 91 categorias e 2.500.000 imagens!  Muitos, embora n√£o 500 milh√µes de anos de evolu√ß√£o. <br><br>  Olhando um pouco √† frente, essa anima√ß√£o gif (um pouco lenta, n√£o rola imediatamente) do tensorboard visualiza o processo de aprendizado.  Como voc√™ pode ver, o modelo come√ßa a produzir um resultado completamente de alta qualidade quase que imediatamente e depois vem a retifica√ß√£o: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2d/356/aa4/d2d356aa49bedf31aa918930d8bf1545.gif" alt="imagem"><br><br>  O "treinador" do sistema API do Tensorflow Object Detection pode executar aprimoramentos de forma independente, cortar partes aleat√≥rias das imagens para treinamento e selecionar exemplos "negativos" (se√ß√µes de fotos que n√£o cont√™m nenhum objeto).  Em teoria, n√£o √© necess√°rio pr√©-processamento de fotos.  No entanto, em um computador dom√©stico com um HDD e uma pequena quantidade de RAM, ele se recusou a trabalhar com imagens de alta resolu√ß√£o: no in√≠cio, ele ficou pendurado por um longo tempo, mexeu em um disco e depois voou para fora. <br><br>  Como resultado, compactamos as fotos em um tamanho de 1000x1000 pixels, mantendo a propor√ß√£o.  Por√©m, como ao compactar uma foto grande, muitos sinais s√£o perdidos, primeiro v√°rios quadrados de tamanho aleat√≥rio foram cortados de cada foto do rack e compactados em 1000x1000.  Como resultado, pacotes em alta resolu√ß√£o (mas n√£o o suficiente) e em pequenos (mas muitos) ca√≠ram nos dados de treinamento.  Repetimos: esse passo √© for√ßado e, muito provavelmente, completamente desnecess√°rio e possivelmente prejudicial. <br><br>  As fotos preparadas e compactadas s√£o salvas em diret√≥rios separados (eval e train), e sua descri√ß√£o (com os pacotes contidos neles) √© formada na forma de dois quadros de dados do pandas (train_df e eval_df): <br><br><img src="https://habrastorage.org/webt/dl/lx/pn/dllxpn03xu5us7wr6h7y3rgzayg.png"><br>  O sistema API do Tensorflow Object Detection exige que a entrada seja apresentada como arquivos tfrecord.  Voc√™ pode form√°-los usando o utilit√°rio, mas vamos torn√°-lo um c√≥digo: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">class_text_to_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row_label)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> row_label == <span class="hljs-string"><span class="hljs-string">'pack'</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, group)</span></span></span><span class="hljs-function">:</span></span>   data = namedtuple(<span class="hljs-string"><span class="hljs-string">'data'</span></span>, [<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, <span class="hljs-string"><span class="hljs-string">'object'</span></span>])   gb = df.groupby(group)   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [data(filename, gb.get_group(x))           <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(gb.groups.keys(), gb.groups)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_tf_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(group, path)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.gfile.GFile(os.path.join(path, <span class="hljs-string"><span class="hljs-string">'{}'</span></span>.format(group.filename)), <span class="hljs-string"><span class="hljs-string">'rb'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> fid:       encoded_jpg = fid.read()   encoded_jpg_io = io.BytesIO(encoded_jpg)   image = Image.open(encoded_jpg_io)   width, height = image.size   filename = group.filename.encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>)   image_format = <span class="hljs-string"><span class="hljs-string">b'jpg'</span></span>   xmins = []   xmaxs = []   ymins = []   ymaxs = []   classes_text = []   classes = []   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index, row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> group.object.iterrows():       xmins.append(row[<span class="hljs-string"><span class="hljs-string">'xmin'</span></span>] / width)       xmaxs.append(row[<span class="hljs-string"><span class="hljs-string">'xmax'</span></span>] / width)       ymins.append(row[<span class="hljs-string"><span class="hljs-string">'ymin'</span></span>] / height)       ymaxs.append(row[<span class="hljs-string"><span class="hljs-string">'ymax'</span></span>] / height)       classes_text.append(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>].encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>))       classes.append(class_text_to_int(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]))   tf_example = tf.train.Example(features=tf.train.Features(feature={       <span class="hljs-string"><span class="hljs-string">'image/height'</span></span>: dataset_util.int64_feature(height),       <span class="hljs-string"><span class="hljs-string">'image/width'</span></span>: dataset_util.int64_feature(width),       <span class="hljs-string"><span class="hljs-string">'image/filename'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/source_id'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/encoded'</span></span>: dataset_util.bytes_feature(encoded_jpg),       <span class="hljs-string"><span class="hljs-string">'image/format'</span></span>: dataset_util.bytes_feature(image_format),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmin'</span></span>: dataset_util.float_list_feature(xmins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmax'</span></span>: dataset_util.float_list_feature(xmaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymin'</span></span>: dataset_util.float_list_feature(ymins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymax'</span></span>: dataset_util.float_list_feature(ymaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/class/text'</span></span>: dataset_util.bytes_list_feature(classes_text),       <span class="hljs-string"><span class="hljs-string">'image/object/class/label'</span></span>: dataset_util.int64_list_feature(classes),   }))   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf_example <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert_to_tf_records</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_path, examples, dst_file)</span></span></span><span class="hljs-function">:</span></span>   writer = tf.python_io.TFRecordWriter(dst_file)   grouped = split(examples, <span class="hljs-string"><span class="hljs-string">'filename'</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grouped:       tf_example = create_tf_example(group, images_path)       writer.write(tf_example.SerializeToString())   writer.close() convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">train/'</span></span>, train_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">train.record'</span></span>) convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">eval/'</span></span>, eval_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">eval.record'</span></span>)</code> </pre><br>  Resta preparar um diret√≥rio especial e iniciar os processos: <br><br><img src="https://habrastorage.org/webt/ld/yd/rb/ldydrbk3ivkixsx9kep69b5eyoy.jpeg"><br><br>  A estrutura pode ser diferente, mas achamos muito conveniente. <br><br>  O diret√≥rio de dados cont√©m os arquivos que criamos com tfrecords (train.record e eval.record), bem como pack.pbtxt com os tipos de objetos para os quais iremos treinar a rede neural.  Como temos apenas um tipo de objeto para definir, o arquivo √© muito curto: <br><br><img src="https://habrastorage.org/webt/pc/jw/x0/pcjwx0l3fc-wuzhiyt7lfo9xtcq.png"><br><br>  O diret√≥rio de modelos (pode haver muitos modelos para resolver um problema) no diret√≥rio filho ssd_mobilenet_v1 cont√©m as configura√ß√µes de treinamento no arquivo .config, al√©m de dois diret√≥rios vazios: train e eval.  No treinamento, o "treinador" salvar√° os pontos de controle do modelo, o "avaliador" os buscar√°, executar√° os dados para avalia√ß√£o e os colocar√° no diret√≥rio eval.  O Tensorboard acompanhar√° esses dois diret√≥rios e exibir√° as informa√ß√µes do processo. <br><br>  Descri√ß√£o detalhada da estrutura dos arquivos de configura√ß√£o, etc.  pode ser encontrado <a href="">aqui</a> e <a href="">aqui</a> .  As instru√ß√µes de instala√ß√£o da API de detec√ß√£o de objetos do Tensorflow podem ser encontradas <a href="">aqui</a> . <br><br>  Entramos no diret√≥rio models / research / object_detection e esvaziamos o modelo pr√©-treinado: <br><br><pre> <code class="bash hljs">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</code> </pre><br>  Copiamos o diret√≥rio pack_detector preparado por n√≥s l√°. <br><br>  Primeiro, inicie o processo de treinamento: <br><br><pre> <code class="bash hljs">python3 train.py --logtostderr \   --train_dir=pack_detector/models/ssd_mobilenet_v1/train/ \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config</code> </pre><br>  Iniciamos o processo de avalia√ß√£o.  Como n√£o temos uma segunda placa de v√≠deo, a inicializamos no processador (usando a instru√ß√£o CUDA_VISIBLE_DEVICES = "").  Por isso, ele se atrasar√° no processo de treinamento, mas isso n√£o √© t√£o ruim: <br><br><pre> <code class="bash hljs">CUDA_VISIBLE_DEVICES=<span class="hljs-string"><span class="hljs-string">""</span></span> python3 eval.py \   --logtostderr \   --checkpoint_dir=pack_detector/models/ssd_mobilenet_v1/train \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --eval_dir=pack_detector/models/ssd_mobilenet_v1/<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span></code> </pre> <br>  Iniciamos o processo do tensorboard: <br><br><pre> <code class="bash hljs">tensorboard --logdir=pack_detector/models/ssd_mobilenet_v1</code> </pre> <br>  Depois disso, podemos ver belos gr√°ficos, bem como o trabalho real do modelo nos dados estimados (gif no in√≠cio): <br><br><img src="https://habrastorage.org/webt/qc/wt/rl/qcwtrlmdugb4zgyhy6gnnaoia9w.jpeg"><br><br>  O processo de treinamento pode ser interrompido e retomado a qualquer momento.  Quando acreditamos que o modelo √© bom o suficiente, salvamos o ponto de verifica√ß√£o na forma de um gr√°fico de infer√™ncia: <br><br><pre> <code class="bash hljs">python3 export_inference_graph.py \   --input_type image_tensor \   --pipeline_config_path pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --trained_checkpoint_prefix pack_detector/models/ssd_mobilenet_v1/train/model.ckpt-13756 \   --output_directory pack_detector/models/ssd_mobilenet_v1/pack_detector_2018_06_03</code> </pre> <br>  Portanto, nesta etapa, obtivemos um gr√°fico de infer√™ncia, que podemos usar para procurar objetos de pacote.  Passamos ao seu uso. <br><br>  <b>Etapa 4. Implementando a pesquisa ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">link do github</a> )</b> <br><br>  O c√≥digo de carregamento e inicializa√ß√£o do gr√°fico de infer√™ncia est√° no link acima.  Principais recursos de pesquisa: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's write function that executes detection def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):   # Run inference   expanded_dims = np.expand_dims(image, 0)   output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})   # all outputs are float32 numpy arrays, so convert types as appropriate   output_dict['num_detections'] = int(output_dict['num_detections'][0])   output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)   output_dict['detection_boxes'] = output_dict['detection_boxes'][0]   output_dict['detection_scores'] = output_dict['detection_scores'][0]   return output_dict # it is useful to be able to run inference not only on the whole image, # but also on its parts # cutoff - minimum detection score needed to take box def run_inference_for_image_part(image_tensor, sess, tensor_dict,                                image, cutoff, ax0, ay0, ax1, ay1):   boxes = []   im = image[ay0:ay1, ax0:ax1]   h, w, c = im.shape   output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)   for i in range(100):       if output_dict['detection_scores'][i] &lt; cutoff:           break       y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \                               output_dict['detection_scores'][i]       x0, y0, x1, y1, score = int(x0*w), int(y0*h), \                               int(x1*w), int(y1*h), \                               int(score * 100)       boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))   return boxes</span></span></code> </pre> <br>  A fun√ß√£o encontra caixas delimitadas para pacotes n√£o na foto inteira, mas em sua parte.  A fun√ß√£o tamb√©m filtra os ret√¢ngulos encontrados com uma baixa pontua√ß√£o de detec√ß√£o especificada no par√¢metro de corte. <br><br>  Isso acaba sendo um dilema.  Por um lado, com um ponto de corte alto, perdemos muitos objetos; por outro lado, com um ponto de corte baixo, come√ßamos a encontrar muitos objetos que n√£o s√£o feixes.  Ao mesmo tempo, ainda n√£o encontramos tudo e n√£o o ideal: <br><img src="https://habrastorage.org/webt/-u/ao/7y/-uao7ylwycrzrn3xqd1kfh2q0bs.jpeg"><br>  No entanto, observe que, se executarmos a fun√ß√£o para um pequeno peda√ßo da foto, o reconhecimento ser√° quase perfeito com ponto de corte = 0,9: <br><br><img src="https://habrastorage.org/webt/gg/qn/ia/ggqniasnrkggb6bjnarwum_6i48.jpeg"><br><br>  Isso ocorre pelo fato de o modelo SSD do MobileNet V1 aceitar fotos de 300x300 como entrada.  Naturalmente, com essa compress√£o, muitos sinais s√£o perdidos. <br><br>  Mas esses sinais persistem se recortarmos um pequeno quadrado contendo v√°rios pacotes.  Isso sugere a id√©ia de usar uma janela flutuante: percorremos um pequeno ret√¢ngulo em uma fotografia e lembramos tudo o que encontramos. <br><br><img src="https://habrastorage.org/webt/zn/7s/dz/zn7sdzl4wcb9dwugzkxilg2d2hk.jpeg"><br><br>  :          ,     .         .   :          (detection score),  ,    ,        overlapTresh (       ): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function for non-maximum suppression def non_max_suppression(boxes, overlapThresh):   if len(boxes) == 0:       return np.array([]).astype("int")   if boxes.dtype.kind == "i":       boxes = boxes.astype("float")   pick = []   x1 = boxes[:,0]   y1 = boxes[:,1]   x2 = boxes[:,2]   y2 = boxes[:,3]   sc = boxes[:,4]   area = (x2 - x1 + 1) * (y2 - y1 + 1)   idxs = np.argsort(sc)   while len(idxs) &gt; 0:       last = len(idxs) - 1       i = idxs[last]       pick.append(i)       xx1 = np.maximum(x1[i], x1[idxs[:last]])       yy1 = np.maximum(y1[i], y1[idxs[:last]])       xx2 = np.minimum(x2[i], x2[idxs[:last]])       yy2 = np.minimum(y2[i], y2[idxs[:last]])       w = np.maximum(0, xx2 - xx1 + 1)       h = np.maximum(0, yy2 - yy1 + 1)       #todo fix overlap-contains...       overlap = (w * h) / area[idxs[:last]]              idxs = np.delete(idxs, np.concatenate(([last],           np.where(overlap &gt; overlapThresh)[0])))     return boxes[pick].astype("int")</span></span></code> </pre> <br>     : <br><br><img src="https://habrastorage.org/webt/go/hd/yz/gohdyzvwuyu8ja4w893big9c1yy.jpeg"><br><br>          : <br><br><img src="https://habrastorage.org/webt/jm/oi/da/jmoida7irvm4u3drey8nu6g00kw.jpeg"><br><br>   ,           ,    . <br><br><h3>  Conclus√£o </h3><br>       ¬´¬ª:         ,       . ,    ,         ..    . <br><br>       ,    ,    : <br><br><ol><li>  150  ,     ,   , </li><li>        3-7  , </li><li>   100    , </li><li>        , </li><li>        (), </li><li>    (,  ), </li><li>    ,        ¬´¬ª, </li><li>  ,   ,     (SSD  ), </li><li>      ,  , </li><li>  . </li></ol><br>         ,      ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt416123/">https://habr.com/ru/post/pt416123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt416111/index.html">Convers√£o de dados GraphQL para o componente CustomTreeData do DevExtreme-Reactive</a></li>
<li><a href="../pt416113/index.html">Steven Wolfram: Mem√≥rias de Steve Jobs</a></li>
<li><a href="../pt416115/index.html">10 pequenos erros de design que ainda cometemos</a></li>
<li><a href="../pt416119/index.html">Postagem na sexta-feira: top dos pacotes NPM mais "essenciais"</a></li>
<li><a href="../pt416121/index.html">Fujitsu Artificial Intelligence calcula a geometria dos materiais magn√©ticos</a></li>
<li><a href="../pt416125/index.html">Instala√ß√£o, configura√ß√£o do sistema e controle para c√¢meras</a></li>
<li><a href="../pt416127/index.html">CUDA e GPU remota</a></li>
<li><a href="../pt416129/index.html">Como a IA aprende a gerar imagens de gatos</a></li>
<li><a href="../pt416131/index.html">Como lidar com a DP na Federa√ß√£o Russa e n√£o violar a lei</a></li>
<li><a href="../pt416133/index.html">Data Center no exterior: Equinix LD8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>