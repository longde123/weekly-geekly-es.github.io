<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçò üë®‚Äçüë©‚Äçüë¶‚Äçüë¶ üëÆ Balanceamento de tr√°fego VoIP tolerante a falhas. Troca de carga entre data centers no hor√°rio de pico üïµüèª üìã üå™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Algumas palavras sobre o que fazemos. A DINS est√° envolvida no desenvolvimento e suporte do servi√ßo UCaaS no mercado internacional para clientes corpo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Balanceamento de tr√°fego VoIP tolerante a falhas. Troca de carga entre data centers no hor√°rio de pico</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dins/blog/436404/"><p>  Algumas palavras sobre o que fazemos.  A DINS est√° envolvida no desenvolvimento e suporte do servi√ßo UCaaS no mercado internacional para clientes corporativos.  O servi√ßo √© usado por pequenas empresas e startups e grandes empresas.  Os clientes se conectam pela Internet via protocolo SIP por TCP, TLS ou WSS.  Isso cria uma carga bastante grande: quase 1,5 milh√£o de conex√µes de dispositivos finais - telefones Polycom / Cisco / Yealink e clientes de software para PC / Mac / IOS / Android. </p><br><p>  Neste artigo, falo sobre como os pontos de entrada de VoIP s√£o organizados. </p><a name="habracut"></a><br><h3 id="predystoriya">  Antecedentes </h3><br><p>  No per√≠metro do sistema (entre os dispositivos terminais e o kernel), h√° SBC comercial (Session Border Controller). </p><br><p>  Desde 2012, usamos solu√ß√µes da Acme Packet, posteriormente adquiridas pela Oracle.  Antes disso, usamos o NatPASS. </p><br><p>  Liste brevemente a funcionalidade que usamos: </p><br><p>  ‚Ä¢ passagem NAT; <br>  ‚Ä¢ B2BUA; <br>  Normaliza√ß√£o do SIP (cabe√ßalhos permitidos / n√£o permitidos, regras de manipula√ß√£o de cabe√ßalhos, etc.) <br>  ‚Ä¢ Descarregamento de TLS e SRTP; <br>  ‚Ä¢ Convers√£o de transporte (dentro do sistema usamos SIP sobre UDP); <br>  ‚Ä¢ monitoramento MOS (via RTCP-XR); <br>  ‚Ä¢ ACLs, detec√ß√£o de Bruteforce; <br>  ‚Ä¢ Tr√°fego de registro reduzido devido ao aumento da expira√ß√£o de contatos (baixa expira no lado do acesso, alta no lado do n√∫cleo); <br>  ‚Ä¢ Controle de mensagens SIP por m√©todo. </p><br><p> Os sistemas comerciais t√™m suas vantagens √≥bvias (funcionalidade pronta para uso, suporte comercial) e desvantagens (pre√ßo, prazo de entrega, falta de oportunidade ou prazos muito longos para a implementa√ß√£o de novos recursos que precisamos, prazos para solu√ß√£o de problemas etc.).  Gradualmente, as falhas come√ßaram a ser superadas, e ficou claro que era necess√°ria a necessidade de desenvolver nossas pr√≥prias solu√ß√µes. </p><br><p>  O desenvolvimento foi lan√ßado h√° um ano e meio.  No subsistema de fronteira, tradicionalmente distingu√≠amos dois componentes principais: servidores SIP e M√≠dia;  balanceadores de carga acima de cada componente.  Estou trabalhando nos pontos de entrada / balanceadores aqui, ent√£o vou tentar falar sobre eles. </p><br><h4 id="trebovaniya">  Exig√™ncias </h4><br><ul><li>  Toler√¢ncia a falhas: o sistema deve fornecer um servi√ßo em caso de falha de uma ou mais inst√¢ncias no data center ou em todo o data center </li><li>  Facilidade de manuten√ß√£o: queremos poder alternar cargas de um data center para outro </li><li>  Escalabilidade: quero aumentar a capacidade de forma r√°pida e barata </li></ul><br><h4 id="balansirovka">  Balanceamento </h4><br><p>  Selecionamos o IPVS (aka LVS) no modo IPIP (tunelamento de tr√°fego).  N√£o entrarei em uma an√°lise comparativa de NAT / DR / TUN / L3DSR (voc√™ pode ler sobre os modos, por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> ), mencionarei apenas os motivos: </p><br><ul><li>  N√£o queremos impor um requisito para que os back-ends estejam na mesma sub-rede do LVS (os pools cont√™m back-ends de nossos pr√≥prios datacenters e remotos); </li><li>  O back-end deve receber o IP de origem original do cliente (ou seu NAT), ou seja, o NAT de origem n√£o √© adequado; </li><li>  O back-end deve oferecer suporte ao trabalho simult√¢neo com v√°rios VIPs. </li></ul><br><p>  Como estamos balanceando o tr√°fego de m√≠dia (acabou sendo muito dif√≠cil, vamos recusar), o esquema de implanta√ß√£o atual no data center √© o seguinte: <br><br><img src="https://habrastorage.org/webt/fk/vh/wg/fkvhwgftogjrev-a931o3a9qk-e.png"><br><br>  A atual estrat√©gia de balanceamento IPVS √© "sed" (atraso mais curto esperado), mais sobre isso.  Diferentemente do Robin Redondo Ponderado / Menor Conex√£o Ponderada, ele permite que voc√™ n√£o transfira tr√°fego para back-end com pesos mais baixos at√© que um determinado limite seja atingido.  O menor atraso esperado √© calculado usando a f√≥rmula (Ci + 1) / Ui, em que Ci √© o n√∫mero de conex√µes no back-end i, Ui √© o peso do back-end.  Por exemplo, se houver back-end no pool com pesos de 50.000 e 2, novas conex√µes ser√£o distribu√≠das pelos primeiros at√© que cada servidor atinja 25.000 conex√µes ou at√© que o limite seja alcan√ßado - um limite para o n√∫mero total de conex√µes. <br>  Leia mais sobre estrat√©gias de balanceamento no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">man ipvsadm</a> . </p><br><p>  O pool de IPVS tem esta apar√™ncia (aqui e abaixo, os endere√ßos IP fict√≠cios est√£o listados): </p><br><pre><code class="plaintext hljs"># ipvsadm -ln Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 1.1.1.1:5060 sed -&gt; 10.11.100.181:5060 Tunnel 50000 5903 4 -&gt; 10.11.100.192:5060 Tunnel 50000 5905 1 -&gt; 10.12.100.137:5060 Tunnel 2 0 0 -&gt; 10.12.100.144:5060 Tunnel 2 0 0</code> </pre> <br><p>  A carga no VIP √© distribu√≠da para servidores com um peso de 50.000 (eles s√£o implantados no mesmo datacenter que uma inst√¢ncia espec√≠fica do LVS). Se eles estiverem sobrecarregados ou entrarem em uma lista negra, a carga ir√° para a parte de backup do pool - servidores com um peso de 2, localizados em data center vizinho. </p><br><p>  O mesmo pool exato, mas com escalas, pelo contr√°rio, √© configurado no data center vizinho (no sistema de produ√ß√£o, √© claro que o n√∫mero de back-end √© muito maior). </p><br><p>  A sincroniza√ß√£o de conex√µes via ipvs sync permite que o LVS de backup conhe√ßa todas as conex√µes atuais. </p><br><p>  Para sincroniza√ß√£o entre data centers, foi aplicada uma t√©cnica "suja", que, no entanto, funciona bem.  A sincroniza√ß√£o IPVS funciona apenas por multicast, o que foi dif√≠cil para n√≥s entregar corretamente ao controlador de dom√≠nio vizinho.  Em vez do multicast, duplicamos o tr√°fego de sincroniza√ß√£o via IPTables target TEE do ipvs master para o ip-ip tunnel para o servidor no DC vizinho e pode haver v√°rios hosts / data centers de destino: </p><br><pre> <code class="plaintext hljs">#### start ipvs sync master role: ipvsadm --start-daemon master --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### duplicate all sync packets to remote LVS servers using iptables TEE target: iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.10 # ip-ip remote lvs server 1 iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.14 # ip-ip remote lvs server 2 #### start ipvs sync backup role: ipvsadm --start-daemon backup --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### be ready to receive sync sync packets from remote LVS servers: iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv01 -j TEE --gateway 127.0.0.1 iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv02 -j TEE --gateway 127.0.0.1</code> </pre> <br><p>  De fato, cada um de nossos servidores LVS desempenha as duas fun√ß√µes ao mesmo tempo (mestre e backup), por um lado, √© apenas conveniente, pois elimina a altera√ß√£o de fun√ß√£o ao alternar tr√°fego, por outro √© necess√°rio, pois cada controlador de dom√≠nio processa o tr√°fego do grupo por padr√£o. VIPs p√∫blicos. </p><br><h4 id="pereklyuchenie-nagruzki-mezhdu-data-centrami">  Troca de carga entre data centers </h4><br><p>  Em opera√ß√£o normal, cada endere√ßo IP p√∫blico √© anunciado na Internet de qualquer lugar (neste diagrama, de dois data centers).  O tr√°fego que chega ao VIP √© roteado para o DC necess√°rio no momento, usando o atributo BGP MED (Discriminador de m√∫ltiplas sa√≠das) com valores diferentes para DC ativo e DC de backup.  Ao mesmo tempo, o Backup DC est√° sempre pronto para aceitar o tr√°fego se algo acontecer com o ativo: <br><br><img src="https://habrastorage.org/webt/ir/s6/ht/irs6htkfhrve2zzevsl0cwcg4k4.png"><br><br>  Alterando os valores dos BGP MEDs e usando o IPVS-sync entre locais, temos a oportunidade de transferir tr√°fego sem problemas dos back-ends de um data center para outro, sem afetar as chamadas telef√¥nicas estabelecidas, que mais cedo ou mais tarde terminar√£o naturalmente.  O processo √© totalmente automatizado (para cada VIP, temos um bot√£o no console de gerenciamento) e fica assim: </p><br><ol><li><p>  O SIP-VIP est√° ativo no DC1 (√† esquerda), o cluster no DC2 (√† direita) √© redundante. Gra√ßas √† sincroniza√ß√£o de ipvs, ele possui informa√ß√µes sobre conex√µes estabelecidas em sua mem√≥ria.  √Ä esquerda, os VIPs ativos s√£o anunciados com um valor de MED 100, √† direita - com um valor de 500: <br><br><img src="https://habrastorage.org/webt/v6/di/th/v6dithpguub8rbq7ggurj_vdlxi.png"></p><br></li><li><p>  O bot√£o do interruptor causa uma mudan√ßa no chamado  ‚ÄúTarget_state‚Äù (um conceito interno que declara os valores dos BGP MEDs em um determinado momento).  Aqui, n√£o esperamos que o DC1 esteja em ordem e pronto para processar o tr√°fego; portanto, o LVS no DC2 entra no estado de "for√ßa ativa", diminuindo o valor de MEDs para 50 e arrastando o tr√°fego para si mesmo.  Se os back-end no DC1 estiverem ativos e dispon√≠veis, as chamadas n√£o ser√£o interrompidas.  Todas as novas conex√µes TCP (registros) ser√£o enviadas para back-end no DC2: <br><br><img src="https://habrastorage.org/webt/lx/zo/cb/lxzocb7gt-zzrznicu5k9hiju2k.png"></p><br></li><li><p>  O DC1 recebeu uma nova replica√ß√£o target_state e defina o valor de backup MEDs (500).  Quando o DC2 descobre isso, ele normaliza seu valor (50 =&gt; 100).  Resta aguardar a conclus√£o de todas as chamadas ativas no DC1 e interromper as conex√µes TCP estabelecidas.  As inst√¢ncias SBC no DC1 introduzem os servi√ßos necess√°rios no chamado  Estado de ‚Äúdesligamento normal‚Äù: ‚Äú503‚Äù responde √†s pr√≥ximas solicita√ß√µes SIP e desconecta, mas n√£o aceita novas conex√µes.  Al√©m disso, essas inst√¢ncias entram na lista negra do LVS.  Ao interromper, o cliente estabelece um novo registro / conex√£o, que j√° vem no DC2: <br><br><img src="https://habrastorage.org/webt/by/rq/uf/byrquf_81kp8zvvryrrjzdtcl-a.png"></p><br></li><li><p>  O processo termina quando todo o tr√°fego no DC2. <br><br><img src="https://habrastorage.org/webt/1m/dm/h8/1mdmh8styniixoof0cragfatdqq.png"></p><br></li><li><p>  Fun√ß√µes comutadas DC1 e DC2. <br><br><img src="https://habrastorage.org/webt/wu/t3/ts/wut3ts2mq5iznucnkesz2r3mnra.png"></p><br></li></ol><br><p>  Em condi√ß√µes de carga alta constante nos pontos de entrada, tornou-se muito conveniente poder trocar de tr√°fego a qualquer momento.  O mesmo mecanismo inicia automaticamente se o controlador de dom√≠nio de backup repentinamente come√ßar a receber tr√°fego.  Ao mesmo tempo, para proteger contra batidas, a comuta√ß√£o √© acionada apenas uma vez em uma dire√ß√£o e a trava √© ajustada para comuta√ß√£o autom√°tica. </p><br><h4 id="chto-vnutri">  O que h√° dentro </h4><br><p>  Cluster VRRP e gerenciador IPVS: Keepalived.  O Keepalived √© respons√°vel pela troca de VIPs dentro do cluster, bem como pela verifica√ß√£o de integridade / lista negra de back-end. </p><br><p>  Pilha BGP: ExaBGP.  Ele √© respons√°vel por an√∫ncios de rotas para endere√ßos VIP e aposi√ß√£o dos BGP MEDs correspondentes.  Totalmente controlado pelo servidor de gerenciamento.  Um daemon BGP confi√°vel, escrito em Python, est√° se desenvolvendo ativamente; ele executa sua tarefa 100%. </p><br><p>  Servidor de gerenciamento (API / Monitoramento / gerenciamento de subcomponentes): Pyro4 + Flask.  √â um servidor de provisionamento para Keepalived e ExaBGP, gerencia todas as outras configura√ß√µes do sistema (sysctl / iptables / ipset / etc), fornece monitoramento (gnlpy), adiciona e remove back-ends a pedido (eles se comunicam com sua API). </p><br><h4 id="cifry">  Figuras </h4><br><p>  Uma m√°quina virtual com CPU Intel Xeon Gold 6140 de quatro n√∫cleos a 2,30 GHz atende a um fluxo de tr√°fego de 300 Mbps / 210 Kpps (tr√°fego de m√≠dia, s√£o processados ‚Äã‚Äãpor meio de tr√°fego de m√≠dia, cerca de 3 mil chamadas simult√¢neas no hor√°rio de pico).  Utiliza√ß√£o da CPU ao mesmo tempo - 60%. </p><br><p>  Agora, isso √© suficiente para atender o tr√°fego de at√© 100 mil dispositivos terminais (telefones de mesa).  Para atender todo o tr√°fego (mais de 1 milh√£o de dispositivos de terminal), estamos construindo cerca de 10 pares desses clusters em v√°rios data centers. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt436404/">https://habr.com/ru/post/pt436404/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt436392/index.html">Benef√≠cios da an√°lise de aplicativos de n√≠vel 7 em firewalls. Parte 1. No√ß√µes b√°sicas</a></li>
<li><a href="../pt436394/index.html">Demis Hassabis - o grande intelecto que criou o grande intelecto</a></li>
<li><a href="../pt436396/index.html">Posso usar programa√ß√£o funcional no meu idioma?</a></li>
<li><a href="../pt436398/index.html">Agua</a></li>
<li><a href="../pt436400/index.html">Configure o ambiente de desenvolvimento para aprender HTML, CSS, PHP no Windows</a></li>
<li><a href="../pt436406/index.html">Como se tornar um desenvolvedor de jogos, se voc√™ √© um corretor de im√≥veis</a></li>
<li><a href="../pt436408/index.html">Modelagem num√©rica - a hist√≥ria de um projeto</a></li>
<li><a href="../pt436412/index.html">Visita fotogr√°fica ao novo escrit√≥rio de Boston no Facebook</a></li>
<li><a href="../pt436416/index.html">Migrando do Mongo para o Postgres: a experi√™ncia do jornal The Guardian</a></li>
<li><a href="../pt436420/index.html">O maior lix√£o da hist√≥ria: 2,7 bilh√µes de contas, das quais 773 milh√µes s√£o √∫nicas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>