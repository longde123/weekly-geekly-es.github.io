<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëãüèª üë®üèº‚Äçüè´ ‚òùÔ∏è AERODISCO Motor: catastr√≥fico. Parte 1 ü§Æ üë©üèø‚Äç‚úàÔ∏è üêÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola lectores de Habr! El tema de este art√≠culo ser√° la implementaci√≥n de la tolerancia a desastres en los sistemas de almacenamiento de AERODISK Engi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AERODISCO Motor: catastr√≥fico. Parte 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/456348/"><p><img src="https://habrastorage.org/webt/2b/54/ub/2b54ub9jta3knw6ff5eseo1buyu.jpeg"></p><br><p> Hola lectores de Habr!  El tema de este art√≠culo ser√° la implementaci√≥n de la tolerancia a desastres en los sistemas de almacenamiento de AERODISK Engine.  Inicialmente, quer√≠amos escribir en un art√≠culo sobre ambos medios: la replicaci√≥n y el cl√∫ster de metro, pero, desafortunadamente, el art√≠culo result√≥ ser demasiado grande, por lo que dividimos el art√≠culo en dos partes.  Pasemos de lo simple a lo complejo.  En este art√≠culo, configuraremos y probaremos la replicaci√≥n s√≠ncrona: descarte un centro de datos y tambi√©n rompamos el canal de comunicaci√≥n entre los centros de datos y veamos qu√© sucede. </p><a name="habracut"></a><br><p>  Nuestros clientes a menudo nos hacen diferentes preguntas sobre la replicaci√≥n, por lo tanto, antes de pasar a configurar y probar la implementaci√≥n de la r√©plica, le diremos un poco sobre qu√© replicaci√≥n hay en los sistemas de almacenamiento. </p><br><h2 id="nemnogo-teorii">  Poco de teor√≠a </h2><br><p>  La replicaci√≥n al almacenamiento es un proceso continuo para garantizar la identidad de los datos en m√∫ltiples sistemas de almacenamiento simult√°neamente.  T√©cnicamente, la replicaci√≥n se realiza por dos m√©todos. </p><br><p>  <strong>La replicaci√≥n s√≠ncrona</strong> es la copia de datos del sistema de almacenamiento principal al de respaldo, seguido de la confirmaci√≥n obligatoria de ambos sistemas de almacenamiento de que los datos se registran y confirman.  Es despu√©s de la confirmaci√≥n de ambos lados (en ambos sistemas de almacenamiento) que los datos se consideran registrados, y puede trabajar con ellos.  Esto garantiza una identidad de datos garantizada en todos los sistemas de almacenamiento que participan en la r√©plica. </p><br><p>  Las ventajas de este m√©todo: </p><br><ul><li>  Los datos son siempre id√©nticos en todos los sistemas de almacenamiento. </li></ul><br><p>  Contras: </p><br><ul><li>  Alto costo de la soluci√≥n (canales de comunicaci√≥n r√°pidos, fibra costosa, transceptores de onda larga, etc.) </li><li>  Restricciones de distancia (dentro de unas pocas decenas de kil√≥metros) </li><li>  No hay protecci√≥n contra la corrupci√≥n de datos l√≥gicos (si los datos est√°n da√±ados (a sabiendas o accidentalmente) en el sistema de almacenamiento principal, entonces se da√±ar√°n autom√°tica e inmediatamente en el almacenamiento de respaldo, ya que los datos son siempre id√©nticos (esto es una paradoja) </li></ul><br><p>  <strong>La replicaci√≥n asincr√≥nica</strong> tambi√©n est√° copiando datos del almacenamiento principal a la copia de seguridad, pero con cierto retraso y sin la necesidad de confirmar el registro en el otro lado.  Puede trabajar con datos inmediatamente despu√©s de escribir en el almacenamiento principal, y en el almacenamiento de respaldo, los datos estar√°n disponibles despu√©s de un tiempo.  La identidad de los datos en este caso, por supuesto, no se proporciona en absoluto.  Los datos sobre el almacenamiento de respaldo siempre est√°n un poco "en el pasado". </p><br><p>  Ventajas de la replicaci√≥n asincr√≥nica: </p><br><ul><li>  Bajo costo de soluci√≥n (cualquier canal de comunicaci√≥n, √≥ptica opcional) </li><li>  Sin l√≠mite de distancia </li><li>  Los datos en el almacenamiento de respaldo no se corrompen si se corrompe en el principal (al menos por un tiempo), si los datos se corrompen, siempre puede detener la r√©plica para evitar la corrupci√≥n de datos en el almacenamiento de respaldo </li></ul><br><p>  Contras: </p><br><ul><li>  Los datos en diferentes centros de datos no siempre son id√©nticos </li></ul><br><p>  Por lo tanto, la elecci√≥n del modo de replicaci√≥n depende de las tareas de la empresa.  Si es cr√≠tico para usted que el centro de datos de respaldo tenga exactamente los mismos datos que los datos principales (es decir, el requisito comercial para RPO = 0), tendr√° que desembolsar y soportar las limitaciones de la r√©plica sincr√≥nica.  Y si el retraso en el estado de los datos es permisible o simplemente no hay dinero, entonces, definitivamente, debe usar el m√©todo asincr√≥nico. </p><br><p>  Tambi√©n distinguimos por separado dicho r√©gimen (m√°s precisamente, ya una topolog√≠a) como un grupo metropolitano.  El modo Metrocluster utiliza la replicaci√≥n sincr√≥nica, pero, a diferencia de una r√©plica normal, el metrocluster permite que ambos sistemas de almacenamiento funcionen en modo activo.  Es decir  no tiene una separaci√≥n de centros de datos activos en espera.  Las aplicaciones funcionan simult√°neamente con dos sistemas de almacenamiento que se encuentran f√≠sicamente en diferentes centros de datos.  Los tiempos de inactividad en una topolog√≠a de este tipo son muy peque√±os (RTO, generalmente minutos).  En este art√≠culo, no consideraremos nuestra implementaci√≥n del cl√∫ster de metro, ya que este es un tema muy amplio y de gran capacidad, por lo que dedicaremos un art√≠culo separado y siguiente a esta continuaci√≥n. </p><br><p>  Tambi√©n muy a menudo, cuando hablamos de replicaci√≥n utilizando sistemas de almacenamiento, muchos tienen una pregunta razonable:&gt; ‚ÄúMuchas aplicaciones tienen sus propias herramientas de replicaci√≥n, ¬øpor qu√© usar la replicaci√≥n en sistemas de almacenamiento?  ¬øEs mejor o peor? </p><br><p>  No hay una respuesta √∫nica, as√≠ que aqu√≠ est√°n los pros y los contras: </p><br><p>  Argumentos PARA la replicaci√≥n de almacenamiento: </p><br><ul><li>  La simplicidad de la soluci√≥n.  De una manera, puede replicar una matriz completa de datos, independientemente del tipo de carga o aplicaci√≥n.  Si utiliza una r√©plica de aplicaciones, deber√° configurar cada aplicaci√≥n por separado.  Si hay m√°s de 2, entonces es extremadamente lento y costoso (la replicaci√≥n de la aplicaci√≥n requiere, por regla general, una licencia separada y no gratuita para cada aplicaci√≥n. Pero m√°s sobre eso a continuaci√≥n). </li><li>  Puede replicar cualquier cosa, cualquier aplicaci√≥n, cualquier dato, y siempre ser√°n consistentes.  Muchas (la mayor√≠a) de las aplicaciones no tienen instalaciones de replicaci√≥n, y las r√©plicas desde el lado del almacenamiento son la √∫nica forma de brindar protecci√≥n contra desastres. </li><li>  No es necesario pagar de m√°s por la funcionalidad de replicaci√≥n de aplicaciones.  Como regla, cuesta mucho, al igual que las licencias para un sistema de almacenamiento de r√©plica.  Pero debe pagar la licencia de replicaci√≥n de almacenamiento solo una vez, y debe comprar la licencia para la r√©plica de la aplicaci√≥n para cada aplicaci√≥n por separado.  Si hay muchas de esas aplicaciones, entonces cuesta un centavo y el costo de las licencias para la replicaci√≥n del almacenamiento se convierte en una gota en el cubo. </li></ul><br><p>  Argumentos EN CONTRA de la replicaci√≥n de almacenamiento: </p><br><ul><li>  La r√©plica que usa las herramientas de la aplicaci√≥n tiene m√°s funcionalidad desde el punto de vista de las propias aplicaciones, la aplicaci√≥n conoce mejor sus datos (lo cual es obvio), por lo que hay m√°s opciones para trabajar con ellos. </li><li>  Los fabricantes de algunas aplicaciones no garantizan la consistencia de sus datos si la replicaci√≥n se realiza con herramientas de terceros.  * * </li></ul><br><p>  * - una tesis controvertida.  Por ejemplo, una conocida empresa de fabricaci√≥n de DBMS, durante mucho tiempo declar√≥ oficialmente que su DBMS normalmente se puede replicar solo por sus medios, y el resto de la replicaci√≥n (incluido SHD-shnaya) "no es cierto".  Pero la vida ha demostrado que esto no es as√≠.  Lo m√°s probable (pero esto no es exacto) simplemente no es el intento m√°s honesto de vender m√°s licencias a los clientes. </p><br><p>  Como resultado, en la mayor√≠a de los casos, la replicaci√≥n desde el lado del almacenamiento es mejor, porque  Esta es una opci√≥n m√°s simple y menos costosa, pero hay casos complejos en los que necesita una funcionalidad espec√≠fica de la aplicaci√≥n y necesita trabajar con la replicaci√≥n a nivel de la aplicaci√≥n. </p><br><h2 id="s-teoriey-zakonchili-teper-praktika">  Con la teor√≠a terminada, ahora practica </h2><br><p>  Vamos a configurar una r√©plica en nuestro laboratorio.  En el laboratorio, emulamos dos centros de datos (de hecho, dos bastidores adyacentes que parecen estar en diferentes edificios).  El soporte consta de dos sistemas de almacenamiento Engine N2, que est√°n interconectados por cables √≥pticos.  Un servidor f√≠sico que ejecuta Windows Server 2016 con 10 Gb Ethernet est√° conectado a ambos sistemas de almacenamiento.  El soporte es bastante simple, pero no cambia la esencia. </p><br><p>  Esquem√°ticamente, se ve as√≠: </p><br><p><img src="https://habrastorage.org/webt/wj/u4/rc/wju4rcak9ilbms68pnffyvsb6ly.png"></p><br><p>  L√≥gicamente, la replicaci√≥n se organiza de la siguiente manera: </p><br><p><img src="https://habrastorage.org/webt/yf/yh/dy/yfyhdy19cbj3sc0gpzp8jx6liz0.jpeg"></p><br><p>  Ahora veamos la funcionalidad de replicaci√≥n que tenemos ahora. <br>  Se admiten dos modos: as√≠ncrono y sincr√≥nico.  Es l√≥gico que el modo s√≠ncrono est√© limitado por la distancia y el canal de comunicaci√≥n.  En particular, el modo s√≠ncrono requiere el uso de fibra como f√≠sica y Ethernet de 10 gigabits (o superior). </p><br><p>  La distancia admitida para la replicaci√≥n s√≠ncrona es de 40 kil√≥metros; el retraso del canal √≥ptico entre los centros de datos es de hasta 2 milisegundos.  En general, funcionar√° con grandes retrasos, pero luego habr√° fuertes frenos durante la grabaci√≥n (que tambi√©n es l√≥gico), por lo que si est√° considerando la replicaci√≥n sincr√≥nica entre los centros de datos, debe verificar la calidad de la √≥ptica y los retrasos. </p><br><p>  Los requisitos de replicaci√≥n asincr√≥nica no son tan serios.  M√°s precisamente, no lo son en absoluto.  Cualquier conexi√≥n Ethernet que funcione es adecuada. </p><br><p>  Por el momento, el almacenamiento de AERODISK ENGINE admite la replicaci√≥n de dispositivos de bloque (LUN) utilizando el protocolo Ethernet (cobre u √≥ptica).  Para proyectos que necesariamente requieren replicaci√≥n a trav√©s de la f√°brica SAN de Fibre Channel, ahora estamos completando la soluci√≥n adecuada, pero hasta ahora no est√° lista, por lo que en nuestro caso solo es Ethernet. </p><br><p>  La replicaci√≥n puede funcionar entre cualquier sistema de almacenamiento de la serie ENGINE (N1, N2, N4) desde sistemas inferiores a sistemas anteriores y viceversa. </p><br><p>  La funcionalidad de ambos modos de replicaci√≥n es completamente id√©ntica.  A continuaci√≥n se muestra m√°s sobre lo que es: </p><br><ul><li>  Replicaci√≥n "uno a uno" o "uno a uno", es decir, la versi√≥n cl√°sica con dos centros de datos, el principal y el de respaldo </li><li>  La replicaci√≥n es "uno a muchos" o "uno a muchos", es decir  un LUN se puede replicar en varios sistemas de almacenamiento a la vez </li><li>  Activaci√≥n, desactivaci√≥n e ‚Äúinversi√≥n‚Äù de la replicaci√≥n, respectivamente, para habilitar, deshabilitar o cambiar la direcci√≥n de la replicaci√≥n. </li><li>  La replicaci√≥n est√° disponible para los grupos RDG (Raid Distributed Group) y DDP (Dynamic Disk Pool).  Sin embargo, el LUN del grupo RDG solo se puede replicar en otro RDG.  C DDP es similar. </li></ul><br><p>  Hay muchas m√°s funciones peque√±as, pero enumerarlas no tiene mucho sentido, las mencionaremos durante la configuraci√≥n. </p><br><h2 id="nastroyka-replikacii">  Configuraci√≥n de replicaci√≥n </h2><br><p>  El proceso de configuraci√≥n es bastante simple y consta de tres etapas. </p><br><ol><li>  Configuraci√≥n de red </li><li>  Configuraci√≥n de almacenamiento </li><li>  Configuraci√≥n de reglas (enlaces) y mapeo </li></ol><br><p>  Un punto importante en la configuraci√≥n de la replicaci√≥n es que las dos primeras etapas deben repetirse en un sistema de almacenamiento remoto, la tercera etapa, solo en la principal. </p><br><h3 id="nastroyka-setevyh-resursov">  Configuraci√≥n de recursos de red </h3><br><p>  El primer paso es configurar los puertos de red a trav√©s de los cuales se transmitir√° el tr√°fico de replicaci√≥n.  Para hacer esto, debe habilitar los puertos y establecer direcciones IP en ellos en la secci√≥n Adaptadores front-end. </p><br><p>  Despu√©s de eso, necesitamos crear un grupo (en nuestro caso RDG) e IP virtual para la replicaci√≥n (VIP).  VIP es una direcci√≥n IP flotante que est√° vinculada a dos direcciones "f√≠sicas" de controladores de almacenamiento (los puertos que acabamos de configurar).  Ser√° la interfaz de replicaci√≥n principal.  Tambi√©n puede operar no con VIP, sino con VLAN si necesita trabajar con tr√°fico etiquetado. </p><br><p><img src="https://habrastorage.org/webt/di/ye/5f/diye5fvbzya3cvtebg7memcs5jo.jpeg"></p><br><p>  El proceso de crear un VIP para una r√©plica no es muy diferente de crear un VIP para E / S (NFS, SMB, iSCSI).  En este caso, creamos un VIP (sin VLAN), pero aseg√∫rese de indicar que es para replicaci√≥n (sin este puntero, no podremos agregar VIP a la regla en el siguiente paso). </p><br><p><img src="https://habrastorage.org/webt/nd/i9/2d/ndi92dbmjvxuqjidju802r-vl7s.png"></p><br><p>  VIP debe estar en la misma subred que los puertos IP entre los cuales "flota". </p><br><p><img src="https://habrastorage.org/webt/vm/no/9m/vmno9ms_uas_guk28o1etun7kg4.png"></p><br><p>  Repetimos estas configuraciones en el sistema de almacenamiento remoto, con otro IP-shnik, por s√≠ mismo. <br>  Los VIP de diferentes sistemas de almacenamiento pueden estar en diferentes subredes, lo principal es que debe haber enrutamiento entre ellos.  En nuestro caso, este ejemplo solo se muestra (192.168.3.XX y 192.168.2.XX) </p><br><p><img src="https://habrastorage.org/webt/w5/r6/re/w5r6rexidxqry4rnfdrvgp5gzcq.jpeg"></p><br><p>  En esto, se completa la preparaci√≥n de la parte de la red. </p><br><h3 id="nastraivaem-hranilischa">  Configurar almacenamiento </h3><br><p>  La configuraci√≥n del almacenamiento para una r√©plica difiere de la habitual solo en que hacemos mapeo a trav√©s del men√∫ especial "Mapeo de replicaci√≥n".  De lo contrario, todo es igual que con la configuraci√≥n habitual.  Ahora en orden. </p><br><p>  En el grupo R02 creado anteriormente, debe crear un LUN.  Crear, ll√°malo LUN1. </p><br><p><img src="https://habrastorage.org/webt/tg/l8/vk/tgl8vkdsqf-4oljacfssnh2_zus.jpeg"></p><br><p>  Tambi√©n necesitamos crear el mismo LUN en un sistema de almacenamiento remoto de volumen id√©ntico.  Nosotros creamos  Para evitar confusiones, el LUN remoto se llamar√° LUN1R </p><br><p><img src="https://habrastorage.org/webt/xm/kl/v4/xmklv4deigknjz1vadluit9pdds.jpeg"></p><br><p>  Si necesit√°ramos tomar un LUN que ya existe, en el momento de la configuraci√≥n de la r√©plica, este LUN productivo necesitar√≠a ser desmontado del host, y en el sistema de almacenamiento remoto simplemente cree un LUN vac√≠o de tama√±o id√©ntico. </p><br><p>  La configuraci√≥n de almacenamiento se completa, procedemos a la creaci√≥n de la regla de replicaci√≥n. </p><br><h3 id="nastroyka-pravil-replikacii-ili-replikacionnyh-svyazey">  Configurar reglas de replicaci√≥n o enlaces de replicaci√≥n </h3><br><p>  Despu√©s de crear LUN en el almacenamiento, que ser√° el principal en este momento, configuramos la regla de replicaci√≥n LUN1 en SHD1 en LUN1R en SHD2. </p><br><p>  La configuraci√≥n se realiza en el men√∫ Replicaci√≥n remota. </p><br><p>  Crea una regla.  Para hacer esto, especifique el destinatario de la r√©plica.  Tambi√©n especificamos el nombre de la conexi√≥n y el tipo de replicaci√≥n (s√≠ncrona o as√≠ncrona). </p><br><p><img src="https://habrastorage.org/webt/xk/yu/8f/xkyu8ftgcubcwu4-ul_ux3vc7lq.jpeg"></p><br><p>  En el campo "sistemas remotos", agregue nuestro SHD2.  Para agregar, debe usar el almacenamiento de IP de gesti√≥n (MGR) y el nombre del LUN remoto al que replicaremos (en nuestro caso, LUN1R).  La administraci√≥n de IP solo se necesita en la etapa de agregar comunicaci√≥n; el tr√°fico de replicaci√≥n a trav√©s de ellas no se transmitir√°; para esto, se utilizar√° el VIP configurado previamente. </p><br><p>  Ya en esta etapa, podemos agregar m√°s de un sistema remoto para la topolog√≠a "uno a muchos": haga clic en el bot√≥n "agregar nodo", como en la figura a continuaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/rv/xb/bh/rvxbbh4umovgds3gduoaxg3tfc8.jpeg"></p><br><p>  En nuestro caso, el sistema remoto es uno, por lo que estamos limitados a esto. </p><br><p>  La regla est√° lista.  Tenga en cuenta que se agrega autom√°ticamente a todos los participantes de la replicaci√≥n (en nuestro caso, hay dos de ellos).  Puede crear tantas reglas como desee, para cualquier n√∫mero de LUN y en cualquier direcci√≥n.  Por ejemplo, para equilibrar la carga, podemos replicar parte de los LUN de SHD1 a SHD2, y la otra parte, por el contrario, de SHD2 a SHD1. </p><br><p>  SHD1.  Inmediatamente despu√©s de la creaci√≥n, comenz√≥ la sincronizaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/y7/v8/gg/y7v8gg7bboqpit0zrow87pgvi5y.jpeg"></p><br><p>  SHD2.  Vemos la misma regla, pero la sincronizaci√≥n ya ha finalizado. </p><br><p><img src="https://habrastorage.org/webt/tb/dl/0k/tbdl0k_anxtcwmecg31bk0s7fmo.jpeg"></p><br><p>  LUN1 en SHD1 tiene el rol de Primario, es decir, est√° activo.  LUN1R en SHD2 est√° en el rol de Secundario, es decir, est√° en espera, en caso de falla de SHD1. <br>  Ahora podemos conectar nuestro LUN al host. </p><br><p>  Haremos la conexi√≥n a trav√©s de iSCSI, aunque se puede hacer a trav√©s de FC.  Configurar el mapeo para iSCSI LUN en una r√©plica pr√°cticamente no es diferente del escenario habitual, por lo que no discutiremos esto en detalle aqu√≠.  En todo caso, este proceso se describe en el art√≠culo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Configuraci√≥n r√°pida</a> . </p><br><p>  La √∫nica diferencia es que creamos mapeo en el men√∫ "Mapeo de replicaci√≥n". </p><br><p><img src="https://habrastorage.org/webt/xn/uy/p9/xnuyp9dccwbpg93ahvefmhifmdq.jpeg"></p><br><p>  Configure el mapeo, dele LUN al host.  El anfitri√≥n vio un LUN. </p><br><p><img src="https://habrastorage.org/webt/qg/y3/vm/qgy3vmfark_2-pvl8liqtozb2iu.jpeg"></p><br><p>  Formatee al sistema de archivos local. </p><br><p><img src="https://habrastorage.org/webt/zd/8l/qg/zd8lqglmv194u9-zsctatxrsuzk.jpeg"></p><br><p>  Eso es todo, la configuraci√≥n est√° completa.  A continuaci√≥n ir√°n las pruebas. </p><br><h2 id="testirovanie">  Prueba </h2><br><p>  Probaremos tres escenarios principales. </p><br><ol><li>  Cambio de roles del personal Secundario&gt; Primario.  Es necesario un cambio de rol regular en caso de que, por ejemplo, necesitemos principalmente un centro de datos para realizar algunas operaciones preventivas, y durante este tiempo, para que los datos est√©n disponibles, transferimos la carga al centro de datos de respaldo. </li><li>  Conmutaci√≥n por error de roles Secundario&gt; Primario (falla del centro de datos).  Este es el escenario principal para el que hay replicaci√≥n, que puede ayudar a sobrevivir a una falla completa del centro de datos sin detener a la compa√±√≠a por mucho tiempo. </li><li>  Canales de comunicaci√≥n rotos entre centros de datos.  Verificando el comportamiento correcto de los dos sistemas de almacenamiento en condiciones en que, por alguna raz√≥n, el canal de comunicaci√≥n entre los centros de datos no est√° disponible (por ejemplo, la excavadora cav√≥ en el lugar equivocado y rasg√≥ la √≥ptica oscura). </li></ol><br><p>  Para comenzar, comenzaremos a escribir datos en nuestro LUN (escribimos archivos con datos aleatorios).  Inmediatamente vemos que se est√° utilizando el canal de comunicaci√≥n entre los sistemas de almacenamiento.  Esto es f√°cil de entender si abre la supervisi√≥n de carga de los puertos responsables de la replicaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/s7/99/bt/s799bttjt3v6q24uhvxoywfrwne.jpeg"></p><br><p>  En ambos sistemas de almacenamiento ahora hay datos "√∫tiles", podemos comenzar la prueba. </p><br><p><img src="https://habrastorage.org/webt/r3/vs/dv/r3vsdvpsp9avchablad41pxrfdu.jpeg"></p><br><p>  Por si acaso, echemos un vistazo a las sumas hash de uno de los archivos y an√≥telo. </p><br><p><img src="https://habrastorage.org/webt/e1/zi/st/e1zistvzwlkimqbupxjtgnltc9o.jpeg"></p><br><h3 id="shtatnoe-pereklyuchenie-roley">  Cambio de roles del personal </h3><br><p>  La operaci√≥n de cambiar roles (cambiar la direcci√≥n de replicaci√≥n) se puede realizar desde cualquier sistema de almacenamiento, pero a√∫n debe ir a ambos, ya que deber√° deshabilitar la asignaci√≥n en Primaria y habilitarla en Secundaria (que se convertir√° en Primaria). </p><br><p>  Quiz√°s ahora surge una pregunta razonable: ¬øpor qu√© no automatizar esto?  Respondemos: todo es simple, la replicaci√≥n es una herramienta simple de tolerancia a desastres basada √∫nicamente en operaciones manuales.  Para automatizar estas operaciones, hay un modo de cl√∫ster metropolitano, totalmente automatizado, pero su configuraci√≥n es mucho m√°s complicada.  Escribiremos sobre la configuraci√≥n del cl√∫ster de metro en el pr√≥ximo art√≠culo. </p><br><p>  Deshabilite la asignaci√≥n en el almacenamiento principal para asegurarse de que se detiene la grabaci√≥n. </p><br><p><img src="https://habrastorage.org/webt/jk/j4/1l/jkj41ltsncz2hmqoclkrecqvguy.jpeg"></p><br><p>  Luego, en uno de los sistemas de almacenamiento (no importa, en el primario o en el de respaldo) en el men√∫ Replicaci√≥n remota, seleccione nuestra conexi√≥n REPL1 y haga clic en "Cambiar rol". </p><br><p><img src="https://habrastorage.org/webt/dc/bb/8t/dcbb8tv24xxhofmg_avtodfelas.jpeg"></p><br><p>  Despu√©s de unos segundos, LUN1R (almacenamiento de respaldo) se convierte en Primario. </p><br><p><img src="https://habrastorage.org/webt/-v/hf/l2/-vhfl2g0v20bnfnomxwupf0_9xk.jpeg"></p><br><p>  Hacemos mapeo LUN1R con SHD2. </p><br><p><img src="https://habrastorage.org/webt/lh/uw/cy/lhuwcyscu0quljitysu2pkk35hg.jpeg"></p><br><p>  Despu√©s de eso, nuestra unidad E: se aferra autom√°ticamente al host, solo que esta vez "vol√≥" con LUN1R. </p><br><p>  Por si acaso, compare las cantidades de hash. </p><br><p><img src="https://habrastorage.org/webt/g6/st/qh/g6stqhn-xr0yqlw84t7y4_y5sqm.png"></p><br><p>  Id√©ntico  Prueba aprobada </p><br><h3 id="avariynoe-pereklyuchenie-otkaz-cod-a">  Conmutaci√≥n por error  Falla del centro de datos </h3><br><p>  Por el momento, el almacenamiento principal despu√©s del cambio regular es SHD2 y LUN1R, respectivamente.  Para simular un accidente, apagamos los dos controladores SHD2. <br>  El acceso a √©l ya no es. </p><br><p>  Observamos lo que est√° sucediendo en el almacenamiento 1 (copia de seguridad en este momento). </p><br><p><img src="https://habrastorage.org/webt/ai/oy/jt/aioyjtl8xqmmgngtidigkvoesai.jpeg"></p><br><p>  Vemos que el LUN primario (LUN1R) no est√° disponible.  Apareci√≥ un mensaje de error en los registros, en el panel de informaci√≥n, as√≠ como en la propia regla de replicaci√≥n.  En consecuencia, los datos del host no est√°n disponibles actualmente. </p><br><p>  Cambie el rol de LUN1 a Primario. </p><br><p><img src="https://habrastorage.org/webt/ef/vr/wv/efvrwvemqzysnrtebprwvmqnxw8.jpeg"></p><br><p>  Asignaci√≥n de asuntos al anfitri√≥n. </p><br><p><img src="https://habrastorage.org/webt/o9/es/oj/o9esojg6xcl-uv_wbbj6afkrz18.jpeg"></p><br><p>  Aseg√∫rese de que la unidad E aparezca en el host. </p><br><p><img src="https://habrastorage.org/webt/rg/kj/0s/rgkj0s-0rgoumtmnzk98bd-bgl4.jpeg"></p><br><p>  Comprueba el hash. </p><br><p><img src="https://habrastorage.org/webt/hn/yb/yq/hnybyqjm7w_g1il4bowg1-xgq5y.jpeg"></p><br><p>  Todo esta bien.  El centro de almacenamiento experiment√≥ una ca√≠da en el centro de datos, que estaba activo.  El tiempo aproximado que pasamos conectando la "reversi√≥n" de la replicaci√≥n y conectando el LUN desde el centro de datos de respaldo fue de aproximadamente 3 minutos.  Est√° claro que en el producto real todo es mucho m√°s complicado, y adem√°s de las acciones con sistemas de almacenamiento, debe realizar muchas m√°s operaciones en la red, en los hosts y en las aplicaciones.  Y en la vida, este per√≠odo de tiempo ser√° mucho m√°s largo. </p><br><p>  Aqu√≠ quiero escribir que todo, la prueba se complet√≥ con √©xito, pero no nos apresuremos.  El almacenamiento principal "miente", sabemos que cuando ella "cay√≥", estaba en el papel de Primaria.  ¬øQu√© pasa si ella se enciende de repente?  Habr√° dos roles principales, que es igual a la corrupci√≥n de datos?  Lo comprobaremos ahora. <br>  De repente vamos a encender el almacenamiento subyacente. </p><br><p>  Se carga durante varios minutos y despu√©s de eso vuelve a funcionar despu√©s de una breve sincronizaci√≥n, pero ya en el rol de Secundario. </p><br><p><img src="https://habrastorage.org/webt/27/hu/q6/27huq6b6guby7o-g7_xkz7quugy.jpeg"></p><br><p>  Todo esta bien  El cerebro partido no sucedi√≥.  Pensamos en esto, y siempre despu√©s de que la ca√≠da del sistema de almacenamiento se eleva en el rol de Secundario, independientemente de qu√© rol fuera "en la vida".  Ahora podemos decir con certeza que la prueba de falla del centro de datos fue exitosa. </p><br><h3 id="otkaz-kanalov-svyazi-mezhdu-cod-ami">  Falla de los canales de comunicaci√≥n entre los centros de datos. </h3><br><p>  La tarea principal de esta prueba es asegurarse de que el sistema de almacenamiento no comenzar√° a enloquecer si pierde temporalmente los canales de comunicaci√≥n entre los dos sistemas de almacenamiento y luego vuelve a aparecer. <br>  Entonces  Desconectamos los cables entre los sistemas de almacenamiento (imagine que una excavadora los cav√≥). </p><br><p>  En Primaria vemos que no hay conexi√≥n con Secundaria. </p><br><p><img src="https://habrastorage.org/webt/yh/nf/ar/yhnfarhppjrnbaxotu4ds4szz5c.jpeg"></p><br><p>  En Secundario, vemos que no hay conexi√≥n con Primario. </p><br><p><img src="https://habrastorage.org/webt/f4/k9/7h/f4k97hzr11uh3cytxpsjlq2anly.jpeg"></p><br><p>  Todo funciona bien y seguimos escribiendo datos en el sistema de almacenamiento principal, es decir, ya se garantiza que diferir√°n del sistema de respaldo, es decir, se han "ido". </p><br><p>  En unos minutos estamos arreglando el canal de comunicaci√≥n.  Tan pronto como los sistemas de almacenamiento se hayan visto, la sincronizaci√≥n de datos se activa autom√°ticamente.  No se requiere nada del administrador. </p><br><p><img src="https://habrastorage.org/webt/wo/os/yy/woosyydo-vvbauzsd7lgu4qwfos.jpeg"></p><br><p>     . </p><br><p><img src="https://habrastorage.org/webt/up/ne/es/upneeslicidwf8manqfmlvcaohu.jpeg"></p><br><p>  ,        ,      . </p><br><h2 id="vyvody">  </h2><br><p>    ‚Äì    ,  ,   .       . </p><br><p>        ,  -    .      .   ,        . </p><br><p>                   active-active,       ,       . </p><br><p>   ,       . </p><br><p>   . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/456348/">https://habr.com/ru/post/456348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456338/index.html">13 √∫tiles l√≠neas simples de JavaScript</a></li>
<li><a href="../456340/index.html">Una historia sobre c√≥mo un equipo de freelancers escribe aplicaciones JavaScript de pila completa</a></li>
<li><a href="../456342/index.html">Un idioma para gobernar todo</a></li>
<li><a href="../456344/index.html">¬øPor qu√© ['1', '7', '11']. Map (parseInt) devuelve [1, NaN, 3] en Javascript?</a></li>
<li><a href="../456346/index.html">Hoja de ruta interactiva para estudiantes de desarrollo web</a></li>
<li><a href="../456350/index.html">Eventos digitales en Mosc√∫ del 17 al 23 de junio.</a></li>
<li><a href="../456352/index.html">M√≥dulo inal√°mbrico de comunicaci√≥n de objetos WISE-4000</a></li>
<li><a href="../456354/index.html">¬øC√≥mo recolectamos cajas de TV?</a></li>
<li><a href="../456358/index.html">Los 13 art√≠culos m√°s infames del a√±o pasado</a></li>
<li><a href="../456362/index.html">Dise√±ador de nivel 6: c√≥mo motivamos y desarrollamos dise√±adores</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>