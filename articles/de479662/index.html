<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíñ üë©üèø‚Äçüíº üçì Wie Yandex k√ºnstliche Intelligenz lehrte, Fehler in den Nachrichten zu finden üëèüèª ‚òÇÔ∏è üê∑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir sprechen oft √ºber Technologien und Bibliotheken, die in Yandex entstanden und entstanden sind. Tats√§chlich wenden wir zumindest L√∂sungen von Dritt...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie Yandex k√ºnstliche Intelligenz lehrte, Fehler in den Nachrichten zu finden</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/479662/">  Wir sprechen oft √ºber Technologien und Bibliotheken, die in Yandex entstanden und entstanden sind.  Tats√§chlich wenden wir zumindest L√∂sungen von Drittanbietern an und entwickeln diese. <br><br>  Heute werde ich der Habr-Gemeinde von einem solchen Beispiel erz√§hlen.  Sie erfahren, warum wir dem neuronalen Netz von BERT das Auffinden von Tippfehlern in Nachrichten√ºberschriften beigebracht haben und nicht das vorgefertigte Modell verwendet haben, warum Sie BERT nicht auf mehreren Grafikkarten ausf√ºhren k√∂nnen und wie wir die Schl√ºsselfunktion dieser Technologie - den Aufmerksamkeitsmechanismus - verwendet haben. <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br><a name="habracut"></a><h2>  Herausforderung </h2><br>  Yandex.News ist ein Dienst, der Nachrichten von Ver√∂ffentlichungen sammelt, die mit uns verbunden sind.  Dies sind nicht nur die meistgelesenen und zitierten Mediennachrichten auf der Hauptseite, sondern auch <a href="https://yandex.ru/sport">thematische</a> Abschnitte oder sogar pers√∂nliche Auswahlen aus allen Ver√∂ffentlichungen.  In jedem Fall sind dies Tausende von Websites und Millionen von √úberschriften, aus denen die Maschine alle paar Minuten eine Auswahl treffen muss. <br><br>  Es ist die Maschine, weil wir nie in das Bild des Tages eingreifen: Wir f√ºgen dort keine Nachrichten manuell hinzu, wir entfernen sie nicht von dort (egal wie sehr wir es m√∂chten), wir bearbeiten die √úberschriften nicht.  Um dies wurde bereits viele Exemplare gebrochen.  Ein vollst√§ndig algorithmischer Ansatz hat sowohl Vor- als auch Nachteile.  Etwas, das wir mit Technologie verbessern k√∂nnen, etwas, das wir nicht k√∂nnen.  Auch wenn die √úberschriften Rechtschreibfehler oder Tippfehler enthalten, werden diese nicht korrigiert.  Wir haben den Schlagzeilen die Favoriten der Ver√∂ffentlichungen hinzugef√ºgt, damit klar ist, woher die Nachrichten kommen.  Dies hat teilweise geholfen, aber wir haben uns nicht mit den Fehlern abgefunden und begannen, nach einer M√∂glichkeit zu suchen, sie zu beseitigen, ohne √Ñnderungen am Text vorzunehmen. <br><br>  Wenn es nicht m√∂glich ist, den Fehler zu beheben, k√∂nnen Sie das Ger√§t so trainieren, dass es Header findet, die aufgrund von Fehlern nicht f√ºr die Oberseite geeignet sind.  Dar√ºber hinaus hat sich Yandex seit dem Zeitpunkt, als der Name noch nicht erfunden wurde, auf die russische Morphologie spezialisiert.  Es scheint, dass wir ein neuronales Netzwerk nehmen - und der Punkt liegt im Hut. <br><br><h2>  Die Werkzeuge </h2><br>  Yandex verf√ºgt √ºber die <a href="https://yandex.ru/dev/speller/">Speller-</a> Technologie zum Auffinden und Beheben von Fehlern.  Dank der <a href="https://habr.com/ru/company/yandex/blog/333522/">CatBoost-Bibliothek f√ºr</a> maschinelles Lernen <a href="https://habr.com/ru/company/yandex/blog/333522/">kann</a> Speller nicht <a href="https://habr.com/ru/company/yandex/blog/333522/">wiedererkennbare</a> W√∂rter entschl√ºsseln (‚ÄûAdjektive‚Äú ‚Üí ‚ÄûKlassenkameraden‚Äú) und den Kontext bei der Suche nach Tippfehlern ber√ºcksichtigen (‚ÄûMusik verpassen‚Äú ‚Üí ‚ÄûMusik herunterladen‚Äú).  Es mag scheinen, dass Speller f√ºr unsere Aufgabe ideal ist, aber nein. <br><br>  Der Rechtschreibfehler (intern als Suchw√§chter bezeichnet) wurde bereits auf Architekturebene gesch√§rft, um eine v√∂llig andere Aufgabe zu l√∂sen: Benutzern dabei zu helfen, das richtige Anforderungsformular wiederherzustellen.  Bei der Suche ist es nicht so wichtig, ob die Gro√ü- / Kleinschreibung richtig ausgew√§hlt ist, ein Gro√übuchstabe oder ein Komma angeh√§ngt ist.  Dort ist es f√ºr die Suchanfrage "Haminguel" wichtiger zu erraten, dass die Person Hemingway im Sinn hatte. <br><br>  Fehler in den Schlagzeilen werden von relativ erfahrenen Leuten gemacht, von denen es unwahrscheinlich ist, dass sie Haminguel schreiben.  Aber falsche Zustimmungen (‚Äûder Flug wurde verschoben‚Äú), fehlende W√∂rter (‚Äûder junge Mann hat das Auto ausprobiert‚Äú) und zus√§tzliche Gro√übuchstaben (‚ÄûPr√§sident der Bank‚Äú) sind an der Tagesordnung.  Schlie√ülich gibt es einen formal korrekten Satz "Ich repariere die Gorki-Stra√üe in Pskow", an den sich ein normaler Vormund nicht klammern wird (naja, was ist, wenn dies ein Versprechen des Autors ist?), Aber dies ist offensichtlich eine verdorbene Schlagzeile.  Au√üerdem galt es in den Nachrichten nicht wie in der Suche: Tippfehler und Fehler nicht zu korrigieren, sondern zu erkennen. <br><br>  Wir hatten andere Optionen, zum Beispiel Modelle, die auf DSSM basierten (wenn interessant, sprachen wir in einem Beitrag √ºber <a href="https://habr.com/ru/company/yandex/blog/314222/">den Palekh-Algorithmus</a> kurz √ºber diesen Ansatz), aber sie hatten auch Einschr√§nkungen.  Beispielsweise wurde die Wortreihenfolge nicht perfekt ber√ºcksichtigt. <br><br>  Im Allgemeinen waren vorgefertigte Werkzeuge entweder nicht f√ºr unsere Aufgabe geeignet oder nur begrenzt verf√ºgbar.  Sie m√ºssen also Ihr eigenes Modell erstellen - um Ihr Modell zu trainieren.  Dies war ein guter Grund, mit der BERT-Technologie zu arbeiten, die den Entwicklern ab 2018 zur Verf√ºgung stand und beeindruckende Ergebnisse zeigte. <br><br><h2>  Wir stellen vor: BERT </h2><br>  Das Hauptproblem der modernen Verarbeitung nat√ºrlicher Sprache (NLP) besteht darin, gen√ºgend Beispiele zu finden, die von Personen zum Trainieren eines neuronalen Netzwerks gekennzeichnet wurden.  Wenn Sie Qualit√§tswachstum ben√∂tigen, sollte die Trainingsstichprobe sehr gro√ü sein - Millionen und Milliarden von Beispielen.  Gleichzeitig gibt es in NLP viele Aufgaben, die alle unterschiedlich sind.  Das Sammeln von Daten in √§hnlichen Mengen f√ºr jede Aufgabe ist langwierig, teuer und oft unm√∂glich.  Auch f√ºr die gr√∂√üten Unternehmen der Welt. <br><br>  Es gibt jedoch eine M√∂glichkeit, dieses Problem zu umgehen - mit Hilfe eines Trainings in zwei Schritten.  Zun√§chst wird dem neuronalen Netz eine Sprachstruktur f√ºr eine lange und teure Zeit auf einer riesigen Menge von Milliarden von W√∂rtern beigebracht (dies ist ein Vortraining).  Dann wird das Netzwerk f√ºr eine bestimmte Aufgabe schnell und kosteng√ºnstig verdreht - zum Beispiel, um √úberpr√ºfungen in gute und schlechte zu unterteilen (dies ist eine Feinabstimmung).  Genug von etwa zehntausend in <a href="https://habr.com/ru/company/yandex/blog/305956/">Tolok</a> markierten <a href="https://habr.com/ru/company/yandex/blog/305956/">Exemplaren</a> . <br><br>  Die BERT-Technologie (Bidirectional Encoder Representations from Transformers) basiert auf dieser Idee.  Die Idee selbst ist nicht neu und wurde bereits angewendet, aber es gibt einen signifikanten Unterschied.  Transformer ist eine solche neuronale Netzwerkarchitektur, mit der Sie den gesamten Kontext auf einmal ber√ºcksichtigen k√∂nnen, einschlie√ülich des anderen Satzendes und des Partizip-Umsatzes irgendwo in der Mitte.  Und das ist der Unterschied zu fr√ºheren modischen Architekturen, die den Kontext ber√ºcksichtigten.  Ein neuronales LSTM-Netzwerk hat zum Beispiel bestenfalls eine Kontextl√§nge von zehn W√∂rtern und hier sind es alle 200. <br><br>  Auf <a href="https://github.com/google-research/bert">GitHub</a> sind TensorFlow-Quellcode und sogar ein vorgefertigtes Universalmodell in 102 Sprachen verf√ºgbar, von Russisch bis Volapyuk.  Nehmen Sie anscheinend die L√∂sung aus dem Karton - und erhalten Sie sofort das Ergebnis.  Aber nein <br><br>  Es stellte sich heraus, dass das Universalmodell in russischen Texten eine deutlich geringere Qualit√§t aufwies als das englische Modell und Rekorde in englischen Texten brach (was, wie Sie sehen, logisch ist).  In russischen Texten verlor sie gegen unsere internen Modelle auf DSSM. <br><br>  Okay, Sie k√∂nnen sich vorerziehen - zum Gl√ºck hat Yandex genug russische Texte und Erfahrung im maschinellen Lernen.  Aber es gibt eine Nuance.  Das Lernen dauert ein Jahr! <br><br>  Fakt ist, dass BERT auf Google-Tensor-Prozessoren (TPUs) ausgelegt ist und daher sofort mit nur einer Grafikkarte (GPU) verwendet werden kann.  Und es ist unm√∂glich, die Stirn mit einem <a href="https://github.com/horovod">Horovod</a> zu parallelisieren: Das √úbertragen von 400 Megabyte Daten von Karte zu Karte bei jedem Schritt ist sehr kostspielig, Parallelisierung wird sinnlos.  Was zu tun ist? <br><br><h2>  Optimierung </h2><br>  Sie begannen, nach Ideen und L√∂sungen zu suchen, die die Angelegenheit erheblich beschleunigen k√∂nnten.  Zuallererst haben wir festgestellt, dass jede Zahl in unserem Modell 32 Bit Speicher belegt (das Standard-Float f√ºr Zahlen im Computer).  Es scheint klein zu sein, aber wenn Sie 100 Millionen Gewichte haben, dann ist dies kritisch.  Wir brauchten nicht √ºberall eine solche Genauigkeit, deshalb haben wir beschlossen, die Zahlen teilweise in das 16-Bit-Format umzuwandeln (dies wird als gemischtes Pr√§zisionstraining bezeichnet). <br><br>  Dabei haben wir mit Hilfe vieler Dateien und Kr√ºcken die XLA-Kompilierung unter Berufung auf das damals noch rohe NVIDIA- <a href="https://github.com/google-research/bert/pull/255">Commit</a> geschraubt.  Dank dessen konnten unsere NVIDIA Tesla V100-Karten (von denen ein kleiner Server als Apartment in einem billigen Viertel von Moskau dient) ihr Potenzial dank der 16-Bit-Arithmetik auf Tensor Cores voll ausspielen. <br><br>  Wir haben uns nur f√ºr russischsprachige Schlagzeilen interessiert, aber das mehrsprachige Modell, das wir zugrunde gelegt haben, wurde in Hunderten von Sprachen trainiert, einschlie√ülich eines k√ºnstlichen Volapuks.  W√∂rter aller in den Vektorraum √ºbersetzten Sprachen wurden im Modell gespeichert.  Au√üerdem k√∂nnen Sie sie nicht nehmen und einfach von dort entfernen - ich musste schwitzen, um das W√∂rterbuch zu verkleinern. <br><br>  Und noch eine Sache.  Wenn Sie Wissenschaftler sind und sich Ihr Computer unter dem Tisch befindet, k√∂nnen Sie dort alles f√ºr jede bestimmte Aufgabe neu konfigurieren.  In einer realen Computing-Cloud, in der Tausende von Maschinen auf die gleiche Weise konfiguriert sind, ist es beispielsweise problematisch, den Kernel f√ºr jede neue TensorFlow-Funktion neu zu erstellen.  Aus diesem Grund haben wir viel M√ºhe darauf verwendet, solche Versionen von Paketen zu sammeln, die f√ºr alle neuen Chips geeignet sind und die keine radikale Aktualisierung und Neukonfiguration von Grafikkarten in der Cloud erfordern. <br><br>  Im Allgemeinen dr√ºckte alle S√§fte, wo immer sie konnten.  Und wir haben es geschafft.  Das Jahr wurde zu einer Woche. <br><br><h2>  Schulung </h2><br>  Das Erstellen des richtigen Datensatzes ist normalerweise der schwierigste Teil der Arbeit.  Zun√§chst lernten wir den Klassifikator anhand von drei Millionen mit Tolokern gekennzeichneten √úberschriften.  Es scheint viel zu sein, aber nur 30.000 - mit Tippfehlern.  Wo gibt es weitere Beispiele? <br><br>  Wir haben uns entschieden, welche √úberschriften die Medien selbst korrigieren.  Es gibt mehr als 2 Millionen solcher in der Geschichte von Yandex.News.  Bingo!  Obwohl es zu fr√ºh war, sich zu freuen. <br><br>  Es stellte sich heraus, dass die Medien sehr oft die Schlagzeilen wiederholen, nicht aufgrund von Fehlern.  Neue Details wurden bekannt - und der Herausgeber ersetzte einen korrekten Wortlaut durch einen anderen.  Aus diesem Grund haben wir uns auf Korrekturen mit einem Unterschied zwischen den Versionen von bis zu drei Buchstaben beschr√§nkt (obwohl es hier immer noch einige Ger√§usche gab: Es wurde ‚Äûzwei gefunden‚Äú - es wurde ‚Äûdrei gefunden‚Äú).  Also haben wir eine Million Tippfehler gemacht.  Wir haben zuerst diese gro√üe Auswahl mit Rauschen und dann eine kleine Tolkermarkierung ohne Rauschen untersucht. <br><br><h2>  Qualit√§t </h2><br>  Bei solchen Aufgaben ist es √ºblich, Genauigkeit und Vollst√§ndigkeit zu messen.  In unserem Fall ist Genauigkeit das Verh√§ltnis der korrekten Urteile zu allen Urteilen √ºber einen Fehler in der Kopfzeile.  Vollst√§ndigkeit - der Anteil der Fehlerk√∂pfe, den wir unter allen Fehlerk√∂pfen abgefangen haben.  Sowohl das als auch ein anderer in der idealen Welt sollten sich um 100% bem√ºhen.  Bei maschinellen Lernaufgaben neigen diese Indikatoren jedoch zu Konflikten.  Das hei√üt, je mehr wir die Genauigkeit verdrehen, desto mehr sinkt die Vollst√§ndigkeit.  Umgekehrt. <br><br>  In unserem vorherigen Ansatz basierend auf DSSM haben wir bereits eine Genauigkeit von 95% erreicht (d. H. 5% falsch positive Urteile).  Dies ist bereits ein ziemlich hoher Indikator.  Aus diesem Grund haben wir uns entschlossen, das gleiche Ma√ü an Genauigkeit beizubehalten und zu pr√ºfen, wie sich die Vollst√§ndigkeit mit dem neuen Modell √§ndert.  Und sie sprang von 21 auf 78%.  Und es ist definitiv ein Erfolg. <br><br>  Hier w√§re es m√∂glich, dem ein Ende zu setzen, aber ich erinnere mich an das Versprechen, √ºber Aufmerksamkeit zu sprechen. <br><br><h2>  Neuronales Netz mit Filzstift </h2><br>  Es ist allgemein anerkannt, dass ein neuronales Netzwerk eine solche Black Box ist.  Wir geben etwas an den Eingang und geben etwas an den Ausgang.  Warum und wie ist ein R√§tsel. <br><br>  Diese Einschr√§nkung soll interpretierte neuronale Netze umgehen.  BERT ist einer von ihnen.  Ihre Interpretierbarkeit liegt im Aufmerksamkeitsmechanismus.  Grob gesagt wiederholen wir in jeder Schicht des neuronalen Netzwerks dieselbe Technik: Wir betrachten die benachbarten W√∂rter mit unterschiedlicher ‚ÄûAufmerksamkeit‚Äú und ber√ºcksichtigen die Interaktion mit ihnen.  Wenn ein neuronales Netzwerk beispielsweise das Pronomen "er" verarbeitet, "schaut" es genau auf das Nomen, auf das sich "er" bezieht. <br><br>  Das Bild unten zeigt in verschiedenen Rott√∂nen, auf welche W√∂rter der Token ‚Äûschaut‚Äú, wodurch Informationen √ºber den gesamten Titel f√ºr die letzte Klassifizierungsebene gesammelt werden.  Wenn ein Tippfehler im Wort - Aufmerksamkeit es hervorhebt, wenn die W√∂rter inkonsistent sind - dann beide (und m√∂glicherweise abh√§ngig von ihnen). <br><br><img src="https://habrastorage.org/webt/tt/hg/e0/tthge0hf0fug6jmwdhad_x-kjbg.png"><br><br>  An dieser Stelle kann man √ºbrigens das volle Potenzial der neuronalen Netze erkennen.  Zu keinem Zeitpunkt des Trainings wei√ü unser Modell genau, wo sich der Tippfehler im Beispiel befindet: Es wei√ü nur, dass der gesamte Titel falsch ist.  Und dennoch erf√§hrt sie, dass ‚Äûeine Schule f√ºr 1224 Pl√§tze‚Äú aufgrund einer inkonsistenten Ziffer falsch geschrieben ist, und hebt die Zahl 4 besonders hervor. <br><br>  Wir h√∂rten nicht bei Tippfehlern auf und wandten einen neuen Ansatz an, um nicht nur nach Fehlern zu suchen, sondern auch veraltete Header zu identifizieren.  Aber das ist eine ganz andere Geschichte, mit der wir hoffen, in naher Zukunft nach Habr zur√ºckzukehren. <br><br><h2>  N√ºtzliche Links f√ºr diejenigen, die sich mit dem Thema besch√§ftigen m√∂chten </h2><br><ul><li>  <a href="https://github.com/google-research/bert">TensorFlow-Code und vorgefertigte Modelle f√ºr BERT</a> </li><li>  <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">Open Sourcing BERT: Modernes Pre-Training f√ºr die Verarbeitung nat√ºrlicher Sprachen</a> </li><li>  <a href="https://timdettmers.com/2018/10/17/tpus-vs-gpus-for-transformers-bert/">TPUs vs GPUs f√ºr Transformatoren (BERT)</a> </li><li>  <a href="http://jalammar.github.io/illustrated-transformer/">Der abgebildete Transformator</a> </li><li>  <a href="https://news.developer.nvidia.com/nvidia-achieves-4x-speedup-on-bert-neural-network/">NVIDIA erzielt 4-fache Geschwindigkeit im BERT Neural Network</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de479662/">https://habr.com/ru/post/de479662/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de479646/index.html">Schlechte Ratschl√§ge oder Gr√ºnde, um nach der Mittelstufe weiter Englisch zu lernen</a></li>
<li><a href="../de479650/index.html">Top 12 der interessantesten dynamischen IT-Infografiken</a></li>
<li><a href="../de479654/index.html">Django Vue Generator</a></li>
<li><a href="../de479656/index.html">PostgreSQL Antipatterns: Statistiken rund um den Kopf</a></li>
<li><a href="../de479660/index.html">3. Malware-Analyse mit Check Point Forensics. Sandstrahl-Handy</a></li>
<li><a href="../de479664/index.html">So funktionieren verwaltete Kubernetes und verwaltetes OpenShift in der IBM Cloud. Teil 1 - Architektur und Sicherheit</a></li>
<li><a href="../de479666/index.html">Golang: Worauf verl√§sst sich ein Go-Spezialist in einem Meer von IT-Spezialit√§ten?</a></li>
<li><a href="../de479668/index.html">QA f√ºr Anf√§nger: Wie teste ich eine Rakete oder ein Flugzeug?</a></li>
<li><a href="../de479672/index.html">KANN schn√ºffeln</a></li>
<li><a href="../de479676/index.html">ExtJS 7 und Spring Boot 2. Wie erstelle ich ein SPA, das mit Ihrer API und externen ReactJS-Plugins interagiert?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>