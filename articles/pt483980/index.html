<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôáüèΩ üóÉÔ∏è üòô Cria√ß√£o de infraestrutura de TI tolerante a falhas. Parte 1 - preparando para implantar o cluster oVirt 4.3 üå≥ ü§∏ üë∂üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Os leitores s√£o convidados a familiarizar-se com os princ√≠pios de constru√ß√£o de uma infraestrutura tolerante a falhas de uma pequena empresa em um dat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cria√ß√£o de infraestrutura de TI tolerante a falhas. Parte 1 - preparando para implantar o cluster oVirt 4.3</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/lenvendo/blog/483980/"><p>  Os leitores s√£o convidados a familiarizar-se com os princ√≠pios de constru√ß√£o de uma infraestrutura tolerante a falhas de uma pequena empresa em um data center, que ser√° examinado em detalhes em uma curta s√©rie de artigos. </p><a name="habracut"></a><br><h2 id="vvodnaya-chast">  1. Introdu√ß√£o </h2><br><p>  Sob o <strong>DPC</strong> (Data Processing Center) pode ser entendido: </p><br><ul><li>  possuir rack em sua ‚Äúsala de servidores‚Äù no territ√≥rio da empresa que atenda aos requisitos m√≠nimos para fornecer equipamentos de energia e refrigera√ß√£o, al√©m de ter acesso √† Internet por meio de dois provedores independentes; </li><li>  rack alugado com seu pr√≥prio equipamento localizado neste data center - o chamado  coloca√ß√£o, em conformidade com os padr√µes de N√≠vel III ou IV, e que garante fonte de alimenta√ß√£o confi√°vel, resfriamento e fornece acesso √† Internet tolerante a falhas; </li><li>  equipamento totalmente alugado em um data center de n√≠vel III ou IV. </li></ul><br><p>  Qual op√ß√£o de acomoda√ß√£o escolher - em cada caso, tudo √© individual e geralmente depende de v√°rios fatores principais: </p><br><ul><li>  por que a empresa possui sua pr√≥pria infraestrutura de TI; </li><li>  o que exatamente a empresa deseja da infraestrutura de TI (confiabilidade, escalabilidade, capacidade de gerenciamento etc.); </li><li>  a quantidade de investimento inicial em infraestrutura de TI, bem como que tipo de custos s√£o - capital (o que significa que voc√™ compra seu equipamento) ou opera√ß√£o (o equipamento geralmente √© alugado); </li><li>  Planejando o horizonte da pr√≥pria empresa. </li></ul><br><p>  Muito pode ser escrito sobre os fatores que influenciam a decis√£o da empresa de criar e usar sua infraestrutura de TI, mas nosso objetivo √© mostrar na pr√°tica como criar essa mesma infraestrutura para que seja tolerante a falhas e que seja poss√≠vel economizar dinheiro - reduzir o custo de aquisi√ß√£o de software comercial ou at√© evit√°-lo. </p><br><p>  Como mostra a pr√°tica, n√£o vale a pena economizar em hardware, pois os avarentos pagam duas vezes e at√© muito mais.  Mas, novamente - bom hardware, isso √© apenas uma recomenda√ß√£o e, finalmente, o que exatamente comprar e quanto depende das capacidades da empresa e da "gan√¢ncia" de seu gerenciamento.  Al√©m disso, a palavra "gan√¢ncia" deve ser entendida em um bom sentido da palavra, uma vez que √© melhor investir em ferro na fase inicial, para que mais tarde n√£o haja problemas s√©rios em seu suporte e dimensionamento adicionais, uma vez que o planejamento inicialmente incorreto e a economia excessiva podem levar a futuros mais caro do que ao iniciar um projeto. </p><br><p>  Portanto, os dados iniciais do projeto: </p><br><ul><li>  existe uma empresa que decidiu criar seu pr√≥prio portal da web e colocar suas atividades na Internet; </li><li>  a empresa decidiu alugar um rack para colocar seu equipamento em um bom data center certificado de acordo com o padr√£o de n√≠vel III; </li><li>  a empresa decidiu n√£o economizar muito em hardware e, portanto, comprou o seguinte equipamento com garantias e suporte estendidos: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Lista de equipamentos</b> <div class="spoiler_text"><blockquote><ul><li>  dois servidores f√≠sicos Dell PowerEdge R640 da seguinte maneira: </li><li>  <em>dois processadores Intel Xeon Gold 5120</em> </li><li>  <em>512 Gb de RAM</em> </li><li>  <em>dois discos SAS em RAID1, para instala√ß√£o do SO</em> </li><li>  <em>placa de rede 1G de 4 portas embutida</em> </li><li>  <em>duas placas de rede 10G de 2 portas</em> </li><li>  <em>um FC HBA 16G de 2 portas.</em> </li><li>  Sistema de armazenamento com 2 controladores Dell MD3820f, conectado via FC 16G diretamente aos hosts da Dell; </li><li> dois comutadores do segundo n√≠vel - Cisco WS-C2960RX-48FPS-L empilhados; </li><li>  dois switches da camada 3 - Cisco WS-C3850-24T-E, empilhados; </li><li>  Servidores de rack, UPS, PDU, console - fornecidos pelo data center. </li></ul><br></blockquote></div></div><br><p>  Como podemos ver, o equipamento existente tem boas perspectivas de escala horizontal e vertical, se a empresa puder competir com outras empresas de perfil semelhante na Internet e come√ßar a obter lucro que pode ser investido na expans√£o de recursos para aumentar a concorr√™ncia e aumentar o lucro. </p><br><p>  Que equipamento podemos adicionar se a empresa decidir aumentar o desempenho do nosso cluster de computa√ß√£o: </p><br><ul><li>  temos uma grande reserva para o n√∫mero de portas nos switches 2960X, o que significa que voc√™ pode adicionar mais servidores de hardware; </li><li>  compre dois comutadores FC para conectar sistemas de armazenamento e servidores adicionais a eles; </li><li>  servidores j√° existentes podem ser atualizados - adicione mem√≥ria, substitua os processadores por outros mais eficientes, conecte adaptadores de rede existentes a uma rede 10G; </li><li>  Voc√™ pode adicionar prateleiras de disco adicionais ao armazenamento com o tipo de disco necess√°rio - SAS, SATA ou SSD, dependendo da carga planejada; </li><li>  depois de adicionar comutadores FC, voc√™ pode comprar outro sistema de armazenamento para aumentar ainda mais a capacidade do disco e, se voc√™ comprar a op√ß√£o especial Replica√ß√£o Remota, poder√° configurar a replica√ß√£o de dados entre sistemas de armazenamento no mesmo datacenter e entre datacenters (mas isso j√° est√° al√©m o escopo do artigo); </li><li>  tamb√©m h√° comutadores de terceiro n√≠vel - o Cisco 3850, que pode ser usado como um n√∫cleo tolerante a falhas da rede para roteamento de alta velocidade entre redes internas.  Isso ajudar√° bastante no futuro, √† medida que a infraestrutura interna crescer.  O 3850 tamb√©m possui portas 10G que podem ser ativadas posteriormente ao atualizar o equipamento de rede para 10G. </li></ul><br><p>  Como agora n√£o h√° lugar para virtualiza√ß√£o, √© claro que estaremos na moda. Ainda mais, essa √© uma √≥tima maneira de reduzir o custo de compra de servidores caros para certos elementos de infraestrutura (servidores da Web, bancos de dados etc.), que nem sempre s√£o ideais usado no caso de baixa carga, e √© exatamente isso que ser√° no in√≠cio do lan√ßamento do projeto. </p><br><p>  Al√©m disso, a virtualiza√ß√£o tem muitas outras vantagens que podem ser muito √∫teis para n√≥s: toler√¢ncia a falhas de VM por falha do servidor de hardware, migra√ß√£o ao vivo entre n√≥s de hardware do cluster para manuten√ß√£o, balanceamento de carga manual ou autom√°tico entre n√≥s do cluster, etc. </p><br><p>  Para o hardware adquirido pela empresa, a implanta√ß√£o do cluster VMware vSphere altamente acess√≠vel sugere a si mesma, mas como qualquer software da VMware √© conhecido por seus pre√ßos a cavalo, usaremos o software de gerenciamento de virtualiza√ß√£o absolutamente gratuito - <a href="https://ru.wikipedia.org/wiki/OVirt"><strong>oVirt</strong></a> , com base no qual √© conhecido, mas produto j√° comercial - <a href="https://ru.bmstu.wiki/RHEV_(Red_Hat_Enterprise_Virtualization)"><strong>RHEV</strong></a> . </p><br><p>  O software <strong>oVirt</strong> √© necess√°rio para combinar todos os elementos da infraestrutura, a fim de poder trabalhar convenientemente com m√°quinas virtuais altamente acess√≠veis - bancos de dados, aplicativos da web, proxies, balanceadores, servidores para coleta de logs e an√°lises, etc. n., ou seja, no que consiste o portal da web da nossa empresa. </p><br><p>  Resumindo esta introdu√ß√£o, os seguintes artigos nos aguardam, que na pr√°tica mostrar√£o como implantar toda a infraestrutura de hardware e software da empresa: </p><br><div class="spoiler">  <b class="spoiler_title">Lista de Artigos</b> <div class="spoiler_text"><ul><li>  <strong>Parte 1.</strong> Prepara√ß√£o para implanta√ß√£o do cluster oVirt 4.3. </li><li>  <strong>Parte 2.</strong> Instalando e configurando o cluster oVirt 4.3. </li><li>  <strong>Parte 3.</strong> Organiza√ß√£o do roteamento tolerante a falhas nos roteadores virtuais VyOS. </li><li>  <strong>Parte 4.</strong> Configurando a pilha Cisco 3850, organizando o roteamento da intranet. </li></ul></div></div><br><h2 id="chast-1-podgotovka-k-razvyortyvaniyu-klastera-ovirt-43">  Parte 1. Preparando para implantar o cluster oVirt 4.3 </h2><br><h3 id="bazovaya-nastroyka-hostov">  Configura√ß√£o b√°sica do host </h3><br><p>  Instalar e configurar o sistema operacional √© a etapa mais f√°cil.  Existem muitos artigos sobre como instalar e configurar corretamente o sistema operacional, portanto, n√£o faz sentido tentar divulgar algo exclusivo sobre isso. </p><br><p>  Portanto, temos dois hosts Dell PowerEdge R640, nos quais voc√™ precisa instalar o sistema operacional e realizar pr√©-configura√ß√µes, para us√°-los como hipervisores para executar m√°quinas virtuais no cluster oVirt 4.3. </p><br><p>  Como planejamos usar o software n√£o comercial gratuito oVirt, o <strong>CentOS 7.7 foi</strong> escolhido para a implanta√ß√£o de hosts, embora outros SOs tamb√©m possam ser instalados nos hosts do oVirt: </p><br><ul><li>  constru√ß√£o especial baseada em RHEL, o chamado  <a href="https://www.ovirt.org/documentation/admin-guide/chap-Hosts.html"><strong>oVirt Node</strong></a> ; </li><li>  O sistema operacional Oracle Linux, no ver√£o de 2019, <a href="https://blogs.oracle.com/virtualization/announcing-oracle-linux-virtualization-manager">foi anunciado o</a> suporte para a opera√ß√£o do oVirt. </li></ul><br><p>  Antes de instalar o sistema operacional, √© recomendado: </p><br><ul><li>  Configure a interface de rede iDRAC nos dois hosts </li><li>  atualizar o firmware para BIOS e iDRAC para as vers√µes mais recentes; </li><li>  √© desej√°vel configurar o servidor System Profile no modo Performance; </li><li>  configure o RAID a partir de discos locais (recomendado o RAID1) para instalar o SO no servidor. </li></ul><br><p>  Em seguida, instalamos o sistema operacional no disco criado anteriormente via iDRAC - o processo de instala√ß√£o √© normal, n√£o h√° momentos especiais nele.  O acesso ao console do servidor para iniciar a instala√ß√£o do sistema operacional tamb√©m pode ser obtido pelo iDRAC, embora n√£o exista nada para impedir a conex√£o do monitor, teclado e mouse diretamente ao servidor e a instala√ß√£o do sistema operacional a partir de uma unidade flash. </p><br><p>  Depois de instalar o sistema operacional, execute suas configura√ß√µes iniciais: </p><br><pre><code class="plaintext hljs">systemctl enable network.service systemctl start network.service systemctl status network.service</code> </pre> <br><pre> <code class="plaintext hljs">systemctl stop NetworkManager systemctl disable NetworkManager systemctl status NetworkManager</code> </pre> <br><pre> <code class="plaintext hljs">yum install -y ntp systemctl enable ntpd.service systemctl start ntpd.service</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysconfig/selinux SELINUX=disabled SELINUXTYPE=targeted</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/security/limits.conf * soft nofile 65536 * hard nofile 65536</code> </pre> <br><pre> <code class="plaintext hljs">cat /etc/sysctl.conf vm.max_map_count = 262144 vm.swappiness = 1</code> </pre> <br><p>  <u>Instale o conjunto b√°sico de software</u> </p><br><p>  Para a configura√ß√£o inicial do sistema operacional, voc√™ precisa configurar qualquer interface de rede no servidor para poder acessar a Internet, atualizar o sistema operacional e instalar os pacotes de software necess√°rios.  Isso pode ser feito durante a instala√ß√£o do sistema operacional e depois dele. </p><br><pre> <code class="plaintext hljs">yum -y install epel-release yum update yum -y install bind-utils yum-utils net-tools git htop iotop nmon pciutils sysfsutils sysstat mc nc rsync wget traceroute gzip unzip telnet</code> </pre> <br><p>  Todas as configura√ß√µes acima e um conjunto de software s√£o uma quest√£o de prefer√™ncia pessoal, e esse conjunto √© apenas uma recomenda√ß√£o. </p><br><p>  Como nosso host desempenhar√° o papel de um hypervisor, habilitaremos o perfil de desempenho desejado: </p><br><pre> <code class="plaintext hljs">systemctl enable tuned systemctl start tuned systemctl status tuned</code> </pre> <br><pre> <code class="plaintext hljs">tuned-adm profile tuned-adm profile virtual-host</code> </pre> <br><p>  Voc√™ pode ler mais sobre o perfil de desempenho aqui: " <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-virtualization_tuning_optimization_guide-tuned">Cap√≠tulo 4. tuned and tuned-adm</a> ". </p><br><p>  Depois de instalar o sistema operacional, passamos √† pr√≥xima parte - configurando interfaces de rede nos hosts e na pilha de switches Cisco 2960X. </p><br><h3 id="nastroyka-steka-kommutatorov-cisco-2960x">  Configurando a pilha de switches Cisco 2960X </h3><br><p>  Nosso projeto usar√° os seguintes n√∫meros de VLANs - ou dom√≠nios de difus√£o isolados um do outro, para separar diferentes tipos de tr√°fego: </p><br><p>  <strong>VLAN 10</strong> - Internet <br>  <strong>VLAN 17</strong> - Gerenciamento (iDRAC, armazenamento, gerenciamento de switches) <br>  <strong>VLAN 32</strong> - rede de produ√ß√£o VM <br>  <strong>VLAN 33</strong> - rede de interconex√£o (para contratantes externos) <br>  <strong>VLAN 34</strong> - rede de teste de VM <br>  <strong>VLAN 35</strong> - rede de desenvolvedores de VM <br>  <strong>VLAN 40</strong> - Rede de monitoramento </p><br><p>  Antes de iniciar o trabalho, apresentamos um diagrama no n√≠vel L2, ao qual devemos finalmente chegar: </p><br><p><img src="https://habrastorage.org/webt/in/ak/8a/inak8aoyty8yujo2zagwdehdytg.jpeg"></p><br><p>  Para intera√ß√£o de rede entre os hosts oVirt e m√°quinas virtuais entre si, bem como para gerenciar nosso armazenamento, voc√™ precisa configurar a pilha de switches Cisco 2960X. </p><br><p>  Os hosts Dell possuem placas de rede de 4 portas integradas; portanto, √© recomend√°vel organizar sua conex√£o com o Cisco 2960X usando uma conex√£o de rede tolerante a falhas, usando o agrupamento de portas de rede f√≠sicas em uma interface l√≥gica e o protocolo LACP (802.3ad): </p><br><ul><li>  as duas primeiras portas no host s√£o configuradas no modo de liga√ß√£o e conectadas ao switch 2960X - nessa interface l√≥gica, uma <strong><em>ponte</em></strong> com um endere√ßo para gerenciar o host, monitoramento, comunica√ß√£o com outros hosts no cluster oVirt ser√° configurada e tamb√©m ser√° usada para migra√ß√£o ao vivo de m√°quinas virtuais; </li><li>  as duas segundas portas no host tamb√©m s√£o configuradas no modo de liga√ß√£o e conectadas ao 2960X - nessa interface l√≥gica usando oVirt, ser√£o criadas pontes posteriores (nas VLANs correspondentes) √†s quais as m√°quinas virtuais ser√£o conectadas. </li><li>  ambas as portas de rede, na mesma interface l√≥gica, estar√£o ativas, ou seja,  O tr√°fego neles pode ser transmitido simultaneamente, no modo de balanceamento. </li><li>  as configura√ß√µes de rede nos n√≥s do cluster devem ser absolutamente MESMAS, com exce√ß√£o dos endere√ßos IP. </li></ul><br><p>  <u>Configura√ß√£o b√°sica da <strong>pilha de</strong> switches <strong>2960X</strong> e suas portas</u> </p><br><p>  Anteriormente, nossos comutadores deveriam ser: </p><br><ul><li>  montado em um rack; </li><li>  conectado por dois cabos especiais do comprimento necess√°rio, por exemplo, CAB-STK-E-1M; </li><li>  conectado √† fonte de alimenta√ß√£o; </li><li>  conectado √† esta√ß√£o de trabalho do administrador atrav√©s da porta do console, para sua configura√ß√£o inicial. </li></ul><br><p>  As orienta√ß√µes necess√°rias para isso est√£o dispon√≠veis na <a href="https://www.cisco.com/c/en/us/support/switches/catalyst-2960-x-series-switches/products-installation-guides-list.html">p√°gina oficial</a> do fabricante. </p><br><p>  Ap√≥s executar as etapas acima, configure os comutadores. <br>  O significado de cada equipe n√£o deve ser descriptografado na estrutura deste artigo; se necess√°rio, todas as informa√ß√µes podem ser encontradas de forma independente. <br>  Nosso objetivo √© configurar a pilha de switches o mais r√°pido poss√≠vel e conectar os hosts e as interfaces de gerenciamento de armazenamento a ela. </p><br><p>  1) Conecte-se ao switch principal, entre no modo privilegiado, entre no modo de configura√ß√£o e fa√ßa as configura√ß√µes b√°sicas. </p><br><div class="spoiler">  <b class="spoiler_title">Configura√ß√£o b√°sica do switch:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"> enable configure terminal hostname 2960X no service pad service timestamps debug datetime msec service timestamps log datetime localtime show-timezone msec no service password-encryption service sequence-numbers switch 1 priority 15 switch 2 priority 14 stack-mac persistent timer 0 clock timezone MSK 3 vtp mode transparent ip subnet-zero vlan 17 name Management vlan 32 name PROD vlan 33 name Interconnect vlan 34 name Test vlan 35 name Dev vlan 40 name Monitoring spanning-tree mode rapid-pvst spanning-tree etherchannel guard misconfig spanning-tree portfast bpduguard default spanning-tree extend system-id spanning-tree vlan 1-40 root primary spanning-tree loopguard default vlan internal allocation policy ascending port-channel load-balance src-dst-ip errdisable recovery cause loopback errdisable recovery cause bpduguard errdisable recovery interval 60 line con 0 session-timeout 60 exec-timeout 60 0 logging synchronous line vty 5 15 session-timeout 60 exec-timeout 60 0 logging synchronous ip http server ip http secure-server no vstack interface Vlan1 no ip address shutdown exit</code> </pre></div></div><br><p>  Salvamos a configura√ß√£o com o comando <strong>wr mem</strong> e recarregamos a pilha de switches com o comando <strong>reload</strong> no switch principal 1. </p><br><p>  2) Configure as portas de rede do switch no modo de acesso na VLAN 17, para conectar as interfaces de gerenciamento dos servidores de armazenamento e iDRAC. </p><br><div class="spoiler">  <b class="spoiler_title">Configura√ß√µes da porta de gerenciamento:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface GigabitEthernet1/0/5 description iDRAC - host1 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet1/0/6 description Storage1 - Cntr0/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/5 description iDRAC - host2 switchport access vlan 17 switchport mode access spanning-tree portfast edge interface GigabitEthernet2/0/6 description Storage1 ‚Äì Cntr1/Eth0 switchport access vlan 17 switchport mode access spanning-tree portfast edge exit</code> </pre> </div></div><br><p>  3) Ap√≥s reiniciar a pilha, verifique se ela funciona corretamente: </p><br><div class="spoiler">  <b class="spoiler_title">Verificando o funcionamento da pilha:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">2960X#show switch stack-ring speed Stack Ring Speed : 20G Stack Ring Configuration: Full Stack Ring Protocol : FlexStack 2960X#show switch stack-ports Switch # Port 1 Port 2 -------- ------ ------ 1 Ok Ok 2 Ok Ok 2960X#show switch neighbors Switch # Port 1 Port 2 -------- ------ ------ 1 2 2 2 1 1 2960X#show switch detail Switch/Stack Mac Address : 0cd0.f8e4. Mac persistency wait time: Indefinite H/W Current Switch# Role Mac Address Priority Version State ---------------------------------------------------------- *1 Master 0cd0.f8e4. 15 4 Ready 2 Member 0029.c251. 14 4 Ready Stack Port Status Neighbors Switch# Port 1 Port 2 Port 1 Port 2 -------------------------------------------------------- 1 Ok Ok 2 2 2 Ok Ok 1 1</code> </pre> </div></div><br><p>  4) Configurando o acesso SSH √† pilha 2960X </p><br><p>  Para gerenciamento de pilha remota via SSH, usaremos o IP 172.20.1.10 configurado no SVI (switch virtual interface) <strong>VLAN17</strong> . </p><br><p>  Embora para fins de gerenciamento, √© aconselh√°vel usar uma porta dedicada especial no switch, mas isso √© uma quest√£o de prefer√™ncia e oportunidade pessoal. </p><br><div class="spoiler">  <b class="spoiler_title">Configurando o acesso SSH √† pilha do comutador:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">ip default-gateway 172.20.1.2 interface vlan 17 ip address 172.20.1.10 255.255.255.0 hostname 2960X ip domain-name hw.home-lab.ru no ip domain-lookup clock set 12:47:04 06 Dec 2019 crypto key generate rsa ip ssh version 2 ip ssh time-out 90 line vty 0 4 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh line vty 5 15 session-timeout 60 exec-timeout 60 0 privilege level 15 logging synchronous transport input ssh aaa new-model aaa authentication login default local username cisco privilege 15 secret my_ssh_password</code> </pre> </div></div><br><p>  Configure uma senha para entrar no modo privilegiado: </p><br><pre> <code class="plaintext hljs">enable secret *myenablepassword* service password-encryption</code> </pre> <br><p>  Configure o NTP: </p><br><pre> <code class="plaintext hljs">ntp server 85.21.78.8 prefer ntp server 89.221.207.113 ntp server 185.22.60.71 ntp server 192.36.143.130 ntp server 185.209.85.222 show ntp status show ntp associations show clock detail</code> </pre> <br><p>  5) Configure interfaces l√≥gicas Etherchannel e portas f√≠sicas conectadas aos hosts.  Para facilitar a configura√ß√£o, todas as VLANs dispon√≠veis ser√£o permitidas em todas as interfaces l√≥gicas, mas geralmente √© recomend√°vel configurar apenas o que voc√™ precisa: </p><br><div class="spoiler">  <b class="spoiler_title">Configure as interfaces Etherchannel:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">interface Port-channel1 description EtherChannel with Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel2 description EtherChannel with Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel3 description EtherChannel with Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface Port-channel4 description EtherChannel with Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk spanning-tree portfast edge trunk interface GigabitEthernet1/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet1/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet1/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet1/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active interface GigabitEthernet2/0/1 description Host1-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 1 mode active interface GigabitEthernet2/0/2 description Host2-management switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 2 mode active interface GigabitEthernet2/0/3 description Host1-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 3 mode active interface GigabitEthernet2/0/4 description Host2-VM switchport trunk allowed vlan 10,17,30-40 switchport mode trunk channel-protocol lacp channel-group 4 mode active</code> </pre> </div></div><br><p>  <u>Configura√ß√£o inicial de interfaces de rede para m√°quinas virtuais no <strong>Host1</strong> e <strong>Host2</strong></u> </p><br><p>  Verificamos a disponibilidade dos m√≥dulos necess√°rios para a liga√ß√£o no sistema, instalamos o m√≥dulo para gerenciar pontes: </p><br><pre> <code class="plaintext hljs">modinfo bonding modinfo 8021q yum install bridge-utils</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Configurando a interface l√≥gica BOND1 para m√°quinas virtuais em hosts e suas interfaces f√≠sicas:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1 #DESCRIPTION - management DEVICE=bond1 NAME=bond1 TYPE=Bond IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em2 #DESCRIPTION - management DEVICE=em2 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em3 #DESCRIPTION - management DEVICE=em3 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond1 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Depois de concluir as configura√ß√µes na pilha e nos hosts <strong>2960X</strong> , reiniciamos a rede nos hosts e verificamos a interface l√≥gica. </p><br><ul><li>  no host: </li></ul><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011) Bonding Mode: IEEE 802.3ad Dynamic link aggregation Transmit Hash Policy: layer2+3 (2) MII Status: up MII Polling Interval (ms): 100 Up Delay (ms): 0 Down Delay (ms): 0 ... 802.3ad info LACP rate: fast Min links: 0 Aggregator selection policy (ad_select): stable System priority: 65535 ... Slave Interface: em2 MII Status: up Speed: 1000 Mbps Duplex: full ... Slave Interface: em3 MII Status: up Speed: 1000 Mbps Duplex: full</code> </pre> <br><ul><li>  na <strong>pilha de</strong> switches <strong>2960X</strong> : </li></ul><br><pre> <code class="plaintext hljs">2960X#show lacp internal Flags: S - Device is requesting Slow LACPDUs F - Device is requesting Fast LACPDUs A - Device is in Active mode P - Device is in Passive mode Channel group 1 LACP port Admin Oper Port Port Port Flags State Priority Key Key Number State Gi1/0/1 SA bndl 32768 0x1 0x1 0x102 0x3D Gi2/0/1 SA bndl 32768 0x1 0x1 0x202 0x3D 2960X#sh etherchannel summary Flags: D - down P - bundled in port-channel I - stand-alone s - suspended H - Hot-standby (LACP only) R - Layer3 S - Layer2 U - in use N - not in use, no aggregation f - failed to allocate aggregator M - not in use, minimum links not met m - not in use, port not aggregated due to minimum links not met u - unsuitable for bundling w - waiting to be aggregated d - default port A - formed by Auto LAG Number of channel-groups in use: 11 Number of aggregators: 11 Group Port-channel Protocol Ports ------+-------------+-----------+----------------------------------------------- 1 Po1(SU) LACP Gi1/0/1(P) Gi2/0/1(P)</code> </pre> <br><p>  Configura√ß√£o inicial de interfaces de rede para gerenciar recursos de cluster no <strong>Host1</strong> e <strong>Host2</strong> </p><br><div class="spoiler">  <b class="spoiler_title">Configurando a interface l√≥gica BOND1 para gerenciamento e suas interfaces f√≠sicas nos hosts:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond0 #DESCRIPTION - management DEVICE=bond0 NAME=bond0 TYPE=Bond BONDING_MASTER=yes IPV6INIT=no ONBOOT=yes USERCTL=no NM_CONTROLLED=no BOOTPROTO=none BONDING_OPTS='mode=4 lacp_rate=1 xmit_hash_policy=2' cat /etc/sysconfig/network-scripts/ifcfg-em0 #DESCRIPTION - management DEVICE=em0 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no cat /etc/sysconfig/network-scripts/ifcfg-em1 #DESCRIPTION - management DEVICE=em1 TYPE=Ethernet BOOTPROTO=none ONBOOT=yes MASTER=bond0 SLAVE=yes USERCTL=no NM_CONTROLLED=no</code> </pre> </div></div><br><p>  Depois de concluir as configura√ß√µes na pilha e nos hosts <strong>2960X</strong> , reiniciamos a rede nos hosts e verificamos a interface l√≥gica. </p><br><pre> <code class="plaintext hljs">systemctl restart network cat /proc/net/bonding/bond1 2960X#show lacp internal 2960X#sh etherchannel summary</code> </pre> <br><p>  Configuramos a interface de rede de controle em cada host da <strong>VLAN 17</strong> e a vinculamos √† interface l√≥gica BOND1: </p><br><div class="spoiler">  <b class="spoiler_title">Configurando a VLAN17 no Host1:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.163 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Configurando a VLAN17 no Host2:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /etc/sysconfig/network-scripts/ifcfg-bond1.17 DEVICE=bond1.17 NAME=bond1-vlan17 BOOTPROTO=none ONBOOT=yes USERCTL=no NM_CONTROLLED=no VLAN=yes MTU=1500 IPV4_FAILURE_FATAL=yes IPV6INIT=no IPADDR=172.20.1.164 NETMASK=255.255.255.0 GATEWAY=172.20.1.2 DEFROUTE=yes DNS1=172.20.1.8 DNS2=172.20.1.9 ZONE=public</code> </pre> </div></div><br><p>  Reiniciamos a rede nos hosts e verificamos sua visibilidade entre si. </p><br><p>  Isso conclui a instala√ß√£o da pilha de switches Cisco 2960X e, se tudo foi feito corretamente, agora temos uma conex√£o de rede de todos os elementos de infraestrutura entre si no n√≠vel L2. </p><br><h3 id="nastroyka-shd-dell-md3820f">  Configurar o armazenamento Dell MD3820f </h3><br><p>  Antes de iniciar o trabalho de configura√ß√£o do sistema de armazenamento, ele j√° deve estar conectado √† pilha de switches Cisco <strong>2960X</strong> pelas interfaces de gerenciamento, bem como aos <strong>hosts Host1</strong> e <strong>Host2</strong> atrav√©s do FC. </p><br><p>  O esquema geral de como o armazenamento deve ser conectado √† pilha de switches foi apresentado no cap√≠tulo anterior. </p><br><p>  O esquema de conex√£o do armazenamento no FC aos hosts deve ficar assim: </p><br><p><img src="https://habrastorage.org/webt/el/k8/5a/elk85aqc6ilmxeiowmh6acrkwri.jpeg"></p><br><p>  Durante a conex√£o, √© necess√°rio registrar endere√ßos WWPN para hosts FC HBA conectados √†s portas FC no sistema de armazenamento - isso ser√° necess√°rio para a configura√ß√£o subsequente da liga√ß√£o de hosts a LUNs no sistema de armazenamento. </p><br><p>  Na esta√ß√£o de trabalho do administrador, fa√ßa o download e instale o utilit√°rio de gerenciamento de armazenamento Dell MD3820f - <strong>MDSM</strong> ( <strong>PowerVault Modular Disk Storage Manager</strong> ). <br>  N√≥s nos conectamos a ele atrav√©s de seus endere√ßos IP padr√£o e, em seguida, configuramos nossos endere√ßos da <strong>VLAN17</strong> para controlar os controladores via TCP / IP: </p><br><p>  <strong>Armazenamento1</strong> : </p><br><pre> <code class="plaintext hljs">ControllerA IP - 172.20.1.13, MASK - 255.255.255.0, Gateway - 172.20.1.2 ControllerB IP - 172.20.1.14, MASK - 255.255.255.0, Gateway - 172.20.1.2</code> </pre> <br><p>  Ap√≥s definir os endere√ßos, acesse a interface de gerenciamento de armazenamento e defina uma senha, defina a hora, atualize o firmware para controladores e discos, se necess√°rio, etc. <br>  Como isso √© feito est√° descrito no <a href="https://www.dell.com/support/manuals/ru/ru/rubsdc/powervault-md3820f/mdseriesagpub/introduction%3Fguid%3Dguid-51208376-0df9-48a8-be66-63c1b06512ae%26lang%3Den-us">guia de administra√ß√£o de</a> armazenamento. </p><br><p>  Depois de concluir as configura√ß√µes acima, precisaremos executar apenas algumas a√ß√µes: </p><br><ol><li>  Configure os <strong>identificadores</strong> FC do <strong>host</strong> . </li><li>  Crie um grupo de <strong>hosts</strong> - grupo de <strong>hosts</strong> e adicione nossos dois hosts Dell. </li><li>  Crie um grupo de discos e discos virtuais (ou LUNs) nele, que ser√£o apresentados aos hosts. </li><li>  Configure a apresenta√ß√£o de discos virtuais (ou LUNs) para hosts. </li></ol><br><p>  A adi√ß√£o de novos hosts e identificadores de liga√ß√£o das portas FC do host a eles √© feita atrav√©s do menu - <strong>Mapeamentos de Host</strong> -&gt; <strong>Definir</strong> -&gt; <strong>Hosts ...</strong> <br>  Os endere√ßos WWPN dos hosts FC HBA podem ser encontrados, por exemplo, em um servidor iDRAC. </p><br><p>  Como resultado, devemos obter algo parecido com isto: </p><br><p><img src="https://habrastorage.org/webt/p_/uk/u4/p_uku4o-ewgqpyi0xudxqjesfjc.png"></p><br><p>  A adi√ß√£o de um novo grupo de hosts e a associa√ß√£o de hosts √© feita atrav√©s do menu - <strong>Mapeamentos de Host</strong> -&gt; <strong>Definir</strong> -&gt; <strong>Grupo de Host ...</strong> <br>  Para hosts, selecione o tipo de SO - <strong><em>Linux (DM-MP)</em></strong> . </p><br><p>  Ap√≥s criar o grupo de hosts, na guia <strong>Servi√ßos de Armazenamento e C√≥pia</strong> , crie um grupo de <strong>discos</strong> - <strong>Grupo de Discos</strong> , com o tipo que depende dos requisitos de toler√¢ncia a falhas, por exemplo, RAID10 e nele discos virtuais do tamanho certo: </p><br><p><img src="https://habrastorage.org/webt/ue/g2/kn/ueg2kn0m3usv1kb4tygvx2vajz0.png"></p><br><p>  E, finalmente, a etapa final √© a apresenta√ß√£o de discos virtuais (ou LUNs) para hosts. <br>  Para fazer isso, atrav√©s do menu - <strong>Mapeamentos de Host</strong> -&gt; <strong>Mapeamento Lun</strong> -&gt; <strong>Adicionar ...</strong> fazemos a liga√ß√£o de discos virtuais aos hosts, atribuindo-lhes n√∫meros. </p><br><p>  Tudo deve ficar como nesta imagem: </p><br><p><img src="https://habrastorage.org/webt/db/a-/xq/dba-xqbacgrjxrutupvhvmua7ro.png"></p><br><p>  Terminamos a configura√ß√£o do sistema de armazenamento e, se tudo foi feito corretamente, os hosts devem ver os LUNs apresentados a eles por meio dos HBAs do FC. <br>  Fa√ßa o sistema atualizar as informa√ß√µes sobre as unidades mapeadas: </p><br><pre> <code class="plaintext hljs">ls -la /sys/class/scsi_host/ echo "- - -" &gt; /sys/class/scsi_host/host[0-9]/scan</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Vamos ver quais dispositivos s√£o vis√≠veis em nossos servidores:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">cat /proc/scsi/scsi Attached devices: Host: scsi0 Channel: 02 Id: 00 Lun: 00 Vendor: DELL Model: PERC H330 Mini Rev: 4.29 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi15 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 00 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 01 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 04 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 11 Vendor: DELL Model: MD38xxf Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 Host: scsi18 Channel: 00 Id: 00 Lun: 31 Vendor: DELL Model: Universal Xport Rev: 0825 Type: Direct-Access ANSI SCSI revision: 05 lsscsi [0:2:0:0] disk DELL PERC H330 Mini 4.29 /dev/sda [15:0:0:0] disk DELL MD38xxf 0825 - [15:0:0:1] disk DELL MD38xxf 0825 /dev/sdb [15:0:0:4] disk DELL MD38xxf 0825 /dev/sdc [15:0:0:11] disk DELL MD38xxf 0825 /dev/sdd [15:0:0:31] disk DELL Universal Xport 0825 - [18:0:0:0] disk DELL MD38xxf 0825 - [18:0:0:1] disk DELL MD38xxf 0825 /dev/sdi [18:0:0:4] disk DELL MD38xxf 0825 /dev/sdj [18:0:0:11] disk DELL MD38xxf 0825 /dev/sdk [18:0:0:31] disk DELL Universal Xport 0825 -</code> </pre> </div></div><br><p>  Voc√™ tamb√©m pode configurar o <strong>multipath</strong> nos hosts e, embora ao instalar o oVirt ele possa fazer isso sozinho, √© melhor verificar o MP com anteced√™ncia. </p><br><p>  <u>Instale e configure o DM Multipath</u> </p><br><pre> <code class="plaintext hljs">yum install device-mapper-multipath mpathconf --enable --user_friendly_names y cat /etc/multipath.conf | egrep -v "^\s*(#|$)" defaults { user_friendly_names yes find_multipaths yes } blacklist { wwid 26353900f02796769 devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*" devnode "^hd[az]" }</code> </pre> <br><p>  Instale o servi√ßo MP no in√≠cio autom√°tico e execute-o: </p><br><pre> <code class="plaintext hljs">systemctl enable multipathd &amp;&amp; systemctl restart multipathd</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Verificando informa√ß√µes sobre os m√≥dulos carregados para opera√ß√£o MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">lsmod | grep dm_multipath dm_multipath 27792 6 dm_service_time dm_mod 124407 139 dm_multipath,dm_log,dm_mirror modinfo dm_multipath filename: /lib/modules/3.10.0-957.12.2.el7.x86_64/kernel/drivers/md/dm-multipath.ko.xz license: GPL author: Sistina Software &lt;dm-devel@redhat.com&gt; description: device-mapper multipath target retpoline: Y rhelversion: 7.6 srcversion: 985A03DCAF053D4910E53EE depends: dm-mod intree: Y vermagic: 3.10.0-957.12.2.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key sig_key: A3:2D:39:46:F2:D3:58:EA:52:30:1F:63:37:8A:37:A5:54:03:00:45 sig_hashalgo: sha256</code> </pre> </div></div><br><p>  Consulte o resumo da configura√ß√£o de caminhos m√∫ltiplos existente: </p><br><pre> <code class="plaintext hljs">mpathconf multipath is enabled find_multipaths is disabled user_friendly_names is disabled dm_multipath module is loaded multipathd is running</code> </pre> <br><p>  Ap√≥s adicionar um novo LUN ao armazenamento e apresent√°-lo ao host, √© necess√°rio fazer uma varredura nele conectado ao host HBA. </p><br><pre> <code class="plaintext hljs">systemctl reload multipathd multipath -v2</code> </pre> <br><p>  E, finalmente, verificamos se todos os LUNs foram apresentados no armazenamento para hosts e se todos t√™m dois caminhos. </p><br><div class="spoiler">  <b class="spoiler_title">Verifique a opera√ß√£o MP:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">multipath -ll 3600a098000e4b4b3000003175cec1840 dm-2 DELL ,MD38xxf size=2.0T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:1 sdb 8:16 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:1 sdi 8:128 active ready running 3600a098000e4b48f000002ab5cec1921 dm-6 DELL ,MD38xxf size=10T features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 18:0:0:11 sdk 8:160 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 15:0:0:11 sdd 8:48 active ready running 3600a098000e4b4b3000003c95d171065 dm-3 DELL ,MD38xxf size=150G features='3 queue_if_no_path pg_init_retries 50' hwhandler='1 rdac' wp=rw |-+- policy='service-time 0' prio=14 status=active | `- 15:0:0:4 sdc 8:32 active ready running `-+- policy='service-time 0' prio=9 status=enabled `- 18:0:0:4 sdj 8:144 active ready running</code> </pre> </div></div><br><p>  Como voc√™ pode ver, todos os tr√™s discos virtuais no sistema de armazenamento s√£o vis√≠veis de duas maneiras.  Assim, todo o trabalho preparat√≥rio foi conclu√≠do, o que significa que podemos prosseguir para a parte principal - a configura√ß√£o do cluster oVirt, que ser√° discutido no pr√≥ximo artigo. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt483980/">https://habr.com/ru/post/pt483980/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt483964/index.html">N√£o tenha medo do JSON ou do seu primeiro aplicativo de API</a></li>
<li><a href="../pt483972/index.html">Como usar o Quora para promover seus neg√≥cios</a></li>
<li><a href="../pt483974/index.html">Ceph via iSCSI - ou esquiar em uma rede</a></li>
<li><a href="../pt483976/index.html">Ciberseguran√ßa e amea√ßas para 2020: o que nos espera ap√≥s as f√©rias</a></li>
<li><a href="../pt483978/index.html">Compreendendo o conceito de desenvolvimento moderno de aplicativos Web em 2020</a></li>
<li><a href="../pt483986/index.html">Controlando pensamentos de rob√¥ com o Emotiv Insight</a></li>
<li><a href="../pt483988/index.html">MicroSPA, ou como inventar uma roda quadrada</a></li>
<li><a href="../pt483992/index.html">Por que alguns planetas comem seu c√©u</a></li>
<li><a href="../pt483994/index.html">Realoca√ß√£o de TI em um iate. Da Su√©cia para a Espanha</a></li>
<li><a href="../pt484004/index.html">@Pythonetc dezembro de 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>