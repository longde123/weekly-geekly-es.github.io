<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛔️ 👊🏼 🌱 Determinación de la raza de un perro: un ciclo de desarrollo completo, desde una red neuronal en Python hasta una aplicación en Google Play 📏 🌞 🍇</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El progreso en el campo de las redes neuronales en general y el reconocimiento de patrones en particular ha llevado al hecho de que puede parecer que ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Determinación de la raza de un perro: un ciclo de desarrollo completo, desde una red neuronal en Python hasta una aplicación en Google Play</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/448316/">  El progreso en el campo de las redes neuronales en general y el reconocimiento de patrones en particular ha llevado al hecho de que puede parecer que crear una aplicación de red neuronal para trabajar con imágenes es una tarea rutinaria.  En cierto sentido, lo es: si se te ocurrió una idea relacionada con el reconocimiento de patrones, no dudes que alguien ya escribió algo así.  Todo lo que se requiere de usted es encontrar el código correspondiente en Google y "compilarlo" del autor. <br><br>  Sin embargo, todavía hay numerosos detalles que hacen que la tarea no sea tan irresoluble como ... aburrida, diría yo.  Lleva demasiado tiempo, especialmente si eres un principiante que necesita liderazgo, paso a paso, un proyecto llevado a cabo ante tus ojos y completado de principio a fin.  Sin lo habitual en tales casos, excusas de "saltear esta parte obvia". <br><br>  En este artículo, consideraremos la tarea de crear un Identificador de raza de perro: crearemos y entrenaremos una red neuronal, y luego la trasladaremos a Java para Android y la publicaremos en Google Play. <br><br>  Si desea ver el resultado final, aquí está: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Aplicación NeuroDog</a> en Google Play. <br><br>  Sitio web con mi robótica (en progreso): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">robotics.snowcron.com</a> . <br>  Sitio web con el programa en sí, que incluye una guía: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Guía del usuario de NeuroDog</a> . <br><br>  Y aquí hay una captura de pantalla del programa: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="imagen"><br><br><a name="habracut"></a><br><br><h3>  Declaración del problema. </h3><br><br>  Utilizaremos Keras: una biblioteca de Google para trabajar con redes neuronales.  Esta es una biblioteca de alto nivel, lo que significa que es más fácil de usar en comparación con las alternativas que conozco.  En todo caso, hay muchos libros de texto en Keras en la red, de alta calidad. <br><br>  Utilizaremos CNN - Redes neuronales convolucionales.  CNN (y configuraciones más avanzadas basadas en ellos) son el estándar de facto en reconocimiento de imágenes.  Al mismo tiempo, entrenar una red de este tipo no siempre es fácil: debe elegir la estructura de red correcta, los parámetros de entrenamiento (todos estos índices de aprendizaje, impulso, L1 y L2, etc.).  La tarea requiere importantes recursos informáticos y, por lo tanto, resolverla simplemente pasando por TODOS los parámetros fallará. <br><br>  Esta es una de varias razones por las cuales en la mayoría de los casos usan el llamado "conocimiento de transferencia", en lugar del llamado enfoque "vainilla".  Transfer Knowlege utiliza una red neuronal entrenada por alguien antes que nosotros (por ejemplo, Google) y generalmente para una tarea similar pero diferente.  Tomamos las capas iniciales, reemplazamos las capas finales con nuestro propio clasificador, y funciona, y funciona muy bien. <br><br>  Al principio, tal resultado puede ser sorprendente: ¿cómo es que tomamos una red de Google entrenada para distinguir gatos de sillas y reconoce razas de perros para nosotros?  Para comprender cómo sucede esto, debe comprender los principios básicos del trabajo de las Redes neuronales profundas, incluidos los utilizados para el reconocimiento de patrones. <br><br>  "Alimentamos" a la red una imagen (una matriz de números, es decir) como entrada.  La primera capa analiza la imagen en busca de patrones simples, como "línea horizontal", "arco", etc.  La siguiente capa recibe estos patrones como entrada, y produce patrones de segundo orden, como "pelaje", "esquina del ojo" ... En última instancia, obtenemos un rompecabezas desde el cual podemos reconstruir al perro: lana, dos ojos y una mano humana en los dientes. <br><br>  Todo lo anterior se realizó con la ayuda de capas pre-entrenadas obtenidas por nosotros (por ejemplo, de Google).  A continuación, agregamos nuestras capas y les enseñamos a extraer información de raza de estos patrones.  Suena lógico <br><br>  Para resumir, en este artículo crearemos CNN "vainilla" y varias variantes de "aprendizaje de transferencia" de diferentes tipos de redes.  En cuanto a "vainilla": lo crearé, pero no planeo configurarlo seleccionando parámetros, ya que es mucho más fácil entrenar y configurar redes "pre-entrenadas". <br><br>  Como planeamos enseñar a nuestra red neuronal a reconocer razas de perros, debemos "mostrarle" muestras de varias razas.  Afortunadamente, hay un conjunto de fotografías creadas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí</a> para una tarea similar (el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">original está aquí</a> ). <br><br>  Entonces planeo portar la mejor de las redes recibidas para Android.  Portar redes Kerasov a Android es relativamente simple, bien formalizado y haremos todos los pasos necesarios, por lo que no será difícil reproducir esta parte. <br><br>  Luego publicaremos todo esto en Google Play.  Naturalmente, Google resistirá, por lo que se utilizarán trucos adicionales.  Por ejemplo, el tamaño de nuestra aplicación (debido a una red neuronal voluminosa) será mayor que el tamaño permitido del APK de Android aceptado por Google Play: tendremos que usar paquetes.  Además, Google no mostrará nuestra aplicación en los resultados de búsqueda, esto se puede solucionar registrando etiquetas de búsqueda en la aplicación, o simplemente esperando ... una o dos semanas. <br><br>  Como resultado, obtenemos una aplicación "comercial" completamente funcional (entre comillas, como se presenta de forma gratuita) para Android y que utiliza redes neuronales. <br><br><h3>  Entorno de desarrollo </h3><br><br>  Puede programar Keras de manera diferente, dependiendo del sistema operativo que utilice (se recomienda Ubuntu), la presencia o ausencia de una tarjeta de video, etc.  No hay nada malo en el desarrollo en la computadora local (y, en consecuencia, su configuración), excepto que esta no es la forma más fácil. <br><br>  Primero, instalar y configurar una gran cantidad de herramientas y bibliotecas lleva tiempo, y luego, cuando se lanzan nuevas versiones, tendrá que pasar tiempo nuevamente.  En segundo lugar, las redes neuronales requieren una gran potencia informática para el entrenamiento.  Puede acelerar (10 veces o más) este proceso si usa una GPU ... al momento de escribir este artículo, las GPU más adecuadas para este trabajo cuestan $ 2,000 - $ 7,000.  Y sí, también deben configurarse. <br><br>  Entonces iremos por el otro lado.  El hecho es que Google permite que los erizos pobres como nosotros usen GPU de su clúster: de forma gratuita, para los cálculos relacionados con las redes neuronales, también proporciona un entorno totalmente configurado, todo junto, esto se llama Google Colab.  El servicio le da acceso a Jupiter Notebook con python, Keras y una gran cantidad de otras bibliotecas ya configuradas.  Todo lo que tiene que hacer es obtener una cuenta de Google (obtenga una cuenta de Gmail y esto le dará acceso a todo lo demás). <br><br>  Por el momento, Colab puede ser contratado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aquí</a> , pero conociendo Google, esto puede cambiar en cualquier momento.  Simplemente google Google Colab. <br><br>  El problema obvio con el uso de Colab es que es un servicio WEB.  ¿Cómo accedemos a nuestros datos?  ¿Guardar la red neuronal después del entrenamiento, por ejemplo, descargar datos específicos de nuestra tarea, etc.? <br><br>  Hay varias (al momento de escribir este artículo, tres) diferentes formas, utilizamos la que creo que es más conveniente: usamos Google Drive. <br><br>  Google Drive es un almacenamiento de datos basado en la nube que funciona de manera muy similar a un disco duro normal, y puede mapearse en Google Colab (consulte el código a continuación).  Después de eso, puede trabajar con él como lo haría con archivos en un disco local.  Es decir, por ejemplo, para acceder a las fotos de perros para entrenar nuestra red neuronal, necesitamos subirlos a Google Drive, eso es todo. <br><br><h2>  Crear y entrenar una red neuronal </h2><br><br>  A continuación doy el código en Python, bloque por bloque (del cuaderno de Júpiter).  Puede copiar este código en su Jupiter Notebook y ejecutarlo, bloque por bloque, también, ya que los bloques se pueden ejecutar de forma independiente (por supuesto, las variables definidas en el bloque inicial pueden ser necesarias en el último, pero esta es una dependencia obvia). <br><br><h3>  Inicialización </h3><br><br>  En primer lugar, montemos Google Drive.  Solo dos líneas.  Este código debe ejecutarse solo una vez en una sesión de Colab (por ejemplo, una vez cada 6 horas).  Si lo llama por segunda vez mientras la sesión aún está "activa", se omitirá ya que la unidad ya está montada. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br>  Al principio, se le pedirá que confirme sus intenciones, no hay nada complicado.  Así es como se ve: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ·········· &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br>  Una sección de <i>inclusión</i> completamente estándar;  es posible que algunos de los archivos incluidos no sean necesarios, bueno ... lo siento.  Además, dado que voy a probar diferentes redes neuronales, tendrá que comentar / descomentar algunos de los módulos incluidos para tipos específicos de redes neuronales: por ejemplo, para usar InceptionV3 NN, descomentar la inclusión de InceptionV3 y comentar, por ejemplo, ResNet50.  O no: todo lo que cambia de esto es el tamaño de la memoria utilizada, y eso no es muy fuerte. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br>  En Google Drive, creamos una carpeta para nuestros archivos.  La segunda línea muestra su contenido: <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br>  Como puede ver, las fotos de los perros (copiadas del conjunto de datos de Stanford (ver arriba) en Google Drive) se guardan primero en la carpeta <i>all_images</i> .  Más tarde, los copiaremos en los directorios de <i>trenes, válidos</i> y de <i>prueba</i> .  Guardaremos modelos entrenados en la carpeta de <i>modelos</i> .  En cuanto al archivo labels.csv, esto es parte del conjunto de datos con fotos, contiene una tabla de correspondencia de los nombres de imágenes y razas de perros. <br><br>  Hay muchas pruebas que puede ejecutar para comprender qué obtuvimos exactamente para el uso temporal de Google.  Por ejemplo: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br>  Como puede ver, la GPU está realmente conectada, y si no, necesita encontrar y habilitar esta opción en la configuración de Jupiter Notebook. <br><br>  A continuación, debemos declarar algunas constantes, como el tamaño de las imágenes, etc.  Utilizaremos imágenes con un tamaño de 256x256 píxeles, esta es una imagen lo suficientemente grande como para no perder detalles, y lo suficientemente pequeña como para que todo encaje en la memoria.  Sin embargo, tenga en cuenta que algunos tipos de redes neuronales que utilizaremos esperan imágenes de 224x224 píxeles.  En tales casos, comentamos 256 y descomentamos 224. <br><br>  Se aplicará el mismo enfoque (comentario uno: descomentar) a los nombres de los modelos que guardamos, simplemente porque no queremos sobrescribir archivos que aún pueden ser útiles. <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3>  Carga de datos </h3><br><br>  En primer lugar, <i>carguemos el</i> archivo <i>labels.csv</i> y <i>divídalo</i> en las partes de capacitación y validación.  Tenga en cuenta que todavía no hay una parte de prueba, ya que voy a hacer trampa para obtener más datos de entrenamiento. <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br>  Luego, copie los archivos de imagen a las carpetas de capacitación / validación / prueba, de acuerdo con los nombres de los archivos.  La siguiente función copia los archivos cuyos nombres transferimos a la carpeta especificada. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br>  Como puede ver, solo copiamos un archivo para cada raza de perro como <i>prueba</i> .  Además, al copiar, creamos subcarpetas, una para cada raza.  En consecuencia, las fotografías se copian a las subcarpetas por raza. <br><br>  Esto se hace porque Keras puede trabajar con un directorio de una estructura similar, cargando archivos de imagen según sea necesario, y no todos a la vez, lo que ahorra memoria.  Cargar las 15,000 imágenes a la vez es una mala idea. <br><br>  Tendremos que llamar a esta función solo una vez, ya que copia imágenes, y ya no es necesaria.  En consecuencia, para uso futuro, debemos comentarlo: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br>  Obtenga una lista de razas de perros: <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3>  Procesamiento de imagen </h3><br><br>  Vamos a utilizar la función de biblioteca de Keras llamada ImageDataGenerators.  ImageDataGenerator puede procesar la imagen, escalar, rotar, etc.  También puede aceptar una función de <i>procesamiento</i> que puede procesar imágenes adicionalmente. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br>  Presta atención al siguiente código: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br>  Podemos normalizar (subdatos dentro del rango 0-1 en lugar del 0-255 original) en ImageDataGenerator.  ¿Por qué entonces necesitamos un preprocesador?  Como ejemplo, considere la llamada borrosa (comentada, no la uso): esta es la misma manipulación de imagen personalizada que puede ser arbitraria.  Cualquier cosa, desde contraste hasta HDR. <br><br>  Utilizaremos dos ImageDataGenerators diferentes, uno para capacitación y otro para validación.  La diferencia es que para el entrenamiento necesitamos giros y escalas para aumentar la "variedad" de datos, pero para la validación, no necesitamos, al menos, no en esta tarea. <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3>  Crear una red neuronal </h3><br><br>  Como ya se mencionó, vamos a crear varios tipos de redes neuronales.  Cada vez llamaremos a otra función para crear, incluir otros archivos y, a veces, determinar un tamaño de imagen diferente.  Entonces, para cambiar entre diferentes tipos de redes neuronales, debemos comentar / descomentar el código apropiado. <br><br>  En primer lugar, cree una CNN "vainilla".  No funciona bien, porque decidí no perder el tiempo depurándolo, pero al menos proporciona una base que se puede desarrollar si hay un deseo (por lo general, esta es una mala idea, ya que las redes pre-entrenadas dan el mejor resultado). <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br>  Cuando creamos redes utilizando <i>transferencia de aprendizaje</i> , el procedimiento cambia: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  La creación de otros tipos de redes sigue el mismo patrón: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Advertencia: ganador!  Este NN mostró el mejor resultado: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Otro: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Se pueden usar diferentes tipos de redes neuronales para diferentes tareas.  Por lo tanto, además de los requisitos para la precisión de la predicción, el tamaño puede ser importante (el NN móvil es 5 veces más pequeño que Inception) y la velocidad (si necesitamos el procesamiento en tiempo real de una transmisión de video, entonces la precisión tendrá que ser sacrificada). <br><br><h3>  Entrenamiento de redes neuronales </h3><br><br>  En primer lugar, estamos <i>experimentando</i> , por lo que deberíamos poder eliminar las redes neuronales que hemos guardado, pero que ya no usamos.  La siguiente función elimina NN si existe: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br>  La forma en que creamos y eliminamos redes neuronales es bastante simple y directa.  Primero, elimine.  Cuando llame a <i>borrar</i> (solo), tenga en cuenta que el Jupiter Notebook tiene una función de "ejecutar selección", seleccione solo lo que desea usar y ejecútelo. <br><br>  Luego creamos una red neuronal si su archivo no existía, o <i>cargamos la</i> llamada si existe: por supuesto, no podemos llamar a "eliminar" y luego esperamos que NN exista, así que para usar una red neuronal guardada, no llame a <i>eliminar</i> . <br><br>  En otras palabras, podemos crear un nuevo NN, o usar el existente, dependiendo de la situación y de lo que estamos experimentando actualmente.  Un escenario simple: entrenamos una red neuronal y luego nos fuimos de vacaciones.  Regresaron y Google cerró la sesión, por lo que debemos cargar la guardada anteriormente: comentar "eliminar" y descomentar "cargar". <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  <b>Los puntos de control</b> son un elemento muy importante de nuestro programa.  Podemos crear una serie de funciones que deberían llamarse al final de cada era de entrenamiento y pasarlas al punto de control.  Por ejemplo, puede guardar una red neuronal <i>si</i> muestra resultados mejores que los ya guardados. <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br>  Finalmente, enseñamos la red neuronal en el conjunto de entrenamiento: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br>  Los gráficos de precisión y pérdida para la mejor de las configuraciones son los siguientes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br>  Como puede ver, la red neuronal está aprendiendo, y no está mal. <br><br><h3>  Prueba de red neuronal </h3><br><br>  Después de completar el entrenamiento, debemos probar el resultado;  para esto, NN presenta imágenes que nunca había visto antes, las que copiamos en la carpeta de prueba, una para cada raza de perro. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3>  Exportar una red neuronal a una aplicación Java </h3><br><br>  En primer lugar, necesitamos organizar la carga de la red neuronal desde el disco.  La razón es clara: la exportación se lleva a cabo en otro bloque de código, por lo que lo más probable es que comencemos la exportación por separado, cuando la red neuronal esté en su estado óptimo.  Es decir, inmediatamente antes de la exportación, en la misma ejecución del programa, no entrenaremos la red.  Si usa el código que se muestra aquí, entonces no hay diferencia, se ha seleccionado la red óptima para usted.  Pero si aprenderá algo propio, entonces entrenar todo de nuevo antes de ahorrar es una pérdida de tiempo, si antes de eso lo guardó todo. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  Por la misma razón, para no saltar el código, incluyo los archivos necesarios para exportar aquí.  Nadie te molesta en moverlos al comienzo del programa si tu sentido de la belleza lo requiere: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br>  Una pequeña prueba después de cargar una red neuronal, solo para asegurarse de que todo esté cargado: funciona: <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="imagen"><br><br>  Luego, necesitamos obtener los nombres de las capas de entrada y salida de la red (ya sea esta o la función de creación, debemos “nombrar” explícitamente las capas, lo que no hicimos). <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br>  Usaremos los nombres de las capas de entrada y salida más tarde cuando importamos la red neuronal en una aplicación Java. <br><br>  Otro código de itinerancia en la red para obtener estos datos: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pero no me gusta y no lo recomiendo. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El siguiente código exportará la red neuronal Keras a </font><font style="vertical-align: inherit;">formato </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pb</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , el que capturaremos desde Android.</font></font><br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Llamando a estas funciones para exportar una red neuronal: </font></font><br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> La última línea imprime la estructura de la red neuronal resultante. </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Crear una aplicación de Android usando una red neuronal </font></font></h2><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La exportación de redes neuronales en Android está bien formalizada y no debería causar dificultades. </font><font style="vertical-align: inherit;">Hay, como siempre, de varias maneras, las más populares (al momento de escribir). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En primer lugar, usamos Android Studio para crear un nuevo proyecto. </font><font style="vertical-align: inherit;">Vamos a "cortar esquinas" porque nuestra tarea no es un tutorial de Android. </font><font style="vertical-align: inherit;">Entonces la aplicación contendrá solo una actividad. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como puede ver, agregamos la carpeta "activos" y copiamos nuestra red neuronal (la que exportamos anteriormente).</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Archivo Gradle </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En este archivo, debe realizar varios cambios. </font><font style="vertical-align: inherit;">En primer lugar, necesitamos importar la biblioteca </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tensorflow-android</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Se utiliza para trabajar con Tensorflow (y, en consecuencia, Keras) de Java: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">otro obstáculo obvio: </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">versionCode</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> y </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">versionName</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Cuando la aplicación cambie, deberá cargar nuevas versiones en Google Play. </font><font style="vertical-align: inherit;">Sin cambiar las versiones en gdadle (por ejemplo, 1 -&gt; 2 -&gt; 3 ...) no puede hacer esto, Google dará un error "esta versión ya existe".</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Manifiesto </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En primer lugar, nuestra aplicación será "pesada": la red neuronal de 100 Mb encajará fácilmente en la memoria de los teléfonos celulares modernos, pero abrir una instancia separada para cada foto "compartida" de Facebook es definitivamente una mala idea. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo tanto, prohibimos crear más de una instancia de nuestra aplicación:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al agregar </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">android: launchMode = "singleTask"</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a MainActivity, le decimos a Android que abra (active) una copia existente de la aplicación, en lugar de crear otra instancia. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Luego, debemos incluir nuestra aplicación en la lista, que el sistema muestra cuando alguien "comparte" la imagen:</font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Finalmente, necesitamos solicitar características y permisos que utilizará nuestra aplicación: </font></font><br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Si está familiarizado con la programación para Android, esta parte no debería causar preguntas. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aplicación de diseño. </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Crearemos dos diseños, uno para retrato y otro para paisaje. </font><font style="vertical-align: inherit;">Así es como se ve el </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">diseño de retrato</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lo que agregaremos: un campo grande (vista) para mostrar la imagen, una lista molesta de anuncios (que se muestra cuando se presiona el botón con un hueso), el botón Ayuda, botones para descargar una imagen de Archivo / Galería y capturar desde la cámara, y finalmente (inicialmente oculto) Botón "Proceso" para el procesamiento de imágenes. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="imagen"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La actividad en sí contiene toda la lógica de mostrar y ocultar, así como habilitar / deshabilitar botones, según el estado de la aplicación.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mainactividad </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Esta actividad hereda (extiende) la Actividad estándar de Android: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Considere el código responsable del funcionamiento de la red neuronal. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En primer lugar, la red neuronal acepta Bitmap. </font><font style="vertical-align: inherit;">Inicialmente, este es un mapa de bits grande (de tamaño arbitrario) de la cámara o de un archivo (m_bitmap), luego lo transformamos, lo que lleva a los 256x256 píxeles estándar (m_bitmapForNn). </font><font style="vertical-align: inherit;">También almacenamos el tamaño del mapa de bits (256) en una constante:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Debemos decirle a la red neuronal los nombres de las capas de entrada y salida; </font><font style="vertical-align: inherit;">los recibimos antes (ver listado), pero tenga en cuenta que en su caso pueden diferir:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Luego declaramos una variable para contener el objeto TensofFlow. </font><font style="vertical-align: inherit;">Además, almacenamos la ruta al archivo de red neuronal (que se encuentra en los activos):</font></font><br><br><p></p><pre><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">private TensorFlowInferenceInterface tf;</font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Cadena privada MODEL_PATH = </font></font><font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
	"archivo: ///android_asset/dogs.pb";</font></font><font></font>
</pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Almacenamos las razas de perros en la lista, para que luego se muestren al usuario, y no los índices de la matriz: </font></font><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inicialmente, descargamos Bitmap. </font><font style="vertical-align: inherit;">Sin embargo, la red neuronal espera una serie de valores RGB, y su salida es una serie de probabilidades de que esta raza sea lo que se muestra en la imagen. </font><font style="vertical-align: inherit;">En consecuencia, debemos agregar dos matrices más (tenga en cuenta que 120 es el número de razas de perros presentes en nuestros datos de entrenamiento):</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Descargar la biblioteca de inferencia de tensorflow: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dado que las operaciones de la red neuronal toman tiempo, necesitamos ejecutarlas en un hilo separado, de lo contrario existe la posibilidad de que recibamos un mensaje del sistema "la aplicación no responde", sin mencionar a un usuario insatisfecho. </font></font><br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> En onCreate () de MainActivity, necesitamos agregar onClickListener para el botón "Proceso": </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aquí processImage () solo llama al hilo que describimos anteriormente: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Notas adicionales </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No planeamos discutir los detalles de la programación de IU para Android, ya que esto ciertamente no se aplica a la tarea de portar redes neuronales. </font><font style="vertical-align: inherit;">Sin embargo, una cosa aún vale la pena mencionar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cuando impedimos la creación de instancias adicionales de nuestra aplicación, también rompimos el orden normal de creación y eliminación de actividad (flujo de control): si "comparte" una imagen de Facebook y luego comparte otra, la aplicación no se reiniciará. </font><font style="vertical-align: inherit;">Esto significa que la forma "tradicional" de capturar los datos transferidos en onCreate no será suficiente, ya que no se llamará a onCreate. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aquí se explica cómo resolver este problema: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. En onCreate en MainActivity, llame a la función onSharedIntent:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> También agregamos un controlador para onNewIntent: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aquí está la función onSharedIntent en sí: </font></font><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ahora procesamos los datos transferidos en onCreate (si la aplicación no estaba en la memoria) o en onNewIntent (si se lanzó antes). </font></font><br><br><br><br><br>  Buena suerte<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Si le gustó el artículo, por favor, haga clic en "Me gusta" en todas las formas posibles, también hay botones "sociales" en el </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sitio</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/448316/">https://habr.com/ru/post/448316/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448300/index.html">Conferencia mailto: CLOUD - sobre nubes y alrededor</a></li>
<li><a href="../448302/index.html">La vulnerabilidad en los filtros AdBlock y uBlock permite ejecutar código arbitrario en el lado del usuario</a></li>
<li><a href="../448304/index.html">El libro "Vue.js en acción"</a></li>
<li><a href="../448308/index.html">Data Science Digest (abril de 2019)</a></li>
<li><a href="../448310/index.html">Escribir un bot de telegramas en python usando la biblioteca telebot parte 1</a></li>
<li><a href="../448320/index.html">¿Por qué silicio y por qué CMOS?</a></li>
<li><a href="../448322/index.html">C ++ Rusia 2019: transmisión gratuita de la primera sala y un poco sobre lo que será en la conferencia</a></li>
<li><a href="../448324/index.html">Crea globos planetarios de procedimiento</a></li>
<li><a href="../448326/index.html">Ver a través de ¿Cómo estudiar asignaturas sin romperlas?</a></li>
<li><a href="../448328/index.html">En Moscú mostrará una impresora que imprime órganos y tejidos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>