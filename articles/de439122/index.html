<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöº üë®üèª‚Äçüöí üê≤ Die Einfachheit und Komplexit√§t von Grundelementen oder wie unn√∂tige Vorverarbeitung f√ºr ein neuronales Netzwerk ermittelt werden kann üôáüèæ üë±üèø ü§æüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist der dritte Artikel √ºber die Analyse und Untersuchung von Ellipsen, Dreiecken und anderen geometrischen Formen. 
 Die vorangegangenen Artikel ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die Einfachheit und Komplexit√§t von Grundelementen oder wie unn√∂tige Vorverarbeitung f√ºr ein neuronales Netzwerk ermittelt werden kann</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439122/">  Dies ist der dritte Artikel √ºber die Analyse und Untersuchung von Ellipsen, Dreiecken und anderen geometrischen Formen. <br>  Die vorangegangenen Artikel haben unter den Lesern einige sehr interessante Fragen aufgeworfen, insbesondere zur Komplexit√§t oder Einfachheit bestimmter Trainingssequenzen.  Die Fragen sind tats√§chlich sehr interessant, zum Beispiel, wie viel schwieriger ist es, ein Dreieck zu lernen als ein Viereck oder ein anderes Polygon? <br><br><img src="https://habrastorage.org/webt/3p/l8/bo/3pl8bouhofjpesjyyzbmosjerxw.jpeg"><br><br>  Versuchen wir zu vergleichen, und zum Vergleich haben wir eine gro√üartige Idee, die von Generationen von Studenten getestet wurde: Je k√ºrzer der Spickzettel, desto einfacher die Pr√ºfung. <br><br>  Dieser Artikel ist auch einfach das Ergebnis von Neugier und m√º√üigem Interesse, nichts davon ist in der Praxis anzutreffen und f√ºr praktische Aufgaben gibt es ein paar gro√üartige Ideen, aber es gibt fast nichts zum Kopieren und Einf√ºgen.  Dies ist eine kleine Studie √ºber die Komplexit√§t von Trainingssequenzen. Die Argumentation und der Code des Autors werden vorgestellt. Sie k√∂nnen alles selbst √ºberpr√ºfen / erg√§nzen / √§ndern. <br><br>  Versuchen wir also herauszufinden, welche geometrische Figur f√ºr die Segmentierung komplizierter oder einfacher ist, welcher Vorlesungskurs f√ºr KI verst√§ndlicher ist und besser aufgenommen wird. <a name="habracut"></a><br><br>  Es gibt viele verschiedene geometrische Formen, aber wir werden nur Dreiecke, Vierecke und f√ºnfzackige Sterne vergleichen.  Wir werden eine einfache Methode zum Konstruieren einer Zugsequenz verwenden - wir werden 128x128 monochrome Bilder in vier Teile teilen und zuf√§llig eine Ellipse und zum Beispiel ein Dreieck in diesen Vierteln platzieren.  Wir werden ein Dreieck mit der gleichen Farbe wie die Ellipse erkennen.  Das hei√üt,  Die Aufgabe besteht darin, das Netzwerk zu trainieren, um beispielsweise ein viereckiges Polygon von einer Ellipse zu unterscheiden, die in derselben Farbe gemalt ist.  Hier sind Beispiele von Bildern, die wir studieren werden <br><br><img src="https://habrastorage.org/webt/nu/qo/8i/nuqo8io482lnoa3ukyvjorfrlyo.png"><br><br><img src="https://habrastorage.org/webt/3h/rf/n6/3hrfn6wwnthkepnuqdrjezoyaas.png"><br><br><img src="https://habrastorage.org/webt/fi/_l/zw/fi_lzwaortnx2k4fbb-50l2_8rs.png"><br><br>  Wir werden kein Dreieck und kein Viereck in einem Bild erkennen, wir werden sie getrennt in verschiedenen Z√ºgen vor dem Hintergrund von Interferenzen in Form einer Ellipse erkennen. <br><br>  Nehmen wir das klassische U-Netz und drei Arten von Trainingssequenzen mit Dreiecken, Vierecken und Sternen f√ºr die Forschung. <br><br>  Also gegeben: <br><br><ul><li>  drei Trainingssequenzen von Bild / Masken-Paaren; </li><li>  das Netzwerk.  Gew√∂hnliches U-Netz, das h√§ufig zur Segmentierung verwendet wird. </li></ul><br>  Idee zu testen: <br><br><ul><li>  Bestimmen Sie, welche der Trainingssequenzen ‚Äûschwerer‚Äú zu lernen ist. </li><li>  wie sich einige Vorverarbeitungstechniken auf das Lernen auswirken </li></ul><br>  Beginnen wir mit der Auswahl von 10.000 Bildpaaren von Vierecken mit Ellipsen und Masken und betrachten Sie diese sorgf√§ltig.  Wir sind daran interessiert, wie kurz die Krippe wird und wie lang sie ist. <br><br><div class="spoiler">  <b class="spoiler_title">Wir laden Bibliotheken, wir bestimmen die Gr√∂√üe eines Arrays von Bildern</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> skimage.draw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ellipse, polygon <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input,Conv2D,Conv2DTranspose,MaxPooling2D,concatenate <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization,Activation,Add,Dropout <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.losses <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> binary_crossentropy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> keras w_size = <span class="hljs-number"><span class="hljs-number">128</span></span> train_num = <span class="hljs-number"><span class="hljs-number">10000</span></span> radius_min = <span class="hljs-number"><span class="hljs-number">10</span></span> radius_max = <span class="hljs-number"><span class="hljs-number">20</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Bestimmen Sie die Verlust- und Genauigkeitsfunktionen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_coef</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> y_true_f = K.flatten(y_true) y_pred = K.cast(y_pred, <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) y_pred_f = K.cast(K.greater(K.flatten(y_pred), <span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-string"><span class="hljs-string">'float32'</span></span>) intersection = y_true_f * y_pred_f score = <span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> smooth = <span class="hljs-number"><span class="hljs-number">1.</span></span> y_true_f = K.flatten(y_true) y_pred_f = K.flatten(y_pred) intersection = y_true_f * y_pred_f score = (<span class="hljs-number"><span class="hljs-number">2.</span></span> * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> - score <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bce_dice_loss</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(y_true, y_pred)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_iou_vector</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(A, B)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Numpy version batch_size = A.shape[0] metric = 0.0 for batch in range(batch_size): t, p = A[batch], B[batch] true = np.sum(t) pred = np.sum(p) # deal with empty mask first if true == 0: metric += (pred == 0) continue # non empty mask case. Union is never empty # hence it is safe to divide by its number of pixels intersection = np.sum(t * p) union = true + pred - intersection iou = intersection / union # iou metrric is a stepwise approximation of the real iou over 0.5 iou = np.floor(max(0, (iou - 0.45)*20)) / 10 metric += iou # teake the average over all images in batch metric /= batch_size return metric def my_iou_metric(label, pred): # Tensorflow version return tf.py_func(get_iou_vector, [label, pred &gt; 0.5], tf.float64) from keras.utils.generic_utils import get_custom_objects get_custom_objects().update({'bce_dice_loss': bce_dice_loss }) get_custom_objects().update({'dice_loss': dice_loss }) get_custom_objects().update({'dice_coef': dice_coef }) get_custom_objects().update({'my_iou_metric': my_iou_metric })</span></span></code> </pre><br></div></div><br>  Wir werden die Metrik aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikel verwenden</a> .  Lassen Sie mich die Leser daran erinnern, dass wir die Maske des Pixels vorhersagen werden - dies ist der "Hintergrund" oder das "Viereck" und die Wahrheit oder Falschheit der Vorhersage bewerten.  Das hei√üt,  Die folgenden vier Optionen sind m√∂glich: Wir haben richtig vorausgesagt, dass ein Pixel ein Hintergrund ist, richtig vorausgesagt, dass ein Pixel ein Viereck ist, oder einen Fehler bei der Vorhersage eines ‚ÄûHintergrunds‚Äú oder ‚ÄûVierecks‚Äú gemacht.  Daher sch√§tzen wir f√ºr alle Bilder und alle Pixel die Anzahl aller vier Optionen und berechnen das Ergebnis - dies ist das Ergebnis des Netzwerks.  Und je weniger fehlerhafte Vorhersagen und wahrer, desto genauer das Ergebnis und desto besser das Netzwerk. <br><br>  Wir untersuchen das Netzwerk als ‚ÄûBlack Box‚Äú, wir werden nicht untersuchen, was mit dem Netzwerk im Inneren passiert, wie sich Gewichte √§ndern und wie Gradienten gew√§hlt werden - wir werden sp√§ter beim Vergleich der Netzwerke in die Eingeweide des Netzwerks schauen. <br><br><div class="spoiler">  <b class="spoiler_title">einfaches U-Netz</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_layer, start_neurons)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># 128 -&gt; 64 conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(input_layer) conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(conv1) pool1 = MaxPooling2D((2, 2))(conv1) pool1 = Dropout(0.25)(pool1) # 64 -&gt; 32 conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(pool1) conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(conv2) pool2 = MaxPooling2D((2, 2))(conv2) pool2 = Dropout(0.5)(pool2) # 32 -&gt; 16 conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(pool2) conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(conv3) pool3 = MaxPooling2D((2, 2))(conv3) pool3 = Dropout(0.5)(pool3) # 16 -&gt; 8 conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(pool3) conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(conv4) pool4 = MaxPooling2D((2, 2))(conv4) pool4 = Dropout(0.5)(pool4) # Middle convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(pool4) convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(convm) # 8 -&gt; 16 deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(convm) uconv4 = concatenate([deconv4, conv4]) uconv4 = Dropout(0.5)(uconv4) uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4) uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4) # 16 -&gt; 32 deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(uconv4) uconv3 = concatenate([deconv3, conv3]) uconv3 = Dropout(0.5)(uconv3) uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3) uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3) # 32 -&gt; 64 deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding="same")(uconv3) uconv2 = concatenate([deconv2, conv2]) uconv2 = Dropout(0.5)(uconv2) uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2) uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2) # 64 -&gt; 128 deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding="same")(uconv2) uconv1 = concatenate([deconv1, conv1]) uconv1 = Dropout(0.5)(uconv1) uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1) uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1) uncov1 = Dropout(0.5)(uconv1) output_layer = Conv2D(1, (1,1), padding="same", activation="sigmoid")(uconv1) return output_layer # model input_layer = Input((w_size, w_size, 1)) output_layer = build_model(input_layer, 26) model = Model(input_layer, output_layer) model.compile(loss=bce_dice_loss, optimizer=Adam(lr=1e-4), metrics=[my_iou_metric]) model.summary()</span></span></code> </pre><br></div></div><br>  Die Funktion zum Erzeugen von Bild / Masken-Paaren.  Auf einem Schwarzwei√übild 128x128 gef√ºllt mit zuf√§lligem Rauschen mit einem zuf√§llig ausgew√§hlten aus zwei Bereichen oder 0,0 ... 0,75 oder 0,25..1,0.  W√§hlen Sie zuf√§llig ein Viertel im Bild aus und platzieren Sie eine zuf√§llig ausgerichtete Ellipse. Im anderen Viertel platzieren wir ein Viereck und eine Farbe mit zuf√§lligem Rauschen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_pair</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> img_l = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">1</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img_h = (np.random.sample((w_size, w_size, <span class="hljs-number"><span class="hljs-number">1</span></span>))* <span class="hljs-number"><span class="hljs-number">0.75</span></span> + <span class="hljs-number"><span class="hljs-number">0.25</span></span>).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) img = np.zeros((w_size, w_size, <span class="hljs-number"><span class="hljs-number">2</span></span>), dtype=<span class="hljs-string"><span class="hljs-string">'float'</span></span>) i0_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) i1_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> i0_qua == i1_qua: i1_qua = math.trunc(np.random.sample()*<span class="hljs-number"><span class="hljs-number">4.</span></span>) _qua = np.int(w_size/<span class="hljs-number"><span class="hljs-number">4</span></span>) qua = np.array([[_qua,_qua],[_qua,_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>],[_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>,_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>],[_qua*<span class="hljs-number"><span class="hljs-number">3</span></span>,_qua]]) p = np.random.sample() - <span class="hljs-number"><span class="hljs-number">0.5</span></span> r = qua[i0_qua,<span class="hljs-number"><span class="hljs-number">0</span></span>] c = qua[i0_qua,<span class="hljs-number"><span class="hljs-number">1</span></span>] r_radius = np.random.sample()*(radius_max-radius_min) + radius_min c_radius = np.random.sample()*(radius_max-radius_min) + radius_min rot = np.random.sample()*<span class="hljs-number"><span class="hljs-number">360</span></span> rr, cc = ellipse( r, c, r_radius, c_radius, rotation=np.deg2rad(rot), shape=img_l.shape ) p0 = np.rint(np.random.sample()*(radius_max-radius_min) + radius_min) p1 = qua[i1_qua,<span class="hljs-number"><span class="hljs-number">0</span></span>] - (radius_max-radius_min) p2 = qua[i1_qua,<span class="hljs-number"><span class="hljs-number">1</span></span>] - (radius_max-radius_min) p3 = np.rint(np.random.sample()*radius_min) p4 = np.rint(np.random.sample()*radius_min) p5 = np.rint(np.random.sample()*radius_min) p6 = np.rint(np.random.sample()*radius_min) p7 = np.rint(np.random.sample()*radius_min) p8 = np.rint(np.random.sample()*radius_min) poly = np.array(( (p1, p2), (p1+p3, p2+p4+p0), (p1+p5+p0, p2+p6+p0), (p1+p7+p0, p2+p8), (p1, p2), )) rr_p, cc_p = polygon(poly[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], poly[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], img_l.shape) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> p &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h[rr_p, cc_p] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: img[:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_h.copy() img[rr, cc,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l[rr, cc] img[rr_p, cc_p,:<span class="hljs-number"><span class="hljs-number">1</span></span>] = img_l[rr_p, cc_p] img[:,:,<span class="hljs-number"><span class="hljs-number">1</span></span>] = <span class="hljs-number"><span class="hljs-number">0.</span></span> img[rr_p, cc_p,<span class="hljs-number"><span class="hljs-number">1</span></span>] = <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> img</code> </pre><br>  Lassen Sie uns eine Trainingssequenz von Paaren erstellen, siehe Zufall 10. Ich m√∂chte Sie daran erinnern, dass die Bilder monochrom und grau sind. <br><br><pre> <code class="python hljs">_txy = [next_pair() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(train_num)] f_imgs = np.array(_txy)[:,:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) f_msks = np.array(_txy)[:,:,:,<span class="hljs-number"><span class="hljs-number">1</span></span>:].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_txy) <span class="hljs-comment"><span class="hljs-comment">#    10   fig, axes = plt.subplots(2, 10, figsize=(20, 5)) for k in range(10): kk = np.random.randint(train_num) axes[0,k].set_axis_off() axes[0,k].imshow(f_imgs[kk]) axes[1,k].set_axis_off() axes[1,k].imshow(f_msks[kk].squeeze())</span></span></code> </pre><br><img src="https://habrastorage.org/webt/nu/qo/8i/nuqo8io482lnoa3ukyvjorfrlyo.png"><br><br><h3>  Erster Schritt.  Wir trainieren am Mindeststart </h3><br>  Der erste Schritt unseres Experiments ist einfach. Wir versuchen, das Netzwerk so zu trainieren, dass nur 11 erste Bilder vorhergesagt werden. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">10</span></span> val_len = <span class="hljs-number"><span class="hljs-number">11</span></span> precision = <span class="hljs-number"><span class="hljs-number">0.85</span></span> m0_select = np.zeros((f_imgs.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]), dtype=<span class="hljs-string"><span class="hljs-string">'int'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(val_len): m0_select[k] = <span class="hljs-number"><span class="hljs-number">1</span></span> t = tqdm() <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: fit = model.fit(f_imgs[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], f_msks[m0_select&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>], batch_size=batch_size, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span> ) current_accu = fit.history[<span class="hljs-string"><span class="hljs-string">'my_iou_metric'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] current_loss = fit.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] t.set_description(<span class="hljs-string"><span class="hljs-string">"accuracy {0:6.4f} loss {1:6.4f} "</span></span>.\ format(current_accu, current_loss)) t.update(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> current_accu &gt; precision: <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> t.close()</code> </pre> <br> <code>accuracy 0.8545 loss 0.0674 lenght 11 : : 793it [00:58, 14.79it/s]</code> <br> <br>  Wir haben die ersten 11 aus der Anfangssequenz ausgew√§hlt und das Netzwerk darauf trainiert.  Jetzt spielt es keine Rolle, ob das Netzwerk diese Bilder speziell speichert oder zusammenfasst. Hauptsache, es kann diese 11 Bilder so erkennen, wie wir es brauchen.  Abh√§ngig vom ausgew√§hlten Datensatz und der Genauigkeit kann das Netzwerktraining sehr lange dauern.  Wir haben aber nur wenige Iterationen.  Ich wiederhole, dass es f√ºr uns jetzt nicht wichtig ist, wie und was das Netzwerk gelernt oder gelernt hat. Hauptsache, es hat die etablierte Genauigkeit der Vorhersage erreicht. <br><br><h3>  Starten Sie nun das Hauptexperiment </h3><br>  Wir werden den Spickzettel erstellen, wir werden solche Spickzettel f√ºr alle drei Trainingssequenzen separat erstellen und ihre L√§nge vergleichen.  Wir werden neue Bild / Masken-Paare aus der konstruierten Sequenz nehmen und versuchen, sie durch das trainierte Netzwerk auf der bereits ausgew√§hlten Sequenz vorherzusagen.  Am Anfang sind es nur 11 Bild- / Maskenpaare und das Netzwerk ist trainiert, vielleicht nicht sehr richtig.  Wenn in einem neuen Paar die Maske aus dem Bild mit akzeptabler Genauigkeit vorhergesagt wird, verwerfen wir dieses Paar, es enth√§lt keine neuen Informationen f√ºr das Netzwerk, es kennt die Maske bereits und kann sie aus diesem Bild berechnen.  Wenn die Genauigkeit der Vorhersage nicht ausreicht, f√ºgen wir dieses Bild mit einer Maske zu unserer Sequenz hinzu und beginnen, das Netzwerk zu trainieren, bis ein akzeptables Genauigkeitsergebnis f√ºr die ausgew√§hlte Sequenz erzielt wird.  Das hei√üt,  Dieses Bild enth√§lt neue Informationen und wir f√ºgen sie unserer Trainingssequenz hinzu und extrahieren die darin enthaltenen Informationen durch Training. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">50</span></span> t_batch_size = <span class="hljs-number"><span class="hljs-number">1024</span></span> raw_len = val_len t = tqdm(<span class="hljs-number"><span class="hljs-number">-1</span></span>) id_train = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-comment"><span class="hljs-comment">#id_select = 1 while True: t.set_description("Accuracy {0:6.4f} loss {1:6.4f}\ selected img {2:5d} tested img {3:5d} ". format(current_accu, current_loss, val_len, raw_len)) t.update(1) if id_train == 1: fit = model.fit(f_imgs[m0_select&gt;0], f_msks[m0_select&gt;0], batch_size=batch_size, epochs=1, verbose=0 ) current_accu = fit.history['my_iou_metric'][0] current_loss = fit.history['loss'][0] if current_accu &gt; precision: id_train = 0 else: t_pred = model.predict( f_imgs[raw_len: min(raw_len+t_batch_size,f_imgs.shape[0])], batch_size=batch_size ) for kk in range(t_pred.shape[0]): val_iou = get_iou_vector( f_msks[raw_len+kk].reshape(1,w_size,w_size,1), t_pred[kk].reshape(1,w_size,w_size,1) &gt; 0.5) if val_iou &lt; precision*0.95: new_img_test = 1 m0_select[raw_len+kk] = 1 val_len += 1 break raw_len += (kk+1) id_train = 1 if raw_len &gt;= train_num: break t.close()</span></span></code> </pre><br><pre> <code class="bash hljs">Accuracy 0.9338 loss 0.0266 selected img 1007 tested img 9985 : : 4291it [49:52, 1.73s/it]</code> </pre> <br>  Hier wird Genauigkeit im Sinne von "Genauigkeit" und nicht als Standard-Keras-Metrik verwendet, und das Unterprogramm "my_iou_metric" wird zur Berechnung der Genauigkeit verwendet. <br><br>  Vergleichen Sie nun den Betrieb desselben Netzwerks mit denselben Parametern in einer anderen Sequenz in Dreiecken <br><br><img src="https://habrastorage.org/webt/3h/rf/n6/3hrfn6wwnthkepnuqdrjezoyaas.png"><br><br>  Und wir bekommen ein ganz anderes Ergebnis <br><br><pre> <code class="bash hljs">Accuracy 0.9823 loss 0.0108 selected img 1913 tested img 9995 : : 6343it [2:11:36, 3.03s/it]</code> </pre> <br>  Das Netzwerk w√§hlte 1913 Bilder mit "neuen" Informationen aus, d.h.  Der Inhalt von Bildern mit Dreiecken ist halb so hoch wie bei Vierecken! <br><br>  Lassen Sie uns dasselbe auf den Sternen √ºberpr√ºfen und das Netzwerk in der dritten Sequenz ausf√ºhren <br><br><img src="https://habrastorage.org/webt/fi/_l/zw/fi_lzwaortnx2k4fbb-50l2_8rs.png"><br><br>  wir bekommen <br><br><pre> <code class="bash hljs">Accuracy 0.8985 loss 0.0478 selected img 476 tested img 9985 : : 2188it [16:13, 1.16it/s]</code> </pre> <br>  Wie Sie sehen k√∂nnen, erwiesen sich die Sterne als die informativsten, nur 476 Bilder in einem Spickzettel. <br><br>  Wir hatten Grund, die Komplexit√§t geometrischer Formen f√ºr die Wahrnehmung anhand ihres neuronalen Netzwerks zu beurteilen.  Am einfachsten ist der Stern mit nur 476 Bildern im Spickzettel, dann das Viereck mit seinen 1007 und das komplexeste als Dreieck - f√ºr das Training ben√∂tigen Sie 1913 Bilder. <br><br>  Denken Sie daran, dies ist f√ºr uns, f√ºr Menschen ist es ein Bild, aber f√ºr das Netzwerk ist es ein Vorlesungskurs √ºber Anerkennung und der Kurs √ºber Dreiecke erwies sich als der schwierigste. <br><br><h3>  Nun zum Ernst </h3><br>  Auf den ersten Blick scheinen sich all diese Ellipsen und Dreiecke verw√∂hnen zu lassen, Sandkuchen und Lego.  Aber hier ist eine spezifische und ernste Frage: Wenn wir eine Art Vorverarbeitung anwenden, filtern Sie auf die urspr√ºngliche Sequenz, wie wird sich die Komplexit√§t der Sequenz √§ndern?  Zum Beispiel nehmen wir alle gleichen Ellipsen und Vierecke und wenden eine solche Vorverarbeitung auf sie an <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.ndimage <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gaussian_filter _tmp = [gaussian_filter(idx, sigma = <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f_imgs] f1_imgs = np.array(_tmp)[:,:,:,:<span class="hljs-number"><span class="hljs-number">1</span></span>].reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>,w_size ,w_size ,<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">del</span></span>(_tmp) fig, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>): kk = np.random.randint(train_num) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">0</span></span>,k].imshow(f1_imgs[kk].squeeze(), cmap=<span class="hljs-string"><span class="hljs-string">"gray"</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].set_axis_off() axes[<span class="hljs-number"><span class="hljs-number">1</span></span>,k].imshow(f_msks[kk].squeeze(), cmap=<span class="hljs-string"><span class="hljs-string">"gray"</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/76/mb/0f/76mb0fpk1weknaahb8fyxn6cevk.png"><br><br>  Auf den ersten Blick ist alles gleich, die gleichen Ellipsen, die gleichen Polygone, aber das Netzwerk begann auf ganz andere Weise zu funktionieren: <br><br><pre> <code class="bash hljs">Accuracy 1.0575 loss 0.0011 selected img 7963 tested img 9999 : : 17765it [29:02:00, 12.40s/it]</code> </pre> <br>  Hier ist eine kleine Erkl√§rung n√∂tig, wir verwenden keine Augmentation, weil  Die Polygonform und die Ellipsenform werden anf√§nglich zuf√§llig ausgew√§hlt.  Daher liefert die Erweiterung keine neuen Informationen und ist in diesem Fall nicht sinnvoll. <br><br>  Wie aus dem Ergebnis der Arbeit hervorgeht, verursachte ein einfacher gaussian_filter viele Probleme f√ºr das Netzwerk und erzeugte viele neue und wahrscheinlich √ºberfl√ºssige Informationen. <br><br>  Nun, f√ºr Liebhaber der Einfachheit in ihrer reinsten Form nehmen wir die gleichen Ellipsen mit Polygonen, aber ohne zuf√§llige Farbgebung <br><br><img src="https://habrastorage.org/webt/8x/7b/vd/8x7bvdqpavgkjuubnk-2ug-kjt4.png"><br><br>  Das Ergebnis legt nahe, dass zuf√§llige Farben √ºberhaupt keine einfache Erg√§nzung sind. <br><br><pre> <code class="bash hljs">Accuracy 0.9004 loss 0.0315 selected img 251 tested img 9832 : : 1000it [06:46, 1.33it/s]</code> </pre><br>  Das Netzwerk war die Informationen aus 251 Bildern absolut wert, fast viermal weniger als aus vielen mit Rauschen gemalten Bildern. <br><br>  Der Zweck des Artikels ist es, einige Werkzeuge und Beispiele seiner Arbeit an leichtfertigen Beispielen zu zeigen, das Lego im Sandkasten.  Wir haben ein Tool zum Vergleichen von zwei Trainingssequenzen. Wir k√∂nnen bewerten, wie sehr unsere Vorverarbeitung die Trainingssequenz kompliziert oder vereinfacht, wie einfach dieses oder jenes Grundelement in der Trainingssequenz zu erkennen ist. <br><br>  Die M√∂glichkeit, dieses Lego-Beispiel in realen F√§llen anzuwenden, liegt auf der Hand, aber die realen Schulungen und die Netzwerke der Leser liegen bei den Lesern selbst. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de439122/">https://habr.com/ru/post/de439122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de439112/index.html">Der Sch√∂pfer von Kate Mobile wurde wegen P√§dophiler verhaftet, der seinen Dienst in Anspruch nahm</a></li>
<li><a href="../de439114/index.html">Vom Chaos zur Ordnung oder "Erstellen Sie eine Projektstruktur in Unity und nicht nur ..."</a></li>
<li><a href="../de439116/index.html">Andrey Game: F√ºrchte die technologische Krise</a></li>
<li><a href="../de439118/index.html">Neu in Browsern: Firefox 66 blockiert standardm√§√üig Video und Ton, Chromium begrenzt das Seitenbudget</a></li>
<li><a href="../de439120/index.html">Funktionsanforderungen und Produktanforderungen</a></li>
<li><a href="../de439124/index.html">Wenn die Software mit √∂ffentlichen Geldern erstellt wird, muss der Code offen sein</a></li>
<li><a href="../de439128/index.html">So organisieren Sie die Arbeit der Qualit√§tssicherung. Ein praktischer Weg</a></li>
<li><a href="../de439130/index.html">13 Markttrends f√ºr Cybersicherheit und Informationssicherheit 2019-2020</a></li>
<li><a href="../de439132/index.html">Unvergessliches Alter</a></li>
<li><a href="../de439136/index.html">Was ist der Unterschied zwischen 4G und 5G?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>