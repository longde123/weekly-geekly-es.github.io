<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üì¶ ü§ôüèª üîî vSAN en la nube de VMware üõÄüèæ üë®üèº‚Äçüç≥ üéÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las tareas de almacenamiento y acceso a datos son un punto de dolor para cualquier sistema de informaci√≥n. Incluso un sistema de almacenamiento bien d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>vSAN en la nube de VMware</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/418753/"><img src="https://habrastorage.org/webt/pl/tj/to/pltjtoutsishswrt2dgxaktv5ka.png"><br><br>  Las tareas de almacenamiento y acceso a datos son un punto de dolor para cualquier sistema de informaci√≥n.  Incluso un sistema de almacenamiento bien dise√±ado (en lo sucesivo denominado SHD) durante la operaci√≥n revela problemas asociados con un rendimiento reducido.  Se debe prestar especial atenci√≥n a un conjunto de problemas de escala cuando la cantidad de recursos involucrados se acerca a los l√≠mites establecidos establecidos por los desarrolladores de almacenamiento. <br><br>  La raz√≥n fundamental de la aparici√≥n de estos problemas es la arquitectura tradicional basada en un enlace estricto con las caracter√≠sticas de hardware de los dispositivos de almacenamiento utilizados.  La mayor√≠a de los clientes a√∫n eligen el m√©todo de almacenamiento y acceso a los datos, teniendo en cuenta las caracter√≠sticas de las interfaces f√≠sicas (SAS / SATA / SCSI) y no las necesidades reales de las aplicaciones utilizadas. <br><a name="habracut"></a><br>  Hace una docena de a√±os, esta fue una decisi√≥n l√≥gica.  Los administradores del sistema seleccionaron cuidadosamente los dispositivos de almacenamiento de informaci√≥n con las especificaciones requeridas, por ejemplo, SATA / SAS, y contaron con la obtenci√≥n de un nivel de rendimiento basado en las capacidades de hardware de los controladores de disco.  La lucha fue por el volumen de cach√©s del controlador RAID y por las opciones que evitan la p√©rdida de datos.  Ahora este enfoque para resolver el problema no es √≥ptimo. <br><br>  En el entorno actual, al elegir sistemas de almacenamiento, tiene sentido comenzar no desde interfaces f√≠sicas, sino desde el rendimiento expresado en IOPS (el n√∫mero de operaciones de E / S por segundo).  El uso de la virtualizaci√≥n le permite utilizar de manera flexible los recursos de hardware existentes y garantizar el nivel de rendimiento requerido.  Por nuestra parte, estamos listos para proporcionar recursos con esas caracter√≠sticas que son realmente necesarias para la aplicaci√≥n. <br><br><h2>  Virtualizaci√≥n de almacenamiento </h2><br>  Con el desarrollo de los sistemas de virtualizaci√≥n, fue necesario encontrar una soluci√≥n innovadora para almacenar y acceder a los datos, al tiempo que se garantizaba la tolerancia a fallas.  Este fue el punto de partida para crear SDS (almacenamiento definido por software).  Para satisfacer las necesidades comerciales, estos repositorios fueron dise√±ados con la separaci√≥n de software y hardware. <br><br>  La arquitectura SDS es fundamentalmente diferente de la tradicional.  La l√≥gica de almacenamiento se ha abstra√≠do a nivel de software.  La organizaci√≥n del almacenamiento se ha vuelto m√°s f√°cil debido a la unificaci√≥n y virtualizaci√≥n de cada uno de los componentes de dicho sistema. <br><br>  ¬øCu√°l es el factor principal que obstaculiza la implementaci√≥n de SDS en todas partes?  Este factor con mayor frecuencia es una evaluaci√≥n incorrecta de las necesidades de las aplicaciones utilizadas y una evaluaci√≥n de riesgos incorrecta.  Para una empresa, la elecci√≥n de la soluci√≥n depende del costo de implementaci√≥n, en funci√≥n de los recursos actuales consumidos.  Pocas personas piensan: qu√© suceder√° cuando la cantidad de informaci√≥n y el rendimiento requerido excedan las capacidades de la arquitectura seleccionada.  Pensar sobre la base del principio metodol√≥gico "uno no deber√≠a multiplicar lo existente sin necesidad", mejor conocido como "la espada de Occam", determina la elecci√≥n a favor de las soluciones tradicionales. <br><br>  Solo unos pocos entienden que la necesidad de escalabilidad y confiabilidad del almacenamiento de datos es m√°s importante de lo que parece a primera vista.  La informaci√≥n es un recurso y, por lo tanto, el riesgo de su p√©rdida debe estar asegurado.  ¬øQu√© suceder√° cuando un sistema de almacenamiento tradicional se caiga?  Deber√° usar la garant√≠a o comprar equipos nuevos.  ¬øY si el sistema de almacenamiento se interrumpe o ha finalizado el "per√≠odo de vida" (el llamado EOL - End-of-Life)?  Este puede ser un d√≠a negro para cualquier organizaci√≥n que no pueda continuar utilizando sus propios servicios familiares. <br><br>  No hay sistemas que no tengan un solo punto de falla.  Pero hay sistemas que pueden sobrevivir f√°cilmente a la falla de uno o m√°s componentes.  Tanto los sistemas de almacenamiento virtuales como los tradicionales se crearon teniendo en cuenta el hecho de que tarde o temprano se producir√° una falla.  Ese es solo el "l√≠mite de fuerza" de los sistemas de almacenamiento tradicionales establecidos en el hardware, pero en los sistemas de almacenamiento virtual se determina en la capa de software. <br><br><h2>  Integraci√≥n </h2><br>  Los cambios dram√°ticos en la infraestructura de TI son siempre un fen√≥meno indeseable, cargado de tiempo de inactividad y p√©rdida de fondos.  Solo la implementaci√≥n fluida de nuevas soluciones hace posible evitar consecuencias negativas y mejorar el trabajo de los servicios.  Es por eso que Selectel dise√±√≥ y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">lanz√≥ la nube basada en VMware</a> , un l√≠der reconocido en el mercado de virtualizaci√≥n.  El servicio creado por nosotros permitir√° que cada empresa resuelva toda la gama de tareas de infraestructura, incluido el almacenamiento de datos. <br><br>  Le diremos exactamente c√≥mo decidimos la elecci√≥n de un sistema de almacenamiento, as√≠ como qu√© ventajas nos dio esta opci√≥n.  Por supuesto, se consideraron tanto los sistemas de almacenamiento tradicionales como las SDS.  Para comprender claramente todos los aspectos de la operaci√≥n y los riesgos, ofrecemos una visi√≥n m√°s profunda del tema. <br><br>  En la etapa de dise√±o, se impusieron los siguientes requisitos a los sistemas de almacenamiento: <br><br><ul><li>  <strong>tolerancia a fallas;</strong> </li><li>  <strong>rendimiento</strong> </li><li>  <strong>escalado</strong> </li><li>  <strong>la capacidad de garantizar velocidad;</strong> </li><li>  <strong>funcionamiento correcto en el ecosistema VMware.</strong> </li></ul><br>  El uso de soluciones de hardware tradicionales no podr√≠a proporcionar el nivel requerido de escalabilidad, ya que es imposible aumentar constantemente el volumen de almacenamiento debido a limitaciones arquitect√≥nicas.  La reserva a nivel de un centro de datos completo tambi√©n fue de gran dificultad.  Por eso dirigimos nuestra atenci√≥n a SDS. <br><br>  Hay varias soluciones de software en el mercado de SDS que nos convienen para construir una nube basada en VMware vSphere.  Entre estas soluciones se pueden observar: <br><br><ul><li>  <strong>Dell EMC ScaleIO;</strong> </li><li>  <strong>SAN virtual hiperconvergente de Datacore;</strong> </li><li>  <strong>HPE StoreVirtual.</strong> </li></ul><br>  Estas soluciones son adecuadas para usar con VMware vSphere, sin embargo, no se integran en el hipervisor y se ejecutan por separado.  Por lo tanto, la elecci√≥n se hizo a favor de VMware vSAN.  Consideremos en detalle c√≥mo se ve la arquitectura virtual de tal soluci√≥n. <br><br><h3>  Arquitectura </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hb/hu/nr/hbhunruzcic-pa1ggjw_-winfcg.png"></div><br>  <sup><i>Imagen tomada de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la documentaci√≥n oficial.</a></i></sup> <br><br>  A diferencia de los sistemas de almacenamiento tradicionales, toda la informaci√≥n no se almacena en ning√∫n punto.  Los datos de la m√°quina virtual se distribuyen uniformemente entre todos los hosts, y el escalado se realiza agregando hosts o instalando unidades de disco adicionales en ellos.  Se admiten dos opciones de configuraci√≥n: <br><br><ul><li>  <strong>Configuraci√≥n AllFlash</strong> (solo unidades de estado s√≥lido, tanto para almacenamiento de datos como para cach√©); </li><li>  <strong>Configuraci√≥n h√≠brida</strong> (almacenamiento magn√©tico y cach√© de estado s√≥lido). </li></ul><br>  El procedimiento para agregar espacio en el disco no requiere configuraciones adicionales, por ejemplo, crear un LUN (N√∫mero de unidad l√≥gica, n√∫meros de disco l√≥gico) y configurar el acceso a ellos.  Tan pronto como se agregue el host al cl√∫ster, su espacio en disco estar√° disponible para todas las m√°quinas virtuales.  Este enfoque tiene varias ventajas significativas: <br><br><ul><li>  <strong>falta de vinculaci√≥n con el fabricante del equipo;</strong> </li><li>  <strong>mayor tolerancia a fallas;</strong> </li><li>  <strong>asegurar la integridad de los datos en caso de falla;</strong> </li><li>  <strong>centro de control √∫nico desde la consola vSphere;</strong> </li><li>  <strong>conveniente escala horizontal y vertical.</strong> </li></ul><br>  Sin embargo, esta arquitectura impone altas demandas en la infraestructura de red.  Para garantizar el m√°ximo rendimiento, en nuestra nube la red se basa en el modelo Spine-Leaf. <br><br><h3>  Red </h3><br>  El modelo de red tradicional de tres niveles (n√∫cleo / agregaci√≥n / acceso) tiene una serie de inconvenientes importantes.  Un ejemplo sorprendente son las limitaciones de los protocolos Spanning-Tree. <br><br>  El modelo Spine-Leaf utiliza solo dos niveles, lo que ofrece las siguientes ventajas: <br><br><ul><li>  <strong>distancia predecible entre dispositivos;</strong> </li><li>  <strong>el tr√°fico va por la mejor ruta;</strong> </li><li>  <strong>facilidad de escalamiento;</strong> </li><li>  <strong>Exclusi√≥n de restricciones de protocolo L2.</strong> </li></ul><br>  Una caracter√≠stica clave de dicha arquitectura es que est√° optimizada para el paso del tr√°fico "horizontal".  Los paquetes de datos pasan por un solo salto, lo que permite una estimaci√≥n clara de los retrasos. <br><br>  Se proporciona una conexi√≥n f√≠sica utilizando varios enlaces de 10 GbE por servidor, cuyo ancho de banda se combina mediante el protocolo de agregaci√≥n.  Por lo tanto, cada host f√≠sico recibe acceso de alta velocidad a todos los objetos de almacenamiento. <br><br>  El intercambio de datos se implementa utilizando un protocolo patentado creado por VMware, que permite un funcionamiento r√°pido y confiable de la red de almacenamiento en el transporte Ethernet (desde 10 GbE y superior). <br><br>  La transici√≥n al modelo de objetos del almacenamiento de datos permiti√≥ un ajuste flexible del uso del almacenamiento de acuerdo con los requisitos de los clientes.  Todos los datos se almacenan en forma de objetos que se distribuyen de cierta manera entre los hosts del cl√∫ster.  Aclaramos los valores de algunos par√°metros que se pueden controlar. <br><br><h3>  Tolerancia a fallos </h3><br><ol><li>  <strong>FTT (Fallos de tolerancia).</strong>  Indica el n√∫mero de fallas de host que el cl√∫ster puede manejar sin interrumpir la operaci√≥n regular. </li><li>  <strong>FTM (M√©todo de tolerancia a fallas).</strong>  El m√©todo para garantizar la tolerancia a fallos a nivel de disco. <br><br>  a.  <strong>Reflejo</strong> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jb/kv/lw/jbkvlwsmyvua7jrw2f8jj0820am.png"></div><br>  <sup><i>Imagen tomada del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">blog de VMware.</a></i></sup> <br><br>  Representa una duplicaci√≥n completa de un objeto, y las r√©plicas siempre se encuentran en diferentes hosts f√≠sicos.  El an√°logo m√°s cercano a este m√©todo es RAID-1.  Su uso permite que el cl√∫ster procese rutinariamente hasta tres fallas de cualquier componente (discos, hosts, p√©rdida de red, etc.).  Este par√°metro se configura configurando la opci√≥n FTT. <br><br>  De forma predeterminada, esta opci√≥n tiene un valor de 1 y se crea 1 r√©plica para el objeto (solo 2 instancias en diferentes hosts).  A medida que aumenta el valor, el n√∫mero de copias ser√° N + 1.  Por lo tanto, con un valor m√°ximo de FTT = 3, 4 instancias del objeto estar√°n en diferentes hosts. <br><br>  Este m√©todo le permite alcanzar el m√°ximo rendimiento a expensas de la eficiencia del espacio en disco.  Se puede utilizar en configuraciones h√≠bridas y AllFlash. <br><br>  b.  <strong>Codificaci√≥n de borrado</strong> (an√°logo de RAID 5/6). <br><img src="https://habrastorage.org/webt/72/zq/hh/72zqhhyrlncoqga9qlzg0wdfgco.png"><br><br>  <sup><i>Imagen tomada del blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cormachogan.com.</a></i></sup> <br><br>  El trabajo de este m√©todo se admite exclusivamente en configuraciones AllFlash.  En el proceso de grabaci√≥n de cada objeto, se calculan los bloques de paridad correspondientes, que permiten recuperar datos de manera √∫nica en caso de falla.  Este enfoque ahorra significativamente espacio en disco en comparaci√≥n con Mirroring. <br><br>  Por supuesto, la operaci√≥n de este m√©todo aumenta la sobrecarga, que se expresa en una disminuci√≥n de la productividad.  Sin embargo, dado el rendimiento de la configuraci√≥n de AllFlash, este inconveniente se nivela, lo que hace que el uso de Codificaci√≥n de borrado sea una opci√≥n aceptable para la mayor√≠a de las tareas. <br><br>  Adem√°s, VMware vSAN presenta el concepto de "dominios de falla", que son una agrupaci√≥n l√≥gica de bastidores de servidores o cestas de discos.  Tan pronto como se agrupan los elementos necesarios, esto conduce a la distribuci√≥n de datos entre diferentes nodos teniendo en cuenta los dominios de falla.  Esto permite que el cl√∫ster sobreviva a la p√©rdida de un dominio completo, ya que todas las r√©plicas correspondientes de los objetos se ubicar√°n en otros hosts en un dominio de falla diferente. <br><br>  El dominio m√°s peque√±o de falla es un grupo de discos, que es una unidad de disco conectada l√≥gicamente.  Cada grupo de discos contiene dos tipos de medios: cach√© y capacidad.  Como un medio de cach√©, el sistema permite usar solo discos de estado s√≥lido, y tanto los discos magn√©ticos como los de estado s√≥lido pueden actuar como portadores de capacidad.  El almacenamiento en cach√© de medios ayuda a acelerar los discos magn√©ticos y reduce la latencia al acceder a los datos. </li></ol><br><h2>  Implementaci√≥n </h2><br>  Hablemos sobre las limitaciones que existen en la arquitectura VMware vSAN y por qu√© son necesarias.  Independientemente de las plataformas de hardware utilizadas, la arquitectura proporciona las siguientes restricciones: <br><br><ul><li>  <strong>no m√°s de 5 grupos de discos por host;</strong> </li><li>  <strong>no m√°s de 7 portadores de capacidad en un grupo de discos;</strong> </li><li>  <strong>no m√°s de 1 portadora de cach√© en un grupo de discos;</strong> </li><li>  <strong>no m√°s de 35 portadores de capacidad por host;</strong> </li><li>  <strong>no m√°s de 9000 componentes por host (incluidos los componentes de testigo);</strong> </li><li>  <strong>no m√°s de 64 hosts en un cl√∫ster;</strong> </li><li>  <strong>no m√°s de 1 vSAN-datastore por cluster.</strong> </li></ul><br>  ¬øPor qu√© se necesita esto?  Hasta que se excedan los l√≠mites especificados, el sistema funcionar√° con la capacidad declarada, manteniendo un equilibrio entre el rendimiento y la capacidad de almacenamiento.  Esto le permite garantizar el funcionamiento correcto de todo el sistema de almacenamiento virtual en su conjunto. <br><br>  Adem√°s de estas limitaciones, se debe recordar una caracter√≠stica importante.  No se recomienda llenar m√°s del 70% del volumen total de almacenamiento.  El hecho es que cuando se alcanza el 80%, el mecanismo de reequilibrio se inicia autom√°ticamente y el sistema de almacenamiento comienza a redistribuir los datos en todos los hosts del cl√∫ster.  El procedimiento requiere muchos recursos y puede afectar seriamente el rendimiento del subsistema de disco. <br><br>  Para satisfacer las necesidades de una amplia variedad de clientes, hemos implementado tres grupos de almacenamiento para facilitar su uso en varios escenarios.  Miremos cada uno de ellos en orden. <br><br><h3>  Agrupaci√≥n r√°pida de discos </h3><br>  La prioridad para crear este grupo era obtener un almacenamiento que proporcionara el m√°ximo rendimiento para alojar sistemas altamente cargados.  Los servidores de este grupo usan un par de Intel P4600 como cach√© y 10 Intel P3520 para el almacenamiento de datos.  El cach√© en este grupo se utiliza para que los datos se lean directamente desde los medios y las operaciones de escritura se realicen a trav√©s del cach√©. <br><br>  Para aumentar la capacidad √∫til y garantizar la tolerancia a fallas, se utiliza un modelo de almacenamiento de datos llamado Erasure Coding.  Este modelo es similar a una matriz RAID 5/6 normal, pero a nivel de almacenamiento de objetos.  Para eliminar la probabilidad de corrupci√≥n de datos, vSAN usa un mecanismo de c√°lculo de suma de verificaci√≥n para cada bloque de datos 4K. <br><br>  La validaci√≥n se realiza en segundo plano durante las operaciones de lectura / escritura, as√≠ como para los datos "en fr√≠o", cuyo acceso no se solicit√≥ durante el a√±o.  Cuando se detecta una falta de coincidencia de suma de comprobaci√≥n y, por lo tanto, se detecta corrupci√≥n de datos, vSAN recuperar√° autom√°ticamente los archivos sobrescribi√©ndolos. <br><br><h3>  Conjunto de unidades h√≠bridas </h3><br>  En el caso de este grupo, su tarea principal es proporcionar una gran cantidad de datos, mientras se garantiza un buen nivel de tolerancia a fallas.  Para muchas tareas, la velocidad de acceso a los datos no es una prioridad, el volumen y el costo de almacenamiento son mucho m√°s importantes.  El uso de unidades de estado s√≥lido como tal almacenamiento tendr√° un costo irrazonablemente alto. <br><br>  Este factor fue la raz√≥n de la creaci√≥n del grupo, que es un h√≠brido de almacenamiento en cach√© de unidades de estado s√≥lido (como en otros grupos es Intel P4600) y discos duros de nivel empresarial desarrollados por HGST.  Un flujo de trabajo h√≠brido acelera el acceso a los datos solicitados frecuentemente al almacenar en cach√© las operaciones de lectura y escritura. <br><br>  A nivel l√≥gico, los datos se reflejan para eliminar la p√©rdida en caso de una falla de hardware.  Cada objeto se divide en componentes id√©nticos y el sistema los distribuye a diferentes hosts. <br><br><h3>  Piscina con recuperaci√≥n ante desastres </h3><br><img src="https://habrastorage.org/webt/hz/ne/xi/hznexild-sq7oxyvy6m9yvr-xm4.png"><br><br>  La tarea principal del grupo es lograr el nivel m√°ximo de tolerancia a fallas y rendimiento.  El uso de la tecnolog√≠a <strong>vSAN estirada</strong> nos permiti√≥ <strong>distribuir</strong> el almacenamiento entre los centros de datos Tsvetochnaya-2 en San Petersburgo y Dubrovka-3 en la regi√≥n de Leningrado.  Cada servidor en este grupo est√° equipado con un par de unidades Intel P4600 de alta velocidad y capacidad para operaci√≥n de cach√© y 6 unidades Intel P3520 para almacenamiento de datos.  A nivel l√≥gico, estos son 2 grupos de discos por host. <br><br>  La configuraci√≥n de AllFlash no tiene un inconveniente grave: una fuerte ca√≠da en IOPS y un aumento en la cola de solicitudes de disco con un mayor volumen de acceso aleatorio a los datos.  Al igual que en un grupo con discos r√°pidos, las operaciones de escritura pasan por el cach√© y la lectura se realiza directamente. <br><br>  Ahora sobre la diferencia principal del resto de las piscinas.  Los datos de cada m√°quina virtual se reflejan dentro de un centro de datos y al mismo tiempo se replican sincr√≥nicamente en otro centro de datos que nos pertenece.  Por lo tanto, incluso un accidente grave, como una interrupci√≥n completa de la conectividad entre los centros de datos, no ser√° un problema.  Incluso una p√©rdida completa del centro de datos no afectar√° los datos. <br><br>  Un accidente con una falla completa del sitio: la situaci√≥n es bastante rara, pero vSAN puede sobrevivir con honor sin perder datos.  Los invitados a nuestro evento <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SelectelTechDay 2018</a> pudieron ver por s√≠ mismos c√≥mo el cl√∫ster vSAN estirado experiment√≥ una falla completa del sitio.  Las m√°quinas virtuales estuvieron disponibles solo un minuto despu√©s de que todos los servidores de uno de los sitios se apagaran.  Todos los mecanismos funcionaron exactamente seg√∫n lo planeado, pero los datos permanecieron intactos. <br><br>  El abandono de la arquitectura de almacenamiento familiar conlleva muchos cambios.  Uno de estos cambios fue la aparici√≥n de nuevas "entidades" virtuales, que incluyen el dispositivo testigo.  El significado de esta soluci√≥n es rastrear el proceso de grabaci√≥n de r√©plicas de datos y determinar cu√°l es relevante.  Al mismo tiempo, los datos en s√≠ no se almacenan en componentes de testigos, solo metadatos sobre el proceso de grabaci√≥n. <br><br>  Este mecanismo surte efecto en caso de un accidente cuando ocurre una falla durante el proceso de replicaci√≥n, lo que resulta en que las r√©plicas no est√©n sincronizadas. <br><br>  Para determinar cu√°l contiene informaci√≥n relevante, se utiliza un mecanismo de determinaci√≥n de qu√≥rum.  Cada componente tiene un "derecho de voto" y se le asigna un cierto n√∫mero de votos (1 o m√°s).  El mismo "derecho de voto" tiene componentes de testigos que desempe√±an el papel de √°rbitros en caso de una situaci√≥n controvertida. <br><br>  Se alcanza un qu√≥rum solo cuando una r√©plica completa est√° disponible para un objeto y el n√∫mero de "votos" actuales es superior al 50%. <br><br><h2>  Conclusi√≥n </h2><br>  La elecci√≥n de VMware vSAN como sistema de almacenamiento se ha convertido en una decisi√≥n importante para nosotros.  Esta opci√≥n pas√≥ las pruebas de estr√©s y las pruebas de tolerancia a fallas antes de que se incluyera en nuestro proyecto de nube basado en VMware. <br><br>  Seg√∫n los resultados de la prueba, qued√≥ claro que la funcionalidad declarada funciona como se esperaba y cumple con todos los requisitos de nuestra infraestructura en la nube. <br><br>  ¬øTiene algo que contar basado en su propia experiencia con vSAN?  Bienvenidos a los comentarios. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es418753/">https://habr.com/ru/post/es418753/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es418739/index.html">Centro de tecnolog√≠a aditiva: impresoras 3D industriales Sistemas 3D, Stratasys, SLM, EOS</a></li>
<li><a href="../es418741/index.html">Agregue cifrado y empuje a SIP normal</a></li>
<li><a href="../es418743/index.html">Historia del primer lugar en ML Boot Camp VI</a></li>
<li><a href="../es418747/index.html">Resoluci√≥n de problemas: ¬øc√≥mo resolver eficazmente los problemas en un equipo?</a></li>
<li><a href="../es418751/index.html">Dex imp√∫dico esp√≠a chino</a></li>
<li><a href="../es418755/index.html">C√≥mo el comercio electr√≥nico sobrevive a las promociones a gran escala. Prepar√°ndose para las cargas m√°ximas en la web [Parte 1]</a></li>
<li><a href="../es418757/index.html">Reducci√≥n de los plazos (criptomonedas, divisas, intercambios)</a></li>
<li><a href="../es418759/index.html">¬øCu√°nto le cuesta a un estudiante emitir un chip?</a></li>
<li><a href="../es418761/index.html">El libro "Pure Python. Las sutilezas de la programaci√≥n para profesionales ¬ª</a></li>
<li><a href="../es418763/index.html">Middle / senior: ¬øc√≥mo salir del pantano?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>