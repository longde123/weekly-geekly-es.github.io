<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äç‚öïÔ∏è üÜé üë©üèº‚Äç‚öñÔ∏è "An√°lisis de datos en Python" en dos partes üë©üèΩ‚Äçüè´ üì≤ üë∂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Los cursos sobre an√°lisis de datos en el centro CS son impartidos por Vadim Leonardovich Abbakumov - Ph.D. Ciencias, trabaja como analista experto jef...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>"An√°lisis de datos en Python" en dos partes</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/JetBrains-education/blog/438058/">  Los cursos sobre an√°lisis de datos en el centro CS son impartidos por Vadim Leonardovich Abbakumov - Ph.D.  Ciencias, trabaja como analista experto jefe en Gazpromneft-Alternative Fuel. <br><br>  Las conferencias est√°n destinadas a dos categor√≠as de estudiantes.  El primero son los analistas novatos a quienes les resulta dif√≠cil comenzar estudiando, por ejemplo, Los elementos del aprendizaje estad√≠stico.  El curso los preparar√° para futuros trabajos.  El segundo es analistas experimentados que no han recibido una educaci√≥n sistem√°tica en el campo del an√°lisis de datos.  Pueden llenar lagunas de conocimiento.  Desde el a√±o pasado, la clase ha estado usando el lenguaje de programaci√≥n Python. <br><br>  Para entender el material, una vez suficientes cursos de an√°lisis matem√°tico, √°lgebra lineal y teor√≠a de la probabilidad y conocimiento b√°sico del lenguaje Python son suficientes. <br><br>  Que tengas una linda vista! <br><a name="habracut"></a><br><h3>  Parte 1 </h3><br>  1. Estad√≠stica descriptiva.  Cuantiles, cuartiles.  Histogramas  Estimaciones de densidad nuclear. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/enpPFqcIFj8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  2. Estad√≠stica descriptiva.  Cajas con bigote.  Emisiones  Mediana y media aritm√©tica como observaciones t√≠picas.  Diagrama de dispersi√≥n.  Matriz de diagramas de dispersi√≥n. <br>  Bar y gr√°fico circular. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/G-zRmitRaJM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  3. An√°lisis jer√°rquico de conglomerados.  Cluster, distancias entre objetos, distancias entre clusters.  Algoritmo para construir un dendrograma.  Rocky scree / codo.  Estandarizaci√≥n de datos.  Errores t√≠picos en la preparaci√≥n de datos.  Interpretaci√≥n de los resultados. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/gXBs4_3aKrs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  4. El m√©todo k-means.  Ejemplos (se omite la parte te√≥rica de la conferencia). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/cZ-deRKi1n4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  5. Prueba de hip√≥tesis estad√≠sticas (introducci√≥n te√≥rica). <br><br>  Hip√≥tesis de acuerdo, homogeneidad, independencia, hip√≥tesis sobre par√°metros de distribuci√≥n. <br>  Errores del primer y segundo tipo, valor p y nivel de significaci√≥n, algoritmo para probar la hip√≥tesis estad√≠stica e interpretaci√≥n de los resultados.  La hip√≥tesis de normalidad de distribuci√≥n.  Criterios de Shapiro-Wilk y Kolmogorov-Smirnov.  Desviaciones menores de la normalidad.  Comparaci√≥n de muestras.  Muestras independientes y pareadas.  La elecci√≥n entre la prueba t de Student, el criterio de Mann-Whitney-Wilcoxon y el criterio del estado de √°nimo.  Variedades del criterio t de Student y comparaci√≥n de varianzas.  Visualizaci√≥n en comparaciones.  Pruebas unilaterales y bilaterales. <br>  Independencia  Coeficientes de correlaci√≥n de Pearson, Kendall y Spearman, errores t√≠picos en el estudio de la relaci√≥n entre los dos fen√≥menos.  Inspecci√≥n visual de hallazgos. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/01PL0UG6ah8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  6. Prueba de hip√≥tesis estad√≠sticas (procedimientos de Python). <br><br>  Criterio de Shapiro-Wilk.  Prueba de Mann-Whitney-Wilcoxon.  Prueba t de Student.  Criterio de Fligner-Kilin. <br><br>  Muestras independientes y pareadas.  Prueba de chi-cuadrado.  Criterio de Pearson. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yBsECOzVGI0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  7. Pruebas A / B.  Prueba de proporciones. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/PZmeuPhgrA0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  8. An√°lisis de regresi√≥n lineal.  Modelo, interpretaci√≥n de estimaciones de coeficientes, coeficiente de determinaci√≥n m√∫ltiple.  Interpretaci√≥n del coeficiente m√∫ltiple de determinaci√≥n, restricciones en el alcance de su aplicaci√≥n.  Identifique los predictores m√°s significativos y eval√∫e la contribuci√≥n de cada predictor.  Algoritmos para ajustar los modelos construidos.  Colinealidad. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_PlC8Niun7U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  9. Predicci√≥n basada en un modelo de regresi√≥n con variables indicadoras estacionales (ficticias, estructurales).  Tendencia, componentes estacionales, cambio en la naturaleza de la serie, emisiones.  El logaritmo es una t√©cnica para convertir la estacionalidad multiplicativa en aditiva. <br>  Variables indicadoras.  Reentrenamiento <br>  El caso de varios componentes estacionales. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/COBcXzKmOyk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  10. Reconocimiento / clasificaci√≥n de patrones. <br>  Par√°metros del modelo, internos y externos. <br>  Criterios de calidad.  Entrenamiento y muestras de prueba. <br>  √Årboles de clasificaci√≥n CART.  La representaci√≥n geom√©trica.  Representaci√≥n en forma de un conjunto de reglas l√≥gicas.  Presentaci√≥n en forma de √°rbol.  Nodos, padres y descendientes, nodos finales.  Umbrales  Medidas de impureza: genio, entrop√≠a, errores de clasificaci√≥n.  Las reglas son los restos del √°rbol de aprendizaje.  Contenido informativo de variables. <br>  √Årboles de clasificaci√≥n en problemas de regresi√≥n. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hqnsTBKJ5Lg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  11. Bosques al azar.  Embolsado  Par√°metros clave del modelo.  Error fuera de bolsa.  Contenido informativo de variables.  An√°lisis de muestras desequilibradas.  Determinando el n√∫mero de √°rboles. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BMD8gtGlQ_o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  12. Impulso.  M√°quina de refuerzo de gradiente.  Par√°metros clave del modelo. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/-b6Y1DDxvL4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3>  Parte 2 </h3><br>  1. El modelo neuronal.  Funci√≥n de activaci√≥n.  Redes de distribuci√≥n directa (Red neuronal FeedForward).  Arquitectura de red neuronal.  Conectivismo (conexionismo). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/5l0e_Q0gpnc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  2. Entrenamiento de la red neuronal.  Error inverso de propagaci√≥n.  El m√©todo de descenso r√°pido (descenso de gradiente) y su generalizaci√≥n.  Epochs y batch'i.  Introducci√≥n a Keras y TensorFlow.  Inicializaci√≥n de pesos de redes neuronales.  La estandarizaci√≥n de datos evita la saturaci√≥n.  Formaci√≥n de redes neuronales de distribuci√≥n directa.  Optimizaci√≥n (optimizadores) en Keras.  F√≥rmulas para correcciones de peso en el entrenamiento de una red neuronal.  Un ejemplo de entrenamiento de una red neuronal. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/RKKhzFBmEBg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  3. Un ejemplo de entrenamiento de una red neuronal.  Criterios de calidad en Keras.  Inicializaci√≥n de pesos de redes neuronales en Keras. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/fyktZrnqOKs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  4. Redes neuronales para el pron√≥stico.  Reducci√≥n del problema de pron√≥stico a un problema de regresi√≥n.  Serie de pron√≥sticos con un componente estacional. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/F0tlV4W62AU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  5. Reconocimiento de imagen.  Haar cascada para resaltar la cara en la imagen. <br>  Convoluci√≥n  Capa de convoluci√≥n  Acolchado  Zancada  Pooling <br>  Abandono y decorrelaci√≥n.  Entrenamiento adicional de redes neuronales.  Ejemplo: reconocimiento de escritura a mano, primera soluci√≥n. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/aisANj2Hzxg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  6. Ejemplo: reconocimiento de d√≠gitos escritos a mano, segunda soluci√≥n.  Augmentaiton.  Arquitectura de red neuronal VGG-16.  La regularizaci√≥n, su prop√≥sito.  Regularizaci√≥n en an√°lisis de regresi√≥n lineal.  Ecuaciones normales de an√°lisis de regresi√≥n lineal.  Agregar un t√©rmino de regularizaci√≥n a las ecuaciones normales.  El papel especial de un miembro libre.  Ejemplo: aproximaci√≥n de puntos por un polinomio.  Muestra de validaci√≥n.  Variantes del t√©rmino de regularizaci√≥n (regresi√≥n de cresta, lazo, red el√°stica).  ¬øPor qu√© Lasso reduce los predictores? <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/UTBHfcVwPtI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  7. Base te√≥rica del m√©todo.  Un ejemplo de resoluci√≥n de un problema en Python usando XGboost.  Muestras no balanceadas.  Precisi√≥n, recuperaci√≥n, F1.  Contenido informativo de variables (Importancia).  Selecci√≥n de par√°metros en XGboost. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/CCDIbNGGBwY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  8. Selecci√≥n de par√°metros en XGboost.  GridSearch para seleccionar par√°metros.  An√°lisis factorial.  Tareas resueltas por an√°lisis factorial. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/OBUmnI8Vddw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  9. Modelos matem√°ticos para el an√°lisis de los principales componentes y an√°lisis factorial.  Interpretaci√≥n de factores.  Un ejemplo de an√°lisis factorial en Python.  Cargas factoriales, etiquetas factoriales, su interpretaci√≥n.  Factores de rotaci√≥n. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/q6BFIg2M3LE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  10. Un ejemplo de an√°lisis factorial en Python. <br>  Modelo matem√°tico de descomposici√≥n SVD.  Descomposici√≥n SVD y an√°lisis de los componentes principales.  Descomposici√≥n SVD como base del an√°lisis sem√°ntico latente (LSA).  SVD descomposici√≥n de una matriz de datos que contiene huecos.  Simon's Funk Method Regularizaci√≥n en Simon's Funk Method.  Descomposici√≥n SVD al construir un sistema de recomendaci√≥n. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/USmGLaUU2Hc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  11. Caracter√≠sticas de la aplicaci√≥n de descomposici√≥n SVD (Descomposici√≥n de valor singular) para datos con un gran n√∫mero de lagunas.  Calibraci√≥n de clasificadores.  Regresi√≥n isot√≥nica  Calibraci√≥n Platt <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/ArOw6qZ_eZA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  12. An√°lisis de muestras desequilibradas.  Precisi√≥n, precisi√≥n, recuperaci√≥n, F1.  Curva ROC (curva ROC) para determinar el valor umbral.  Curva ROC para comparar clasificadores.  √Årea bajo curva (AUC).  Regresi√≥n log√≠stica <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/9q7Am6ZMrvs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/438058/">https://habr.com/ru/post/438058/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../438028/index.html">No necesita blockchain: 8 casos de usuarios populares y por qu√© no funcionan</a></li>
<li><a href="../438032/index.html">C√≥digo abierto popular - segunda parte: 5 herramientas de administraci√≥n en la nube</a></li>
<li><a href="../438034/index.html">Android, Rx y Kotlin, o c√≥mo hacer que una garra de Lego se encoja. Parte 1</a></li>
<li><a href="../438036/index.html">3blue1brown y MIT en ruso</a></li>
<li><a href="../438038/index.html">Carrera de esteroides. Historias reales</a></li>
<li><a href="../438060/index.html">Estimaci√≥n de la orientaci√≥n espacial, o C√≥mo no temer a los filtros de Mahoney y Majwik</a></li>
<li><a href="../438062/index.html">Mi direcci√≥n no es una casa o calle, mi direcci√≥n es la Uni√≥n Sovi√©tica.</a></li>
<li><a href="../438064/index.html">Lista de verificaci√≥n: lo que hab√≠a que hacer antes de iniciar microservicios en prod</a></li>
<li><a href="../438066/index.html">10 canales educativos de YouTube en ingl√©s de los que nunca has o√≠do hablar</a></li>
<li><a href="../438070/index.html">¬øC√≥mo se convirti√≥ la generaci√≥n Y en una generaci√≥n quemada?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>