<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßöüèΩ üöê üçì Tupperware: Facebook-Killer Kubernetes? ü§üüèø ü§±üèø ‚úåüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Effizientes und zuverl√§ssiges Cluster-Management in jeder Gr√∂√üenordnung mit Tupperware 





 Heute haben wir auf der Systems @ Scale-Konferenz Tupper...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tupperware: Facebook-Killer Kubernetes?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/455579/"><p>  Effizientes und zuverl√§ssiges Cluster-Management in jeder Gr√∂√üenordnung mit Tupperware </p><br><p><img src="https://habrastorage.org/webt/gm/mv/jn/gmmvjn5ev7lk2kyhaiejighzrf0.jpeg"></p><br><p>  Heute <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben</a> wir auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Systems @ Scale-Konferenz</a> Tupperware vorgestellt, unser Cluster-Management-System, das Container auf Millionen von Servern orchestriert, auf denen fast alle unsere Services funktionieren.  Wir haben Tupperware erstmals im Jahr 2011 eingef√ºhrt. Seitdem ist unsere Infrastruktur von einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rechenzentrum auf</a> bis zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">15 geoverteilte Rechenzentren</a> angewachsen.  W√§hrend dieser ganzen Zeit stand Tupperware nicht still und entwickelte sich mit uns.  Wir werden Ihnen sagen, in welchen Situationen Tupperware erstklassiges Cluster-Management bietet, einschlie√ülich praktischer Unterst√ºtzung f√ºr Stateful Services, eines einzigen Control Panels f√ºr alle Rechenzentren und der M√∂glichkeit, die Energie in Echtzeit auf die Services zu verteilen.  Und wir werden die Lektionen teilen, die wir bei der Entwicklung unserer Infrastruktur gelernt haben. </p><br><p> Tupperware f√ºhrt verschiedene Aufgaben aus.  Anwendungsentwickler verwenden es, um Anwendungen bereitzustellen und zu verwalten.  Es packt die Code- und Anwendungsabh√§ngigkeiten in ein Image und liefert sie in Form von Containern an die Server.  Container bieten Isolation zwischen Anwendungen auf demselben Server, sodass Entwickler mit der Anwendungslogik besch√§ftigt sind und nicht dar√ºber nachdenken, wie Server gefunden oder Updates gesteuert werden.  Tupperware √ºberwacht auch die Leistung des Servers. Wenn ein Fehler festgestellt wird, werden Container vom problematischen Server √ºbertragen. </p><a name="habracut"></a><br><p>  Kapazit√§tsplanungsingenieure verwenden Tupperware, um die Serverkapazit√§ten je nach Budget und Einschr√§nkungen in Teams zu verteilen.  Sie verwenden es auch, um die Servernutzung zu verbessern.  Betreiber von Rechenzentren wenden sich an Tupperware, um Container ordnungsgem√§√ü auf Rechenzentren zu verteilen und Container w√§hrend der Wartung anzuhalten oder zu verschieben.  Aus diesem Grund erfordert die Wartung von Servern, Netzwerken und Ger√§ten nur eine minimale menschliche Beteiligung. </p><br><h3 id="arhitektura-tupperware">  Tupperware-Architektur </h3><br><p> <a href=""><img src="https://habrastorage.org/webt/e7/q1/oz/e7q1ozhv85xlsvgczzofpgwoikg.jpeg"></a> </p><br><p>  <em>Architektur Tupperware PRN ist eine der Regionen unserer Rechenzentren.</em>  <em>Die Region besteht aus mehreren Rechenzentrumsgeb√§uden (PRN1 und PRN2) in der N√§he.</em>  <em>Wir planen ein Control Panel, das alle Server in einer Region verwaltet.</em> </p><br><p>  Anwendungsentwickler stellen Dienste in Form von Tupperware-Jobs bereit.  Eine Aufgabe besteht aus mehreren Containern, die normalerweise denselben Anwendungscode ausf√ºhren. </p><br><p>  Tupperware ist f√ºr die Containerbereitstellung und das Lebenszyklusmanagement verantwortlich.  Es besteht aus mehreren Komponenten: </p><br><ul><li>  Das Tupperware-Frontend bietet eine API f√ºr die Benutzeroberfl√§che, die CLI und andere Automatisierungstools, √ºber die Sie mit Tupperware interagieren k√∂nnen.  Sie verbergen die gesamte interne Struktur vor Tupperware-Jobbesitzern. </li><li>  Der Tupperware Scheduler ist das Control Panel, das f√ºr die Verwaltung des Container- und Joblebenszyklus verantwortlich ist.  Es wird auf regionaler und globaler Ebene bereitgestellt, wobei ein regionaler Scheduler Server in einer Region verwaltet und ein globaler Scheduler Server aus verschiedenen Regionen verwaltet.  Der Scheduler ist in Shards unterteilt, und jeder Shard steuert eine Reihe von Aufgaben. </li><li>  Der Scheduler-Proxy in Tupperware verbirgt das interne Sharding und bietet Tupperware-Benutzern ein praktisches einheitliches Kontrollfeld. </li><li>  Der Tupperware Distributor weist den Servern Container zu.  Der Scheduler ist f√ºr das Stoppen, Starten, Aktualisieren und Fehlschlagen von Containern verantwortlich.  Derzeit kann ein einzelner Distributor eine gesamte Region verwalten, ohne sich in Shards aufzuteilen.  (Beachten Sie den Unterschied in der Terminologie. Beispielsweise entspricht der Scheduler in Tupperware dem Control Panel in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes</a> , und der Tupperware-Distributor wird in Kubernetes als Scheduler bezeichnet.) </li><li>  Der Ressourcenbroker speichert die Wahrheitsquelle f√ºr die Server- und Serviceereignisse.  Wir f√ºhren f√ºr jedes Rechenzentrum einen Ressourcenbroker aus, der alle Serverinformationen in diesem Rechenzentrum speichert.  Ein Ressourcenbroker und ein Kapazit√§tsverwaltungssystem oder ein Ressourcenzuweisungssystem entscheiden dynamisch, welche Scheduler-Bereitstellung welchen Server steuert.  Der Integrit√§tspr√ºfungsdienst √ºberwacht Server und speichert Daten zu ihrem Zustand im Ressourcenbroker.  Wenn der Server Probleme hat oder gewartet werden muss, weist der Ressourcenbroker den Distributor und den Scheduler an, die Container anzuhalten oder auf andere Server zu √ºbertragen. </li><li>  Tupperware Agent ist ein Daemon, der auf jedem Server ausgef√ºhrt wird und Container vorbereitet und entfernt.  Anwendungen arbeiten im Container, wodurch sie besser isoliert und reproduzierbar sind.  Auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letztj√§hrigen Systems @ Scale-Konferenz haben</a> wir bereits beschrieben, wie einzelne Tupperware-Container mithilfe von Images, btrfs, cgroupv2 und systemd erstellt werden. </li></ul><br><h3 id="otlichitelnye-osobennosti-tupperware">  Besonderheiten von Tupperware </h3><br><p>  Tupperware ist anderen Cluster-Management-Systemen wie Kubernetes und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mesos</a> sehr √§hnlich, es gibt jedoch einige Unterschiede: </p><br><ul><li>  Native Unterst√ºtzung f√ºr Stateful Services. </li><li>  Ein einziges Control Panel f√ºr Server in verschiedenen Rechenzentren zur Automatisierung der Lieferung von Containern basierend auf Absicht, Stilllegung von Clustern und Wartung. </li><li>  Klare Trennung des Bedienfelds zum Zoomen. </li><li>  Mit flexiblen Berechnungen k√∂nnen Sie die Leistung in Echtzeit auf die Dienste verteilen. </li></ul><br><p>  Wir haben diese coolen Funktionen entwickelt, um eine Vielzahl von zustandslosen und zustandsbehafteten Anwendungen in einem riesigen globalen gemeinsam genutzten Serverpark zu unterst√ºtzen. </p><br><h3 id="vstroennaya-podderzhka-stateful-sevisov">  Native Unterst√ºtzung f√ºr Stateful Services. </h3><br><p>  Tupperware verwaltet viele wichtige Stateful Services, die persistente Produktdaten f√ºr Facebook, Instagram, Messenger und WhatsApp speichern.  Dies k√∂nnen gro√üe Schl√ºssel-Wert-Paare (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ZippyDB</a> ) und √úberwachungsdatenspeicher (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ODS Gorilla</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scuba</a> ) sein.  Die Aufrechterhaltung zustandsbehafteter Dienste ist nicht einfach, da das System sicherstellen muss, dass Containerlieferungen gro√üen Ausf√§llen standhalten k√∂nnen, einschlie√ülich eines Stromausfalls oder eines Stromausfalls.  Obwohl herk√∂mmliche Methoden wie das Verteilen von Containern √ºber Fehlerdom√§nen f√ºr zustandslose Dienste gut geeignet sind, ben√∂tigen Stateful Services zus√§tzliche Unterst√ºtzung. </p><br><p>  Wenn beispielsweise aufgrund eines Serverausfalls ein Replikat der Datenbank nicht mehr verf√ºgbar ist, muss eine automatische Wartung zugelassen werden, mit der die Kernel auf 50 Servern aus einem 10-Tausendstel-Pool aktualisiert werden?  Das h√§ngt von der Situation ab.  Wenn sich auf einem dieser 50 Server ein anderes Replikat derselben Datenbank befindet, ist es besser zu warten und nicht zwei Replikate gleichzeitig zu verlieren.  Um dynamisch Entscheidungen √ºber die Wartung und den Zustand des Systems treffen zu k√∂nnen, ben√∂tigen Sie Informationen zur internen Datenreplikation und zur Standortlogik jedes Stateful Service. </p><br><p>  √úber die TaskControl-Schnittstelle k√∂nnen Stateful Services Entscheidungen beeinflussen, die sich auf die Datenverf√ºgbarkeit auswirken.  √úber diese Schnittstelle benachrichtigt der Scheduler externe Anwendungen √ºber Containervorg√§nge (Neustart, Aktualisierung, Migration, Wartung).  Der Stateful-Dienst implementiert einen Controller, der Tupperware mitteilt, wann jeder Vorgang sicher ausgef√ºhrt werden kann, und diese Vorg√§nge k√∂nnen ausgetauscht oder vor√ºbergehend verz√∂gert werden.  Im obigen Beispiel weist der Datenbankcontroller Tupperware m√∂glicherweise an, 49 der 50 Server zu aktualisieren, ber√ºhrt jedoch bisher keinen bestimmten Server (X).  Wenn der Kernel-Aktualisierungszeitraum verstrichen ist und die Datenbank das Problemreplikat immer noch nicht wiederherstellen kann, aktualisiert Tupperware den X-Server weiterhin. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/xi/5q/xuxi5qpox1v3gpc6khbipxgna0i.jpeg"></a> </p><br><p>  Viele Stateful Services in Tupperware verwenden TaskControl nicht direkt, sondern √ºber ShardManager, eine g√§ngige Plattform zum Erstellen von Stateful Services auf Facebook.  Mit Tupperware k√∂nnen Entwickler angeben, wie Container auf Rechenzentren verteilt werden sollen.  Mit ShardManager geben Entwickler ihre Absicht an, wie Data Shards auf Container verteilt werden sollen.  ShardManager ist sich des Datenhostings und der Replikation seiner Anwendungen bewusst und interagiert mit Tupperware √ºber die TaskControl-Schnittstelle, um Containerbetriebe ohne direkte Anwendungsbeteiligung zu planen.  Diese Integration vereinfacht die Verwaltung von Stateful Services erheblich, TaskControl bietet jedoch mehr.  Unsere umfangreiche Webschicht ist beispielsweise zustandslos und verwendet TaskControl, um die Geschwindigkeit von Aktualisierungen in Containern dynamisch anzupassen.  Infolgedessen kann die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Webschicht schnell mehrere Softwareversionen</a> pro Tag <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fertigstellen,</a> ohne die Verf√ºgbarkeit zu beeintr√§chtigen. </p><br><h3 id="upravlenie-serverami-v-datacentrah">  Serververwaltung in Rechenzentren </h3><br><p>  Als Tupperware 2011 zum ersten Mal erschien, kontrollierte ein separater Scheduler jeden Servercluster.  Dann war der Facebook-Cluster eine Gruppe von Server-Racks, die mit einem Netzwerk-Switch verbunden waren, und das Rechenzentrum enthielt mehrere Cluster.  Der Scheduler kann Server nur in einem Cluster verwalten, dh die Aufgabe kann nicht auf mehrere Cluster ausgedehnt werden.  Unsere Infrastruktur wuchs, wir schrieben zunehmend Cluster ab.  Da Tupperware die Aufgabe nicht ohne √Ñnderungen vom stillgelegten Cluster auf andere Cluster √ºbertragen konnte, war viel Aufwand und sorgf√§ltige Koordination zwischen Anwendungsentwicklern und Rechenzentrumsbetreibern erforderlich.  Dieser Prozess f√ºhrte zu einer Verschwendung von Ressourcen, wenn die Server aufgrund des Stilllegungsverfahrens monatelang inaktiv waren. </p><br><p>  Wir haben einen Ressourcenbroker erstellt, um das Problem der Au√üerbetriebnahme von Clustern zu l√∂sen und andere Arten von Wartungsaufgaben zu koordinieren.  Der Ressourcenbroker √ºberwacht alle dem Server zugeordneten physischen Informationen und entscheidet dynamisch, welcher Scheduler jeden Server verwaltet.  Durch die dynamische Bindung von Servern an Scheduler kann der Scheduler Server in verschiedenen Rechenzentren verwalten.  Da der Tupperware-Job nicht mehr auf einen Cluster beschr√§nkt ist, k√∂nnen Tupperware-Benutzer festlegen, wie Container auf die Fehlerdom√§nen verteilt werden sollen.  Beispielsweise kann ein Entwickler seine Absicht erkl√§ren (z. B. "Meine Aufgabe auf 2 Fehlerdom√§nen in der PRN-Region ausf√ºhren"), ohne bestimmte Verf√ºgbarkeitszonen anzugeben.  Tupperware selbst findet die richtigen Server, um diese Absicht auch bei der Au√üerbetriebnahme eines Clusters oder Dienstes zu verwirklichen. </p><br><h3 id="masshtabirovanie-dlya-podderzhki-vsey-globalnoy-sistemy">  Skalierung zur Unterst√ºtzung des gesamten globalen Systems </h3><br><p>  In der Vergangenheit wurde unsere Infrastruktur in Hunderte dedizierter Serverpools f√ºr einzelne Teams unterteilt.  Aufgrund der Fragmentierung und des Fehlens von Standards hatten wir hohe Transaktionskosten und es war schwieriger, nicht genutzte Server wieder zu verwenden.  Auf der letztj√§hrigen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systems @ Scale-</a> Konferenz haben wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Infrastructure as a Service (IaaS) vorgestellt</a> , mit dem unsere Infrastruktur in eine gro√üe, einheitliche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Serverflotte</a> integriert werden soll.  Eine einzelne Serverflotte hat jedoch ihre eigenen Schwierigkeiten.  Es muss bestimmte Anforderungen erf√ºllen: </p><br><ul><li>  <strong>Skalierbarkeit.</strong>  Unsere Infrastruktur wuchs durch die Hinzuf√ºgung von Rechenzentren in jeder Region.  Server sind kleiner und energieeffizienter geworden, daher gibt es in jeder Region viel mehr.  Infolgedessen kann ein einzelner Scheduler f√ºr eine Region die Anzahl der Container nicht bew√§ltigen, die auf Hunderttausenden von Servern in jeder Region ausgef√ºhrt werden k√∂nnen. </li><li>  <strong>Zuverl√§ssigkeit</strong>  Selbst wenn der Umfang des Schedulers aufgrund des gro√üen Umfangs des Schedulers so vergr√∂√üert werden kann, ist das Fehlerrisiko h√∂her und der gesamte Bereich der Container kann un√ºberschaubar werden. </li><li>  <strong>Fehlertoleranz.</strong>  Bei einem gro√üen Infrastrukturausfall (z. B. aufgrund eines Netzwerkausfalls oder eines Stromausfalls fallen die Server, auf denen der Scheduler ausgef√ºhrt wird, aus) hat nur ein Teil der Server in der Region negative Folgen. </li><li>  <strong>Benutzerfreundlichkeit.</strong>  M√∂glicherweise m√ºssen Sie mehrere unabh√§ngige Scheduler in einer Region ausf√ºhren.  Aus praktischen Gr√ºnden vereinfacht ein einziger Einstiegspunkt in einen gemeinsamen Pool in der Region das Kapazit√§ts- und Jobmanagement. </li></ul><br><p>  Wir haben den Scheduler in Shards unterteilt, um Probleme bei der Unterst√ºtzung eines gro√üen gemeinsamen Pools zu l√∂sen.  Jeder Scheduler-Shard verwaltet seine Aufgaben in der Region, wodurch das mit dem Scheduler verbundene Risiko verringert wird.  Wenn der Gesamtpool w√§chst, k√∂nnen wir weitere Scheduler-Shards hinzuf√ºgen.  F√ºr Tupperware-Benutzer sehen Shards und Proxy-Scheduler wie ein einziges Bedienfeld aus.  Sie m√ºssen nicht mit einer Reihe von Shards arbeiten, die Aufgaben koordinieren.  Die Scheduler-Shards unterscheiden sich grundlegend von den zuvor verwendeten Cluster-Schedulern, als das Control Panel ohne statische Trennung des gemeinsamen Serverpools gem√§√ü der Netzwerktopologie aufgeteilt wurde. </p><br><h3 id="povyshenie-effektivnosti-ispolzovaniya-s-pomoschyu-elastichnyh-vychisleniy">  Verbesserung der Nutzung durch Elastic Computing </h3><br><p>  Je gr√∂√üer unsere Infrastruktur ist, desto wichtiger ist es, unsere Server effizient zu nutzen, um die Infrastrukturkosten zu optimieren und die Last zu reduzieren.  Es gibt zwei M√∂glichkeiten, die Servernutzung zu verbessern: </p><br><ul><li>  Flexibles Computing - Reduzieren Sie den Umfang der Onlinedienste in ruhigen Stunden und verwenden Sie die freigegebenen Server f√ºr Offline-Lasten, z. B. f√ºr maschinelles Lernen und MapReduce-Aufgaben. </li><li>  √úberm√§√üiges Laden - Hosten Sie Onlinedienste und Batch-Workloads auf denselben Servern, damit Batch-Ladevorg√§nge mit niedriger Priorit√§t ausgef√ºhrt werden. </li></ul><br><p>  Der Engpass in unseren Rechenzentren ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Energieverbrauch</a> .  Daher bevorzugen wir kleine, energieeffiziente Server, die zusammen mehr Rechenleistung bieten.  Leider ist √ºberm√§√üiges Laden auf kleinen Servern mit einer geringen Menge an Prozessorressourcen und Speicher weniger effizient.  Nat√ºrlich k√∂nnen wir mehrere Container mit kleinen Diensten auf einem kleinen, energieeffizienten Server platzieren, die wenig Prozessorressourcen und Speicher verbrauchen, aber gro√üe Dienste weisen in dieser Situation eine geringe Leistung auf.  Daher empfehlen wir den Entwicklern unserer gro√üen Services, diese so zu optimieren, dass sie den gesamten Server nutzen. </p><br><p>  Grunds√§tzlich verbessern wir die Auslastung mit Elastic Computing.  Die Nutzungsintensit√§t vieler unserer gro√üen Dienste, z. B. Newsfeeds, Nachrichtenfunktionen und Front-End-Web-Level, h√§ngt von der Tageszeit ab.  Wir reduzieren absichtlich den Umfang von Onlinediensten w√§hrend ruhiger Stunden und verwenden die freigegebenen Server f√ºr Offline-Lasten, beispielsweise f√ºr maschinelles Lernen und MapReduce-Aufgaben. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/6w/zu/dp/6wzudppzm9tobgoryvtrssaxlra.jpeg"></a> </p><br><p>  Aus Erfahrung wissen wir, dass es am besten ist, ganze Server als Einheiten elastischer Leistung bereitzustellen, da gro√üe Dienste sowohl die Hauptgeber als auch die Hauptverbraucher elastischer Leistung sind und f√ºr die Verwendung ganzer Server optimiert sind.  Wenn der Server in den ruhigen Stunden vom Onlinedienst befreit wird, gibt der Ressourcenbroker den Server an den Scheduler zur vor√ºbergehenden Verwendung weiter, damit er offline geladen wird.  Wenn in einem Onlinedienst ein Lastpeak auftritt, ruft der Ressourcenbroker den ausgeliehenen Server schnell zur√ºck und gibt ihn zusammen mit dem Planer an den Onlinedienst zur√ºck. </p><br><h3 id="usvoennye-uroki-i-plany-na-buduschee">  Lessons Learned und Zukunftspl√§ne </h3><br><p>  In den letzten 8 Jahren haben wir Tupperware entwickelt, um mit der rasanten Entwicklung von Facebook Schritt zu halten.  Wir sprechen √ºber das, was wir gelernt haben, und hoffen, dass es anderen hilft, schnell wachsende Infrastrukturen zu verwalten: </p><br><ul><li>  Richten Sie eine flexible Kommunikation zwischen dem Control Panel und den von ihm verwalteten Servern ein.  Diese Flexibilit√§t erm√∂glicht es dem Control Panel, Server in verschiedenen Rechenzentren zu verwalten, die Au√üerbetriebnahme und Wartung von Clustern zu automatisieren und eine dynamische Energieverteilung mithilfe flexibler Datenverarbeitung bereitzustellen. </li><li>  Mit einem einzigen Bedienfeld in der Region wird es einfacher, mit Aufgaben zu arbeiten und eine gro√üe gemeinsame Serverflotte einfacher zu verwalten.  Bitte beachten Sie, dass das Bedienfeld einen einzelnen Einstiegspunkt unterst√ºtzt, auch wenn seine interne Struktur aus Gr√ºnden der Skalierung oder Fehlertoleranz unterteilt ist. </li><li>  Mithilfe des Plug-In-Modells kann das Bedienfeld externe Anwendungen √ºber bevorstehende Containerbetriebe informieren.  Dar√ºber hinaus k√∂nnen Stateful Services die Plugin-Schnittstelle verwenden, um die Containerverwaltung zu konfigurieren.  Mit diesem Plug-In-Modell bietet das Bedienfeld Einfachheit und bedient effektiv viele verschiedene Stateful Services. </li><li>  Wir glauben, dass Elastic Computing, bei dem wir ganze Server f√ºr Batch-Jobs, maschinelles Lernen und andere nicht dringende Dienste von Geberdiensten √ºbernehmen, der beste Weg ist, um die Effizienz der Verwendung kleiner und energieeffizienter Server zu steigern. </li></ul><br><p>  Wir beginnen gerade mit der Implementierung eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzelnen globalen gemeinsamen Serverparks</a> .  Jetzt befinden sich ungef√§hr 20% unserer Server im gemeinsamen Pool.  Um 100% zu erreichen, m√ºssen Sie viele Probleme l√∂sen, einschlie√ülich der Unterst√ºtzung eines gemeinsamen Pools f√ºr Speichersysteme, der Automatisierung der Wartung, der Verwaltung der Anforderungen verschiedener Clients, der Verbesserung der Servernutzung und der Unterst√ºtzung f√ºr Workloads beim maschinellen Lernen.  Wir k√∂nnen es kaum erwarten, diese Aufgaben zu bew√§ltigen und unsere Erfolge zu teilen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455579/">https://habr.com/ru/post/de455579/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455565/index.html">Kann der Geist das Universum vort√§uschen?</a></li>
<li><a href="../de455569/index.html">Wir laden Sie zur Tarantool-Konferenz am 17. Juni ein</a></li>
<li><a href="../de455571/index.html">DB-Cursor in der Lehre</a></li>
<li><a href="../de455575/index.html">Neural Matching: So passen Sie Inhalte an die Realit√§t von Google an</a></li>
<li><a href="../de455577/index.html">SDL 2 Lektionen: Lektion 3 - Ereignisse</a></li>
<li><a href="../de455580/index.html">Animationen f√ºr mobile Anwendungen sind ein Muss</a></li>
<li><a href="../de455582/index.html">Navigation im Laden: durch Augmented Reality zum gew√ºnschten Regal</a></li>
<li><a href="../de455584/index.html">Kundenspezifische Interviews mit den internen Kr√§ften des Unternehmens: durch Fehler zu Entdeckungen</a></li>
<li><a href="../de455586/index.html">Vorlesungsreihe √ºber Robotik von Professor Gregor Sch√∂ner, Direktor des Instituts f√ºr Neuroinformatik (INI) Bochum</a></li>
<li><a href="../de455588/index.html">Wie Sie Ihre Gemeinde erziehen, um nicht mit einem Tamburin zu tanzen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>