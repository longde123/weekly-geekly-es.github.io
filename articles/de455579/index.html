<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧚🏽 🚐 🍓 Tupperware: Facebook-Killer Kubernetes? 🤟🏿 🤱🏿 ✌🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Effizientes und zuverlässiges Cluster-Management in jeder Größenordnung mit Tupperware 





 Heute haben wir auf der Systems @ Scale-Konferenz Tupper...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tupperware: Facebook-Killer Kubernetes?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/455579/"><p>  Effizientes und zuverlässiges Cluster-Management in jeder Größenordnung mit Tupperware </p><br><p><img src="https://habrastorage.org/webt/gm/mv/jn/gmmvjn5ev7lk2kyhaiejighzrf0.jpeg"></p><br><p>  Heute <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">haben</a> wir auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Systems @ Scale-Konferenz</a> Tupperware vorgestellt, unser Cluster-Management-System, das Container auf Millionen von Servern orchestriert, auf denen fast alle unsere Services funktionieren.  Wir haben Tupperware erstmals im Jahr 2011 eingeführt. Seitdem ist unsere Infrastruktur von einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rechenzentrum auf</a> bis zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">15 geoverteilte Rechenzentren</a> angewachsen.  Während dieser ganzen Zeit stand Tupperware nicht still und entwickelte sich mit uns.  Wir werden Ihnen sagen, in welchen Situationen Tupperware erstklassiges Cluster-Management bietet, einschließlich praktischer Unterstützung für Stateful Services, eines einzigen Control Panels für alle Rechenzentren und der Möglichkeit, die Energie in Echtzeit auf die Services zu verteilen.  Und wir werden die Lektionen teilen, die wir bei der Entwicklung unserer Infrastruktur gelernt haben. </p><br><p> Tupperware führt verschiedene Aufgaben aus.  Anwendungsentwickler verwenden es, um Anwendungen bereitzustellen und zu verwalten.  Es packt die Code- und Anwendungsabhängigkeiten in ein Image und liefert sie in Form von Containern an die Server.  Container bieten Isolation zwischen Anwendungen auf demselben Server, sodass Entwickler mit der Anwendungslogik beschäftigt sind und nicht darüber nachdenken, wie Server gefunden oder Updates gesteuert werden.  Tupperware überwacht auch die Leistung des Servers. Wenn ein Fehler festgestellt wird, werden Container vom problematischen Server übertragen. </p><a name="habracut"></a><br><p>  Kapazitätsplanungsingenieure verwenden Tupperware, um die Serverkapazitäten je nach Budget und Einschränkungen in Teams zu verteilen.  Sie verwenden es auch, um die Servernutzung zu verbessern.  Betreiber von Rechenzentren wenden sich an Tupperware, um Container ordnungsgemäß auf Rechenzentren zu verteilen und Container während der Wartung anzuhalten oder zu verschieben.  Aus diesem Grund erfordert die Wartung von Servern, Netzwerken und Geräten nur eine minimale menschliche Beteiligung. </p><br><h3 id="arhitektura-tupperware">  Tupperware-Architektur </h3><br><p> <a href=""><img src="https://habrastorage.org/webt/e7/q1/oz/e7q1ozhv85xlsvgczzofpgwoikg.jpeg"></a> </p><br><p>  <em>Architektur Tupperware PRN ist eine der Regionen unserer Rechenzentren.</em>  <em>Die Region besteht aus mehreren Rechenzentrumsgebäuden (PRN1 und PRN2) in der Nähe.</em>  <em>Wir planen ein Control Panel, das alle Server in einer Region verwaltet.</em> </p><br><p>  Anwendungsentwickler stellen Dienste in Form von Tupperware-Jobs bereit.  Eine Aufgabe besteht aus mehreren Containern, die normalerweise denselben Anwendungscode ausführen. </p><br><p>  Tupperware ist für die Containerbereitstellung und das Lebenszyklusmanagement verantwortlich.  Es besteht aus mehreren Komponenten: </p><br><ul><li>  Das Tupperware-Frontend bietet eine API für die Benutzeroberfläche, die CLI und andere Automatisierungstools, über die Sie mit Tupperware interagieren können.  Sie verbergen die gesamte interne Struktur vor Tupperware-Jobbesitzern. </li><li>  Der Tupperware Scheduler ist das Control Panel, das für die Verwaltung des Container- und Joblebenszyklus verantwortlich ist.  Es wird auf regionaler und globaler Ebene bereitgestellt, wobei ein regionaler Scheduler Server in einer Region verwaltet und ein globaler Scheduler Server aus verschiedenen Regionen verwaltet.  Der Scheduler ist in Shards unterteilt, und jeder Shard steuert eine Reihe von Aufgaben. </li><li>  Der Scheduler-Proxy in Tupperware verbirgt das interne Sharding und bietet Tupperware-Benutzern ein praktisches einheitliches Kontrollfeld. </li><li>  Der Tupperware Distributor weist den Servern Container zu.  Der Scheduler ist für das Stoppen, Starten, Aktualisieren und Fehlschlagen von Containern verantwortlich.  Derzeit kann ein einzelner Distributor eine gesamte Region verwalten, ohne sich in Shards aufzuteilen.  (Beachten Sie den Unterschied in der Terminologie. Beispielsweise entspricht der Scheduler in Tupperware dem Control Panel in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kubernetes</a> , und der Tupperware-Distributor wird in Kubernetes als Scheduler bezeichnet.) </li><li>  Der Ressourcenbroker speichert die Wahrheitsquelle für die Server- und Serviceereignisse.  Wir führen für jedes Rechenzentrum einen Ressourcenbroker aus, der alle Serverinformationen in diesem Rechenzentrum speichert.  Ein Ressourcenbroker und ein Kapazitätsverwaltungssystem oder ein Ressourcenzuweisungssystem entscheiden dynamisch, welche Scheduler-Bereitstellung welchen Server steuert.  Der Integritätsprüfungsdienst überwacht Server und speichert Daten zu ihrem Zustand im Ressourcenbroker.  Wenn der Server Probleme hat oder gewartet werden muss, weist der Ressourcenbroker den Distributor und den Scheduler an, die Container anzuhalten oder auf andere Server zu übertragen. </li><li>  Tupperware Agent ist ein Daemon, der auf jedem Server ausgeführt wird und Container vorbereitet und entfernt.  Anwendungen arbeiten im Container, wodurch sie besser isoliert und reproduzierbar sind.  Auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">letztjährigen Systems @ Scale-Konferenz haben</a> wir bereits beschrieben, wie einzelne Tupperware-Container mithilfe von Images, btrfs, cgroupv2 und systemd erstellt werden. </li></ul><br><h3 id="otlichitelnye-osobennosti-tupperware">  Besonderheiten von Tupperware </h3><br><p>  Tupperware ist anderen Cluster-Management-Systemen wie Kubernetes und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mesos</a> sehr ähnlich, es gibt jedoch einige Unterschiede: </p><br><ul><li>  Native Unterstützung für Stateful Services. </li><li>  Ein einziges Control Panel für Server in verschiedenen Rechenzentren zur Automatisierung der Lieferung von Containern basierend auf Absicht, Stilllegung von Clustern und Wartung. </li><li>  Klare Trennung des Bedienfelds zum Zoomen. </li><li>  Mit flexiblen Berechnungen können Sie die Leistung in Echtzeit auf die Dienste verteilen. </li></ul><br><p>  Wir haben diese coolen Funktionen entwickelt, um eine Vielzahl von zustandslosen und zustandsbehafteten Anwendungen in einem riesigen globalen gemeinsam genutzten Serverpark zu unterstützen. </p><br><h3 id="vstroennaya-podderzhka-stateful-sevisov">  Native Unterstützung für Stateful Services. </h3><br><p>  Tupperware verwaltet viele wichtige Stateful Services, die persistente Produktdaten für Facebook, Instagram, Messenger und WhatsApp speichern.  Dies können große Schlüssel-Wert-Paare (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ZippyDB</a> ) und Überwachungsdatenspeicher (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ODS Gorilla</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scuba</a> ) sein.  Die Aufrechterhaltung zustandsbehafteter Dienste ist nicht einfach, da das System sicherstellen muss, dass Containerlieferungen großen Ausfällen standhalten können, einschließlich eines Stromausfalls oder eines Stromausfalls.  Obwohl herkömmliche Methoden wie das Verteilen von Containern über Fehlerdomänen für zustandslose Dienste gut geeignet sind, benötigen Stateful Services zusätzliche Unterstützung. </p><br><p>  Wenn beispielsweise aufgrund eines Serverausfalls ein Replikat der Datenbank nicht mehr verfügbar ist, muss eine automatische Wartung zugelassen werden, mit der die Kernel auf 50 Servern aus einem 10-Tausendstel-Pool aktualisiert werden?  Das hängt von der Situation ab.  Wenn sich auf einem dieser 50 Server ein anderes Replikat derselben Datenbank befindet, ist es besser zu warten und nicht zwei Replikate gleichzeitig zu verlieren.  Um dynamisch Entscheidungen über die Wartung und den Zustand des Systems treffen zu können, benötigen Sie Informationen zur internen Datenreplikation und zur Standortlogik jedes Stateful Service. </p><br><p>  Über die TaskControl-Schnittstelle können Stateful Services Entscheidungen beeinflussen, die sich auf die Datenverfügbarkeit auswirken.  Über diese Schnittstelle benachrichtigt der Scheduler externe Anwendungen über Containervorgänge (Neustart, Aktualisierung, Migration, Wartung).  Der Stateful-Dienst implementiert einen Controller, der Tupperware mitteilt, wann jeder Vorgang sicher ausgeführt werden kann, und diese Vorgänge können ausgetauscht oder vorübergehend verzögert werden.  Im obigen Beispiel weist der Datenbankcontroller Tupperware möglicherweise an, 49 der 50 Server zu aktualisieren, berührt jedoch bisher keinen bestimmten Server (X).  Wenn der Kernel-Aktualisierungszeitraum verstrichen ist und die Datenbank das Problemreplikat immer noch nicht wiederherstellen kann, aktualisiert Tupperware den X-Server weiterhin. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/xi/5q/xuxi5qpox1v3gpc6khbipxgna0i.jpeg"></a> </p><br><p>  Viele Stateful Services in Tupperware verwenden TaskControl nicht direkt, sondern über ShardManager, eine gängige Plattform zum Erstellen von Stateful Services auf Facebook.  Mit Tupperware können Entwickler angeben, wie Container auf Rechenzentren verteilt werden sollen.  Mit ShardManager geben Entwickler ihre Absicht an, wie Data Shards auf Container verteilt werden sollen.  ShardManager ist sich des Datenhostings und der Replikation seiner Anwendungen bewusst und interagiert mit Tupperware über die TaskControl-Schnittstelle, um Containerbetriebe ohne direkte Anwendungsbeteiligung zu planen.  Diese Integration vereinfacht die Verwaltung von Stateful Services erheblich, TaskControl bietet jedoch mehr.  Unsere umfangreiche Webschicht ist beispielsweise zustandslos und verwendet TaskControl, um die Geschwindigkeit von Aktualisierungen in Containern dynamisch anzupassen.  Infolgedessen kann die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Webschicht schnell mehrere Softwareversionen</a> pro Tag <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fertigstellen,</a> ohne die Verfügbarkeit zu beeinträchtigen. </p><br><h3 id="upravlenie-serverami-v-datacentrah">  Serververwaltung in Rechenzentren </h3><br><p>  Als Tupperware 2011 zum ersten Mal erschien, kontrollierte ein separater Scheduler jeden Servercluster.  Dann war der Facebook-Cluster eine Gruppe von Server-Racks, die mit einem Netzwerk-Switch verbunden waren, und das Rechenzentrum enthielt mehrere Cluster.  Der Scheduler kann Server nur in einem Cluster verwalten, dh die Aufgabe kann nicht auf mehrere Cluster ausgedehnt werden.  Unsere Infrastruktur wuchs, wir schrieben zunehmend Cluster ab.  Da Tupperware die Aufgabe nicht ohne Änderungen vom stillgelegten Cluster auf andere Cluster übertragen konnte, war viel Aufwand und sorgfältige Koordination zwischen Anwendungsentwicklern und Rechenzentrumsbetreibern erforderlich.  Dieser Prozess führte zu einer Verschwendung von Ressourcen, wenn die Server aufgrund des Stilllegungsverfahrens monatelang inaktiv waren. </p><br><p>  Wir haben einen Ressourcenbroker erstellt, um das Problem der Außerbetriebnahme von Clustern zu lösen und andere Arten von Wartungsaufgaben zu koordinieren.  Der Ressourcenbroker überwacht alle dem Server zugeordneten physischen Informationen und entscheidet dynamisch, welcher Scheduler jeden Server verwaltet.  Durch die dynamische Bindung von Servern an Scheduler kann der Scheduler Server in verschiedenen Rechenzentren verwalten.  Da der Tupperware-Job nicht mehr auf einen Cluster beschränkt ist, können Tupperware-Benutzer festlegen, wie Container auf die Fehlerdomänen verteilt werden sollen.  Beispielsweise kann ein Entwickler seine Absicht erklären (z. B. "Meine Aufgabe auf 2 Fehlerdomänen in der PRN-Region ausführen"), ohne bestimmte Verfügbarkeitszonen anzugeben.  Tupperware selbst findet die richtigen Server, um diese Absicht auch bei der Außerbetriebnahme eines Clusters oder Dienstes zu verwirklichen. </p><br><h3 id="masshtabirovanie-dlya-podderzhki-vsey-globalnoy-sistemy">  Skalierung zur Unterstützung des gesamten globalen Systems </h3><br><p>  In der Vergangenheit wurde unsere Infrastruktur in Hunderte dedizierter Serverpools für einzelne Teams unterteilt.  Aufgrund der Fragmentierung und des Fehlens von Standards hatten wir hohe Transaktionskosten und es war schwieriger, nicht genutzte Server wieder zu verwenden.  Auf der letztjährigen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Systems @ Scale-</a> Konferenz haben wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Infrastructure as a Service (IaaS) vorgestellt</a> , mit dem unsere Infrastruktur in eine große, einheitliche <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Serverflotte</a> integriert werden soll.  Eine einzelne Serverflotte hat jedoch ihre eigenen Schwierigkeiten.  Es muss bestimmte Anforderungen erfüllen: </p><br><ul><li>  <strong>Skalierbarkeit.</strong>  Unsere Infrastruktur wuchs durch die Hinzufügung von Rechenzentren in jeder Region.  Server sind kleiner und energieeffizienter geworden, daher gibt es in jeder Region viel mehr.  Infolgedessen kann ein einzelner Scheduler für eine Region die Anzahl der Container nicht bewältigen, die auf Hunderttausenden von Servern in jeder Region ausgeführt werden können. </li><li>  <strong>Zuverlässigkeit</strong>  Selbst wenn der Umfang des Schedulers aufgrund des großen Umfangs des Schedulers so vergrößert werden kann, ist das Fehlerrisiko höher und der gesamte Bereich der Container kann unüberschaubar werden. </li><li>  <strong>Fehlertoleranz.</strong>  Bei einem großen Infrastrukturausfall (z. B. aufgrund eines Netzwerkausfalls oder eines Stromausfalls fallen die Server, auf denen der Scheduler ausgeführt wird, aus) hat nur ein Teil der Server in der Region negative Folgen. </li><li>  <strong>Benutzerfreundlichkeit.</strong>  Möglicherweise müssen Sie mehrere unabhängige Scheduler in einer Region ausführen.  Aus praktischen Gründen vereinfacht ein einziger Einstiegspunkt in einen gemeinsamen Pool in der Region das Kapazitäts- und Jobmanagement. </li></ul><br><p>  Wir haben den Scheduler in Shards unterteilt, um Probleme bei der Unterstützung eines großen gemeinsamen Pools zu lösen.  Jeder Scheduler-Shard verwaltet seine Aufgaben in der Region, wodurch das mit dem Scheduler verbundene Risiko verringert wird.  Wenn der Gesamtpool wächst, können wir weitere Scheduler-Shards hinzufügen.  Für Tupperware-Benutzer sehen Shards und Proxy-Scheduler wie ein einziges Bedienfeld aus.  Sie müssen nicht mit einer Reihe von Shards arbeiten, die Aufgaben koordinieren.  Die Scheduler-Shards unterscheiden sich grundlegend von den zuvor verwendeten Cluster-Schedulern, als das Control Panel ohne statische Trennung des gemeinsamen Serverpools gemäß der Netzwerktopologie aufgeteilt wurde. </p><br><h3 id="povyshenie-effektivnosti-ispolzovaniya-s-pomoschyu-elastichnyh-vychisleniy">  Verbesserung der Nutzung durch Elastic Computing </h3><br><p>  Je größer unsere Infrastruktur ist, desto wichtiger ist es, unsere Server effizient zu nutzen, um die Infrastrukturkosten zu optimieren und die Last zu reduzieren.  Es gibt zwei Möglichkeiten, die Servernutzung zu verbessern: </p><br><ul><li>  Flexibles Computing - Reduzieren Sie den Umfang der Onlinedienste in ruhigen Stunden und verwenden Sie die freigegebenen Server für Offline-Lasten, z. B. für maschinelles Lernen und MapReduce-Aufgaben. </li><li>  Übermäßiges Laden - Hosten Sie Onlinedienste und Batch-Workloads auf denselben Servern, damit Batch-Ladevorgänge mit niedriger Priorität ausgeführt werden. </li></ul><br><p>  Der Engpass in unseren Rechenzentren ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Energieverbrauch</a> .  Daher bevorzugen wir kleine, energieeffiziente Server, die zusammen mehr Rechenleistung bieten.  Leider ist übermäßiges Laden auf kleinen Servern mit einer geringen Menge an Prozessorressourcen und Speicher weniger effizient.  Natürlich können wir mehrere Container mit kleinen Diensten auf einem kleinen, energieeffizienten Server platzieren, die wenig Prozessorressourcen und Speicher verbrauchen, aber große Dienste weisen in dieser Situation eine geringe Leistung auf.  Daher empfehlen wir den Entwicklern unserer großen Services, diese so zu optimieren, dass sie den gesamten Server nutzen. </p><br><p>  Grundsätzlich verbessern wir die Auslastung mit Elastic Computing.  Die Nutzungsintensität vieler unserer großen Dienste, z. B. Newsfeeds, Nachrichtenfunktionen und Front-End-Web-Level, hängt von der Tageszeit ab.  Wir reduzieren absichtlich den Umfang von Onlinediensten während ruhiger Stunden und verwenden die freigegebenen Server für Offline-Lasten, beispielsweise für maschinelles Lernen und MapReduce-Aufgaben. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/6w/zu/dp/6wzudppzm9tobgoryvtrssaxlra.jpeg"></a> </p><br><p>  Aus Erfahrung wissen wir, dass es am besten ist, ganze Server als Einheiten elastischer Leistung bereitzustellen, da große Dienste sowohl die Hauptgeber als auch die Hauptverbraucher elastischer Leistung sind und für die Verwendung ganzer Server optimiert sind.  Wenn der Server in den ruhigen Stunden vom Onlinedienst befreit wird, gibt der Ressourcenbroker den Server an den Scheduler zur vorübergehenden Verwendung weiter, damit er offline geladen wird.  Wenn in einem Onlinedienst ein Lastpeak auftritt, ruft der Ressourcenbroker den ausgeliehenen Server schnell zurück und gibt ihn zusammen mit dem Planer an den Onlinedienst zurück. </p><br><h3 id="usvoennye-uroki-i-plany-na-buduschee">  Lessons Learned und Zukunftspläne </h3><br><p>  In den letzten 8 Jahren haben wir Tupperware entwickelt, um mit der rasanten Entwicklung von Facebook Schritt zu halten.  Wir sprechen über das, was wir gelernt haben, und hoffen, dass es anderen hilft, schnell wachsende Infrastrukturen zu verwalten: </p><br><ul><li>  Richten Sie eine flexible Kommunikation zwischen dem Control Panel und den von ihm verwalteten Servern ein.  Diese Flexibilität ermöglicht es dem Control Panel, Server in verschiedenen Rechenzentren zu verwalten, die Außerbetriebnahme und Wartung von Clustern zu automatisieren und eine dynamische Energieverteilung mithilfe flexibler Datenverarbeitung bereitzustellen. </li><li>  Mit einem einzigen Bedienfeld in der Region wird es einfacher, mit Aufgaben zu arbeiten und eine große gemeinsame Serverflotte einfacher zu verwalten.  Bitte beachten Sie, dass das Bedienfeld einen einzelnen Einstiegspunkt unterstützt, auch wenn seine interne Struktur aus Gründen der Skalierung oder Fehlertoleranz unterteilt ist. </li><li>  Mithilfe des Plug-In-Modells kann das Bedienfeld externe Anwendungen über bevorstehende Containerbetriebe informieren.  Darüber hinaus können Stateful Services die Plugin-Schnittstelle verwenden, um die Containerverwaltung zu konfigurieren.  Mit diesem Plug-In-Modell bietet das Bedienfeld Einfachheit und bedient effektiv viele verschiedene Stateful Services. </li><li>  Wir glauben, dass Elastic Computing, bei dem wir ganze Server für Batch-Jobs, maschinelles Lernen und andere nicht dringende Dienste von Geberdiensten übernehmen, der beste Weg ist, um die Effizienz der Verwendung kleiner und energieeffizienter Server zu steigern. </li></ul><br><p>  Wir beginnen gerade mit der Implementierung eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einzelnen globalen gemeinsamen Serverparks</a> .  Jetzt befinden sich ungefähr 20% unserer Server im gemeinsamen Pool.  Um 100% zu erreichen, müssen Sie viele Probleme lösen, einschließlich der Unterstützung eines gemeinsamen Pools für Speichersysteme, der Automatisierung der Wartung, der Verwaltung der Anforderungen verschiedener Clients, der Verbesserung der Servernutzung und der Unterstützung für Workloads beim maschinellen Lernen.  Wir können es kaum erwarten, diese Aufgaben zu bewältigen und unsere Erfolge zu teilen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de455579/">https://habr.com/ru/post/de455579/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de455565/index.html">Kann der Geist das Universum vortäuschen?</a></li>
<li><a href="../de455569/index.html">Wir laden Sie zur Tarantool-Konferenz am 17. Juni ein</a></li>
<li><a href="../de455571/index.html">DB-Cursor in der Lehre</a></li>
<li><a href="../de455575/index.html">Neural Matching: So passen Sie Inhalte an die Realität von Google an</a></li>
<li><a href="../de455577/index.html">SDL 2 Lektionen: Lektion 3 - Ereignisse</a></li>
<li><a href="../de455580/index.html">Animationen für mobile Anwendungen sind ein Muss</a></li>
<li><a href="../de455582/index.html">Navigation im Laden: durch Augmented Reality zum gewünschten Regal</a></li>
<li><a href="../de455584/index.html">Kundenspezifische Interviews mit den internen Kräften des Unternehmens: durch Fehler zu Entdeckungen</a></li>
<li><a href="../de455586/index.html">Vorlesungsreihe über Robotik von Professor Gregor Schöner, Direktor des Instituts für Neuroinformatik (INI) Bochum</a></li>
<li><a href="../de455588/index.html">Wie Sie Ihre Gemeinde erziehen, um nicht mit einem Tamburin zu tanzen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>