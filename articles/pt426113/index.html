<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßëüèø‚Äçü§ù‚ÄçüßëüèΩ ü§≥ ü§Æ Google News e Leo Tolstoy: visualiza√ß√£o de representa√ß√µes vetoriais de palavras com t-SNE üëπ üçí üòπ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cada um de n√≥s percebe os textos √† sua maneira, sejam not√≠cias na Internet, poesia ou romances cl√°ssicos. O mesmo se aplica aos algoritmos e m√©todos d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Google News e Leo Tolstoy: visualiza√ß√£o de representa√ß√µes vetoriais de palavras com t-SNE</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/426113/"><img src="https://habrastorage.org/webt/6c/ux/7m/6cux7mvmp3phb8d8efjwmqrb_yc.gif"><br><br>  Cada um de n√≥s percebe os textos √† sua maneira, sejam not√≠cias na Internet, poesia ou romances cl√°ssicos.  O mesmo se aplica aos algoritmos e m√©todos de aprendizado de m√°quina, que, via de regra, percebem textos em forma matem√°tica, na forma de um espa√ßo vetorial multidimensional. <br><br>  O artigo √© dedicado √† visualiza√ß√£o usando t-SNE calculado pelas representa√ß√µes vetoriais multidimensionais de palavras do Word2Vec.  A visualiza√ß√£o ajudar√° a entender melhor o princ√≠pio do Word2Vec e como interpretar o relacionamento entre os vetores de palavras antes do uso posterior em redes neurais e outros algoritmos de aprendizado de m√°quina.  O artigo foca na visualiza√ß√£o, pesquisas adicionais e an√°lise de dados n√£o s√£o consideradas.  Como fonte de dados, usamos artigos do Google Not√≠cias e obras cl√°ssicas de L.N.  Tolstoi.  Escreveremos o c√≥digo em Python no Jupyter Notebook. <br><a name="habracut"></a><br><h1>  Incorpora√ß√£o estoc√°stica de vizinhos distribu√≠dos em T </h1><br>  O T-SNE √© um algoritmo de aprendizado de m√°quina para visualiza√ß√£o de dados com base no m√©todo de redu√ß√£o dimensional n√£o linear, descrito em detalhes no artigo original [1] e em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Habr√©</a> .  O princ√≠pio b√°sico da opera√ß√£o do t-SNE √© reduzir as dist√¢ncias entre pares entre os pontos, mantendo sua posi√ß√£o relativa.  Em outras palavras, o algoritmo mapeia dados multidimensionais para um espa√ßo de menor dimens√£o, mantendo a estrutura da vizinhan√ßa dos pontos. <br><br><h1>  Representa√ß√µes vetoriais de palavras e Word2Vec </h1><br>  Primeiro de tudo, precisamos apresentar as palavras em forma de vetor.  Para esta tarefa, escolhi o utilit√°rio de sem√¢ntica de distribui√ß√£o do Word2Vec, projetado para exibir o significado sem√¢ntico das palavras no espa√ßo vetorial.  O Word2Vec localiza relacionamentos entre palavras, assumindo que palavras semanticamente relacionadas s√£o encontradas em contextos semelhantes.  Voc√™ pode ler mais sobre o Word2Vec no artigo original [2], bem como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br>  Como entrada, utilizamos artigos do Google Not√≠cias e romances de L.N.  Tolstoi.  No primeiro caso, usaremos os vetores pr√©-treinados no conjunto de dados do Google Not√≠cias (cerca de 100 bilh√µes de palavras) publicado pelo Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na p√°gina do projeto</a> . <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gensim model = gensim.models.KeyedVectors.load_word2vec_format(<span class="hljs-string"><span class="hljs-string">'GoogleNews-vectors-negative300.bin'</span></span>, binary=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br>  Al√©m de vetores pr√©-treinados usando a biblioteca Gensim [3], treinaremos outro modelo nos textos de L.N.  Tolstoi.  Como o Word2Vec aceita uma matriz de frases como entrada, usamos o modelo pr√©-treinado do Punkt Senten√ßa Tokenizer do pacote NLTK para dividir automaticamente o texto em frases.  O modelo para o idioma russo pode ser baixado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_text</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> text = re.sub(<span class="hljs-string"><span class="hljs-string">'[^a-zA-Z--1-9]+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) text = re.sub(<span class="hljs-string"><span class="hljs-string">' +'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, text) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text.strip() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">prepare_for_w2v</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename_from, filename_to, lang)</span></span></span><span class="hljs-function">:</span></span> raw_text = codecs.open(filename_from, <span class="hljs-string"><span class="hljs-string">"r"</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'windows-1251'</span></span>).read() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename_to, <span class="hljs-string"><span class="hljs-string">'w'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sentence <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(raw_text, lang): print(preprocess_text(sentence.lower()), file=f)</code> </pre><br>  Em seguida, usando a biblioteca Gensim, treinaremos o modelo Word2Vec com os seguintes par√¢metros: <br><br><ul><li>  <i>tamanho = 200</i> - dimens√£o do espa√ßo de atributo; </li><li>  <i>window = 5</i> - o n√∫mero de palavras do contexto que o algoritmo analisa; </li><li>  <i>min_count = 5</i> - a palavra deve ocorrer pelo menos cinco vezes para que o modelo a <i>leve</i> em considera√ß√£o. </li></ul><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_word2vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> data = gensim.models.word2vec.LineSentence(filename) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Word2Vec(data, size=<span class="hljs-number"><span class="hljs-number">200</span></span>, window=<span class="hljs-number"><span class="hljs-number">5</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>, workers=multiprocessing.cpu_count())</code> </pre><br><h1>  Visualiza√ß√£o de representa√ß√µes vetoriais de palavras usando t-SNE </h1><br>  O T-SNE √© extremamente √∫til para visualizar semelhan√ßas entre objetos em um espa√ßo multidimensional.  √Ä medida que a quantidade de dados aumenta, torna-se cada vez mais dif√≠cil criar um gr√°fico visual; portanto, na pr√°tica, as palavras relacionadas s√£o combinadas em grupos para visualiza√ß√£o adicional.  Tomemos, por exemplo, algumas palavras de um dicion√°rio do modelo Word2Vec previamente treinado no Google Not√≠cias. <br><br><pre> <code class="python hljs">keys = [<span class="hljs-string"><span class="hljs-string">'Paris'</span></span>, <span class="hljs-string"><span class="hljs-string">'Python'</span></span>, <span class="hljs-string"><span class="hljs-string">'Sunday'</span></span>, <span class="hljs-string"><span class="hljs-string">'Tolstoy'</span></span>, <span class="hljs-string"><span class="hljs-string">'Twitter'</span></span>, <span class="hljs-string"><span class="hljs-string">'bachelor'</span></span>, <span class="hljs-string"><span class="hljs-string">'delivery'</span></span>, <span class="hljs-string"><span class="hljs-string">'election'</span></span>, <span class="hljs-string"><span class="hljs-string">'expensive'</span></span>, <span class="hljs-string"><span class="hljs-string">'experience'</span></span>, <span class="hljs-string"><span class="hljs-string">'financial'</span></span>, <span class="hljs-string"><span class="hljs-string">'food'</span></span>, <span class="hljs-string"><span class="hljs-string">'iOS'</span></span>, <span class="hljs-string"><span class="hljs-string">'peace'</span></span>, <span class="hljs-string"><span class="hljs-string">'release'</span></span>, <span class="hljs-string"><span class="hljs-string">'war'</span></span>] embedding_clusters = [] word_clusters = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> keys: embeddings = [] words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> similar_word, _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> model.most_similar(word, topn=<span class="hljs-number"><span class="hljs-number">30</span></span>): words.append(similar_word) embeddings.append(model[similar_word]) embedding_clusters.append(embeddings) word_clusters.append(words)</code> </pre> <br><img src="https://habrastorage.org/webt/uc/1k/o6/uc1ko6efgx_d-wnbwolgdabifl8.gif"><br>  <i>Figura 1. Grupos de palavras semelhantes do Google Not√≠cias com diferentes valores de preplexidade.</i> <br><br>  Em seguida, passamos ao fragmento mais not√°vel do artigo, a configura√ß√£o t-SNE.  Aqui, antes de tudo, voc√™ deve prestar aten√ß√£o aos seguintes hiperpar√¢metros: <br><br><ul><li>  <i>n_components</i> - o n√∫mero de componentes, isto √©, a dimens√£o do espa√ßo de valor; </li><li>  <i>perplexidade</i> - perplexidade, cujo valor em t-SNE pode ser igualado ao n√∫mero efetivo de vizinhos.  Est√° relacionado ao n√∫mero de vizinhos mais pr√≥ximos, que √© usado em outros modelos de aprendizado com base em variedades (veja a figura acima).  Seu valor √© recomendado [1] para ser definido na faixa de 5-50; </li><li>  <i>init</i> - tipo de inicializa√ß√£o inicial de vetores. </li></ul><br><pre> <code class="python hljs">tsne_model_en_2d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">15</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embedding_clusters = np.array(embedding_clusters) n, m, k = embedding_clusters.shape embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  Abaixo est√° um script para criar um gr√°fico bidimensional usando o Matplotlib, uma das bibliotecas mais populares para visualiza√ß√£o de dados no Python. <br><br><img src="https://habrastorage.org/webt/34/9y/7h/349y7hxuanvvttqfxkpb48j4n-q.png"><br>  <i>Figura 2. Grupos de palavras semelhantes do Google Not√≠cias (preplexity = 15).</i> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.manifold <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TSNE <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.cm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np % matplotlib inline <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_similar_words</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(labels, embedding_clusters, word_clusters, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.7</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, len(labels))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label, embeddings, words, color <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(labels, embedding_clusters, word_clusters, colors): x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=color, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"f/.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_similar_words(keys, embeddings_en_2d, word_clusters)</code> </pre> <br>  √Äs vezes √© necess√°rio criar n√£o grupos separados de palavras, mas todo o dicion√°rio.  Para esse fim, vamos analisar Anna Karenina, a grande hist√≥ria de paix√£o, trai√ß√£o, trag√©dia e expia√ß√£o. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/Anna Karenina by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_ak = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_anna_karenina_ru.txt'</span></span>) words = [] embeddings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_ak.wv.vocab): embeddings.append(model_ak.wv[word]) words.append(word) tsne_ak_2d = TSNE(n_components=<span class="hljs-number"><span class="hljs-number">2</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">32</span></span>) embeddings_ak_2d = tsne_ak_2d.fit_transform(embeddings)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label, embeddings, words=[], a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = embeddings[:,<span class="hljs-number"><span class="hljs-number">0</span></span>] y = embeddings[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] plt.scatter(x, y, c=colors, alpha=a, label=label) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(words): plt.annotate(word, alpha=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, xy=(x[i], y[i]), xytext=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), textcoords=<span class="hljs-string"><span class="hljs-string">'offset points'</span></span>, ha=<span class="hljs-string"><span class="hljs-string">'right'</span></span>, va=<span class="hljs-string"><span class="hljs-string">'bottom'</span></span>, size=<span class="hljs-number"><span class="hljs-number">10</span></span>) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) plt.savefig(<span class="hljs-string"><span class="hljs-string">"hhh.png"</span></span>, format=<span class="hljs-string"><span class="hljs-string">'png'</span></span>, dpi=<span class="hljs-number"><span class="hljs-number">150</span></span>, bbox_inches=<span class="hljs-string"><span class="hljs-string">'tight'</span></span>) plt.show() tsne_plot_2d(<span class="hljs-string"><span class="hljs-string">'Anna Karenina by Leo Tolstoy'</span></span>, embeddings_ak_2d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/j5/hn/82/j5hn8285nih2kwlop_badd2lzlk.png"><br><br><img src="https://habrastorage.org/webt/x6/jc/i7/x6jci7vka7-efczouqxpiqueisq.png"><br>  <i>Figura 3. Visualiza√ß√£o do dicion√°rio do modelo Word2Vec, treinado no romance "Anna Karenina".</i> <br><br>  A imagem pode se tornar ainda mais informativa se usarmos o espa√ßo tridimensional.  D√™ uma olhada em Guerra e Paz, um dos principais romances da literatura mundial. <br><br><pre> <code class="python hljs">prepare_for_w2v(<span class="hljs-string"><span class="hljs-string">'data/War and Peace by Leo Tolstoy (ru).txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>) model_wp = train_word2vec(<span class="hljs-string"><span class="hljs-string">'train_war_and_peace_ru.txt'</span></span>) words_wp = [] embeddings_wp = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> list(model_wp.wv.vocab): embeddings_wp.append(model_wp.wv[word]) words_wp.append(word) tsne_wp_3d = TSNE(perplexity=<span class="hljs-number"><span class="hljs-number">30</span></span>, n_components=<span class="hljs-number"><span class="hljs-number">3</span></span>, init=<span class="hljs-string"><span class="hljs-string">'pca'</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">3500</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">12</span></span>) embeddings_wp_3d = tsne_wp_3d.fit_transform(embeddings_wp)</code> </pre><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tsne_plot_3d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(title, label, embeddings, a=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure() ax = Axes3D(fig) colors = cm.rainbow(np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) plt.scatter(embeddings[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], embeddings[:, <span class="hljs-number"><span class="hljs-number">2</span></span>], c=colors, alpha=a, label=label) plt.legend(loc=<span class="hljs-number"><span class="hljs-number">4</span></span>) plt.title(title) plt.show() tsne_plot_3d(<span class="hljs-string"><span class="hljs-string">'Visualizing Embeddings using t-SNE'</span></span>, <span class="hljs-string"><span class="hljs-string">'War and Peace'</span></span>, embeddings_wp_3d, a=<span class="hljs-number"><span class="hljs-number">0.1</span></span>)</code> </pre> <br><img src="https://habrastorage.org/webt/ch/jm/os/chjmos082qn6ktw9afdeavbz6_a.png"><br>  <i>Figura 4. Visualiza√ß√£o do dicion√°rio do modelo Word2Vec, treinado no romance "Guerra e Paz".</i> <br><br><h1>  C√≥digo fonte </h1><br>  O c√≥digo est√° dispon√≠vel no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GitHub</a> .  L√° voc√™ pode encontrar o c√≥digo para renderizar anima√ß√µes. <br><br><h1>  Fontes </h1><br><ol><li>  Maaten L., Hinton G. Visualizando dados usando t-SNE // Jornal de pesquisa de aprendizado de m√°quina.  - 2008. - T. 9. - S. 2579-2605. </li><li>  Representa√ß√µes distribu√≠das de palavras e frases e sua composicionalidade // <i>Avan√ßos nos sistemas de processamento de informa√ß√µes neurais</i> .  - 2013 .-- S. 3111-3119. </li><li>  Rehurek R., Sojka P. Framework de software para modelagem de t√≥picos com grandes corpora // Em Anais do LREC 2010 Workshop on New Challenges for NLP Frameworks.  - 2010. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt426113/">https://habr.com/ru/post/pt426113/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt426099/index.html">Not√≠cias do Typegram</a></li>
<li><a href="../pt426101/index.html">Integridade de dados na arquitetura de microsservi√ßos - como garantir isso sem transa√ß√µes distribu√≠das e conectividade r√≠gida</a></li>
<li><a href="../pt426103/index.html">Semin√°rio on-line ‚ÄúVoc√™ precisa do Kubernetes‚Äù 15 de outubro √†s 19:00</a></li>
<li><a href="../pt426105/index.html">Minha captura em uma semana</a></li>
<li><a href="../pt426111/index.html">Google ainda vai lan√ßar um servi√ßo de busca censurado na China</a></li>
<li><a href="../pt426115/index.html">A pr√°tica de usar o modelo de ator na plataforma backend da Quake Champions</a></li>
<li><a href="../pt426117/index.html">O ombudsman prop√µe bloquear an√∫ncios de produtos para beb√™s potencialmente perigosos</a></li>
<li><a href="../pt426119/index.html">Antiguidades: Ferro Cryptonomicon</a></li>
<li><a href="../pt426121/index.html">MC.exe (compilador de mensagens), rc.exe, link.exe para gerar .dll para EventMessageFile</a></li>
<li><a href="../pt426123/index.html">Aprenda o OpenGL. Li√ß√£o 6.1. PBR ou renderiza√ß√£o fisicamente correta. Teoria</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>