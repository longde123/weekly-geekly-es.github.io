<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§üüèæ ü§µüèæ ü§° Speicher f√ºr HPC-Infrastruktur oder wie wir 65 PB-Speicher im RIKEN Japan Research Center gesammelt haben üöê ‚õëÔ∏è üë®üèæ‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 Im vergangenen Jahr wurde die derzeit gr√∂√üte RAIDIX-basierte Speicherinstallation implementiert. Am RIKEN Institute of Comp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Speicher f√ºr HPC-Infrastruktur oder wie wir 65 PB-Speicher im RIKEN Japan Research Center gesammelt haben</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/431230/"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  Im vergangenen Jahr wurde die derzeit gr√∂√üte RAIDIX-basierte Speicherinstallation implementiert.  Am RIKEN Institute of Computing Sciences (Japan) wurde ein System von 11 Failover-Clustern eingesetzt.  Der Hauptzweck des Systems ist die Speicherung der HPC-Infrastruktur (HPCI), die im Rahmen des gro√ü angelegten akademischen Austauschs akademischer Informationen in der Academic Cloud (basierend auf dem SINET-Netzwerk) implementiert wird. <br><br>  Ein wesentliches Merkmal dieses Projekts ist sein Gesamtvolumen von 65 PB, von denen das nutzbare Volumen des Systems 51,4 PB betr√§gt.  Um diesen Wert besser zu verstehen, f√ºgen wir hinzu, dass es sich um 6512 Festplatten mit jeweils 10 TB handelt (die modernsten zum Zeitpunkt der Installation).  Das ist viel. <br><a name="habracut"></a><br>  Die Arbeiten an dem Projekt wurden das ganze Jahr √ºber fortgesetzt. Danach wurde die √úberwachung der Stabilit√§t des Systems etwa ein Jahr lang fortgesetzt.  Die erhaltenen Indikatoren erf√ºllten die angegebenen Anforderungen, und jetzt k√∂nnen wir √ºber den Erfolg dieser Aufzeichnung und des f√ºr uns bedeutenden Projekts sprechen. <br><br><h2>  Supercomputer im Rechenzentrum des RIKEN-Instituts </h2><br>  F√ºr die IKT-Branche ist das RIKEN-Institut vor allem f√ºr seinen legend√§ren ‚ÄûK-Computer‚Äú (vom japanischen ‚ÄûKei‚Äú, was 10 Billiarden bedeutet) bekannt, der zum Zeitpunkt des Starts (Juni 2011) als der leistungsst√§rkste Supercomputer der Welt galt. <br><br><div class="spoiler">  <b class="spoiler_title">Lesen Sie mehr √ºber den K-Computer</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  Der Supercomputer unterst√ºtzt das Zentrum f√ºr Computerwissenschaften bei der Durchf√ºhrung komplexer gro√ü angelegter Studien: Er erm√∂glicht die Modellierung von Klima, Wetterbedingungen und molekularem Verhalten, die Berechnung und Analyse von Reaktionen in der Kernphysik, die Vorhersage von Erdbeben und vieles mehr.  Supercomputerkapazit√§ten werden auch f√ºr mehr ‚Äûallt√§gliche‚Äú und angewandte Forschung verwendet - um nach √ñlfeldern zu suchen und Trends an den Aktienm√§rkten vorherzusagen. <br><br>  Solche Berechnungen und Experimente erzeugen eine gro√üe Datenmenge, deren Wert und Bedeutung nicht √ºbersch√§tzt werden kann.  Um das Beste daraus zu machen, entwickelten japanische Wissenschaftler das Konzept eines einzigen Informationsraums, in dem HPC-Experten aus verschiedenen Forschungszentren Zugriff auf die erhaltenen HPC-Ressourcen haben. <br><br><h2>  Hochleistungsrechnerinfrastruktur (HPCI) </h2><br>  HPCI arbeitet auf der Basis von SINET (The Science Information Network), einem Backbone-Netzwerk f√ºr den Austausch wissenschaftlicher Daten zwischen japanischen Universit√§ten und Forschungszentren.  Derzeit bringt SINET rund 850 Institute und Universit√§ten zusammen und bietet enorme M√∂glichkeiten f√ºr den Informationsaustausch in der Forschung, die sich auf Kernphysik, Astronomie, Geod√§sie, Seismologie und Informatik auswirken. <br><br>  HPCI ist ein einzigartiges Infrastrukturprojekt, das ein einheitliches Informationsaustauschsystem im Bereich Hochleistungsrechnen zwischen Universit√§ten und Forschungszentren in Japan bildet. <br><br>  Durch die Kombination der Funktionen des Supercomputers ‚ÄûK‚Äú und anderer Forschungszentren in zug√§nglicher Form erh√§lt die wissenschaftliche Gemeinschaft offensichtliche Vorteile f√ºr die Arbeit mit wertvollen Daten, die durch Supercomputer-Computing erstellt wurden. <br><br>  Um einen effektiven gemeinsamen Benutzerzugriff auf die HPCI-Umgebung zu erm√∂glichen, wurden hohe Anforderungen an den Speicher hinsichtlich der Zugriffsgeschwindigkeit gestellt.  Und dank der "Hyperproduktivit√§t" des K-Computers konnte der Speichercluster im Zentrum f√ºr Computerwissenschaften des RIKEN-Instituts mit einem Arbeitsvolumen von mindestens 50 PB erstellt werden. <br><br>  Die HPCI-Projektinfrastruktur wurde auf dem Gfarm-Dateisystem aufgebaut, das ein hohes Leistungsniveau erm√∂glichte und unterschiedliche Speichercluster in einem einzigen gemeinsam genutzten Bereich kombinierte. <br><br><h2>  Gfarm-Dateisystem </h2><br>  Gfarm ist ein verteiltes Open Source-Dateisystem, das von japanischen Ingenieuren entwickelt wurde.  Gfarm ist das Ergebnis der Entwicklung des Instituts f√ºr fortgeschrittene industrielle Wissenschaft und Technologie (AIST), und der Name des Systems bezieht sich auf die von Grid Data Farm verwendete Architektur. <br><br>  Dieses Dateisystem kombiniert eine Reihe von scheinbar inkompatiblen Eigenschaften: <br><br><ul><li>  Hohe Skalierbarkeit in Volumen und Leistung </li><li>  Fernnetzverteilung mit Unterst√ºtzung eines einzelnen Namespace f√ºr mehrere verschiedene Forschungszentren </li><li>  POSIX API-Unterst√ºtzung </li><li>  Hohe Leistung f√ºr paralleles Rechnen erforderlich </li><li>  Datenspeichersicherheit </li></ul><br>  Gfarm erstellt ein virtuelles Dateisystem unter Verwendung von Speicherressourcen von mehreren Servern.  Daten werden vom Metadatenserver verteilt, und das Verteilungsschema selbst ist f√ºr Benutzer nicht sichtbar.  Ich muss sagen, dass Gfarm nicht nur aus einem Speichercluster besteht, sondern auch aus einem Rechenraster, das die Ressourcen derselben Server verwendet.  Das Funktionsprinzip des Systems √§hnelt Hadoop: Die eingereichte Arbeit wird auf den Knoten ‚Äûabgesenkt‚Äú, auf dem sich die Daten befinden. <br><br>  Die Dateisystemarchitektur ist asymmetrisch.  Die Rollen sind klar zugeordnet: Speicherserver, Metadatenserver, Client.  Gleichzeitig k√∂nnen jedoch alle drei Rollen von derselben Maschine ausgef√ºhrt werden.  Speicherserver speichern viele Kopien von Dateien, und Metadatenserver arbeiten im Master-Slave-Modus. <br><br><h2>  Projektarbeit </h2><br>  Core Micro Systems, ein strategischer Partner und exklusiver Lieferant von RAIDIX in Japan, implementierte die Implementierung im RIKEN Institute of Computing Sciences Center.  Die Umsetzung des Projekts dauerte etwa 12 Monate, an denen nicht nur die Mitarbeiter von Core Micro Systems, sondern auch die technischen Spezialisten des Reydix-Teams aktiv teilnahmen. <br><br>  Gleichzeitig schien der √úbergang zu einem anderen Speichersystem unwahrscheinlich: Das bestehende System hatte viele technische Bindungen, die den √úbergang zu einer neuen Marke erschwerten. <br><br>  Bei langwierigen Tests, √úberpr√ºfungen und Verbesserungen hat RAIDIX bei der Arbeit mit einer derart beeindruckenden Datenmenge eine konstant hohe Leistung und Effizienz gezeigt. <br><br>  √úber die Verbesserungen lohnt es sich, etwas mehr zu erz√§hlen.  Es war nicht nur notwendig, die Integration von Speichersystemen in das Gfarm-Dateisystem zu erstellen, sondern auch einige funktionale Merkmale der Software zu erweitern.  Um beispielsweise die festgelegten Anforderungen der technischen Spezifikationen zu erf√ºllen, musste die Technologie des automatischen Durchschreibens so schnell wie m√∂glich entwickelt und implementiert werden. <br><br>  Die Bereitstellung des Systems selbst erfolgte systematisch.  Ingenieure von Core Micro Systems f√ºhrten jede Testphase sorgf√§ltig und genau durch und vergr√∂√üerten schrittweise den Ma√üstab des Systems. <br><br>  Im August 2017 wurde die erste Bereitstellungsphase abgeschlossen, als das Systemvolumen 18 PB erreichte.  Im Oktober desselben Jahres wurde die zweite Phase durchgef√ºhrt, in der das Volumen auf einen Rekordwert von 51 PB stieg. <br><br><h2>  L√∂sungsarchitektur </h2><br>  Die L√∂sung wurde durch die Integration von RAIDIX-Speichersystemen und dem verteilten Gfarm-Dateisystem erstellt.  In Verbindung mit Gfarm die M√∂glichkeit, skalierbaren Speicher mit 11 RAIDIX-Systemen mit zwei Controllern zu erstellen. <br><br>  Die Verbindung zu Gfarm-Servern erfolgt √ºber 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">1. Image eines Clusters mit einem separaten Datenserver f√ºr jeden Knoten</font></i> <br><br>  (1) 48 Gbit / s √ó 8 SAN-Netzverbindungen;  Bandbreite: 384 Gbit / s <br>  (2) 48 Gbit / s √ó 40 Mesh FABRIC-Verbindungen;  Bandbreite: 1920 Gbit / s <br><br><h3>  Konfiguration der Dual-Controller-Plattform </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4 St√ºck </td></tr><tr><td>  Hauptplatine </td><td>  Kompatibel mit dem Prozessormodell, das PCI Express 3.0 x8 / x16 unterst√ºtzt </td></tr><tr><td>  Interner Cache </td><td>  256 GB f√ºr jeden Knoten </td></tr><tr><td>  Chassis </td><td>  2U </td></tr><tr><td>  SAS-Controller zum Verbinden von Festplattenregalen, Servern und zur Synchronisierung des Schreibcaches </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  Festplatte </td><td>  HGST Helium 10 TB SAS-Festplatte </td></tr><tr><td>  Herzschlag synchronisieren </td><td>  Ethernet 1 GbE </td></tr><tr><td>  CacheSync Sync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Beide Knoten des Failoverclusters sind √ºber 20 SAS 12G-Ports f√ºr jeden Knoten mit 10 JBODs (60 Festplatten mit jeweils 10 TB) verbunden.  Auf diesen Festplattenregalen wurden 58 10-TB-RAID6-Arrays erstellt (8 Datenfestplatten (D) + 2 Parit√§tsfestplatten (P)) und 12 Festplatten f√ºr den ‚ÄûHot-Swap‚Äú zugewiesen. <br><br>  10 JBOD =&gt; 58 √ó RAID6 (8 Datenfestplatten (D) + 2 Parit√§tsfestplatten (P)), LUN von 580 HDD + 12 HDD f√ºr ‚ÄûHot Swap‚Äú (2,06% des Gesamtvolumens) <br><br>  592 Festplatte (10 TB SAS / 7,2 KB Festplatte) pro Cluster * Festplatte: HGST (MTBF: 2 500 000 Stunden) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">2. Failover-Cluster mit 10 JBOD-Verbindungsdiagramm</font></i> <br><br><h3>  Allgemeines System und Anschlussplan </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Abb.</font></i>  <i><font color="#99999">3. Image eines einzelnen Clusters innerhalb des HPCI-Systems</font></i> <br><br><h2>  Wichtige Projektindikatoren </h2><br><blockquote>  Nutzbare Kapazit√§t pro Cluster: <b>4,64 PB</b> ((RAID6 / 8D + 2P) LUN √ó 58) <br><br>  Die insgesamt nutzbare Kapazit√§t des gesamten Systems: <b>51,04 PB</b> (4,64 PB √ó 11 Cluster). <br><br>  Gesamtsystemkapazit√§t: <b>65 PB</b> . <br><br>  Die Systemleistung betrug: <b>17 GB / s</b> zum Schreiben, <b>22 GB / s</b> zum Lesen. <br><br>  Die Gesamtleistung des Festplattensubsystems des Clusters auf 11 RAIDIX-Speichersystemen: <b>250 GB / s</b> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de431230/">https://habr.com/ru/post/de431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de431218/index.html">Wie ich ein Lovecraft-Comic gemacht habe</a></li>
<li><a href="../de431220/index.html">Der Blick eines Biologen auf die Wurzeln unseres Alterns</a></li>
<li><a href="../de431222/index.html">Website-Archivierung</a></li>
<li><a href="../de431226/index.html">Das Snake-Spiel f√ºr FPGA Cyclone IV (mit VGA- und SPI-Joystick)</a></li>
<li><a href="../de431228/index.html">Hindernislauf f√ºr Licht: Fl√ºssigkristalle helfen</a></li>
<li><a href="../de431232/index.html">Wir generieren wundersch√∂ne SVG-Platzhalter auf Node.js.</a></li>
<li><a href="../de431234/index.html">11. Dezember, Moskau - Alfa JS MeetUp</a></li>
<li><a href="../de431236/index.html">Wie schreibe ich 2018 auf Objective-C? Teil 1</a></li>
<li><a href="../de431238/index.html">Die Zusammenfassung der Ereignisse f√ºr HR-Experten im Bereich IT f√ºr Dezember 2018</a></li>
<li><a href="../de431242/index.html">TLS- und Webzertifikate</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>