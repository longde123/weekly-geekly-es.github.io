<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üß¶ üç• üå¨Ô∏è Revisi√≥n de Keras para TensorFlow üë©üèº‚Äçüíª üö• üë©üèª‚Äçü§ù‚Äçüë®üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Traducci√≥n de la gu√≠a general de Tensorflow.org. Esta gu√≠a le dar√° los conceptos b√°sicos para comenzar a usar Keras. La lectura lleva 10 minutos. 

 I...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Revisi√≥n de Keras para TensorFlow</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482126/"><img src="https://habrastorage.org/webt/wz/o5/gm/wzo5gmjyybfdr1ea_lwtrhsjzvw.jpeg"><br><br>  Traducci√≥n de la gu√≠a general de Tensorflow.org.  Esta gu√≠a le dar√° los conceptos b√°sicos para comenzar a usar Keras.  La lectura lleva 10 minutos. <br><a name="habracut"></a><br><h2>  Importar tf.keras </h2><br>  <code>tf.keras</code> es una implementaci√≥n de la especificaci√≥n API TensorFlow Keras.  Esta es una API de alto nivel para construir y entrenar modelos que incluye soporte de primera clase para funcionalidades espec√≠ficas de TensorFlow como <i>ejecuci√≥n ansiosa</i> , tuber√≠as de datos <code>tf.data</code> y <i>estimadores</i> .  <code>tf.keras</code> facilita el uso de TensorFlow sin sacrificar la flexibilidad y el rendimiento. <br><br>  Para comenzar, importe <code>tf.keras</code> como parte de su configuraci√≥n de TensorFlow: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras</code> </pre> <br>  <code>tf.keras</code> puede ejecutar cualquier c√≥digo compatible con Keras, pero tenga en cuenta: <br><br><ul><li>  La versi√≥n de <code>tf.keras</code> en la √∫ltima versi√≥n de TensorFlow puede ser diferente de la √∫ltima versi√≥n de <code>keras</code> en PyPI.  Echa un vistazo a <code>tf.keras.__version__</code> . </li><li>  Cuando guarda los pesos del modelo, <code>tf.keras</code> hace de manera predeterminada en formato de punto de control.  Pase el <code>save_format='h5'</code> para usar HDF5 (o agregue la extensi√≥n <code>.h5</code> al <code>.h5</code> del archivo). </li></ul><br><h2>  Construye un modelo simple </h2><br><h3>  Modelo secuencial </h3><br>  En Keras, recolecta <i>capas</i> para construir <i>modelos</i> .  Un modelo es un gr√°fico de capa (generalmente).  El tipo de modelo m√°s com√∫n es la pila de capas: <code>tf.keras.Sequential</code> . Modelo <code>tf.keras.Sequential</code> . <br><br>  Construimos una red simple totalmente conectada (es decir, un perceptr√≥n multicapa): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers model = tf.keras.Sequential() <span class="hljs-comment"><span class="hljs-comment">#       64 : model.add(layers.Dense(64, activation='relu')) #   : model.add(layers.Dense(64, activation='relu')) #   softmax  10 : model.add(layers.Dense(10, activation='softmax'))</span></span></code> </pre> <br><h3>  Personaliza capas </h3><br>  Muchas variedades de capas <code>tf.keras.layers</code> est√°n <code>tf.keras.layers</code> .  La mayor√≠a de ellos usa un constructor de argumento com√∫n: <br><br><ul><li>  <code>activation</code> : establece la funci√≥n de activaci√≥n para la capa.  Este par√°metro especifica el nombre de la funci√≥n incorporada o el objeto llamado.  El par√°metro no tiene valor predeterminado. </li><li>  <code>kernel_initializer</code> y <code>bias_initializer</code> : esquemas de inicializaci√≥n que crean pesos de capa (n√∫cleo y desplazamiento).  Este par√°metro puede ser el nombre o el objeto llamado.  El inicializador predeterminado es <code>"Glorot uniform"</code> . </li><li>  <code>kernel_regularizer</code> y <code>bias_regularizer</code> : esquemas de regularizaci√≥n agregados a los pesos de capa (n√∫cleo y desplazamiento), como la regularizaci√≥n L1 o L2.  Por defecto, la regularizaci√≥n no est√° establecida. </li></ul><br>  Los siguientes ejemplos de instancias de las capas `tf.keras.layers.Dense` usan argumentos de constructor: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : layers.Dense(64, activation='sigmoid') # : layers.Dense(64, activation=tf.keras.activations.sigmoid) #     L1   0.01    : layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01)) #     L2   0.01    : layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01)) #        : layers.Dense(64, kernel_initializer='orthogonal') #        2.0: layers.Dense(64, bias_initializer=tf.keras.initializers.Constant(2.0))</span></span></code> </pre> <br><h2>  Entrenamiento y evaluaci√≥n </h2><br><h3>  Configuraci√≥n de entrenamiento </h3><br>  Despu√©s de construir el modelo, configure el proceso de aprendizaje llamando al m√©todo de <code>compile</code> : <br><br><pre> <code class="python hljs">model = tf.keras.Sequential([ <span class="hljs-comment"><span class="hljs-comment">#     64   : layers.Dense(64, activation='relu', input_shape=(32,)), #  : layers.Dense(64, activation='relu'), #   softmax  10 : layers.Dense(10, activation='softmax')]) model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='categorical_crossentropy', metrics=['accuracy'])</span></span></code> </pre> <br>  <code>tf.keras.Model.compile</code> acepta tres argumentos importantes: <br><br><ul><li>  <code>optimizer</code> : Este objeto define el procedimiento de entrenamiento.  Pase instancias de optimizador desde el m√≥dulo <code>tf.keras.optimizers</code> , como <code>tf.keras.optimizers.Adam</code> o <code>tf.keras.optimizers.SGD</code> .  Si solo desea utilizar las opciones predeterminadas, tambi√©n puede especificar optimizadores con palabras clave como <code>'adam'</code> o <code>'sgd'</code> . </li><li>  <code>loss</code> : esta es una funci√≥n que se minimiza en el proceso de aprendizaje.  Entre las variaciones comunes est√°n el error est√°ndar ( <code>mse</code> ), <code>categorical_crossentropy</code> , <code>binary_crossentropy</code> .  Las funciones de p√©rdida se especifican por nombre o pasando el objeto llamado desde el m√≥dulo <code>tf.keras.losses</code> . </li><li>  <code>metrics</code> : se utilizan para monitorear la capacitaci√≥n.  Estos son nombres de cadenas u objetos llamados del m√≥dulo <code>tf.keras.metrics</code> . </li><li>  Adem√°s, para asegurarse de que el modelo est√© entrenado y evaluado con entusiasmo, verifique que pase el par√°metro <code>run_eagerly=True</code> al compilador </li></ul><br>  A continuaci√≥n, veremos algunos ejemplos de configuraci√≥n de modelos para capacitaci√≥n: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#       . model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='mse', # mean squared error metrics=['mae']) # mean absolute error #     . model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy()])</span></span></code> </pre> <br><h3>  Aprendiendo de los datos de NumPy </h3><br>  Para conjuntos de datos m√°s peque√±os, use las matrices de memoria de NumPy para capacitaci√≥n y evaluaci√≥n de modelos.  El modelo est√° "entrenado" en datos de entrenamiento utilizando el m√©todo `fit`: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np data = np.random.random((<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)) labels = np.random.random((<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) model.fit(data, labels, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>)</code> </pre> <br>  <code>tf.keras.Model.fit</code> toma tres argumentos importantes: <br><br><ul><li>  <code>epochs</code> : el aprendizaje se divide en * era *.  La era es una iteraci√≥n sobre todos los datos de entrada (esto se hace en peque√±os lotes). </li><li>  <code>batch_size</code> : al transmitir datos NumPy, el modelo divide los datos en bloques m√°s peque√±os (lotes) e itera sobre estos bloques durante el entrenamiento.  Este n√∫mero indica el tama√±o de cada bloque de datos.  Recuerde que el √∫ltimo bloque puede ser m√°s peque√±o si el n√∫mero total de registros no se divide por el tama√±o del lote. </li><li>  <code>validation_data</code> : al crear prototipos de un modelo, desea realizar un seguimiento f√°cil de su rendimiento en los datos de validaci√≥n.  Al pasar una tupla de datos de entrada y etiquetas con este argumento, el modelo puede mostrar los valores de la funci√≥n de p√©rdida y las m√©tricas en modo de salida para los datos que se transmiten al final de cada era. </li></ul><br>  Aqu√≠ hay un ejemplo usando <code>validation_data</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np data = np.random.random((<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)) labels = np.random.random((<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) val_data = np.random.random((<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>)) val_labels = np.random.random((<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) model.fit(data, labels, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">32</span></span>, validation_data=(val_data, val_labels))</code> </pre> <br><h3>  Entrenamiento usando conjuntos de datos tf.data </h3><br>  Use la API de conjuntos de datos para escalar grandes bases de datos o capacitaci√≥n en m√∫ltiples dispositivos.  Pase la instancia de `tf.data.Dataset` al m√©todo de <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32) model.fit(dataset, epochs=10)</span></span></code> </pre> <br>  Dado que <code>Dataset</code> proporciona datos en lotes, este c√≥digo no requiere el argumento <code>batch_size</code> . <br><br>  Los conjuntos de datos tambi√©n se pueden usar para validar: <br><br><pre> <code class="python hljs">dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(<span class="hljs-number"><span class="hljs-number">32</span></span>) val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels)) val_dataset = val_dataset.batch(<span class="hljs-number"><span class="hljs-number">32</span></span>) model.fit(dataset, epochs=<span class="hljs-number"><span class="hljs-number">10</span></span>, validation_data=val_dataset)</code> </pre> <br><h3>  Valoraci√≥n y predicci√≥n </h3><br>  Los <code>tf.keras.Model.predict</code> <code>tf.keras.Model.evaluate</code> y <code>tf.keras.Model.predict</code> pueden usar los <code>tf.data.Dataset</code> NumPy y <code>tf.data.Dataset</code> . <br><br>  As√≠ es como puede <i>estimar las</i> p√©rdidas en el modo de salida y las m√©tricas para los datos proporcionados: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   Numpy data = np.random.random((1000, 32)) labels = np.random.random((1000, 10)) model.evaluate(data, labels, batch_size=32) #   dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32) model.evaluate(dataset)</span></span></code> </pre> <br>  Y aqu√≠ est√° c√≥mo <i>predecir la</i> salida del √∫ltimo nivel en modo de salida para los datos proporcionados en forma de una matriz NumPy: <br><br><h2>  Construyendo modelos complejos </h2><br><h3>  La API funcional </h3><br>  El modelo <code>tf.keras.Sequential</code> es una pila de capas simple con la que no puede imaginar un modelo arbitrario.  Utilice la API funcional de Keras para crear topolog√≠as de modelos complejos, como: <br><br><ul><li>  M√∫ltiples modelos de entrada </li><li>  Modelos con m√∫ltiples salidas, </li><li>  Modelos con capas comunes (la misma capa se llama varias veces), </li><li>  Modelos con flujos de datos inconsistentes (por ejemplo, relaciones residuales). </li></ul><br>  La construcci√≥n de un modelo con una API funcional funciona de la siguiente manera: <br><br><ol><li>  La instancia de capa es invocable y devuelve un tensor. </li><li>  Los tensores de entrada y salida se utilizan para determinar la instancia de <code>tf.keras.Model</code> </li><li>  Este modelo est√° entrenado como el modelo 'secuencial'. </li></ol><br>  El siguiente ejemplo usa la API funcional para construir una red simple y completamente conectada: <br><br><pre> <code class="python hljs">inputs = tf.keras.Input(shape=(<span class="hljs-number"><span class="hljs-number">32</span></span>,)) <span class="hljs-comment"><span class="hljs-comment">#    #        . x = layers.Dense(64, activation='relu')(inputs) x = layers.Dense(64, activation='relu')(x) predictions = layers.Dense(10, activation='softmax')(x)</span></span></code> </pre> <br>  Cree una instancia del modelo con estas entradas y salidas. <br><br><pre> <code class="python hljs">model = tf.keras.Model(inputs=inputs, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#     . model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss='categorical_crossentropy', metrics=['accuracy']) #   5  model.fit(data, labels, batch_size=32, epochs=5)</span></span></code> </pre> <br><h3>  Modelos de subclases </h3><br>  Cree un modelo totalmente personalizable utilizando la subclase <code>tf.keras.Model</code> y definiendo su propia distribuci√≥n directa.  Cree capas en el m√©todo <code>__init__</code> y <code>__init__</code> como atributos de la instancia de clase.  Definir propagaci√≥n directa en el m√©todo de <code>call</code> . <br><br>  Subclasificar un modelo es especialmente √∫til cuando se habilita la ejecuci√≥n ansiosa, ya que le permite escribir la distribuci√≥n directa de manera imperativa. <br><br>  Nota: si desea que su modelo <i>se</i> ejecute <i>siempre</i> imperativamente, puede establecer <code>dynamic=True</code> cuando llame al <code>super</code> . <br><blockquote>  Punto clave: utilice la API correcta para trabajar.  Si bien la subclasificaci√≥n de un modelo proporciona flexibilidad, debe pagarlo con mayor complejidad y mayor potencial de errores personalizados.  Si es posible, elija la API funcional. </blockquote>  El siguiente ejemplo muestra un modelo subclases tf.keras.Model que usa distribuci√≥n directa personalizada, que no es obligatorio que sea obligatorio: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MyModel</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(tf.keras.Model)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> super(MyModel, self).__init__(name=<span class="hljs-string"><span class="hljs-string">'my_model'</span></span>) self.num_classes = num_classes <span class="hljs-comment"><span class="hljs-comment">#    . self.dense_1 = layers.Dense(32, activation='relu') self.dense_2 = layers.Dense(num_classes, activation='sigmoid') def call(self, inputs): #     , #      ( `__init__`). x = self.dense_1(inputs) return self.dense_2(x)</span></span></code> </pre> <br>  Cree una instancia de la nueva clase de modelo: <br><br><pre> <code class="python hljs">model = MyModel(num_classes=<span class="hljs-number"><span class="hljs-number">10</span></span>) <span class="hljs-comment"><span class="hljs-comment">#     . model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss='categorical_crossentropy', metrics=['accuracy']) #   5 . model.fit(data, labels, batch_size=32, epochs=5)</span></span></code> </pre> <br><h3>  Capas personalizadas </h3><br>  Cree una capa personalizada subclasificando <code>tf.keras.layers.Layer</code> e implementando los siguientes m√©todos: <br><br><ul><li>  <code>__init__</code> : opcionalmente, especifique las subcapas que se utilizar√°n en esta capa. </li><li>  * <code>build</code> : crear pesos de capa.  Agregue pesos utilizando el m√©todo <code>add_weight</code> </li><li>  <code>call</code> : Definir distribuci√≥n directa. </li><li>  Opcionalmente, la capa se puede serializar implementando el m√©todo <code>get_config</code> y el <code>from_config</code> clase <code>from_config</code> . </li></ul><br>  A continuaci√≥n se muestra un ejemplo de una capa de usuario que multiplica la matriz ( <code>matmul</code> ) presentada con la entrada de matriz del n√∫cleo: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MyLayer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(layers.Layer)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, output_dim, **kwargs)</span></span></span><span class="hljs-function">:</span></span> self.output_dim = output_dim super(MyLayer, self).__init__(**kwargs) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input_shape)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#       . self.kernel = self.add_weight(name='kernel', shape=(input_shape[1], self.output_dim), initializer='uniform', trainable=True) def call(self, inputs): return tf.matmul(inputs, self.kernel) def get_config(self): base_config = super(MyLayer, self).get_config() base_config['output_dim'] = self.output_dim return base_config @classmethod def from_config(cls, config): return cls(**config)</span></span></code> </pre><br>  Crea un modelo usando tu capa personalizada: <br><br><pre> <code class="python hljs">model = tf.keras.Sequential([ MyLayer(<span class="hljs-number"><span class="hljs-number">10</span></span>), layers.Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)]) <span class="hljs-comment"><span class="hljs-comment">#      model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss='categorical_crossentropy', metrics=['accuracy']) #   5 . model.fit(data, labels, batch_size=32, epochs=5)</span></span></code> </pre> <br><h2>  Kolbeki </h2><br>  Kolbek es un objeto transferido al modelo para personalizar y expandir su comportamiento durante el entrenamiento.  Puede escribir su propia devoluci√≥n de llamada personalizada o usar el <code>tf.keras.callbacks</code> que incluye: <br><br>  <code>tf.keras.callbacks.ModelCheckpoint</code> : guardar puntos de interrupci√≥n del modelo a intervalos regulares. <br>  <code>tf.keras.callbacks.LearningRateScheduler</code> : cambia din√°micamente el paso de aprendizaje. <br>  <code>tf.keras.callbacks.EarlyStopping</code> : detiene el entrenamiento cuando el resultado de la validaci√≥n deja de mejorar. <br>  <code>tf.keras.callbacks.TensorBoard:</code> monitorizar el comportamiento del modelo mediante <br>  Tensorboard <br><br>  Para usar <code>tf.keras.callbacks.Callback</code> , <code>tf.keras.callbacks.Callback</code> al m√©todo de <code>fit</code> modelo: <br><br><pre> <code class="python hljs">callbacks = [ <span class="hljs-comment"><span class="hljs-comment">#    `val_loss`     2  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'), #   TensorBoard   `./logs` directory tf.keras.callbacks.TensorBoard(log_dir='./logs') ] model.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks, validation_data=(val_data, val_labels))</span></span></code> </pre> <br><h2>  Guardar y Restaurar </h2><br><h3>  Guardar solo valores de peso </h3><br>  Guarde y cargue pesos de modelos utilizando <code>tf.keras.Model.save_weights</code> : <br><br><pre> <code class="python hljs">model = tf.keras.Sequential([ layers.Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">32</span></span>,)), layers.Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)]) model.compile(optimizer=tf.keras.optimizers.Adam(<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     TensorFlow Checkpoint model.save_weights('./weights/my_model') #    #        . model.load_weights('./weights/my_model')</span></span></code> </pre> <br>  Por defecto, los pesos del modelo se guardan en el formato de punto de control TensorFlow.  Los pesos tambi√©n se pueden guardar en formato Keras HDF5 (valor predeterminado para la implementaci√≥n universal de Keras): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     HDF5 model.save_weights('my_model.h5', save_format='h5') #    model.load_weights('my_model.h5')</span></span></code> </pre> <br><h3>  Guardar solo la configuraci√≥n del modelo </h3><br>  La configuraci√≥n del modelo se puede guardar; esto serializa la arquitectura del modelo sin ning√∫n peso.  Una configuraci√≥n guardada puede restaurar e inicializar el mismo modelo, incluso sin c√≥digo que defina el modelo original.  Keras admite formatos de serializaci√≥n JSON y YAML: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     JSON json_string = model.to_json() json_string</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pprint pprint.pprint(json.loads(json_string))</code> </pre> <br>  Restaurar un modelo (reinicializado) desde JSON: <br><br><pre> <code class="python hljs">fresh_model = tf.keras.models.model_from_json(json_string)</code> </pre><br>  Serializar el modelo a YAML requiere la instalaci√≥n de `pyyaml` <i>antes de importar TensorFlow</i> : <br><br><pre> <code class="python hljs">yaml_string = model.to_yaml() print(yaml_string)</code> </pre> <br>  Restaurando un modelo de YAML: <br><br><pre> <code class="python hljs">fresh_model = tf.keras.models.model_from_yaml(yaml_string)</code> </pre> <br><blockquote>  Nota: los modelos subclasificados no son serializables, porque su arquitectura est√° definida por el c√≥digo Python en el cuerpo del m√©todo `call`. </blockquote><br><h3>  Guardar todo el modelo en un archivo </h3><br>  Todo el modelo se puede guardar en un archivo que contiene los valores de los pesos, la configuraci√≥n del modelo e incluso la configuraci√≥n del optimizador.  Esto le permitir√° establecer un punto de interrupci√≥n del modelo y continuar entrenando m√°s tarde desde exactamente la misma posici√≥n, incluso sin acceso al c√≥digo fuente. <br><br><pre> <code class="plaintext hljs">#    model = tf.keras.Sequential([ layers.Dense(10, activation='softmax', input_shape=(32,)), layers.Dense(10, activation='softmax') ]) model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels, batch_size=32, epochs=5) #      HDF5 model.save('my_model.h5') #         . model = tf.keras.models.load_model('my_model.h5')</code> </pre> <br><h2>  Ansiosa ejecuci√≥n </h2><br>  La ejecuci√≥n ansiosa es un entorno de programaci√≥n imperativo que realiza operaciones de inmediato.  Esto no es necesario para Keras, pero es compatible con <code>tf.keras</code> y es √∫til para verificar su programa y depurar. <br><br>  Todos los modelos de construcci√≥n de la API `tf.keras` son compatibles con la ejecuci√≥n ansiosa.  Aunque se pueden usar las API `secuenciales` y funcionales, la ejecuci√≥n ansiosa es especialmente √∫til cuando se <i>subclasifica un modelo</i> y se crean <i>capas personalizadas</i> ; estas API requieren que escriba una distribuci√≥n directa en forma de c√≥digo (en lugar de las API que crean modelos ensamblando capas existentes). <br><br><h2>  Distribuci√≥n </h2><br><h3>  M√∫ltiples GPU </h3><br>  <code>tf.keras</code> modelos <code>tf.keras</code> se pueden ejecutar en m√∫ltiples GPU usando <code>tf.distribute.Strategy</code> .  Esta API proporciona aprendizaje distribuido en m√∫ltiples GPU con poco o ning√∫n cambio en el c√≥digo existente. <br><br>  Actualmente, <code>tf.distribute.MirroredStrategy</code> √∫nica estrategia de distribuci√≥n compatible.  <code>MirroredStrategy</code> replica gr√°ficos con <br>  Aprendizaje sincr√≥nico con All-Reduce en una m√°quina.  Para usar ' <code>distribute.Strategy</code> . <code>Strategy</code> ', anide la instalaci√≥n del optimizador, el dise√±o y la compilaci√≥n del modelo en ` <code>Strategy</code> . <br><br>  El siguiente ejemplo distribuye <code>tf.keras.Model</code> entre m√∫ltiples GPU en la misma m√°quina. <br><br>  Primero, definimos un modelo dentro del √°rea de una estrategia distribuida: <br><br><pre> <code class="python hljs">strategy = tf.distribute.MirroredStrategy() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> strategy.scope(): model = tf.keras.Sequential() model.add(layers.Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">10</span></span>,))) model.add(layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) optimizer = tf.keras.optimizers.SGD(<span class="hljs-number"><span class="hljs-number">0.2</span></span>) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=optimizer) model.summary()</code> </pre> <br>  Luego entrenamos el modelo en los datos como de costumbre: <br><br><pre> <code class="python hljs">x = np.random.random((<span class="hljs-number"><span class="hljs-number">1024</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) y = np.random.randint(<span class="hljs-number"><span class="hljs-number">2</span></span>, size=(<span class="hljs-number"><span class="hljs-number">1024</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x = tf.cast(x, tf.float32) dataset = tf.data.Dataset.from_tensor_slices((x, y)) dataset = dataset.shuffle(buffer_size=<span class="hljs-number"><span class="hljs-number">1024</span></span>).batch(<span class="hljs-number"><span class="hljs-number">32</span></span>) model.fit(dataset, epochs=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br>  <i>Despu√©s de la verificaci√≥n, la traducci√≥n tambi√©n aparecer√° en Tensorflow.org.</i>  <i>Si desea participar en la traducci√≥n de la documentaci√≥n del sitio web de Tensorflow.org al ruso, comun√≠quese personalmente o env√≠e sus comentarios.</i>  <i>Cualquier correcci√≥n o comentario son apreciados.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/482126/">https://habr.com/ru/post/482126/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../482102/index.html">Preguntas y respuestas de Habr 2019: resultados del a√±o</a></li>
<li><a href="../482104/index.html">C√≥mo lidiar con los h√°bitos de las personas programadas</a></li>
<li><a href="../482106/index.html">Habr Freelance 2019: resultados del a√±o</a></li>
<li><a href="../482110/index.html">Linux se ejecuta en mi tarjeta de visita</a></li>
<li><a href="../482114/index.html">Enviar correos electr√≥nicos utilizando asyncio y aiohttp desde una aplicaci√≥n Django</a></li>
<li><a href="../482128/index.html">gReebok detectado. Dermatovener√≥logo mismo</a></li>
<li><a href="../482130/index.html">Asignaci√≥n de derechos a gran escala a usuarios de dominio de diferentes bosques</a></li>
<li><a href="../482132/index.html">La copia de Tesla Cybertruck fue vista en Mosc√∫. Este es un ... LADA rusa Samara</a></li>
<li><a href="../482134/index.html">Comparaci√≥n de h√≠bridos o lo que se espera de los propietarios de los auriculares rumanos Meze por 84 990 y 239 990 rublos</a></li>
<li><a href="../482140/index.html">Juegos de mesa para j√≥venes programadores de 4 a 10 a√±os. Lo que se puede encontrar en el mercado a finales de 2019</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>