<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äç‚úàÔ∏è üë®‚Äçüåæ üôå Theorie und Praxis der Verwendung von HBase üî∂ üç≥ üçΩÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Guten Tag! Mein Name ist Danil Lipova, unser Team in Sbertech hat begonnen, HBase als Repository f√ºr Betriebsdaten zu verwenden. W√§hrend seines Studiu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Theorie und Praxis der Verwendung von HBase</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/420425/">  Guten Tag!  Mein Name ist Danil Lipova, unser Team in Sbertech hat begonnen, HBase als Repository f√ºr Betriebsdaten zu verwenden.  W√§hrend seines Studiums wurden Erfahrungen gesammelt, die ich systematisieren und beschreiben wollte (wir hoffen, dass dies f√ºr viele n√ºtzlich sein wird).  Alle folgenden Experimente wurden mit Versionen von HBase 1.2.0-cdh5.14.2 und 2.0.0-cdh6.0.0-beta1 durchgef√ºhrt. <br><br><ol><li>  Allgemeine Architektur </li><li>  Daten in HBASE schreiben </li><li>  Daten aus HBASE lesen </li><li>  Daten-Caching </li><li>  Stapelverarbeitung MultiGet / MultiPut </li><li>  Strategie zum Aufteilen von Tabellen in Regionen (Versch√ºtten) </li><li>  Fehlertoleranz, Verdichtung und Datenlokalit√§t </li><li>  Einstellungen und Leistung </li><li>  Lasttest </li><li>  Schlussfolgerungen </li></ol><a name="habracut"></a><br><h2>  1. Allgemeine Architektur </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y9/wa/vl/y9wavltynzhs9r_7v5sn1d8ff68.png"></div><br>  Der Standby-Master h√∂rt den aktiven Herzschlag auf dem ZooKeeper-Knoten und √ºbernimmt im Falle eines Verschwindens die Funktionen des Masters. <br><br><h2>  2. Daten in HBASE schreiben </h2><br>  Betrachten Sie zun√§chst den einfachsten Fall: Schreiben eines Schl√ºsselwertobjekts in eine bestimmte Tabelle mit put (Zeilenschl√ºssel).  Der Client muss zuerst herausfinden, wo sich der Root-Region-Server (RRS) befindet, auf dem die Meta-Tabelle hbase: gespeichert ist.  Er erh√§lt diese Informationen von ZooKeeper.  Dann wendet er sich an RRS und liest die Tabelle hbase: meta, aus der er die Informationen abruft, f√ºr die RegionServer (RS) verantwortlich ist, um Daten f√ºr den angegebenen Zeilenschl√ºssel in der f√ºr ihn interessanten Tabelle zu speichern.  F√ºr die zuk√ºnftige Verwendung wird die Metatabelle vom Client zwischengespeichert, und nachfolgende Aufrufe werden daher schneller direkt an RS weitergeleitet. <br><br>  Nachdem RS die Anforderung erhalten hat, schreibt er sie zun√§chst in WriteAheadLog (WAL), das f√ºr die Wiederherstellung im Falle eines Absturzes erforderlich ist.  Anschlie√üend werden die Daten im MemStore gespeichert.  Dies ist ein Puffer im Speicher, der einen sortierten Satz von Schl√ºsseln f√ºr eine bestimmte Region enth√§lt.  Die Tabelle kann in Regionen (Partitionen) unterteilt werden, von denen jede einen disjunkten Satz von Schl√ºsseln enth√§lt.  Dies erm√∂glicht das Platzieren von Regionen auf verschiedenen Servern, um eine h√∂here Leistung zu erzielen.  Trotz der Offensichtlichkeit dieser Aussage werden wir sp√§ter sehen, dass dies nicht in allen F√§llen funktioniert. <br><br>  Nach dem Platzieren des Datensatzes in MemStore erh√§lt der Client eine Antwort, dass der Datensatz erfolgreich gespeichert wurde.  Gleichzeitig wird es wirklich nur im Puffer gespeichert und gelangt erst nach Ablauf einer bestimmten Zeit oder wenn es mit neuen Daten gef√ºllt ist, auf die Festplatte. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/xi/p7/xtxip7moylyjdgqggsqiu8j_cm4.png"></div><br>  Bei der Ausf√ºhrung des L√∂schvorgangs wird keine physische Datenl√∂schung durchgef√ºhrt.  Sie werden einfach als gel√∂scht markiert, und die Zerst√∂rung selbst tritt auf, wenn die Hauptkompaktfunktion aufgerufen wird, die in Abschnitt 7 ausf√ºhrlicher beschrieben wird. <br><br>  Dateien im HFile-Format werden in HDFS gesammelt, und von Zeit zu Zeit beginnt der kleinere Kompaktprozess, bei dem kleine Dateien einfach in gr√∂√üere Dateien eingeklebt werden, ohne dass etwas gel√∂scht wird.  Im Laufe der Zeit wird dies zu einem Problem, das sich nur beim Lesen von Daten manifestiert (wir werden sp√§ter darauf zur√ºckkommen). <br><br>  Zus√§tzlich zu dem oben beschriebenen Startvorgang gibt es ein viel effizienteres Verfahren, das wahrscheinlich die leistungsst√§rkste Seite dieser Datenbank ist - BulkLoad.  Es besteht darin, dass wir HFiles unabh√§ngig voneinander erstellen und auf die Festplatte legen, wodurch wir perfekt skalieren und sehr anst√§ndige Geschwindigkeiten erreichen k√∂nnen.  In der Tat ist die Einschr√§nkung hier nicht HBase, sondern die M√∂glichkeiten von Eisen.  Nachfolgend sind die Ergebnisse des Ladens in einen Cluster bestehend aus 16 RegionServern und 16 NodeManager YARN (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Threads), Version HBase 1.2.0-cdh5.14.2, aufgef√ºhrt. <br><br><img src="https://habrastorage.org/webt/ro/bu/hf/robuhfegpwqyed6gmwg7he2bmfk.png"><br><br>  Es ist ersichtlich, dass durch Erh√∂hen der Anzahl der Partitionen (Regionen) in der Tabelle sowie der ausf√ºhrbaren Dateien Spark die Download-Geschwindigkeit erh√∂ht wird.  Die Geschwindigkeit h√§ngt auch von der Aufnahmemenge ab.  Gro√üe Bl√∂cke erh√∂hen die Messung von MB / s, kleine Bl√∂cke die Anzahl der eingef√ºgten Datens√§tze pro Zeiteinheit, alle anderen Dinge sind gleich. <br><br>  Sie k√∂nnen auch gleichzeitig in zwei Tabellen laden und die Geschwindigkeit verdoppeln.  Es ist unten zu sehen, dass 10-KB-Bl√∂cke mit einer Geschwindigkeit von jeweils etwa 600 Mbit / s (insgesamt 1275 Mbit / s) gleichzeitig in zwei Tabellen geschrieben werden, was mit der Schreibgeschwindigkeit von 623 MB / s in eine Tabelle √ºbereinstimmt (siehe Nr. 11 oben). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gr/05/vp/gr05vpmhauzclwmn310erbgj22u.png"></div><br>  Der zweite Start mit Aufzeichnungen von 50 KB zeigt jedoch, dass die Download-Geschwindigkeit bereits leicht ansteigt, was auf eine Ann√§herung an die Grenzwerte hinweist.  Es sollte beachtet werden, dass HBASE selbst praktisch nicht belastet wird. Alles, was dazu erforderlich ist, ist, zuerst die Daten von hbase: meta anzugeben und nach dem Auskleiden von HFiles die BlockCache-Daten zu leeren und den MemStore-Puffer auf der Festplatte zu speichern, wenn dies nicht der Fall ist leer. <br><br><h2>  3. Lesen von Daten aus HBASE </h2><br>  Wenn wir davon ausgehen, dass alle Informationen von hbase: meta bereits einen Client haben (siehe Abschnitt 2), geht die Anfrage sofort an die RS, wo der gew√ºnschte Schl√ºssel gespeichert ist.  Zuerst wird die Suche in MemCache durchgef√ºhrt.  Unabh√§ngig davon, ob dort Daten vorhanden sind oder nicht, wird die Suche auch im BlockCache-Puffer und gegebenenfalls in HFiles durchgef√ºhrt.  Wenn die Daten in einer Datei gefunden wurden, werden sie in BlockCache abgelegt und bei der n√§chsten Anforderung schneller zur√ºckgegeben.  HFile-Suchen sind aufgrund der Verwendung des Bloom-Filters relativ schnell, d.h.  Nachdem er eine kleine Datenmenge gelesen hat, stellt er sofort fest, ob diese Datei den gew√ºnschten Schl√ºssel enth√§lt, und f√§hrt dann mit dem n√§chsten fort. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8-/zz/wz/8-zzwzcoed7obxmlgzxl3ihrzzw.png"></div><br>  Nachdem RS Daten aus diesen drei Quellen erhalten hat, bildet es eine Antwort.  Insbesondere k√∂nnen mehrere Versionen des gefundenen Objekts gleichzeitig √ºbertragen werden, wenn der Client die Versionierung angefordert hat. <br><br><h2>  4. Daten-Caching </h2><br>  MemStore- und BlockCache-Puffer belegen bis zu 80% des zugewiesenen RS-Speichers auf dem Heap (der Rest ist f√ºr RS-Serviceaufgaben reserviert).  Wenn der typische Verwendungsmodus so ist, dass die Prozesse dieselben Daten schreiben und sofort lesen, ist es sinnvoll, BlockCache zu reduzieren und MemStore zu erh√∂hen, weil  Wenn das Schreiben von Daten in den Lesecache nicht abf√§llt, wird BlockCache seltener verwendet.  Der BlockCache-Puffer besteht aus zwei Teilen: LruBlockCache (immer auf dem Heap) und BucketCache (normalerweise au√üerhalb des Heaps oder auf der SSD).  BucketCache sollte verwendet werden, wenn viele Leseanforderungen vorliegen und diese nicht in LruBlockCache passen, was zur aktiven Arbeit von Garbage Collector f√ºhrt.  Gleichzeitig sollten Sie mit der Verwendung des Lesecaches keine radikale Leistungssteigerung erwarten, aber wir werden in Abschnitt 8 darauf zur√ºckkommen <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/oe/nw/rqoenwgqtngb-a37gsof7sqbgn0.png"></div><br>  BlockCache ist einer f√ºr den gesamten RS, und MemStore hat einen eigenen f√ºr jede Tabelle (einen f√ºr jede Spaltenfamilie). <br><br>  Wie theoretisch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beschrieben</a> , werden solche Parameter CACHE_DATA_ON_WRITE f√ºr die Tabelle und "Cache DATA on Write" f√ºr RS auf false gesetzt, wenn das Schreiben von Daten nicht in den Cache f√§llt.  Wenn Sie jedoch in der Praxis Daten in MemStore schreiben, diese dann auf die Festplatte leeren (auf diese Weise bereinigen), dann die resultierende Datei l√∂schen und durch Ausf√ºhren einer Abrufanforderung die Daten erfolgreich empfangen.  Und selbst wenn Sie BlockCache vollst√§ndig deaktivieren und die Tabelle mit neuen Daten f√ºllen, den MemStore auf die Festplatte bringen, l√∂schen und von einer anderen Sitzung anfordern, werden sie dennoch von irgendwoher abgerufen.  HBase speichert also nicht nur Daten, sondern auch mysteri√∂se R√§tsel. <br><br><pre><code class="bash hljs">hbase(main):001:0&gt; create <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf'</span></span> Created table ns:magic Took 1.1533 seconds hbase(main):002:0&gt; put <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf:c'</span></span>, <span class="hljs-string"><span class="hljs-string">'try_to_delete_me'</span></span> Took 0.2610 seconds hbase(main):003:0&gt; flush <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span> Took 0.6161 seconds hdfs dfs -mv /data/hbase/data/ns/magic/* /tmp/trash hbase(main):002:0&gt; get <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span> cf:c timestamp=1534440690218, value=try_to_delete_me</code> </pre> <br>  Cache DATA on Read ist auf false gesetzt.  Wenn Sie Ideen haben, k√∂nnen Sie diese gerne in den Kommentaren diskutieren. <br><br><h2>  5. Stapelverarbeitung von MultiGet / MultiPut-Daten </h2><br>  Das Verarbeiten einzelner Anforderungen (Abrufen / Einf√ºgen / L√∂schen) ist ein ziemlich teurer Vorgang. Sie sollten sie daher so weit wie m√∂glich in einer Liste oder Liste kombinieren, um eine erhebliche Leistungssteigerung zu erzielen.  Dies gilt insbesondere f√ºr die Schreiboperation, aber beim Lesen gibt es die folgende Gefahr.  Die folgende Grafik zeigt die Lesezeit von 50.000 Datens√§tzen aus MemStore.  Das Lesen wurde in einem Stream durchgef√ºhrt und die horizontale Achse zeigt die Anzahl der Schl√ºssel in der Anforderung.  Es ist ersichtlich, dass die Ausf√ºhrungszeit sinkt, wenn Sie in einer Anforderung auf tausend Schl√ºssel erh√∂hen, d. H.  Geschwindigkeit steigt.  Wenn der MSLAB-Modus jedoch standardm√§√üig aktiviert ist, beginnt nach diesem Schwellenwert ein dramatischer Leistungsabfall. Je gr√∂√üer die Datenmenge im Datensatz ist, desto l√§nger ist die Zeit. <br><br><img src="https://habrastorage.org/webt/1k/ic/hj/1kichjm1xdpxskbx7avzppmlqty.png"><br><br>  Die Tests wurden auf einer virtuellen Maschine mit 8 Kernen, HBase Version 2.0.0-cdh6.0.0-beta1, durchgef√ºhrt. <br><br>  Der MSLAB-Modus wurde entwickelt, um die Heap-Fragmentierung zu reduzieren, die durch das Mischen von Daten der neuen und alten Generation entsteht.  Als L√∂sung f√ºr das Problem, wenn MSLAB aktiviert ist, werden Daten in relativ kleinen Zellen (Chunk) abgelegt und stapelweise verarbeitet.  Wenn das Volumen im angeforderten Datenpaket die zugewiesene Gr√∂√üe √ºberschreitet, sinkt die Leistung daher stark.  Andererseits ist es auch nicht ratsam, diesen Modus auszuschalten, da dies in Momenten intensiver Datenarbeit zu GC-Stopps f√ºhren kann.  Ein guter Ausweg besteht darin, das Volumen der Zelle zu erh√∂hen, wenn beim aktiven Schreiben √ºber das Put gleichzeitig mit dem Lesen gearbeitet wird.  Es ist zu beachten, dass das Problem nicht auftritt, wenn Sie nach der Aufzeichnung den Befehl flush ausf√ºhren, mit dem MemStore auf die Festplatte geleert wird, oder wenn das Laden mit BulkLoad ausgef√ºhrt wird.  Die folgende Tabelle zeigt, dass Abfragen von MemStore-Daten eines gr√∂√üeren Volumens (und derselben Menge) zu einer Verlangsamung f√ºhren.  Durch Erh√∂hen der Blockgr√∂√üe wird jedoch die normale Verarbeitungszeit wiederhergestellt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zb/jq/s3/zbjqs3tou2ywnnzc16p93fttbva.png"></div><br>  Zus√§tzlich zur Erh√∂hung der Blockgr√∂√üe hilft die Fragmentierung von Daten nach Region, d.h.  Tischaufteilung.  Dies f√ºhrt dazu, dass weniger Anforderungen in jede Region kommen und wenn sie in einer Zelle platziert werden, bleibt die Antwort gut. <br><br><h2>  6. Die Strategie der Aufteilung von Tabellen in Regionen (Schneiden) </h2><br>  Da HBase ein Schl√ºsselwertspeicher ist und die Partitionierung nach Schl√ºsseln erfolgt, ist es √§u√üerst wichtig, Daten gleichm√§√üig √ºber alle Regionen hinweg zu teilen.  Wenn Sie beispielsweise eine solche Tabelle in drei Teile unterteilen, werden die Daten in drei Bereiche unterteilt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/4c/9d/rg4c9dm-gbtodx0dqr3quc9h6we.png"></div><br>  Es kommt vor, dass dies zu einer starken Verlangsamung f√ºhrt, wenn die in Zukunft geladenen Daten beispielsweise wie lange Werte aussehen, von denen die meisten mit derselben Ziffer beginnen, zum Beispiel: <br><br>  1000001 <br>  1000002 <br>  ... <br>  1100003 <br><br>  Da die Schl√ºssel als Array von Bytes gespeichert sind, beginnen alle auf dieselbe Weise und geh√∂ren zu derselben Region Nr. 1, in der dieser Schl√ºsselbereich gespeichert ist.  Es gibt verschiedene Split-Strategien: <br><br>  HexStringSplit - Verwandelt den Schl√ºssel in eine Zeichenfolge mit hexadezimaler Codierung im Bereich "00000000" =&gt; "FFFFFFFF" und f√ºllt ihn links mit Nullen. <br><br>  UniformSplit - Verwandelt einen Schl√ºssel in ein Byte-Array mit hexadezimaler Codierung im Bereich "00" =&gt; "FF" und f√ºllt es rechts mit Nullen. <br><br>  Dar√ºber hinaus k√∂nnen Sie einen beliebigen Bereich oder Schl√ºsselsatz zum Teilen und Einrichten der automatischen Aufteilung angeben.  Einer der einfachsten und effektivsten Ans√§tze ist jedoch UniformSplit und die Verwendung der Hash-Verkettung, z. B. ein hohes Bytepaar, wenn ein Schl√ºssel √ºber die CRC32-Funktion (Zeilenschl√ºssel) und den Zeilenschl√ºssel selbst ausgef√ºhrt wird: <br><br>  Hash + Rowkey <br><br>  Dann werden alle Daten gleichm√§√üig auf die Regionen verteilt.  Beim Lesen werden die ersten beiden Bytes einfach verworfen und der urspr√ºngliche Schl√ºssel bleibt erhalten.  RS steuert auch die Daten- und Schl√ºsselmenge in der Region. Wenn Grenzwerte √ºberschritten werden, werden diese automatisch in Teile zerlegt. <br><br><h2>  7. Fehlertoleranz und Datenlokalit√§t </h2><br>  Da nur eine Region f√ºr jeden Schl√ºsselsatz verantwortlich ist, besteht die L√∂sung f√ºr Probleme im Zusammenhang mit RS-Abst√ºrzen oder Au√üerbetriebnahme darin, alle erforderlichen Daten in HDFS zu speichern.  Wenn RS abst√ºrzt, erkennt der Master dies durch das Fehlen eines Herzschlags auf dem ZooKeeper-Knoten.  Dann weist er die bediente Region einer anderen RS zu, und da die HFiles in einem verteilten Dateisystem gespeichert sind, liest der neue Host sie und bedient die Daten weiterhin.  Da sich einige der Daten m√∂glicherweise im MemStore befinden und keine Zeit hatten, in HFiles zu gelangen, werden WALs, die ebenfalls in HDFS gespeichert sind, zum Wiederherstellen des Betriebsverlaufs verwendet.  Nach dem Rollover von √Ñnderungen kann RS auf Anforderungen reagieren. Die Verschiebung f√ºhrt jedoch dazu, dass sich ein Teil der Daten und ihre Prozesse auf verschiedenen Knoten befinden, d. H.  verminderte Lokalit√§t. <br><br>  Die L√∂sung des Problems ist eine umfassende Komprimierung. Durch diese Prozedur werden Dateien auf die f√ºr sie verantwortlichen Knoten (wo sich ihre Regionen befinden) verschoben, wodurch die Belastung des Netzwerks und der Festplatten w√§hrend dieser Prozedur stark zunimmt.  In Zukunft wird der Zugriff auf Daten jedoch sp√ºrbar beschleunigt.  Dar√ºber hinaus kombiniert major_compaction alle HFiles in einer Datei innerhalb der Region und bereinigt die Daten abh√§ngig von den Tabelleneinstellungen.  Beispielsweise k√∂nnen Sie die Anzahl der Versionen eines Objekts, die Sie speichern m√∂chten, oder seine Lebensdauer angeben. Danach wird das Objekt physisch gel√∂scht. <br><br>  Dieses Verfahren kann sich sehr positiv auf HBase auswirken.  Das folgende Bild zeigt, wie sich die Leistung durch aktive Datenaufzeichnung verschlechtert.  Hier k√∂nnen Sie sehen, wie 40 Streams in eine Tabelle geschrieben wurden und 40 Streams gleichzeitig Daten lesen.  Schreibstr√∂me bilden immer mehr HFiles, die von anderen Str√∂men gelesen werden.  Infolgedessen m√ºssen immer mehr Daten aus dem Speicher gel√∂scht werden, und am Ende beginnt der GC zu arbeiten, was praktisch die gesamte Arbeit l√§hmt.  Der Start einer gr√∂√üeren Verdichtung f√ºhrte zur Beseitigung der daraus resultierenden Blockaden und zur Wiederherstellung der Leistung. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/ha/ga/x2haga1cohdfilxz5ffu_vatfzy.png"></div><br>  Der Test wurde mit 3 DataNode und 4 RS (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Threads) durchgef√ºhrt.  HBase Version 1.2.0-cdh5.14.2 <br><br>  Es ist erw√§hnenswert, dass der Start der Hauptverdichtung an einer "Live" -Tabelle durchgef√ºhrt wurde, in die Daten aktiv geschrieben und gelesen wurden.  Im Netzwerk gab es eine Erkl√§rung, dass dies beim Lesen von Daten zu einer falschen Antwort f√ºhren k√∂nnte.  Zur √úberpr√ºfung wurde ein Prozess gestartet, der neue Daten generierte und in die Tabelle schrieb.  Danach habe ich sofort gelesen und √ºberpr√ºft, ob der erhaltene Wert mit dem aufgezeichneten Wert √ºbereinstimmt.  W√§hrend dieses Prozesses wurde die Hauptverdichtung ungef√§hr 200 Mal gestartet und kein einziger Fehler wurde aufgezeichnet.  M√∂glicherweise tritt das Problem selten und nur bei hoher Last auf. Daher ist es sicherer, die Schreib- und Lesevorg√§nge weiterhin planm√§√üig anzuhalten und die Reinigung durchzuf√ºhren, ohne solche GC-Drawdowns zuzulassen. <br><br>  Au√üerdem wirkt sich eine gr√∂√üere Komprimierung nicht auf den Status von MemStore aus. Um ihn auf die Festplatte zu leeren und zu komprimieren, m√ºssen Sie flush (connection.getAdmin (). Flush (TableName.valueOf (tblName)) verwenden. <br><br><h2>  8. Einstellungen und Leistung </h2><br>  Wie bereits erw√§hnt, zeigt HBase den gr√∂√üten Erfolg, wenn bei der Ausf√ºhrung von BulkLoad nichts unternommen werden muss.  Dies gilt jedoch f√ºr die meisten Systeme und Personen.  Dieses Tool eignet sich jedoch besser f√ºr das Massenstapeln von Daten in gro√üen Bl√∂cken. Wenn f√ºr den Prozess viele konkurrierende Lese- und Schreibanforderungen erforderlich sind, werden die oben beschriebenen Befehle Get und Put verwendet.  Um die optimalen Parameter zu bestimmen, wurden Starts mit verschiedenen Kombinationen von Tabellenparametern und Einstellungen durchgef√ºhrt: <br><br><ul><li>  10 Threads wurden dreimal hintereinander gleichzeitig gestartet (nennen wir es einen Threadblock). </li><li>  Die Betriebszeit aller Fl√ºsse im Block wurde gemittelt und war das Endergebnis des Betriebs des Blocks. </li><li>  Alle Threads arbeiteten mit derselben Tabelle. </li><li>  Vor jedem Start des Thread-Blocks wurde eine gr√∂√üere Komprimierung ausgef√ºhrt. </li><li>  Jeder Block f√ºhrte nur eine der folgenden Operationen aus: </li></ul><br>  - Put <br>  - bekommen <br>  - Holen Sie sich + Put <br><br><ul><li>  Jeder Block f√ºhrte 50.000 Wiederholungen seiner Operation durch. </li><li>  Die Datensatzgr√∂√üe im Block betr√§gt 100 Byte, 1000 Byte oder 10000 Byte (zuf√§llig). </li><li>  Bl√∂cke wurden mit einer anderen Anzahl angeforderter Schl√ºssel (entweder einem Schl√ºssel oder 10) gestartet. </li><li>  Bl√∂cke wurden bei verschiedenen Tabelleneinstellungen gestartet.  Parameter ge√§ndert: </li></ul><br>  - BlockCache = ein- oder ausgeschaltet <br>  - BlockSize = 65 Kb oder 16 Kb <br>  - Partitionen = 1, 5 oder 30 <br>  - MSLAB = ein oder aus <br><br>  Der Block sieht also folgenderma√üen aus: <br><br>  a.  MSLAB-Modus ein- / ausgeschaltet. <br>  b.  Es wurde eine Tabelle erstellt, f√ºr die die folgenden Parameter festgelegt wurden: BlockCache = true / none, BlockSize = 65/16 Kb, Partitions = 1/5/30. <br>  c.  Stellen Sie die Komprimierung GZ ein. <br>  d.  10 Threads wurden gleichzeitig gestartet, wobei 1/10 der Put / Get / Get + Put-Operationen in dieser Tabelle mit Datens√§tzen von 100/1000/10000 Bytes ausgef√ºhrt wurden, wobei 50.000 Abfragen hintereinander ausgef√ºhrt wurden (zuf√§llige Schl√ºssel). <br>  e.  Punkt d wurde dreimal wiederholt. <br>  f.  Die Betriebszeit aller Threads wurde gemittelt. <br><br>  Alle m√∂glichen Kombinationen wurden √ºberpr√ºft.  Es ist vorhersehbar, dass mit zunehmender Aufnahmegr√∂√üe die Geschwindigkeit abnimmt oder das Deaktivieren des Cachings langsamer wird.  Ziel war es jedoch, den Grad und die Bedeutung des Einflusses jedes Parameters zu verstehen. Daher wurden die gesammelten Daten in die Eingabe der linearen Regressionsfunktion eingespeist, wodurch die Zuverl√§ssigkeit mithilfe von t-Statistiken bewertet werden kann.  Nachfolgend sind die Ergebnisse der Bl√∂cke aufgef√ºhrt, die Put-Operationen ausf√ºhren.  Ein vollst√§ndiger Satz von Kombinationen 2 * 2 * 3 * 2 * 3 = 144 Optionen + 72 seit  einige wurden zweimal durchgef√ºhrt.  Daher werden insgesamt 216 Starts durchgef√ºhrt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/5u/wj/ai5uwj0fvmyo9hyqkjg4-cigceq.png"></div><br>  Die Tests wurden an einem Mini-Cluster durchgef√ºhrt, der aus 3 DataNode und 4 RS (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Streams) bestand.  HBase Version 1.2.0-cdh5.14.2. <br><br>  Die h√∂chste Einf√ºgungsgeschwindigkeit von 3,7 Sekunden wurde erreicht, wenn der MSLAB-Modus auf einer Tabelle mit einer Partition mit aktiviertem BlockCache, BlockSize = 16, Datens√§tzen von 100 Bytes mit 10 Teilen pro Packung deaktiviert wurde. <br>  Die niedrigste Einf√ºgungsgeschwindigkeit von 82,8 Sekunden wurde erreicht, wenn der MSLAB-Modus in einer Tabelle mit einer Partition mit aktiviertem BlockCache, BlockSize = 16, Datens√§tzen mit jeweils 10.000 Byte aktiviert wurde. <br><br>  Schauen wir uns nun das Modell an.  Wir sehen ein Modell von guter Qualit√§t f√ºr R2, aber es ist klar, dass eine Extrapolation hier kontraindiziert ist.  Das tats√§chliche Verhalten des Systems beim √Ñndern der Parameter ist nicht linear. Dieses Modell wird nicht f√ºr Prognosen ben√∂tigt, sondern um zu verstehen, was innerhalb der angegebenen Parameter passiert ist.  Hier sehen wir beispielsweise anhand des Student-Kriteriums, dass f√ºr die Put-Operation die Parameter BlockSize und BlockCache keine Rolle spielen (was im Allgemeinen vorhersehbar ist): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/aq/vg/xt/aqvgxt9uyfs_l4m4crnm3adhliy.png"></div><br>  Die Tatsache, dass eine Erh√∂hung der Anzahl der Partitionen zu einer Verringerung der Leistung f√ºhrt, ist jedoch etwas unerwartet (wir haben bereits den positiven Effekt einer Erh√∂hung der Anzahl der Partitionen mit BulkLoad gesehen), obwohl dies verst√§ndlich ist.  Erstens ist es f√ºr die Verarbeitung erforderlich, Abfragen an 30 Regionen anstatt an eine zu bilden, und die Datenmenge ist nicht so, dass sie einen Gewinn ergibt.  Zweitens wird die Gesamtbetriebszeit durch die langsamste RS bestimmt, und da die Anzahl der Datenknoten geringer als die Anzahl der RS ‚Äã‚Äãist, haben einige Regionen keine Lokalit√§t.  Schauen wir uns die Top 5 an: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/md/iu/bxmdiuzfdzqlfe1l8_s_2ktecb0.png"></div><br>  Lassen Sie uns nun die Ergebnisse der Ausf√ºhrung von Get-Bl√∂cken auswerten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/2b/no/zl2bnojdyx-byfpty6yr9poebzg.png"></div><br>  Die Anzahl der Partitionen hat an Bedeutung verloren, was wahrscheinlich darauf zur√ºckzuf√ºhren ist, dass die Daten gut zwischengespeichert sind und der Lesecache der signifikanteste (statistisch) Parameter ist.  Nat√ºrlich ist das Erh√∂hen der Anzahl von Nachrichten in einer Anforderung auch f√ºr die Leistung sehr n√ºtzlich.  Die besten Ergebnisse: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/6d/pu/f06dpurnzlck4po4jw1xpyrphl8.png"></div><br>  Schauen Sie sich zum Schluss zuerst das Modell des ausgef√ºhrten Blocks an und setzen Sie dann: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/fc/23/aefc23q9mcfbtbumdc5qjmw_qrg.png"></div><br>  Hier sind alle Parameter von Bedeutung.  Und die Ergebnisse der F√ºhrer: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q0/vo/th/q0vothoybhc8k6m1ysefviz3tco.png"></div><br><h2>  9. Lasttest </h2><br>  Nun, endlich werden wir eine mehr oder weniger anst√§ndige Ladung starten, aber es ist immer interessanter, wenn es etwas zu vergleichen gibt.  Die Site von DataStax, einem Schl√ºsselentwickler von Cassandra, enth√§lt die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ergebnisse von</a> NT einer Reihe von NoSQL-Repositorys, einschlie√ülich HBase Version 0.98.6-1.  Das Laden wurde von 40 Streams, Datengr√∂√üe 100 Bytes, SSD-Festplatten durchgef√ºhrt.  Das Ergebnis des Testens von Lese-, √Ñnderungs- und Schreibvorg√§ngen zeigte diese Ergebnisse. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ha/7b/bw/ha7bbwydc612f04jtwyqvdvg_ae.png"></div><br>  Soweit ich wei√ü, wurde das Lesen in Bl√∂cken von 100 Datens√§tzen durchgef√ºhrt, und f√ºr 16 HBase-Knoten zeigte der DataStax-Test eine Leistung von 10.000 Operationen pro Sekunde. <br><br>  Es ist ein Gl√ºck, dass unser Cluster auch 16 Knoten hat, aber nicht sehr "gl√ºcklich", dass jeder 64 Kerne (Threads) hat, w√§hrend der DataStax-Test nur 4 hat. Andererseits haben sie SSD-Festplatten und wir haben HDD und mehr Die neue Version der HBase- und CPU-Auslastung w√§hrend des Ladens stieg praktisch nicht signifikant an (visuell um 5-10 Prozent).  Trotzdem werden wir versuchen, mit dieser Konfiguration zu beginnen.  Tabelleneinstellungen Standardm√§√üig wird das Lesen in einem Bereich von Tasten von 0 bis 50 Millionen zuf√§llig durchgef√ºhrt (d. H. Tats√§chlich jedes Mal, wenn eine neue erstellt wird).  In der Tabelle sind 50 Millionen Eintr√§ge in 64 Partitionen unterteilt.  Schl√ºssel sind crc32 gehasht.  Tabelleneinstellungen sind voreingestellt, MSLAB ist aktiviert.  Ab 40 Threads liest jeder Thread einen Satz von 100 zuf√§lligen Schl√ºsseln und schreibt die generierten 100 Bytes sofort auf diese Schl√ºssel zur√ºck. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/po/sd/el/posdel66zx7quvvo3kvrjb6uif8.png"></div><br>  Stand: 16 DataNode und 16 RS (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Streams).  HBase Version 1.2.0-cdh5.14.2. <br><br>  Das durchschnittliche Ergebnis liegt n√§her bei 40.000 Vorg√§ngen pro Sekunde, was deutlich besser ist als im DataStax-Test.  F√ºr die Zwecke des Experiments k√∂nnen die Bedingungen jedoch geringf√ºgig ge√§ndert werden.  Es ist ziemlich unwahrscheinlich, dass alle Arbeiten ausschlie√ülich mit einem Tisch sowie nur mit eindeutigen Schl√ºsseln ausgef√ºhrt werden.  Angenommen, es gibt einen bestimmten ‚Äûhei√üen‚Äú Schl√ºsselsatz, der die Hauptlast erzeugt.  Daher werden wir versuchen, eine Last mit gr√∂√üeren Datens√§tzen (10 KB), ebenfalls in Packungen zu je 100, in 4 verschiedenen Tabellen zu erstellen und den Bereich der angeforderten Schl√ºssel auf 50.000 zu begrenzen. Die folgende Grafik zeigt den Beginn von 40 Threads, jeder Stream liest einen Satz von 100 Schl√ºsseln und schreibt sofort zuf√§llige 10 KB auf diesen Schl√ºsseln zur√ºck. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/ec/mr/f7ecmrebgulvlcyru05c7jzytyi.png"></div><br>  Stand: 16 DataNode und 16 RS (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Streams).  HBase Version 1.2.0-cdh5.14.2. <br><br>  W√§hrend des Ladens wurde mehrmals eine gr√∂√üere Verdichtung gestartet, wie oben gezeigt, ohne dieses Verfahren. Die Leistung wird allm√§hlich beeintr√§chtigt, es tritt jedoch auch eine zus√§tzliche Last w√§hrend der Ausf√ºhrung auf.  Drawdowns werden aus verschiedenen Gr√ºnden verursacht.  Manchmal wurden Threads beendet und w√§hrend des Neustarts gab es eine Pause. Manchmal haben Anwendungen von Drittanbietern den Cluster belastet. <br><br>  Sofortiges Lesen und Schreiben ist eines der schwierigsten Arbeitsszenarien f√ºr HBase.  Wenn Sie nur Put-Anforderungen mit einer geringen Gr√∂√üe, z. B. jeweils 100 Byte, in Stapeln von 10 bis 50.000 Teilen platzieren, k√∂nnen Sie Hunderttausende von Vorg√§ngen pro Sekunde ausf√ºhren, und die Situation ist bei schreibgesch√ºtzten Anforderungen √§hnlich.  Es ist erw√§hnenswert, dass die Ergebnisse radikal besser sind als diejenigen, die bei DataStax vor allem aufgrund von Anfragen in Bl√∂cken von 50.000 erhalten wurden. <br><br><img src="https://habrastorage.org/webt/kv/_j/bv/kv_jbvizskwbod1nxapokckg9s8.png"><br>  Stand: 16 DataNode und 16 RS (CPU Xeon E5-2680 v4 bei 2,40 GHz * 64 Streams).  HBase Version 1.2.0-cdh5.14.2. <br><br><h2>  10. Schlussfolgerungen </h2><br>  Dieses System ist flexibel genug, um konfiguriert zu werden, aber die Auswirkung einer gro√üen Anzahl von Parametern ist noch unbekannt.  Einige von ihnen wurden getestet, waren jedoch nicht in der resultierenden Testsuite enthalten.  Zum Beispiel zeigten vorl√§ufige Experimente die Bedeutungslosigkeit eines Parameters wie DATA_BLOCK_ENCODING, der Informationen unter Verwendung von Werten aus benachbarten Zellen codiert, was f√ºr zuf√§llig generierte Daten durchaus verst√§ndlich ist.  Bei Verwendung einer gro√üen Anzahl sich wiederholender Objekte kann die Verst√§rkung erheblich sein.  Generell kann man sagen, dass HBase den Eindruck einer ziemlich seri√∂sen und gut durchdachten Datenbank erweckt, die bei gro√üen Datenbl√∂cken sehr produktiv sein kann.  Insbesondere wenn es m√∂glich ist, die Lese- und Schreibvorg√§nge rechtzeitig zu verteilen. <br><br>  Wenn etwas Ihrer Meinung nach nicht ausreichend offengelegt wird, bin ich bereit, dies genauer zu erl√§utern.  Wir empfehlen Ihnen, Ihre Erfahrungen mitzuteilen oder zu debattieren, wenn Sie mit etwas nicht einverstanden sind. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de420425/">https://habr.com/ru/post/de420425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de420409/index.html">Lerne OpenGL. Lektion 5.7 - HDR</a></li>
<li><a href="../de420413/index.html">SQLite und NW.js - Schritt-f√ºr-Schritt-Anleitungen zum Erstellen starker Freundschaften</a></li>
<li><a href="../de420415/index.html">Alles, was Sie √ºber das Testen von Wi-Fi-Adaptern wissen wollten, aber Angst hatten zu fragen</a></li>
<li><a href="../de420419/index.html">L√§ufer f√ºr diejenigen, die Dem√ºtigung m√∂gen oder wie wir PixJam ver√§ndert und modifiziert haben</a></li>
<li><a href="../de420423/index.html">Probleme mit der Schnittstelle zwischen Bodenkreuzung</a></li>
<li><a href="../de420429/index.html">USE, RED, PgBouncer, seine Einstellungen und √úberwachung</a></li>
<li><a href="../de420431/index.html">Mars Praktischer Leitfaden zum Terraforming f√ºr Hausfrauen</a></li>
<li><a href="../de420433/index.html">‚ÄûFreitag-Format‚Äú: Musikalische Stra√üen - was ist das und warum sind sie nicht in Russland?</a></li>
<li><a href="../de420435/index.html">Schnellstart mit ARM Mbed: Entwicklung moderner Mikrocontroller f√ºr Anf√§nger</a></li>
<li><a href="../de420437/index.html">Eine praktische Einf√ºhrung in den Paketmanager f√ºr Kubernetes - Helm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>