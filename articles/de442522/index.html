<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘¨ğŸ¿â€ğŸš’ ğŸ¢ â†ªï¸ Intuitives RL (Reinforcement Learning): EinfÃ¼hrung in Advantage-Actor-Critic (A2C) ğŸ‘¨ğŸ¼â€ğŸ­ ğŸ§‘ğŸ¼â€ğŸ¤â€ğŸ§‘ğŸ» ğŸ‘¼ğŸ¿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich mache Sie auf eine Ãœbersetzung des Artikels von Rudy Gilman und Katherine Wang aufmerksam. Intuitive RL: EinfÃ¼hrung in Advantage-Actor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intuitives RL (Reinforcement Learning): EinfÃ¼hrung in Advantage-Actor-Critic (A2C)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442522/"><p>  Hallo Habr!  Ich mache Sie auf eine Ãœbersetzung des Artikels von Rudy Gilman und Katherine Wang aufmerksam. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intuitive RL: EinfÃ¼hrung in Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Reinforced Learning Specialists (RL) haben viele hervorragende Tutorials erstellt.  Die meisten beschreiben RL jedoch anhand mathematischer Gleichungen und abstrakter Diagramme.  Wir denken gerne aus einer anderen Perspektive Ã¼ber das Thema nach.  RL selbst ist davon inspiriert, wie Tiere lernen. Warum also nicht den zugrunde liegenden RL-Mechanismus zurÃ¼ck in natÃ¼rliche PhÃ¤nomene Ã¼bersetzen, die simuliert werden sollen?  Menschen lernen am besten durch Geschichten. </p><br><p>  Dies ist die Geschichte des Actor Advantage Critic (A2C) -Modells.  Das Subjekt-Kritiker-Modell ist eine beliebte Form des Policy-Gradient-Modells, das an sich ein traditioneller RL-Algorithmus ist.  Wenn Sie A2C verstehen, verstehen Sie tiefes RL. </p><br><a name="habracut"></a><p>  Nachdem Sie ein intuitives VerstÃ¤ndnis von A2C erhalten haben, Ã¼berprÃ¼fen Sie: </p><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unsere einfache Implementierung</a> von A2C-Code (fÃ¼r Schulungen) oder unsere industrielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version von PyTorch</a> basierend auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI TensorFlow Baselines-</a> Modell; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine EinfÃ¼hrung in RL von Barto &amp; Sutton</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David Silvers kanonischem Kurs</a> , eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rezension von Yusi Lee</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Denny Brits Repository auf GitHub</a> fÃ¼r einen tiefen Einblick in RL; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der erstaunliche fast.ai-Kurs</a> fÃ¼r die intuitive und praktische Berichterstattung Ã¼ber Deep Learning im Allgemeinen, der in PyTorch implementiert ist. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arthur Giuliani</a> RL Tutorials bei TensorFlow implementiert. </li></ul><p>  Illustrationen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@embermarke</a> </p><br><p>  In RL bewegt sich der Agent, der Klyukovka-Fuchs, durch die von Aktionen umgebenen Staaten und versucht, die Belohnungen wÃ¤hrend der Reise zu maximieren. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C empfÃ¤ngt StatuseingÃ¤nge - SensoreingÃ¤nge bei Klukovka - und generiert zwei AusgÃ¤nge: <br>  1) Eine Bewertung, wie viel VergÃ¼tung ab dem Zeitpunkt des aktuellen Zustands mit Ausnahme der aktuellen (bestehenden) VergÃ¼tung erhalten wird. <br>  2) Eine Empfehlung, welche MaÃŸnahmen zu ergreifen sind (Politik). <br><br>  Kritiker: Wow, was fÃ¼r ein wundervolles Tal!  Es wird ein fruchtbarer Tag fÃ¼r die Nahrungssuche!  Ich wette heute sammle ich 20 Punkte vor Sonnenuntergang. <br>  "Betreff": Diese Blumen sehen wunderschÃ¶n aus, ich habe ein Verlangen nach "A". </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Deep RL-Modelle sind wie jedes andere Klassifizierungs- oder Regressionsmodell Input-Output-Mapping-Maschinen.  Anstatt Bilder oder Text zu kategorisieren, bringen tiefe RL-Modelle ZustÃ¤nde zu Aktionen und / oder ZustÃ¤nde zu Zustandswerten.  A2C macht beides. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  Diese Menge an staatlicher Handlungsbelohnung ist eine Beobachtung.  Sie wird diese Datenzeile in ihr Tagebuch schreiben, aber sie wird noch nicht darÃ¼ber nachdenken.  Sie wird es fÃ¼llen, wenn sie innehÃ¤lt, um nachzudenken. <br><br>  Einige Autoren assoziieren Belohnung 1 mit Zeitschritt 1, andere assoziieren sie mit Schritt 2, aber alle denken an dasselbe Konzept: Die Belohnung bezieht sich auf den Zustand und die Aktion geht unmittelbar davor. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Das Einhaken wiederholt den Vorgang erneut.  ZunÃ¤chst nimmt sie ihre Umgebung wahr und entwickelt eine Funktion V (S) und eine Handlungsempfehlung. <br><br>  Kritiker: Dieses Tal sieht ziemlich normal aus.  V (S) = 19. <br>  Betreff: Die Aktionsoptionen sehen sehr Ã¤hnlich aus.  Ich denke, ich gehe einfach auf Spur "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Dann handelt es. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  ErhÃ¤lt eine Belohnung von +20!  Und zeichnet die Beobachtung auf. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  Sie wiederholt den Vorgang erneut. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  Nachdem Klyukovka drei Beobachtungen gesammelt hat, bleibt er stehen, um nachzudenken. <br><br>  Andere Modellfamilien warten bis zum Ende des Tages (Monte Carlo), wÃ¤hrend andere nach jedem Schritt nachdenken (Ein-Schritte). <br>  Bevor sie ihre interne Kritikerin einstellen kann, muss Klukovka berechnen, wie viele Punkte sie in jedem Staat tatsÃ¤chlich erhalten wird. <br><br>  Aber zuerst! <br>  Schauen wir uns an, wie Klukovkas Cousine Lis Monte Carlo die wahre Bedeutung jedes Staates berechnet. <br><br>  Monte-Carlo-Modelle spiegeln ihre Erfahrung erst am Ende des Spiels wider. Da der Wert des letzten Zustands Null ist, ist es sehr einfach, den wahren Wert dieses vorherigen Zustands als Summe der nach diesem Moment erhaltenen Belohnungen zu ermitteln. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  TatsÃ¤chlich ist dies nur eine Probe V (S) mit hoher Dispersion.  Der Agent kÃ¶nnte leicht einer anderen Flugbahn aus demselben Zustand folgen und somit eine andere Gesamtbelohnung erhalten. </p><br><p>  Aber Klyukovka geht, bleibt stehen und reflektiert viele Male, bis der Tag zu Ende geht.  Sie mÃ¶chte wissen, wie viele Punkte sie wirklich von jedem Staat bis zum Ende des Spiels erhalten wird, da bis zum Ende des Spiels noch einige Stunden verbleiben. <br><br>  Dort macht sie etwas wirklich Kluges - der Fuchs Klyukovka schÃ¤tzt, wie viele Punkte sie fÃ¼r den letzten Staat in diesem Set erhalten wird.  GlÃ¼cklicherweise hat sie eine korrekte EinschÃ¤tzung ihres Zustands - ihren Kritiker. <br>  Mit dieser EinschÃ¤tzung kann Klyukovka die â€richtigenâ€œ Werte der vorherigen ZustÃ¤nde genau wie der Monte-Carlo-Fuchs berechnen. <br><br>  Lis Monte Carlo wertet die Zielmarken aus, setzt die Flugbahn ein und fÃ¼gt Belohnungen aus jedem Zustand hinzu.  A2C schneidet diese Flugbahn und ersetzt sie durch eine EinschÃ¤tzung seines Kritikers.  Diese anfÃ¤ngliche Belastung verringert die Varianz der Punktzahl und ermÃ¶glicht es dem A2C, kontinuierlich zu laufen, wenn auch durch EinfÃ¼hren einer kleinen Vorspannung. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  Die PrÃ¤mien werden hÃ¤ufig reduziert, um der Tatsache Rechnung zu tragen, dass die VergÃ¼tung jetzt besser ist als in der Zukunft.  Der Einfachheit halber reduziert Klukovka seine Belohnungen nicht. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p>  Klukovka kann nun jede Datenzeile durchgehen und seine SchÃ¤tzungen der Zustandswerte mit seinen tatsÃ¤chlichen Werten vergleichen.  Sie nutzt den Unterschied zwischen diesen Zahlen, um ihre VorhersagefÃ¤higkeiten zu perfektionieren.  Klyukovka sammelt den ganzen Tag Ã¼ber alle drei Schritte wertvolle Erfahrungen, die es wert sind, in Betracht gezogen zu werden. <br><br>  â€Ich habe die ZustÃ¤nde 1 und 2 schlecht bewertet. Was habe ich falsch gemacht?  Ja!  Wenn ich das nÃ¤chste Mal solche Federn sehe, werde ich V (S) erhÃ¶hen. <br><br>  Es mag verrÃ¼ckt erscheinen, dass Klukovka ihre V (S) -Bewertung als Grundlage fÃ¼r den Vergleich mit anderen Prognosen verwenden kann.  Aber Tiere (einschlieÃŸlich uns) tun dies die ganze Zeit!  Wenn Sie das GefÃ¼hl haben, dass die Dinge gut laufen, mÃ¼ssen Sie die Aktionen, die Sie in diesen Zustand gebracht haben, nicht neu trainieren. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  Durch Trimmen unserer berechneten Ausgaben und Ersetzen durch eine anfÃ¤ngliche LastschÃ¤tzung haben wir die groÃŸe Monte-Carlo-Varianz durch eine kleine Abweichung ersetzt.  RL-Modelle leiden normalerweise unter einer hohen Streuung (die alle mÃ¶glichen Pfade darstellt), und ein solcher Austausch lohnt sich normalerweise. </p><br><p>  Klukovka wiederholt diesen Vorgang den ganzen Tag, sammelt drei Beobachtungen der staatlichen Handlungsbelohnung und reflektiert sie. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Jeder Satz von drei Beobachtungen ist eine kleine, autokorrelierte Reihe von markierten Trainingsdaten.  Um diese Autokorrelation zu verringern, trainieren viele A2Cs viele Agenten parallel und addieren ihre Erfahrungen, bevor sie sie an ein gemeinsames neuronales Netzwerk senden. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  Der Tag geht endlich zu Ende.  Nur noch zwei Schritte. <br>  Wie bereits erwÃ¤hnt, werden die Empfehlungen der MaÃŸnahmen von Klukovka in prozentualem Vertrauen in seine FÃ¤higkeiten ausgedrÃ¼ckt.  Anstatt nur die zuverlÃ¤ssigste Wahl zu treffen, wÃ¤hlt Klukovka aus dieser Verteilung von Aktionen.  Dies stellt sicher, dass sie nicht immer sicheren, aber mÃ¶glicherweise mittelmÃ¤ÃŸigen Handlungen zustimmt. </p><br><p>  Ich kÃ¶nnte es bereuen, aber ... Manchmal kann man bei der Erforschung unbekannter Dinge zu aufregenden neuen Entdeckungen kommen ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  Um die Forschung weiter zu fÃ¶rdern, wird ein Wert namens Entropie von der Verlustfunktion abgezogen.  Entropie bedeutet den â€Umfangâ€œ der Verteilung von Aktionen. <br>  - Es scheint, dass sich das Spiel ausgezahlt hat! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  Oder nicht? <br><br>  Manchmal befindet sich der Agent in einem Zustand, in dem alle Aktionen zu negativen Ergebnissen fÃ¼hren.  A2C kommt jedoch mit schlechten Situationen gut zurecht. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  Als die Sonne unterging, dachte Klyukovka Ã¼ber die letzten LÃ¶sungen nach. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  Wir haben darÃ¼ber gesprochen, wie Klyukovka seinen inneren Kritiker aufstellt.  Aber wie verfeinert sie ihr inneres â€Themaâ€œ?  Wie lernt sie, solch exquisite Entscheidungen zu treffen? </p><br><p>  Die einfÃ¤ltige Fuchs-Gradienten-Politik wÃ¼rde das tatsÃ¤chliche Einkommen nach der Aktion untersuchen und ihre Politik anpassen, um ein gutes Einkommen wahrscheinlicher zu machen: - Es scheint, dass meine Politik in diesem Zustand zu einem Verlust von 20 Punkten gefÃ¼hrt hat. Ich denke, dass es in Zukunft besser ist, â€Câ€œ zu machen. weniger wahrscheinlich. <br><br>  - Aber warte!  Es ist unfair, die Aktion "C" zu beschuldigen.  Dieser Zustand hatte einen geschÃ¤tzten Wert von -100, also war die Wahl von "C" und das Ende mit -20 tatsÃ¤chlich eine relative Verbesserung von 80!  Ich muss "C" in Zukunft wahrscheinlicher machen. <br><br>  Anstatt seine Richtlinie an die Gesamteinnahmen anzupassen, die sie durch Auswahl von Aktion C erhalten hat, passt sie ihre Aktion an die relativen Einnahmen aus Aktion C an. Dies wird als â€Vorteilâ€œ bezeichnet. </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  Was wir als Vorteil bezeichnet haben, ist einfach ein Fehler.  Als Vorteil nutzt Klukovka es, um AktivitÃ¤ten, die Ã¼berraschend gut waren, wahrscheinlicher zu machen.  Aus Versehen verwendet sie den gleichen Betrag, um ihren internen Kritiker dazu zu bewegen, ihre EinschÃ¤tzung des Statuswerts zu verbessern. <br><br>  Betreff nutzt: <br>  - "Wow, das hat besser funktioniert als ich dachte, Aktion C muss eine gute Idee sein." <br>  Der Kritiker benutzt den Fehler: <br>  â€Aber warum war ich Ã¼berrascht?  Ich hÃ¤tte diesen Zustand wahrscheinlich nicht so negativ bewerten sollen. " <br><br>  Jetzt kÃ¶nnen wir zeigen, wie die Gesamtverluste berechnet werden - wir minimieren diese Funktion, um unser Modell zu verbessern. <br>  "Totalverlust = Handlungsverlust + Wertverlust - Entropie" <br><br>  Bitte beachten Sie, dass wir zur Berechnung der Gradienten von drei qualitativ unterschiedlichen Typen die Werte â€bis einsâ€œ verwenden.  Dies ist effektiv, kann jedoch die Konvergenz erschweren. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Wie alle Tiere wird Klyukovka mit zunehmendem Alter seine FÃ¤higkeit verbessern, die Werte von Staaten vorherzusagen, mehr Vertrauen in seine Handlungen zu gewinnen und weniger hÃ¤ufig Ã¼ber Auszeichnungen Ã¼berrascht zu sein. </p><br><p>  RL-Agenten wie Klukovka generieren nicht nur alle erforderlichen Daten, sondern interagieren einfach mit der Umgebung, sondern bewerten auch die Zielbezeichnungen selbst.  Richtig, RL-Modelle aktualisieren frÃ¼here Noten, um sie besser an neue und verbesserte Noten anzupassen. <br><br>  Wie Dr. David Silver, Leiter der RL-Gruppe bei Google Deepmind, sagt: AI = DL + RL.  Wenn ein Agent wie Klyukovka seine eigene Intelligenz einstellen kann, sind die MÃ¶glichkeiten endlos ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442522/">https://habr.com/ru/post/de442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442512/index.html">Anpassung von Django ORM am Beispiel von ZomboDB</a></li>
<li><a href="../de442514/index.html">Verteilte Systeme. Entwurfsmuster. Buchbesprechung</a></li>
<li><a href="../de442516/index.html">Pandas Leitfaden zur Big Data-Analyse</a></li>
<li><a href="../de442518/index.html">Top 10 Web-Hacking-Techniken 2018</a></li>
<li><a href="../de442520/index.html">Fall. Einsparung von 300 000 p. pro Monat auf kontextbezogene Werbung</a></li>
<li><a href="../de442524/index.html">So erhÃ¶hen Sie die Sicherheit von Personalidentifikations- und Zugangskontrollsystemen</a></li>
<li><a href="../de442526/index.html">Die Geschichte der sowjetischen Kassettenrekorder (Teil 2): â€‹â€‹der Boom von Walkmen, ein GerÃ¤t fÃ¼r den KGB und TonbandgerÃ¤te</a></li>
<li><a href="../de442528/index.html">Wie man das Spiel mit 60fps zum Laufen bringt</a></li>
<li><a href="../de442530/index.html">Wireshark 3.0.0: ÃœberprÃ¼fung von Innovationen</a></li>
<li><a href="../de442532/index.html">Videorecorder fÃ¼r die VideoÃ¼berwachung - kostenlos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>