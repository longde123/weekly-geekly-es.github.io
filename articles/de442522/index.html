<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏿‍🚒 🍢 ↪️ Intuitives RL (Reinforcement Learning): Einführung in Advantage-Actor-Critic (A2C) 👨🏼‍🏭 🧑🏼‍🤝‍🧑🏻 👼🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich mache Sie auf eine Übersetzung des Artikels von Rudy Gilman und Katherine Wang aufmerksam. Intuitive RL: Einführung in Advantage-Actor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intuitives RL (Reinforcement Learning): Einführung in Advantage-Actor-Critic (A2C)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/442522/"><p>  Hallo Habr!  Ich mache Sie auf eine Übersetzung des Artikels von Rudy Gilman und Katherine Wang aufmerksam. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intuitive RL: Einführung in Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Reinforced Learning Specialists (RL) haben viele hervorragende Tutorials erstellt.  Die meisten beschreiben RL jedoch anhand mathematischer Gleichungen und abstrakter Diagramme.  Wir denken gerne aus einer anderen Perspektive über das Thema nach.  RL selbst ist davon inspiriert, wie Tiere lernen. Warum also nicht den zugrunde liegenden RL-Mechanismus zurück in natürliche Phänomene übersetzen, die simuliert werden sollen?  Menschen lernen am besten durch Geschichten. </p><br><p>  Dies ist die Geschichte des Actor Advantage Critic (A2C) -Modells.  Das Subjekt-Kritiker-Modell ist eine beliebte Form des Policy-Gradient-Modells, das an sich ein traditioneller RL-Algorithmus ist.  Wenn Sie A2C verstehen, verstehen Sie tiefes RL. </p><br><a name="habracut"></a><p>  Nachdem Sie ein intuitives Verständnis von A2C erhalten haben, überprüfen Sie: </p><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Unsere einfache Implementierung</a> von A2C-Code (für Schulungen) oder unsere industrielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version von PyTorch</a> basierend auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI TensorFlow Baselines-</a> Modell; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Einführung in RL von Barto &amp; Sutton</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David Silvers kanonischem Kurs</a> , eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rezension von Yusi Lee</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Denny Brits Repository auf GitHub</a> für einen tiefen Einblick in RL; </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der erstaunliche fast.ai-Kurs</a> für die intuitive und praktische Berichterstattung über Deep Learning im Allgemeinen, der in PyTorch implementiert ist. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arthur Giuliani</a> RL Tutorials bei TensorFlow implementiert. </li></ul><p>  Illustrationen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@embermarke</a> </p><br><p>  In RL bewegt sich der Agent, der Klyukovka-Fuchs, durch die von Aktionen umgebenen Staaten und versucht, die Belohnungen während der Reise zu maximieren. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C empfängt Statuseingänge - Sensoreingänge bei Klukovka - und generiert zwei Ausgänge: <br>  1) Eine Bewertung, wie viel Vergütung ab dem Zeitpunkt des aktuellen Zustands mit Ausnahme der aktuellen (bestehenden) Vergütung erhalten wird. <br>  2) Eine Empfehlung, welche Maßnahmen zu ergreifen sind (Politik). <br><br>  Kritiker: Wow, was für ein wundervolles Tal!  Es wird ein fruchtbarer Tag für die Nahrungssuche!  Ich wette heute sammle ich 20 Punkte vor Sonnenuntergang. <br>  "Betreff": Diese Blumen sehen wunderschön aus, ich habe ein Verlangen nach "A". </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Deep RL-Modelle sind wie jedes andere Klassifizierungs- oder Regressionsmodell Input-Output-Mapping-Maschinen.  Anstatt Bilder oder Text zu kategorisieren, bringen tiefe RL-Modelle Zustände zu Aktionen und / oder Zustände zu Zustandswerten.  A2C macht beides. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  Diese Menge an staatlicher Handlungsbelohnung ist eine Beobachtung.  Sie wird diese Datenzeile in ihr Tagebuch schreiben, aber sie wird noch nicht darüber nachdenken.  Sie wird es füllen, wenn sie innehält, um nachzudenken. <br><br>  Einige Autoren assoziieren Belohnung 1 mit Zeitschritt 1, andere assoziieren sie mit Schritt 2, aber alle denken an dasselbe Konzept: Die Belohnung bezieht sich auf den Zustand und die Aktion geht unmittelbar davor. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Das Einhaken wiederholt den Vorgang erneut.  Zunächst nimmt sie ihre Umgebung wahr und entwickelt eine Funktion V (S) und eine Handlungsempfehlung. <br><br>  Kritiker: Dieses Tal sieht ziemlich normal aus.  V (S) = 19. <br>  Betreff: Die Aktionsoptionen sehen sehr ähnlich aus.  Ich denke, ich gehe einfach auf Spur "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Dann handelt es. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  Erhält eine Belohnung von +20!  Und zeichnet die Beobachtung auf. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  Sie wiederholt den Vorgang erneut. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  Nachdem Klyukovka drei Beobachtungen gesammelt hat, bleibt er stehen, um nachzudenken. <br><br>  Andere Modellfamilien warten bis zum Ende des Tages (Monte Carlo), während andere nach jedem Schritt nachdenken (Ein-Schritte). <br>  Bevor sie ihre interne Kritikerin einstellen kann, muss Klukovka berechnen, wie viele Punkte sie in jedem Staat tatsächlich erhalten wird. <br><br>  Aber zuerst! <br>  Schauen wir uns an, wie Klukovkas Cousine Lis Monte Carlo die wahre Bedeutung jedes Staates berechnet. <br><br>  Monte-Carlo-Modelle spiegeln ihre Erfahrung erst am Ende des Spiels wider. Da der Wert des letzten Zustands Null ist, ist es sehr einfach, den wahren Wert dieses vorherigen Zustands als Summe der nach diesem Moment erhaltenen Belohnungen zu ermitteln. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  Tatsächlich ist dies nur eine Probe V (S) mit hoher Dispersion.  Der Agent könnte leicht einer anderen Flugbahn aus demselben Zustand folgen und somit eine andere Gesamtbelohnung erhalten. </p><br><p>  Aber Klyukovka geht, bleibt stehen und reflektiert viele Male, bis der Tag zu Ende geht.  Sie möchte wissen, wie viele Punkte sie wirklich von jedem Staat bis zum Ende des Spiels erhalten wird, da bis zum Ende des Spiels noch einige Stunden verbleiben. <br><br>  Dort macht sie etwas wirklich Kluges - der Fuchs Klyukovka schätzt, wie viele Punkte sie für den letzten Staat in diesem Set erhalten wird.  Glücklicherweise hat sie eine korrekte Einschätzung ihres Zustands - ihren Kritiker. <br>  Mit dieser Einschätzung kann Klyukovka die „richtigen“ Werte der vorherigen Zustände genau wie der Monte-Carlo-Fuchs berechnen. <br><br>  Lis Monte Carlo wertet die Zielmarken aus, setzt die Flugbahn ein und fügt Belohnungen aus jedem Zustand hinzu.  A2C schneidet diese Flugbahn und ersetzt sie durch eine Einschätzung seines Kritikers.  Diese anfängliche Belastung verringert die Varianz der Punktzahl und ermöglicht es dem A2C, kontinuierlich zu laufen, wenn auch durch Einführen einer kleinen Vorspannung. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  Die Prämien werden häufig reduziert, um der Tatsache Rechnung zu tragen, dass die Vergütung jetzt besser ist als in der Zukunft.  Der Einfachheit halber reduziert Klukovka seine Belohnungen nicht. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p>  Klukovka kann nun jede Datenzeile durchgehen und seine Schätzungen der Zustandswerte mit seinen tatsächlichen Werten vergleichen.  Sie nutzt den Unterschied zwischen diesen Zahlen, um ihre Vorhersagefähigkeiten zu perfektionieren.  Klyukovka sammelt den ganzen Tag über alle drei Schritte wertvolle Erfahrungen, die es wert sind, in Betracht gezogen zu werden. <br><br>  „Ich habe die Zustände 1 und 2 schlecht bewertet. Was habe ich falsch gemacht?  Ja!  Wenn ich das nächste Mal solche Federn sehe, werde ich V (S) erhöhen. <br><br>  Es mag verrückt erscheinen, dass Klukovka ihre V (S) -Bewertung als Grundlage für den Vergleich mit anderen Prognosen verwenden kann.  Aber Tiere (einschließlich uns) tun dies die ganze Zeit!  Wenn Sie das Gefühl haben, dass die Dinge gut laufen, müssen Sie die Aktionen, die Sie in diesen Zustand gebracht haben, nicht neu trainieren. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  Durch Trimmen unserer berechneten Ausgaben und Ersetzen durch eine anfängliche Lastschätzung haben wir die große Monte-Carlo-Varianz durch eine kleine Abweichung ersetzt.  RL-Modelle leiden normalerweise unter einer hohen Streuung (die alle möglichen Pfade darstellt), und ein solcher Austausch lohnt sich normalerweise. </p><br><p>  Klukovka wiederholt diesen Vorgang den ganzen Tag, sammelt drei Beobachtungen der staatlichen Handlungsbelohnung und reflektiert sie. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Jeder Satz von drei Beobachtungen ist eine kleine, autokorrelierte Reihe von markierten Trainingsdaten.  Um diese Autokorrelation zu verringern, trainieren viele A2Cs viele Agenten parallel und addieren ihre Erfahrungen, bevor sie sie an ein gemeinsames neuronales Netzwerk senden. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  Der Tag geht endlich zu Ende.  Nur noch zwei Schritte. <br>  Wie bereits erwähnt, werden die Empfehlungen der Maßnahmen von Klukovka in prozentualem Vertrauen in seine Fähigkeiten ausgedrückt.  Anstatt nur die zuverlässigste Wahl zu treffen, wählt Klukovka aus dieser Verteilung von Aktionen.  Dies stellt sicher, dass sie nicht immer sicheren, aber möglicherweise mittelmäßigen Handlungen zustimmt. </p><br><p>  Ich könnte es bereuen, aber ... Manchmal kann man bei der Erforschung unbekannter Dinge zu aufregenden neuen Entdeckungen kommen ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  Um die Forschung weiter zu fördern, wird ein Wert namens Entropie von der Verlustfunktion abgezogen.  Entropie bedeutet den „Umfang“ der Verteilung von Aktionen. <br>  - Es scheint, dass sich das Spiel ausgezahlt hat! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  Oder nicht? <br><br>  Manchmal befindet sich der Agent in einem Zustand, in dem alle Aktionen zu negativen Ergebnissen führen.  A2C kommt jedoch mit schlechten Situationen gut zurecht. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  Als die Sonne unterging, dachte Klyukovka über die letzten Lösungen nach. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  Wir haben darüber gesprochen, wie Klyukovka seinen inneren Kritiker aufstellt.  Aber wie verfeinert sie ihr inneres „Thema“?  Wie lernt sie, solch exquisite Entscheidungen zu treffen? </p><br><p>  Die einfältige Fuchs-Gradienten-Politik würde das tatsächliche Einkommen nach der Aktion untersuchen und ihre Politik anpassen, um ein gutes Einkommen wahrscheinlicher zu machen: - Es scheint, dass meine Politik in diesem Zustand zu einem Verlust von 20 Punkten geführt hat. Ich denke, dass es in Zukunft besser ist, „C“ zu machen. weniger wahrscheinlich. <br><br>  - Aber warte!  Es ist unfair, die Aktion "C" zu beschuldigen.  Dieser Zustand hatte einen geschätzten Wert von -100, also war die Wahl von "C" und das Ende mit -20 tatsächlich eine relative Verbesserung von 80!  Ich muss "C" in Zukunft wahrscheinlicher machen. <br><br>  Anstatt seine Richtlinie an die Gesamteinnahmen anzupassen, die sie durch Auswahl von Aktion C erhalten hat, passt sie ihre Aktion an die relativen Einnahmen aus Aktion C an. Dies wird als „Vorteil“ bezeichnet. </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  Was wir als Vorteil bezeichnet haben, ist einfach ein Fehler.  Als Vorteil nutzt Klukovka es, um Aktivitäten, die überraschend gut waren, wahrscheinlicher zu machen.  Aus Versehen verwendet sie den gleichen Betrag, um ihren internen Kritiker dazu zu bewegen, ihre Einschätzung des Statuswerts zu verbessern. <br><br>  Betreff nutzt: <br>  - "Wow, das hat besser funktioniert als ich dachte, Aktion C muss eine gute Idee sein." <br>  Der Kritiker benutzt den Fehler: <br>  „Aber warum war ich überrascht?  Ich hätte diesen Zustand wahrscheinlich nicht so negativ bewerten sollen. " <br><br>  Jetzt können wir zeigen, wie die Gesamtverluste berechnet werden - wir minimieren diese Funktion, um unser Modell zu verbessern. <br>  "Totalverlust = Handlungsverlust + Wertverlust - Entropie" <br><br>  Bitte beachten Sie, dass wir zur Berechnung der Gradienten von drei qualitativ unterschiedlichen Typen die Werte „bis eins“ verwenden.  Dies ist effektiv, kann jedoch die Konvergenz erschweren. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Wie alle Tiere wird Klyukovka mit zunehmendem Alter seine Fähigkeit verbessern, die Werte von Staaten vorherzusagen, mehr Vertrauen in seine Handlungen zu gewinnen und weniger häufig über Auszeichnungen überrascht zu sein. </p><br><p>  RL-Agenten wie Klukovka generieren nicht nur alle erforderlichen Daten, sondern interagieren einfach mit der Umgebung, sondern bewerten auch die Zielbezeichnungen selbst.  Richtig, RL-Modelle aktualisieren frühere Noten, um sie besser an neue und verbesserte Noten anzupassen. <br><br>  Wie Dr. David Silver, Leiter der RL-Gruppe bei Google Deepmind, sagt: AI = DL + RL.  Wenn ein Agent wie Klyukovka seine eigene Intelligenz einstellen kann, sind die Möglichkeiten endlos ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de442522/">https://habr.com/ru/post/de442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de442512/index.html">Anpassung von Django ORM am Beispiel von ZomboDB</a></li>
<li><a href="../de442514/index.html">Verteilte Systeme. Entwurfsmuster. Buchbesprechung</a></li>
<li><a href="../de442516/index.html">Pandas Leitfaden zur Big Data-Analyse</a></li>
<li><a href="../de442518/index.html">Top 10 Web-Hacking-Techniken 2018</a></li>
<li><a href="../de442520/index.html">Fall. Einsparung von 300 000 p. pro Monat auf kontextbezogene Werbung</a></li>
<li><a href="../de442524/index.html">So erhöhen Sie die Sicherheit von Personalidentifikations- und Zugangskontrollsystemen</a></li>
<li><a href="../de442526/index.html">Die Geschichte der sowjetischen Kassettenrekorder (Teil 2): ​​der Boom von Walkmen, ein Gerät für den KGB und Tonbandgeräte</a></li>
<li><a href="../de442528/index.html">Wie man das Spiel mit 60fps zum Laufen bringt</a></li>
<li><a href="../de442530/index.html">Wireshark 3.0.0: Überprüfung von Innovationen</a></li>
<li><a href="../de442532/index.html">Videorecorder für die Videoüberwachung - kostenlos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>