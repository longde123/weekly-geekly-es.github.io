<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏂🏼 🖕🏻 🚣🏻 Cómo se implementa la arquitectura web tolerante a fallas en la plataforma Mail.ru Cloud Solutions 🚴🏾 🎓 🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Soy Artyom Karamyshev, jefe del equipo de administración de sistemas de Mail.Ru Cloud Solutions (MCS) . Durante el año pasado, hemos tenido...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cómo se implementa la arquitectura web tolerante a fallas en la plataforma Mail.ru Cloud Solutions</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/474180/"><img src="https://habrastorage.org/webt/gd/wp/de/gdwpdevye3ploqkbmd4rwjqkvva.jpeg"><br><br>  Hola Habr!  Soy Artyom Karamyshev, jefe del equipo de administración de sistemas de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mail.Ru Cloud Solutions (MCS)</a> .  Durante el año pasado, hemos tenido muchos lanzamientos de nuevos productos.  Queríamos que los servicios API se escalaran fácilmente, fueran tolerantes a fallas y estuvieran listos para un aumento rápido en la carga de usuarios.  Nuestra plataforma está implementada en OpenStack, y quiero decirle qué problemas de tolerancia a fallas de componentes tuvimos que cerrar para obtener un sistema tolerante a fallas.  Creo que esto será interesante para quienes también desarrollan productos en OpenStack. <br><br>  La tolerancia a fallos global de la plataforma consiste en la estabilidad de sus componentes.  Entonces, pasaremos gradualmente por todos los niveles en los que descubrimos los riesgos y los cerramos. <br><br>  Se puede ver una versión en video de esta historia, cuya fuente original fue un informe en la conferencia Uptime day 4 organizada por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ITSumma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el canal de YouTube Uptime Community</a> . <br><a name="habracut"></a><br>
<h2>  Tolerancia a fallos de la arquitectura física </h2><br>  La parte pública de la nube MCS se basa ahora en dos centros de datos de Nivel III, entre ellos hay una fibra oscura propia, reservada en la capa física por diferentes rutas, con un rendimiento de 200 Gb / s.  El nivel de nivel III proporciona el nivel necesario de resistencia de la infraestructura física. <br><br>  La fibra oscura está reservada tanto a nivel físico como lógico.  El proceso de reserva de canales fue iterativo, surgieron problemas y estamos mejorando constantemente la comunicación entre los centros de datos. <br><br><blockquote>  Por ejemplo, no hace mucho tiempo, cuando trabajaba en un pozo al lado de uno de los centros de datos, una excavadora perforaba una tubería, dentro de esta tubería había un cable óptico principal y uno de respaldo.  Nuestro canal de comunicación tolerante a fallas con el centro de datos resultó ser vulnerable en un punto, en el pozo.  En consecuencia, hemos perdido parte de la infraestructura.  Llegamos a conclusiones, tomamos una serie de acciones, incluida la colocación de ópticas adicionales a lo largo de un pozo vecino. </blockquote><br>  En los centros de datos hay puntos de presencia de proveedores de comunicación a los que transmitimos nuestros prefijos a través de BGP.  Para cada dirección de red, se selecciona la mejor métrica, lo que permite a diferentes clientes proporcionar la mejor calidad de conexión.  Si se desconecta la comunicación a través de un proveedor, reconstruimos nuestra ruta a través de los proveedores disponibles. <br><br>  En caso de falla de un proveedor, cambiamos automáticamente al siguiente.  En caso de falla de uno de los centros de datos, tenemos una copia espejo de nuestros servicios en el segundo centro de datos, que se llevan toda la carga. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d0/m8/ly/d0m8lykvifmc-gum-h9mbppyn3g.jpeg"></div><br>  <i>Resistencia de infraestructura física</i> <br><br><h2>  Lo que usamos para la tolerancia a fallas a nivel de aplicación </h2><br>  Nuestro servicio se basa en una serie de componentes de código abierto. <br><br>  <b>ExaBGP</b> es un servicio que implementa una serie de funciones utilizando el protocolo de enrutamiento dinámico basado en BGP.  Lo usamos activamente para anunciar nuestras direcciones IP blancas a través de las cuales los usuarios obtienen acceso a la API. <br><br>  <b>HAProxy</b> es un equilibrador altamente cargado que le permite configurar reglas muy flexibles para equilibrar el tráfico en diferentes niveles del modelo OSI.  Lo usamos para equilibrar todos los servicios: bases de datos, corredores de mensajes, servicios API, servicios web, nuestros proyectos internos: todo está detrás de HAProxy. <br><br>  <b>Aplicación API</b> : una <b>aplicación</b> web escrita en python, con la cual el usuario controla su infraestructura, su servicio. <br><br>  <b>Aplicación de trabajador</b> (en lo sucesivo, simplemente denominado trabajador): en los servicios de OpenStack es un demonio de infraestructura que le permite traducir comandos de API a la infraestructura.  Por ejemplo, se crea un disco en trabajador, y una solicitud de creación se encuentra en la API de la aplicación. <br><br><h2>  Arquitectura de aplicación estándar de OpenStack </h2><br>  La mayoría de los servicios desarrollados para OpenStack intentan seguir un solo paradigma.  Un servicio generalmente consta de 2 partes: API y trabajadores (ejecutores de fondo).  Por lo general, una API es una aplicación WSGI de Python que se ejecuta como un proceso independiente (daemon) o utilizando un servidor web Nginx, Apache.  La API procesa la solicitud del usuario y pasa más instrucciones a la aplicación del trabajador.  La transmisión se realiza utilizando un intermediario de mensajes, generalmente RabbitMQ, el resto está mal soportado.  Cuando los mensajes llegan al agente, los trabajadores los procesan y, si es necesario, devuelven una respuesta. <br><br>  Este paradigma implica puntos comunes de falla aislados: RabbitMQ y la base de datos.  Pero RabbitMQ está aislado dentro de un servicio y, en teoría, puede ser individual para cada servicio.  Entonces, en MCS compartimos estos servicios tanto como sea posible, para cada proyecto individual creamos una base de datos separada, un RabbitMQ separado.  Este enfoque es bueno porque en el caso de un accidente en algunos puntos vulnerables, no todos los servicios se rompen, sino solo una parte. <br><br>  El número de aplicaciones de los trabajadores es ilimitado, por lo que la API puede escalar fácilmente horizontalmente detrás de los equilibradores para aumentar la productividad y la tolerancia a fallas. <br><br><blockquote>  Algunos servicios requieren coordinación dentro del servicio, cuando ocurren operaciones secuenciales complejas entre API y trabajadores.  En este caso, se utiliza un solo centro de coordinación, un sistema de clúster como Redis, Memcache, etc., que permite a un trabajador decirle al otro que esta tarea se le ha asignado ("por favor, no la tome").  Usamos etcd.  Como regla general, los trabajadores se comunican activamente con la base de datos, escriben y leen información desde allí.  Como base de datos, usamos mariadb, que tenemos en el clúster multimaster. <br></blockquote><br>  Tal servicio clásico de usuario único está organizado de una manera generalmente aceptada para OpenStack.  Puede considerarse como un sistema cerrado, para el cual los métodos de escala y tolerancia a fallas son bastante obvios.  Por ejemplo, para la tolerancia a fallos de la API, es suficiente poner un equilibrador delante de ellos.  La escala de los trabajadores se logra aumentando su número. <br><br>  Los puntos débiles en todo el esquema son RabbitMQ y MariaDB.  Su arquitectura merece un artículo separado. En este artículo quiero centrarme en la tolerancia a fallos de la API. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1a/ab/i1/1aabi1ew0ctxnlrcm2j78nbhefk.jpeg"></div><br>  <i>Arquitectura de aplicación de OpenStack</i>  <i>Equilibrio y resistencia de la plataforma en la nube</i> <br><br><h2>  Hacer que HAProxy Balancer sea resistente con ExaBGP </h2><br>  Para que nuestras API sean escalables, rápidas y tolerantes a fallas, configuramos un equilibrador frente a ellas.  Elegimos HAProxy.  En mi opinión, tiene todas las características necesarias para nuestra tarea: equilibrio en varios niveles de OSI, interfaz de gestión, flexibilidad y escalabilidad, una gran cantidad de métodos de equilibrio, soporte para tablas de sesión. <br><br>  El primer problema que debía resolverse era la tolerancia a fallas del equilibrador en sí.  Simplemente instalar el equilibrador también crea un punto de falla: el equilibrador se rompe, el servicio se cae.  Para evitar esto, utilizamos HAProxy junto con ExaBGP. <br><br>  ExaBGP le permite implementar un mecanismo para verificar el estado de un servicio.  Utilizamos este mecanismo para verificar la funcionalidad de HAProxy y, en caso de problemas, deshabilitar el servicio HAProxy de BGP. <br><br>  <b>Esquema ExaBGP + HAProxy</b> <br><br><ol><li>  Instalamos el software necesario en tres servidores, ExaBGP y HAProxy. </li><li>  En cada uno de los servidores creamos una interfaz de bucle invertido. </li><li>  En los tres servidores, asignamos la misma dirección IP blanca a esta interfaz. </li><li>  Se anuncia una dirección IP blanca en Internet a través de ExaBGP. </li></ol><br>  La tolerancia a fallas se logra al anunciar la misma dirección IP de los tres servidores.  Desde el punto de vista de la red, se puede acceder a la misma dirección desde tres próximas esperanzas diferentes.  El enrutador ve tres rutas idénticas, selecciona la mayor prioridad de ellas según su propia métrica (esta suele ser la misma opción) y el tráfico va solo a uno de los servidores. <br><br>  En caso de problemas con la operación de HAProxy o falla del servidor, ExaBGP deja de anunciar la ruta y el tráfico cambia sin problemas a otro servidor. <br><br>  Por lo tanto, hemos logrado la tolerancia a fallos del equilibrador. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ij/c_/cz/ijc_cz1jvhlug0axiwiiqjqpcww.jpeg"></div><br>  <i>Tolerancia a fallos de equilibradores de HAProxy</i> <br><br>  El esquema resultó ser imperfecto: aprendimos cómo reservar HAProxy, pero no aprendimos cómo distribuir la carga dentro de los servicios.  Por lo tanto, ampliamos un poco este esquema: pasamos al equilibrio entre varias direcciones IP blancas. <br><br><h2>  Equilibrio basado en DNS más BGP </h2><br>  El problema del equilibrio de carga antes de nuestro HAProxy seguía sin resolverse.  Sin embargo, se puede resolver de manera bastante simple, como lo hicimos en casa. <br><br>  Para equilibrar los tres servidores, necesitará 3 direcciones IP blancas y un buen DNS antiguo.  Cada una de estas direcciones se define en la interfaz de bucle invertido de cada HAProxy y se anuncia en Internet. <br><br>  OpenStack utiliza un catálogo de servicios para administrar recursos, que establece la API de punto final de un servicio.  En este directorio, prescribimos un nombre de dominio: public.infra.mail.ru, que se resuelve a través de DNS con tres direcciones IP diferentes.  Como resultado, obtenemos un equilibrio de carga entre las tres direcciones a través de DNS. <br><br>  Pero desde que anunciamos direcciones IP blancas, no controlamos las prioridades de selección del servidor, hasta ahora esto no es equilibrado.  Como regla, solo se seleccionará un servidor por prioridad de la dirección IP, y los otros dos estarán inactivos, ya que no se especifican métricas en BGP. <br><br>  Comenzamos a dar rutas a través de ExaBGP con diferentes métricas.  Cada equilibrador anuncia las tres direcciones IP blancas, pero una de ellas, la principal para este equilibrador, se anuncia con una métrica mínima.  Entonces, mientras los tres equilibradores están en funcionamiento, las llamadas a la primera dirección IP recaen en el primer equilibrador, las llamadas al segundo al segundo, al tercero al tercero. <br><br>  ¿Qué sucede cuando cae uno de los balanceadores?  En caso de falla de cualquier equilibrador por su base, la dirección aún se anuncia de los otros dos, el tráfico entre ellos se redistribuye.  Por lo tanto, le damos al usuario a través del DNS varias direcciones IP a la vez.  Al equilibrar DNS y diferentes métricas, obtenemos una distribución de carga uniforme en los tres equilibradores.  Y al mismo tiempo no perdemos la tolerancia a fallas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ek/xc/3z/ekxc3zsz5oazwdiqziwp_idfk7a.jpeg"></div><br>  <i>Equilibrio HAProxy basado en DNS + BGP</i> <br><br><h2>  Interacción entre ExaBGP y HAProxy </h2><br>  Entonces, implementamos tolerancia a fallas en caso de que el servidor se fuera, en función de la terminación del anuncio de rutas.  Pero HAProxy también se puede desconectar por otras razones además de la falla del servidor: errores de administración, fallas de servicio.  Queremos eliminar el equilibrador roto debajo de la carga y en estos casos, y necesitamos otro mecanismo. <br><br>  Por lo tanto, al expandir el esquema anterior, implementamos un latido entre ExaBGP y HAProxy.  Esta es una implementación de software de la interacción entre ExaBGP y HAProxy, cuando ExaBGP usa scripts personalizados para verificar el estado de las aplicaciones. <br><br>  Para hacer esto, en la configuración ExaBGP, debe configurar un verificador de salud que pueda verificar el estado de HAProxy.  En nuestro caso, configuramos el backend de salud en HAProxy, y desde el lado de ExaBGP verificamos con una simple solicitud GET.  Si el anuncio deja de ocurrir, lo más probable es que HAProxy no funcione y no es necesario anunciarlo. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/w0/ac/5c/w0ac5cvqsvjtki2cqzcgtgyk4x4.jpeg"></div><br>  <i>HAProxy Health Check</i> <br><br><h2>  HAProxy Peers: sincronización de sesión </h2><br>  Lo siguiente que hizo fue sincronizar las sesiones.  Cuando se trabaja a través de equilibradores distribuidos, es difícil organizar el almacenamiento de información sobre sesiones de clientes.  Pero HAProxy es uno de los pocos equilibradores que puede hacer esto debido a la funcionalidad Peers: la capacidad de transferir tablas de sesión entre diferentes procesos de HAProxy. <br><br>  Existen diferentes métodos de equilibrio: simples, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">round-robin</a> , y avanzados, cuando se recuerda una sesión del cliente, y cada vez que llega al mismo servidor que antes.  Queríamos implementar la segunda opción. <br><br>  HAProxy utiliza tablas de palo para guardar sesiones de clientes para este mecanismo.  Guardan la dirección IP de origen del cliente, la dirección de destino seleccionada (backend) y alguna información de servicio.  Por lo general, las tablas de palo se usan para guardar el par de origen-IP + destino-IP, lo cual es especialmente útil para aplicaciones que no pueden transmitir el contexto de una sesión de usuario cuando se cambia a otro equilibrador, por ejemplo, en el modo de equilibrio RoundRobin. <br><br>  Si se enseña a la tabla de palo a moverse entre diferentes procesos HAProxy (entre los cuales se produce el equilibrio), nuestros equilibradores podrán trabajar con un grupo de tablas de palo.  Esto permitirá cambiar sin problemas la red del cliente cuando uno de los equilibradores caiga, el trabajo con las sesiones del cliente continuará en los mismos backends que se seleccionaron previamente. <br><br>  Para un funcionamiento correcto, se debe resolver la dirección IP de origen del equilibrador desde el que se establece la sesión.  En nuestro caso, esta es una dirección dinámica en la interfaz de bucle invertido. <br><br>  El funcionamiento correcto de los compañeros se logra solo en ciertas condiciones.  Es decir, los tiempos de espera de TCP deben ser lo suficientemente grandes o el conmutador debe ser lo suficientemente rápido para que la sesión de TCP no tenga tiempo de interrumpirse.  Sin embargo, esto permite una conmutación perfecta. <br><br>  En IaaS tenemos un servicio basado en la misma tecnología.  Este es un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Load Balancer como un servicio para OpenStack</a> llamado Octavia.  Se basa en dos procesos HAProxy, originalmente incluía soporte de pares.  Han demostrado su valía en este servicio. <br><br>  La imagen muestra esquemáticamente el movimiento de las tablas de pares entre tres instancias de HAProxy, se sugiere una configuración, cómo se puede configurar esto: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ov/ol/wr/ovolwrmp-gzrvyybotjjagtb-re.jpeg"></div><br>  <i>HAProxy Peers (sincronización de sesión)</i> <br><br>  Si implementa el mismo esquema, su trabajo debe probarse cuidadosamente.  No es el hecho de que esto funcione de la misma manera en el 100% de los casos.  Pero al menos no perderá tablas de memoria cuando necesite recordar la IP de origen del cliente. <br><br><h2>  Limitar el número de solicitudes simultáneas del mismo cliente </h2><br>  Cualquier servicio que sea de dominio público, incluidas nuestras API, puede estar sujeto a avalanchas de solicitudes.  Los motivos pueden ser completamente diferentes, desde errores de usuario hasta ataques dirigidos.  Periódicamente, hacemos DDoS en direcciones IP.  Los clientes a menudo cometen errores en sus scripts; nos hacen mini-DDoS. <br><br>  De una forma u otra, se debe proporcionar protección adicional.  La solución obvia es limitar el número de solicitudes de API y no perder el tiempo de CPU procesando solicitudes maliciosas. <br><br>  Para implementar tales restricciones, usamos límites de velocidad, organizados en base a HAProxy, usando las mismas tablas de palo.  Los límites se configuran de manera bastante simple y le permiten limitar al usuario por la cantidad de solicitudes a la API.  El algoritmo recuerda la IP de origen desde la cual se realizan las solicitudes y limita el número de solicitudes simultáneas de un usuario.  Por supuesto, calculamos el perfil de carga de API promedio para cada servicio y establecemos el límite ≈ 10 veces este valor.  Hasta ahora, continuamos monitoreando de cerca la situación, mantenemos nuestro dedo en el pulso. <br><br>  ¿Cómo se ve en la práctica?  Tenemos clientes que usan constantemente nuestras API de escalado automático.  Crean aproximadamente doscientas o trescientas máquinas virtuales más cerca de la mañana y las eliminan más cerca de la tarde.  Para OpenStack, cree una máquina virtual, también con servicios PaaS, al menos 1000 solicitudes API, ya que la interacción entre los servicios también se lleva a cabo a través de la API. <br><br>  Tal lanzamiento de tareas provoca una carga bastante grande.  Estimamos esta carga, recolectamos picos diarios, los incrementamos diez veces y este se convirtió en nuestro límite de velocidad.  Mantenemos nuestro dedo en el pulso.  A menudo vemos bots, escáneres, que intentan mirarnos, si tenemos scripts CGA que se pueden ejecutar, los cortamos activamente. <br><br><h2>  Cómo actualizar la base de código discretamente para los usuarios </h2><br>  También implementamos tolerancia a fallas a nivel de procesos de implementación de código.  Hay bloqueos durante los despliegues, pero su impacto en la disponibilidad del servicio se puede minimizar. <br><br>  Estamos constantemente actualizando nuestros servicios y deberíamos garantizar el proceso de actualización de la base del código sin efecto para los usuarios.  Logramos resolver este problema utilizando las capacidades de administración de HAProxy y la implementación de Graceful Shutdown en nuestros servicios. <br><br>  Para resolver este problema, era necesario proporcionar un control equilibrador y el cierre "correcto" de los servicios: <br><br><ul><li>  En el caso de HAProxy, el control se realiza a través del archivo de estadísticas, que es esencialmente un socket y se define en la configuración de HAProxy.  Puede enviarle comandos a través de stdio.  Pero nuestra principal herramienta de control de configuración es ansible, por lo que tiene un módulo incorporado para administrar HAProxy.  Que estamos usando activamente. </li><li>  La mayoría de nuestros servicios de API y motor admiten tecnologías de apagado elegantes: al apagarse, esperan a que se complete la tarea actual, ya sea una solicitud HTTP o algún tipo de tarea de utilidad.  Lo mismo sucede con el trabajador.  Él conoce todas las tareas que hace y termina cuando ha completado con éxito todo. </li></ul><br>  Gracias a estos dos puntos, el algoritmo seguro de nuestra implementación es el siguiente. <br><br><ol><li>  El desarrollador construye un nuevo paquete de código (tenemos RPM), prueba en el entorno de desarrollo, prueba en la etapa y lo deja en el repositorio de la etapa. </li><li>  El desarrollador coloca la tarea en la implementación con la descripción más detallada de los "artefactos": la versión del nuevo paquete, una descripción de la nueva funcionalidad y otros detalles sobre la implementación, si es necesario. </li><li>  El administrador del sistema inicia la actualización.  Lanza el libro de jugadas Ansible, que a su vez hace lo siguiente: <br><ul><li>  Toma un paquete del repositorio de la etapa, actualiza la versión del paquete en el repositorio del producto con él. </li><li>  Hace una lista de backends del servicio actualizado. </li><li>  Apaga el primer servicio actualizado en HAProxy y espera el final de sus procesos.  Gracias al apagado correcto, estamos seguros de que todas las solicitudes actuales de los clientes se completarán con éxito. </li><li>  Después de que la API, los trabajadores y HAProxy se detienen por completo, el código se actualiza. </li><li>  Ansible lanza servicios. </li><li>  Para cada servicio, extrae ciertos "bolígrafos" que realizan pruebas unitarias para una serie de pruebas clave predefinidas.  Se produce una comprobación básica del nuevo código. </li><li>  Si no se encontraron errores en el paso anterior, se activa el backend. </li><li>  Ir al siguiente backend. </li></ul></li><li>  Después de actualizar todos los backends, se inician las pruebas funcionales.  Si no son suficientes, entonces el desarrollador analiza cualquier nueva funcionalidad que hizo. </li></ol><br>  En este despliegue se completa. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-b/km/dt/-bkmdt98ituj53jetxiaay4uf4c.jpeg"></div><br>  <i>Ciclo de actualización del servicio</i> <br><br>  Este esquema no funcionaría si no tuviéramos una regla.  Apoyamos las versiones antiguas y nuevas en la batalla.  De antemano, en la etapa de desarrollo de software, se establece que incluso si hay cambios en la base de datos del servicio, no romperán el código anterior.  Como resultado, la base del código se actualiza gradualmente. <br><br><h2>  Conclusión </h2><br>  Al compartir mis propios pensamientos sobre la arquitectura WEB tolerante a fallas, quiero señalar una vez más sus puntos clave: <br><br><ul><li>  tolerancia a fallas físicas; </li><li>  tolerancia a fallos de red (equilibradores, BGP); </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tolerancia a fallas de software usado y desarrollado. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ¡Todo el tiempo de actividad estable! </font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/474180/">https://habr.com/ru/post/474180/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../474166/index.html">Índices de portada para GiST</a></li>
<li><a href="../474170/index.html">Confesión de diseño - 15 de noviembre, Moscú, DI Telegraph</a></li>
<li><a href="../474172/index.html">Una multa de 30 mil euros por el uso ilegal de cookies.</a></li>
<li><a href="../474176/index.html">11 videos del primer día de DevFest 2019 en Kaliningrado</a></li>
<li><a href="../474178/index.html">IVR en Webhook</a></li>
<li><a href="../474184/index.html">Pasamos el desafío de Callum Macrae al 100%</a></li>
<li><a href="../474186/index.html">Refutando mitos: prácticas reales de TI en Armenia</a></li>
<li><a href="../474192/index.html">¿Por qué cambié de UX a PM y luego a Lead PM y qué ha cambiado?</a></li>
<li><a href="../474194/index.html">Equipo de brújula</a></li>
<li><a href="../474196/index.html">Los 10 hitos más importantes en el desarrollo de IA hoy</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>