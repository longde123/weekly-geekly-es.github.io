<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí≠ üßíüèª ‚öñÔ∏è √âquilibrage du trafic VoIP √† tol√©rance de pannes. Commutation de charge entre les centres de donn√©es aux heures de pointe üìÜ üë©üèΩ‚Äç‚öñÔ∏è üë∂üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quelques mots sur ce que nous faisons. DINS est impliqu√© dans le d√©veloppement et le support du service UCaaS sur le march√© international pour les ent...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>√âquilibrage du trafic VoIP √† tol√©rance de pannes. Commutation de charge entre les centres de donn√©es aux heures de pointe</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dins/blog/436404/"><p>  Quelques mots sur ce que nous faisons.  DINS est impliqu√© dans le d√©veloppement et le support du service UCaaS sur le march√© international pour les entreprises clientes.  Le service est utilis√© √† la fois par les petites entreprises et les startups, ainsi que par les grandes entreprises.  Les clients se connectent via Internet via le protocole SIP via TCP, TLS ou WSS.  Cela cr√©e une charge assez importante: pr√®s de 1,5 million de connexions depuis les terminaux - t√©l√©phones Polycom / Cisco / Yealink et clients logiciels pour PC / Mac / IOS / Android. </p><br><p>  Dans cet article, je parle de la fa√ßon dont les points d'entr√©e VoIP sont organis√©s. </p><a name="habracut"></a><br><h3 id="predystoriya">  Contexte </h3><br><p>  Sur le p√©rim√®tre du syst√®me (entre les terminaux et le noyau) se trouvent SBC (Session Border Controller) commercial. </p><br><p>  Depuis 2012, nous utilisons des solutions d'Acme Packet, acquises plus tard par Oracle.  Avant cela, nous utilisions NatPASS. </p><br><p>  √ânum√©rez bri√®vement les fonctionnalit√©s que nous utilisons: </p><br><p>  ‚Ä¢ Travers√©e NAT; <br>  ‚Ä¢ B2BUA; <br>  ‚Ä¢ Normalisation SIP (en-t√™tes autoris√©s / interdits, r√®gles de manipulation d'en-t√™te, etc.) <br>  ‚Ä¢ d√©chargement TLS et SRTP; <br>  ‚Ä¢ Conversion de transport (√† l'int√©rieur du syst√®me, nous utilisons SIP sur UDP); <br>  ‚Ä¢ Surveillance MOS (via RTCP-XR); <br>  ‚Ä¢ ACL, d√©tection de Bruteforce; <br>  ‚Ä¢ R√©duction du trafic d'enregistrement en raison de l'expiration accrue des contacts (faible expiration c√¥t√© acc√®s, haut c√¥t√© c≈ìur); <br>  ‚Ä¢ Limitation des messages SIP par m√©thode. </p><br><p> Les syst√®mes commerciaux ont leurs avantages √©vidents (fonctionnalit√© pr√™te √† l'emploi, support commercial) et leurs inconv√©nients (prix, d√©lai de livraison, manque d'opportunit√© ou d√©lais trop longs pour mettre en ≈ìuvre les nouvelles fonctionnalit√©s dont nous avons besoin, d√©lais pour r√©soudre les probl√®mes, etc.).  Peu √† peu, les d√©fauts ont commenc√© √† √™tre d√©pass√©s et il est devenu clair que le besoin √©tait venu de d√©velopper nos propres solutions. </p><br><p>  Le d√©veloppement a √©t√© lanc√© il y a un an et demi.  Dans le sous-syst√®me frontalier, nous distinguions traditionnellement 2 composants principaux: les serveurs SIP et Media;  √©quilibreurs de charge au-dessus de chaque composant.  Je travaille sur les points d'entr√©e / √©quilibreurs ici, donc j'essaierai d'en parler. </p><br><h4 id="trebovaniya">  Pr√©requis </h4><br><ul><li>  Tol√©rance aux pannes: le syst√®me doit fournir un service en cas de d√©faillance d'une ou plusieurs instances dans le centre de donn√©es ou l'ensemble du centre de donn√©es </li><li>  Facilit√© de maintenance: nous voulons pouvoir basculer les charges d'un centre de donn√©es √† un autre </li><li>  √âvolutivit√©: je veux augmenter la capacit√© rapidement et √† peu de frais </li></ul><br><h4 id="balansirovka">  √âquilibrage </h4><br><p>  Nous avons s√©lectionn√© IPVS (alias LVS) en mode IPIP (tunnel de trafic).  Je ne vais pas entrer dans une analyse comparative de NAT / DR / TUN / L3DSR, (vous pouvez lire sur les modes, par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ), je mentionnerai seulement les raisons: </p><br><ul><li>  Nous ne voulons pas imposer aux backends l'exigence d'√™tre sur le m√™me sous-r√©seau que LVS (les pools contiennent des backends √† la fois de nos propres centres de donn√©es et des centres de donn√©es distants); </li><li>  Le backend doit recevoir l'IP source d'origine du client (ou son NAT), en d'autres termes, le NAT source ne convient pas; </li><li>  Le backend doit prendre en charge le travail simultan√© avec plusieurs VIP. </li></ul><br><p>  Nous √©quilibrons le trafic multim√©dia (il s'est av√©r√© tr√®s difficile, nous allons refuser), donc le sch√©ma de d√©ploiement actuel dans le centre de donn√©es est le suivant: <br><br><img src="https://habrastorage.org/webt/fk/vh/wg/fkvhwgftogjrev-a931o3a9qk-e.png"><br><br>  La strat√©gie d'√©quilibrage IPVS actuelle est ¬´sed¬ª (le d√©lai le plus court attendu), plus √† ce sujet.  Contrairement √† Weighted Round Robin / Weighted Least-Connection, il vous permet de ne pas transf√©rer le trafic vers des backends avec des poids inf√©rieurs jusqu'√† ce qu'un certain seuil soit atteint.  Le d√©lai le plus court attendu est calcul√© par la formule (Ci + 1) / Ui, o√π Ci est le nombre de connexions sur le backend i, Ui est le poids du backend.  Par exemple, s'il y a des backends dans le pool avec des poids de 50 000 et 2, les nouvelles connexions seront distribu√©es par les premiers jusqu'√† ce que chaque serveur atteigne 25 000 connexions ou jusqu'√† ce que uthreshold soit atteint - une limite sur le nombre total de connexions. <br>  En savoir plus sur les strat√©gies d'√©quilibrage dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">man ipvsadm</a> . </p><br><p>  Le pool IPVS ressemble √† ceci (ici et ci-dessous, les adresses IP fictives sont r√©pertori√©es): </p><br><pre><code class="plaintext hljs"># ipvsadm -ln Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 1.1.1.1:5060 sed -&gt; 10.11.100.181:5060 Tunnel 50000 5903 4 -&gt; 10.11.100.192:5060 Tunnel 50000 5905 1 -&gt; 10.12.100.137:5060 Tunnel 2 0 0 -&gt; 10.12.100.144:5060 Tunnel 2 0 0</code> </pre> <br><p>  La charge sur le VIP est r√©partie sur des serveurs avec un poids de 50 000 (ils sont d√©ploy√©s dans le m√™me centre de donn√©es qu'une instance LVS sp√©cifique), s'ils sont surcharg√©s ou entrent dans une liste noire, la charge ira √† la partie de sauvegarde du pool - serveurs avec un poids de 2, qui sont situ√©s dans centre de donn√©es voisin. </p><br><p>  Exactement le m√™me pool, mais avec des balances, au contraire, est configur√© dans le centre de donn√©es voisin (sur le syst√®me de production, le nombre de backends, bien s√ªr, est beaucoup plus important). </p><br><p>  La synchronisation des connexions via la synchronisation ipvs permet au LVS de sauvegarde de conna√Ætre toutes les connexions actuelles. </p><br><p>  Pour la synchronisation entre les centres de donn√©es, une technique ¬´sale¬ª a √©t√© appliqu√©e, qui fonctionne n√©anmoins tr√®s bien.  La synchronisation IPVS ne fonctionne que par multidiffusion, ce qui nous a √©t√© difficile √† livrer correctement au contr√¥leur de domaine voisin.  Au lieu de la multidiffusion, nous dupliquons le trafic de synchronisation via TEE cible iptables du ma√Ætre ipvs vers le tunnel ip-ip vers le serveur dans le DC voisin, et il peut y avoir plusieurs h√¥tes / centres de donn√©es cibles: </p><br><pre> <code class="plaintext hljs">#### start ipvs sync master role: ipvsadm --start-daemon master --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### duplicate all sync packets to remote LVS servers using iptables TEE target: iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.10 # ip-ip remote lvs server 1 iptables -t mangle -A POSTROUTING -d 224.0.0.81/32 -o sync01 -j TEE --gateway 172.20.21.14 # ip-ip remote lvs server 2 #### start ipvs sync backup role: ipvsadm --start-daemon backup --syncid 10 --sync-maxlen 1460 --mcast-interface sync01 --mcast-group 224.0.0.81 --mcast-port 8848 --mcast-ttl 1 #### be ready to receive sync sync packets from remote LVS servers: iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv01 -j TEE --gateway 127.0.0.1 iptables -t mangle -A PREROUTING -d 224.0.0.81/32 -i loc02_srv02 -j TEE --gateway 127.0.0.1</code> </pre> <br><p>  En fait, chacun de nos serveurs LVS joue les deux r√¥les √† la fois (ma√Ætre et sauvegarde), d'une part, c'est juste pratique, car il √©limine le changement de r√¥le lors du changement de trafic, de l'autre il est n√©cessaire, car chaque DC traite son trafic de groupe par d√©faut VIP publics. </p><br><h4 id="pereklyuchenie-nagruzki-mezhdu-data-centrami">  Commutation de charge entre les centres de donn√©es </h4><br><p>  En fonctionnement normal, chaque adresse IP publique est annonc√©e sur Internet depuis n'importe o√π (dans ce diagramme, √† partir de deux centres de donn√©es).  Le trafic entrant vers le VIP est achemin√© vers le contr√¥leur de domaine dont nous avons besoin pour le moment en utilisant l'attribut BGP MED (Discriminateur de sortie multiple) avec diff√©rentes valeurs pour Active DC et Backup DC.  Dans le m√™me temps, Backup DC est toujours pr√™t √† accepter du trafic si quelque chose arrive √† l'actif: <br><br><img src="https://habrastorage.org/webt/ir/s6/ht/irs6htkfhrve2zzevsl0cwcg4k4.png"><br><br>  En modifiant les valeurs des BGP MED et en utilisant la synchronisation IPVS inter-emplacements, nous avons la possibilit√© de transf√©rer en toute transparence le trafic des backends d'un centre de donn√©es √† un autre, sans affecter les appels t√©l√©phoniques √©tablis, qui se termineront t√¥t ou tard naturellement.  Le processus est enti√®rement automatis√© (pour chaque VIP, nous avons un bouton dans la console de gestion), et ressemble √† ceci: </p><br><ol><li><p>  SIP-VIP est actif dans DC1 (√† gauche), le cluster dans DC2 (√† droite) est redondant, gr√¢ce √† la synchronisation ipvs, il a des informations sur les connexions √©tablies dans sa m√©moire.  A gauche, les VIP actifs sont annonc√©s avec une valeur de MED 100, √† droite - avec une valeur de 500: <br><br><img src="https://habrastorage.org/webt/v6/di/th/v6dithpguub8rbq7ggurj_vdlxi.png"></p><br></li><li><p>  Le bouton de commutation provoque une modification de la soi-disant  "Target_state" (un concept interne d√©clarant les valeurs des BGP MED √† un moment donn√©).  Ici, nous n'esp√©rons pas que DC1 est en ordre et pr√™t √† traiter le trafic, donc LVS dans DC2 entre dans l'√©tat ¬´force active¬ª, abaissant la valeur MED √† 50, et entra√Æne ainsi le trafic sur lui-m√™me.  Si les backends de DC1 sont actifs et disponibles, les appels ne seront pas interrompus.  Toutes les nouvelles connexions TCP (enregistrements) seront envoy√©es aux backends dans DC2: <br><br><img src="https://habrastorage.org/webt/lx/zo/cb/lxzocb7gt-zzrznicu5k9hiju2k.png"></p><br></li><li><p>  DC1 a re√ßu une nouvelle r√©plication target_state et a d√©fini la valeur de sauvegarde MEDs (500).  Lorsque DC2 s'en rend compte, il normalise sa valeur (50 =&gt; 100).  Il reste √† attendre la fin de tous les appels actifs dans DC1 et √† rompre les connexions TCP √©tablies.  Les instances SBC dans DC1 introduisent les services n√©cessaires dans ce que l'on appelle  √âtat ¬´arr√™t progressif¬ª: ¬´503¬ª r√©pond aux prochaines demandes SIP et se d√©connecte, mais n'accepte pas les nouvelles connexions.  De plus, ces instances entrent dans la liste noire de LVS.  Lors de la rupture, le client √©tablit une nouvelle inscription / connexion, qui vient d√©j√† dans DC2: <br><br><img src="https://habrastorage.org/webt/by/rq/uf/byrquf_81kp8zvvryrrjzdtcl-a.png"></p><br></li><li><p>  Le processus se termine lorsque tout le trafic dans DC2. <br><br><img src="https://habrastorage.org/webt/1m/dm/h8/1mdmh8styniixoof0cragfatdqq.png"></p><br></li><li><p>  DC1 et DC2 ont chang√© de r√¥le. <br><br><img src="https://habrastorage.org/webt/wu/t3/ts/wut3ts2mq5iznucnkesz2r3mnra.png"></p><br></li></ol><br><p>  Dans des conditions de charge √©lev√©e constante sur les points d'entr√©e, il s'est av√©r√© tr√®s pratique de pouvoir changer de trafic √† tout moment.  Le m√™me m√©canisme d√©marre automatiquement si le DC de sauvegarde commence soudainement √† recevoir du trafic.  Dans le m√™me temps, pour se prot√©ger contre les battements, la commutation n'est d√©clench√©e qu'une seule fois dans une direction et la serrure est r√©gl√©e sur la commutation automatique, pour l'enlever, une intervention humaine est requise. </p><br><h4 id="chto-vnutri">  √Ä l'int√©rieur </h4><br><p>  Cluster VRRP et gestionnaire IPVS: Keepalived.  Keepalived est responsable du changement de VIP au sein du cluster, ainsi que du contr√¥le de sant√© / liste noire des backends. </p><br><p>  Pile BGP: ExaBGP.  Il est responsable de l'annonce des itin√©raires vers les adresses VIP et de l'apposition des BGP MED correspondants.  Enti√®rement contr√¥l√© par le serveur de gestion.  Un d√©mon BGP fiable √©crit en Python se d√©veloppe activement, il effectue sa t√¢che √† 100%. </p><br><p>  Serveur de gestion (API / Monitoring / gestion des sous-composants): Pyro4 + Flask.  Il s'agit d'un serveur de provisionnement pour Keepalived et ExaBGP, g√®re tous les autres param√®tres syst√®me (sysctl / iptables / ipset / etc), fournit une surveillance (gnlpy), ajoute et supprime des backends √† la demande (ils communiquent avec son API). </p><br><h4 id="cifry">  Les chiffres </h4><br><p>  Une machine virtuelle √† quatre c≈ìurs Intel Xeon Gold 6140 CPU √† 2,30 GHz sert un flux de trafic de 300 Mbps / 210 Kpps (trafic multim√©dia, environ 3 000 appels simultan√©s en heure de pointe sont trait√©s par leur interm√©diaire).  Utilisation du CPU en m√™me temps - 60%. </p><br><p>  Maintenant, cela suffit pour desservir le trafic de jusqu'√† 100 000 terminaux (t√©l√©phones de bureau).  Pour desservir tout le trafic (plus d'un million de terminaux), nous construisons environ 10 paires de tels clusters dans plusieurs centres de donn√©es. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436404/">https://habr.com/ru/post/fr436404/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436392/index.html">Avantages de l'analyse des applications de niveau 7 dans les pare-feu. Partie 1. Bases</a></li>
<li><a href="../fr436394/index.html">Demis Hassabis - le grand intellect qui a cr√©√© le grand intellect</a></li>
<li><a href="../fr436396/index.html">Puis-je utiliser la programmation fonctionnelle dans mon langage?</a></li>
<li><a href="../fr436398/index.html">De l'eau</a></li>
<li><a href="../fr436400/index.html">Configurer l'environnement de d√©veloppement pour l'apprentissage du HTML, CSS, PHP dans Windows</a></li>
<li><a href="../fr436406/index.html">Comment devenir d√©veloppeur de jeux si vous √™tes un agent immobilier</a></li>
<li><a href="../fr436408/index.html">Mod√©lisation num√©rique - l'histoire d'un projet</a></li>
<li><a href="../fr436412/index.html">Visite photo du nouveau bureau Facebook de Boston</a></li>
<li><a href="../fr436416/index.html">Migration de Mongo √† Postgres: l'exp√©rience du journal The Guardian</a></li>
<li><a href="../fr436420/index.html">Le plus grand d√©potoir de l'histoire: 2,7 milliards de comptes, dont 773 millions sont uniques</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>