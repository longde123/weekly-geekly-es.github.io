<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>✋🏽 🔽 ↘️ 强化学习简介 👨‍👩‍👦‍👦 Ⓜ️ 👨🏽‍⚖️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="大家好！ 

 我们为机器学习课程开辟了新的渠道 ，因此请在不久的将来等待与此相关的文章，可以说是纪律。 好吧，当然要开研讨会。 现在让我们看看什么是强化学习。 

 强化学习是机器学习的一种重要形式，代理可以通过执行动作和查看结果来学习在环境中的行为。 

 近年来，我们在这一引人入胜的研究领域中...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>强化学习简介</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/429090/"> 大家好！ <br><br> 我们为<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">机器学习</a>课程开辟了新的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">渠道</a> ，因此请在不久的将来等待与此相关的文章，可以说是纪律。 好吧，当然要开研讨会。 现在让我们看看什么是强化学习。 <br><br> 强化学习是机器学习的一种重要形式，代理可以通过执行动作和查看结果来学习在环境中的行为。 <br><br> 近年来，我们在这一引人入胜的研究领域中取得了许多成功。 例如，2014年的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">DeepMind和Deep Q Learning Architecture</a> ，2016年的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">AlphaGo击败围棋冠军，</a> 2017年的<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">OpenAI和PPO</a>等等。 <br><br><img src="https://habrastorage.org/webt/_q/bo/5c/_qbo5cmhnkdgpvhwcipzifybbeg.png"><a name="habracut"></a><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>DeepMind DQN</i> <br><br> 在本系列文章中，我们将重点研究当今用于解决强化学习问题的不同体系结构。 这些包括Q学习，深度Q学习，策略梯度，Actor评论家和PPO。 <br><br> 在本文中，您将学习： <br><br><ul><li> 什么是强化学习以及为什么奖励是中心思想 </li><li> 三种强化学习方法 </li><li>  “深度”在深度强化学习中意味着什么 </li></ul><br> 在投入使用强化学习代理之前，掌握这些方面非常重要。 <br><br> 强化训练的思想是，代理人将通过与环境互动并从执行动作中获得奖励而从环境中学习。 <br><br><img src="https://habrastorage.org/webt/iu/8r/w1/iu8rw1wpn1ubnc5zkssp6azkous.png"><br><br> 通过与环境互动来学习是我们自然的经验。 假设您是客厅里的孩子。 您会看到壁炉并转到壁炉。 <br><br><img src="https://habrastorage.org/webt/si/fb/7j/sifb7jkorawb7spj6iivlojdgly.png"><br><br> 附近温暖，你感觉很好（积极奖励+1）。 您了解火灾是一件积极的事情。 <br><br><img src="https://habrastorage.org/webt/nw/cs/an/nwcsanbxf1aauksjabgtkpwrd6a.png"><br><br> 但是，然后您尝试着火。  ！ 他烧了手（负奖励-1）。 您刚刚意识到，距离足够远时，火是积极的，因为它会产生热量。 但是，如果您接近他，您会被烫伤。 <br><br> 这就是人们通过互动学习的方式。 强化学习只是通过行动学习的一种计算方法。 <br><br>  <b>强化学习过程</b> <b><br></b> <br><img src="https://habrastorage.org/webt/lz/0b/it/lz0bitmkpzmwkrvwyfbetdj5ho8.png"><br><br> 举例来说，假设有一位经纪人正在学习玩《超级马里奥兄弟》。 强化学习（RL）过程可以建模为一个循环，其工作方式如下： <br><br><ul><li> 代理从环境接收状态S0（在我们的示例中，我们从超级马里奥兄弟（环境）获得游戏的第一帧（状态）） </li><li> 基于此状态S0，座席执行操作A0（座席将向右移动） </li><li> 环境移至新状态S1（新框架） </li><li> 环境给R1代理人一些奖励（未死：+1） </li></ul><br> 此RL周期产生一系列<b>状态，动作和奖励。</b> <br> 代理商的目标是最大化预期的累积奖励。 <br><br>  <b>中心思想奖励假设</b> <br><br> 为什么代理商的目标是最大化预期的累积奖励？ 好吧，强化学习是基于奖励假设的思想。 可以通过最大化预期的累积奖励来描述所有目标。 <br><br>  <b>因此，在强化训练中，为了实现最佳行为，我们需要最大化预期的累积奖励。</b> <br><br> 每个时间步t的累积奖励可以写成： <br><br><img src="https://habrastorage.org/webt/zn/by/di/znbydiiglnylw5d_bsro8mj96f4.gif"><br><br> 这等效于： <br><br><img src="https://habrastorage.org/webt/ti/ku/p8/tikup8qeypsldwfqd1yqttboonm.png"><br><br> 但是，实际上，我们不能简单地增加这种奖励。 较早到达（在游戏开始时）的奖励，因为它们比将来的奖励更可预测。 <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br> 假设您的代理人是一只小老鼠，而您的对手是一只猫。 您的目标是在猫吃掉您之前最多吃奶酪。 正如我们在图中所看到的，鼠标比猫附近的奶酪更有可能在自己旁边吃奶酪（距离我们越近，它就越危险）。 <br><br> 结果，即使更大（更多奶酪），猫的奖励也会减少。 我们不确定是否可以吃它。 为了减少报酬，我们执行以下操作： <br><br><ul><li> 我们确定称为伽玛的折现率。 它应该在0到1之间。 </li><li> 伽玛值越大，折扣越低。 这意味着学习代理人更关心长期奖励。 </li><li> 另一方面，伽玛值越小，折扣越大。 这意味着优先考虑短期奖励（最近的奶酪）。 </li></ul><br> 考虑到折现，累计预期对价如下： <br><br><img src="https://habrastorage.org/webt/1o/j-/i9/1oj-i95d4zwuj9wgt1vjpp5-qbc.png"><br><br> 粗略地说，将使用时间指示器的伽马来减少每种奖励。 随着时间步长的增加，猫离我们越来越近，因此未来获得奖励的可能性越来越小。 <br><br>  <b>偶尔或连续的任务</b> <br><br> 任务是强化学习问题的一个实例。 我们可以有两种任务：情景任务和连续任务。 <br><br>  <b>情景任务</b> <br><br> 在这种情况下，我们有一个起点和一个终点<b>（终端状态）。</b>  <b>这会创建一个情节</b> ：状态，动作，奖励和新状态的列表。 <br> 以《超级马里奥兄弟》为例：该情节从新马里奥的发布开始，到您被杀死或达到关卡结束时结束。 <br><br><img src="https://habrastorage.org/webt/w8/jr/dk/w8jrdkdy31kbnbkpg5g1pbn6luw.png"><br>  <i>新剧集的开始</i> <br><br>  <b>连续任务</b> <br><br>  <b>这些是永远进行的任务（没有终端状态）</b> 。 在这种情况下，代理必须学会选择最佳操作，并同时与环境互动。 <br><br> 例如，执行自动股票交易的代理。 此任务没有起点和终点状态。  <b>代理将继续工作，直到我们决定停止他为止。</b> <br><br><img src="https://habrastorage.org/webt/uk/ih/ul/ukihulbmt8ffshfb4pkhssvwvsk.jpeg"><br><br>  <b>蒙特卡洛与时差法</b> <br><br> 有两种学习方法： <br><br><ul><li> 在剧集结束时收集奖励，然后计算最大预期未来奖励-蒙特卡洛方法 </li><li> 评估奖励的每一步-暂时的差异 </li></ul><br>  <b>蒙特卡洛</b> <br><br> 当情节结束时（特工达到“终极状态”），特工查看累积的总奖励以查看他的表现如何。 在蒙特卡洛方法中，仅在游戏结束时才收到奖励。 <br><br> 然后，我们用增强的知识开始新的游戏。  <b>代理在每次迭代时都会做出最佳决策。</b> <br><br><img src="https://habrastorage.org/webt/m0/gh/l7/m0ghl7rlxmsvdmtdgtrjhd8kpcc.png"><br><br> 这是一个例子： <br><br><img src="https://habrastorage.org/webt/q3/pj/pg/q3pjpgo4x-blfbjxkrd2qtlhefw.png"><br><br> 如果我们以迷宫为环境： <br><br><ul><li> 我们总是从相同的起点开始。 </li><li> 如果猫吃了我们，或者我们移动了20步，我们将停止该情节。 </li><li> 在剧集的结尾，我们有状态，动作，奖励和新状态的列表。 </li><li> 代理人汇总了Gt的总奖励（以查看他的表现如何）。 </li><li> 然后根据上述公式更新V（st）。 </li><li> 然后，新游戏将以新知识开始。 </li></ul><br> 通过运行越来越多的剧集， <b>特工将学会玩得</b>越来越<b>好。</b> <br><br>  <b>时差：每步学习</b> <br><br> 时间差异学习（TD）方法不会等待情节结束来更新可能的最高奖励。 他将根据获得的经验更新V。 <br><br> 此方法称为TD（0）或<b>逐步TD（在任何单个步骤后更新实用程序功能）。</b> <br><br><img src="https://habrastorage.org/webt/pw/03/xt/pw03xtwh65lmrhsqffyz1nr70lu.png"><br><br>  TD方法仅期望下一个<b>步骤更新值。</b> 在时间t +1处<b>，使用奖励Rt +1和当前额定值V（St +1）形成TD目标。</b> <br><br>  TD目标是对期望值的估计：实际上，您可以在一步之内将先前的V（St）额定值更新为目标。 <br><br>  <b>损害探索/运营</b> <br><br> 在考虑解决强化训练问题的各种策略之前，我们必须考虑另一个非常重要的主题：勘探与开发之间的权衡。 <br><br><ul><li> 情报可以找到有关环境的更多信息。 </li><li> 剥削利用已知信息来最大化回报。 </li></ul><br> 请记住，我们的RL代理商的目标是最大化预期的累积奖励。 但是，我们可能会陷入一个普遍的陷阱。 <br><br><img src="https://habrastorage.org/webt/vt/xr/s7/vtxrs70s3g0ryq_bzn0tenuw4ke.png"><br><br> 在这个游戏中，我们的鼠标可以装无限数量的小块奶酪（每个+1）。 但是在迷宫的顶部有一块巨大的奶酪（+1000）。 但是，如果我们只专注于奖励，那么我们的代理商将永远无法达成巨大的任务。 相反，他将仅使用最接近的奖励来源，即使该来源很小（开发）。 但是，如果我们的经纪人稍加调情，他将能够找到丰厚的回报。 <br><br> 这就是我们所谓的勘探与开发之间的折衷。 我们必须定义一条规则，以帮助应对这种妥协。 在以后的文章中，您将学习执行此操作的不同方法。 <br><br>  <b>三种强化学习方法</b> <br><br> 现在，我们已经确定了强化学习的主要元素，让我们继续三种解决强化学习的方法：基于成本，基于策略和基于模型。 <br><br>  <b>根据费用</b> <br><br> 在基于成本的RL中，目标是优化效用函数V（s）。 <br> 效用函数是一种功能，它可以告知我们代理在每种状态下将获得的最大预期奖励。 <br><br> 每个状态的值是代理从该状态开始可以预期在将来累积的奖励总额。 <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br> 代理将使用此实用程序功能来决定在每个步骤中选择哪个状态。 代理选择具有最高值的状态。 <br><br><img src="https://habrastorage.org/webt/xf/am/qe/xfamqeqhethtlspy9rxfj_srzua.png"><br><br> 在迷宫式示例中，在每个步骤中，我们将采用最高值：-7，然后是-6，然后是-5（以此类推）以实现目标。 <br><br>  <b>基于政策</b> <br><br> 在基于策略的RL中，我们希望不使用效用函数直接优化π（s）策略函数。 策略是确定代理在给定时间点的行为的因素。 <br><br><img src="https://habrastorage.org/webt/8o/eh/rn/8oehrnzytpqnnbiryj-s_ohfns4.png"><br>  <i>行动=政策（状态）</i> <br> 我们研究政治的功能。 这使我们能够将每个状态与最适当的操作相关联。 <br><br> 有两种类型的策略： <br><br><ul><li> 确定性的：给定状态下的政治总是会返回相同的动作。 </li><li> 随机：显示按动作分布的概率。 </li></ul><br><img src="https://habrastorage.org/webt/u-/mc/kt/u-mckt0bhgwx8-ziyv-rwi1e_u8.png"><br><br><img src="https://habrastorage.org/webt/9n/b3/zf/9nb3zfpdt6_ukndkwsuza8t9iva.png"><br><br> 如您所见，该策略直接指示每个步骤的最佳操作。 <br><br>  <b>基于模型</b> <br><br> 在基于模型的RL中，我们对环境进行建模。 这意味着我们正在创建一个环境行为模型。 问题在于每种环境都需要模型的不同视图。 这就是为什么我们在以下文章中不会过多地关注此类培训。 <br><br>  <b>引入深度强化学习</b> <br><br> 深度强化学习引入了深度神经网络来解决强化学习的问题，因此被称为“深度”。 <br> 例如，在下一篇文章中，我们将研究Q学习（经典强化学习）和深度Q学习。 <br><br> 您将看到以下事实的差异，在第一种方法中，我们使用传统算法创建Q表，这有助于我们找到针对每种状态采取的操作。 <br><br> 在第二种方法中，我们将使用神经网络（以近似基于状态的奖励：q值）。 <br><br><img src="https://habrastorage.org/webt/bf/yn/5u/bfyn5uyg0bpq-wd-ab4-tgzj3b8.png"><br>  <i>Udacity启发的Q设计图</i> <i><br></i> <br><br> 仅此而已。 与往常一样，我们在这里等待您的评论或问题，或者您可以在<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">开放式</a>网络课程中向课程老师<a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">Arthur Kadurin提问</a> 。 </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN429090/">https://habr.com/ru/post/zh-CN429090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN429078/index.html">如何用不到100行代码来创建程序艺术</a></li>
<li><a href="../zh-CN429082/index.html">泰国没有刻板印象</a></li>
<li><a href="../zh-CN429084/index.html">电烤箱“哈尔科夫”的第二人生</a></li>
<li><a href="../zh-CN429086/index.html">备用啤酒派对</a></li>
<li><a href="../zh-CN429088/index.html">使用OdataToEntity运行GraphQL查询</a></li>
<li><a href="../zh-CN429092/index.html">为什么太空中仍然存在隐形</a></li>
<li><a href="../zh-CN429094/index.html">定向声音：可以代替耳机的技术-工作原理</a></li>
<li><a href="../zh-CN429096/index.html">上古：ZX Spectrum，卡带程序和高清晰度</a></li>
<li><a href="../zh-CN429102/index.html">加拿大的电动汽车销售</a></li>
<li><a href="../zh-CN429104/index.html">高数据</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>