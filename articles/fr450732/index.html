<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ üåù üíç Je vois, cela signifie que j'existe: une revue du Deep Learning in Computer Vision (partie 1) üñêüèΩ üö≥ üë©üèª‚Äçüíª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vision par ordinateur. Maintenant, ils en parlent beaucoup, o√π il est appliqu√© et mis en ≈ìuvre beaucoup. Et il y a quelque temps, il n'y avait pas d'a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Je vois, cela signifie que j'existe: une revue du Deep Learning in Computer Vision (partie 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Vision par ordinateur.  Maintenant, ils en parlent beaucoup, o√π il est appliqu√© et mis en ≈ìuvre beaucoup.  Et il y a quelque temps, il n'y avait pas d'articles de synth√®se sur Habr√© sur CV, avec des exemples d'architectures et de t√¢ches modernes.  Mais il y en a beaucoup et ils sont vraiment cool!  Si vous √™tes int√©ress√© par ce qui se passe actuellement en vision par ordinateur, non seulement du point de vue de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherche et des articles</a> , mais aussi du point de vue des probl√®mes appliqu√©s, vous √™tes les bienvenus chez cat.  De plus, l'article peut √™tre une bonne introduction pour ceux qui ont longtemps voulu commencer √† comprendre tout cela, mais quelque chose √©tait sur le chemin;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="image"><br><a name="habracut"></a><br>  Aujourd'hui, au PhysTech, il y a une collaboration active de l '"Acad√©mie" et des partenaires industriels.  En particulier, il existe de nombreux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">laboratoires int√©ressants</a> de soci√©t√©s telles que Sberbank, Biocad, 1C, Tinkoff, MTS, Huawei √† la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PhysTech School of Applied Mathematics and Computer Science</a> . <br><br>  J'ai √©t√© inspir√© pour √©crire cet article en travaillant dans le laboratoire des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">syst√®mes intelligents hybrides</a> , ouvert par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VkusVill</a> .  Le laboratoire a une t√¢che ambitieuse: construire un magasin qui fonctionne sans caisses, principalement √† l'aide de la vision par ordinateur.  Pendant pr√®s d'un an de travail, j'ai eu l'occasion de travailler sur de nombreuses t√¢ches de vision, qui seront discut√©es dans ces deux parties. <br><br><div class="spoiler">  <b class="spoiler_title">Acheter sans caisses?</b>  <b class="spoiler_title">Quelque part, je l'ai d√©j√† entendu ..</b> <div class="spoiler_text">  Cher lecteur, vous avez probablement pens√© √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Amazon Go</a> .  Dans un sens, la t√¢che consiste √† r√©p√©ter leur succ√®s, mais notre d√©cision est plus sur la mise en ≈ìuvre que sur la construction d'un tel magasin √† partir de z√©ro pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">beaucoup d'argent</a> . <br></div></div><br>  Nous nous d√©placerons selon le plan: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La motivation et ce qui se passe</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La classification comme mode de vie</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Architectures de r√©seaux de neurones convolutifs: 1000 fa√ßons d'atteindre un objectif</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Visualisation des r√©seaux de neurones convolutionnels: montrez-moi la passion</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Je suis moi-m√™me une sorte de chirurgien: on extrait les traits des r√©seaux de neurones</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Restez proches: l'apprentissage de la repr√©sentation pour les personnes et les individus</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: <s>d√©tecter, √©valuer la posture et reconna√Ætre les actions</s> sans spoilers</a> </li></ol><br><a name="1"></a><h2>  La motivation et ce qui se passe </h2><br><div class="spoiler">  <b class="spoiler_title">√Ä qui s'adresse l'article?</b> <div class="spoiler_text">  L'article se concentre davantage sur les personnes qui connaissent d√©j√† l'apprentissage automatique et les r√©seaux de neurones.  Cependant, je vous conseille de lire au moins les deux premi√®res sections - du coup tout sera clair :) <br></div></div><br>  En 2019, tout le monde parle d'intelligence artificielle, de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quatri√®me r√©volution industrielle</a> et de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">approche de l'humanit√© √† une singularit√©</a> .  Cool, cool, mais je veux des d√©tails.  Apr√®s tout, nous sommes des techniciens curieux qui ne croient pas aux contes de f√©es sur l'IA, nous croyons au cadre formel des t√¢ches, des math√©matiques et de la programmation.  Dans cet article, nous parlerons de cas sp√©cifiques d'utilisation de l'IA tr√®s moderne - l'utilisation de l'apprentissage en profondeur (√† savoir, les r√©seaux de neurones convolutionnels) dans une vari√©t√© de t√¢ches de vision par ordinateur. <br><br>  Oui, nous parlerons sp√©cifiquement des grilles, en mentionnant parfois quelques id√©es d'un point de vue "classique" (nous appellerons l'ensemble des m√©thodes en vision qui √©taient utilis√©es avant les r√©seaux de neurones, mais cela ne signifie nullement qu'elles ne sont pas utilis√©es maintenant). <br><br><div class="spoiler">  <b class="spoiler_title">Je veux apprendre la vision par ordinateur √† partir de z√©ro</b> <div class="spoiler_text">  Je recommande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le cours d'Anton Konushin "Introduction √† la vision par ordinateur"</a> .  Personnellement, je suis pass√© par son homologue au SHAD, qui a jet√© des bases solides dans la compr√©hension du traitement d'image et de vid√©o. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="image" width="300"></div><br>  √Ä mon avis, la premi√®re application vraiment int√©ressante des r√©seaux de neurones en vision, qui a √©t√© couverte dans les m√©dias en 1993, est la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconnaissance de l'√©criture manuscrite par</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jan LeCun</a> .  Maintenant, il est l'un des principaux IA de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Facebook AI Research</a> , leur √©quipe a d√©j√† publi√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">beaucoup de choses utiles en Open Source</a> . <br><br>  Aujourd'hui, la vision est utilis√©e dans de nombreux domaines.  Je ne donnerai que quelques exemples frappants: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="image" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="image" width="400" height="300"></div><br><br>  <i>V√©hicules sans pilote <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tesla</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="image" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Analyse d'imagerie m√©dicale</a> et <a href="">pr√©diction du cancer</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="image" width="500"></div><br><br>  <i>Consoles de jeu: Kinect 2.0 (bien qu'il utilise √©galement des informations de profondeur, c'est-√†-dire des images RGB-D)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="image" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Reconnaissance <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">faciale</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apple FaceID</a> (utilisant plusieurs capteurs)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="image" width="400"></div><br><br>  <i>Cote du visage: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">masques Snapchat</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="image" width="400"></div><br><br>  <i>Biom√©trie des mouvements du visage et des yeux (exemple du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="image" width="400"></div><br><br>  <i>Recherche par image: Yandex et Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="image" width="500"></div><br><br>  <i>Reconnaissance du texte dans l'image ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconnaissance optique des caract√®res</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="image" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="image" width="400"></div><br><br>  <i>Drones et robots: recevoir et traiter des informations par la vision</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="image" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Odom√©trie</a> : cr√©ation d'une carte et planification lors du d√©placement de robots</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="image" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Am√©lioration des graphismes et des textures dans les jeux vid√©o</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="image" width="200" height="300"></div><br><br>  <i>Traduction d'images: Yandex et Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="image" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="image" width="500"></div><br><br>  <i>R√©alit√© augment√©e: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Leap Motion (Project North Star)</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="image" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Transfert de style et de texture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PicsArt</a></i> <br><br>  Sans oublier les nombreuses applications dans diverses t√¢ches internes des entreprises.  Facebook, par exemple, utilise √©galement la vision pour filtrer le contenu multim√©dia.  Les m√©thodes de vision par ordinateur sont √©galement utilis√©es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans les tests de qualit√© / dommages dans l'industrie</a> . <br><br>  En fait, la r√©alit√© augment√©e doit faire l'objet d'une attention particuli√®re, car <s>elle ne fonctionne pas</s> dans un avenir proche, cela peut devenir l'un des principaux domaines d'application de la vision. <br><br>  Motiv√©.  Charg√©.  C'est parti: <br><br><a name="2"></a><h2>  La classification comme mode de vie </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="image"><br><br>  Comme je l'ai dit, dans les ann√©es 90, les filets ont √©t√© tir√©s √† vue.  Et ils ont tourn√© dans une t√¢che sp√©cifique - la t√¢che de classer les images de nombres manuscrits (le c√©l√®bre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de donn√©es MNIST</a> ).  Historiquement, c'√©tait la t√¢che de classer les images qui √©tait devenue la base pour r√©soudre presque toutes les t√¢ches ult√©rieures en vision.  Prenons un exemple sp√©cifique: <br><br>  <b>T√¢che</b> : Un dossier avec des photos est donn√© √† l'entr√©e, chaque photo a un objet particulier: soit un chat, soit un chien, soit une personne (m√™me s'il n'y a pas de photos "poubelles", c'est une t√¢che super-non vitale, mais il faut commencer quelque part).  Vous devez d√©composer les images dans trois dossiers: <code>/cats</code> , <code>/dogs</code> et <s><code>/leather_bags</code></s> <code>/humans</code> , en ne pla√ßant que des photos avec les objets correspondants dans chaque dossier. <br><br><div class="spoiler">  <b class="spoiler_title">Qu'est-ce qu'une image / photo?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="image"><br>  Presque partout dans la vision, il est habituel de travailler avec des images au format RVB.  Chaque image a une hauteur (H), une largeur (W) et une profondeur de 3 (couleurs).  Ainsi, une image peut √™tre repr√©sent√©e comme un tenseur de dimension HxWx3 (chaque pixel est un ensemble de trois nombres - valeurs d'intensit√© dans les canaux). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Imaginez que nous ne connaissions pas encore la vision par ordinateur, mais nous connaissons l'apprentissage automatique.  Les images sont simplement des tenseurs num√©riques dans la m√©moire de l'ordinateur.  Nous formalisons le probl√®me en termes d'apprentissage automatique: les objets sont des images, leurs signes sont des valeurs en pixels, la r√©ponse pour chacun des objets est une √©tiquette de classe (chat, chien ou personne).  Il s'agit d'une pure <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">t√¢che de classification</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Si maintenant c'est devenu difficile ..</b> <div class="spoiler_text">  ... alors il est pr√©f√©rable de lire d'abord les 4 premiers articles du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cours ouvert OpenDataScience ML</a> et de lire un article plus introductif sur la vision, par exemple, une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bonne conf√©rence dans Small ShAD</a> . <br></div></div><br>  Vous pouvez prendre certaines m√©thodes de la vue ¬´classique¬ª ou de l'apprentissage automatique ¬´classique¬ª, c'est-√†-dire pas d'un r√©seau de neurones.  Fondamentalement, ces m√©thodes consistent √† mettre en √©vidence sur les images de certaines fonctionnalit√©s (points sp√©ciaux) ou des r√©gions locales qui caract√©riseront l'image (¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sac de mots visuels</a> ¬ª).  Habituellement, tout se r√©sume √† quelque chose comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SVM</a> sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> . <br><br>  Mais nous nous sommes r√©unis ici pour parler des r√©seaux de neurones, donc nous ne voulons pas utiliser les signes que nous avons invent√©s, mais voulons que le r√©seau fasse tout pour nous.  Notre classificateur prendra les signes d'un objet en entr√©e et renverra une pr√©diction (√©tiquette de classe).  Ici, les valeurs d'intensit√© en pixels agissent comme des signes (voir le mod√®le d'image dans <br>  spoiler ci-dessus).  N'oubliez pas qu'une image est un tenseur de taille (Hauteur, Largeur, 3) (s'il s'agit de couleur).  Lorsque vous apprenez √† entrer dans la grille, tout cela n'est g√©n√©ralement pas servi par une image et non par un ensemble de donn√©es entier, mais par lots, c'est-√†-dire  dans de petites portions d'objets (par exemple, 64 images dans le lot). <br><br>  Ainsi, le r√©seau re√ßoit un tenseur d'entr√©e de taille (BATCH_SIZE, H, W, 3).  Vous pouvez ¬´agrandir¬ª chaque image dans une ligne vectorielle de nombres H * W * 3 et travailler avec les valeurs en pixels comme avec les signes dans l'apprentissage automatique, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Perceptron multicouche (MLP)</a> normal ferait exactement cela, mais honn√™tement, c'est comme √ßa ligne de base, car travailler avec des pixels comme une ligne vectorielle ne prend pas en compte, par exemple, l'invariance translationnelle des objets dans l'image.  Le m√™me chat peut √™tre au milieu de la photo et dans le coin, MLP n'apprendra pas ce sch√©ma. <br><br>  Vous avez donc besoin de quelque chose de plus intelligent, par exemple, une op√©ration de convolution.  Et cela concerne la vision moderne, <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les r√©seaux de neurones convolutifs</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">Le code de formation du r√©seau de convolution peut ressembler √† ceci (sur le framework PyTorch)</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Puisque nous parlons maintenant de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation avec un enseignant</a> , nous avons besoin de plusieurs composants pour former un r√©seau de neurones: <br><br><ul><li>  Donn√©es (existe d√©j√†) </li><li>  Architecture de r√©seau (mise en √©vidence) </li><li>  Une fonction de perte qui dira comment le r√©seau neuronal apprendra (ici ce sera l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entropie crois√©e</a> ) </li><li>  M√©thode d'optimisation (changera le poids du r√©seau dans la bonne direction) </li><li>  D√©finir l'architecture et les hyperparam√®tres de l'optimiseur (par exemple, la taille des pas de l'optimiseur, le nombre de neurones dans les couches, les coefficients de r√©gularisation) </li></ul><br>  C'est exactement ce qui est impl√©ment√© dans le code; le r√©seau neuronal convolutionnel lui-m√™me est d√©crit dans la classe Net (). <br><br>  Si vous voulez apprendre lentement et d√®s le d√©but sur les faisceaux et les r√©seaux de convolution, je recommande une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conf√©rence √† la Deep Learning School (MIPT MIPT) (en russe)</a> sur ce sujet et, bien s√ªr, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le cours de Stanford cs231n (en anglais)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - qu'est-ce que c'est?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning School</a> au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Laboratoire d'Innovation FPMI MIPT</a> est une organisation qui est activement engag√©e dans le d√©veloppement d'un cours ouvert en langue russe sur les r√©seaux de neurones.  Dans l'article, je ferai r√©f√©rence √† plusieurs fois √† ces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">didacticiels vid√©o</a> . <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="image" width="650"></div><br>  En bref, l'op√©ration de convolution vous permet de trouver des motifs sur des images en fonction de leur variabilit√©.  Lorsque nous formons des r√©seaux de neurones √† convolution (eng: R√©seaux de neurones √† convolution), nous trouvons en fait des filtres de convolution (poids des neurones) qui d√©crivent bien les images, et si bien que nous pouvons d√©terminer avec pr√©cision la classe √† partir d'eux.  De nombreuses fa√ßons ont √©t√© invent√©es pour construire un tel r√©seau.  Plus que vous ne le pensez ... <br><br><a name="3"></a><h3>  Architectures de r√©seaux de neurones convolutifs: 1000 fa√ßons d'atteindre un objectif </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="image" width="500"></div><br><br>  Oui, oui, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une autre revue architecturale</a> .  Mais ici, je vais essayer de le rendre aussi pertinent que possible! <br><br>  Il y a d'abord eu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LeNet</a> , il a aid√© Jan LeCun √† reconna√Ætre les nombres en 1998.  Ce fut le premier r√©seau neuronal convolutif pour la classification.  Sa principale caract√©ristique √©tait qu'elle commen√ßait essentiellement √† utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des</a> op√©rations de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolution et de mise en commun</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="image"><br><br>  Ensuite, il y a eu une pause dans le d√©veloppement des grilles, mais le mat√©riel ne s'est pas arr√™t√©; des calculs efficaces sur GPU et <abbr title="Alg√®bre lin√©aire acc√©l√©r√©e">XLA ont √©t√© d√©velopp√©s</abbr> .  En 2012, AlexNet est apparue, elle a particip√© au concours ILSVRC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet Large-Scale Visual Recognition Challenge</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Une petite digression sur ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet a</a> √©t√© assembl√© en 2012, et un sous-ensemble de milliers d'images et de 1000 classes a √©t√© utilis√© pour le concours ILSVRC.  ImageNet compte actuellement environ 14 millions de photos et 21 841 classes (prises sur le site officiel), mais pour la comp√©tition, elles ne s√©lectionnent g√©n√©ralement qu'un sous-ensemble.  ILSVRC est alors devenu le plus grand concours annuel de classification d'images.  Soit dit en passant, nous avons r√©cemment compris comment <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">s'entra√Æner sur ImageNet en quelques minutes</a> . <br><br>  C'est sur ImageNet (en ILSVRC) de 2010 √† 2018 qu'ils ont re√ßu les r√©seaux <abbr title="Etat de l'art">SOTA</abbr> dans la classification des images.  Certes, depuis 2016, les concours de localisation, de d√©tection et de compr√©hension de la sc√®ne, plut√¥t que de classification, sont plus pertinents. <br></div></div><br>  En r√®gle g√©n√©rale, diverses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">revues d'architecture</a> mettent en lumi√®re celles qui ont √©t√© les premi√®res √† l'ILSVRC de 2010 √† 2016 et sur certains r√©seaux individuels.  Afin de ne pas encombrer l'histoire, je les ai plac√©s sous le spoiler ci-dessous, en essayant de souligner les id√©es principales: <br><br><div class="spoiler">  <b class="spoiler_title">Architecture de 2012 √† 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ann√©e </th><th>  Article </th><th>  Id√©e cl√© </th><th>  Le poids </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alexnet</a> </td><td>  utilisez deux faisceaux d'affil√©e;  diviser la formation r√©seau en deux branches parall√®les </td><td>  240 Mo </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Zfnet</a> </td><td>  taille de filtre, nombre de filtres dans les couches </td><td>  - </td></tr><tr><td>  2013 </td><td>  <a href="">Surpasser</a> </td><td>  l'un des premiers d√©tecteurs de r√©seau neuronal </td><td>  - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  profondeur du r√©seau (13-19 couches), utilisation de plusieurs blocs Conv-Conv-Pool avec une taille de convolution plus petite (3x3) </td><td>  549 Mo (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Inception (v1) (alias GoogLeNet)</a> </td><td>  Convolution 1x1 (id√©e du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Network-in-Network</a> ), pertes auxiliaires (ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">supervision profonde</a> ), empilement des sorties de plusieurs convolutions (Inception-block) </td><td>  - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">connexions r√©siduelles</a> , tr√®s profondes (152 couches ..) </td><td>  98 Mo (ResNet-50), 232 Mo (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  Les id√©es de toutes ces architectures (√† l'exception de ZFNet, il est g√©n√©ralement peu mentionn√©) √©taient √† une √©poque un nouveau mot dans les r√©seaux de neurones pour la vision.  Cependant, apr√®s 2015, il y a eu de nombreuses autres am√©liorations importantes, par exemple, Inception-ResNet, Xception, DenseNet, SENet.  Ci-dessous, j'ai essay√© de les rassembler en un seul endroit. <br><br><div class="spoiler">  <b class="spoiler_title">Architecture de 2015 √† 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ann√©e </th><th>  Article </th><th>  Id√©e cl√© </th><th>  Le poids </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Inception v2 et v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d√©composition des packages en packages 1xN et Nx1</a> </td><td>  92 Mo </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Inception v4 et Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">combinaison de Inception et ResNet</a> </td><td>  215 Mo </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Resnext</a> </td><td>  2√®me place ILSVRC, l'utilisation de nombreuses branches (bloc Inception ¬´g√©n√©ralis√©¬ª) </td><td>  - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolution s√©parable en profondeur</a> , p√®se moins avec une pr√©cision comparable √† Inception </td><td>  88 Mo </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bloc dense</a>  l√©ger mais pr√©cis </td><td>  33 Mo (DenseNet-121), 80 Mo (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bloc de compression et d'excitation</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">46 Mo (SENet-Inception), 440 Mo (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  La plupart de ces mod√®les pour PyTorch peuvent √™tre trouv√©s <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , et il y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une telle chose cool</a> . <br><br>  Vous avez peut-√™tre remarqu√© que tout cela p√®se beaucoup (je voudrais 20 Mo maximum, voire moins), alors qu'aujourd'hui ils utilisent des appareils mobiles partout et que l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IoT</a> gagne en popularit√©, ce qui signifie que vous souhaitez √©galement utiliser des grilles l√†-bas. <br><br><div class="spoiler">  <b class="spoiler_title">Relation entre le poids et la vitesse du mod√®le</b> <div class="spoiler_text">  √âtant donn√© que les r√©seaux de neurones en eux-m√™mes ne multiplient que les tenseurs, le nombre d'op√©rations de multiplication (lire: le nombre de poids) affecte directement la vitesse de travail (si le post-traitement ou le pr√©traitement √† forte intensit√© de main-d'≈ìuvre n'est pas utilis√©).  La vitesse du r√©seau lui-m√™me d√©pend de l'impl√©mentation (framework), du mat√©riel sur lequel il s'ex√©cute et de la taille de l'image d'entr√©e. <br></div></div><br>  Les auteurs de nombreux articles ont pris le chemin d'inventer des architectures rapides, j'ai rassembl√© leurs m√©thodes sous le spoiler ci-dessous: <br><br><div class="spoiler">  <b class="spoiler_title">Architecture l√©g√®re de CNN</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Ann√©e </th><th>  Article </th><th>  Id√©e cl√© </th><th>  Le poids </th><th>  Exemple d'impl√©mentation </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Compression FireModule</a> </td><td>  0,5 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">obtenu par une recherche neuronale d'architectures, il s'agit d'un r√©seau de la cat√©gorie AutoML</a> </td><td>  23 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conv. de groupe point par point, lecture al√©atoire des canaux</a> </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolutions s√©parables en profondeur et de nombreuses autres astuces</a> </td><td>  16 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Je recommande cet article sur Habr√©</a> </td><td>  14 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Squeezenext</a> </td><td>  voir les photos dans le r√©f√©rentiel d'origine </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">architecture neuronale recherche sp√©cifiquement pour les appareils mobiles utilisant RL</a> </td><td>  ~ 2 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v3)</a> </td><td>  elle est sortie pendant que j'√©crivais un article :) </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Les nombres dans toutes les tables <s>sont tir√©s du plafond</s> des r√©f√©rentiels, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la table Keras Applications</a> et de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . <br><br>  Vous demandez: ¬´Pourquoi avez-vous √©crit sur tout ce¬´ zoo ¬ªde mod√®les?  Et pourquoi est la t√¢che de la classification?  Mais nous voulons apprendre aux machines √† voir, et la classification n'est qu'une sorte de t√¢che √©troite. ¬ª.  Le fait est que les r√©seaux de neurones pour d√©tecter les objets, √©valuer les postures / points, r√©-identifier et rechercher dans une image utilisent exactement les mod√®les pour la classification en tant que <b><abbr title="base, litt√©ralement - la colonne vert√©brale">colonne vert√©brale</abbr></b> , et 80% du succ√®s en d√©pend. <br><br>  Mais je veux en quelque sorte faire davantage confiance √† CNN, ou ils ont imagin√© des bo√Ætes noires, mais ce qui est "√† l'int√©rieur" n'est pas √©vident.  Pour mieux comprendre le m√©canisme de fonctionnement des r√©seaux convolutifs, les chercheurs ont imagin√© l'utilisation de la visualisation. <br><br><a name="4"></a><h3>  Visualisation des r√©seaux de neurones convolutionnels: montrez-moi la passion </h3><br>  Une √©tape importante vers la compr√©hension de ce qui se passe √† l'int√©rieur des r√©seaux convolutionnels est l'article <a href="">¬´Visualiser et comprendre les r√©seaux convolutionnels¬ª</a> .  Dans ce document, les auteurs ont propos√© plusieurs fa√ßons de visualiser exactement √† quoi (sur quelles parties de l'image) les neurones dans diff√©rentes couches CNN r√©pondent (je recommande √©galement de regarder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une conf√©rence de Stanford sur ce sujet</a> ).  Les r√©sultats √©taient tr√®s impressionnants: les auteurs ont montr√© que les premi√®res couches du r√©seau convolutionnel r√©pondent √† certaines ¬´choses de bas niveau¬ª par le type de bords / angles / lignes, et les derni√®res couches r√©pondent d√©j√† √† des parties enti√®res des images (voir l'image ci-dessous), c'est-√†-dire qu'elles portent d√©j√† en soi une s√©mantique. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="image"></div><br><br>  De plus, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet de visualisation en profondeur de l'Universit√© Cornell et de la soci√©t√© a</a> avanc√© la visualisation encore plus loin, tandis que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">c√©l√®bre DeepDream a</a> appris √† d√©former dans un style int√©ressant et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">addictif</a> (ci-dessous, une image de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="image" width="500"></div><br><br>  En 2017, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tr√®s bon article a √©t√© publi√© sur Distill</a> , dans lequel ils ont effectu√© une analyse d√©taill√©e de ce que chaque couche ¬´voit¬ª, et plus r√©cemment (en mars 2019), Google a invent√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des atlas d'activation</a> : des cartes uniques qui peuvent √™tre construites pour chaque couche r√©seau, ce qui nous rapproche de la compr√©hension de l'image globale du travail de CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Si vous voulez jouer avec la visualisation vous-m√™me, je recommanderais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lucid</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TensorSpace</a> . <br><br>  D'accord, CNN semble √™tre vrai dans une certaine mesure.  Nous devons apprendre √† l'utiliser dans d'autres t√¢ches, et pas seulement dans la classification.  Cela nous aidera √† extraire les images Embedding'ov et √† transf√©rer l'apprentissage. <br><br><a name="5"></a><h2>  Je suis moi-m√™me une sorte de chirurgien: on extrait les traits des r√©seaux de neurones </h2><br>  Imaginez qu'il y ait une image, et nous voulons trouver celles qui lui ressemblent visuellement (c'est, par exemple, la recherche dans une image dans Yandex.Pictures).  Auparavant (avant les r√©seaux de neurones), les ing√©nieurs extrayaient manuellement des fonctionnalit√©s pour cela, par exemple, inventant quelque chose qui d√©crit bien l'image et permet de la comparer avec d'autres.  Fondamentalement, ces m√©thodes ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> ) fonctionnent avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des d√©grad√©s d'image</a> , g√©n√©ralement ces choses sont appel√©es descripteurs d'image ¬´classiques¬ª.  D'un int√©r√™t particulier, je me r√©f√®re √† l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> et au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cours d'Anton Konushin</a> (ce n'est pas de la publicit√©, juste un bon cours :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="image" width="500"></div><br><br>  En utilisant des r√©seaux de neurones, nous ne pouvons pas inventer nous-m√™mes ces caract√©ristiques et ces heuristiques, mais former correctement le mod√®le, puis <b>prendre la sortie d'une ou plusieurs couches du r√©seau comme signes de l'image</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="image" width="650"></div><br>  Un examen plus approfondi de toutes les architectures montre clairement qu'il y a deux √©tapes de classification dans CNN: <br>  1).  Couches d'extraction de fonctionnalit√©s pour extraire des fonctionnalit√©s informatives √† partir d'images √† l'aide de couches convolutionnelles <br>  2).  Apprendre en plus de ces fonctionnalit√©s Couches de classificateur <b>enti√®rement connect√©es (FC)</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="image" width="500"></div><br><br>  <b>L'incorporation d'images (fonctionnalit√©s)</b> est √† peu pr√®s le fait que vous pouvez prendre leurs signes apr√®s l'extracteur de fonctionnalit√©s d'un r√©seau de neurones convolutionnels (bien qu'elles puissent √™tre agr√©g√©es de diff√©rentes mani√®res) comme description informative des images.  Autrement dit, nous avons form√© le r√©seau pour la classification, puis prenons simplement la sortie devant les couches de classification.  Ces signes sont appel√©s <i>caract√©ristiques</i> , <i>descripteurs de r√©seau de neurones</i> ou <i>imbrications d'</i> images (bien que les incorporations soient g√©n√©ralement accept√©es dans la PNL, car il s'agit de vision, je parlerai souvent de <i>fonctionnalit√©s</i> ).  Il s'agit g√©n√©ralement d'une sorte de vecteur num√©rique, par exemple, 128 nombres, avec lequel vous pouvez d√©j√† travailler. <br><br><div class="spoiler">  <b class="spoiler_title">Mais qu'en est-il des encodeurs automatiques?</b> <div class="spoiler_text">  Oui, en fait, les fonctionnalit√©s peuvent √™tre obtenues par des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">auto-encodeurs</a> .  Dans ma pratique, ils l'ont fait de diff√©rentes mani√®res, mais, par exemple, dans des articles sur la r√©-identification (qui seront discut√©s plus tard), plus souvent, ils prennent toujours des fonctionnalit√©s apr√®s l'extracteur, plut√¥t que de former l'auto-encodeur pour cela.  Il me semble qu'il vaut la peine de mener des exp√©riences dans les deux sens, si la question est de savoir ce qui fonctionne le mieux. <br></div></div><br>  Ainsi, le pipeline pour r√©soudre le <b>probl√®me de la recherche par image</b> peut √™tre organis√© simplement: nous ex√©cutons les images via CNN, prenons les signes des couches souhait√©es et comparons ces caract√©ristiques les unes avec les autres √† partir d'images diff√©rentes.  Par exemple, nous consid√©rons simplement la distance euclidienne de ces vecteurs. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="image" width="500"></div><br><br>  <b>Le transfert d'apprentissage</b> est une technique bien connue pour la formation efficace de r√©seaux de neurones qui sont d√©j√† form√©s sur un ensemble de donn√©es sp√©cifique pour leur t√¢che.  Souvent, ils disent √©galement que le r√©glage fin au lieu de l'apprentissage par transfert, dans les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notes de cours de Stanford cs231n,</a> ces concepts sont partag√©s, disent-ils, l'apprentissage par transfert est une id√©e g√©n√©rale et le r√©glage fin est l'une des impl√©mentations de la technique.  Ce n'est pas si important pour nous √† l'avenir, l'essentiel est de comprendre que nous pouvons simplement former le r√©seau √† bien pr√©dire sur le nouvel ensemble de donn√©es, √† partir non pas de poids al√©atoires, mais de ceux form√©s sur un grand type ImageNet.  Cela est particuli√®rement vrai lorsqu'il y a peu de donn√©es et que vous souhaitez r√©soudre le probl√®me de mani√®re qualitative. <br><br><div class="spoiler">  <b class="spoiler_title">En savoir plus sur le transfert d'apprentissage</b> <div class="spoiler_text">  <a href="">Article original</a> , mais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pourquoi lire beaucoup de texte, si vous pouvez regarder la vid√©o</a> <br></div></div><br>  Cependant, le simple fait de prendre les fonctionnalit√©s n√©cessaires et de faire une formation suppl√©mentaire de l'ensemble de donn√©es √† l'ensemble de donn√©es peut ne pas √™tre suffisant, par exemple, pour des t√¢ches de recherche de personnes / personnes / quelque chose de sp√©cifique similaires.  Les photos d'une m√™me personne visuellement peuvent parfois √™tre encore plus diff√©rentes que les photographies de personnes diff√©rentes.  Il est n√©cessaire de faire en sorte que le r√©seau souligne exactement les signes inh√©rents √† une personne / un objet, m√™me s'il nous est difficile de le faire avec nos yeux.  Bienvenue dans le monde de l' <b>apprentissage</b> de la <b>repr√©sentation</b> . <br><br><a name="6"></a><h2>  Restez proches: l'apprentissage de la repr√©sentation pour les personnes et les individus </h2><br><div class="spoiler">  <b class="spoiler_title">Note terminologique</b> <div class="spoiler_text">  Si vous lisez des articles scientifiques, il semble parfois que certains auteurs comprennent diff√©remment l'expression <b>apprentissage m√©trique</b> , et il n'y a pas de consensus sur les m√©thodes √† appeler apprentissage m√©trique et celles qui ne le sont pas.  C'est pourquoi dans cet article, j'ai d√©cid√© d'√©viter cette phrase particuli√®re et j'ai utilis√© un <b>apprentissage de la repr√©sentation</b> plus logique, certains lecteurs peuvent ne pas √™tre d'accord avec cela - je serai heureux de discuter dans les commentaires. <br></div></div><br>  Nous d√©finissons les t√¢ches: <br><br><ul><li>  <b>T√¢che 1</b> : il y a une galerie (ensemble) de photographies de visages de personnes, nous voulons que le r√©seau puisse r√©pondre selon une nouvelle photo soit avec le nom d'une personne de la galerie (soi-disant c'est √ßa), soit dit qu'il n'y a pas une telle personne dans la galerie (et, peut-√™tre, nous y ajoutons nouvelle personne) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>T√¢che 2</b> : la m√™me chose, mais nous ne travaillons pas avec des photographies de visages, mais avec un recadrage complet de personnes <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  La premi√®re t√¢che est g√©n√©ralement appel√©e <b>reconnaissance faciale</b> , la seconde - la <b>r√©-identification</b> (en abr√©g√© <i>Reid</i> ).  Je les ai combin√©s en un seul bloc, car leurs solutions utilisent aujourd'hui des id√©es similaires: afin d'apprendre des imbrications d'images efficaces qui peuvent faire face √† des situations plut√¥t difficiles, elles utilisent aujourd'hui diff√©rents types de pertes, comme, par exemple, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de triplet</a> , la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de quadruplet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de centre contrastif</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de cosinus</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Il existe encore de merveilleux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©seaux</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">siamois</a> , mais honn√™tement, je ne les ai pas utilis√©s moi-m√™me.  Soit dit en passant, non seulement la perte elle-m√™me ¬´d√©cide¬ª, mais aussi comment √©chantillonner des paires de positifs et de n√©gatifs pour elle, soulignent les auteurs de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'√©chantillonnage compte dans l'apprentissage enracin√© en profondeur</a> . <br><br>  L'essence de toutes ces pertes et r√©seaux siamois est simple - nous voulons que les images d'une classe (personne) dans l'espace latent des fonctionnalit√©s (plongements) soient ¬´proches¬ª, et de diff√©rentes classes (personnes) soient ¬´√©loign√©es¬ª.  La proximit√© est g√©n√©ralement mesur√©e comme suit: des incorporations d'images √† partir d'un r√©seau de neurones sont prises (par exemple, un vecteur de 128 nombres) et nous consid√©rons la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distance euclidienne</a> habituelle entre ces vecteurs ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la proximit√© cosinus.</a>  Comment mesurer, il vaut mieux choisir sur votre ensemble de donn√©es / t√¢che. <br><br>  Une repr√©sentation sch√©matique d'un pipeline de r√©solution de probl√®mes sur l'apprentissage de la repr√©sentation ressemble √† ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Mais pour √™tre plus pr√©cis, comme √ßa</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        ‚Äî   .   ,     ‚Äî     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="image" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="d√©tection humaine coup√©e d'une grande photographie"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> Reid ‚Äî    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> 2016 </a> . ,     ,    ‚Äî   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="image" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   ‚Äî    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TorchReid</a> .      ‚Äî   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  ,   </a> ).   ?  ‚Ä¶ <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="image" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr450732/">https://habr.com/ru/post/fr450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr450720/index.html">Comment d√©velopper une application conviviale</a></li>
<li><a href="../fr450724/index.html">Pr√©sentation de Python pour les camarades d√©passant le ¬´langage A contre V¬ª langue B "et autres pr√©jug√©s</a></li>
<li><a href="../fr450726/index.html">Cr√©ation d'un outil pour √©crire rapidement et efficacement des autotests sur Selenium</a></li>
<li><a href="../fr450728/index.html">NLog: r√®gles et filtres</a></li>
<li><a href="../fr450730/index.html">ok.tech: rencontre frontend</a></li>
<li><a href="../fr450734/index.html">Le fuzzing est une √©tape importante dans un d√©veloppement s√ªr</a></li>
<li><a href="../fr450736/index.html">¬´Isoler Internet est beaucoup plus facile et moins cher que de lui fournir un blocage externe.¬ª</a></li>
<li><a href="../fr450738/index.html">Robots dans le centre de donn√©es: comment l'intelligence artificielle peut-elle √™tre utile?</a></li>
<li><a href="../fr450740/index.html">Pied de lampe intelligent REDMOND - ajouter √† la maison intelligente</a></li>
<li><a href="../fr450744/index.html">Infrastructure cyclable de Minsk pour un expatri√© informatique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>