<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽 🌝 💍 Je vois, cela signifie que j'existe: une revue du Deep Learning in Computer Vision (partie 1) 🖐🏽 🚳 👩🏻‍💻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vision par ordinateur. Maintenant, ils en parlent beaucoup, où il est appliqué et mis en œuvre beaucoup. Et il y a quelque temps, il n'y avait pas d'a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Je vois, cela signifie que j'existe: une revue du Deep Learning in Computer Vision (partie 1)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mipt/blog/450732/">  Vision par ordinateur.  Maintenant, ils en parlent beaucoup, où il est appliqué et mis en œuvre beaucoup.  Et il y a quelque temps, il n'y avait pas d'articles de synthèse sur Habré sur CV, avec des exemples d'architectures et de tâches modernes.  Mais il y en a beaucoup et ils sont vraiment cool!  Si vous êtes intéressé par ce qui se passe actuellement en vision par ordinateur, non seulement du point de vue de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherche et des articles</a> , mais aussi du point de vue des problèmes appliqués, vous êtes les bienvenus chez cat.  De plus, l'article peut être une bonne introduction pour ceux qui ont longtemps voulu commencer à comprendre tout cela, mais quelque chose était sur le chemin;) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="image"><br><a name="habracut"></a><br>  Aujourd'hui, au PhysTech, il y a une collaboration active de l '"Académie" et des partenaires industriels.  En particulier, il existe de nombreux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">laboratoires intéressants</a> de sociétés telles que Sberbank, Biocad, 1C, Tinkoff, MTS, Huawei à la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PhysTech School of Applied Mathematics and Computer Science</a> . <br><br>  J'ai été inspiré pour écrire cet article en travaillant dans le laboratoire des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systèmes intelligents hybrides</a> , ouvert par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VkusVill</a> .  Le laboratoire a une tâche ambitieuse: construire un magasin qui fonctionne sans caisses, principalement à l'aide de la vision par ordinateur.  Pendant près d'un an de travail, j'ai eu l'occasion de travailler sur de nombreuses tâches de vision, qui seront discutées dans ces deux parties. <br><br><div class="spoiler">  <b class="spoiler_title">Acheter sans caisses?</b>  <b class="spoiler_title">Quelque part, je l'ai déjà entendu ..</b> <div class="spoiler_text">  Cher lecteur, vous avez probablement pensé à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Amazon Go</a> .  Dans un sens, la tâche consiste à répéter leur succès, mais notre décision est plus sur la mise en œuvre que sur la construction d'un tel magasin à partir de zéro pour <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">beaucoup d'argent</a> . <br></div></div><br>  Nous nous déplacerons selon le plan: <br><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La motivation et ce qui se passe</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La classification comme mode de vie</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Architectures de réseaux de neurones convolutifs: 1000 façons d'atteindre un objectif</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Visualisation des réseaux de neurones convolutionnels: montrez-moi la passion</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Je suis moi-même une sorte de chirurgien: on extrait les traits des réseaux de neurones</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Restez proches: l'apprentissage de la représentation pour les personnes et les individus</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Partie 2: <s>détecter, évaluer la posture et reconnaître les actions</s> sans spoilers</a> </li></ol><br><a name="1"></a><h2>  La motivation et ce qui se passe </h2><br><div class="spoiler">  <b class="spoiler_title">À qui s'adresse l'article?</b> <div class="spoiler_text">  L'article se concentre davantage sur les personnes qui connaissent déjà l'apprentissage automatique et les réseaux de neurones.  Cependant, je vous conseille de lire au moins les deux premières sections - du coup tout sera clair :) <br></div></div><br>  En 2019, tout le monde parle d'intelligence artificielle, de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">quatrième révolution industrielle</a> et de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">approche de l'humanité à une singularité</a> .  Cool, cool, mais je veux des détails.  Après tout, nous sommes des techniciens curieux qui ne croient pas aux contes de fées sur l'IA, nous croyons au cadre formel des tâches, des mathématiques et de la programmation.  Dans cet article, nous parlerons de cas spécifiques d'utilisation de l'IA très moderne - l'utilisation de l'apprentissage en profondeur (à savoir, les réseaux de neurones convolutionnels) dans une variété de tâches de vision par ordinateur. <br><br>  Oui, nous parlerons spécifiquement des grilles, en mentionnant parfois quelques idées d'un point de vue "classique" (nous appellerons l'ensemble des méthodes en vision qui étaient utilisées avant les réseaux de neurones, mais cela ne signifie nullement qu'elles ne sont pas utilisées maintenant). <br><br><div class="spoiler">  <b class="spoiler_title">Je veux apprendre la vision par ordinateur à partir de zéro</b> <div class="spoiler_text">  Je recommande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le cours d'Anton Konushin "Introduction à la vision par ordinateur"</a> .  Personnellement, je suis passé par son homologue au SHAD, qui a jeté des bases solides dans la compréhension du traitement d'image et de vidéo. <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="image" width="300"></div><br>  À mon avis, la première application vraiment intéressante des réseaux de neurones en vision, qui a été couverte dans les médias en 1993, est la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconnaissance de l'écriture manuscrite par</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jan LeCun</a> .  Maintenant, il est l'un des principaux IA de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Facebook AI Research</a> , leur équipe a déjà publié <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">beaucoup de choses utiles en Open Source</a> . <br><br>  Aujourd'hui, la vision est utilisée dans de nombreux domaines.  Je ne donnerai que quelques exemples frappants: <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="image" width="400"></div><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="image" width="400" height="300"></div><br><br>  <i>Véhicules sans pilote <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tesla</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Yandex</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="image" width="400"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Analyse d'imagerie médicale</a> et <a href="">prédiction du cancer</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="image" width="500"></div><br><br>  <i>Consoles de jeu: Kinect 2.0 (bien qu'il utilise également des informations de profondeur, c'est-à-dire des images RGB-D)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="image" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400"></div><br><br>  <i>Reconnaissance <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">faciale</a> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apple FaceID</a> (utilisant plusieurs capteurs)</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="image" width="400"></div><br><br>  <i>Cote du visage: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">masques Snapchat</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="image" width="400"></div><br><br>  <i>Biométrie des mouvements du visage et des yeux (exemple du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet FPMI MIPT</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" alt="image" width="400"></div><br><br>  <i>Recherche par image: Yandex et Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="image" width="500"></div><br><br>  <i>Reconnaissance du texte dans l'image ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">reconnaissance optique des caractères</a> )</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="image" width="400"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="image" width="400"></div><br><br>  <i>Drones et robots: recevoir et traiter des informations par la vision</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="image" width="500"></div><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Odométrie</a> : création d'une carte et planification lors du déplacement de robots</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" alt="image" width="500"></div><br><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Amélioration des graphismes et des textures dans les jeux vidéo</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="image" width="200" height="300"></div><br><br>  <i>Traduction d'images: Yandex et Google</i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="image" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="image" width="500"></div><br><br>  <i>Réalité augmentée: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Leap Motion (Project North Star)</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Microsoft Hololens</a></i> <br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="image" width="250" height="200"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300"></div><br><br>  <i>Transfert de style et de texture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Prisma</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PicsArt</a></i> <br><br>  Sans oublier les nombreuses applications dans diverses tâches internes des entreprises.  Facebook, par exemple, utilise également la vision pour filtrer le contenu multimédia.  Les méthodes de vision par ordinateur sont également utilisées <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans les tests de qualité / dommages dans l'industrie</a> . <br><br>  En fait, la réalité augmentée doit faire l'objet d'une attention particulière, car <s>elle ne fonctionne pas</s> dans un avenir proche, cela peut devenir l'un des principaux domaines d'application de la vision. <br><br>  Motivé.  Chargé.  C'est parti: <br><br><a name="2"></a><h2>  La classification comme mode de vie </h2><br><br><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png" alt="image"><br><br>  Comme je l'ai dit, dans les années 90, les filets ont été tirés à vue.  Et ils ont tourné dans une tâche spécifique - la tâche de classer les images de nombres manuscrits (le célèbre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ensemble de données MNIST</a> ).  Historiquement, c'était la tâche de classer les images qui était devenue la base pour résoudre presque toutes les tâches ultérieures en vision.  Prenons un exemple spécifique: <br><br>  <b>Tâche</b> : Un dossier avec des photos est donné à l'entrée, chaque photo a un objet particulier: soit un chat, soit un chien, soit une personne (même s'il n'y a pas de photos "poubelles", c'est une tâche super-non vitale, mais il faut commencer quelque part).  Vous devez décomposer les images dans trois dossiers: <code>/cats</code> , <code>/dogs</code> et <s><code>/leather_bags</code></s> <code>/humans</code> , en ne plaçant que des photos avec les objets correspondants dans chaque dossier. <br><br><div class="spoiler">  <b class="spoiler_title">Qu'est-ce qu'une image / photo?</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png" alt="image"><br>  Presque partout dans la vision, il est habituel de travailler avec des images au format RVB.  Chaque image a une hauteur (H), une largeur (W) et une profondeur de 3 (couleurs).  Ainsi, une image peut être représentée comme un tenseur de dimension HxWx3 (chaque pixel est un ensemble de trois nombres - valeurs d'intensité dans les canaux). <br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400"></div><br><br>  Imaginez que nous ne connaissions pas encore la vision par ordinateur, mais nous connaissons l'apprentissage automatique.  Les images sont simplement des tenseurs numériques dans la mémoire de l'ordinateur.  Nous formalisons le problème en termes d'apprentissage automatique: les objets sont des images, leurs signes sont des valeurs en pixels, la réponse pour chacun des objets est une étiquette de classe (chat, chien ou personne).  Il s'agit d'une pure <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tâche de classification</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Si maintenant c'est devenu difficile ..</b> <div class="spoiler_text">  ... alors il est préférable de lire d'abord les 4 premiers articles du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cours ouvert OpenDataScience ML</a> et de lire un article plus introductif sur la vision, par exemple, une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bonne conférence dans Small ShAD</a> . <br></div></div><br>  Vous pouvez prendre certaines méthodes de la vue «classique» ou de l'apprentissage automatique «classique», c'est-à-dire pas d'un réseau de neurones.  Fondamentalement, ces méthodes consistent à mettre en évidence sur les images de certaines fonctionnalités (points spéciaux) ou des régions locales qui caractériseront l'image (« <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sac de mots visuels</a> »).  Habituellement, tout se résume à quelque chose comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SVM</a> sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HOG</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> . <br><br>  Mais nous nous sommes réunis ici pour parler des réseaux de neurones, donc nous ne voulons pas utiliser les signes que nous avons inventés, mais voulons que le réseau fasse tout pour nous.  Notre classificateur prendra les signes d'un objet en entrée et renverra une prédiction (étiquette de classe).  Ici, les valeurs d'intensité en pixels agissent comme des signes (voir le modèle d'image dans <br>  spoiler ci-dessus).  N'oubliez pas qu'une image est un tenseur de taille (Hauteur, Largeur, 3) (s'il s'agit de couleur).  Lorsque vous apprenez à entrer dans la grille, tout cela n'est généralement pas servi par une image et non par un ensemble de données entier, mais par lots, c'est-à-dire  dans de petites portions d'objets (par exemple, 64 images dans le lot). <br><br>  Ainsi, le réseau reçoit un tenseur d'entrée de taille (BATCH_SIZE, H, W, 3).  Vous pouvez «agrandir» chaque image dans une ligne vectorielle de nombres H * W * 3 et travailler avec les valeurs en pixels comme avec les signes dans l'apprentissage automatique, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Perceptron multicouche (MLP)</a> normal ferait exactement cela, mais honnêtement, c'est comme ça ligne de base, car travailler avec des pixels comme une ligne vectorielle ne prend pas en compte, par exemple, l'invariance translationnelle des objets dans l'image.  Le même chat peut être au milieu de la photo et dans le coin, MLP n'apprendra pas ce schéma. <br><br>  Vous avez donc besoin de quelque chose de plus intelligent, par exemple, une opération de convolution.  Et cela concerne la vision moderne, <b><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les réseaux de neurones convolutifs</a></b> : <br><br><div class="spoiler">  <b class="spoiler_title">Le code de formation du réseau de convolution peut ressembler à ceci (sur le framework PyTorch)</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    : # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html import torch.nn as nn import torch.nn.functional as F import torch.optim as optim class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training')</span></span></code> </pre><br></div></div><br>  Puisque nous parlons maintenant de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">formation avec un enseignant</a> , nous avons besoin de plusieurs composants pour former un réseau de neurones: <br><br><ul><li>  Données (existe déjà) </li><li>  Architecture de réseau (mise en évidence) </li><li>  Une fonction de perte qui dira comment le réseau neuronal apprendra (ici ce sera l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">entropie croisée</a> ) </li><li>  Méthode d'optimisation (changera le poids du réseau dans la bonne direction) </li><li>  Définir l'architecture et les hyperparamètres de l'optimiseur (par exemple, la taille des pas de l'optimiseur, le nombre de neurones dans les couches, les coefficients de régularisation) </li></ul><br>  C'est exactement ce qui est implémenté dans le code; le réseau neuronal convolutionnel lui-même est décrit dans la classe Net (). <br><br>  Si vous voulez apprendre lentement et dès le début sur les faisceaux et les réseaux de convolution, je recommande une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conférence à la Deep Learning School (MIPT MIPT) (en russe)</a> sur ce sujet et, bien sûr, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le cours de Stanford cs231n (en anglais)</a> . <br><br><div class="spoiler">  <b class="spoiler_title">Deep Learning School - qu'est-ce que c'est?</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning School</a> au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Laboratoire d'Innovation FPMI MIPT</a> est une organisation qui est activement engagée dans le développement d'un cours ouvert en langue russe sur les réseaux de neurones.  Dans l'article, je ferai référence à plusieurs fois à ces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">didacticiels vidéo</a> . <br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="image" width="650"></div><br>  En bref, l'opération de convolution vous permet de trouver des motifs sur des images en fonction de leur variabilité.  Lorsque nous formons des réseaux de neurones à convolution (eng: Réseaux de neurones à convolution), nous trouvons en fait des filtres de convolution (poids des neurones) qui décrivent bien les images, et si bien que nous pouvons déterminer avec précision la classe à partir d'eux.  De nombreuses façons ont été inventées pour construire un tel réseau.  Plus que vous ne le pensez ... <br><br><a name="3"></a><h3>  Architectures de réseaux de neurones convolutifs: 1000 façons d'atteindre un objectif </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" alt="image" width="500"></div><br><br>  Oui, oui, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une autre revue architecturale</a> .  Mais ici, je vais essayer de le rendre aussi pertinent que possible! <br><br>  Il y a d'abord eu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LeNet</a> , il a aidé Jan LeCun à reconnaître les nombres en 1998.  Ce fut le premier réseau neuronal convolutif pour la classification.  Sa principale caractéristique était qu'elle commençait essentiellement à utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des</a> opérations de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolution et de mise en commun</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png" alt="image"><br><br>  Ensuite, il y a eu une pause dans le développement des grilles, mais le matériel ne s'est pas arrêté; des calculs efficaces sur GPU et <abbr title="Algèbre linéaire accélérée">XLA ont été développés</abbr> .  En 2012, AlexNet est apparue, elle a participé au concours ILSVRC ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet Large-Scale Visual Recognition Challenge</a> ). <br><br><div class="spoiler">  <b class="spoiler_title">Une petite digression sur ILSVRC</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet a</a> été assemblé en 2012, et un sous-ensemble de milliers d'images et de 1000 classes a été utilisé pour le concours ILSVRC.  ImageNet compte actuellement environ 14 millions de photos et 21 841 classes (prises sur le site officiel), mais pour la compétition, elles ne sélectionnent généralement qu'un sous-ensemble.  ILSVRC est alors devenu le plus grand concours annuel de classification d'images.  Soit dit en passant, nous avons récemment compris comment <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">s'entraîner sur ImageNet en quelques minutes</a> . <br><br>  C'est sur ImageNet (en ILSVRC) de 2010 à 2018 qu'ils ont reçu les réseaux <abbr title="Etat de l'art">SOTA</abbr> dans la classification des images.  Certes, depuis 2016, les concours de localisation, de détection et de compréhension de la scène, plutôt que de classification, sont plus pertinents. <br></div></div><br>  En règle générale, diverses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">revues d'architecture</a> mettent en lumière celles qui ont été les premières à l'ILSVRC de 2010 à 2016 et sur certains réseaux individuels.  Afin de ne pas encombrer l'histoire, je les ai placés sous le spoiler ci-dessous, en essayant de souligner les idées principales: <br><br><div class="spoiler">  <b class="spoiler_title">Architecture de 2012 à 2015</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Année </th><th>  Article </th><th>  Idée clé </th><th>  Le poids </th></tr><tr><td>  2012 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Alexnet</a> </td><td>  utilisez deux faisceaux d'affilée;  diviser la formation réseau en deux branches parallèles </td><td>  240 Mo </td></tr><tr><td>  2013 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Zfnet</a> </td><td>  taille de filtre, nombre de filtres dans les couches </td><td>  - </td></tr><tr><td>  2013 </td><td>  <a href="">Surpasser</a> </td><td>  l'un des premiers détecteurs de réseau neuronal </td><td>  - </td></tr><tr><td>  2014 </td><td>  <a href="">Vgg</a> </td><td>  profondeur du réseau (13-19 couches), utilisation de plusieurs blocs Conv-Conv-Pool avec une taille de convolution plus petite (3x3) </td><td>  549 Mo (VGG-19) </td></tr><tr><td>  2014 </td><td>  <a href="">Inception (v1) (alias GoogLeNet)</a> </td><td>  Convolution 1x1 (idée du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Network-in-Network</a> ), pertes auxiliaires (ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">supervision profonde</a> ), empilement des sorties de plusieurs convolutions (Inception-block) </td><td>  - </td></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Resnet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">connexions résiduelles</a> , très profondes (152 couches ..) </td><td>  98 Mo (ResNet-50), 232 Mo (ResNet-152) </td></tr></tbody></table></div><br></div></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" width="500"></div><br><br>  Les idées de toutes ces architectures (à l'exception de ZFNet, il est généralement peu mentionné) étaient à une époque un nouveau mot dans les réseaux de neurones pour la vision.  Cependant, après 2015, il y a eu de nombreuses autres améliorations importantes, par exemple, Inception-ResNet, Xception, DenseNet, SENet.  Ci-dessous, j'ai essayé de les rassembler en un seul endroit. <br><br><div class="spoiler">  <b class="spoiler_title">Architecture de 2015 à 2019</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Année </th><th>  Article </th><th>  Idée clé </th><th>  Le poids </th></tr><tr><td>  2015 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Inception v2 et v3</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">décomposition des packages en packages 1xN et Nx1</a> </td><td>  92 Mo </td></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Inception v4 et Inception-ResNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">combinaison de Inception et ResNet</a> </td><td>  215 Mo </td></tr><tr><td>  2016-17 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Resnext</a> </td><td>  2ème place ILSVRC, l'utilisation de nombreuses branches (bloc Inception «généralisé») </td><td>  - </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Xception</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolution séparable en profondeur</a> , pèse moins avec une précision comparable à Inception </td><td>  88 Mo </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Densenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bloc dense</a>  léger mais précis </td><td>  33 Mo (DenseNet-121), 80 Mo (DenseNet-201) </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Senet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Bloc de compression et d'excitation</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">46 Mo (SENet-Inception), 440 Mo (SENet-154)</a> </td></tr></tbody></table></div><br></div></div><br>  La plupart de ces modèles pour PyTorch peuvent être trouvés <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> , et il y a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une telle chose cool</a> . <br><br>  Vous avez peut-être remarqué que tout cela pèse beaucoup (je voudrais 20 Mo maximum, voire moins), alors qu'aujourd'hui ils utilisent des appareils mobiles partout et que l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">IoT</a> gagne en popularité, ce qui signifie que vous souhaitez également utiliser des grilles là-bas. <br><br><div class="spoiler">  <b class="spoiler_title">Relation entre le poids et la vitesse du modèle</b> <div class="spoiler_text">  Étant donné que les réseaux de neurones en eux-mêmes ne multiplient que les tenseurs, le nombre d'opérations de multiplication (lire: le nombre de poids) affecte directement la vitesse de travail (si le post-traitement ou le prétraitement à forte intensité de main-d'œuvre n'est pas utilisé).  La vitesse du réseau lui-même dépend de l'implémentation (framework), du matériel sur lequel il s'exécute et de la taille de l'image d'entrée. <br></div></div><br>  Les auteurs de nombreux articles ont pris le chemin d'inventer des architectures rapides, j'ai rassemblé leurs méthodes sous le spoiler ci-dessous: <br><br><div class="spoiler">  <b class="spoiler_title">Architecture légère de CNN</b> <div class="spoiler_text"><div class="scrollable-table"><table><tbody><tr><th>  Année </th><th>  Article </th><th>  Idée clé </th><th>  Le poids </th><th>  Exemple d'implémentation </th></tr><tr><td>  2016 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Squeezenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Compression FireModule</a> </td><td>  0,5 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NASNet</a> </td><td><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">obtenu par une recherche neuronale d'architectures, il s'agit d'un réseau de la catégorie AutoML</a> </td><td>  23 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pytorch</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Shufflenet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">conv. de groupe point par point, lecture aléatoire des canaux</a> </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2017 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v1)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">convolutions séparables en profondeur et de nombreuses autres astuces</a> </td><td>  16 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v2)</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Je recommande cet article sur Habré</a> </td><td>  14 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Squeezenext</a> </td><td>  voir les photos dans le référentiel d'origine </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Caffe</a> </td></tr><tr><td>  2018 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MnasNet</a> </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">architecture neuronale recherche spécifiquement pour les appareils mobiles utilisant RL</a> </td><td>  ~ 2 Mo </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tensorflow</a> </td></tr><tr><td>  2019 </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MobileNet (v3)</a> </td><td>  elle est sortie pendant que j'écrivais un article :) </td><td>  - </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pytorch</a> </td></tr></tbody></table></div><br></div></div><br>  Les nombres dans toutes les tables <s>sont tirés du plafond</s> des référentiels, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la table Keras Applications</a> et de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> . <br><br>  Vous demandez: «Pourquoi avez-vous écrit sur tout ce« zoo »de modèles?  Et pourquoi est la tâche de la classification?  Mais nous voulons apprendre aux machines à voir, et la classification n'est qu'une sorte de tâche étroite. ».  Le fait est que les réseaux de neurones pour détecter les objets, évaluer les postures / points, ré-identifier et rechercher dans une image utilisent exactement les modèles pour la classification en tant que <b><abbr title="base, littéralement - la colonne vertébrale">colonne vertébrale</abbr></b> , et 80% du succès en dépend. <br><br>  Mais je veux en quelque sorte faire davantage confiance à CNN, ou ils ont imaginé des boîtes noires, mais ce qui est "à l'intérieur" n'est pas évident.  Pour mieux comprendre le mécanisme de fonctionnement des réseaux convolutifs, les chercheurs ont imaginé l'utilisation de la visualisation. <br><br><a name="4"></a><h3>  Visualisation des réseaux de neurones convolutionnels: montrez-moi la passion </h3><br>  Une étape importante vers la compréhension de ce qui se passe à l'intérieur des réseaux convolutionnels est l'article <a href="">«Visualiser et comprendre les réseaux convolutionnels»</a> .  Dans ce document, les auteurs ont proposé plusieurs façons de visualiser exactement à quoi (sur quelles parties de l'image) les neurones dans différentes couches CNN répondent (je recommande également de regarder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une conférence de Stanford sur ce sujet</a> ).  Les résultats étaient très impressionnants: les auteurs ont montré que les premières couches du réseau convolutionnel répondent à certaines «choses de bas niveau» par le type de bords / angles / lignes, et les dernières couches répondent déjà à des parties entières des images (voir l'image ci-dessous), c'est-à-dire qu'elles portent déjà en soi une sémantique. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="image"></div><br><br>  De plus, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">projet de visualisation en profondeur de l'Université Cornell et de la société a</a> avancé la visualisation encore plus loin, tandis que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">célèbre DeepDream a</a> appris à déformer dans un style intéressant et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">addictif</a> (ci-dessous, une image de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">deepdreamgenerator.com</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="image" width="500"></div><br><br>  En 2017, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">très bon article a été publié sur Distill</a> , dans lequel ils ont effectué une analyse détaillée de ce que chaque couche «voit», et plus récemment (en mars 2019), Google a inventé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des atlas d'activation</a> : des cartes uniques qui peuvent être construites pour chaque couche réseau, ce qui nous rapproche de la compréhension de l'image globale du travail de CNN. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" width="700"></div><br><br>  Si vous voulez jouer avec la visualisation vous-même, je recommanderais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lucid</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TensorSpace</a> . <br><br>  D'accord, CNN semble être vrai dans une certaine mesure.  Nous devons apprendre à l'utiliser dans d'autres tâches, et pas seulement dans la classification.  Cela nous aidera à extraire les images Embedding'ov et à transférer l'apprentissage. <br><br><a name="5"></a><h2>  Je suis moi-même une sorte de chirurgien: on extrait les traits des réseaux de neurones </h2><br>  Imaginez qu'il y ait une image, et nous voulons trouver celles qui lui ressemblent visuellement (c'est, par exemple, la recherche dans une image dans Yandex.Pictures).  Auparavant (avant les réseaux de neurones), les ingénieurs extrayaient manuellement des fonctionnalités pour cela, par exemple, inventant quelque chose qui décrit bien l'image et permet de la comparer avec d'autres.  Fondamentalement, ces méthodes ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HOG</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> ) fonctionnent avec <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des dégradés d'image</a> , généralement ces choses sont appelées descripteurs d'image «classiques».  D'un intérêt particulier, je me réfère à l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> et au <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cours d'Anton Konushin</a> (ce n'est pas de la publicité, juste un bon cours :) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="image" width="500"></div><br><br>  En utilisant des réseaux de neurones, nous ne pouvons pas inventer nous-mêmes ces caractéristiques et ces heuristiques, mais former correctement le modèle, puis <b>prendre la sortie d'une ou plusieurs couches du réseau comme signes de l'image</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" alt="image" width="650"></div><br>  Un examen plus approfondi de toutes les architectures montre clairement qu'il y a deux étapes de classification dans CNN: <br>  1).  Couches d'extraction de fonctionnalités pour extraire des fonctionnalités informatives à partir d'images à l'aide de couches convolutionnelles <br>  2).  Apprendre en plus de ces fonctionnalités Couches de classificateur <b>entièrement connectées (FC)</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" alt="image" width="500"></div><br><br>  <b>L'incorporation d'images (fonctionnalités)</b> est à peu près le fait que vous pouvez prendre leurs signes après l'extracteur de fonctionnalités d'un réseau de neurones convolutionnels (bien qu'elles puissent être agrégées de différentes manières) comme description informative des images.  Autrement dit, nous avons formé le réseau pour la classification, puis prenons simplement la sortie devant les couches de classification.  Ces signes sont appelés <i>caractéristiques</i> , <i>descripteurs de réseau de neurones</i> ou <i>imbrications d'</i> images (bien que les incorporations soient généralement acceptées dans la PNL, car il s'agit de vision, je parlerai souvent de <i>fonctionnalités</i> ).  Il s'agit généralement d'une sorte de vecteur numérique, par exemple, 128 nombres, avec lequel vous pouvez déjà travailler. <br><br><div class="spoiler">  <b class="spoiler_title">Mais qu'en est-il des encodeurs automatiques?</b> <div class="spoiler_text">  Oui, en fait, les fonctionnalités peuvent être obtenues par des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">auto-encodeurs</a> .  Dans ma pratique, ils l'ont fait de différentes manières, mais, par exemple, dans des articles sur la ré-identification (qui seront discutés plus tard), plus souvent, ils prennent toujours des fonctionnalités après l'extracteur, plutôt que de former l'auto-encodeur pour cela.  Il me semble qu'il vaut la peine de mener des expériences dans les deux sens, si la question est de savoir ce qui fonctionne le mieux. <br></div></div><br>  Ainsi, le pipeline pour résoudre le <b>problème de la recherche par image</b> peut être organisé simplement: nous exécutons les images via CNN, prenons les signes des couches souhaitées et comparons ces caractéristiques les unes avec les autres à partir d'images différentes.  Par exemple, nous considérons simplement la distance euclidienne de ces vecteurs. <br><br><div style="text-align:center;"><img src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" alt="image" width="500"></div><br><br>  <b>Le transfert d'apprentissage</b> est une technique bien connue pour la formation efficace de réseaux de neurones qui sont déjà formés sur un ensemble de données spécifique pour leur tâche.  Souvent, ils disent également que le réglage fin au lieu de l'apprentissage par transfert, dans les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notes de cours de Stanford cs231n,</a> ces concepts sont partagés, disent-ils, l'apprentissage par transfert est une idée générale et le réglage fin est l'une des implémentations de la technique.  Ce n'est pas si important pour nous à l'avenir, l'essentiel est de comprendre que nous pouvons simplement former le réseau à bien prédire sur le nouvel ensemble de données, à partir non pas de poids aléatoires, mais de ceux formés sur un grand type ImageNet.  Cela est particulièrement vrai lorsqu'il y a peu de données et que vous souhaitez résoudre le problème de manière qualitative. <br><br><div class="spoiler">  <b class="spoiler_title">En savoir plus sur le transfert d'apprentissage</b> <div class="spoiler_text">  <a href="">Article original</a> , mais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pourquoi lire beaucoup de texte, si vous pouvez regarder la vidéo</a> <br></div></div><br>  Cependant, le simple fait de prendre les fonctionnalités nécessaires et de faire une formation supplémentaire de l'ensemble de données à l'ensemble de données peut ne pas être suffisant, par exemple, pour des tâches de recherche de personnes / personnes / quelque chose de spécifique similaires.  Les photos d'une même personne visuellement peuvent parfois être encore plus différentes que les photographies de personnes différentes.  Il est nécessaire de faire en sorte que le réseau souligne exactement les signes inhérents à une personne / un objet, même s'il nous est difficile de le faire avec nos yeux.  Bienvenue dans le monde de l' <b>apprentissage</b> de la <b>représentation</b> . <br><br><a name="6"></a><h2>  Restez proches: l'apprentissage de la représentation pour les personnes et les individus </h2><br><div class="spoiler">  <b class="spoiler_title">Note terminologique</b> <div class="spoiler_text">  Si vous lisez des articles scientifiques, il semble parfois que certains auteurs comprennent différemment l'expression <b>apprentissage métrique</b> , et il n'y a pas de consensus sur les méthodes à appeler apprentissage métrique et celles qui ne le sont pas.  C'est pourquoi dans cet article, j'ai décidé d'éviter cette phrase particulière et j'ai utilisé un <b>apprentissage de la représentation</b> plus logique, certains lecteurs peuvent ne pas être d'accord avec cela - je serai heureux de discuter dans les commentaires. <br></div></div><br>  Nous définissons les tâches: <br><br><ul><li>  <b>Tâche 1</b> : il y a une galerie (ensemble) de photographies de visages de personnes, nous voulons que le réseau puisse répondre selon une nouvelle photo soit avec le nom d'une personne de la galerie (soi-disant c'est ça), soit dit qu'il n'y a pas une telle personne dans la galerie (et, peut-être, nous y ajoutons nouvelle personne) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300"></div></li><li>  <b>Tâche 2</b> : la même chose, mais nous ne travaillons pas avec des photographies de visages, mais avec un recadrage complet de personnes <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400"></div></li></ul><br><br>  La première tâche est généralement appelée <b>reconnaissance faciale</b> , la seconde - la <b>ré-identification</b> (en abrégé <i>Reid</i> ).  Je les ai combinés en un seul bloc, car leurs solutions utilisent aujourd'hui des idées similaires: afin d'apprendre des imbrications d'images efficaces qui peuvent faire face à des situations plutôt difficiles, elles utilisent aujourd'hui différents types de pertes, comme, par exemple, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de triplet</a> , la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de quadruplet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de centre contrastif</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">perte de cosinus</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550"></div><br><br>  Il existe encore de merveilleux <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">réseaux</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">siamois</a> , mais honnêtement, je ne les ai pas utilisés moi-même.  Soit dit en passant, non seulement la perte elle-même «décide», mais aussi comment échantillonner des paires de positifs et de négatifs pour elle, soulignent les auteurs de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'échantillonnage compte dans l'apprentissage enraciné en profondeur</a> . <br><br>  L'essence de toutes ces pertes et réseaux siamois est simple - nous voulons que les images d'une classe (personne) dans l'espace latent des fonctionnalités (plongements) soient «proches», et de différentes classes (personnes) soient «éloignées».  La proximité est généralement mesurée comme suit: des incorporations d'images à partir d'un réseau de neurones sont prises (par exemple, un vecteur de 128 nombres) et nous considérons la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">distance euclidienne</a> habituelle entre ces vecteurs ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la proximité cosinus.</a>  Comment mesurer, il vaut mieux choisir sur votre ensemble de données / tâche. <br><br>  Une représentation schématique d'un pipeline de résolution de problèmes sur l'apprentissage de la représentation ressemble à ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" width="850"></div><br><br><div class="spoiler">  <b class="spoiler_title">Mais pour être plus précis, comme ça</b> <div class="spoiler_text"> <b>  </b> :      (Softmax + CrossEntropy),      (Triplet, Contrastive, etc.).        positive'  negative'    <br><br> <b>  </b> :     -     ,        —   .   ,     —     -   ,       (,     <i></i> ).                 .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> <br></div></div><br><br>   <b> </b>    : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">- ( <b>MUST READ!</b> )</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">FaceNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ArcFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">CosFace</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" alt="image" width="700"></div><br><br>   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dlib</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenFace</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">FaceNet repo</a> ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">       </a> . ,      ArcFace  CosFace (  ,    - ,    - ). <br><br>        ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> ,   ? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" width="500"></div><br>   ,   <b>-</b>   ,    ,    , -   , -    . <br><br><img src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"><br><br>    Reid  :    <abbr title="détection humaine coupée d'une grande photographie"></abbr> , , 10 ,    5  (    ),   50   .    (),   ,       ,          ID.   ,       : , , , <s></s> ,   ,    ,   ( /   ..). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" width="700"></div><br><br>  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> Reid —    .    , -       , -      negative'  positive'. <br><br>      Reid   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> 2016 </a> . ,     ,    —   representation learning.    ,     -, ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Aligned Re-Id</a>      (,         <s>, </s> ),  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Generative Adversarial Networks (GAN)</a> . <br><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><ul><li>    , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  handcrafted-    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">       </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  Transfer Learning     </a> </li></ul><br></div></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="image" width="650"></div><br><br><div class="spoiler"> <b class="spoiler_title"></b> <div class="spoiler_text">      ,   , -,        . ,  -  ,     ,    ,     <s>   </s> .   —    ! <br></div></div><br><br>      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenReid</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">TorchReid</a> .      —   ,        ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .     PyTorch,   Readme       Person Re-identification,  . <br><br>     face-  reid-    ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  ,   </a> ).   ?  … <br><br><h3>     </h3><br>     ,      .   ,       ,      ?       ( )   : <br><br><ul><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li><li> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> </li></ul><br>      float64, , , float32   .    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  low-precision training</a> . , , Google  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">MorphNet</a> ,  ( )    . <br><br><a name="7"></a><h3>   ? </h3><br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="image" width="500"></div><br><br>          DL  CV: ,  , , .          : , ,  .    ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  </a> ,    ,    .         . <br><br> Stay tuned! <br><br><div class="spoiler"> <b class="spoiler_title">PS:     -  ?</b> <div class="spoiler_text">      (,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,   ),      <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> </a> ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> .   ,          ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">   </a> (  ). <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr450732/">https://habr.com/ru/post/fr450732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr450720/index.html">Comment développer une application conviviale</a></li>
<li><a href="../fr450724/index.html">Présentation de Python pour les camarades dépassant le «langage A contre V» langue B "et autres préjugés</a></li>
<li><a href="../fr450726/index.html">Création d'un outil pour écrire rapidement et efficacement des autotests sur Selenium</a></li>
<li><a href="../fr450728/index.html">NLog: règles et filtres</a></li>
<li><a href="../fr450730/index.html">ok.tech: rencontre frontend</a></li>
<li><a href="../fr450734/index.html">Le fuzzing est une étape importante dans un développement sûr</a></li>
<li><a href="../fr450736/index.html">«Isoler Internet est beaucoup plus facile et moins cher que de lui fournir un blocage externe.»</a></li>
<li><a href="../fr450738/index.html">Robots dans le centre de données: comment l'intelligence artificielle peut-elle être utile?</a></li>
<li><a href="../fr450740/index.html">Pied de lampe intelligent REDMOND - ajouter à la maison intelligente</a></li>
<li><a href="../fr450744/index.html">Infrastructure cyclable de Minsk pour un expatrié informatique</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>