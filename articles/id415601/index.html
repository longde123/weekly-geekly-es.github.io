<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèæ üçò üë®üèΩ‚Äç‚öñÔ∏è Kubernetes HA cluster dengan contenterd. Atau adakah kehidupan tanpa buruh pelabuhan? üç´ üòπ üëºüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Menyebarkan kubernetes HA dengan contenterd 



 Selamat siang, para pembaca Habr! Pada 24 Mei 2018, sebuah artikel berjudul Kubernetes Containerd Int...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Kubernetes HA cluster dengan contenterd. Atau adakah kehidupan tanpa buruh pelabuhan?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/415601/"><h1 id="razvertyvaenie-kubernetes-ha-s-containerd">  Menyebarkan kubernetes HA dengan contenterd </h1><br><p><img src="https://habrastorage.org/webt/0p/w3/7g/0pw37gyankmz9a8s2gcshvto_ek.png"><br>  Selamat siang, para pembaca Habr!  Pada 24 Mei 2018, sebuah artikel berjudul <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes Containerd Integration Goes GA</a> diterbitkan di blog resmi Kubernetes, yang menyatakan bahwa integrasierdengan Kubernetes siap diproduksi.  Juga, orang-orang dari perusahaan Flant memposting <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">terjemahan artikel ke dalam bahasa Rusia di</a> blog mereka, menambahkan sedikit klarifikasi dari diri mereka sendiri.  Setelah membaca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi proyek di github</a> , saya memutuskan untuk mencoba memuat di "kulit saya sendiri." </p><br><p>  Perusahaan kami memiliki beberapa proyek dalam tahap "masih sangat jauh dari produksi."  Jadi mereka akan menjadi eksperimen kami;  untuk mereka, kami memutuskan untuk mencoba menggunakan kluster failover Kubernet menggunakan contenterd dan melihat apakah ada kehidupan tanpa buruh pelabuhan. </p><br><p>  Jika Anda tertarik untuk melihat bagaimana kami melakukannya dan apa yang terjadi, selamat datang di kucing. </p><a name="habracut"></a><br><p>  <b>Deskripsi skematis dan penerapan</b> <br><img src="https://habrastorage.org/webt/db/xm/pn/dbxmpnpsth-psiiyn_ittkfkc4a.png"></p><br><p>  Ketika menggunakan cluster, seperti biasa, (saya menulis tentang ini di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel sebelumnya</a> </p><div class="spoiler">  <b class="spoiler_title">keepalived - implementasi VRRP (Virtual Router Redundancy Protocol) untuk Linux</b> <div class="spoiler_text">  Keepalived menciptakan IP virtual (VIRTIP) yang "menunjuk" (membuat subinterface) ke IP salah satu dari tiga master.  Daemon keepalived memantau kesehatan mesin dan, jika terjadi kegagalan, mengecualikan server yang gagal dari daftar server aktif dengan mengalihkan VIRTIP ke IP server lain, sesuai dengan "bobot" yang ditentukan ketika mengkonfigurasi keepalived di setiap server. </div></div><br><p>  Daemon keepalived berkomunikasi melalui VRRP, saling mengirim pesan ke alamat 224.0.0.18. </p><br><p>  Jika tetangga belum mengirim pesannya, maka setelah periode dia dianggap mati.  Segera setelah server macet mulai mengirim pesannya ke jaringan, semuanya kembali ke tempatnya </p><br><p>  Kami mengkonfigurasi pekerjaan dengan server API pada node kubernetes sebagai berikut. </p><br><p>  Setelah menginstal cluster, konfigurasikan kube-proxy, ubah port dari 6443 menjadi 16443 (detail di bawah).  Pada masing-masing master, nginx digunakan, yang berfungsi sebagai loadbalancer, mendengarkan pada port 16443 dan melakukan upstream dari ketiga master pada port 6443 (detail di bawah). </p><br><p>  Skema ini telah mencapai peningkatan toleransi kesalahan menggunakan keepalived, serta menggunakan nginx, penyeimbangan antara server API pada penyihir telah tercapai. <br></p><br><p>  Dalam artikel sebelumnya, saya menjelaskan penggunaan nginx dan etcd di buruh pelabuhan.  Tetapi dalam kasus ini, kita tidak memiliki buruh pelabuhan, jadi nginx dan etcd akan bekerja secara lokal pada masternodes. </p><br><p>  Secara teoritis, adalah mungkin untuk menggunakan nginx dan lain-lain menggunakan contenterd, tetapi jika ada masalah pendekatan ini akan mempersulit diagnosis, jadi kami memutuskan untuk tidak bereksperimen dan menjalankannya secara lokal. </p><br><p>  <b>Deskripsi server untuk ditempatkan:</b> </p><br><table><thead><tr><th>  Nama </th><th>  IP </th><th>  Layanan </th></tr></thead><tbody><tr><td>  VIRTIP </td><td>  172.26.133.160 </td><td>  ------ </td></tr><tr><td>  kubus-master01 </td><td>  172.26.133.161 </td><td>  kubeadm, kubelet, kubectl, etcd, mengandungerd, nginx, keepalived </td></tr><tr><td>  kubus-master02 </td><td>  172.26.133.162 </td><td>  kubeadm, kubelet, kubectl, etcd, mengandungerd, nginx, keepalived </td></tr><tr><td>  kubus-master03 </td><td>  172.26.133.163 </td><td>  kubeadm, kubelet, kubectl, etcd, mengandungerd, nginx, keepalived </td></tr><tr><td>  kubus-simpul01 </td><td>  172.26.133.164 </td><td>  kubeadm, kubelet, kubectl, mengandungerd </td></tr><tr><td>  kubus-simpul02 </td><td>  172.26.133.165 </td><td>  kubeadm, kubelet, kubectl, mengandungerd </td></tr><tr><td>  kubus-simpul03 </td><td>  172.26.133.166 </td><td>  kubeadm, kubelet, kubectl, mengandungerd </td></tr></tbody></table><br><p>  <b>Instal kubeadm, kubelet, kubectl dan paket terkait</b> </p><br><p>  Semua perintah dijalankan dari root </p><br><pre><code class="plaintext hljs">sudo -i</code> </pre> <br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl unzip tar apt-transport-https btrfs-tools libseccomp2 socat util-linux mc vim keepalived</code> </pre> <br><p>  <b>Instal conteinerd</b> <br><img src="https://habrastorage.org/webt/ul/nw/vg/ulnwvgeblmt74ivcsnsz2tuxcfa.png"></p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> / wget https://storage.googleapis.com/cri-containerd-release/cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz tar -xvf cri-containerd-1.1.0-rc.0.linux-amd64.tar.gz</code> </pre> <br><p>  <b>Mengkonfigurasi konfigurasi yang berisi</b> </p><br><pre> <code class="bash hljs">mkdir -p /etc/containerd nano /etc/containerd/config.toml</code> </pre> <br><p>  Tambahkan ke file: </p><br><pre> <code class="bash hljs">[plugins.cri] enable_tls_streaming = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Kami mulai conteinerd kami memeriksa bahwa semuanya OK </p><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> containerd systemctl start containerd systemctl status containerd ‚óè containerd.service - containerd container runtime Loaded: loaded (/etc/systemd/system/containerd.service; disabled; vendor preset: enabled) Active: active (running) since Mon 2018-06-25 12:32:01 MSK; 7s ago Docs: https://containerd.io Process: 10725 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Main PID: 10730 (containerd) Tasks: 15 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 14.9M CPU: 375ms CGroup: /system.slice/containerd.service ‚îî‚îÄ10730 /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/containerd Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=info msg=<span class="hljs-string"><span class="hljs-string">"Get image filesystem path "</span></span>/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs<span class="hljs-string"><span class="hljs-string">""</span></span> Jun 25 12:32:01 hb-master02 containerd[10730]: time=<span class="hljs-string"><span class="hljs-string">"2018-06-25T12:32:01+03:00"</span></span> level=error msg=<span class="hljs-string"><span class="hljs-string">"Failed to load cni during init, please check CRI plugin status before setting up network for pods"</span></span> error=<span class="hljs-string"><span class="hljs-string">"cni con Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>loading plugin <span class="hljs-string"><span class="hljs-string">"io.containerd.grpc.v1.introspection"</span></span>...<span class="hljs-string"><span class="hljs-string">" type=io.containerd.grpc.v1 Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start subscribing containerd event<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start recovering state<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg=serving... address="</span></span>/run/containerd/containerd.sock<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>containerd successfully booted <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 0.308755s<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start event monitor<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start snapshots syncer<span class="hljs-string"><span class="hljs-string">" Jun 25 12:32:01 hb-master02 containerd[10730]: time="</span></span>2018-06-25T12:32:01+03:00<span class="hljs-string"><span class="hljs-string">" level=info msg="</span></span>Start streaming server<span class="hljs-string"><span class="hljs-string">"</span></span></code> </pre> <br><p>  <b>Instal dan jalankan etcd</b> </p><br><p>  Catatan penting, saya menginstal kubernetes versi 1.10.  Hanya beberapa hari kemudian, ketika menulis artikel, versi 1.11 dirilis. Jika Anda menginstal versi 1.11, maka atur variabel ETCD_VERSION = "v3.2.17", jika 1.10 maka ETCD_VERSION = "v3.1.12". </p><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCD_VERSION=<span class="hljs-string"><span class="hljs-string">"v3.1.12"</span></span> curl -sSL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>/etcd-<span class="hljs-variable"><span class="hljs-variable">${ETCD_VERSION}</span></span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin/</code> </pre> <br><p>  Salin konfigurasi dari gitahab. </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/k8s-containerd.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> k8s-containerd</code> </pre> <br><p>  Konfigurasikan variabel dalam file konfigurasi. </p><br><pre> <code class="bash hljs">vim create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Deskripsi variabel file create-config.sh</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.161 # local machine etcd name, options: etcd1, etcd2, etcd3 export K8SHA_ETCDNAME=kube-master01 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101,100 MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.160 # master01 ip address export K8SHA_IP1=172.26.133.161 # master02 ip address export K8SHA_IP2=172.26.133.162 # master03 ip address export K8SHA_IP3=172.26.133.163 # master01 hostname export K8SHA_HOSTNAME1=kube-master01 # master02 hostname export K8SHA_HOSTNAME2=kube-master02 # master03 hostname export K8SHA_HOSTNAME3=kube-master03 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=56cf8dd754c90194d1600c483e10abfr #etcd tocken: export ETCD_TOKEN=9489bf67bdfe1b3ae077d6fd9e7efefd # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=535tdi.utzk5hf75b04ht8l # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16</span></span></code> </pre> <br><p>  pengaturan pada mesin lokal setiap node (masing-masing node memiliki sendiri) <br>  <b>K8SHA_IPLOCAL</b> - alamat IP dari node di mana skrip dikonfigurasi <br>  <b>K8SHA_ETCDNAME</b> - nama mesin lokal di gugus ETCD <br>  <b>K8SHA_KA_STATE</b> - peran dalam <b>kenang</b> - <b>kenangan</b> .  Satu simpul MASTER, semua CADANGAN lainnya. <br>  <b>K8SHA_KA_PRIO</b> - prioritas tetap, master memiliki 102 untuk 101, 100 yang tersisa. Ketika master dengan nomor 102 jatuh, node dengan nomor 101 mengambil tempat dan seterusnya. <br>  <b>K8SHA_KA_INTF</b> - antarmuka jaringan keepalived.  Nama antarmuka yang disimpan akan mendengarkan. </p><br><p>  Pengaturan umum untuk semua masternode adalah sama: </p><br><p>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.160 - IP virtual cluster. <br>  <b>K8SHA_IP1 ... K8SHA_IP3 -</b> alamat <b>IP</b> master <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME3</b> - nama host untuk masternodes.  Poin penting, dengan nama-nama ini kubeadm akan menghasilkan sertifikat. <br>  <b>K8SHA_KA_AUTH</b> - kata sandi untuk tetap <b>disimpan</b> .  Anda dapat menentukan apa saja <br>  <b>K8SHA_TOKEN</b> - klaster tanda.  Dapat dihasilkan dengan perintah <b>generate token kubeadm</b> <br>  <b>K8SHA_CIDR</b> - alamat subnet untuk perapian.  Saya menggunakan flanel jadi CIDR 0.244.0.0/16.  Pastikan untuk menyaring - dalam konfigurasi harus K8SHA_CIDR = 10.244.0.0 \ / 16 </p></div></div><br><p>  Jalankan skrip yang akan mengkonfigurasi nginx, keepalived, etcd dan kubeadmin </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><p>  Kami mulai, dll. </p><br>  dll saya angkat tanpa tls.  Jika Anda memerlukan tls, maka dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi kubernet resmi</a> ditulis secara rinci cara membuat sertifikat untuk etcd. <br><br><br><pre> <code class="bash hljs">systemctl daemon-reload &amp;&amp; systemctl start etcd &amp;&amp; systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> etcd</code> </pre> <br><p>  Pemeriksaan Status </p><br><pre> <code class="bash hljs">etcdctl cluster-health member ad059013ec46f37 is healthy: got healthy result from http://192.168.5.49:2379 member 4d63136c9a3226a1 is healthy: got healthy result from http://192.168.4.169:2379 member d61978cb3555071e is healthy: got healthy result from http://192.168.4.170:2379 cluster is healthy etcdctl member list ad059013ec46f37: name=hb-master03 peerURLs=http://192.168.5.48:2380 clientURLs=http://192.168.5.49:2379,http://192.168.5.49:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4d63136c9a3226a1: name=hb-master01 peerURLs=http://192.168.4.169:2380 clientURLs=http://192.168.4.169:2379,http://192.168.4.169:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> d61978cb3555071e: name=hb-master02 peerURLs=http://192.168.4.170:2380 clientURLs=http://192.168.4.170:2379,http://192.168.4.170:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Jika semuanya baik-baik saja, lanjutkan ke langkah berikutnya. </p><br><p>  <b>Konfigurasikan kubeadmin</b> <br>  Jika Anda menggunakan kubeadm versi 1.11, Anda dapat melewati langkah ini <br>  Agar kybernet mulai bekerja bukan dengan buruh pelabuhan, tetapi dengan contenterd, konfigurasikan konfigurasi kubeadmin </p><br><pre> <code class="bash hljs">vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> </pre> <br><p>  Setelah [Layanan] tambahkan baris ke blok </p><br><pre> <code class="bash hljs">Environment=<span class="hljs-string"><span class="hljs-string">"KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Keseluruhan konfigurasi akan terlihat seperti ini:</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Service] Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock" Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_SYSTEM_PODS_ARGS=--pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true" Environment="KUBELET_NETWORK_ARGS=--network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin" Environment="KUBELET_DNS_ARGS=--cluster-dns=10.96.0.10 --cluster-domain=cluster.local" Environment="KUBELET_AUTHZ_ARGS=--authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt" Environment="KUBELET_CADVISOR_ARGS=--cadvisor-port=0" Environment="KUBELET_CERTIFICATE_ARGS=--rotate-certificates=true --cert-dir=/var/lib/kubelet/pki" ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS</code> </pre> </div></div><br><p>  Jika Anda menginstal versi 1.11 dan ingin bereksperimen dengan CoreDNS alih-alih kube-dns dan menguji konfigurasi dinamis, batalkan komentar pada blok berikut di file konfigurasi kubeadm-init.yaml: </p><br><pre> <code class="bash hljs">feature-gates: DynamicKubeletConfig: <span class="hljs-literal"><span class="hljs-literal">true</span></span> CoreDNS: <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Mulai kembali kubelet </p><br><pre> <code class="plaintext hljs">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</code> </pre> <br><p>  <b>Inisialisasi panduan pertama</b> </p><br><p>  Sebelum memulai kubeadm, Anda harus memulai ulang keepalived dan memeriksa statusnya </p><br><pre> <code class="bash hljs">systemctl restart keepalived.service systemctl status keepalived.service ‚óè keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-06-27 10:40:03 MSK; 1min 44s ago Process: 4589 ExecStart=/usr/sbin/keepalived <span class="hljs-variable"><span class="hljs-variable">$DAEMON_ARGS</span></span> (code=exited, status=0/SUCCESS) Main PID: 4590 (keepalived) Tasks: 7 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 15.3M CPU: 968ms CGroup: /system.slice/keepalived.service ‚îú‚îÄ4590 /usr/sbin/keepalived ‚îú‚îÄ4591 /usr/sbin/keepalived ‚îú‚îÄ4593 /usr/sbin/keepalived ‚îú‚îÄ5222 /usr/sbin/keepalived ‚îú‚îÄ5223 sh -c /etc/keepalived/check_apiserver.sh ‚îú‚îÄ5224 /bin/bash /etc/keepalived/check_apiserver.sh ‚îî‚îÄ5231 sleep 5</code> </pre> <br><p>  periksa apakah ping VIRTIP </p><br><pre> <code class="bash hljs">ping -c 4 172.26.133.160 PING 172.26.133.160 (172.26.133.160) 56(84) bytes of data. 64 bytes from 172.26.133.160: icmp_seq=1 ttl=64 time=0.030 ms 64 bytes from 172.26.133.160: icmp_seq=2 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=3 ttl=64 time=0.050 ms 64 bytes from 172.26.133.160: icmp_seq=4 ttl=64 time=0.056 ms --- 172.26.133.160 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3069ms rtt min/avg/max/mdev = 0.030/0.046/0.056/0.012 ms</code> </pre> <br><p>  Setelah itu, jalankan kubeadmin.  Pastikan untuk menyertakan baris --skip-preflight-checking.  Kubeadmin secara default mencari buruh pelabuhan dan tanpa melewati pemeriksaan akan gagal dengan kesalahan. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  Setelah kubeadm bekerja, simpan baris yang dihasilkan.  Ini akan diperlukan untuk memasukkan node yang bekerja ke dalam cluster. </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre> <br><p>  Selanjutnya, tunjukkan di mana file admin.conf disimpan <br>  Jika kita bekerja sebagai root, maka: </p><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><p>  Untuk pengguna sederhana, ikuti instruksi di layar. </p><br><pre> <code class="bash hljs">mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre> <br><p>  Tambahkan 2 penyihir lagi ke cluster.  Untuk melakukan ini, salin sertifikat dari kube-master01 ke kube-master02 dan kube-master03 ke direktori / etc / kubernetes /.  Untuk melakukan ini, saya mengkonfigurasi akses ssh untuk root, dan setelah menyalin file, saya mengembalikan pengaturan. </p><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.162:/etc/kubernetes/ scp -r /etc/kubernetes/pki 172.26.133.163:/etc/kubernetes/</code> </pre> <br><p>  Setelah menyalin ke kube-master02 dan kube-master03, jalankan. </p><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml --skip-preflight-checks</code> </pre> <br><p>  <b>Instal flanel CIDR</b> </p><br><p>  pada eksekusi kube-master01 </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml</code> </pre> <br><p>  Versi flanel saat ini dapat ditemukan di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi kubernetes</a> . </p><br><p>  Kami menunggu sampai semua wadah dibuat. </p><br><pre> <code class="bash hljs">watch -n1 kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system kube-apiserver-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-apiserver-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-apiserver-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-controller-manager-kube-master01 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-controller-manager-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-controller-manager-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-dns-86f4d74b45-8c24s 3/3 Running 0 17m 10.244.2.2 kube-master03 kube-system kube-flannel-ds-4h4w7 1/1 Running 0 2m 172.26.133.163 kube-master03 kube-system kube-flannel-ds-kf5mj 1/1 Running 0 2m 172.26.133.162 kube-master02 kube-system kube-flannel-ds-q6k4z 1/1 Running 0 2m 172.26.133.161 kube-master01 kube-system kube-proxy-9cjtp 1/1 Running 0 6m 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 0 17m 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master01 1/1 Running 0 18m 172.26.133.161 kube-master01 kube-system kube-scheduler-kube-master02 1/1 Running 0 6m 172.26.133.162 kube-master02 kube-system kube-scheduler-kube-master03 1/1 Running 0 6m 172.26.133.163 kube-master03</code> </pre> <br><p>  <b>Kami membuat replikasi kube-dns ke ketiga master</b> </p><br><p>  Pada kube-master01 jalankan </p><br><pre> <code class="bash hljs">kubectl scale --replicas=3 -n kube-system deployment/kube-dns</code> </pre> <br><p>  <b>Instal dan konfigurasikan nginx</b> </p><br><p>  Pada setiap master node, instal nginx sebagai penyeimbang untuk API Kubernetes <br>  Saya memiliki semua mesin cluster di debian.  Dari paket nginx, modul stream tidak mendukung, jadi tambahkan repositori nginx dan instal dari repositori nginx`a.  Jika Anda memiliki OS yang berbeda, lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi nginx</a> . </p><br><pre> <code class="bash hljs">wget https://nginx.org/keys/nginx_signing.key sudo apt-key add nginx_signing.key <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> -e <span class="hljs-string"><span class="hljs-string">"\n#nginx\n\ deb http://nginx.org/packages/debian/ stretch nginx\n\ deb-src http://nginx.org/packages/debian/ stretch nginx"</span></span> &gt;&gt; /etc/apt/sources.list apt-get update &amp;&amp; apt-get install nginx -y</code> </pre> <br><p>  Buat nginx config (jika belum dibuat) </p><br><pre> <code class="bash hljs">./create-config.sh</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">nginx.conf</b> <div class="spoiler_text"><p>  pengguna nginx; <br>  pekerja_proses otomatis; </p><br><p>  peringatan error_log /var/log/nginx/error.log; <br>  pid /var/run/nginx.pid; </p><br><p>  acara { <br>  koneksi pekerja_1024; <br>  } </p><br><p>  http { <br>  termasuk /etc/nginx/mime.types; <br>  default_type application / octet-stream; </p><br><pre> <code class="plaintext hljs">log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;</code> </pre> <br><p>  } </p><br><p>  aliran { <br>  apiserver hulu { <br>  server 172.26.133.161:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  server 172.26.133.162:6443 weight = 5 max_fails = 3 fail_timeout = 30s; <br>  server 172.26.133.163:6443 weight = 5 max_fails = 3 fail_timeout = 30s; </p><br><pre> <code class="plaintext hljs">} server { listen 16443; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass apiserver; }</code> </pre> <br><p>  } </p></div></div><br><p>  Kami memeriksa bahwa semuanya OK dan menerapkan konfigurasi </p><br><pre> <code class="bash hljs">nginx -t nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> is successful systemctl restart nginx systemctl status nginx ‚óè nginx.service - nginx - high performance web server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Thu 2018-06-28 08:48:09 MSK; 22s ago Docs: http://nginx.org/en/docs/ Process: 22132 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 22133 (nginx) Tasks: 2 (<span class="hljs-built_in"><span class="hljs-built_in">limit</span></span>: 4915) Memory: 1.6M CPU: 7ms CGroup: /system.slice/nginx.service ‚îú‚îÄ22133 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.conf ‚îî‚îÄ22134 nginx: worker process</code> </pre> <br><p>  Uji penyeimbang </p><br><pre> <code class="bash hljs">curl -k https://172.26.133.161:16443 | wc -l % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 12348 0 --:--:-- --:--:-- --:--:-- 12944</code> </pre> <br><p>  <b>Konfigurasikan kube-proxy untuk bekerja dengan penyeimbang</b> </p><br><p>  Setelah penyeimbang dikonfigurasi, edit port di pengaturan kubernetes. </p><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy</code> </pre> <br><p>  Ubah pengaturan server menjadi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https://172.26.133.160:16443</a> <br>  Selanjutnya, Anda perlu mengonfigurasi kube-proxy untuk bekerja dengan port baru </p><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-9cjtp 1/1 Running 1 22h 172.26.133.163 kube-master03 kube-system kube-proxy-9sqk2 1/1 Running 1 22h 172.26.133.161 kube-master01 kube-system kube-proxy-jg2pt 1/1 Running 4 22h 172.26.133.162 kube-</code> </pre> <br><p>  Kami menghapus semua pod, setelah dihapus, pod tersebut dibuat ulang secara otomatis dengan pengaturan baru </p><br><pre> <code class="bash hljs">kubectl delete pod -n kube-system kube-proxy-XXX ```bash    .      ```bash kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-hqrsw 1/1 Running 0 33s 172.26.133.161 kube-master01 kube-system kube-proxy-kzvw5 1/1 Running 0 47s 172.26.133.163 kube-master03 kube-system kube-proxy-zzkz5 1/1 Running 0 7s 172.26.133.162 kube-master02</code> </pre> <br><p>  <b>Menambahkan node kerja ke cluster</b> </p><br><p>  Pada setiap catatan root, jalankan perintah yang dihasilkan oleh kubeadm </p><br><pre> <code class="bash hljs">kubeadm join 172.26.133.160:6443 --token XXXXXXXXXXXXXXXXXXXXXXXXX --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX --cri-socket /run/containerd/containerd.sock --skip-preflight-checks</code> </pre> <br><p>  Jika garis "hilang", maka Anda harus membuat yang baru </p><br><pre> <code class="bash hljs">kubeadm token generate kubeadm token create &lt;generated-token&gt; --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command --ttl=0</code> </pre> <br><p>  Pada node yang berfungsi di file /etc/kubernetes/bootstrap-kubelet.conf dan /etc/kubernetes/kubelet.conf <br>  nilai variabel server untuk virtip kami </p><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.60:16443 vim /etc/kubernetes/kubelet.conf server: https://172.26.133.60:16443</code> </pre> <br><p>  Dan mulai ulang contenterd dan kubernetes </p><br><pre> <code class="bash hljs">systemctl restart containerd kubelet</code> </pre> <br><p>  <b>Instalasi dasbor</b> </p><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br><p>  Buat pengguna dengan hak istimewa admin: </p><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml</code> </pre> <br><p>  Kami mendapatkan token untuk masuk: </p><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre> <br><p>  Mengonfigurasi akses dasbor melalui NodePort di VIRTIP </p><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br><p>  Kami mengganti nilai tipe: ClusterIP dengan tipe: NodePort dan di bagian port: tambahkan nilai nodePort: 30000 (atau port di kisaran 30000 hingga 32000 tempat Anda ingin panel dapat diakses): </p><br><p><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"></p><br><p>  Panel sekarang tersedia di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">https: // VIRTIP: 30000</a> </p><br><p>  <b>Heapster</b> </p><br><p>  Selanjutnya, instal Heapster, alat untuk mendapatkan metrik komponen kluster. </p><br><p>  Instalasi: </p><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml</code> </pre> <br><p>  <b>Kesimpulan</b> </p><br><p>  Saya tidak melihat adanya masalah khusus ketika bekerja dengan contenterd.  Suatu ketika ada kesalahan yang tidak bisa dipahami dengan perapian setelah penyebaran dihapus.  Kubernetes percaya bahwa di bawah telah dihapus, tetapi di bawah menjadi "zombie" aneh. Masih ada di node, tetapi dalam status diperpanjang. </p><br><p>  Saya percaya bahwa Containerd lebih berorientasi sebagai runtime wadah untuk kubernetes.  Kemungkinan besar, di masa depan, sebagai lingkungan untuk meluncurkan layanan microser di Kubernetes, adalah mungkin dan perlu untuk menggunakan lingkungan yang berbeda yang akan diorientasikan untuk berbagai tugas, proyek, dll. </p><br><p>  Proyek ini berkembang sangat cepat.  Alibaba Cloud telah mulai secara aktif menggunakan conatinerd dan menekankan bahwa itu adalah lingkungan yang ideal untuk menjalankan kontainer. </p><br><p>  Menurut pengembang, integrasi contenterd di platform cloud Google Kubernetes sekarang setara dengan integrasi Docker. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Contoh yang bagus dari utilitas konsol crictl</a> .  Saya juga akan memberikan beberapa contoh dari cluster yang dibuat: </p><br><pre> <code class="plaintext hljs">kubectl describe nodes | grep "Container Runtime Version:"</code> </pre> <br><p><img src="https://habrastorage.org/webt/gr/_l/of/gr_lofuou-20jmqzb800qdi5yny.png"></p><br><p>  CLI Docker tidak memiliki konsep dasar Kubernetes, misalnya, pod dan namespace, sementara crictl mendukung konsep-konsep ini </p><br><pre> <code class="plaintext hljs">crictl pods</code> </pre> <br><p><img src="https://habrastorage.org/webt/kr/im/hl/krimhlcrmwcaxysx3wgd9mjtvuo.png"></p><br><p>  Dan jika perlu, kita bisa melihat wadah dalam format yang biasa, seperti buruh pelabuhan </p><br><pre> <code class="plaintext hljs">crictl ps</code> </pre> <br><p><img src="https://habrastorage.org/webt/zo/2m/ez/zo2mezfxjgohjpu8f35n-nazf74.png"></p><br><p>  Kita bisa melihat gambar yang ada di node </p><br><pre> <code class="plaintext hljs">crictl images</code> </pre> <br><p><img src="https://habrastorage.org/webt/mw/u6/tu/mwu6tunxz4re5yrr0h-ajq-_3xs.png"></p><br><p>  Ternyata, hidup tanpa buruh pelabuhan adalah :) </p><br><p>  Masih terlalu dini untuk berbicara tentang bug dan gangguan, cluster telah bekerja bersama kami selama sekitar satu minggu.  Dalam waktu dekat tes akan ditransfer ke sana, dan jika berhasil, kemungkinan besar berdiri dev salah satu proyek.  Ada ide tentang ini untuk menulis serangkaian artikel yang mencakup proses DevOps, seperti: membuat cluster, mengatur pengontrol masuknya dan memindahkannya ke node yang terpisah dari cluster, mengotomasi perakitan gambar, memeriksa gambar untuk kerentanan, penyebaran, dll.  Sementara itu, kita akan melihat stabilitas pekerjaan, mencari bug dan mengembangkan produk baru. </p><br><p>  Juga, manual ini cocok untuk menggunakan gugus failover dengan buruh pelabuhan, Anda hanya perlu menginstal buruh pelabuhan sesuai dengan instruksi dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi Kubernet resmi</a> dan lewati langkah-langkah untuk menginstal contenterd dan mengkonfigurasi konfigurasi kubeadm. </p><br><p>  Atau Anda dapat menempatkan contenterd dan buruh pelabuhan secara bersamaan di host yang sama, dan, seperti yang dipastikan pengembang, mereka akan bekerja sama dengan sempurna.  Containerd adalah lingkungan peluncuran container konbernetes, dan buruh pelabuhan seperti buruh pelabuhan))) </p><br><p><img src="https://habrastorage.org/webt/qj/f2/r2/qjf2r2vn_j4odysnyxytokycz9u.png"><br></p><br>  Repositori memiliki containerd <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pedoman ansible</a> untuk menginstal sebuah cluster dengan satu master.  Tapi itu lebih menarik bagi saya untuk "meningkatkan" sistem dengan tangan saya untuk memahami secara lebih rinci konfigurasi setiap komponen dan memahami cara kerjanya dalam praktik. <br><p>  Mungkin suatu hari nanti tangan saya akan mencapai, dan saya akan menulis buku pedoman saya untuk menyebarkan sebuah cluster dengan HA, karena selama enam bulan terakhir saya telah menggunakan mereka selama lebih dari selusin dan mungkin akan menjadi waktu untuk mengotomatiskan proses. </p><br><p>  Juga, saat menulis artikel ini, versi kubernetes 1.11 dirilis.  Anda dapat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">membaca</a> tentang perubahan utama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di blog Flant</a> atau di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">blog kubernet resmi</a> .  Kami memperbarui gugus uji ke versi 1.11 dan mengganti kube-dns dengan CoreDNS.  Selain itu, kami menyertakan fungsi DynamicKubeletConfig untuk menguji kemampuan pembaruan dinamis konfigurasi. </p><br><p>  Bahan yang digunakan: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes Containerd Integration Goes GA</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernet mengandung integrasi yang menggantikan Docker yang siap diproduksi</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengandung gen github</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi Kubernetes</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi NGINX</a> </li></ul><br><p>  Terima kasih sudah membaca sampai akhir. </p><br><p>  Karena informasi tentang kubernetes, terutama pada cluster yang beroperasi dalam kondisi nyata, sangat langka di RuNet, indikasi ketidakakuratan diterima, seperti komentar pada skema penyebaran cluster umum.  Saya akan mencoba untuk memperhitungkannya dan membuat koreksi yang tepat.  Dan saya selalu siap untuk menjawab pertanyaan di komentar, di githab dan di jejaring sosial apa pun yang ditunjukkan di profil saya. </p><br><p>  Hormat kami, Eugene. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id415601/">https://habr.com/ru/post/id415601/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id415591/index.html">Insinyur Apple Trap Perangkap Keyboard MacBook Pro</a></li>
<li><a href="../id415593/index.html">6 tahun kemudian, versi baru dari distribusi crash legendaris Hiren's BootCD</a></li>
<li><a href="../id415595/index.html">Metode untuk meningkatkan retensi pemain menggunakan game SLOT sebagai contoh: Bagian 1</a></li>
<li><a href="../id415597/index.html">Postfix - amavisd-new tanpa localhost atau server mail dengan cara baru</a></li>
<li><a href="../id415599/index.html">India juga ingin mendapatkan helium-3</a></li>
<li><a href="../id415605/index.html">Bagaimana kami menyimpan pemrosesan kartu dengan Exadata</a></li>
<li><a href="../id415611/index.html">PKI: Perpustakaan GCrypt dan KSBA sebagai alternatif untuk OpenSSL dengan dukungan untuk kriptografi Rusia. Lanjutan</a></li>
<li><a href="../id415613/index.html">Mengapa Anda tidak membeli lampu LED</a></li>
<li><a href="../id415615/index.html">Interaksi dengan server melalui API di iOS pada Swift 3. Bagian 2</a></li>
<li><a href="../id415617/index.html">Menggunakan Perpustakaan FPC InternetPCools di Delphi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>