<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äç‚úàÔ∏è üåπ ‚òîÔ∏è Cluster de basculement PostgreSQL + Patroni. Exp√©rience de mise en ≈ìuvre üëèüèª üèÇüèø üó®Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans l'article, je vais vous expliquer comment nous avons abord√© le probl√®me de tol√©rance aux pannes de PostgreSQL, pourquoi cela est devenu important...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cluster de basculement PostgreSQL + Patroni. Exp√©rience de mise en ≈ìuvre</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/457326/">  Dans l'article, je vais vous expliquer comment nous avons abord√© le probl√®me de tol√©rance aux pannes de PostgreSQL, pourquoi cela est devenu important pour nous et ce qui s'est finalement produit. <br><br>  Nous avons un service tr√®s charg√©: 2,5 millions d'utilisateurs dans le monde, plus de 50 000 utilisateurs actifs chaque jour.  Les serveurs sont situ√©s √† Amazone dans une r√©gion de l'Irlande: il y a constamment plus de 100 serveurs diff√©rents en fonctionnement, dont pr√®s de 50 avec des bases de donn√©es. <br><br>  L'ensemble du backend est une grande application Java monolithique avec √©tat qui maintient une connexion websocket constante au client.  Gr√¢ce au travail simultan√© de plusieurs utilisateurs sur une m√™me carte, ils voient tous les changements en temps r√©el, car nous enregistrons chaque changement dans la base de donn√©es.  Nous avons environ 10 000 requ√™tes par seconde dans nos bases de donn√©es.  Au pic de charge dans Redis, nous √©crivons √† 80-100K requ√™tes par seconde. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><a name="habracut"></a><br><br><h2>  Pourquoi nous sommes pass√©s de Redis √† PostgreSQL </h2><br>  Au d√©part, notre service fonctionnait avec Redis, un r√©f√©rentiel de valeurs-cl√©s qui stocke toutes les donn√©es dans la RAM du serveur. <br><br>  Avantages de Redis: <br><br><ol><li>  Taux de r√©ponse √©lev√©, comme  tout est stock√© en m√©moire; </li><li>  Commodit√© de sauvegarde et de r√©plication. </li></ol><br>  Contre Redis pour nous: <br><br><ol><li> Il n'y a pas de transactions r√©elles.  Nous avons essay√© de les simuler au niveau de notre application.  Malheureusement, cela n'a pas toujours bien fonctionn√© et a n√©cessit√© l'√©criture de code tr√®s complexe. </li><li>  La quantit√© de donn√©es est limit√©e par la quantit√© de m√©moire.  √Ä mesure que la quantit√© de donn√©es augmente, la m√©moire augmentera et, √† la fin, nous rencontrerons les caract√©ristiques de l'instance s√©lectionn√©e, ce qui dans AWS n√©cessite l'arr√™t de notre service pour changer le type d'instance. </li><li>  Il est n√©cessaire de maintenir constamment un faible niveau de latence,  Nous avons un tr√®s grand nombre de demandes.  Le niveau de retard optimal pour nous est de 17-20 ms.  Au niveau de 30 √† 40 ms, nous obtenons de longues r√©ponses aux demandes de notre application et √† la d√©gradation du service.  Malheureusement, cela s'est produit avec nous en septembre 2018, lorsque l'une des instances de Redis pour une raison quelconque a re√ßu une latence 2 fois plus √©lev√©e que d'habitude.  Pour r√©soudre le probl√®me, nous avons arr√™t√© le service en milieu de journ√©e pour une maintenance impr√©vue et remplac√© l'instance Redis probl√©matique. </li><li>  Il est facile d'obtenir une incoh√©rence des donn√©es m√™me avec des erreurs mineures dans le code, puis de passer beaucoup de temps √† √©crire du code pour corriger ces donn√©es. </li></ol><br>  Nous avons pris en compte les inconv√©nients et r√©alis√© que nous devons passer √† quelque chose de plus pratique, avec des transactions normales et moins de d√©pendance √† la latence.  A men√© une √©tude, analys√© de nombreuses options et choisi PostgreSQL. <br><br>  Nous √©voluons vers une nouvelle base de donn√©es depuis 1,5 ans et n'avons transf√©r√© qu'une petite partie des donn√©es, nous travaillons donc simultan√©ment avec Redis et PostgreSQL.  Plus d'informations sur les √©tapes de d√©placement et de commutation des donn√©es entre les bases de donn√©es sont √©crites dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de mon coll√®gue</a> . <br><br>  Lorsque nous avons commenc√© √† d√©m√©nager, notre application a travaill√© directement avec la base de donn√©es et s'est tourn√©e vers l'assistant Redis et PostgreSQL.  Le cluster PostgreSQL √©tait compos√© d'un ma√Ætre et d'une r√©plique de r√©plique asynchrone.  Voici √† quoi ressemblait le sch√©ma de fonctionnement de la base de donn√©es: <br><img src="https://habrastorage.org/webt/wc/wg/ef/wcwgefzqham9mw7hm-5xc37pfp0.png"><br><br><h2>  D√©ploiement de PgBouncer </h2><br>  Pendant que nous d√©m√©nagions, le produit a √©galement √©volu√©: le nombre d'utilisateurs et le nombre de serveurs fonctionnant avec PostgreSQL ont augment√© et nous avons commenc√© √† manquer des connexions.  PostgreSQL cr√©e un processus distinct pour chaque connexion et consomme des ressources.  Vous pouvez augmenter le nombre de connexions jusqu'√† un certain point, sinon il y a une chance d'obtenir un fonctionnement de base de donn√©es non optimal.  L'option id√©ale dans cette situation serait le choix d'un gestionnaire de connexion qui se tiendra devant la base. <br><br>  Nous avions deux options pour le gestionnaire de connexions: Pgpool et PgBouncer.  Mais le premier ne prend pas en charge le mode transactionnel de travail avec la base de donn√©es, nous avons donc choisi PgBouncer. <br><br>  Nous avons mis en place le sch√©ma de fonctionnement suivant: notre application acc√®de √† un PgBouncer, suivi de Masters PostgreSQL, et derri√®re chaque ma√Ætre, une r√©plique avec r√©plication asynchrone. <br><img src="https://habrastorage.org/webt/ql/uq/vh/qluqvh64yeyzwvow79wc3tt0nm0.png"><br><br>  Dans le m√™me temps, nous ne pouvions pas stocker la totalit√© des donn√©es dans PostgreSQL, et la vitesse de travail avec la base de donn√©es √©tait importante pour nous, nous avons donc commenc√© √† partager PostgreSQL au niveau de l'application.  Le sch√©ma d√©crit ci-dessus est relativement pratique pour cela: lors de l'ajout d'un nouveau fragment PostgreSQL, il suffit de mettre √† jour la configuration de PgBouncer et l'application peut imm√©diatement fonctionner avec le nouveau fragment. <br><br><h3>  PgBouncer Fault Tolerance </h3><br>  Ce sch√©ma a fonctionn√© jusqu'√† la mort de la seule instance de PgBouncer.  Nous sommes situ√©s dans AWS, o√π toutes les instances s'ex√©cutent sur du mat√©riel qui s'√©teint p√©riodiquement.  Dans de tels cas, l'instance se d√©place simplement vers le nouveau mat√©riel et fonctionne √† nouveau.  Cela s'est produit avec PgBouncer, mais il est devenu indisponible.  Le r√©sultat de cet automne a √©t√© l'inaccessibilit√© de notre service pendant 25 minutes.  AWS recommande d'utiliser la redondance c√¥t√© utilisateur pour de telles situations, qui n'√©tait pas impl√©ment√©e avec nous √† l'√©poque. <br><br>  Apr√®s cela, nous avons s√©rieusement r√©fl√©chi √† la tol√©rance aux pannes des clusters PgBouncer et PostgreSQL, car une situation similaire pourrait se reproduire avec n'importe quelle instance de notre compte AWS. <br><br>  Nous avons construit le sch√©ma de tol√©rance aux pannes PgBouncer comme suit: tous les serveurs d'applications acc√®dent au Network Load Balancer, derri√®re lequel se trouvent deux PgBouncer.  Chacun des PgBouncer regarde le m√™me PostgreSQL ma√Ætre de chaque fragment.  Si l'instance AWS se bloque √† nouveau, tout le trafic est redirig√© via un autre PgBouncer.  Tol√©rance aux pannes Network Load Balancer fournit AWS. <br><br>  Ce sch√©ma vous permet d'ajouter facilement de nouveaux serveurs PgBouncer. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br><h2>  Cr√©ation d'un cluster de basculement PostgreSQL </h2><br>  Pour r√©soudre ce probl√®me, nous avons envisag√© diff√©rentes options: basculement auto-√©crit, repmgr, AWS RDS, Patroni. <br><br><h3>  Scripts auto-√©crits </h3><br>  Ils peuvent surveiller le travail du ma√Ætre et, en cas de chute, promouvoir la r√©plique aupr√®s du ma√Ætre et mettre √† jour la configuration de PgBouncer. <br><br>  Les avantages de cette approche sont une simplicit√© maximale, car vous √©crivez vous-m√™me des scripts et comprenez exactement comment ils fonctionnent. <br><br>  Inconv√©nients: <br><br><ul><li>  Le ma√Ætre peut ne pas mourir; au lieu de cela, une d√©faillance du r√©seau peut se produire.  Le basculement, sans le savoir, fera avancer la r√©plique vers le ma√Ætre, et l'ancien ma√Ætre continuera de fonctionner.  En cons√©quence, nous obtenons deux serveurs dans le r√¥le de ma√Ætre et nous ne savons pas lequel d'entre eux poss√®de les derni√®res donn√©es r√©elles.  Cette situation est √©galement appel√©e split-brain; </li><li>  Nous nous sommes retrouv√©s sans r√©plique.  Dans notre configuration, le ma√Ætre et une r√©plique, apr√®s avoir chang√© de r√©plique, il se d√©place vers le ma√Ætre et nous n'avons plus de r√©pliques, nous devons donc ajouter manuellement une nouvelle r√©plique; </li><li>  Nous avons besoin d'une surveillance suppl√©mentaire du fonctionnement du basculement, alors que nous avons 12 fragments PostgreSQL, ce qui signifie que nous devons surveiller 12 clusters.  Si vous augmentez le nombre de fragments, vous devez toujours vous rappeler de mettre √† jour le basculement. </li></ul><br>  Le basculement auto-√©crit semble tr√®s compliqu√© et n√©cessite un support non trivial.  Avec un seul cluster PostgreSQL, ce sera l'option la plus simple, mais elle n'est pas √©volutive, donc elle ne nous convient pas. <br><br><h3>  Repmgr </h3><br>  Replication Manager pour les clusters PostgreSQL, qui peut g√©rer le fonctionnement d'un cluster PostgreSQL.  Dans le m√™me temps, il n'y a pas de basculement automatique ¬´pr√™t √† l'emploi¬ª, donc pour le travail, vous devrez √©crire votre propre ¬´wrapper¬ª au-dessus de la solution finie.  Tout peut donc s'av√©rer encore plus compliqu√© qu'avec des scripts auto-√©crits, nous n'avons donc m√™me pas essay√© Repmgr. <br><br><h3>  AWS RDS </h3><br>  Il prend en charge tout ce dont vous avez besoin pour nous, sait comment sauvegarder et prend en charge un pool de connexions.  Il a une commutation automatique: √† la mort du ma√Ætre, la r√©plique devient le nouveau ma√Ætre et AWS change l'enregistrement DNS en nouveau ma√Ætre, tandis que les r√©pliques peuvent √™tre dans diff√©rents AZ. <br><br>  Les inconv√©nients incluent le manque de param√®tres subtils.  √Ä titre d'exemple de r√©glage fin: sur nos instances, il existe des restrictions pour les connexions TCP, ce qui, malheureusement, ne peut pas √™tre effectu√© dans RDS: <br><br><pre><code class="python hljs">net.ipv4.tcp_keepalive_time=<span class="hljs-number"><span class="hljs-number">10</span></span> net.ipv4.tcp_keepalive_intvl=<span class="hljs-number"><span class="hljs-number">1</span></span> net.ipv4.tcp_keepalive_probes=<span class="hljs-number"><span class="hljs-number">5</span></span> net.ipv4.tcp_retries2=<span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br>  De plus, le prix AWS RDS est presque deux fois plus √©lev√© que le prix d'instance normal, ce qui √©tait la principale raison du rejet de cette d√©cision. <br><br><h3>  Patroni </h3><br>  Il s'agit d'un mod√®le python pour la gestion de PostgreSQL avec une bonne documentation, un basculement automatique et un code source github. <br><br>  Avantages de Patroni: <br><br><ul><li>  Chaque param√®tre de configuration est peint, son fonctionnement est clair; </li><li>  Le basculement automatique fonctionne d√®s la sortie de la bo√Æte; </li><li>  Il est √©crit en python, et puisque nous √©crivons beaucoup en python nous-m√™mes, il nous sera plus facile de g√©rer les probl√®mes et, √©ventuellement, m√™me d'aider au d√©veloppement du projet; </li><li>  Il contr√¥le enti√®rement PostgreSQL, vous permet de modifier la configuration sur tous les n≈ìuds du cluster √† la fois, et si un red√©marrage du cluster est n√©cessaire pour appliquer la nouvelle configuration, cela peut √™tre fait √† nouveau √† l'aide de Patroni. </li></ul><br>  Inconv√©nients: <br><br><ul><li>  D'apr√®s la documentation, il n'est pas clair comment travailler avec PgBouncer.  Bien qu'il soit difficile de l'appeler un inconv√©nient, car la t√¢che de Patroni est de g√©rer PostgreSQL, et la fa√ßon dont les connexions √† Patroni se passeront est notre probl√®me; </li><li>  Il existe peu d'exemples d'impl√©mentation de Patroni sur de gros volumes, tandis que de nombreux exemples d'impl√©mentation √† partir de z√©ro. </li></ul><br>  Par cons√©quent, pour cr√©er un cluster de basculement, nous avons choisi Patroni. <br><br><h2>  Processus de mise en ≈ìuvre des patrons </h2><br>  Avant Patroni, nous avions 12 fragments PostgreSQL en configuration, un ma√Ætre et une r√©plique avec r√©plication asynchrone.  Les serveurs d'applications ont acc√©d√© aux bases de donn√©es via le Network Load Balancer, derri√®re lequel il y avait deux instances avec PgBouncer, et derri√®re eux se trouvaient tous les serveurs PostgreSQL. <br><img src="https://habrastorage.org/webt/05/uc/da/05ucdayudomunfsxjc_gggs5abe.png"><br><br>  Pour impl√©menter Patroni, nous devions s√©lectionner un r√©f√©rentiel de configuration de cluster distribu√©.  Patroni fonctionne avec des syst√®mes de stockage de configuration distribu√©e tels que etcd, Zookeeper, Consul.  Nous avons juste un cluster Consul √† part enti√®re sur prod qui fonctionne en conjonction avec Vault et nous ne l'utilisons plus.  Une excellente raison de commencer √† utiliser Consul conform√©ment √† sa destination. <br><br><h3>  Comment Patroni travaille avec le consul </h3><br>  Nous avons un cluster Consul, qui se compose de trois n≈ìuds, et un cluster Patroni, qui se compose d'un chef de file et d'une r√©plique (dans Patroni, un ma√Ætre est appel√© chef de cluster et les esclaves sont appel√©s r√©pliques).  Chaque instance d'un cluster Patroni envoie constamment des informations sur l'√©tat du cluster au Consul.  Par cons√©quent, aupr√®s de Consul, vous pouvez toujours conna√Ætre la configuration actuelle du cluster Patroni et qui est le leader en ce moment. <br><br><img src="https://habrastorage.org/webt/jx/j5/is/jxj5ispkzoegn8x80dw-qtynw3q.png"><br><br>  Pour connecter Patroni √† Consul, il suffit d'√©tudier la documentation officielle, qui dit que vous devez sp√©cifier l'h√¥te au format http ou https, selon la fa√ßon dont nous travaillons avec Consul, et le sch√©ma de connexion, facultatif: <br><br><pre> <code class="plaintext hljs">host: the host:port for the Consul endpoint, in format: http(s)://host:port scheme: (optional) http or https, defaults to http</code> </pre> <br>  Cela semble simple, mais ici les √©cueils commencent.  Avec Consul, nous travaillons sur une connexion s√©curis√©e via https et notre configuration de connexion ressemblera √† ceci: <br><br><pre> <code class="python hljs">consul: host: https://server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br>  Mais √ßa ne marche pas.  Au d√©but, Patroni ne peut pas se connecter √† Consul, car il essaie quand m√™me de suivre http. <br><br>  Le code source de Patroni a aid√© √† r√©soudre le probl√®me.  Heureusement qu'il est √©crit en python.  Il s'av√®re que le param√®tre h√¥te n'est pas analys√© du tout et que le protocole doit √™tre sp√©cifi√© dans le sch√©ma.  Voici le bloc de configuration de travail pour travailler avec Consul avec nous: <br><br><pre> <code class="python hljs">consul: host: server.production.consul:<span class="hljs-number"><span class="hljs-number">8080</span></span> scheme: https verify: true cacert: {{ consul_cacert }} cert: {{ consul_cert }} key: {{ consul_key }}</code> </pre> <br><h3>  Mod√®le de consul </h3><br>  Nous avons donc choisi le stockage pour une configuration.  Vous devez maintenant comprendre comment PgBouncer changera de configuration lors du changement de leader dans le cluster Patroni.  La documentation ne r√©pond pas √† cette question, car  l√†, en principe, le travail avec PgBouncer n'est pas d√©crit. <br><br>  √Ä la recherche d'une solution, nous avons trouv√© un article (je ne me souviens pas du nom, malheureusement), o√π il √©tait √©crit que le mod√®le Consul avait beaucoup aid√© √† connecter PgBouncer et Patroni.  Cela nous a incit√©s √† √©tudier le travail du mod√®le de consul. <br><br>  Il s'est av√©r√© que le mod√®le Consul surveille en permanence la configuration du cluster PostgreSQL dans Consul.  Lorsque le leader change, il met √† jour la configuration de PgBouncer et envoie une commande pour le red√©marrer. <br><br><img src="https://habrastorage.org/webt/iv/_f/j-/iv_fj-sjbnqtfabqlnmu986sa0o.png"><br><br>  Le gros avantage du mod√®le est qu'il est stock√© sous forme de code, donc lors de l'ajout d'un nouveau fragment, il suffit de faire un nouveau commit et de mettre √† jour le mod√®le en mode automatique, prenant en charge le principe de l'infrastructure en tant que code. <br><br><h3>  Nouvelle architecture avec Patroni </h3><br>  En cons√©quence, nous avons obtenu ce sch√©ma de travail: <br><img src="https://habrastorage.org/webt/7b/-m/-v/7b-m-vyorrbbuognzt2qx2-fymm.png"><br><br>  Tous les serveurs d'applications acc√®dent √† l'√©quilibreur ‚Üí deux instances de PgBouncer sont derri√®re lui ‚Üí sur chaque instance, un mod√®le onsul est lanc√©, qui surveille l'√©tat de chaque cluster Patroni et surveille la pertinence de la configuration PgBouncer, qui envoie des demandes au leader actuel de chaque cluster. <br><br><h3>  Test manuel </h3><br>  Avant de lancer le programme, nous avons lanc√© ce circuit sur un petit environnement de test et v√©rifi√© le fonctionnement de la commutation automatique.  Ils ont ouvert le tableau, d√©plac√© l'autocollant et √† ce moment "tu√©" le chef du cluster.  Dans AWS, d√©sactivez simplement l'instance via la console. <br><br><img src="https://habrastorage.org/webt/ly/yf/aj/lyyfaj6bxodfoaqrsds5j00ycnw.gif"><br><br>  L'autocollant est revenu en 10 √† 20 secondes, puis a recommenc√© √† se d√©placer normalement.  Cela signifie que le cluster Patroni a fonctionn√© correctement: il a chang√© de leader, envoy√© les informations au consul, et le mod√®le de consul a imm√©diatement r√©cup√©r√© ces informations, remplac√© la configuration de PgBouncer et envoy√© la commande √† recharger. <br><br><h2>  Comment survivre sous une charge √©lev√©e et maintenir un temps d'arr√™t minimum? </h2><br>  Tout fonctionne tr√®s bien!  Mais de nouvelles questions se posent: comment fonctionnera-t-il sous une charge √©lev√©e?  Comment tout mettre en production rapidement et en toute s√©curit√©? <br><br>  L'environnement de test dans lequel nous effectuons les tests de charge nous aide √† r√©pondre √† la premi√®re question.  Il est compl√®tement identique √† la production en architecture et a g√©n√©r√© des donn√©es de test, qui sont approximativement √©gales en volume √† la production.  Nous d√©cidons de simplement ¬´tuer¬ª l'un des assistants PostgreSQL pendant le test et de voir ce qui se passe.  Mais avant cela, il est important de v√©rifier le roulement automatique, car dans cet environnement, nous avons plusieurs fragments PostgreSQL, nous obtiendrons donc d'excellents tests de scripts de configuration avant de vendre. <br><br>  Les deux t√¢ches semblent ambitieuses, mais nous avons PostgreSQL 9.6.  Peut-√™tre que nous passerons imm√©diatement √† 11.2? <br><br>  Nous d√©cidons de le faire en 2 √©tapes: premi√®re mise √† niveau vers 11.2, puis lancement de Patroni. <br><br><h3>  Mise √† jour PostgreSQL </h3><br>  Pour mettre √† niveau rapidement la version de PostgreSQL, vous devez utiliser l'option <b>-k</b> , qui cr√©e un lien dur sur le disque et il n'est pas n√©cessaire de copier vos donn√©es.  Sur des bases de 300 √† 400 Go, la mise √† jour prend 1 seconde. <br><br>  Nous avons beaucoup d'√©clats, donc la mise √† jour doit √™tre effectu√©e automatiquement.  Pour ce faire, nous avons √©crit le playbook Ansible, qui effectue le processus de mise √† jour complet pour nous: <br><br><pre> <code class="plaintext hljs">/usr/lib/postgresql/11/bin/pg_upgrade \ &lt;b&gt;--link \&lt;/b&gt; --old-datadir='' --new-datadir='' \ --old-bindir='' --new-bindir='' \ --old-options=' -c config_file=' \ --new-options=' -c config_file='</code> </pre> <br>  Il est important de noter ici qu'avant de d√©marrer la mise √† niveau, il est n√©cessaire de l'ex√©cuter avec le param√®tre <b>--check</b> pour √™tre s√ªr de la possibilit√© d'une mise √† niveau.  Notre script effectue √©galement la substitution des configurations pour la mise √† niveau.  Le script que nous avons termin√© en 30 secondes, c'est un excellent r√©sultat. <br><br><h3>  Lancer Patroni </h3><br>  Pour r√©soudre le deuxi√®me probl√®me, il suffit de regarder la configuration de Patroni.  Dans le r√©f√©rentiel officiel, il existe un exemple de configuration avec initdb, qui est responsable de l'initialisation d'une nouvelle base de donn√©es lors du premier lancement de Patroni.  Mais comme nous avons une base de donn√©es pr√™te √† l'emploi, nous venons de supprimer cette section de la configuration. <br><br>  Lorsque nous avons commenc√© √† installer Patroni sur un cluster PostgreSQL pr√™t √† l'emploi et √† l'ex√©cuter, nous avons √©t√© confront√©s √† un nouveau probl√®me: les deux serveurs ont commenc√© en tant que leader.  Patroni ne sait rien de l'√©tat initial du cluster et essaie de d√©marrer les deux serveurs en tant que deux clusters distincts portant le m√™me nom.  Pour r√©soudre ce probl√®me, supprimez le r√©pertoire de donn√©es sur l'esclave: <br><br><pre> <code class="plaintext hljs">rm -rf /var/lib/postgresql/</code> </pre> <br>  <b>Cela doit √™tre fait uniquement sur l'esclave!</b> <br><br>  Lors de la connexion d'une r√©plique propre, Patroni cr√©e un leader de sauvegarde de base et le restaure sur la r√©plique, puis rattrape l'√©tat actuel par wal-logs. <br><br>  Une autre difficult√© que nous avons rencontr√©e est que tous les clusters PostgreSQL sont appel√©s main par d√©faut.  Lorsque chaque cluster ne sait rien de l'autre, c'est normal.  Mais lorsque vous souhaitez utiliser Patroni, tous les clusters doivent avoir un nom unique.  La solution consiste √† changer le nom du cluster dans la configuration PostgreSQL. <br><br><h3>  Test de charge </h3><br>  Nous avons lanc√© un test qui simule le travail des utilisateurs sur les cartes.  Lorsque la charge a atteint notre valeur quotidienne moyenne, nous avons r√©p√©t√© exactement le m√™me test, nous avons d√©sactiv√© une instance avec le leader PostgreSQL.  Le basculement automatique a fonctionn√© comme pr√©vu: Patroni a chang√© de leader, Consul-template a mis √† jour la configuration de PgBouncer et envoy√© la commande √† recharger.  Selon nos graphiques de Grafana, il √©tait clair qu'il y avait des retards de 20 √† 30 secondes et une petite quantit√© d'erreurs des serveurs li√©es √† la connexion √† la base de donn√©es.  Il s'agit d'une situation normale, de telles valeurs sont valables pour notre basculement et certainement meilleures que le temps d'arr√™t du service. <br><br><h2>  Production de Patroni √† la production </h2><br>  En cons√©quence, nous avons obtenu le plan suivant: <br><br><ul><li>  D√©ployer Consul-template sur le serveur PgBouncer et lancer; </li><li>  Mises √† jour de PostgreSQL vers la version 11.2; </li><li>  Changement de nom de cluster; </li><li>  D√©marrage d'un cluster Patroni. </li></ul><br>  Dans le m√™me temps, notre sch√©ma vous permet de cr√©er le premier √©l√©ment √† presque n'importe quel moment, nous pouvons √† tour de r√¥le retirer chaque PgBouncer du travail et effectuer un d√©ploiement et un mod√®le de consul dessus.  Nous l'avons donc fait. <br><br>  Pour un roulement rapide, nous avons utilis√© Ansible, car nous avons d√©j√† v√©rifi√© tout le playbook dans un environnement de test, et le temps d'ex√©cution du script complet √©tait de 1,5 √† 2 minutes pour chaque fragment.  Nous pourrions tout d√©rouler alternativement pour chaque fragment sans arr√™ter notre service, mais nous devrons d√©sactiver chaque PostgreSQL pendant quelques minutes.  Dans ce cas, les utilisateurs dont les donn√©es se trouvent sur ce fragment ne pouvaient pas fonctionner pleinement √† ce moment, et cela est inacceptable pour nous. <br><br>  La solution √† cette situation a √©t√© l'entretien planifi√©, qui a lieu tous les 3 mois.  Il s'agit d'une fen√™tre pour le travail planifi√© lorsque nous d√©sactivons compl√®tement notre service et mettons √† jour les instances de base de donn√©es.  Il restait une semaine avant la fen√™tre suivante, et nous avons d√©cid√© d'attendre et de nous pr√©parer davantage.  Pendant l'attente, nous nous sommes √©galement assur√©s: pour chaque fragment PostgreSQL, nous avons g√©n√©r√© une r√©plique de rechange en cas d'√©chec de l'enregistrement des derni√®res donn√©es, et ajout√© une nouvelle instance pour chaque fragment, qui devrait devenir une nouvelle r√©plique dans le cluster Patroni, afin de ne pas ex√©cuter la commande de suppression des donn√©es .  Tout cela a permis de minimiser le risque d'erreur. <br><img src="https://habrastorage.org/webt/ps/ge/yh/psgeyhvrazg-zd1nrochwhl1hag.png"><br><br>  Nous avons red√©marr√© notre service, tout a fonctionn√© comme il se doit, les utilisateurs ont continu√© √† travailler, mais sur les graphiques, nous avons remarqu√© une charge anormalement √©lev√©e sur le serveur Consul. <br><img src="https://habrastorage.org/webt/uk/b9/gu/ukb9guaj4wfabq60v262qe098am.png"><br><br>  Pourquoi ne l'avons-nous pas vu dans l'environnement de test?  Ce probl√®me illustre tr√®s bien qu'il est n√©cessaire de suivre le principe de l'infrastructure en tant que code et d'affiner l'ensemble de l'infrastructure, en commen√ßant par les environnements de test et en terminant par la production.  Sinon, il est tr√®s facile d'obtenir le genre de probl√®me que nous avons rencontr√©.  Qu'est-il arriv√©?  Consul est apparu pour la premi√®re fois sur la production, puis sur les environnements de test. En cons√©quence, sur les environnements de test, la version de Consul √©tait sup√©rieure √† la production.  Juste dans l'une des versions, une fuite de processeur a √©t√© r√©solue lors de l'utilisation de consul-template.  Par cons√©quent, nous venons de mettre √† jour Consul, r√©solvant ainsi le probl√®me. <br><br><h3>  Red√©marrez le cluster Patroni </h3><br>  Cependant, nous avons eu un nouveau probl√®me dont nous n'√©tions m√™me pas conscients.  Lors de la mise √† jour de Consul, nous supprimons simplement le n≈ìud Consul du cluster √† l'aide de la commande consul Leave ‚Üí Patroni se connecte √† un autre serveur Consul ‚Üí tout fonctionne.  Mais lorsque nous avons atteint la derni√®re instance du cluster Consul et que nous lui avons envoy√© la commande cong√© consul, tous les clusters Patroni ont simplement red√©marr√© et dans les journaux, nous avons vu l'erreur suivante: <br><br><pre> <code class="plaintext hljs">ERROR: get_cluster Traceback (most recent call last): ... RetryFailedError: 'Exceeded retry deadline' ERROR: Error communicating with DCS &lt;b&gt;LOG: database system is shut down&lt;/b&gt;</code> </pre> <br>  Le cluster Patroni n'a pas pu obtenir d'informations sur son cluster et a red√©marr√©. <br><br>  Pour trouver une solution, nous avons contact√© les auteurs de Patroni via issue sur github.  Ils ont sugg√©r√© des am√©liorations √† nos fichiers de configuration: <br><br><pre> <code class="python hljs">consul: consul.checks: [] bootstrap: dcs: retry_timeout: <span class="hljs-number"><span class="hljs-number">8</span></span></code> </pre> <br>  Nous avons pu r√©p√©ter le probl√®me sur un environnement de test et y tester ces param√®tres, mais malheureusement, cela n'a pas fonctionn√©. <br><br>  Le probl√®me n'est toujours pas r√©solu.  Nous pr√©voyons d'essayer les solutions suivantes: <br><br><ul><li>  Utilisez Consul-agent sur chaque instance du cluster Patroni; </li><li>  Corrigez le probl√®me dans le code. </li></ul><br>  Nous comprenons l'endroit o√π l'erreur s'est produite: le probl√®me utilise probablement le d√©lai d'expiration par d√©faut, qui n'est pas remplac√© par le fichier de configuration.  Lorsque le dernier serveur Consul est supprim√© du cluster, l'ensemble du cluster Consul se bloque pendant plus d'une seconde, car Patroni ne peut pas obtenir l'√©tat du cluster et red√©marre compl√®tement l'ensemble du cluster. <br><br>  Heureusement, nous n'avons plus rencontr√© d'erreurs. <br><br><h2>  R√©sultats de l'utilisation de Patroni </h2><br>  Apr√®s le lancement r√©ussi de Patroni, nous avons ajout√© une r√©plique suppl√©mentaire dans chaque cluster.  Maintenant, dans chaque cluster, il y a un semblant de quorum: un leader et deux r√©pliques - pour assurer contre le cas de split-brain lors du changement. <br><img src="https://habrastorage.org/webt/ef/pn/er/efpner_0rhtuim1zt0gwrhff3b8.png"><br><br>  Patroni travaille √† la production depuis plus de trois mois.  Pendant ce temps, il a d√©j√† r√©ussi √† nous aider.  R√©cemment, le leader de l'un des clusters est d√©c√©d√© dans AWS, le basculement automatique a fonctionn√© et les utilisateurs ont continu√© √† travailler.  Patroni a termin√© sa t√¢che principale. <br><br>  <b>Un petit r√©sum√© de l'utilisation de Patroni:</b> <br><br><ul><li>  Facilit√© de changement de configuration.  Il suffit de modifier la configuration sur une seule instance et elle sera d√©ploy√©e sur l'ensemble du cluster.  Si un red√©marrage est n√©cessaire pour appliquer la nouvelle configuration, Patroni le signalera.  Patroni peut red√©marrer l'ensemble du cluster avec une seule commande, ce qui est √©galement tr√®s pratique. </li><li>  Le basculement automatique fonctionne et a d√©j√† r√©ussi √† nous aider. </li><li>  Mise √† jour PostgreSQL sans interruption de l'application.  Vous devez d'abord mettre √† niveau les r√©pliques vers la nouvelle version, puis changer le leader dans le cluster Patroni et mettre √† jour l'ancien leader.  Dans ce cas, le test n√©cessaire du basculement automatique se produit. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr457326/">https://habr.com/ru/post/fr457326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr457308/index.html">Sur le chemin de Sergey Pavlovich Korolev. Projet habit√© russe moderne. Partie 2. Rocket</a></li>
<li><a href="../fr457310/index.html">Biologie de la d√©pendance √† l'information</a></li>
<li><a href="../fr457312/index.html">Introduction √† la th√©orie des ensembles</a></li>
<li><a href="../fr457316/index.html">Comment le jeu de r√¥le dans le monde r√©el est organis√© pour les invit√©s d'Arm√©nie avec des voyages dans la moiti√© du pays</a></li>
<li><a href="../fr457324/index.html">√âv√©nements num√©riques √† Moscou du 24 au 30 juin</a></li>
<li><a href="../fr457328/index.html">Cat√©gories au lieu de r√©pertoires, ou le syst√®me de fichiers s√©mantique pour Linux</a></li>
<li><a href="../fr457330/index.html">Comment v√©rifier rapidement les avertissements int√©ressants donn√©s par l'analyseur PVS-Studio pour le code C et C ++?</a></li>
<li><a href="../fr457332/index.html">Comment voir rapidement les avertissements int√©ressants g√©n√©r√©s par l'analyseur PVS-Studio pour le code C et C ++?</a></li>
<li><a href="../fr457334/index.html">TacacsGUI, Configuration Manager</a></li>
<li><a href="../fr457336/index.html">Les cons√©quences du retrait pr√©matur√© des dents de sagesse</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>