<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòæ üöî üöâ Aber du sagst Ceph ... ist er so gut? üé≥ üë®üèΩ‚Äçüç≥ üßóüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ich liebe Ceph. Ich arbeite seit 4 Jahren mit ihm (0.80.x -  12.2.6  12.2.5). Manchmal bin ich so leidenschaftlich √ºber ihn, dass ich Abende und N√§cht...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aber du sagst Ceph ... ist er so gut?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/422905/"><p><img src="https://habrastorage.org/webt/fm/pp/3b/fmpp3bma4xf_j2pxwystnjgezc0.png"></p><br><p>  Ich liebe Ceph.  Ich arbeite seit 4 Jahren mit ihm (0.80.x - <del>  12.2.6 </del>  12.2.5).  Manchmal bin ich so leidenschaftlich √ºber ihn, dass ich Abende und N√§chte in seiner Gesellschaft verbringe und nicht mit meiner Freundin.  Ich habe verschiedene Probleme mit diesem Produkt festgestellt und lebe bis heute mit einigen.  Manchmal freute ich mich √ºber einfache Entscheidungen und manchmal tr√§umte ich davon, mich mit Entwicklern zu treffen, um meine Emp√∂rung auszudr√ºcken.  Aber Ceph wird immer noch in unserem Projekt verwendet und es ist m√∂glich, dass es zumindest von mir f√ºr neue Aufgaben verwendet wird.  In dieser Geschichte werde ich unsere Erfahrungen mit dem Betrieb von Ceph teilen, mich auf irgendeine Weise zum Thema √§u√üern, was mir an dieser L√∂sung nicht gef√§llt, und vielleicht denen helfen, die sie nur betrachten.  Die Ereignisse, die vor etwa einem Jahr begannen, als ich Dell EMC ScaleIO, jetzt als Dell EMC VxFlex OS bekannt, mitbrachte, veranlassten mich, diesen Artikel zu schreiben. </p><br><p> Dies ist keinesfalls eine Werbung f√ºr Dell EMC oder deren Produkt!  Pers√∂nlich bin ich nicht sehr gut mit gro√üen Unternehmen und Black Boxes wie VxFlex OS.  Aber wie Sie wissen, ist alles auf der Welt relativ und am Beispiel von VxFlex OS ist es sehr praktisch zu zeigen, was Ceph in Bezug auf den Betrieb ist, und ich werde versuchen, es zu tun. <a name="habracut"></a></p><br><h2 id="parametry-rech-idet-o-4-znachnyh-chislah">  Parameter  Es geht um 4-stellige Zahlen! </h2><br><p>  Ceph-Dienste wie MON, OSD usw.  haben verschiedene Parameter zum Einrichten aller Arten von Subsystemen.  Die Parameter werden in der Konfigurationsdatei festgelegt und von Daemons zum Zeitpunkt des Starts gelesen.  Einige Werte k√∂nnen bequem im laufenden Betrieb mithilfe des "Injektions" -Mechanismus ge√§ndert werden, der unten beschrieben wird.  Alles ist fast super, wenn Sie den Moment weglassen, in dem es Hunderte von Parametern gibt: <br><br>  Hammer: </p><br><pre><code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 863</code> </pre> <br><p>  Leuchtend: </p><br><pre> <code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 1401</code> </pre> <br><p>  Es stellt sich in zwei Jahren ~ 500 neue Parameter heraus.  Im Allgemeinen ist die Parametrisierung cool. Es ist nicht cool, dass es Schwierigkeiten gibt, 80% dieser Liste zu verstehen.  Die Dokumentation beschreibt nach meinen Sch√§tzungen ~ 20% und ist an einigen Stellen nicht eindeutig.  Ein Verst√§ndnis der Bedeutung der meisten Parameter muss im Github des Projekts oder in den Mailinglisten gefunden werden, aber dies hilft nicht immer. </p><br><p>  Hier ist ein Beispiel f√ºr einige Parameter, die mich erst k√ºrzlich interessiert haben. Ich habe sie im Blog einer Ceph-Gadfly gefunden: </p><br><pre> <code class="html hljs xml">throttler_perf_counter = false // enable/disable throttler perf counter osd_enable_op_tracker = false // enable/disable OSD op tracking</code> </pre> <br><p>  Codekommentare im Sinne von Best Practices.  Als ob ich die W√∂rter verstehe und sogar grob, worum es geht, aber was es mir geben wird, ist es nicht. </p><br><p>  Oder hier: <strong>osd_op_threads</strong> in Luminous war verschwunden und nur die Quellen halfen, einen neuen Namen zu finden: <strong>osd_peering_wq threads</strong> </p><br><p>  Mir gef√§llt auch, dass es besonders ganzheitliche M√∂glichkeiten gibt.  Hier zeigt der Typ, dass es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gut</a> ist, <strong>rgw_num _rados_handles zu erh√∂hen</strong> : </p><br><p>  und der andere Typ denkt, dass&gt; 1 unm√∂glich und sogar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gef√§hrlich ist</a> . </p><br><p>  Und meine Lieblingssache ist, wenn Anf√§nger in ihren Blog-Posts Beispiele f√ºr eine Konfiguration angeben, bei der alle Parameter gedankenlos (wie mir scheint) aus einem anderen Blog der gleichen Art kopiert werden und so eine Reihe von Parametern, von denen niemand au√üer dem Autor des Codes wei√ü, abwandern config zu config. </p><br><p>  Ich brenne auch nur wild mit dem, was sie in Luminous gemacht haben.  Es gibt eine super coole Funktion - Parameter im laufenden Betrieb √§ndern, ohne Prozesse neu zu starten.  Sie k√∂nnen beispielsweise den Parameter eines bestimmten OSD √§ndern: </p><br><pre> <code class="html hljs xml">&gt; ceph tell osd.12 injectargs '--filestore_fd_cache_size=512'</code> </pre> <br><p>  oder setzen Sie '*' anstelle von 12 und der Wert wird auf allen OSDs ge√§ndert.  Es ist wirklich sehr cool.  Aber wie viel in Ceph geschieht dies mit dem linken Fu√ü.  Bai Design k√∂nnen nicht alle Parameterwerte im laufenden Betrieb ge√§ndert werden.  Genauer gesagt k√∂nnen sie eingestellt werden und erscheinen in der Ausgabe ge√§ndert, aber tats√§chlich werden nur wenige erneut gelesen und erneut angewendet.  Beispielsweise k√∂nnen Sie die Gr√∂√üe des Thread-Pools nicht √§ndern, ohne den Prozess neu zu starten.  Damit der Team-Executor versteht, dass es sinnlos ist, den Parameter auf diese Weise zu √§ndern, haben sie beschlossen, eine Nachricht zu drucken.  Hallo. </p><br><p>  Zum Beispiel: </p><br><pre> <code class="html hljs xml">&gt; ceph tell mon.* injectargs '--mon_allow_pool_delete=true' mon.c: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.a: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.b: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</code> </pre> <br><p>  Mehrdeutig.  Tats√§chlich wird das Entfernen von Pools nach der Injektion m√∂glich.  Das hei√üt, diese Warnung ist f√ºr diesen Parameter nicht relevant.  Ok, aber es gibt immer noch Hunderte von Parametern, einschlie√ülich sehr n√ºtzlicher, die ebenfalls eine Warnung enthalten, und es gibt keine M√∂glichkeit, ihre tats√§chliche Anwendbarkeit zu √ºberpr√ºfen.  Im Moment kann ich anhand des Codes nicht einmal verstehen, welche Parameter nach der Injektion angewendet werden und welche nicht.  Um die Zuverl√§ssigkeit zu gew√§hrleisten, m√ºssen Sie die Dienste neu starten. Dies macht Sie w√ºtend.  W√ºtend, weil ich wei√ü, dass es einen Injektionsmechanismus gibt. </p><br><p>  Was ist mit VxFlex OS?  √Ñhnliche Prozesse wie MON (in VxFlex ist es MDM), OSD (SDS in VxFlex) haben auch Konfigurationsdateien, in denen es Dutzende von Parametern f√ºr alle gibt.  Zwar sagen ihre Namen auch nichts aus, aber die gute Nachricht ist, dass wir nie auf sie zur√ºckgegriffen haben, um so viel zu brennen wie bei Ceph. </p><br><h2 id="tehnicheskiy-dolg">  Technische Schulden </h2><br><p>  Wenn Sie Ihre Bekanntschaft mit Ceph mit der aktuellsten Version f√ºr heute beginnen, scheint alles in Ordnung zu sein, und Sie m√∂chten einen positiven Artikel schreiben.  Aber wenn Sie mit ihm in der Prod von Version 0.80 leben, dann sieht nicht alles so rosig aus. </p><br><p>  Vor Jewel wurden Ceph-Prozesse als Root ausgef√ºhrt.  Jewel entschied, dass sie vom Benutzer 'ceph' aus arbeiten sollten, und dies erforderte einen Eigent√ºmerwechsel f√ºr alle Verzeichnisse, die von Ceph-Diensten verwendet werden.  Es scheint, dass dies?  Stellen Sie sich ein OSD vor, das eine 2-TB-SATA-Magnetplatte mit voller Kapazit√§t bedient.  Das Parallelschalten einer solchen Festplatte (zu verschiedenen Unterverzeichnissen) bei voller Festplattenauslastung dauert also 3-4 Stunden.  Stellen Sie sich zum Beispiel vor, Sie haben dreihundert solcher Festplatten.  Selbst wenn Sie die Knoten aktualisieren (sofort 8-12 Festplatten), erhalten Sie ein ziemlich langes Update, bei dem der Cluster √ºber OSD verschiedener Versionen verf√ºgt und ein Replikat von Daten zum Zeitpunkt der Serveraktualisierung weniger ist.  Im Allgemeinen hielten wir es f√ºr absurd, bauten Ceph-Pakete neu auf und lie√üen OSD als Root laufen.  Wir haben beschlossen, das OSD beim Eingeben oder Ersetzen an einen neuen Benutzer zu √ºbertragen.  Jetzt √§ndern wir 2-3 Laufwerke pro Monat und f√ºgen 1-2 hinzu. Ich denke, wir k√∂nnen das bis 2022 schaffen. </p><br><p>  CRUSH Tunables </p><br><p>  <strong>CRUSH</strong> ist das Herz von Ceph, alles dreht sich darum.  Dies ist der Algorithmus, mit dem pseudozuf√§llig der Datenort ausgew√§hlt wird und dank dessen die mit dem RADOS-Cluster arbeitenden Clients herausfinden, auf welchem ‚Äã‚ÄãOSD die ben√∂tigten Daten (Objekte) gespeichert sind.  Das Hauptmerkmal von CRUSH ist, dass keine Metadatenserver wie Lustre oder IBM GPFS (jetzt Spectrum Scale) erforderlich sind.  Mit CRUSH k√∂nnen Clients und OSD direkt miteinander interagieren.  Nat√ºrlich ist es schwierig, die primitiven RADOS-Objektspeicher- und Dateisysteme zu vergleichen, die ich als Beispiel gegeben habe, aber ich denke, die Idee ist klar. </p><br><p>  CRUSH-Tunables wiederum sind eine Reihe von Parametern / Flags, die sich auf den Betrieb von CRUSH auswirken und es zumindest theoretisch effizienter machen. </p><br><p>  Beim Upgrade von Hammer auf Jewel (Test nat√ºrlich) wurde eine Warnung angezeigt, die besagt, dass das Tunables-Profil Parameter enth√§lt, die f√ºr die aktuelle Version (Jewel) nicht optimal sind, und es wird empfohlen, das Profil auf das optimale Profil umzuschalten.  Im Allgemeinen ist alles klar.  Das Dock sagt, dass dies sehr wichtig und der richtige Weg ist, aber es wird auch gesagt, dass nach dem Datenwechsel eine Rebellion von 10% der Daten auftritt.  10% - es klingt nicht be√§ngstigend, aber wir haben beschlossen, es zu testen.  Bei einem Cluster ist es ungef√§hr zehnmal weniger als bei einem Produkt. Bei der gleichen Anzahl von PGs pro OSD, die mit Testdaten gef√ºllt sind, haben wir eine Rebellion von 60%!  Stellen Sie sich zum Beispiel vor, mit 100 TB Daten beginnen sich 60 TB zwischen OSDs zu bewegen, und dies bei st√§ndig steigender Clientlast, die Latenz erfordert!  Wenn ich es noch nicht gesagt habe, bieten wir s3 an und wir haben auch nachts nicht viel weniger Last auf rgw, von denen es unter statischen Websites 8 und 4 mehr gibt.  Im Allgemeinen haben wir entschieden, dass dies nicht unser Weg ist, zumal ein solcher Umbau der neuen Version, mit der wir nicht an dem Produkt gearbeitet hatten, zumindest zu optimistisch war.  Dar√ºber hinaus hatten wir gro√üe Bucket-Indizes, die nur sehr schlecht neu erstellt werden, und dies war auch der Grund f√ºr die Verz√∂gerung beim Umschalten des Profils.  √úber die Indizes wird separat etwas niedriger sein.  Am Ende haben wir einfach die Warnung entfernt und beschlossen, sp√§ter darauf zur√ºckzukommen. </p><br><p>  Beim Wechseln des Profils beim Testen fielen Cephfs-Clients in den CentOS 7.2-Kerneln aus, weil sie mit dem neueren Hashing-Algorithmus des neuen Profils nicht arbeiten konnten.  Wir verwenden keine Cephfs im Produkt, aber wenn wir es fr√ºher getan h√§tten, w√§re dies ein weiterer Grund, das Profil nicht zu wechseln. </p><br><p>  Das Dock sagt √ºbrigens, dass Sie das Profil zur√ºcksetzen k√∂nnen, wenn das, was w√§hrend des Aufstands passiert, nicht zu Ihnen passt.  Nach einer Neuinstallation der Hammer-Version und einem Upgrade auf Jewel sieht das Profil folgenderma√üen aus: </p><br><pre> <code class="html hljs xml">&gt; ceph osd crush show-tunables { ... "straw_calc_version": 1, "allowed_bucket_algs": 22, "profile": "unknown", "optimal_tunables": 0, ... }</code> </pre> <br><p>  Es ist wichtig, dass es "unbekannt" ist. Wenn Sie versuchen, den Wiederaufbau zu stoppen, indem Sie ihn auf "Legacy" (wie im Dock angegeben) oder sogar auf "Hammer" umstellen, h√∂rt der Rebellance nicht auf, sondern setzt sich nur in √úbereinstimmung mit anderen Tunables fort und nicht " optimal. "  Im Allgemeinen muss alles gr√ºndlich √ºberpr√ºft und doppelt √ºberpr√ºft werden, Ceph ist nicht vertrauensw√ºrdig. </p><br><p>  CRUSH Trade-of </p><br><p>  Wie Sie wissen, ist alles auf dieser Welt ausgeglichen und Nachteile werden auf alle Vorteile angewendet.  Der Nachteil von CRUSH besteht darin, dass PGs auch bei gleichem Gewicht ungleichm√§√üig auf verschiedene OSDs verteilt sind.  Au√üerdem hindert nichts verschiedene PGs daran, mit unterschiedlichen Geschwindigkeiten zu wachsen, w√§hrend die Hash-Funktion abf√§llt.  Insbesondere haben wir eine OSD-Auslastung von 48-84%, obwohl sie die gleiche Gr√∂√üe und das gleiche Gewicht haben.  Wir versuchen sogar, Server gleich schwer zu machen, aber das ist so, nur unser Perfektionismus, nicht mehr.  Angesichts der Tatsache, dass E / A ungleichm√§√üig auf die Festplatten verteilt ist, ist das Schlimmste, dass bei Erreichen des vollst√§ndigen Status (95%) von mindestens einem OSD im Cluster die gesamte Aufzeichnung gestoppt wird und der Cluster schreibgesch√ºtzt wird.  Der ganze Cluster!  Und es spielt keine Rolle, dass der Cluster immer noch voller Speicherplatz ist.  Alles, das Finale, kommt raus!  Dies ist ein architektonisches Merkmal von CRUSH.  Stellen Sie sich vor, Sie sind im Urlaub, einige OSDs haben die Marke von 85% √ºberschritten (standardm√§√üig die erste Warnung) und Sie haben 10% auf Lager, um zu verhindern, dass die Aufzeichnung gestoppt wird.  Und 10% bei aktiver Aufnahme sind nicht so lang.  Idealerweise ben√∂tigt Ceph bei einem solchen Design eine diensthabende Person, die in solchen F√§llen die vorbereiteten Anweisungen befolgen kann. </p><br><p>  Deshalb haben wir beschlossen, die Daten im Cluster aus dem Gleichgewicht zu bringen, weil  Mehrere OSDs lagen nahe an der Vollwertmarke (85%). </p><br><p>  Es gibt verschiedene M√∂glichkeiten: </p><br><ul><li>  Laufwerke hinzuf√ºgen </li></ul><br><p>  Der einfachste Weg ist etwas verschwenderisch und nicht sehr effektiv, weil  Die Daten selbst bewegen sich m√∂glicherweise nicht aus dem √ºberf√ºllten OSD, oder die Bewegung ist vernachl√§ssigbar. </p><br><ul><li>  √Ñndern Sie das permanente Gewicht des OSD (GEWICHT) </li></ul><br><p>  Dies f√ºhrt zu einer √Ñnderung des Gewichts aller h√∂heren Bucket-Hierarchien (CRUSH-Terminologie), des OSD-Servers, des Rechenzentrums usw.  und infolgedessen auf die Bewegung von Daten, auch nicht von jenen OSD, von denen es notwendig ist. <br>  Wir haben versucht, das Gewicht eines OSD zu reduzieren, nachdem die Daten neu erstellt wurden, haben wir es reduziert, dann das dritte und wir haben festgestellt, dass wir dies f√ºr eine lange Zeit spielen w√ºrden. </p><br><ul><li>  Nicht permanentes OSD-Gewicht √§ndern (REWEIGHT) </li></ul><br><p>  Dies geschieht durch Aufrufen von "ceph osd reweight-by-usage".  Dies f√ºhrt zu einer √Ñnderung des sogenannten OSD-Einstellgewichts, und das Gewicht des h√∂heren Eimers √§ndert sich nicht.  Infolgedessen werden die Daten sozusagen zwischen verschiedenen OSDs eines Servers ausgeglichen, ohne die Grenzen des CRUSH-Buckets zu √ºberschreiten.  Dieser Ansatz hat uns sehr gut gefallen. Wir haben uns im Trockenlauf angesehen, welche √Ñnderungen am Produkt vorgenommen werden sollen.  Alles war in Ordnung, bis der Aufstandsprozess in der Mitte einen Einsatz bekam.  Wieder googeln, Newsletter lesen, mit verschiedenen Optionen experimentieren und am Ende stellt sich heraus, dass der Stopp durch das Fehlen einiger Tunables im oben genannten Profil verursacht wurde.  Wieder waren wir in technische Schulden verwickelt.  Infolgedessen gingen wir den Weg des Hinzuf√ºgens von Festplatten und des ineffektivsten Wiederaufbaus.  Zum Gl√ºck mussten wir das noch tun, weil  Es war geplant, das CRUSH-Profil mit einem ausreichenden Kapazit√§tsspielraum zu wechseln. </p><br><p>  Ja, wir kennen den Balancer (Luminous and Higher), der Teil von mgr ist und das Problem der ungleichm√§√üigen Datenverteilung l√∂sen soll, indem PG beispielsweise nachts zwischen OSDs verschoben wird.  Aber ich habe noch keine positiven Kritiken √ºber seine Arbeit geh√∂rt, auch nicht im aktuellen Mimic. </p><br><p>  Sie werden wahrscheinlich sagen, dass technische Schulden nur unser Problem sind, und ich w√ºrde dem wahrscheinlich zustimmen.  Aber f√ºr vier Jahre mit Ceph im Produkt hatten wir nur eine Ausfallzeit s3 aufgezeichnet, die eine ganze Stunde dauerte.  Und dann lag das Problem nicht bei RADOS, sondern bei RGW, das nach Eingabe seiner Standard-100-Threads festgefahren war und die meisten Benutzer die Anforderungen nicht erf√ºllten.  Es war immer noch auf Hammer.  Meiner Meinung nach ist dies ein guter Indikator und wird dadurch erreicht, dass wir keine pl√∂tzlichen Bewegungen ausf√ºhren und bei Ceph eher skeptisch gegen√ºber allem sind. </p><br><h2 id="dikiy-gc">  Wild gc </h2><br><p>  Wie Sie wissen, ist das L√∂schen von Daten direkt von der Festplatte eine ziemlich anspruchsvolle Aufgabe. In fortgeschrittenen Systemen wird das L√∂schen verz√∂gert oder gar nicht ausgef√ºhrt.  Ceph ist auch ein fortschrittliches System, und im Fall von RGW werden beim L√∂schen eines s3-Objekts die entsprechenden RADOS-Objekte nicht sofort von der Festplatte gel√∂scht.  RGW markiert s3-Objekte als gel√∂scht, und ein separater gc-Stream l√∂scht Objekte direkt aus RADOS-Pools und wird dementsprechend von Datentr√§gern verschoben.  Nach der Aktualisierung auf Luminous √§nderte sich das Verhalten von gc merklich, es begann aggressiver zu arbeiten, obwohl die gc-Parameter gleich blieben.  Mit dem Wort merklich meine ich, dass wir anfingen zu sehen, wie gc an der externen √úberwachung des Dienstes auf Sprunglatenz arbeitet.  Dies wurde von einem hohen IO im rgw.gc.-Pool begleitet  Das Problem, mit dem wir konfrontiert sind, ist jedoch viel epischer als nur IO.  Wenn gc funktioniert, werden viele Protokolle des Formulars generiert: </p><br><pre> <code class="html hljs xml">0 <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">cls</span></span></span><span class="hljs-tag">&gt;</span></span> /builddir/build/BUILD/ceph-12.2.5/src/cls/rgw/cls_rgw.cc:3284: gc_iterate_entries end_key=1_01530264199.726582828</code> </pre> <br><p>  Wobei 0 am Anfang die Protokollierungsstufe ist, auf der diese Nachricht gedruckt wird.  Es gibt sozusagen keinen Weg, die Protokollierung unter Null zu senken.  Infolgedessen wurde in ein paar Stunden ~ 1 GB Protokolle von einem OSD in uns generiert, und alles w√§re in Ordnung gewesen, wenn die Ceph-Knoten nicht plattenlos gewesen w√§ren ... Wir laden das Betriebssystem √ºber PXE direkt in den Speicher und verwenden keine lokale Festplatte oder NFS, NBD f√ºr die Systempartition (/).  Es stellt sich heraus, zustandslose Server.  Nach einem Neustart wird der gesamte Status durch Automatisierung gerollt.  Wie es funktioniert, werde ich irgendwie in einem separaten Artikel beschreiben. Jetzt ist es wichtig, dass 6 GB Speicher f√ºr "/" zugewiesen werden, von denen ~ 4 normalerweise frei sind.  Wir senden alle Protokolle an Graylog und verwenden eine ziemlich aggressive Protokollrotationsrichtlinie. In der Regel treten keine Probleme mit dem Festplatten- / RAM-√úberlauf auf.  Aber wir waren nicht bereit daf√ºr, mit 12 OSDs f√ºllte sich der "/" - Server sehr schnell, die Teilnehmer reagierten nicht rechtzeitig auf den Ausl√∂ser in Zabbix und das OSD wurde gerade gestoppt, da kein Protokoll geschrieben werden konnte.  Infolgedessen haben wir die Intensit√§t von gc reduziert, das Ticket wurde nicht gestartet, weil  Es war bereits vorhanden, und wir haben cron ein Skript hinzugef√ºgt, in dem die OSD-Protokolle abgeschnitten werden, wenn ein bestimmter Betrag √ºberschritten wird, ohne auf die Protokollierung zu warten.  √úbrigens wurde der Protokollierungsgrad <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erh√∂ht</a> . </p><br><h2 id="placement-groups-i-hvalyonaya-masshtabiruemost">  Platzierungsgruppen und gelobte Skalierbarkeit </h2><br><p>  Meiner Meinung nach ist PG die am schwierigsten zu verstehende Abstraktion.  PG wird ben√∂tigt, um CRUSH effektiver zu machen.  Der Hauptzweck von PG besteht darin, Objekte zu gruppieren, um den Ressourcenverbrauch zu reduzieren, die Produktivit√§t und Skalierbarkeit zu erh√∂hen.  Objekte direkt und einzeln zu adressieren, ohne sie zu PG zu kombinieren, w√§re sehr teuer. </p><br><p>  Das Hauptproblem von PG besteht darin, die Anzahl f√ºr einen neuen Pool zu bestimmen.  Aus dem Ceph-Blog: </p><br><blockquote>  "Die Auswahl der richtigen Anzahl von PGs f√ºr Ihren Cluster ist ein bisschen schwarze Kunst - und ein Albtraum f√ºr die Benutzerfreundlichkeit." </blockquote><p>  Dies ist immer sehr spezifisch f√ºr eine bestimmte Installation und erfordert viel Nachdenken und Berechnung. </p><br><p>  Wichtige Empfehlungen: </p><br><ul><li>  Zu viele PGs auf dem OSD sind schlecht, und es wird zu viel Ressourcen f√ºr ihre Wartung und Bremsen w√§hrend des Neuausgleichs / der Wiederherstellung geben. </li><li>  Nur wenige PGs auf OSD sind schlecht, die Leistung leidet und OSDs werden ungleichm√§√üig gef√ºllt. </li><li>  Die Zahl PG muss ein Vielfaches von Grad 2 sein. Dies wird dazu beitragen, "Power of CRUSH" zu erhalten. </li></ul><br><p>  Und hier brennt es bei mir.  PGs sind weder im Volumen noch in der Anzahl der Objekte begrenzt.  Wie viele Ressourcen (in reellen Zahlen) werden ben√∂tigt, um ein PG zu warten?  Kommt es auf seine Gr√∂√üe an?  Kommt es auf die Anzahl der Replikate dieses PG an?  Sollte ich ein Dampfbad nehmen, wenn ich genug Speicher, schnelle CPUs und ein gutes Netzwerk habe? <br>  Au√üerdem m√ºssen Sie √ºber das zuk√ºnftige Wachstum des Clusters nachdenken.  Die PG-Nummer kann nicht reduziert, sondern nur erh√∂ht werden.  Gleichzeitig wird dies nicht empfohlen, da dies im Wesentlichen die Aufteilung eines Teils von PG in neue und wilde Umbauten zur Folge hat. </p><br><blockquote>  "Das Erh√∂hen der PG-Anzahl eines Pools ist eines der wirkungsvollsten Ereignisse in einem Ceph-Cluster und sollte nach M√∂glichkeit f√ºr Produktionscluster vermieden werden." </blockquote><p>  Daher m√ºssen Sie nach M√∂glichkeit sofort √ºber die Zukunft nachdenken. </p><br><p>  Ein echtes Beispiel. </p><br><p>  Ein Cluster von 3 Servern mit jeweils 14 x 2 TB OSD, insgesamt 42 OSDs.  Replik 3, n√ºtzlicher Ort ~ 28 TB.  Um unter S3 verwendet zu werden, m√ºssen Sie die Anzahl der PGs f√ºr den Datenpool und den Indexpool berechnen.  RGW verwendet mehr Pools, aber die beiden sind prim√§r. </p><br><p>  Wir gehen in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den PG-Rechner</a> (es gibt einen solchen Rechner), wir betrachten mit den empfohlenen 100 PG auf dem OSD nur 1312 PG.  Aber nicht alles ist so einfach: Wir haben eine Einf√ºhrung - der Cluster wird definitiv innerhalb eines Jahres dreimal wachsen, aber das Eisen wird etwas sp√§ter gekauft.  Wir erh√∂hen die "Ziel-PGs pro OSD" dreimal auf 300 und erhalten 4480 PG. </p><br><p><img src="https://habrastorage.org/webt/ce/o3/bo/ceo3boailgnmcx_2_w6un6_covi.png"></p><br><p>  Stellen Sie die Anzahl der PGs f√ºr die entsprechenden Pools ein - wir erhalten eine Warnung: Es sind zu viele PGs pro OSD ... eingetroffen.  Empfangen ~ 300 PG auf OSD mit einem Limit von 200 (Luminous).  Fr√ºher waren es √ºbrigens 300.  Und das Interessanteste ist, dass nicht alle unn√∂tigen PGs sp√§hen d√ºrfen, das hei√üt, dies ist nicht nur eine Warnung.  Aus diesem Grund glauben wir, dass wir alles richtig machen, Grenzen erh√∂hen, die Warnung ausschalten und weitermachen. </p><br><p>  Ein weiteres reales Beispiel ist interessanter. </p><br><p>  S3, 152 TB nutzbares Volumen, 252 OSD bei 1,81 TB, ~ 105 PG bei OSD.  Der Cluster wuchs allm√§hlich, alles war in Ordnung, bis mit den neuen Gesetzen in unserem Land ein Wachstum auf 1 PB, d. H. + ~ 850 TB, erforderlich war und gleichzeitig die Leistung aufrechterhalten werden muss, was jetzt f√ºr S3 ziemlich gut ist.  Angenommen, wir nehmen Festplatten mit 6 (5,7 Real) TB und ber√ºcksichtigen unter Ber√ºcksichtigung von Replik 3 + 447 OSD.  Unter Ber√ºcksichtigung der aktuellen erhalten wir 699 OSDs mit jeweils 37 PGs, und wenn wir unterschiedliche Gewichte ber√ºcksichtigen, stellt sich heraus, dass die alten OSDs nur ein Dutzend PGs haben.  Also sagst du mir, wie ertr√§glich das funktionieren wird?  Die Leistung eines Clusters mit einer anderen Anzahl von PGs ist ziemlich schwer synthetisch zu messen, aber die von mir durchgef√ºhrten Tests zeigen, dass f√ºr eine optimale Leistung 50 PG bis 2 TB OSD erforderlich sind.  Und was ist mit weiterem Wachstum?  Ohne die Anzahl der PG zu erh√∂hen, k√∂nnen Sie mit der Zuordnung von PG zu OSD 1: 1 fortfahren.  Vielleicht verstehe ich etwas nicht? </p><br><p>  Ja, Sie k√∂nnen einen neuen Pool f√ºr RGW mit der gew√ºnschten Anzahl von PGs erstellen und ihm eine separate S3-Region zuordnen.  Oder bauen Sie sogar einen neuen Cluster in der N√§he.  Aber Sie m√ºssen zugeben, dass dies alles Kr√ºcken sind.  Und es stellt sich heraus, dass es aufgrund seines Konzepts wie ein gut skalierbarer Ceph erscheint, PG skaliert mit Vorbehalten.  Sie m√ºssen entweder mit behinderten Vorings leben, um sich auf das Wachstum vorzubereiten, oder irgendwann alle Daten im Cluster neu erstellen oder nach Leistung punkten und mit dem leben, was passiert.  Oder alles durchgehen. </p><br><p>  Ich bin froh, dass die Entwickler von Ceph <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verstehen,</a> dass PG eine komplexe und √ºberfl√ºssige Abstraktion f√ºr den Benutzer ist und er es besser ist, nichts davon zu wissen. </p><br><blockquote>  "In Luminous haben wir wichtige Schritte unternommen, um eine der gebr√§uchlichsten Methoden, um Ihren Cluster in einen Graben zu treiben, endg√ºltig zu eliminieren. Wir freuen uns darauf, PGs schlie√ülich vollst√§ndig auszublenden, damit sie nicht etwas sind, was die meisten Benutzer jemals wissen m√ºssen oder m√ºssen." denke an ". </blockquote><p>  In vxFlex gibt es kein Konzept f√ºr PG oder Analoga.  Sie f√ºgen dem Pool einfach Festplatten hinzu und fertig.  Und so weiter bis zu 16 PB.  Stellen Sie sich vor, es muss nichts berechnet werden, es gibt keine Haufen von Status dieser PGs, die Festplatten werden w√§hrend des gesamten Wachstums gleichm√§√üig entsorgt.  Weil  Festplatten werden an vxFlex als Ganzes √ºbergeben (es gibt kein Dateisystem dar√ºber). Es gibt keine M√∂glichkeit, die F√ºlle zu beurteilen, und es gibt √ºberhaupt kein solches Problem.  Ich wei√ü nicht einmal, wie ich Ihnen vermitteln soll, wie angenehm es ist. </p><br><h2 id="nuzhno-zhdat-sp1">  "Ich muss auf SP1 warten" </h2><br><p>  Eine andere Geschichte von "Erfolg".  Wie Sie wissen, ist RADOS der primitivste Schl√ºsselwertspeicher.  S3, das auf RADOS implementiert ist, ist ebenfalls primitiv, aber immer noch etwas funktionaler. ,  S3      .  ,   , RGW       .   ‚Äî  RADOS-,      OSD.         .   ,               . OSD            down.   ,      ,   .  ,   scrub'          .      ,    -  503,      . </p><br><p> <strong>Bucket Index resharding</strong> ‚Äî  ,       (RADOS-) , ,    OSD,         . </p><br><p> ,  ,        Jewel        !  Hammer,      ..    -.       ? </p><br><p>  Hammer       20+  ,       ,     OSD     Graylog ,    .     , ..   IO   .    Luminous, ..         .    Luminous,    , .   ,      . IO    index-,   ,         .    ,  IO     ,      . ,     ‚Ä¶   ; <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> : </p><br><p>  ,      .       , ..           ,   . </p><br><p> ,  Hammer-&gt;Jewel   -   . OSD     -  .        ,    OSD       . </p><br><p>    ‚Äî   ,     ,       .   Hammer    s3,   .      ,  .       ,    ,      etag,   body,     .           .      ,     . Suspend    .   ""           .            ,        . </p><br><h2 id="holivary-na-temu-chisla-replik">      </h2><br><p>       ,    2 ‚Äî  ,         Cloudmouse. ,    Ceph, , . </p><br><p>  vxFlex OS   2    . ,             .       ,    .           ,         .        ,    ,    ,     Dell EMC. </p><br><h2 id="proizvoditelnost">  Leistung </h2><br><p>    .       ,       ?  Gute Frage. ,      .   ,     Ceph, vxFlex          .       -  .        ,               . </p><br><p>   9   ceph-devel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> : ,     CPU  (  Xeon'  !)   IOPS  All-NVMe   Ceph 12.2.7  bluestore. </p><br><p> ,       ,  "" Ceph    .    (  Hammer)         Ceph    ,        s3     .  ,  ScaleIO  Ceph RBD   .   Ceph,     ‚Äî      CPU.       RDMA  InfiniBand, jemalloc   . ,    10-20 ,       iops,      io, Ceph      . vxFlex          .    ‚Äî  Ceph  system time,  scaleio ‚Äî io wait. ,    bluestore,      ,    ,         -, ,     Ceph.    ScaleIO  .  ,      , Ceph           Dell EMC. </p><br><p> ,       ,         PG.        (),     IO. -   PG       IO,     ,      . ,               nearfull.       ,    . </p><br><p>  vxFlex     -   ,      .       (   ceph-volume),         ,     . </p><br><h2 id="scrub"> Scrub </h2><br><p> , .  , ,      Ceph. </p><br><p>             ,      . " "    ‚Äî   -    ,    . ,      2 TB     &gt;50%,       Ceph,     .            .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,       . </p><br><p>  vxFlex OS         ,    ,     .        ‚Äî bandwidth  .            .         ,        . </p><br><p> ,  , vxFlex     scrub-error. Ceph      2   . </p><br><h2 id="monitoring">  √úberwachung </h2><br><p> Luminous ‚Äî     .        .    MGR-     Zabbix              (3 ).       .   ,   ,  -         IO  ,     gc, .   ‚Äî   RGW . </p><br><p><img src="https://habrastorage.org/webt/ys/ya/xq/ysyaxqffjukjtgkv3ecnu0_q2ro.png"></p><br><p>       .     . <br>      S3,    "" : </p><br><p><img src="https://habrastorage.org/webt/ax/ge/u5/axgeu5iyyjami3qszjo97akj6oc.png"></p><br><p>   Ceph  , ,   ,       ,    . </p><br><p>  ,   eph   Graylog   GELF    .  , ,  OSD down, out, failed  .          , ,   OSD    down  ,     . </p><br><p><img src="https://habrastorage.org/webt/sd/hz/av/sdhzavl-jyjrmajz5zlsjamujzo.png"></p><br><p> - ,    OSD       heartbeat      failed (.  ).    <code>vm.zone_reclaim_mode=1</code>     NUMA. </p><br><p>     Ceph.  c vxFlex   .       : </p><br><p><img src="https://habrastorage.org/webt/hu/nz/e2/hunze2e6ucygc4anzyep2wku050.png"></p><br><p>     : </p><br><p><img src="https://habrastorage.org/webt/jv/fa/ey/jvfaeyd7ql3kalcrcabrckekuqg.png"></p><br><p>  IO    : </p><br><p><img src="https://habrastorage.org/webt/sy/vp/lq/syvplqbfmjtek_wii033vszoq9s.png"></p><br><p>              IO,      Ceph. </p><br><p>    : </p><br><p><img src="https://habrastorage.org/webt/h2/8j/ff/h28jff_jzpstuucf5wfx8vrrmay.png"></p><br><p>   Ceph,    Luminous     .   2.0,    Mimic  ,      . </p><br><h2 id="vxflex-tozhe-ne-idealen"> vxFlex    </h2><br><p>     <strong>Degraded state</strong> ,          . </p><br><p>  vxFlex ‚Äî        RH   .   7.5  , .  Ceph   RBD  cephfs ‚Äî          . </p><br><p> vxFlex       Ceph. vxFlex ‚Äî    ,   , , . </p><br><p>     16 PB,     .  eph     2 PB ‚Ä¶ </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  ,  Ceph       ,     ,      ,      Ceph ‚Äî .      . </p><br><p>       ,  Ceph       " ".       ,  "  ,   ,     R&amp;D,  - ".    .       " ",  Ceph      ,    ,     . </p><br><p>      Ceph  2k18  ,    .      24/7       ( S3, ,  EBS),             ,   Ceph    .  ,    .        ‚Äî       .         /     maintenance        backfilling  ,  c Ceph   , ,       . </p><br><p>      Ceph    ?  , "     ".      Ceph.    .      ,         ,   ,  ,    ‚Ä¶ <br>    ! <br>  HEALTH_OK! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de422905/">https://habr.com/ru/post/de422905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de422895/index.html">Google m√∂chte URLs t√∂ten</a></li>
<li><a href="../de422897/index.html">Schlecht, aber meins: wie man wirklich schreckliches CSS schreibt</a></li>
<li><a href="../de422899/index.html">Unter wachsamer Aufsicht: So √ºberwachen Sie Hoster-Tarife und halten den VPS-Katalog auf dem neuesten Stand</a></li>
<li><a href="../de422901/index.html">Ein Herzfrequenzmesser f√ºr Putin oder was ist ein Ritmer?</a></li>
<li><a href="../de422903/index.html">Wie und warum haben wir einen hoch geladenen skalierbaren Dienst f√ºr 1C geschrieben: Enterprise: Java, PostgreSQL, Hazelcast</a></li>
<li><a href="../de422907/index.html">Kurzreferenz f√ºr Roboterstaubsauger 2018</a></li>
<li><a href="../de422909/index.html">10 beliebtesten 404 Festival Retro Talk Videos</a></li>
<li><a href="../de422915/index.html">Ich suche einen Senior ohne B√ºro und Cookies: Wie haben wir eine Suche nach Mitarbeitern organisiert, die zu 100% entfernt sind?</a></li>
<li><a href="../de422917/index.html">Ich habe keinen Mund, aber ich muss schreien. √úberlegungen zu KI und Ethik</a></li>
<li><a href="../de422919/index.html">Fahrrad-SIPs und Cloud-Telefonie-Gespr√§che miteinander</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>