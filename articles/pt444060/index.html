<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë§ üò∫ üöº Migrando de Nagios para Icinga2 na Austr√°lia üë∞ üçå üë≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal. 


 Eu sou o administrador do sistema linux, mudei da R√∫ssia para a Austr√°lia com um visto profissional independente em 2015, mas o artig...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migrando de Nagios para Icinga2 na Austr√°lia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444060/"><p>  Ol√° pessoal. </p><br><p>  Eu sou o administrador do sistema linux, mudei da R√∫ssia para a Austr√°lia com um visto profissional independente em 2015, mas o artigo n√£o abordar√° como obter um leit√£o para um trator.  Esses artigos j√° s√£o suficientes (no entanto, se houver interesse, tamb√©m escreverei sobre isso), gostaria de falar sobre como, no meu trabalho na Austr√°lia como engenheiro de opera√ß√µes Linux, iniciei a migra√ß√£o de um sistema monitorando para outro.  Especificamente - Nagios =&gt; Icinga2. </p><br><p>  O artigo √© parcialmente t√©cnico e parcialmente sobre comunica√ß√£o com pessoas e problemas associados √† diferen√ßa de cultura e m√©todos de trabalho. </p><a name="habracut"></a><br><p>  Infelizmente, a tag "code" n√£o destaca o c√≥digo Puppet e yaml, ent√£o tive que usar "plaintext". </p><br><p>  Nada deu errado na manh√£ de 21 de dezembro de 2016.  Como sempre, li Habr com um an√¥nimo n√£o registrado na primeira meia hora do dia √∫til, absorvendo caf√© e me deparei com <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este artigo</a> . </p><br><p>  Como o Nagios foi usado na minha empresa, sem pensar duas vezes, criei um ticket no Redmine e joguei o link no chat geral, porque achei importante.  A iniciativa √© pun√≠vel at√© na Austr√°lia, ent√£o o engenheiro-chefe pendurou esse problema em mim, desde que eu o descobri. </p><br><div class="spoiler">  <b class="spoiler_title">Tela do Redmine</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/ge/uc/gu/geucgutwhu3_zu4uwes2ozuwa3e.jpeg"></div></div><br><p>  Em nosso departamento, antes de expressar nossa opini√£o, √© costume propor pelo menos uma alternativa, mesmo que a escolha seja √≥bvia, ent√£o comecei pesquisando quais sistemas de monitoramento em geral s√£o atualmente relevantes, j√° que na R√∫ssia, no √∫ltimo local em que trabalhei, eu tinha meu pr√≥prio sistema de grava√ß√£o pessoal, muito primitivo, mas, no entanto, bastante funcional e cumprindo todas as tarefas atribu√≠das a ele.  Python, Polit√©cnica de S√£o Petersburgo e a regra do metro.  N√£o, o metr√¥ √© p√©ssimo.  Isso √© pessoal (11 anos de trabalho) e merece um artigo separado, mas n√£o agora. </p><br><p>  Um pouco sobre as regras para fazer altera√ß√µes na configura√ß√£o da infraestrutura na minha localiza√ß√£o atual.  Utilizamos o Puppet, o Gitlab e o princ√≠pio de infraestrutura como c√≥digo, para que: </p><br><ul><li>  Nenhuma altera√ß√£o manual via SSH modificando manualmente os arquivos nas m√°quinas virtuais.  Por tr√™s anos de trabalho, recebi um chap√©u por isso muitas vezes, o √∫ltimo h√° uma semana, e acho que n√£o foi a √∫ltima vez.  Bem, na verdade - corrija uma linha na configura√ß√£o, reinicie o servi√ßo e veja se o problema foi resolvido - 10 segundos.  Crie uma nova ramifica√ß√£o no Gitlab, fa√ßa as altera√ß√µes, aguarde r10k funcionar no Puppetmaster, execute Puppet --environment = mybranch e aguarde mais alguns minutos at√© que tudo funcione - no m√≠nimo 5 minutos. </li><li>  Quaisquer altera√ß√µes s√£o feitas atrav√©s da cria√ß√£o de uma solicita√ß√£o de mesclagem no Gitlab e voc√™ precisa obter a aprova√ß√£o de pelo menos um membro da equipe.  Grandes mudan√ßas no l√≠der da equipe exigem duas ou tr√™s aprova√ß√µes. </li><li>  Todas as altera√ß√µes s√£o textuais de uma maneira ou de outra (como os manifestos do Puppet, os scripts e os dados Hiera s√£o textos), os bin√°rios s√£o altamente desencorajados e s√£o necess√°rias boas raz√µes para aprov√°-las. </li></ul><br><p>  Ent√£o, as op√ß√µes que eu olhei s√£o: </p><br><ul><li>  Munin - se houver mais de 10 servidores na infraestrutura, a administra√ß√£o se transformar√° em um inferno ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deste artigo</a> . Eu n√£o tinha muito desejo de verificar isso, ent√£o aceitei minha palavra). </li><li>  Zabbix - h√° muito tempo est√° de olho na R√∫ssia, mas era redundante para minhas tarefas.  Aqui - teve que ser descartado devido ao uso do Puppet como gerenciador de configura√ß√£o e do Gitlab como sistema de controle de vers√£o.  Nesse momento, pelo que entendi, o Zabbix armazena toda a configura√ß√£o em um banco de dados e, portanto, n√£o ficou claro como gerenciar a configura√ß√£o nas condi√ß√µes atuais e como rastrear altera√ß√µes. </li><li>  Prometheus √© o que chegaremos no final, a julgar pelo humor do departamento, mas naquela √©poca eu n√£o conseguia domin√°-lo e n√£o conseguia demonstrar uma amostra realmente funcional (Prova de Conceito), ent√£o tive que recusar. </li><li>  Havia v√°rias outras op√ß√µes que exigiam uma reformula√ß√£o completa do sistema ou estavam na inf√¢ncia / abandonadas e, pelo mesmo motivo, foram rejeitadas. </li></ul><br><p>  No final, decidi pelo Icinga2 por tr√™s raz√µes: </p><br><p>  1 - compatibilidade com o Nrpe (um servi√ßo de cliente que executa verifica√ß√µes nos comandos do Nagios).  Isso foi muito importante, porque naquela √©poca t√≠nhamos 135 (agora s√£o 165 em 2019) m√°quinas virtuais com um monte de servi√ßos / verifica√ß√µes auto-escritas e refazer tudo isso seria uma terr√≠vel hemorr√≥ida. <br>  2 - todos os arquivos de configura√ß√£o s√£o texto, o que facilita a edi√ß√£o deste assunto, cria solicita√ß√µes de mesclagem com a capacidade de ver o que foi adicionado ou exclu√≠do. <br>  3 √© um projeto OpenSource animado e em crescimento.  N√≥s gostamos muito do OpenSource e oferecemos uma contribui√ß√£o vi√°vel, criando solicita√ß√µes e problemas de pull para resolver problemas. </p><br><p>  Ent√£o vamos l√°, Icinga2. </p><br><p>  A primeira coisa que tive que enfrentar foi a in√©rcia dos meus colegas.  Todo mundo est√° acostumado a Nagios / Najios (embora mesmo aqui eles n√£o pudessem se comprometer em como pronunciar isso) e a interface CheckMK.  A interface do icinga parece completamente diferente (era um sinal de menos), mas √© poss√≠vel configurar de maneira flex√≠vel o que voc√™ precisa ver com filtros literalmente por qualquer par√¢metro (foi um plus, mas lutei por isso notavelmente). </p><br><div class="spoiler">  <b class="spoiler_title">Filtros</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/kx/bf/0e/kxbf0eqj-gh7bfkpgtzgjjgvhwm.jpeg"></div></div><br><p>  Estime a propor√ß√£o do tamanho da barra de rolagem para o tamanho do campo de rolagem. </p><br><p>  Segundo, todo mundo est√° acostumado a ver toda a infraestrutura em um monitor, porque o CheckMk permite que voc√™ trabalhe com v√°rios hosts do Nagios, mas a Icinga n√£o sabia como faz√™-lo (na verdade, sabia, mas mais sobre isso abaixo).  Uma alternativa era uma coisa chamada Thruk, mas seu design causava v√¥mito para todos os membros da equipe, exceto um - aquele que sugeriu (n√£o eu). </p><br><div class="spoiler">  <b class="spoiler_title">Thruk Firebox - Decis√£o un√¢nime da equipe</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/tv/1c/_l/tv1c_lkx-h9rbwgpdy63jdohnve.png"></div></div><br><p>  Ap√≥s alguns dias de uma tempestade cerebral, propus a id√©ia de monitoramento de cluster, quando h√° um host principal na zona de produ√ß√£o e dois subordinados - um em dev / test e um host externo localizado em outro provedor, a fim de monitorar nossos servi√ßos do ponto de vista de um cliente ou de um outsider observador.  Essa configura√ß√£o me permitiu ver todos os problemas em uma interface baseada na Web e funcionou para mim, mas o Puppet ... O problema com o Puppet era que o host principal agora precisava conhecer todos os hosts e servi√ßos / verifica√ß√µes no sistema e distribu√≠-los entre as zonas. (dev-test, staging-prod, ext), mas o envio de altera√ß√µes pela API Icinga leva alguns segundos, mas a compila√ß√£o do diret√≥rio Puppet de todos os servi√ßos para todos os hosts leva alguns minutos.  Isso ainda me foi atribu√≠do, embora eu j√° tenha explicado v√°rias vezes como tudo funciona e por que leva tanto tempo. </p><br><p>  Terceiro - um monte de SnowFlakes (flocos de neve) - coisas que s√£o eliminadas do sistema geral, porque t√™m algo especial, portanto as regras gerais n√£o se aplicam a elas.  Foi decidido por um ataque frontal - se houver um alarme, mas na verdade tudo estiver em ordem, ent√£o aqui voc√™ precisa se aprofundar e entender por que isso me alerta, embora n√£o deva.  Ou vice-versa - por que Nagios est√° em p√¢nico, mas Icinga n√£o. </p><br><p>  Quarto - Nagios trabalhou aqui para mim por tr√™s anos e, inicialmente, havia mais confian√ßa nele do que no meu novo sistema hipster, ent√£o toda vez que Icinga criava p√¢nico - ningu√©m fazia nada at√© Nagios se empolgar com o mesmo problema.  Mas muito raramente Icinga emitia alarmes reais mais cedo que Nagios, e considero isso um batente s√©rio, que discutirei na se√ß√£o "Conclus√µes". </p><br><p>  Como resultado, o comissionamento foi adiado por mais de 5 meses (planejado em 28 de junho de 2018, de fato - 3 de dezembro de 2018), principalmente por causa da ‚Äúverifica√ß√£o de paridade‚Äù - que porcaria quando existem v√°rios servi√ßos em Nagios dos quais ningu√©m est√° falando N√£o ouvi nada nos √∫ltimos dois anos, mas agora eles emitiram cr√≠ticas sem motivo e eu tive que explicar por que eles n√£o estavam no meu painel e tive que adicion√°-los ao Icinga para "a verifica√ß√£o de paridade est√° conclu√≠da" (todos os servi√ßos / verifica√ß√µes) em Nagios correspondem a servi√ßos / cheques em Icinga) </p><br><p>  Implementa√ß√£o: <br>  O primeiro √© a guerra de c√≥digos versus dados, como o Puppet Style.  Todos os dados, bem aqui tudo em geral, devem estar em Hiera e nada mais.  Todo o c√≥digo est√° em arquivos .pp.  Vari√°veis, abstra√ß√µes, fun√ß√µes - tudo sai em pp. <br>  Como resultado, temos v√°rias m√°quinas virtuais (165 no momento da reda√ß√£o) e 68 aplicativos da web que precisam ser monitorados quanto √† integridade e validade dos certificados SSL.  Por√©m, devido a hemorr√≥idas hist√≥ricas, as informa√ß√µes para monitorar aplicativos s√£o obtidas de um reposit√≥rio separado do gitlab e o formato dos dados n√£o mudou desde o Puppet 3, o que cria dificuldades adicionais de configura√ß√£o. </p><br><div class="spoiler">  <b class="spoiler_title">C√≥digo de marionete para aplica√ß√µes, tome cuidado</b> <div class="spoiler_text"><pre><code class="plaintext hljs">define profiles::services::monitoring::docker_apps( Hash $app_list, Hash $apps_accessible_from, Hash $apps_access_list, Hash $webhost_defaults, Hash $webcheck_defaults, Hash $service_overrides, Hash $targets, Hash $app_checks, ) { #### APPS #### $zone = $name $app_list.each | String $app_name, Hash $app_data | { $notify_group = { 'notify_group' =&gt; ($webcheck_defaults[$zone]['notify_group'] + pick($app_data['notify_group'], {} )) } # adds notifications for default group (systems) + any group defined in int/pm_docker_apps.eyaml $data = merge($webhost_defaults, $apps_accessible_from, $app_data) $site_domain = $app_data['site_domain'] $regexp = pick($app_data['check_regex'], 'html') # Pick a regex to check $check_url = $app_data['check_url'] ? { undef =&gt; { 'http_uri' =&gt; '/' }, default =&gt; { 'http_uri' =&gt; $app_data['check_url'] } } $check_regex = $regexp ?{ 'absent' =&gt; {}, default =&gt; {'http_expect_body_regex' =&gt; $regexp} } $site_domain.each | String $vhost, Hash $vdata | { # Split an app by domains if there are two or more $vhost_name = {'http_vhost' =&gt; $vhost} $vars = $data['vars'] + $vhost_name + $check_regex + $check_url $web_ipaddress = is_array($vdata['web_ipaddress']) ? { # Make IP-address an array if it's not, because askizzy has 2 ips and it's an array true =&gt; $vdata['web_ipaddress'], false =&gt; [$vdata['web_ipaddress']], } $access_from_zones = [$zone] + $apps_access_list[$data['accessible_from']] # Merge default zone (where the app is defined) and extra zones if they exist $web_ipaddress.each | String $ip_address | { # For each IP (if we have multiple) $suffix = length($web_ipaddress) ? { # If we have more than one - add IP as a suffix to this hostname to avoid duplicating resources 1 =&gt; '', default =&gt; "_${ip_address}" } $octets = split($ip_address, '\.') $ip_tag = "${octets[2]}.${octets[3]}" # Using last octet only causes a collision between nginx-vip 203.15.70.94 and ext. ip 49.255.194.94 $access_from_zones.each | $zone_prefix |{ $zone_target = $targets[$zone_prefix] $nginx_vip_name = "${zone_prefix}_nginx-vip-${ip_tag}" # If it's a host for ext - prefix becomes 'ext_' (ext_nginx-vip...) $nginx_host_vip = { $nginx_vip_name =&gt; { ensure =&gt; present, target =&gt; $zone_target, address =&gt; $ip_address, check_command =&gt; 'hostalive', groups =&gt; ['nginx_vip',], } } $ssl_vars = $app_checks['ssl'] $regex_vars = $app_checks['http'] + $vars + $webcheck_defaults[$zone] + $notify_group if !defined( Profiles::Services::Monitoring::Host[$nginx_vip_name] ) { ensure_resources('profiles::services::monitoring::host', $nginx_host_vip) } if !defined( Icinga2::Object::Service["${nginx_vip_name}_ssl"] ) { icinga2::object::service {"${nginx_vip_name}_ssl": ensure =&gt; $data['ensure'], assign =&gt; ["host.name == $nginx_vip_name",], groups =&gt; ['webchecks',], check_command =&gt; 'ssl', check_interval =&gt; $service_overrides['ssl']['check_interval'], target =&gt; $targets['services'], apply =&gt; true, vars =&gt; $ssl_vars } } if $regexp != 'absent'{ if !defined(Icinga2::Object::Service["${vhost}${$suffix} regex"]){ icinga2::object::service {"${vhost}${$suffix} regex": ensure =&gt; $data['ensure'], assign =&gt; ["match(*_nginx-vip-${ip_tag}, host.name)",], groups =&gt; ['webchecks',], check_command =&gt; 'http', check_interval =&gt; $service_overrides['regex']['check_interval'], target =&gt; $targets['services'], enable_flapping =&gt; true, apply =&gt; true, vars =&gt; $regex_vars } } } } } } } }</code> </pre> </div></div><br><p>  O c√≥digo de configura√ß√£o do host e do servi√ßo tamb√©m parece horr√≠vel: </p><br><div class="spoiler">  <b class="spoiler_title">Monitoring / config.pp</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">class profiles::services::monitoring::config( Array $default_config, Array $hostgroups, Hash $hosts = {}, Hash $host_defaults, Hash $services, Hash $service_defaults, Hash $service_overrides, Hash $webcheck_defaults, Hash $servicegroups, String $servicegroup_target, Hash $user_defaults, Hash $users, Hash $oncall, Hash $usergroup_defaults, Hash $usergroups, Hash $notifications, Hash $notification_defaults, Hash $notification_commands, Hash $timeperiods, Hash $webhost_defaults, Hash $apps_access_list, Hash $check_commands, Hash $hosts_api = {}, Hash $targets = {}, Hash $host_api_defaults = {}, ) { # Profiles::Services::Monitoring::Hostgroup &lt;&lt;| |&gt;&gt; # will be enabled when we move to icinga completely #### APPS #### case $location { 'int', 'ext': { $apps_by_zone = {} } 'pm': { $int_apps = hiera('int_docker_apps') $int_app_defaults = hiera('int_docker_app_common') $st_apps = hiera('staging_docker_apps') $srs_apps = hiera('pm_docker_apps_srs') $pm_apps = hiera('pm_docker_apps') + $st_apps + $srs_apps $pm_app_defaults = hiera('pm_docker_app_common') $apps_by_zone = { 'int' =&gt; $int_apps, 'pm' =&gt; $pm_apps, } $app_access_by_zone = { 'int' =&gt; {'accessible_from' =&gt; $int_app_defaults['accessible_from']}, 'pm' =&gt; {'accessible_from' =&gt; $pm_app_defaults['accessible_from']}, } } default: { fail('Please ensure the node has $location fact set (int, pm, ext)') } } file { '/etc/icinga2/conf.d/': ensure =&gt; directory, recurse =&gt; true, purge =&gt; true, owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0750', notify =&gt; Service['icinga2'], } $default_config.each | String $file_name |{ file {"/etc/icinga2/conf.d/${file_name}": ensure =&gt; present, source =&gt; "puppet:///modules/profiles/services/monitoring/default_config/${file_name}", owner =&gt; 'icinga', group =&gt; 'icinga', mode =&gt; '0640', } } $app_checks = { 'ssl' =&gt; $services['webchecks']['checks']['ssl']['vars'], 'http' =&gt; $services['webchecks']['checks']['http_regexp']['vars'] } $apps_by_zone.each | String $zone, Hash $app_list | { profiles::services::monitoring::docker_apps{$zone: app_list =&gt; $app_list, apps_accessible_from =&gt; $app_access_by_zone[$zone], apps_access_list =&gt; $apps_access_list, webhost_defaults =&gt; $webhost_defaults, webcheck_defaults =&gt; $webcheck_defaults, service_overrides =&gt; $service_overrides, targets =&gt; $targets, app_checks =&gt; $app_checks, } } #### HOSTS #### # Profiles::Services::Monitoring::Host &lt;&lt;| |&gt;&gt; # This is for spaceship invasion when it's ready. $hosts_has_large_disks = query_nodes('mountpoints.*.size_bytes &gt;= 1099511627776') $hosts.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_notify_group}, $vars_has_large_disks)) # Merging vars separately $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) profiles::services::monitoring::host{$host_name: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; $host_data['target'], check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } if !empty($hosts_api){ # All hosts managed by API $hosts_api.each | String $zone, Hash $hosts_api_zone | { # Split api hosts by zones $hosts_api_zone.each | String $hostgroup, Hash $list_of_hosts_with_settings | { # Splitting site lists by hostgroups - docker_host/gluster_host/etc $list_of_hosts_in_group = $list_of_hosts_with_settings['hosts'] $hostgroup_settings = $list_of_hosts_with_settings['settings'] $merged_hostgroup_settings = deep_merge($host_api_defaults, $list_of_hosts_with_settings['settings']) $list_of_hosts_in_group.each | String $host_name, Hash $host_settings |{ # Splitting grouplists by hosts # Is this host in the array $hosts_has_large_disks ? If so set host.vars.has_large_disks if ( $hosts_has_large_disks.reduce(false) | $found, $value| { ( $value =~ "^${host_name}" ) or $found } ) { $vars_has_large_disks = { 'has_large_disks' =&gt; true } } else { $vars_has_large_disks = {} } $host_data = deep_merge($merged_hostgroup_settings, $host_settings) $hostgroup_settings_vars = pick($hostgroup_settings['vars'], {}) $host_settings_vars = pick($host_settings['vars'], {}) $host_api_notify_group = delete_undef_values($host_defaults['vars']['notify_group'] + $hostgroup_settings_vars['notify_group'] + $host_settings_vars['notify_group']) $host_data_vars = delete_undef_values(deep_merge($host_data['vars'] , {'notify_group' =&gt; $host_api_notify_group}, $vars_has_large_disks)) $hostgroups = delete_undef_values([$hostgroup] + $host_data['groups']) if defined(Profiles::Services::Monitoring::Host[$host_name]){ $hostname = "${host_name}_from_${zone}" } else { $hostname = $host_name } profiles::services::monitoring::host{$hostname: ensure =&gt; $host_data['ensure'], display_name =&gt; $host_data['display_name'], address =&gt; $host_data['address'], groups =&gt; $hostgroups, target =&gt; "${host_data['target_base']}/${zone}/hosts.conf", check_command =&gt; $host_data['check_command'], check_interval =&gt; $host_data['check_interval'], max_check_attempts =&gt; $host_data['max_check_attempts'], vars =&gt; $host_data_vars, template =&gt; $host_data['template'], } } } } } #### END OF HOSTS #### #### SERVICES #### $services.each | String $service_group, Hash $s_list |{ # Service_group and list of services in that group $service_list = $s_list['checks'] # List of actual checks, separately from SG settings $service_list.each | String $service_name, Hash $data |{ $merged_defaults = merge($service_defaults, $s_list['settings']) # global service defaults + service group defaults $merged_data = merge($merged_defaults, $data) $settings_vars = pick($s_list['settings']['vars'], {}) $this_service_vars = pick($data['vars'], {}) $all_service_vars = delete_undef_values($service_defaults['vars'] + $settings_vars + $this_service_vars) # If we override default check_timeout, but not nrpe_timeout, make nrpe_timeout the same as check_timeout if ( $merged_data['check_timeout'] and ! $this_service_vars['nrpe_timeout'] ) { # NB: Icinga will convert 1m to 60 automatically! $nrpe = { 'nrpe_timeout' =&gt; $merged_data['check_timeout'] } } else { $nrpe = {} } # By default we use nrpe and all commands are run via nrpe. So vars.nrpe_command = $service_name is a default value # If it's server-side Icinga command - we don't need 'nrpe_command' # but there is no harm to have that var and the code is shorter if $merged_data['check_command'] == 'nrpe'{ $check_command = $merged_data['vars']['nrpe_command'] ? { undef =&gt; { 'nrpe_command' =&gt; $service_name }, default =&gt; { 'nrpe_command' =&gt; $merged_data['vars']['nrpe_command'] } } }else{ $check_command = {} } # Assembling $vars from Global Default service settings, servicegroup settings, this particular check settings and let's not forget nrpe settings. if $all_service_vars['graphite_template'] { $graphite_template = {'check_command' =&gt; $all_service_vars['graphite_template']} }else{ $graphite_template = {'check_command' =&gt; $service_name} } $service_notify = [] + pick($settings_vars['notify_group'], []) + pick($this_service_vars['notify_group'], []) # pick is required everywhere, otherwise becomes "The value '' cannot be converted to Numeric" $service_notify_group = $service_notify ? { [] =&gt; $service_defaults['vars']['notify_group'], default =&gt; $service_notify } # Assing default group (systems) if no other groups are defined $vars = $all_service_vars + $nrpe + $check_command + $graphite_template + {'notify_group' =&gt; $service_notify_group} # This needs to be merged separately, because merging it as part of MERGED_DATA overwrites arrays instead of merging them, so we lose some "assign" and "ignore" values $assign = delete_undef_values($service_defaults['assign'] + $s_list['settings']['assign'] + $data['assign']) $ignore = delete_undef_values($service_defaults['ignore'] + $s_list['settings']['ignore'] + $data['ignore']) icinga2::object::service {$service_name: ensure =&gt; $merged_data['ensure'], apply =&gt; $merged_data['apply'], enable_flapping =&gt; $merged_data['enable_flapping'], assign =&gt; $assign, ignore =&gt; $ignore, groups =&gt; [$service_group], check_command =&gt; $merged_data['check_command'], check_interval =&gt; $merged_data['check_interval'], check_timeout =&gt; $merged_data['check_timeout'], check_period =&gt; $merged_data['check_period'], display_name =&gt; $merged_data['display_name'], event_command =&gt; $merged_data['event_command'], retry_interval =&gt; $merged_data['retry_interval'], max_check_attempts =&gt; $merged_data['max_check_attempts'], target =&gt; $merged_data['target'], vars =&gt; $vars, template =&gt; $merged_data['template'], } } } #### END OF SERVICES #### #### OTHER BORING STUFF #### $servicegroups.each | $servicegroup, $description |{ icinga2::object::servicegroup{ $servicegroup: target =&gt; $servicegroup_target, display_name =&gt; $description } } $hostgroups.each| String $hostgroup |{ profiles::services::monitoring::hostgroup { $hostgroup:} } $notifications.each | String $name, Hash $settings |{ $assign = pick($notification_defaults['assign'], []) + $settings['assign'] $ignore = pick($notification_defaults['ignore'], []) + $settings['ignore'] $merged_settings = $settings + $notification_defaults icinga2::object::notification{$name: target =&gt; $merged_settings['target'], apply =&gt; $merged_settings['apply'], apply_target =&gt; $merged_settings['apply_target'], command =&gt; $merged_settings['command'], interval =&gt; $merged_settings['interval'], states =&gt; $merged_settings['states'], types =&gt; $merged_settings['types'], assign =&gt; delete_undef_values($assign), ignore =&gt; delete_undef_values($ignore), user_groups =&gt; $merged_settings['user_groups'], period =&gt; $merged_settings['period'], vars =&gt; $merged_settings['vars'], } } # Merging notification settings for users with other settings $users_oncall = deep_merge($users, $oncall) # Magic. Do not touch. create_resources('icinga2::object::user', $users_oncall, $user_defaults) create_resources('icinga2::object::usergroup', $usergroups, $usergroup_defaults) create_resources('icinga2::object::timeperiod',$timeperiods) create_resources('icinga2::object::checkcommand', $check_commands) create_resources('icinga2::object::notificationcommand', $notification_commands) profiles::services::sudoers { 'icinga_runs_ping_l2': ensure =&gt; present, sudoersd_template =&gt; 'profiles/os/redhat/centos7/sudoers/icinga.erb', } }</code> </pre> </div></div><br><p>  Ainda estou trabalhando nesse macarr√£o e melhorando-o sempre que poss√≠vel.  No entanto, foi esse c√≥digo que possibilitou o uso de sintaxe simples e clara no Hiera: </p><br><div class="spoiler">  <b class="spoiler_title">Dados</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">profiles::services::monitoring::config::services: perf_checks: settings: check_interval: '2m' assign: - 'host.vars.type == linux' checks: procs: {} load: {} memory: {} disk: check_interval: '5m' vars: notification_period: '24x7' disk_iops: vars: notifications: - 'silent' cpu: vars: notifications: - 'silent' dns_fqdn: check_interval: '15m' ignore: - 'xenserver in host.groups' vars: notifications: - 'silent' iftraffic_nrpe: vars: notifications: - 'silent' logging: settings: assign: - 'logserver in host.groups' checks: rsyslog: {} nginx_limit_req_other: {} nginx_limit_req_s2s: {} nginx_limit_req_s2x: {} nginx_limit_req_srs: {} logstash: {} logstash_api: vars: notifications: - 'silent'</code> </pre> </div></div><br><p>  Todas as verifica√ß√µes s√£o divididas em grupos, cada grupo tem configura√ß√µes padr√£o, como onde e com que freq√º√™ncia executar essas verifica√ß√µes, quais notifica√ß√µes enviar e para quem. </p><br><p>  Em cada verifica√ß√£o, voc√™ pode substituir qualquer op√ß√£o e tudo isso acaba adicionando as configura√ß√µes padr√£o de todas as verifica√ß√µes como um todo.  Portanto, esse macarr√£o √© gravado em config.pp - h√° uma fus√£o de todas as configura√ß√µes padr√£o com as configura√ß√µes dos grupos e depois com cada verifica√ß√£o individual. </p><br><p>  Al√©m disso, uma mudan√ßa muito importante foi a capacidade de usar fun√ß√µes nas configura√ß√µes, por exemplo, a fun√ß√£o de alterar a porta, endere√ßo e URL para verificar http_regex. </p><br><pre> <code class="plaintext hljs">http_regexp: assign: - 'host.vars.http_regex' - 'static_sites in host.groups' check_command: 'http' check_interval: '1m' retry_interval: '20s' max_check_attempts: 6 http_port: '{{ if(host.vars.http_port) { return host.vars.http_port } else { return 443 } }}' vars: notification_period: 'host.vars.notification_period' http_vhost: '{{ if(host.vars.http_vhost) { return host.vars.http_vhost } else { return host.name } }}' http_ssl: '{{ if(host.vars.http_ssl) { return false } else { return true } }}' http_expect_body_regex: 'host.vars.http_regex' http_uri: '{{ if(host.vars.http_uri) { return host.vars.http_uri } else { return "/" } }}' http_onredirect: 'follow' http_warn_time: 8 http_critical_time: 15 http_timeout: 30 http_sni: true</code> </pre> <br><p>  Isso significa - se houver uma vari√°vel <em>http_port</em> na <em>defini√ß√£o de</em> host - use-a, caso contr√°rio 443. Por exemplo, a interface da web do jabber trava no 9090 e Unifi no 7443. <br>  <em>http_vhost</em> significa ignorar o DNS e <em>usar</em> este endere√ßo. <br>  Se uri for especificado no host, siga-o, caso contr√°rio, use "/". </p><br><p>  Uma hist√≥ria engra√ßada foi publicada com http_ssl - essa infec√ß√£o n√£o queria se desconectar sob demanda.  Estupidamente, tropecei nessa linha por um longo tempo, at√© que me ocorreu que havia uma vari√°vel na defini√ß√£o de host: </p><br><pre> <code class="plaintext hljs">http_ssl: false</code> </pre> <br><p>  Substitui a express√£o </p><br><pre> <code class="plaintext hljs">if(host.vars.http_ssl) { return false } else { return true }</code> </pre> <br><p>  como <strong>falso</strong> e acontece </p><br><pre> <code class="plaintext hljs">if(false) { return false } else { return true }</code> </pre> <br><p>  isto √©, a verifica√ß√£o ssl est√° sempre ativa.  Foi decidido substituindo a sintaxe: </p><br><pre> <code class="plaintext hljs">http_ssl: no</code> </pre> <br><p>  <strong>Conclus√µes</strong> : </p><br><p>  Pr√≥s: </p><br><ul><li>  Agora, temos um sistema de monitoramento, e n√£o dois, como nos √∫ltimos 7-8 meses, ou um, desatualizado e vulner√°vel. </li><li>  A estrutura de dados dos hosts / servi√ßos (verifica√ß√µes) √© agora (na minha opini√£o) muito mais leg√≠vel e compreens√≠vel.  Para outros, isso n√£o era t√£o √≥bvio, ent√£o eu tive que cortar algumas p√°ginas no wiki local para explicar como funciona e onde edit√°-lo. </li><li>  √â poss√≠vel configurar verifica√ß√µes de forma flex√≠vel usando vari√°veis ‚Äã‚Äãe fun√ß√µes, por exemplo, para verificar http_regexp, o padr√£o desejado, c√≥digo de retorno, URL e porta podem ser definidos nas configura√ß√µes do host. </li><li>  Existem v√°rios pain√©is, para cada um dos quais voc√™ pode definir sua pr√≥pria lista de alarmes exibidos e gerenciar tudo isso atrav√©s de solicita√ß√µes de Puppet e mesclagem. </li></ul><br><p>  Contras: </p><br><ul><li>  In√©rcia dos membros da equipe - Nagios trabalhou, trabalhou e trabalhou, e este seu Isinga constantemente falha e diminui a velocidade.  E como posso ver a hist√≥ria?  E, caramba, ele n√£o √© atualizado ... (O verdadeiro problema √© que o hist√≥rico de alarmes n√£o √© atualizado automaticamente, apenas por F5) </li><li>  A in√©rcia do sistema - quando clico em "verificar agora" na interface da web - o resultado da execu√ß√£o depende do clima em Marte, especialmente em servi√ßos complexos que levam dezenas de segundos para serem conclu√≠dos.  Um resultado semelhante √© uma coisa normal. <img src="https://habrastorage.org/webt/ue/73/wa/ue73wa4yt4bhedebd1n66kizsf8.jpeg"></li><li>  Em geral, de acordo com as estat√≠sticas semestrais dos dois sistemas trabalhando lado a lado, o Nagios sempre trabalhava mais r√°pido que o Icinga e isso realmente me irritava.  Parece-me que h√° algo enganado com temporizadores e uma verifica√ß√£o de cinco minutos do fato acontece a cada 5:30 ou algo assim. </li><li>  Se voc√™ reiniciar o servi√ßo a qualquer momento (systemctl restart icinga2) - todas as verifica√ß√µes em andamento naquele momento acionar√£o um alarme cr√≠tico &lt;terminado pelo sinal 15&gt; na tela e, pelo lado, parecer√° que tudo caiu ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">erro confirmado</a> ) </li></ul><br><p>  Mas, em geral - funciona. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt444060/">https://habr.com/ru/post/pt444060/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt444048/index.html">Dicas e truques do Digital Forensics: como detectar altera√ß√µes de diretiva de grupo direcionadas a intrusos</a></li>
<li><a href="../pt444050/index.html">Discuss√£o: O armazenamento de DNA se tornar√° maci√ßo</a></li>
<li><a href="../pt444052/index.html">Como n√≥s, no IntelliJ IDEA, procuramos express√µes lambda</a></li>
<li><a href="../pt444056/index.html">Provedores de Internet na Crim√©ia aumentaram acentuadamente os pre√ßos dos servi√ßos</a></li>
<li><a href="../pt444058/index.html">Quando as crian√ßas entendem que toda a sua vida j√° est√° online</a></li>
<li><a href="../pt444062/index.html">Acenda! Transforma√ß√µes noturnas do Centro Lakhta</a></li>
<li><a href="../pt444064/index.html">Novas id√©ias para um novo futuro</a></li>
<li><a href="../pt444068/index.html">Quem est√° assistindo?</a></li>
<li><a href="../pt444070/index.html">Desenvolvendo um hexapod a partir do zero (parte 4) - trajet√≥rias e sequ√™ncias matem√°ticas</a></li>
<li><a href="../pt444072/index.html">Android Shopping - Play Billing Library</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>