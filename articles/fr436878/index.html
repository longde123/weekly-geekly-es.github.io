<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õπüèΩ ‚úçüèæ üí§ BERT est un mod√®le de langage de pointe pour 104 langues. Tutoriel pour lancer BERT localement et sur Google Colab üêû ‚öïÔ∏è üö∏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERT est un r√©seau neuronal de Google, qui a montr√© par une large marge des r√©sultats de pointe sur un certain nombre de t√¢ches. En utilisant BERT, vo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>BERT est un mod√®le de langage de pointe pour 104 langues. Tutoriel pour lancer BERT localement et sur Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="image"></p><br><p>  BERT est un r√©seau neuronal de Google, qui a montr√© par une large marge des r√©sultats de pointe sur un certain nombre de t√¢ches.  En utilisant BERT, vous pouvez cr√©er des programmes d'IA pour traiter un langage naturel: r√©pondre aux questions pos√©es sous n'importe quelle forme, cr√©er des robots de discussion, des traducteurs automatiques, analyser du texte, etc. </p><br><p>  Google a publi√© des mod√®les BERT pr√©-form√©s, mais comme c'est g√©n√©ralement le cas avec le Machine Learning, ils souffrent d'un manque de documentation.  Par cons√©quent, dans ce didacticiel, nous allons apprendre √† ex√©cuter le r√©seau neuronal BERT sur l'ordinateur local, ainsi que sur le GPU du serveur gratuit sur Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Pourquoi est-ce n√©cessaire </h2><br><p>  Pour soumettre du texte √† l'entr√©e d'un r√©seau de neurones, vous devez en quelque sorte le pr√©senter sous forme de nombres.  Il est plus facile de faire cette lettre par lettre, en appliquant une lettre √† chaque entr√©e de r√©seau neuronal.  Ensuite, chaque lettre sera cod√©e avec un nombre de 0 √† 32 (plus une sorte de marge pour les signes de ponctuation).  C'est ce qu'on appelle le niveau des personnages. </p><br><p>  Mais de bien meilleurs r√©sultats sont obtenus si nous pr√©sentons des propositions non pas par une seule lettre, mais en soumettant √† chaque entr√©e du r√©seau neuronal imm√©diatement un mot entier (ou au moins des syllabes).  Ce sera d√©j√† un niveau mot.  L'option la plus simple consiste √† compiler un dictionnaire avec tous les mots existants et √† fournir au r√©seau le nombre de mots de ce dictionnaire.  Par exemple, si le mot "chien" est dans ce dictionnaire √† la place 1678, alors nous entrons le nombre 1678 pour l'entr√©e du r√©seau neuronal pour ce mot. </p><br><p>  Mais ce n'est que dans le langage naturel avec le mot ¬´chien¬ª qu'une personne fait imm√©diatement appara√Ætre beaucoup d'associations: ¬´moelleux¬ª, ¬´diabolique¬ª, ¬´ami de l'homme¬ª.  Est-il possible de coder d'une mani√®re ou d'une autre cette caract√©ristique de notre pens√©e dans la pr√©sentation du r√©seau neuronal?  Il s'av√®re que vous le pouvez.  Pour ce faire, il suffit de r√©organiser les num√©ros de mots pour que les mots dont la signification est proche se tiennent c√¥te √† c√¥te.  Que ce soit, par exemple, pour ¬´chien¬ª le nombre 1678, et pour le mot ¬´pelucheux¬ª le nombre 1680. Et pour le mot ¬´th√©i√®re¬ª le nombre est 9000. Comme vous pouvez le voir, les nombres 1678 et 1680 sont beaucoup plus proches les uns des autres que le nombre 9000. </p><br><p>  En pratique, chaque mot ne se voit pas attribuer un seul num√©ro, mais plusieurs - un vecteur, disons, de 32 chiffres.  Et les distances sont mesur√©es comme les distances entre les points vers lesquels ces vecteurs pointent dans l'espace de la dimension correspondante (pour un vecteur de 32 chiffres, il s'agit d'un espace √† 32 dimensions ou √† 32 axes).  Cela vous permet de comparer un mot √† la fois avec plusieurs mots dont le sens est proche (selon l'axe √† compter).  De plus, des op√©rations arithm√©tiques peuvent √™tre effectu√©es avec des vecteurs.  Un exemple classique: si vous soustrayez le vecteur ¬´homme¬ª du vecteur d√©signant le mot ¬´roi¬ª et ajoutez le vecteur pour le mot ¬´femme¬ª, vous obtenez un certain vecteur de r√©sultat.  Et il correspondra miraculeusement au mot "reine".  Et en effet, "le roi est homme + femme = reine".  La magie!  Et ce n'est pas un exemple abstrait, mais √ßa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arrive vraiment</a> .  √âtant donn√© que les r√©seaux de neurones sont bien adapt√©s aux transformations math√©matiques sur leurs entr√©es, cela fournit apparemment une efficacit√© si √©lev√©e de cette m√©thode. </p><br><p>  Cette approche est appel√©e Embeddings.  Tous les packages d'apprentissage automatique (TensorFlow, PyTorch) permettent √† la premi√®re couche du r√©seau de neurones de mettre une couche sp√©ciale de couche d'int√©gration, qui le fait automatiquement.  C'est-√†-dire qu'√† l'entr√©e du r√©seau de neurones, nous donnons le num√©ro de mot habituel dans le dictionnaire, et Embedding Layer, auto-apprentissage, traduit chaque mot en un vecteur de la longueur sp√©cifi√©e, disons 32 chiffres. </p><br><p>  Mais ils ont rapidement r√©alis√© qu'il √©tait beaucoup plus rentable de pr√©-former une telle repr√©sentation vectorielle des mots sur un √©norme corpus de textes, par exemple, sur l'ensemble de Wikip√©dia, et d'utiliser des vecteurs de mots pr√™ts √† l'emploi dans des r√©seaux de neurones sp√©cifiques plut√¥t que de les former √† chaque fois. </p><br><p>  Il existe plusieurs fa√ßons de repr√©senter les mots comme vecteurs; ils ont progressivement √©volu√©: word2vec, GloVe, Elmo. </p><br><p>  √Ä l'√©t√© 2018, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI a remarqu√©</a> que si vous pr√©-entra√Ænez un r√©seau de neurones sur l'architecture <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Transformer</a> sur de gros volumes de texte, alors de mani√®re inattendue et par une grande marge, il donne d'excellents r√©sultats dans de nombreux types de t√¢ches de traitement du langage naturel.  En fait, un tel r√©seau de neurones √† sa sortie cr√©e des repr√©sentations vectorielles pour les mots, et m√™me des phrases enti√®res.  Et en accrochant au-dessus d'un tel mod√®le de langage un petit bloc d'une paire de couches suppl√©mentaires de neurones, vous pouvez entra√Æner ce r√©seau neuronal pour toutes les t√¢ches. </p><br><p>  BERT de Google est un r√©seau GPA avanc√© d'OpenAI (bidirectionnel au lieu d'unidirectionnel, etc.), √©galement bas√© sur l'architecture Transformer.  √Ä l'heure actuelle, BERT est √† la pointe de la technologie sur presque tous les benchmarks NLP populaires. </p><br><h2 id="kak-oni-eto-sdelali">  Comment l'ont-ils fait </h2><br><p>  L'id√©e derri√®re BERT est tr√®s simple: alimentons le r√©seau neuronal avec des phrases dans lesquelles nous rempla√ßons 15% des mots par [MASQUE] et entra√Ænons le r√©seau neuronal √† pr√©dire ces mots masqu√©s. </p><br><p>  Par exemple, si nous envoyons la phrase ¬´Je suis venu √† [MASQUE] et j'ai achet√© [MASQUE]¬ª √† l'entr√©e du r√©seau neuronal, il devrait afficher les mots ¬´stocker¬ª et ¬´lait¬ª √† la sortie.  Ceci est un exemple simplifi√© de la page officielle du BERT; sur des phrases plus longues, la gamme d'options possibles devient plus petite et la r√©ponse du r√©seau neuronal est sans ambigu√Øt√©. </p><br><p>  Et pour que le r√©seau neuronal apprenne √† comprendre la relation entre diff√©rentes phrases, nous allons √©galement l'entra√Æner √† pr√©dire si la deuxi√®me phrase est une continuation logique de la premi√®re.  Ou est-ce une phrase al√©atoire qui n'a rien √† voir avec la premi√®re. </p><br><p>  Donc, pour deux phrases: "Je suis all√© au magasin."  et "Et achet√© du lait l√†-bas.", le r√©seau neuronal devrait r√©pondre que c'est logique.  Et si la deuxi√®me phrase est "Crucian sky Pluto", alors je dois r√©pondre que cette proposition n'est pas li√©e √† la premi√®re.  Nous allons jouer avec ces deux modes BERT ci-dessous. </p><br><p>  Ayant ainsi form√© le r√©seau neuronal sur le corpus de textes de Wikip√©dia et la collection de livres BookCorpus pendant 4 jours √† 16 TPU, nous avons obtenu le BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Installation et configuration </h2><br><p>  <em><strong>Remarque</strong> : dans cette section, nous allons lancer et jouer avec BERT sur l'ordinateur local.</em>  <em>Pour ex√©cuter ce r√©seau neuronal sur un GPU local, vous aurez besoin d'un NVidia GTX 970 avec 4 Go de m√©moire vid√©o ou plus.</em>  <em>Si vous souhaitez simplement ex√©cuter BERT dans un navigateur (vous n'avez m√™me pas besoin d'un GPU sur votre ordinateur pour cela), acc√©dez √† la section Google Colab.</em> </p><br><p>  Installez d'abord TensorFlow si vous ne l'avez pas d√©j√†, en suivant les instructions de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.tensorflow.org/install</a> .  Pour prendre en charge le GPU, vous devez d'abord installer CUDA Toolkit 9.0, puis cuDNN SDK 7.2, puis TensorFlow lui-m√™me avec prise en charge GPU: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Fondamentalement, cela suffit pour ex√©cuter BERT.  Mais il n'y a aucune instruction en tant que telle, vous pouvez la composer vous-m√™me en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">triant</a> les sources dans le fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">run_classifier.py</a> (la situation habituelle dans Machine Learning est lorsque vous devez aller dans les sources au lieu de la documentation).  Mais nous allons le faire plus facilement et utiliser le shell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Keras BERT</a> (il peut √©galement √™tre utile pour affiner le r√©seau plus tard, car il fournit une interface Keras pratique). </p><br><p>  Pour ce faire, installez Keras lui-m√™me: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  Et apr√®s Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Nous aurons √©galement besoin du fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tokenization.py</a> du github original BERT.  Soit cliquez sur le bouton Raw et enregistrez-le dans le dossier avec le futur script, ou t√©l√©chargez le r√©f√©rentiel entier et r√©cup√©rez le fichier √† partir de l√†, ou prenez une copie du r√©f√©rentiel avec ce code <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Il est maintenant temps de t√©l√©charger le r√©seau neuronal pr√©-form√©.  Il existe plusieurs options pour BERT, qui sont toutes r√©pertori√©es sur la page officielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/google-research/bert</a> .  Nous prendrons le multilingue universel "BERT-Base, Multilingual Cased", pour 104 langues.  T√©l√©chargez le fichier <a href="">multi_cased_L-12_H-768_A-12.zip</a> (632 Mo) et d√©compressez-le dans le dossier avec le futur script. </p><br><p>  Tout est pr√™t, cr√©ez le fichier BERT.py, puis il y aura un peu de code. </p><br><p>  Importer les biblioth√®ques requises et d√©finir les chemins </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Puisque nous devrons traduire des lignes de texte ordinaires dans un format sp√©cial de jetons, nous cr√©erons un objet sp√©cial pour cela.  Faites attention √† do_lower_case = False, car nous utilisons le mod√®le Cased BERT, qui est sensible √† la casse. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Mod√®le de chargement </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT peut fonctionner en deux modes: deviner les mots manqu√©s dans la phrase, ou deviner si la deuxi√®me phrase est logique apr√®s la premi√®re.  Nous ferons les deux options. </p><br><p>  Pour le premier mode, vous devez soumettre une phrase au format: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  Le r√©seau neuronal devrait renvoyer une phrase compl√®te avec les mots remplis √† la place des masques: "Je suis venu au magasin et j'ai achet√© du lait." </p><br><p>  Pour le deuxi√®me mode, les deux phrases s√©par√©es par un s√©parateur doivent √™tre introduites √† l'entr√©e du r√©seau neuronal: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  Le r√©seau neuronal doit r√©pondre si la deuxi√®me phrase est une continuation logique de la premi√®re.  Ou est-ce une phrase al√©atoire qui n'a rien √† voir avec la premi√®re. </p><br><p>  Pour que BERT fonctionne, vous devez pr√©parer trois vecteurs, chacun avec une longueur de 512 nombres: token_input, seg_input et mask_input. </p><br><p>  <strong>Token_input</strong> stockera notre code source traduit en jetons √† l'aide de tokenizer.  La phrase sous forme d'index dans le dictionnaire sera au d√©but de ce vecteur et le reste sera rempli de z√©ros. </p><br><p>  Dans <strong>mask_input,</strong> nous devons mettre 1 pour toutes les positions o√π se trouve le masque [MASK] et remplir le reste avec des z√©ros. </p><br><p>  Dans <strong>seg_input,</strong> nous devons <strong>d√©signer la</strong> premi√®re phrase (y compris le s√©parateur CLS et SEP de d√©part) comme 0, la deuxi√®me phrase (y compris le SEP final) comme 1, et remplir le reste √† la fin du vecteur avec des z√©ros. </p><br><p>  BERT n'utilise pas un dictionnaire de mots entiers, mais plut√¥t des syllabes les plus courantes.  Bien qu'il ait aussi des mots entiers.  Vous pouvez ouvrir le fichier vocab.txt dans le r√©seau neuronal t√©l√©charg√© et voir quels mots le r√©seau neuronal utilise √† son entr√©e.  Il y a des mots entiers comme la France.  Mais la plupart des mots russes doivent √™tre d√©compos√©s en syllabes.  Ainsi, le mot ¬´est venu¬ª doit √™tre d√©compos√© en ¬´avec¬ª et ¬´## est all√©¬ª.  Pour vous aider √† convertir des lignes de texte r√©guli√®res au format requis par BERT, nous utilisons le module tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Mode 1: Pr√©diction des mots ferm√©s par jeton [MASQUE] dans une phrase </h2><br><p>  La phrase d'entr√©e qui est envoy√©e √† l'entr√©e du r√©seau neuronal </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Convertissez-le en jetons.  Le probl√®me est que le tokenizer ne peut pas traiter les marques de service comme [CLS] et [MASK], bien que vocab.txt les ait dans le dictionnaire.  Par cons√©quent, nous devrons rompre manuellement notre ligne avec les marqueurs [MASK] et en extraire des morceaux de texte brut afin de le convertir en jetons BERT en utilisant le tokenizer.  Ajoutez √©galement [CLS] au d√©but et [SEP] √† la fin de la phrase. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Les jetons ont maintenant des jetons qui sont garantis pour √™tre convertis en index dans le dictionnaire.  Faisons-le: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Maintenant, dans token_input, il y a une s√©rie de nombres (num√©ros de mots dans le dictionnaire vocab.txt) qui doivent √™tre introduits dans l'entr√©e du r√©seau neuronal.  Il ne reste plus qu'√† √©tendre ce vecteur √† une longueur de 512 √©l√©ments.  La construction Python [0] * length cr√©e un tableau de longueur de longueur, rempli de z√©ros.  Ajoutez-le simplement √† nos jetons, qui en python combinent deux tableaux en un. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Cr√©ez maintenant un masque de 512 de longueur, en mettant 1 partout, o√π le nombre 103 appara√Æt dans les jetons (qui correspond au marqueur [MASK] dans le dictionnaire vocab.txt), et remplissez le reste avec 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Pour le premier mode de fonctionnement BERT, seg_input doit √™tre compl√®tement rempli de z√©ros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  La derni√®re √©tape, vous devez convertir les tableaux python en tableaux numpy avec une forme (1 512), pour lesquels nous les avons mis dans un sous-tableau []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  OK, c'est fait.  Maintenant, ex√©cutez la pr√©diction du r√©seau neuronal! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Formatez maintenant le r√©sultat des jetons en une cha√Æne s√©par√©e par des espaces </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  Et sortez le r√©sultat: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  Dans notre exemple, pour la phrase ¬´Je suis venu √† [MASK] et j'ai achet√© [MASK]¬ª.  le r√©seau de neurones a produit le r√©sultat ¬´maison¬ª et ¬´√ßa¬ª: ¬´Je suis venu √† la maison et je l'ai achet√©¬ª.  Enfin, pas si mal pour la premi√®re fois.  L'achat d'une maison est certainement mieux que le lait). </p><br><div class="spoiler">  <b class="spoiler_title">Autres exemples (je ne donne pas ceux qui ont √©chou√©, il y en a beaucoup plus que ceux qui ont r√©ussi. Dans la plupart des cas, le r√©seau donne une r√©ponse vide):</b> <div class="spoiler_text"><p>  La Terre est le troisi√®me [MASQUE] du Soleil <br>  R√©sultat: Star </p><br><p>  meilleur sandwich [MASQUE] au beurre <br>  R√©sultat: rencontre </p><br><p>  apr√®s le [MASQUE] le d√©jeuner est cens√© dormir <br>  R√©sultat: de ceci </p><br><p>  s'√©loigner de [MASK] <br>  R√©sultat: ## oh - est-ce une sorte de mal√©diction?  ) </p><br><p>  [MASQUE] de la porte <br>  R√©sultat: voir </p><br><p>  Avec [MASQUE] le marteau et les clous peuvent faire une armoire <br>  R√©sultat: aide </p><br><p>  Et si demain ne l'est pas?  Aujourd'hui, par exemple, ce n'est pas [MASQUE]! <br>  R√©sultat: sera </p><br><p>  Comment pouvez-vous vous lasser d'ignorer [MASQUE]? <br>  R√©sultat: elle </p><br><p>  Il y a la logique de tous les jours, il y a la logique f√©minine, mais on ne sait rien du m√¢le <br>  R√©sultat: philosophie </p><br><p>  Chez les femmes, √† l'√¢ge de trente ans, une image du prince se forme, qui convient √† tout [MASQUE]. <br>  R√©sultat: homme </p><br><p>  Par un vote majoritaire, Blanche-Neige et les sept Nains ont vot√© pour [MASQUE], avec une voix contre. <br>  R√©sultat: village - la premi√®re lettre est correcte </p><br><p>  √âvaluez votre ennui sur une √©chelle de 10 points: [MASQUE] points <br>  R√©sultat: 10 </p><br><p>  Votre [MASQUE], [MASQUE] et [MASQUE]! <br>  R√©sultat: aime-moi, je - non, BERT, je ne le pensais pas du tout </p></div></div><br><p>  Vous pouvez saisir des phrases en anglais (et n'importe laquelle dans 104 langues, dont la liste <a href="">est ici</a> ) </p><br><p>  [MASQUE] doit continuer! <br>  R√©sultat: I </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Mode 2: v√©rification de la coh√©rence de deux phrases </h2><br><p>  Nous d√©finissons deux phrases cons√©cutives qui seront aliment√©es √† l'entr√©e du r√©seau neuronal </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Nous allons cr√©er des jetons au format [CLS] phrase_1 [SEP] phrase_2 [SEP], en convertissant du texte brut en jetons √† l'aide de tokenizer: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Nous convertissons les jetons de cha√Æne en indices num√©riques (nombres de mots dans le dictionnaire vocab.txt) et √©tendons le vecteur √† 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Le masque de mot dans ce cas est compl√®tement rempli de z√©ros </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Mais le masque de proposition doit √™tre rempli sous la deuxi√®me phrase (y compris le SEP final) avec des unit√©s, et tout le reste avec des z√©ros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Nous passons les phrases √† travers le r√©seau de neurones (cette fois le r√©sultat est en [1], et non en [0], comme c'√©tait le cas ci-dessus) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  Et nous d√©rivons la probabilit√© que la deuxi√®me phrase soit un ensemble de mots normal et non al√©atoire </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  En deux phrases: </p><br><p>  Je suis venu au magasin.  -&gt; Et achet√© du lait. </p><br><p>  R√©ponse du r√©seau neuronal: </p><br><p>  La peine est bonne: 99% </p><br><p>  Et si la deuxi√®me phrase est "Pluton du ciel de Crucian", alors la r√©ponse sera: </p><br><p>  La peine est bonne: 4% </p><br><h2 id="google-colab">  Google colab </h2><br><p>  Google fournit un GPU de serveur Tesla K80 gratuit avec 12 Go de m√©moire vid√©o (les TPU sont maintenant disponibles, mais leur configuration est un peu plus compliqu√©e).  Tout le code pour Colab doit √™tre con√ßu comme un cahier jupyter.  Pour lancer BERT dans un navigateur, il suffit d'ouvrir le lien </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  Dans le menu <strong>Runtime</strong> , s√©lectionnez <strong>Ex√©cuter tout</strong> , de sorte que pour la premi√®re fois toutes les cellules d√©marrent, les t√©l√©chargements de mod√®le et les biblioth√®ques n√©cessaires sont connect√©s.  Acceptez de r√©initialiser tout le Runtime si n√©cessaire. </p><br><div class="spoiler">  <b class="spoiler_title">En cas de probl√®me ...</b> <div class="spoiler_text"><p>  Assurez-vous que GPU et Python 3 sont s√©lectionn√©s dans le menu Runtime -&gt; Change runtime type </p><br><p>  Si le bouton de connexion n'est pas actif, cliquez dessus pour devenir connect√©. </p></div></div><br><p>  Maintenant, changez la <strong>phrase des</strong> lignes d'entr√©e <strong>phrase</strong> , <strong>phrase_1</strong> et <strong>phrase_2</strong> , et cliquez sur l'ic√¥ne de lecture sur la gauche pour d√©marrer uniquement la cellule actuelle.  L'ex√©cution de l'int√©gralit√© du portable n'est plus n√©cessaire. </p><br><p>  Vous pouvez ex√©cuter BERT sur Google Colab m√™me √† partir d'un smartphone, mais s'il ne s'ouvre pas, vous devrez peut-√™tre activer la case √† cocher Version compl√®te dans les param√®tres de votre navigateur. </p><br><h2 id="chto-dalshe">  Et ensuite? </h2><br><p>  Pour former le BERT √† une t√¢che sp√©cifique, vous devez ajouter une ou deux couches d'un r√©seau Feed Forward simple au-dessus de celui-ci et ne le former que sans toucher au r√©seau BERT principal.  Cela peut √™tre fait sur TensorFlow nu ou via le shell Keras BERT.  Cette formation suppl√©mentaire pour un domaine sp√©cifique se produit tr√®s rapidement et est compl√®tement similaire au r√©glage fin dans les r√©seaux √† convolution.  Ainsi, pour la t√¢che SQuAD, vous pouvez former un r√©seau neuronal sur un TPU en seulement 30 minutes (contre 4 jours sur 16 TPU pour la formation du BERT lui-m√™me). </p><br><p>  Pour ce faire, vous devrez √©tudier la fa√ßon dont les derni√®res couches sont repr√©sent√©es dans BERT, ainsi que disposer d'un ensemble de donn√©es appropri√©.  Sur la page officielle du BERT <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/google-research/bert,</a> vous trouverez plusieurs exemples de diff√©rentes t√¢ches, ainsi que des instructions sur la fa√ßon de commencer un recyclage sur les TPU cloud.  Et tout le reste devra chercher dans la source dans les fichiers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">run_classifier.py</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  Le code et le bloc-notes jupyter pour Google Colab pr√©sent√©s ici <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>sont h√©berg√©s dans le r√©f√©rentiel</strong></a> . </p><br><p>  Il ne faut pas s'attendre √† des miracles.  Ne vous attendez pas √† ce que BERT parle comme une personne.  Le statut de l'√©tat de l'art ne signifie pas du tout que les progr√®s de la PNL ont atteint un niveau acceptable.  Cela signifie simplement que BERT est meilleur que les mod√®les pr√©c√©dents, qui √©taient encore pires.  Une IA conversationnelle solide est encore tr√®s loin.  De plus, BERT est principalement un mod√®le de langage, pas un bot de chat pr√™t √† l'emploi, il ne montre donc de bons r√©sultats qu'apr√®s un recyclage pour une t√¢che sp√©cifique. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436878/">https://habr.com/ru/post/fr436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436866/index.html">Comment reconna√Ætre les faux projets Agile</a></li>
<li><a href="../fr436868/index.html">Int√©grez une analyse statique dans le processus, ne cherchez pas de bogues avec elle</a></li>
<li><a href="../fr436872/index.html">PGConf.Russie 2019 √† venir</a></li>
<li><a href="../fr436874/index.html">Danses du Nouvel An autour de l'adaptateur FC ou un conte sur la distance entre les causes du probl√®me et les sympt√¥mes</a></li>
<li><a href="../fr436876/index.html">[SAP] SAPUI5 for dummies part 1: Un exercice complet √©tape par √©tape</a></li>
<li><a href="../fr436880/index.html">Notions de base sur les mod√®les C ++: mod√®les de fonctions</a></li>
<li><a href="../fr436884/index.html">Nous ma√Ætrisons async / wait sur un exemple r√©el</a></li>
<li><a href="../fr436886/index.html">Utiliser Babel et Webpack pour configurer un projet React √† partir de z√©ro</a></li>
<li><a href="../fr436888/index.html">Histoire sur la conception d'une API</a></li>
<li><a href="../fr436890/index.html">Tutoriel React Partie 10: Atelier sur l'utilisation des propri√©t√©s et du style des composants</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>