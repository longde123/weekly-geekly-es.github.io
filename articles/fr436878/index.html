<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>⛹🏽 ✍🏾 💤 BERT est un modèle de langage de pointe pour 104 langues. Tutoriel pour lancer BERT localement et sur Google Colab 🐞 ⚕️ 🚸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="BERT est un réseau neuronal de Google, qui a montré par une large marge des résultats de pointe sur un certain nombre de tâches. En utilisant BERT, vo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>BERT est un modèle de langage de pointe pour 104 langues. Tutoriel pour lancer BERT localement et sur Google Colab</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436878/"><p><img src="https://habrastorage.org/getpro/habr/post_images/2bd/0ba/1c4/2bd0ba1c4fb80fe4d771f555168c9ff0.png" alt="image"></p><br><p>  BERT est un réseau neuronal de Google, qui a montré par une large marge des résultats de pointe sur un certain nombre de tâches.  En utilisant BERT, vous pouvez créer des programmes d'IA pour traiter un langage naturel: répondre aux questions posées sous n'importe quelle forme, créer des robots de discussion, des traducteurs automatiques, analyser du texte, etc. </p><br><p>  Google a publié des modèles BERT pré-formés, mais comme c'est généralement le cas avec le Machine Learning, ils souffrent d'un manque de documentation.  Par conséquent, dans ce didacticiel, nous allons apprendre à exécuter le réseau neuronal BERT sur l'ordinateur local, ainsi que sur le GPU du serveur gratuit sur Google Colab. </p><a name="habracut"></a><br><h2 id="zachem-eto-voobsche-nuzhno">  Pourquoi est-ce nécessaire </h2><br><p>  Pour soumettre du texte à l'entrée d'un réseau de neurones, vous devez en quelque sorte le présenter sous forme de nombres.  Il est plus facile de faire cette lettre par lettre, en appliquant une lettre à chaque entrée de réseau neuronal.  Ensuite, chaque lettre sera codée avec un nombre de 0 à 32 (plus une sorte de marge pour les signes de ponctuation).  C'est ce qu'on appelle le niveau des personnages. </p><br><p>  Mais de bien meilleurs résultats sont obtenus si nous présentons des propositions non pas par une seule lettre, mais en soumettant à chaque entrée du réseau neuronal immédiatement un mot entier (ou au moins des syllabes).  Ce sera déjà un niveau mot.  L'option la plus simple consiste à compiler un dictionnaire avec tous les mots existants et à fournir au réseau le nombre de mots de ce dictionnaire.  Par exemple, si le mot "chien" est dans ce dictionnaire à la place 1678, alors nous entrons le nombre 1678 pour l'entrée du réseau neuronal pour ce mot. </p><br><p>  Mais ce n'est que dans le langage naturel avec le mot «chien» qu'une personne fait immédiatement apparaître beaucoup d'associations: «moelleux», «diabolique», «ami de l'homme».  Est-il possible de coder d'une manière ou d'une autre cette caractéristique de notre pensée dans la présentation du réseau neuronal?  Il s'avère que vous le pouvez.  Pour ce faire, il suffit de réorganiser les numéros de mots pour que les mots dont la signification est proche se tiennent côte à côte.  Que ce soit, par exemple, pour «chien» le nombre 1678, et pour le mot «pelucheux» le nombre 1680. Et pour le mot «théière» le nombre est 9000. Comme vous pouvez le voir, les nombres 1678 et 1680 sont beaucoup plus proches les uns des autres que le nombre 9000. </p><br><p>  En pratique, chaque mot ne se voit pas attribuer un seul numéro, mais plusieurs - un vecteur, disons, de 32 chiffres.  Et les distances sont mesurées comme les distances entre les points vers lesquels ces vecteurs pointent dans l'espace de la dimension correspondante (pour un vecteur de 32 chiffres, il s'agit d'un espace à 32 dimensions ou à 32 axes).  Cela vous permet de comparer un mot à la fois avec plusieurs mots dont le sens est proche (selon l'axe à compter).  De plus, des opérations arithmétiques peuvent être effectuées avec des vecteurs.  Un exemple classique: si vous soustrayez le vecteur «homme» du vecteur désignant le mot «roi» et ajoutez le vecteur pour le mot «femme», vous obtenez un certain vecteur de résultat.  Et il correspondra miraculeusement au mot "reine".  Et en effet, "le roi est homme + femme = reine".  La magie!  Et ce n'est pas un exemple abstrait, mais ça <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arrive vraiment</a> .  Étant donné que les réseaux de neurones sont bien adaptés aux transformations mathématiques sur leurs entrées, cela fournit apparemment une efficacité si élevée de cette méthode. </p><br><p>  Cette approche est appelée Embeddings.  Tous les packages d'apprentissage automatique (TensorFlow, PyTorch) permettent à la première couche du réseau de neurones de mettre une couche spéciale de couche d'intégration, qui le fait automatiquement.  C'est-à-dire qu'à l'entrée du réseau de neurones, nous donnons le numéro de mot habituel dans le dictionnaire, et Embedding Layer, auto-apprentissage, traduit chaque mot en un vecteur de la longueur spécifiée, disons 32 chiffres. </p><br><p>  Mais ils ont rapidement réalisé qu'il était beaucoup plus rentable de pré-former une telle représentation vectorielle des mots sur un énorme corpus de textes, par exemple, sur l'ensemble de Wikipédia, et d'utiliser des vecteurs de mots prêts à l'emploi dans des réseaux de neurones spécifiques plutôt que de les former à chaque fois. </p><br><p>  Il existe plusieurs façons de représenter les mots comme vecteurs; ils ont progressivement évolué: word2vec, GloVe, Elmo. </p><br><p>  À l'été 2018, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI a remarqué</a> que si vous pré-entraînez un réseau de neurones sur l'architecture <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Transformer</a> sur de gros volumes de texte, alors de manière inattendue et par une grande marge, il donne d'excellents résultats dans de nombreux types de tâches de traitement du langage naturel.  En fait, un tel réseau de neurones à sa sortie crée des représentations vectorielles pour les mots, et même des phrases entières.  Et en accrochant au-dessus d'un tel modèle de langage un petit bloc d'une paire de couches supplémentaires de neurones, vous pouvez entraîner ce réseau neuronal pour toutes les tâches. </p><br><p>  BERT de Google est un réseau GPA avancé d'OpenAI (bidirectionnel au lieu d'unidirectionnel, etc.), également basé sur l'architecture Transformer.  À l'heure actuelle, BERT est à la pointe de la technologie sur presque tous les benchmarks NLP populaires. </p><br><h2 id="kak-oni-eto-sdelali">  Comment l'ont-ils fait </h2><br><p>  L'idée derrière BERT est très simple: alimentons le réseau neuronal avec des phrases dans lesquelles nous remplaçons 15% des mots par [MASQUE] et entraînons le réseau neuronal à prédire ces mots masqués. </p><br><p>  Par exemple, si nous envoyons la phrase «Je suis venu à [MASQUE] et j'ai acheté [MASQUE]» à l'entrée du réseau neuronal, il devrait afficher les mots «stocker» et «lait» à la sortie.  Ceci est un exemple simplifié de la page officielle du BERT; sur des phrases plus longues, la gamme d'options possibles devient plus petite et la réponse du réseau neuronal est sans ambiguïté. </p><br><p>  Et pour que le réseau neuronal apprenne à comprendre la relation entre différentes phrases, nous allons également l'entraîner à prédire si la deuxième phrase est une continuation logique de la première.  Ou est-ce une phrase aléatoire qui n'a rien à voir avec la première. </p><br><p>  Donc, pour deux phrases: "Je suis allé au magasin."  et "Et acheté du lait là-bas.", le réseau neuronal devrait répondre que c'est logique.  Et si la deuxième phrase est "Crucian sky Pluto", alors je dois répondre que cette proposition n'est pas liée à la première.  Nous allons jouer avec ces deux modes BERT ci-dessous. </p><br><p>  Ayant ainsi formé le réseau neuronal sur le corpus de textes de Wikipédia et la collection de livres BookCorpus pendant 4 jours à 16 TPU, nous avons obtenu le BERT. </p><br><h2 id="ustanovka-i-nastroyka">  Installation et configuration </h2><br><p>  <em><strong>Remarque</strong> : dans cette section, nous allons lancer et jouer avec BERT sur l'ordinateur local.</em>  <em>Pour exécuter ce réseau neuronal sur un GPU local, vous aurez besoin d'un NVidia GTX 970 avec 4 Go de mémoire vidéo ou plus.</em>  <em>Si vous souhaitez simplement exécuter BERT dans un navigateur (vous n'avez même pas besoin d'un GPU sur votre ordinateur pour cela), accédez à la section Google Colab.</em> </p><br><p>  Installez d'abord TensorFlow si vous ne l'avez pas déjà, en suivant les instructions de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://www.tensorflow.org/install</a> .  Pour prendre en charge le GPU, vous devez d'abord installer CUDA Toolkit 9.0, puis cuDNN SDK 7.2, puis TensorFlow lui-même avec prise en charge GPU: </p><br><pre><code class="dos hljs">pip install tensorflow-gpu</code> </pre> <br><p>  Fondamentalement, cela suffit pour exécuter BERT.  Mais il n'y a aucune instruction en tant que telle, vous pouvez la composer vous-même en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">triant</a> les sources dans le fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">run_classifier.py</a> (la situation habituelle dans Machine Learning est lorsque vous devez aller dans les sources au lieu de la documentation).  Mais nous allons le faire plus facilement et utiliser le shell <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Keras BERT</a> (il peut également être utile pour affiner le réseau plus tard, car il fournit une interface Keras pratique). </p><br><p>  Pour ce faire, installez Keras lui-même: </p><br><pre> <code class="dos hljs">pip install keras</code> </pre> <br><p>  Et après Keras BERT: </p><br><pre> <code class="dos hljs">pip install keras-bert</code> </pre> <br><p>  Nous aurons également besoin du fichier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tokenization.py</a> du github original BERT.  Soit cliquez sur le bouton Raw et enregistrez-le dans le dossier avec le futur script, ou téléchargez le référentiel entier et récupérez le fichier à partir de là, ou prenez une copie du référentiel avec ce code <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/blade1780/bert</a> . </p><br><p>  Il est maintenant temps de télécharger le réseau neuronal pré-formé.  Il existe plusieurs options pour BERT, qui sont toutes répertoriées sur la page officielle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/google-research/bert</a> .  Nous prendrons le multilingue universel "BERT-Base, Multilingual Cased", pour 104 langues.  Téléchargez le fichier <a href="">multi_cased_L-12_H-768_A-12.zip</a> (632 Mo) et décompressez-le dans le dossier avec le futur script. </p><br><p>  Tout est prêt, créez le fichier BERT.py, puis il y aura un peu de code. </p><br><p>  Importer les bibliothèques requises et définir les chemins </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding: utf-8 import sys import codecs import numpy as np from keras_bert import load_trained_model_from_checkpoint import tokenization # ,     BERT folder = 'multi_cased_L-12_H-768_A-12' config_path = folder+'/bert_config.json' checkpoint_path = folder+'/bert_model.ckpt' vocab_path = folder+'/vocab.txt'</span></span></code> </pre> <br><p>  Puisque nous devrons traduire des lignes de texte ordinaires dans un format spécial de jetons, nous créerons un objet spécial pour cela.  Faites attention à do_lower_case = False, car nous utilisons le modèle Cased BERT, qui est sensible à la casse. </p><br><pre> <code class="python hljs">tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br><p>  Modèle de chargement </p><br><pre> <code class="python hljs">model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.summary()</code> </pre> <br><p>  BERT peut fonctionner en deux modes: deviner les mots manqués dans la phrase, ou deviner si la deuxième phrase est logique après la première.  Nous ferons les deux options. </p><br><p>  Pour le premier mode, vous devez soumettre une phrase au format: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    [MASK]   [MASK]. [SEP]</code> </pre> <br><p>  Le réseau neuronal devrait renvoyer une phrase complète avec les mots remplis à la place des masques: "Je suis venu au magasin et j'ai acheté du lait." </p><br><p>  Pour le deuxième mode, les deux phrases séparées par un séparateur doivent être introduites à l'entrée du réseau neuronal: </p><br><pre> <code class="dos hljs">[<span class="hljs-built_in"><span class="hljs-built_in">CLS</span></span>]    . [SEP]   . [SEP]</code> </pre> <br><p>  Le réseau neuronal doit répondre si la deuxième phrase est une continuation logique de la première.  Ou est-ce une phrase aléatoire qui n'a rien à voir avec la première. </p><br><p>  Pour que BERT fonctionne, vous devez préparer trois vecteurs, chacun avec une longueur de 512 nombres: token_input, seg_input et mask_input. </p><br><p>  <strong>Token_input</strong> stockera notre code source traduit en jetons à l'aide de tokenizer.  La phrase sous forme d'index dans le dictionnaire sera au début de ce vecteur et le reste sera rempli de zéros. </p><br><p>  Dans <strong>mask_input,</strong> nous devons mettre 1 pour toutes les positions où se trouve le masque [MASK] et remplir le reste avec des zéros. </p><br><p>  Dans <strong>seg_input,</strong> nous devons <strong>désigner la</strong> première phrase (y compris le séparateur CLS et SEP de départ) comme 0, la deuxième phrase (y compris le SEP final) comme 1, et remplir le reste à la fin du vecteur avec des zéros. </p><br><p>  BERT n'utilise pas un dictionnaire de mots entiers, mais plutôt des syllabes les plus courantes.  Bien qu'il ait aussi des mots entiers.  Vous pouvez ouvrir le fichier vocab.txt dans le réseau neuronal téléchargé et voir quels mots le réseau neuronal utilise à son entrée.  Il y a des mots entiers comme la France.  Mais la plupart des mots russes doivent être décomposés en syllabes.  Ainsi, le mot «est venu» doit être décomposé en «avec» et «## est allé».  Pour vous aider à convertir des lignes de texte régulières au format requis par BERT, nous utilisons le module tokenization.py. </p><br><h2 id="rezhim-1-predskazanie-slov-zakrytyh-tokenom-mask-v-fraze">  Mode 1: Prédiction des mots fermés par jeton [MASQUE] dans une phrase </h2><br><p>  La phrase d'entrée qui est envoyée à l'entrée du réseau neuronal </p><br><pre> <code class="python hljs">sentence = <span class="hljs-string"><span class="hljs-string">'   [MASK]   [MASK].'</span></span> print(sentence)</code> </pre> <br><p>  Convertissez-le en jetons.  Le problème est que le tokenizer ne peut pas traiter les marques de service comme [CLS] et [MASK], bien que vocab.txt les ait dans le dictionnaire.  Par conséquent, nous devrons rompre manuellement notre ligne avec les marqueurs [MASK] et en extraire des morceaux de texte brut afin de le convertir en jetons BERT en utilisant le tokenizer.  Ajoutez également [CLS] au début et [SEP] à la fin de la phrase. </p><br><pre> <code class="python hljs">sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">'[MASK] '</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>); sentence = sentence.replace(<span class="hljs-string"><span class="hljs-string">' [MASK]'</span></span>,<span class="hljs-string"><span class="hljs-string">'[MASK]'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#    sentence = sentence.split('[MASK]') #     tokens = ['[CLS]'] #      [CLS] #        tokenizer.tokenize(),    [MASK] for i in range(len(sentence)): if i == 0: tokens = tokens + tokenizer.tokenize(sentence[i]) else: tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) tokens = tokens + ['[SEP]'] #      [SEP]</span></span></code> </pre> <br><p>  Les jetons ont maintenant des jetons qui sont garantis pour être convertis en index dans le dictionnaire.  Faisons-le: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens)</code> </pre> <br><p>  Maintenant, dans token_input, il y a une série de nombres (numéros de mots dans le dictionnaire vocab.txt) qui doivent être introduits dans l'entrée du réseau neuronal.  Il ne reste plus qu'à étendre ce vecteur à une longueur de 512 éléments.  La construction Python [0] * length crée un tableau de longueur de longueur, rempli de zéros.  Ajoutez-le simplement à nos jetons, qui en python combinent deux tableaux en un. </p><br><pre> <code class="python hljs">token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Créez maintenant un masque de 512 de longueur, en mettant 1 partout, où le nombre 103 apparaît dans les jetons (qui correspond au marqueur [MASK] dans le dictionnaire vocab.txt), et remplissez le reste avec 0: </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(mask_input)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> token_input[i] == <span class="hljs-number"><span class="hljs-number">103</span></span>: mask_input[i] = <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br><p>  Pour le premier mode de fonctionnement BERT, seg_input doit être complètement rempli de zéros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  La dernière étape, vous devez convertir les tableaux python en tableaux numpy avec une forme (1 512), pour lesquels nous les avons mis dans un sous-tableau []: </p><br><pre> <code class="python hljs">token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</code> </pre> <br><p>  OK, c'est fait.  Maintenant, exécutez la prédiction du réseau neuronal! </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">0</span></span>] predicts = np.argmax(predicts, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicts = predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][:len(tokens)] <span class="hljs-comment"><span class="hljs-comment">#   ,    ,       </span></span></code> </pre> <br><p>  Formatez maintenant le résultat des jetons en une chaîne séparée par des espaces </p><br><pre> <code class="python hljs">out = [] <span class="hljs-comment"><span class="hljs-comment">#   out     [MASK],    1  mask_input for i in range(len(mask_input[0])): if mask_input[0][i] == 1: # [0][i], ..   batch   (1,512),       out.append(predicts[i]) out = tokenizer.convert_ids_to_tokens(out) #     out = ' '.join(out) #       out = tokenization.printable_text(out) #    out = out.replace(' ##','') #   : " ##" -&gt; ""</span></span></code> </pre> <br><p>  Et sortez le résultat: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Result:'</span></span>, out)</code> </pre> <br><p>  Dans notre exemple, pour la phrase «Je suis venu à [MASK] et j'ai acheté [MASK]».  le réseau de neurones a produit le résultat «maison» et «ça»: «Je suis venu à la maison et je l'ai acheté».  Enfin, pas si mal pour la première fois.  L'achat d'une maison est certainement mieux que le lait). </p><br><div class="spoiler">  <b class="spoiler_title">Autres exemples (je ne donne pas ceux qui ont échoué, il y en a beaucoup plus que ceux qui ont réussi. Dans la plupart des cas, le réseau donne une réponse vide):</b> <div class="spoiler_text"><p>  La Terre est le troisième [MASQUE] du Soleil <br>  Résultat: Star </p><br><p>  meilleur sandwich [MASQUE] au beurre <br>  Résultat: rencontre </p><br><p>  après le [MASQUE] le déjeuner est censé dormir <br>  Résultat: de ceci </p><br><p>  s'éloigner de [MASK] <br>  Résultat: ## oh - est-ce une sorte de malédiction?  ) </p><br><p>  [MASQUE] de la porte <br>  Résultat: voir </p><br><p>  Avec [MASQUE] le marteau et les clous peuvent faire une armoire <br>  Résultat: aide </p><br><p>  Et si demain ne l'est pas?  Aujourd'hui, par exemple, ce n'est pas [MASQUE]! <br>  Résultat: sera </p><br><p>  Comment pouvez-vous vous lasser d'ignorer [MASQUE]? <br>  Résultat: elle </p><br><p>  Il y a la logique de tous les jours, il y a la logique féminine, mais on ne sait rien du mâle <br>  Résultat: philosophie </p><br><p>  Chez les femmes, à l'âge de trente ans, une image du prince se forme, qui convient à tout [MASQUE]. <br>  Résultat: homme </p><br><p>  Par un vote majoritaire, Blanche-Neige et les sept Nains ont voté pour [MASQUE], avec une voix contre. <br>  Résultat: village - la première lettre est correcte </p><br><p>  Évaluez votre ennui sur une échelle de 10 points: [MASQUE] points <br>  Résultat: 10 </p><br><p>  Votre [MASQUE], [MASQUE] et [MASQUE]! <br>  Résultat: aime-moi, je - non, BERT, je ne le pensais pas du tout </p></div></div><br><p>  Vous pouvez saisir des phrases en anglais (et n'importe laquelle dans 104 langues, dont la liste <a href="">est ici</a> ) </p><br><p>  [MASQUE] doit continuer! <br>  Résultat: I </p><br><h2 id="rezhim-2-proverka-logichnosti-dvuh-fraz">  Mode 2: vérification de la cohérence de deux phrases </h2><br><p>  Nous définissons deux phrases consécutives qui seront alimentées à l'entrée du réseau neuronal </p><br><pre> <code class="python hljs">sentence_1 = <span class="hljs-string"><span class="hljs-string">'   .'</span></span> sentence_2 = <span class="hljs-string"><span class="hljs-string">'  .'</span></span> print(sentence_1, <span class="hljs-string"><span class="hljs-string">'-&gt;'</span></span>, sentence_2)</code> </pre> <br><p>  Nous allons créer des jetons au format [CLS] phrase_1 [SEP] phrase_2 [SEP], en convertissant du texte brut en jetons à l'aide de tokenizer: </p><br><pre> <code class="python hljs">tokens_sen_1 = tokenizer.tokenize(sentence_1) tokens_sen_2 = tokenizer.tokenize(sentence_2) tokens = [<span class="hljs-string"><span class="hljs-string">'[CLS]'</span></span>] + tokens_sen_1 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>] + tokens_sen_2 + [<span class="hljs-string"><span class="hljs-string">'[SEP]'</span></span>]</code> </pre> <br><p>  Nous convertissons les jetons de chaîne en indices numériques (nombres de mots dans le dictionnaire vocab.txt) et étendons le vecteur à 512: </p><br><pre> <code class="python hljs">token_input = tokenizer.convert_tokens_to_ids(tokens) token_input = token_input + [<span class="hljs-number"><span class="hljs-number">0</span></span>] * (<span class="hljs-number"><span class="hljs-number">512</span></span> - len(token_input))</code> </pre> <br><p>  Le masque de mot dans ce cas est complètement rempli de zéros </p><br><pre> <code class="python hljs">mask_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">512</span></span></code> </pre> <br><p>  Mais le masque de proposition doit être rempli sous la deuxième phrase (y compris le SEP final) avec des unités, et tout le reste avec des zéros: </p><br><pre> <code class="python hljs">seg_input = [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">512</span></span> len_1 = len(tokens_sen_1) + <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-comment"><span class="hljs-comment">#   , +2 -   CLS   SEP for i in range(len(tokens_sen_2)+1): # +1, ..   SEP seg_input[len_1 + i] = 1 #   ,   SEP,  #   numpy   (1,) -&gt; (1,512) token_input = np.asarray([token_input]) mask_input = np.asarray([mask_input]) seg_input = np.asarray([seg_input])</span></span></code> </pre> <br><p>  Nous passons les phrases à travers le réseau de neurones (cette fois le résultat est en [1], et non en [0], comme c'était le cas ci-dessus) </p><br><pre> <code class="python hljs">predicts = model.predict([token_input, seg_input, mask_input])[<span class="hljs-number"><span class="hljs-number">1</span></span>]</code> </pre> <br><p>  Et nous dérivons la probabilité que la deuxième phrase soit un ensemble de mots normal et non aléatoire </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">'Sentence is okey:'</span></span>, int(round(predicts[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>)), <span class="hljs-string"><span class="hljs-string">'%'</span></span>)</code> </pre> <br><p>  En deux phrases: </p><br><p>  Je suis venu au magasin.  -&gt; Et acheté du lait. </p><br><p>  Réponse du réseau neuronal: </p><br><p>  La peine est bonne: 99% </p><br><p>  Et si la deuxième phrase est "Pluton du ciel de Crucian", alors la réponse sera: </p><br><p>  La peine est bonne: 4% </p><br><h2 id="google-colab">  Google colab </h2><br><p>  Google fournit un GPU de serveur Tesla K80 gratuit avec 12 Go de mémoire vidéo (les TPU sont maintenant disponibles, mais leur configuration est un peu plus compliquée).  Tout le code pour Colab doit être conçu comme un cahier jupyter.  Pour lancer BERT dans un navigateur, il suffit d'ouvrir le lien </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/BERT.ipynb</a> </p><br><p>  Dans le menu <strong>Runtime</strong> , sélectionnez <strong>Exécuter tout</strong> , de sorte que pour la première fois toutes les cellules démarrent, les téléchargements de modèle et les bibliothèques nécessaires sont connectés.  Acceptez de réinitialiser tout le Runtime si nécessaire. </p><br><div class="spoiler">  <b class="spoiler_title">En cas de problème ...</b> <div class="spoiler_text"><p>  Assurez-vous que GPU et Python 3 sont sélectionnés dans le menu Runtime -&gt; Change runtime type </p><br><p>  Si le bouton de connexion n'est pas actif, cliquez dessus pour devenir connecté. </p></div></div><br><p>  Maintenant, changez la <strong>phrase des</strong> lignes d'entrée <strong>phrase</strong> , <strong>phrase_1</strong> et <strong>phrase_2</strong> , et cliquez sur l'icône de lecture sur la gauche pour démarrer uniquement la cellule actuelle.  L'exécution de l'intégralité du portable n'est plus nécessaire. </p><br><p>  Vous pouvez exécuter BERT sur Google Colab même à partir d'un smartphone, mais s'il ne s'ouvre pas, vous devrez peut-être activer la case à cocher Version complète dans les paramètres de votre navigateur. </p><br><h2 id="chto-dalshe">  Et ensuite? </h2><br><p>  Pour former le BERT à une tâche spécifique, vous devez ajouter une ou deux couches d'un réseau Feed Forward simple au-dessus de celui-ci et ne le former que sans toucher au réseau BERT principal.  Cela peut être fait sur TensorFlow nu ou via le shell Keras BERT.  Cette formation supplémentaire pour un domaine spécifique se produit très rapidement et est complètement similaire au réglage fin dans les réseaux à convolution.  Ainsi, pour la tâche SQuAD, vous pouvez former un réseau neuronal sur un TPU en seulement 30 minutes (contre 4 jours sur 16 TPU pour la formation du BERT lui-même). </p><br><p>  Pour ce faire, vous devrez étudier la façon dont les dernières couches sont représentées dans BERT, ainsi que disposer d'un ensemble de données approprié.  Sur la page officielle du BERT <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://github.com/google-research/bert,</a> vous trouverez plusieurs exemples de différentes tâches, ainsi que des instructions sur la façon de commencer un recyclage sur les TPU cloud.  Et tout le reste devra chercher dans la source dans les fichiers <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">run_classifier.py</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">extract_features.py</a> . </p><br><h3 id="ps">  PS </h3><br><p>  Le code et le bloc-notes jupyter pour Google Colab présentés ici <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><strong>sont hébergés dans le référentiel</strong></a> . </p><br><p>  Il ne faut pas s'attendre à des miracles.  Ne vous attendez pas à ce que BERT parle comme une personne.  Le statut de l'état de l'art ne signifie pas du tout que les progrès de la PNL ont atteint un niveau acceptable.  Cela signifie simplement que BERT est meilleur que les modèles précédents, qui étaient encore pires.  Une IA conversationnelle solide est encore très loin.  De plus, BERT est principalement un modèle de langage, pas un bot de chat prêt à l'emploi, il ne montre donc de bons résultats qu'après un recyclage pour une tâche spécifique. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr436878/">https://habr.com/ru/post/fr436878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr436866/index.html">Comment reconnaître les faux projets Agile</a></li>
<li><a href="../fr436868/index.html">Intégrez une analyse statique dans le processus, ne cherchez pas de bogues avec elle</a></li>
<li><a href="../fr436872/index.html">PGConf.Russie 2019 à venir</a></li>
<li><a href="../fr436874/index.html">Danses du Nouvel An autour de l'adaptateur FC ou un conte sur la distance entre les causes du problème et les symptômes</a></li>
<li><a href="../fr436876/index.html">[SAP] SAPUI5 for dummies part 1: Un exercice complet étape par étape</a></li>
<li><a href="../fr436880/index.html">Notions de base sur les modèles C ++: modèles de fonctions</a></li>
<li><a href="../fr436884/index.html">Nous maîtrisons async / wait sur un exemple réel</a></li>
<li><a href="../fr436886/index.html">Utiliser Babel et Webpack pour configurer un projet React à partir de zéro</a></li>
<li><a href="../fr436888/index.html">Histoire sur la conception d'une API</a></li>
<li><a href="../fr436890/index.html">Tutoriel React Partie 10: Atelier sur l'utilisation des propriétés et du style des composants</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>