<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ó üö¥üèΩ üë™ Neuronale Netze und tiefes Lernen, Kapitel 3, Teil 3: Wie werden Hyperparameter f√ºr neuronale Netze ausgew√§hlt? ü§§ üë©üèΩ‚Äçüíª üéá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inhalt 

- Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen 
- Kapitel 2: Funktionsweise des Backpropagation-Algorithmus 
-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und tiefes Lernen, Kapitel 3, Teil 3: Wie werden Hyperparameter f√ºr neuronale Netze ausgew√§hlt?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460711/"><div class="spoiler">  <b class="spoiler_title">Inhalt</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 1: Verwenden neuronaler Netze zum Erkennen handgeschriebener Zahlen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 2: Funktionsweise des Backpropagation-Algorithmus</a> </li><li>  Kapitel 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Verbesserung der Methode zum Trainieren neuronaler Netze</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: Warum tr√§gt die Regularisierung dazu bei, die Umschulung zu reduzieren?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 3: Wie w√§hlt man Hyperparameter f√ºr neuronale Netze?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 4: Visueller Beweis, dass neuronale Netze jede Funktion berechnen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kapitel 5: Warum sind tiefe neuronale Netze so schwer zu trainieren?</a> </li><li>  Kapitel 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 1: Deep Learning</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Teil 2: J√ºngste Fortschritte bei der Bilderkennung</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nachwort: Gibt es einen einfachen Algorithmus zum Erstellen von Intelligenz?</a> </li></ul></div></div><br>  Bisher habe ich nicht erkl√§rt, wie ich die Werte von Hyperparametern w√§hle - die Lerngeschwindigkeit Œ∑, den Regularisierungsparameter Œª und so weiter.  Ich habe nur sch√∂ne Arbeitswerte gegeben.  In der Praxis kann es schwierig sein, gute Hyperparameter zu finden, wenn Sie ein neuronales Netzwerk verwenden, um ein Problem anzugreifen.  Stellen Sie sich zum Beispiel vor, wir h√§tten gerade etwas √ºber das MNIST-Problem erfahren und damit begonnen, ohne etwas √ºber die Werte geeigneter Hyperparameter zu wissen.  Nehmen wir an, wir hatten zuf√§llig Gl√ºck und haben in den ersten Experimenten viele Hyperparameter ausgew√§hlt, wie wir es bereits in diesem Kapitel getan haben: 30 versteckte Neuronen, eine Mini-Paketgr√∂√üe von 10, Training f√ºr 30 Epochen und die Verwendung von Kreuzentropie.  Wir haben jedoch die Lernrate Œ∑ = 10,0 und den Regularisierungsparameter Œª = 1000,0 gew√§hlt.  Und hier ist, was ich bei einem solchen Lauf gesehen habe: <br><a name="habracut"></a><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist_loader &gt;&gt;&gt; training_data, validation_data, test_data = \ ... mnist_loader.load_data_wrapper() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> network2 &gt;&gt;&gt; net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10.0</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">1000.0</span></span>, ... evaluation_data=validation_data, monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) Epoch <span class="hljs-number"><span class="hljs-number">0</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">1030</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span> Epoch <span class="hljs-number"><span class="hljs-number">1</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">990</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">1009</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span> ... Epoch <span class="hljs-number"><span class="hljs-number">27</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">1009</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span> Epoch <span class="hljs-number"><span class="hljs-number">28</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">983</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span> Epoch <span class="hljs-number"><span class="hljs-number">29</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">967</span></span> / <span class="hljs-number"><span class="hljs-number">10000</span></span></code> </pre> <br>  Unsere Klassifizierung funktioniert nicht besser als Stichproben!  Unser Netzwerk arbeitet als Zufallsrauschgenerator! <br><br>  "Nun, das ist einfach zu beheben", k√∂nnte man sagen, "reduzieren Sie einfach Hyperparameter wie Lerngeschwindigkeit und Regularisierung."  Leider haben Sie a priori keine Informationen dar√ºber, welche genauen Hyperparameter Sie anpassen m√ºssen.  Vielleicht ist das Hauptproblem, dass unsere 30 versteckten Neuronen niemals funktionieren, unabh√§ngig davon, wie die anderen Hyperparameter ausgew√§hlt werden?  Vielleicht brauchen wir mindestens 100 versteckte Neuronen?  Oder 300?  Oder viele versteckte Schichten?  Oder ein anderer Ansatz zur Ausgabecodierung?  Vielleicht lernt unser Netzwerk, aber wir m√ºssen es mehr Epochen trainieren?  Vielleicht ist die Gr√∂√üe der Minipakete zu klein?  Vielleicht h√§tten wir es besser gemacht, wenn wir zur quadratischen Funktion des Wertes zur√ºckgekehrt w√§ren?  Vielleicht m√ºssen wir einen anderen Ansatz zum Initialisieren von Gewichten ausprobieren?  Und so weiter und so fort.  Im Raum der Hyperparameter kann man sich leicht verlaufen.  Dies kann sehr unangenehm sein, wenn Ihr Netzwerk sehr gro√ü ist oder gro√üe Mengen an Trainingsdaten verwendet und Sie sie stunden-, tag- oder wochenlang trainieren k√∂nnen, ohne Ergebnisse zu erhalten.  In einer solchen Situation beginnt Ihr Vertrauen zu vergehen.  Vielleicht waren neuronale Netze der falsche Ansatz, um Ihr Problem zu l√∂sen?  Vielleicht k√ºndigst du und machst Imkerei? <br><br>  In diesem Abschnitt werde ich einige heuristische Ans√§tze erl√§utern, mit denen Sie Hyperparameter in einem neuronalen Netzwerk konfigurieren k√∂nnen.  Ziel ist es, Ihnen bei der Ausarbeitung eines Workflows zu helfen, mit dem Sie Hyperparameter recht gut konfigurieren k√∂nnen.  Nat√ºrlich kann ich nicht das gesamte Thema der Hyperparameteroptimierung behandeln.  Dies ist ein riesiger Bereich, und dies ist kein Problem, das vollst√§ndig oder nach den richtigen L√∂sungsstrategien gel√∂st werden kann, bei denen eine allgemeine √úbereinstimmung besteht.  Es besteht immer die M√∂glichkeit, einen anderen Trick auszuprobieren, um zus√§tzliche Ergebnisse aus Ihrem neuronalen Netzwerk herauszuholen.  Die Heuristik in diesem Abschnitt sollte Ihnen jedoch einen Ausgangspunkt geben. <br><br><h3>  Allgemeine Strategie </h3><br>  Wenn ein neuronales Netzwerk verwendet wird, um ein neues Problem anzugreifen, besteht die erste Schwierigkeit darin, nicht triviale Ergebnisse aus dem Netzwerk zu erhalten, dh eine zuf√§llige Wahrscheinlichkeit zu √ºberschreiten.  Dies kann √ºberraschend schwierig sein, insbesondere wenn Sie mit einer neuen Klasse von Aufgaben konfrontiert sind.  Schauen wir uns einige Strategien an, die f√ºr diese Art von Schwierigkeit verwendet werden k√∂nnen. <br><br>  Angenommen, Sie sind der erste, der die MNIST-Aufgabe angreift.  Sie beginnen mit gro√üer Begeisterung, aber der vollst√§ndige Ausfall Ihres ersten Netzwerks ist ein wenig entmutigend, wie im obigen Beispiel beschrieben.  Dann m√ºssen Sie das Problem in Teile zerlegen.  Sie m√ºssen alle Trainings- und Unterst√ºtzungsbilder entfernen, mit Ausnahme von Bildern mit Nullen und Einsen.  Versuchen Sie dann, das Netzwerk so zu trainieren, dass 0 von 1 unterschieden wird. Diese Aufgabe ist nicht nur wesentlich einfacher als das Unterscheiden aller zehn Ziffern, sondern reduziert auch die Menge der Trainingsdaten um 80% und beschleunigt das Lernen um das F√ºnffache.  Auf diese Weise k√∂nnen Sie Experimente viel schneller durchf√ºhren und schnell verstehen, wie Sie ein gutes Netzwerk erstellen. <br><br>  Experimente k√∂nnen weiter beschleunigt werden, indem das Netzwerk auf eine Mindestgr√∂√üe reduziert wird, die wahrscheinlich sinnvoll trainiert wird.  Wenn Sie der Meinung sind, dass das Netzwerk [784, 10] MNIST-Ziffern sehr wahrscheinlich besser klassifizieren kann als eine Zufallsstichprobe, experimentieren Sie damit.  Es ist viel schneller als das Training [784, 30, 10], und Sie k√∂nnen bereits sp√§ter darauf aufbauen. <br><br>  Eine weitere Beschleunigung der Experimente kann durch Erh√∂hen der Verfolgungsfrequenz erhalten werden.  Im Programm network2.py √ºberwachen wir die Qualit√§t der Arbeit am Ende jeder √Ñra.  Bei der Verarbeitung von 50.000 Bildern pro √Ñra m√ºssen wir ziemlich lange warten - etwa 10 Sekunden pro √Ñra auf meinem Laptop, wenn ich das Netzwerk lerne [784, 30, 10] -, bevor wir Feedback zur Qualit√§t des Netzwerklernens erhalten.  Nat√ºrlich sind zehn Sekunden nicht so lang, aber wenn Sie mehrere Dutzend verschiedener Hyperparameter ausprobieren m√∂chten, wird es √§rgerlich, und wenn Sie Hunderte oder Tausende von Optionen ausprobieren m√∂chten, ist dies nur verheerend.  Feedback kann viel schneller empfangen werden, indem die Best√§tigungsgenauigkeit h√§ufiger verfolgt wird, z. B. alle 1000 Trainingsbilder.  Anstatt den vollst√§ndigen Satz von 10.000 Best√§tigungsbildern zu verwenden, k√∂nnen wir mit nur 100 Best√§tigungsbildern eine viel schnellere Sch√§tzung erhalten.  Die Hauptsache ist, dass das Netzwerk gen√ºgend Bilder sieht, um wirklich zu lernen und eine ausreichend gute Einsch√§tzung der Effektivit√§t zu erhalten.  Nat√ºrlich bietet unser network2.py ein solches Tracking noch nicht an.  Um diesen Effekt zur Veranschaulichung zu erzielen, beschneiden wir unsere Trainingsdaten auf die ersten 1000 MNIST-Bilder.  Lassen Sie uns versuchen zu sehen, was passiert (der Einfachheit halber habe ich nicht die Idee verwendet, nur die Bilder 0 und 1 zu belassen - dies kann auch mit etwas mehr Aufwand realisiert werden). <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data[:<span class="hljs-number"><span class="hljs-number">1000</span></span>], <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10.0</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">1000.0</span></span>, \ ... evaluation_data=validation_data[:<span class="hljs-number"><span class="hljs-number">100</span></span>], \ ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) Epoch <span class="hljs-number"><span class="hljs-number">0</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">1</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> ...</code> </pre> <br>  Wir bekommen immer noch reines Rauschen, aber wir haben einen gro√üen Vorteil: Das Feedback wird in Sekundenbruchteilen und nicht alle zehn Sekunden aktualisiert.  Dies bedeutet, dass Sie mit der Auswahl von Hyperparametern viel schneller experimentieren oder sogar fast gleichzeitig mit vielen verschiedenen Hyperparametern experimentieren k√∂nnen. <br><br>  Im obigen Beispiel habe ich den Wert von Œª wie zuvor gleich 1000,0 belassen.  Da wir jedoch die Anzahl der Trainingsbeispiele ge√§ndert haben, m√ºssen wir Œª √§ndern, damit die Schw√§chung der Gewichte gleich ist.  Dies bedeutet, dass wir Œª um 20,0 √§ndern.  In diesem Fall stellt sich Folgendes heraus: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data[:<span class="hljs-number"><span class="hljs-number">1000</span></span>], <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10.0</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">20.0</span></span>, \ ... evaluation_data=validation_data[:<span class="hljs-number"><span class="hljs-number">100</span></span>], \ ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) Epoch <span class="hljs-number"><span class="hljs-number">0</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">12</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">1</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">14</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">25</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">18</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> ...</code> </pre> <br>  Ja!  Wir haben ein Signal.  Nicht besonders gut, aber es gibt.  Dies kann bereits als Ausgangspunkt genommen werden und die Hyperparameter √§ndern, um weitere Verbesserungen zu erzielen.  Angenommen, wir entscheiden, dass wir die Lerngeschwindigkeit erh√∂hen m√ºssen (wie Sie wahrscheinlich verstanden haben, haben wir uns aus dem Grund, den wir sp√§ter diskutieren werden, falsch entschieden, aber versuchen wir dies zun√§chst).  Um unsere Vermutung zu testen, drehen wir Œ∑ auf 100,0: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data[:<span class="hljs-number"><span class="hljs-number">1000</span></span>], <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100.0</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">20.0</span></span>, \ ... evaluation_data=validation_data[:<span class="hljs-number"><span class="hljs-number">100</span></span>], \ ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) Epoch <span class="hljs-number"><span class="hljs-number">0</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">1</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">10</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> ...</code> </pre> <br>  Alles ist schlecht!  Anscheinend war unsere Vermutung falsch und das Problem lag nicht im sehr niedrigen Wert der Lerngeschwindigkeit.  Wir versuchen, Œ∑ auf einen kleinen Wert von 1,0 festzuziehen: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>net = network2.Network([<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]) &gt;&gt;&gt; net.SGD(training_data[:<span class="hljs-number"><span class="hljs-number">1000</span></span>], <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, lmbda = <span class="hljs-number"><span class="hljs-number">20.0</span></span>, \ ... evaluation_data=validation_data[:<span class="hljs-number"><span class="hljs-number">100</span></span>], \ ... monitor_evaluation_accuracy=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) Epoch <span class="hljs-number"><span class="hljs-number">0</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">62</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">1</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">42</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">43</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span> training complete Accuracy on evaluation data: <span class="hljs-number"><span class="hljs-number">61</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> ...</code> </pre> <br>  Das ist besser!  Und so k√∂nnen wir weiter machen, jeden Hyperparameter verdrehen und die Effizienz schrittweise verbessern.  Nachdem wir die Situation untersucht und einen verbesserten Wert f√ºr Œ∑ gefunden haben, suchen wir nach einem guten Wert f√ºr Œª.  Dann werden wir ein Experiment mit einer komplexeren Architektur durchf√ºhren, zum Beispiel mit einem Netzwerk von 10 versteckten Neuronen.  Dann optimieren wir erneut die Parameter f√ºr Œ∑ und Œª.  Dann werden wir das Netzwerk auf 20 versteckte Neuronen erh√∂hen.  Ein wenig an den Hyperparametern anpassen.  Und so weiter, indem Sie die Wirksamkeit bei jedem Schritt anhand eines Teils unserer unterst√ºtzenden Daten bewerten und anhand dieser Sch√§tzungen die besten Hyperparameter ausw√§hlen.  Im Zuge von Verbesserungen dauert es immer l√§nger, den Effekt der Optimierung von Hyperparametern zu erkennen, sodass wir die Tracking-Frequenz schrittweise reduzieren k√∂nnen. <br><br>  Als Gesamtstrategie sieht dieser Ansatz vielversprechend aus.  Ich m√∂chte jedoch zu diesem ersten Schritt bei der Suche nach Hyperparametern zur√ºckkehren, die es dem Netzwerk erm√∂glichen, zumindest irgendwie zu lernen.  Selbst im obigen Beispiel war die Situation zu optimistisch.  Die Arbeit mit einem Netzwerk, das nichts lernt, kann √§u√üerst √§rgerlich sein.  Sie k√∂nnen Hyperparameter f√ºr mehrere Tage anpassen und erhalten keine aussagekr√§ftigen Antworten.  Daher m√∂chte ich noch einmal betonen, dass Sie in einem fr√ºhen Stadium sicherstellen m√ºssen, dass Sie schnelles Feedback von Experimenten erhalten.  Intuitiv scheint es, dass die Vereinfachung des Problems und der Architektur Sie nur verlangsamen wird.  Dies beschleunigt den Prozess, da Sie ein Netzwerk mit einem aussagekr√§ftigen Signal viel schneller finden k√∂nnen.  Wenn Sie ein solches Signal empfangen haben, k√∂nnen Sie beim Einstellen von Hyperparametern h√§ufig schnelle Verbesserungen erzielen.  Wie in vielen Lebenssituationen ist es am schwierigsten, den Prozess zu starten. <br><br>  Okay, das ist eine allgemeine Strategie.  Schauen wir uns nun die spezifischen Empfehlungen f√ºr die Verschreibung von Hyperparametern an.  Ich werde mich auf die Lerngeschwindigkeit Œ∑, den Regularisierungsparameter L2 Œª und die Gr√∂√üe des Minipakets konzentrieren.  Viele Kommentare werden jedoch auf andere Hyperparameter anwendbar sein, einschlie√ülich solcher, die sich auf die Netzwerkarchitektur, andere Formen der Regularisierung und einige Hyperparameter beziehen, die wir sp√§ter in diesem Buch erfahren werden, beispielsweise den Impulskoeffizienten. <br><br><h3>  Lerngeschwindigkeit </h3><br>  Angenommen, wir haben drei MNIST-Netzwerke mit drei unterschiedlichen Lerngeschwindigkeiten gestartet: Œ∑ = 0,025, Œ∑ = 0,25 bzw. Œ∑ = 2,5.  Wir werden den Rest der Hyperparameter so lassen, wie sie in den vorherigen Abschnitten waren - 30 Epochen, die Gr√∂√üe des Minipakets betr√§gt 10, Œª = 5,0.  Wir werden auch wieder alle 50.000 Trainingsbilder verwenden.  Hier ist eine Grafik, die das Verhalten der Schulungskosten zeigt (erstellt vom Programm multiple_eta.py): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ec/dc3/761/1ecdc37614b9207a1f03ce2aad97e61d.png"><br><br>  Bei Œ∑ = 0,025 sinken die Kosten reibungslos bis zur letzten √Ñra.  Mit Œ∑ = 0,25 sinken die Kosten zun√§chst, aber nach 20 Epochen sind sie ges√§ttigt, sodass sich die meisten √Ñnderungen als kleine und offensichtlich zuf√§llige Schwankungen herausstellen.  Mit Œ∑ = 2,5 variieren die Kosten von Anfang an stark.  Um den Grund f√ºr diese Schwankungen zu verstehen, erinnern wir uns, dass der stochastische Gradientenabstieg uns allm√§hlich in das Tal der Kostenfunktion senken sollte: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/37e/5f9/af1/37e5f9af1985610568cba31157e35763.png"><br><br>  Dieses Bild hilft, sich intuitiv vorzustellen, was passiert, ist jedoch keine vollst√§ndige und umfassende Erkl√§rung.  Genauer gesagt, aber kurz, verwendet der Gradientenabstieg eine N√§herung erster Ordnung f√ºr die Kostenfunktion, um zu verstehen, wie die Kosten gesenkt werden k√∂nnen.  F√ºr gr√∂√üere Œ∑ werden Mitglieder einer Kostenfunktion h√∂herer Ordnung wichtiger, und sie k√∂nnen das Verhalten dominieren, indem sie den Gradientenabstieg unterbrechen.  Dies ist besonders wahrscheinlich, wenn Sie sich den Minima und lokalen Minima der Kostenfunktion n√§hern, da neben solchen Punkten der Gradient klein wird, was es Mitgliedern h√∂herer Ordnung erleichtert, zu dominieren. <br><br>  Wenn jedoch Œ∑ zu gro√ü ist, sind die Stufen so gro√ü, dass sie ein Minimum √ºberspringen k√∂nnen, wodurch der Algorithmus aus dem Tal aufsteigt.  Wahrscheinlich ist dies der Grund, warum der Preis bei Œ∑ = 2,5 schwankt.  Die Wahl von Œ∑ = 0,25 f√ºhrt dazu, dass die ersten Schritte uns wirklich zu einem Minimum der Kostenfunktion f√ºhren, und erst wenn wir dazu kommen, treten Schwierigkeiten beim Springen auf.  Und wenn wir Œ∑ = 0,025 w√§hlen, haben wir in den ersten 30 Epochen keine derartigen Schwierigkeiten.  Die Wahl eines so kleinen Wertes von Œ∑ schafft nat√ºrlich eine weitere Schwierigkeit - n√§mlich die Verlangsamung des stochastischen Gradientenabfalls.  Der beste Ansatz w√§re, mit Œ∑ = 0,25 zu beginnen, 20 Epochen zu lernen und dann zu Œ∑ = 0,025 zu gehen.  Sp√§ter werden wir eine solche variable Lernrate diskutieren.  Lassen Sie uns in der Zwischenzeit auf die Frage eingehen, einen geeigneten Wert f√ºr die Lerngeschwindigkeit Œ∑ zu finden. <br><br>  In diesem Sinne k√∂nnen wir Œ∑ wie folgt w√§hlen.  Zun√§chst bewerten wir den Schwellenwert Œ∑, bei dem die Kosten f√ºr Trainingsdaten sofort zu sinken beginnen, aber nicht schwanken und nicht zunehmen.  Diese Sch√§tzung muss nicht genau sein.  Die Reihenfolge kann gesch√§tzt werden, indem mit Œ∑ = 0,01 begonnen wird.  Wenn die Kosten in den ersten Epochen sinken, lohnt es sich, Œ∑ = 0,1, dann 1,0 usw. zu versuchen, bis Sie einen Wert finden, bei dem der Wert in den ersten Epochen schwankt oder steigt.  Und umgekehrt, wenn der Wert in den ersten Epochen mit Œ∑ = 0,01 schwankt oder zunimmt, versuchen Sie es mit Œ∑ = 0,001, Œ∑ = 0,0001, bis Sie den Wert finden, bei dem die Kosten in den ersten Epochen sinken.  Diese Prozedur gibt Ihnen die Reihenfolge des Schwellenwerts Œ∑.  Wenn Sie m√∂chten, k√∂nnen Sie Ihre Bewertung verfeinern, indem Sie den h√∂chsten Wert f√ºr Œ∑ ausw√§hlen, bei dem die Kosten in den ersten Epochen sinken, z. B. Œ∑ = 0,5 oder Œ∑ = 0,2 (Supergenauigkeit ist hier nicht erforderlich).  Dies gibt uns eine Sch√§tzung des Schwellenwerts Œ∑. <br><br>  Der reale Wert von Œ∑ sollte offensichtlich den ausgew√§hlten Schwellenwert nicht √ºberschreiten.  Damit der Œ∑-Wert f√ºr viele Epochen n√ºtzlich bleibt, sollten Sie einen Wert verwenden, der zweimal kleiner als der Schwellenwert ist.  Eine solche Wahl erm√∂glicht es Ihnen normalerweise, aus vielen Epochen zu lernen, ohne Ihr Lernen stark zu verlangsamen. <br><br>  Im Fall von MNIST-Daten f√ºhrt das Befolgen dieser Strategie zu einer Sch√§tzung der Schwellenordnung von Œ∑ bei 0,1.  Nach einiger Verfeinerung erhalten wir den Wert Œ∑ = 0,5.  Nach dem obigen Rezept sollten wir Œ∑ = 0,25 f√ºr unsere Lerngeschwindigkeit verwenden.  Tats√§chlich stellte ich jedoch fest, dass Œ∑ = 0,5 f√ºr 30 Epochen gut funktionierte, sodass ich mir keine Sorgen dar√ºber machte, es zu verringern. <br><br>  Das alles sieht ziemlich einfach aus.  Die Verwendung der Trainingskosten zur Auswahl von Œ∑ scheint jedoch im Widerspruch zu dem zu stehen, was ich zuvor gesagt habe: Wir w√§hlen Hyperparameter und bewerten die Effektivit√§t des Netzwerks anhand ausgew√§hlter Best√§tigungsdaten.  Tats√§chlich werden wir die Genauigkeit der Best√§tigung verwenden, um die Regularisierungshyperparameter, die Gr√∂√üe des Minipakets und Netzwerkparameter wie die Anzahl der Schichten und versteckten Neuronen usw. auszuw√§hlen.  Warum machen wir es mit Lerngeschwindigkeit anders?  Ehrlich gesagt ist diese Wahl auf meine pers√∂nlichen √§sthetischen Vorlieben zur√ºckzuf√ºhren und wahrscheinlich voreingenommen.  Das Argument ist, dass andere Hyperparameter die endg√ºltige Klassifizierungsgenauigkeit des Testsatzes verbessern sollten. Daher ist es sinnvoll, sie basierend auf der Genauigkeit der Best√§tigung auszuw√§hlen.  Die Lernrate wirkt sich jedoch nur indirekt auf die endg√ºltige Klassifizierungsgenauigkeit aus.  Das Hauptziel besteht darin, die Schrittgr√∂√üe des Gradientenabstiegs zu steuern und die Trainingskosten bestm√∂glich zu verfolgen, um eine zu gro√üe Schrittgr√∂√üe zu erkennen.  Dies ist jedoch eine pers√∂nliche √§sthetische Pr√§ferenz.  In den fr√ºhen Phasen des Trainings sinken die Trainingskosten normalerweise nur, wenn die Genauigkeit der Best√§tigung zunimmt. In der Praxis sollte es daher keine Rolle spielen, welche Kriterien verwendet werden sollen. <br><br><h3>  Verwenden Sie einen fr√ºhen Stopp, um die Anzahl der Trainingszeiten zu bestimmen </h3><br>  Wie wir in diesem Kapitel erw√§hnt haben, bedeutet ein fr√ºher Stopp, dass wir am Ende jeder √Ñra die Genauigkeit der Klassifizierung anhand der unterst√ºtzenden Daten berechnen m√ºssen.  Wenn es sich nicht mehr verbessert, h√∂ren wir auf zu arbeiten.  Infolgedessen wird das Festlegen der Anzahl der Epochen zu einer einfachen Angelegenheit.  Dies bedeutet insbesondere, dass wir nicht genau herausfinden m√ºssen, wie die Anzahl der Epochen von anderen Hyperparametern abh√§ngt.  Dies geschieht automatisch.  Dar√ºber hinaus verhindert ein fr√ºher Stopp automatisch eine Umschulung.  Dies ist nat√ºrlich gut, obwohl es n√ºtzlich sein kann, den fr√ºhen Stopp in den fr√ºhen Phasen der Experimente auszuschalten, damit Sie Anzeichen einer Umschulung erkennen und sie zur Feinabstimmung des Regularisierungsansatzes verwenden k√∂nnen. <br><br>  Um den RO zu implementieren, m√ºssen wir genauer beschreiben, was es bedeutet, die Verbesserung der Klassifizierungsgenauigkeit zu stoppen.  Wie wir gesehen haben, kann die Genauigkeit sehr weit hin und her springen, selbst wenn sich der Gesamttrend verbessert.  Wenn wir zum ersten Mal anhalten und die Genauigkeit abnimmt, werden wir mit ziemlicher Sicherheit keine weiteren Verbesserungen erzielen.  Der beste Ansatz besteht darin, das Lernen zu beenden, wenn sich die beste Klassifizierungsgenauigkeit √ºber einen l√§ngeren Zeitraum nicht verbessert.  Nehmen wir zum Beispiel an, wir besch√§ftigen uns mit MNIST.  Dann k√∂nnen wir entscheiden, den Prozess zu stoppen, wenn sich die Genauigkeit der Klassifizierung in den letzten zehn Epochen nicht verbessert hat.  Dies stellt sicher, dass wir nicht zu fr√ºh aufh√∂ren, weil das Training fehlgeschlagen ist, aber wir werden nicht ewig auf Verbesserungen warten, die nicht eintreten werden. <br><br>  Diese Regel ‚ÄûKeine Verbesserung gegen√ºber zehn Epochen‚Äú eignet sich gut f√ºr die erste MNIST-Studie.  Netzwerke k√∂nnen jedoch manchmal ein Plateau nahe einer bestimmten Klassifizierungsgenauigkeit erreichen, dort einige Zeit bleiben und sich dann wieder verbessern.  Wenn Sie eine sehr gute Leistung erzielen m√∂chten, ist die Regel ‚ÄûKeine Verbesserung √ºber zehn Epochen‚Äú m√∂glicherweise zu aggressiv.  Daher empfehle ich, f√ºr prim√§re Experimente die Regel "Keine Verbesserung √ºber zehn Epochen" zu verwenden und nach und nach weichere Regeln anzuwenden, wenn Sie das Verhalten Ihres Netzwerks besser verstehen: "Keine Verbesserung √ºber zwanzig Epochen", "Keine Verbesserung √ºber f√ºnfzig Epochen" usw. weiter.  Dies gibt uns nat√ºrlich einen weiteren Hyperparameter f√ºr die Optimierung!  In der Praxis ist dieser Hyperparameter jedoch normalerweise leicht auf gute Ergebnisse abzustimmen.  Und f√ºr andere Aufgaben als MNIST kann die Regel ‚ÄûKeine Verbesserung √ºber zehn Epochen‚Äú zu aggressiv oder nicht aggressiv genug sein, abh√§ngig von den Einzelheiten einer bestimmten Aufgabe.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nach einigem Experimentieren ist es jedoch normalerweise recht einfach, eine geeignete Early-Stop-Strategie zu finden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben in unseren Experimenten mit MNIST noch keinen fr√ºhen Stopp verwendet. </font><font style="vertical-align: inherit;">Dies liegt an der Tatsache, dass wir viele Vergleiche verschiedener Lernans√§tze durchgef√ºhrt haben. </font><font style="vertical-align: inherit;">F√ºr solche Vergleiche ist es sinnvoll, in allen F√§llen die gleiche Anzahl von Epochen zu verwenden. </font><font style="vertical-align: inherit;">Es lohnt sich jedoch, network2.py zu √§ndern, indem Sie den RO in das Programm einf√ºhren.</font></font><br><br><h3>  Die Aufgaben </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √Ñndern Sie network2.py so, dass die Bestellung dort gem√§√ü der Regel "Keine √Ñnderung f√ºr n Epochen" angezeigt wird, wobei n ein konfigurierbarer Parameter ist. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stellen Sie sich eine andere Regel f√ºr den fr√ºhen Stopp vor als ‚Äûin n Epochen unver√§ndert‚Äú. </font><font style="vertical-align: inherit;">Im Idealfall sollte die Regel einen Kompromiss zwischen der Erzielung einer Genauigkeit mit hoher Best√§tigung und einer relativ kurzen Schulungszeit suchen. </font><font style="vertical-align: inherit;">F√ºgen Sie network2.py eine Regel hinzu, und f√ºhren Sie drei Experimente durch, in denen die Validierungsgenauigkeit und die Anzahl der Trainingsperioden mit der Regel "Keine √Ñnderung √ºber 10 Epochen" verglichen werden.</font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Plan zur √Ñnderung der Lerngeschwindigkeit </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W√§hrend wir die Lerngeschwindigkeit Œ∑ konstant hielten. </font><font style="vertical-align: inherit;">Es ist jedoch oft n√ºtzlich, es zu √§ndern. </font><font style="vertical-align: inherit;">In den fr√ºhen Phasen des Trainingsprozesses werden Gewichte h√∂chstwahrscheinlich v√∂llig falsch zugewiesen. </font><font style="vertical-align: inherit;">Daher ist es besser, eine hohe Trainingsrate zu verwenden, da sich die Gewichte schneller √§ndern. </font><font style="vertical-align: inherit;">Dann k√∂nnen Sie die Trainingsgeschwindigkeit reduzieren, um die Waage feiner einzustellen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie skizzieren wir einen Plan zur √Ñnderung der Lerngeschwindigkeit? Hier k√∂nnen Sie viele Ans√§tze anwenden. Eine nat√ºrliche Option besteht darin, dieselbe Grundidee wie in RO zu verwenden. Wir halten die Lerngeschwindigkeit konstant, bis sich die Genauigkeit der Best√§tigung zu verschlechtern beginnt. Dann reduzieren wir den CO um einen bestimmten Betrag, beispielsweise zwei- oder zehnmal. Wir wiederholen dies viele Male, bis der CO 1024 (oder 1000) Mal weniger als der urspr√ºngliche ist. Und beende das Training.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein Plan zur √Ñnderung der Lerngeschwindigkeit kann die Effizienz verbessern und bietet enorme M√∂glichkeiten f√ºr die Auswahl eines Plans. </font><font style="vertical-align: inherit;">Und das kann Kopfschmerzen bereiten - Sie k√∂nnen den Plan f√ºr immer optimieren. </font><font style="vertical-align: inherit;">F√ºr die ersten Experimente w√ºrde ich vorschlagen, einen einzelnen konstanten CO-Wert zu verwenden. </font><font style="vertical-align: inherit;">Dies gibt Ihnen eine gute erste Ann√§herung. </font><font style="vertical-align: inherit;">Wenn Sie sp√§ter die beste Effizienz aus dem Netzwerk herausholen m√∂chten, sollten Sie mit dem Plan experimentieren, um die Lerngeschwindigkeit so zu √§ndern, wie ich es beschrieben habe. </font><font style="vertical-align: inherit;">Eine ziemlich einfach zu lesende </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wissenschaftliche Arbeit von</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 2010 zeigt die Vorteile variabler Lerngeschwindigkeiten beim Angriff auf MNIST.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbung </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √Ñndern Sie network2.py so, dass der folgende Plan zum √Ñndern der Lerngeschwindigkeit implementiert wird: Halbieren Sie die CR jedes Mal, wenn die Genauigkeit der Best√§tigung der Regel ‚ÄûKeine √Ñnderung in 10 Epochen‚Äú entspricht, und beenden Sie das Lernen, wenn die Lerngeschwindigkeit von der urspr√ºnglichen auf 1/128 abf√§llt. </font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Regularisierungsparameter Œª </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich empfehle, √ºberhaupt ohne Regularisierung zu beginnen (Œª = 0,0) und den Wert von Œ∑ wie oben angegeben zu bestimmen. </font><font style="vertical-align: inherit;">Unter Verwendung des ausgew√§hlten Wertes von Œ∑ k√∂nnen wir dann die unterst√ºtzenden Daten verwenden, um einen guten Wert von Œª auszuw√§hlen. </font><font style="vertical-align: inherit;">Beginnen Sie mit Œª = 1,0 (ich habe kein gutes Argument f√ºr eine solche Wahl) und erh√∂hen oder verringern Sie sie dann um das Zehnfache, um die Effizienz bei der Arbeit mit best√§tigenden Daten zu erh√∂hen. </font><font style="vertical-align: inherit;">Nachdem wir die richtige Gr√∂√üenordnung gefunden haben, k√∂nnen wir den Wert von Œª genauer einstellen. </font><font style="vertical-align: inherit;">Danach ist es notwendig, wieder zur Optimierung Œ∑ zur√ºckzukehren.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbung </font></font></h3><br><ul><li>     ,        ,  Œª  Œ∑.      ,        Œª?      ,        Œ∑? </li></ul><br><h3>        </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie die Empfehlungen aus diesem Abschnitt verwenden, werden Sie feststellen, dass die ausgew√§hlten Werte von Œ∑ und Œª nicht immer genau denen entsprechen, die ich zuvor verwendet habe. Es ist nur so, dass das Buch Textbeschr√§nkungen aufweist, was es manchmal unpraktisch machte, Hyperparameter zu optimieren. Erinnern Sie sich an alle Vergleiche der verschiedenen Trainingsans√§tze, an denen wir gearbeitet haben - Vergleichen der quadratischen Kostenfunktion und der Kreuzentropie, alter und neuer Methoden zum Initialisieren von Gewichten, beginnend mit und ohne Regularisierung usw. Um diese Vergleiche aussagekr√§ftig zu machen, habe ich versucht, die Hyperparameter zwischen den verglichenen Ans√§tzen nicht zu √§ndern (oder sie richtig zu skalieren). Nat√ºrlich gibt es keinen Grund daf√ºr, dass dieselben Hyperparameter f√ºr alle unterschiedlichen Lernans√§tze optimal sind. Die von mir verwendeten Hyperparameter waren daher das Ergebnis eines Kompromisses.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alternativ k√∂nnte ich versuchen, alle Hyperparameter f√ºr jeden Lernansatz maximal zu optimieren. </font><font style="vertical-align: inherit;">Es w√§re ein besserer und ehrlicherer Ansatz, da wir von jedem Lernansatz das Beste nehmen w√ºrden. </font><font style="vertical-align: inherit;">Wir haben jedoch Dutzende von Vergleichen durchgef√ºhrt, und in der Praxis w√§re dies zu rechenintensiv. </font><font style="vertical-align: inherit;">Daher habe ich mich entschlossen, Kompromisse einzugehen, um ausreichend (aber nicht unbedingt optimale) Hyperparameteroptionen zu verwenden.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mini Pack Gr√∂√üe </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie w√§hle ich die Gr√∂√üe des Mini-Pakets? </font><font style="vertical-align: inherit;">Um diese Frage zu beantworten, nehmen wir zun√§chst an, dass wir an Online-Schulungen teilnehmen, dh wir verwenden ein Minipaket der Gr√∂√üe 1.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das offensichtliche Problem beim Online-Lernen besteht darin, dass die Verwendung von Minipaketen, die aus einem einzelnen Trainingsbeispiel bestehen, zu schwerwiegenden Fehlern bei der Sch√§tzung des Gradienten f√ºhrt. Tats√§chlich stellen diese Fehler jedoch kein so ernstes Problem dar. Der Grund ist, dass einzelne Gradientensch√§tzungen nicht sehr genau sein m√ºssen. Wir brauchen nur eine ausreichend genaue Sch√§tzung, damit unsere Kostenfunktion abnimmt. Es ist, als w√ºrden Sie versuchen, zum Nordmagnetpol zu gelangen, aber Sie h√§tten einen unzuverl√§ssigen Kompass, bei dem jede Messung um 10 bis 20 Grad falsch ist. Wenn Sie den Kompass ziemlich oft √ºberpr√ºfen und im Durchschnitt die richtige Richtung anzeigen, k√∂nnen Sie schlie√ülich zum Nordmagnetpol gelangen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Angesichts dieses Arguments sollten wir das Online-Lernen nutzen. In Wirklichkeit ist die Situation jedoch etwas komplizierter. In der Aufgabe zum letzten Kapitel habe ich darauf hingewiesen, dass Sie zur Berechnung der Gradientenaktualisierung f√ºr alle Beispiele im Minipaket Matrixtechniken gleichzeitig anstelle einer Schleife verwenden k√∂nnen. Abh√§ngig von den Details Ihrer Hardware und der linearen Algebra-Bibliothek kann es sich als viel schneller herausstellen, eine Sch√§tzung f√ºr ein Minipaket von beispielsweise 100 zu berechnen, als eine Gradientensch√§tzung f√ºr ein Minipaket in einem Zyklus f√ºr 100 Trainingsbeispiele zu berechnen. Dies kann sich beispielsweise als nur 50-mal langsamer und nicht als 100 herausstellen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Auf den ersten Blick scheint uns dies nicht viel zu helfen. Bei einer Mini-Paketgr√∂√üe von 100 sieht die Trainingsregel f√ºr Gewichte folgenderma√üen aus:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-1"><span class="MJXp-mtable" id="MJXp-Span-2"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-3" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-4" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-5"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-6" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font></span><span class="MJXp-msup" id="MJXp-Span-7"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-9" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-10" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-11"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-12" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ∑ </font></font></span><span class="MJXp-mfrac" id="MJXp-Span-14" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-15"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-16"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">100</font></font></span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-17"><span class=""><span class="MJXp-mo" id="MJXp-Span-18" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àë</font></font></span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19" style="margin-left: 0px;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></span></span></span><span class="MJXp-mi" id="MJXp-Span-20"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àá</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-21"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-23" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="6.154ex" viewBox="0 -1558.2 33095.6 2649.6" role="img" focusable="false" style="vertical-align: -2.535ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(30815,0)"><g id="mjx-eqn-100" transform="translate(0,157)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="1390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-29" x="1891" y="0"></use></g></g><g transform="translate(10345,0)"><g transform="translate(-15,0)"><g transform="translate(0,157)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2192" x="994" y="0"></use><g transform="translate(2272,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-3D" x="3561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="4617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2212" x="5556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-3B7" x="6557" y="0"></use><g transform="translate(7060,0)"><g transform="translate(120,0)"><rect stroke="none" width="1621" height="60" x="0" y="220"></rect><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31" x="560" y="676"></use><g transform="translate(60,-686)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="1001" y="0"></use></g></g></g><g transform="translate(9089,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJSZ2-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-78" x="735" y="-1487"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2207" x="10700" y="0"></use><g transform="translate(11533,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-78" x="1011" y="-213"></use></g></g></g></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> w \rightarrow w' = w-\eta \frac{1}{100} \sum_x \nabla C_x \tag{100} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hier geht die Summe √ºber die Trainingsbeispiele im Minipaket. </font><font style="vertical-align: inherit;">Vergleiche mit</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-24"><span class="MJXp-mtable" id="MJXp-Span-25"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-26" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-27" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-28"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-29" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font></span><span class="MJXp-msup" id="MJXp-Span-30"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-32" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-33" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-34"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-35" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ∑ </font></font></span><span class="MJXp-mi" id="MJXp-Span-37"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àá </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-38"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-39" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-40" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="2.901ex" viewBox="0 -883.9 33095.6 1249" role="img" focusable="false" style="vertical-align: -0.848ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(30815,0)"><g id="mjx-eqn-101" transform="translate(0,-30)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31" x="1390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-29" x="1891" y="0"></use></g></g><g transform="translate(12164,0)"><g transform="translate(-15,0)"><g transform="translate(0,-30)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2192" x="994" y="0"></use><g transform="translate(2272,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-3D" x="3561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="4617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2212" x="5556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-3B7" x="6557" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2207" x="7060" y="0"></use><g transform="translate(7894,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-78" x="1011" y="-213"></use></g></g></g></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> w \rightarrow w' = w-\eta \nabla C_x \tag{101} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">f√ºr das Online-Lernen. </font><font style="vertical-align: inherit;">Selbst wenn die Aktualisierung des Mini-Pakets 50-mal l√§nger dauert, scheint Online-Training die beste Option zu sein, da wir h√§ufiger aktualisiert werden. </font><font style="vertical-align: inherit;">Nehmen wir jedoch an, dass wir im Fall des Mini-Pakets die Lerngeschwindigkeit um das 100-fache erh√∂ht haben, dann wird die Aktualisierungsregel zu:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-41"><span class="MJXp-mtable" id="MJXp-Span-42"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-43" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-44" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-46" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font></span><span class="MJXp-msup" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-49" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-50" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-51"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-52" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ∑ </font></font></span><span class="MJXp-munderover" id="MJXp-Span-54"><span class=""><span class="MJXp-mo" id="MJXp-Span-55" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àë</font></font></span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-56" style="margin-left: 0px;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> x</font></font></span></span></span><span class="MJXp-mi" id="MJXp-Span-57"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àá </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-58"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-60" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="76.867ex" height="5.31ex" viewBox="0 -1402.6 33095.6 2286.5" role="img" focusable="false" style="vertical-align: -2.053ex; max-width: 638px;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g transform="translate(30815,0)"><g id="mjx-eqn-102" transform="translate(0,354)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-28"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-31" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-30" x="890" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-32" x="1390" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-29" x="1891" y="0"></use></g></g><g transform="translate(11275,0)"><g transform="translate(-15,0)"><g transform="translate(0,354)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2192" x="994" y="0"></use><g transform="translate(2272,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2032" x="1013" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-3D" x="3561" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-77" x="4617" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2212" x="5556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-3B7" x="6557" y="0"></use><g transform="translate(7227,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJSZ2-2211" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-78" x="735" y="-1487"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMAIN-2207" x="8838" y="0"></use><g transform="translate(9672,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/post/460711/&amp;usg=ALkJrhiL4MfxNrDJKoob5UnhiZqlE6tL5g#MJMATHI-78" x="1011" y="-213"></use></g></g></g></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> w \rightarrow w' = w-\eta \sum_x \nabla C_x \tag{102} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies √§hnelt 100 separaten Stufen des Online-Lernens mit einer Lerngeschwindigkeit von Œ∑. </font><font style="vertical-align: inherit;">Ein Schritt beim Online-Lernen dauert jedoch nur 50-mal so lange. </font><font style="vertical-align: inherit;">In der Realit√§t sind dies nat√ºrlich nicht genau 100 Ebenen des Online-Lernens, da im Minipaket alle ‚àáC </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> im Gegensatz zum kumulativen Lernen im Online-Fall f√ºr denselben Satz von Gewichten bewertet werden. </font><font style="vertical-align: inherit;">Und doch scheint die Verwendung gr√∂√üerer Minipakete den Prozess zu beschleunigen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Angesichts all dieser Faktoren ist die Auswahl der besten Mini-Pack-Gr√∂√üe ein Kompromiss. W√§hlen Sie zu klein und profitieren Sie nicht von guten Matrixbibliotheken, die f√ºr schnelle Hardware optimiert sind. W√§hlen Sie zu gro√ü und aktualisieren Sie das Gewicht nicht oft genug. Sie m√ºssen einen Kompromisswert ausw√§hlen, der die Lerngeschwindigkeit maximiert. Gl√ºcklicherweise ist die Wahl der Minipaketgr√∂√üe, bei der die Geschwindigkeit maximiert wird, relativ unabh√§ngig von anderen Hyperparametern (mit Ausnahme der allgemeinen Architektur). Um eine gute Minipaketgr√∂√üe zu finden, ist es daher nicht erforderlich, diese zu optimieren. Daher ist es ausreichend, akzeptable (nicht unbedingt optimale) Werte f√ºr andere Hyperparameter zu verwenden und dann mehrere verschiedene Gr√∂√üen von Minipaketen zu versuchen, wobei Œ∑ skaliert wird, wie oben angegeben.Erstellen Sie ein Diagramm der Genauigkeit der Best√§tigung im Verh√§ltnis zur Zeit (tats√§chlich verstrichene Zeit, keine Epochen!) Und w√§hlen Sie eine Minipaketgr√∂√üe, die die schnellste Leistungsverbesserung bietet. Mit der ausgew√§hlten Minipaketgr√∂√üe k√∂nnen Sie andere Hyperparameter optimieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sie zweifellos bereits verstanden haben, habe ich in unserer Arbeit nat√ºrlich keine solche Optimierung durchgef√ºhrt. </font><font style="vertical-align: inherit;">Bei unserer Umsetzung der Nationalversammlung wird ein schneller Ansatz zur Aktualisierung von Minipaketen √ºberhaupt nicht verwendet. </font><font style="vertical-align: inherit;">In fast allen Beispielen habe ich einfach die Mini-Paketgr√∂√üe 10 verwendet, ohne sie zu kommentieren oder zu erkl√§ren. </font><font style="vertical-align: inherit;">Im Allgemeinen k√∂nnten wir das Lernen beschleunigen, indem wir die Gr√∂√üe des Minipakets reduzieren. </font><font style="vertical-align: inherit;">Ich habe dies insbesondere nicht getan, weil meine vorl√§ufigen Experimente darauf hindeuteten, dass die Beschleunigung eher bescheiden sein w√ºrde. </font><font style="vertical-align: inherit;">In praktischen Implementierungen m√∂chten wir jedoch auf jeden Fall den schnellsten Ansatz zur Aktualisierung von Minipaketen implementieren und versuchen, deren Gr√∂√üe zu optimieren, um die Gesamtgeschwindigkeit zu maximieren.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Automatisierte Techniken </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich habe diese heuristischen Ans√§tze als etwas beschrieben, das von Hand optimiert werden muss. Manuelle Optimierung ist ein guter Weg, um eine Vorstellung davon zu bekommen, wie NS funktioniert. √úbrigens ist es nicht verwunderlich, dass bereits viel an der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatisierung</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dieses Projekts gearbeitet wurde. Eine √ºbliche Technik ist eine Rastersuche, bei der ein Raster systematisch im Raum der Hyperparameter gesiebt wird. Eine √úbersicht √ºber die Erfolge und Grenzen dieser Technik (sowie Empfehlungen zu einfach zu implementierenden Alternativen) finden Sie </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2012</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Viele ausgefeilte Techniken wurden vorgeschlagen. Ich werde nicht alle √ºberpr√ºfen, aber ich m√∂chte die vielversprechende Arbeit von 2012 zur </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bayes'schen Optimierung von</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hyperparametern </font><font style="vertical-align: inherit;">erw√§hnen </font><font style="vertical-align: inherit;">. Der Code von der Arbeit steht </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">allen offen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und mit einigem Erfolg wurde von anderen Forschern verwendet. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fassen Sie zusammen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit den von mir beschriebenen √úbungsregeln erzielen Sie mit Ihrem PS nicht die bestm√∂glichen Ergebnisse. Aber sie bieten Ihnen wahrscheinlich einen guten Ausgangspunkt und eine Grundlage f√ºr weitere Verbesserungen. Insbesondere habe ich Hyperparameter grunds√§tzlich unabh√§ngig beschrieben. In der Praxis besteht eine Verbindung zwischen ihnen. Sie k√∂nnen mit Œ∑ experimentieren, entscheiden, dass Sie den richtigen Wert gefunden haben, dann mit der Optimierung von Œª beginnen und feststellen, dass dies Ihre Œ∑-Optimierung verletzt. In der Praxis ist es n√ºtzlich, sich in verschiedene Richtungen zu bewegen und sich allm√§hlich guten Werten anzun√§hern. Denken Sie vor allem daran, dass die von mir beschriebenen heuristischen Ans√§tze einfache √úbungsregeln sind, aber keine in Stein gemei√üelten. Sie m√ºssen nach Anzeichen daf√ºr suchen, dass etwas nicht funktioniert, und Sie m√∂chten experimentieren. Insbesondere,√úberwachen Sie sorgf√§ltig das Verhalten Ihres neuronalen Netzwerks, insbesondere die Genauigkeit der Best√§tigung.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Komplexit√§t der Auswahl von Hyperparametern wird durch die Tatsache verst√§rkt, dass das praktische Wissen √ºber ihre Wahl √ºber viele Forschungsarbeiten und -programme verteilt ist und h√§ufig nur in den K√∂pfen einzelner Praktiker liegt. Es gibt eine Menge Arbeit mit Beschreibungen, was zu tun ist (oft im Widerspruch zueinander). Es gibt jedoch einige besonders n√ºtzliche Arbeiten, die einen gro√üen Teil dieses Wissens zusammenfassen und hervorheben. In </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dem</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Joshua Benji von 2012 gibt praktische Ratschl√§ge f√ºr die Verwendung von Backpropagation-Gradientenabfallsaktualisierung und Ausbildung f√ºr die Nationalversammlung, einschlie√ülich der Nationalversammlung und tief. Benjio beschreibt viele Details viel detaillierter. Als ich, einschlie√ülich einer systematischen Suche nach Hyperparametern. Ein weiterer guter Job ist die </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeit.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1998 Yanna Lekuna und andere. Beide Werke erscheinen in dem √§u√üerst n√ºtzlichen Buch von 2012, das viele Tricks enth√§lt, die in der Nationalversammlung h√§ufig verwendet werden: " </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neuronale Netze: Handwerkstricks</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ". Das Buch ist teuer, aber viele seiner Artikel wurden von ihren Autoren im Internet ver√∂ffentlicht und sind in Suchmaschinen zu finden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aus diesen Artikeln und insbesondere aus unseren eigenen Experimenten wird eines klar: Das Problem der Optimierung von Hyperparametern kann nicht als vollst√§ndig gel√∂st bezeichnet werden. Es gibt immer einen anderen Trick, mit dem Sie versuchen k√∂nnen, die Effizienz zu verbessern. Autoren haben das Sprichwort, dass ein Buch nicht fertiggestellt, sondern nur fallen gelassen werden kann. Gleiches gilt f√ºr die NS-Optimierung: Der Raum der Hyperparameter ist so gro√ü, dass die Optimierung nicht abgeschlossen, sondern nur gestoppt werden kann und die NS den Nachkommen √ºberlassen bleibt. Ihr Ziel ist es daher, einen Workflow zu entwickeln, mit dem Sie schnell eine gute Optimierung durchf√ºhren k√∂nnen und bei Bedarf detailliertere Optimierungsoptionen ausprobieren k√∂nnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schwierigkeiten bei der Auswahl von Hyperparametern lassen einige Leute dar√ºber klagen, dass NS im Vergleich zu anderen MO-Techniken zu viel Aufwand erfordern. Ich habe viele Varianten von Beschwerden geh√∂rt wie: ‚ÄûJa, ein gut abgestimmter NS kann bei der L√∂sung eines Problems die beste Effizienz erzielen. Auf der anderen Seite kann ich eine zuf√§llige Gesamtstruktur [oder SVM oder eine andere Ihrer Lieblingstechnologien] ausprobieren, und es funktioniert einfach. Ich habe keine Zeit herauszufinden, welche NA f√ºr mich richtig ist. " Aus praktischer Sicht ist es nat√ºrlich gut, einfach zu verwendende Techniken unter einem Freund zu haben. Dies ist besonders gut, wenn Sie gerade erst anfangen, mit einer Aufgabe zu arbeiten, und es ist immer noch unklar, ob das MO √ºberhaupt helfen kann, sie zu l√∂sen. Wenn es f√ºr Sie wichtig ist, optimale Ergebnisse zu erzielen, m√ºssen Sie m√∂glicherweise mehrere Ans√§tze ausprobieren, die spezialisierteres Wissen erfordern. Es w√§re toll,Wenn MO immer einfach w√§re, aber es gibt keine Gr√ºnde, warum es a priori trivial sein sollte.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Techniken </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jede der in diesem Kapitel entwickelten Techniken ist f√ºr sich wertvoll, aber dies ist nicht der einzige Grund, warum ich sie beschrieben habe. </font><font style="vertical-align: inherit;">Es ist wichtiger, sich mit einigen der Probleme vertraut zu machen, die im Bereich der NA auftreten k√∂nnen, und mit einem Analysestil, der helfen kann, diese zu √ºberwinden. </font><font style="vertical-align: inherit;">In gewisser Weise lernen wir, √ºber die NS nachzudenken. </font><font style="vertical-align: inherit;">Im Rest dieses Kapitels werde ich kurz eine Reihe anderer Techniken beschreiben. </font><font style="vertical-align: inherit;">Ihre Beschreibungen werden nicht so tief sein wie in den vorherigen, aber sie sollten einige Empfindungen hinsichtlich der Vielfalt der Techniken vermitteln, die auf dem Gebiet der NA anzutreffen sind.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Variationen des stochastischen Gradientenabfalls </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der stochastische Gradientenabstieg durch Backpropagation hat uns beim Angriff auf das Problem der Klassifizierung handgeschriebener Zahlen von MNIST gute Dienste geleistet. </font><font style="vertical-align: inherit;">Es gibt jedoch viele andere Ans√§tze zur Optimierung der Kostenfunktion, und manchmal zeigen sie eine Effizienz, die der des stochastischen Gradientenabfalls mit Minipaketen √ºberlegen ist. </font><font style="vertical-align: inherit;">In diesem Abschnitt beschreibe ich kurz zwei solche Ans√§tze, Hessisch und Momentum.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hessisch </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lassen Sie uns zun√§chst die Nationalversammlung beiseite legen. </font><font style="vertical-align: inherit;">Stattdessen betrachten wir einfach das abstrakte Problem der Minimierung der Kostenfunktion C vieler Variablen, w = w1, w2, ..., dh C = C (w). </font><font style="vertical-align: inherit;">Nach dem </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Taylorschen Theorem kann</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die Kostenfunktion am Punkt w angen√§hert werden:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-61"><span class="MJXp-mtable" id="MJXp-Span-62"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-63" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-64" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-66" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-67"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W </font></font></span><span class="MJXp-mo" id="MJXp-Span-68" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-69"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W </font></font></span><span class="MJXp-mo" id="MJXp-Span-71" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-72" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-73"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-74" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-76" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-77" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-munderover" id="MJXp-Span-78"><span class=""><span class="MJXp-mo" id="MJXp-Span-79" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ£</font></font></span></span></span><span class=" MJXp-script"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80" style="margin-left: 0px;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> j </font></font></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-81" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-82"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àÇ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-83"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-84"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àÇ </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-85"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-86" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-87" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-88"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Œî</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-89"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-90" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-91" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mspace" id="MJXp-Span-92" style="width: 0em; height: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-93" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mfrac" id="MJXp-Span-94" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-95"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-96"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span></span></span><span class="MJXp-munderover" id="MJXp-Span-97"><span class=""><span class="MJXp-mo" id="MJXp-Span-98" style="margin-left: 0.111em; margin-right: 0.167em;"><span class="MJXp-largeop"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àë</font></font></span></span></span><span class=" MJXp-script"><span class="MJXp-mrow" id="MJXp-Span-99" style="margin-left: 0px;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-100"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-101"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-102"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-103"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-105" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mfrac" id="MJXp-Span-106" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-107"><span class="MJXp-mi" id="MJXp-Span-108" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àÇ</font></font></span><span class="MJXp-mn MJXp-script" id="MJXp-Span-109" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-110"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C.</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mi" id="MJXp-Span-111"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àÇ </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-112"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-113" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-114" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></span></span><span class="MJXp-mi" id="MJXp-Span-115"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇ </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-116"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-118" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></span></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-119"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Œî</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-120"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-121" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-122" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-123" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></span><span class="MJXp-mo" id="MJXp-Span-124" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">...</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-4"> C(w+\Delta w) = C(w) + \sum_j \frac{\partial C}{\partial w_j} \Delta w_j \nonumber \\ + \frac{1}{2} \sum_{jk} \Delta w_j \frac{\partial^2 C}{\partial w_j \partial w_k} \Delta w_k + \ldots \tag{103} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir k√∂nnen es kompakter umschreiben als </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-125"><span class="MJXp-mtable" id="MJXp-Span-126"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-127" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-128" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-130" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-131"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-132" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-133"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-135" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-136" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-137"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-138" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-139"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-140" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-141" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-142"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àá </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-144" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ãÖ </font></font></span><span class="MJXp-mi" id="MJXp-Span-145"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-146"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-147" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mfrac" id="MJXp-Span-148" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-149"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-150"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-151"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Œî</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-152"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-154" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">H</font></font></span><span class="MJXp-mi" id="MJXp-Span-156"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî</font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-157"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mo" id="MJXp-Span-158" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+</font></font></span><span class="MJXp-mo" id="MJXp-Span-159" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">...</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> C(w+\Delta w) = C(w) + \nabla C \cdot \Delta w + \frac{1}{2} \Delta w^T H \Delta w + \ldots \tag{104} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wobei ‚àáC der gew√∂hnliche Gradientenvektor ist und H die als </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hessische Matrix</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> bekannte </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">Matrix</font></a><font style="vertical-align: inherit;"> anstelle von jk ist, in der ‚àÇ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C / ‚àÇw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àÇw </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k ist</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Angenommen, wir approximieren C, indem wir Terme h√∂herer Ordnung aufgeben, die sich in der Formel hinter den Auslassungspunkten verstecken:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-160"><span class="MJXp-mtable" id="MJXp-Span-161"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-162" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-163" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-165" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-166"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-167" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-168"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&amp; </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dgr </font></font></span><span class="MJXp-mo" id="MJXp-Span-170" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">; </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-170" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">) </font></span></font><span class="MJXp-mo" id="MJXp-Span-171" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚â§ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-172"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-173" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-174"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-175" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-176" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-177"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚â§ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C </font></font></span><span class="MJXp-mo" id="MJXp-Span-179" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚â§ </font></font></span><span class="MJXp-mi" id="MJXp-Span-180"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&amp; </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mo" id="MJXp-Span-179" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">Dgr; </font></span></font><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-182" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mfrac" id="MJXp-Span-183" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-184"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-185"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span></span></span><span class="MJXp-mi" id="MJXp-Span-186"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> &amp; Dgr;</font></font></span><span class="MJXp-msubsup" id="MJXp-Span-187"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-188" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-189" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">T</font></font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">H</font></font></span><span class="MJXp-mi" id="MJXp-Span-191"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&amp; Dgr;</font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-192"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-6"> C(w+\Delta w) \approx C(w) + \nabla C \cdot \Delta w + \frac{1}{2} \Delta w^T H \Delta w \tag{105} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mit Algebra kann gezeigt werden, dass der Ausdruck auf der rechten Seite minimiert werden kann, indem Folgendes ausgew√§hlt wird: </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-193"><span class="MJXp-mtable" id="MJXp-Span-194"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-195" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-196" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-197"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œî </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-198"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-199" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mo" id="MJXp-Span-200" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-201"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-202" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">H </font></font></span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-203" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-204"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mn" id="MJXp-Span-205"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></span></span></span><span class="MJXp-mi" id="MJXp-Span-206"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àá </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C.</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-7"> \Delta w = -H^{-1} \nabla C \tag{106} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Streng genommen m√ºssen wir davon ausgehen, dass die hessische Matrix eindeutig positiver ist, damit dies nur ein Minimum und nicht nur ein Extrem ist. </font><font style="vertical-align: inherit;">Intuitiv bedeutet dies, dass Funktion C wie ein Tal ist, nicht wie ein Berg oder ein Sattel. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn (105) eine gute Ann√§herung an die Kostenfunktion ist, sollten wir erwarten, dass der √úbergang vom Punkt w zum Punkt w + Œîw = w - H - 1‚àáC die Kostenfunktion signifikant reduzieren sollte. </font><font style="vertical-align: inherit;">Dies bietet einen m√∂glichen Algorithmus zur Kostenminimierung:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Startpunkt w ausw√§hlen. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aktualisiere w auf einen neuen Punkt, w ‚Ä≤ = w - H </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àí1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àáC, wobei das Hessische H und ‚àáC in w berechnet werden.</font></font></li><li>  w'   , w‚Ä≤‚Ä≤=w‚Ä≤‚àíH‚Ä≤ <sup>‚àí1</sup> ‚àá‚Ä≤C,   H  ‚àáC   w'. </li><li>  ... </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der Praxis ist (105) nur eine Ann√§herung, und es ist besser, kleinere Schritte zu unternehmen. Wir werden dies tun, indem wir w st√§ndig um Œîw = ‚àíŒ∑H - 1‚àáC aktualisieren, wobei Œ∑ die Lerngeschwindigkeit ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieser Ansatz zur Minimierung der Kostenfunktion wird als hessische Optimierung bezeichnet. Es gibt theoretische und empirische Ergebnisse, die zeigen, dass die hessischen Methoden in weniger Schritten auf ein Minimum konvergieren als ein Standardgradientenabstieg. Insbesondere durch die Einbeziehung von Informationen √ºber √Ñnderungen zweiter Ordnung in die Kostenfunktion ist es m√∂glich, viele Pathologien zu vermeiden, die beim Gradientenabstieg im hessischen Ansatz auftreten. Dar√ºber hinaus gibt es Versionen des Backpropagation-Algorithmus, mit denen der Hessische berechnet werden kann.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn die hessische Optimierung so cool ist, warum verwenden wir sie dann nicht in unserer NS? </font><font style="vertical-align: inherit;">Obwohl es viele w√ºnschenswerte Eigenschaften hat, ist leider eines sehr unerw√ºnscht: Es ist sehr schwierig, es in die Praxis umzusetzen. </font><font style="vertical-align: inherit;">Ein Teil des Problems ist die enorme Gr√∂√üe der hessischen Matrix. </font><font style="vertical-align: inherit;">Angenommen, wir haben eine NS mit 10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Gewichten und Offsets. </font><font style="vertical-align: inherit;">Dann gibt es in der entsprechenden hessischen Matrix 10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √ó 10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 10 </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">14</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Elemente. </font><font style="vertical-align: inherit;">Zu viel! </font><font style="vertical-align: inherit;">Infolgedessen </font><font style="vertical-align: inherit;">stellt sich heraus, dass es in der Praxis sehr schwierig ist, </font><font style="vertical-align: inherit;">H </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àí1</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚àáC zu </font><font style="vertical-align: inherit;">berechnen </font><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Das hei√üt aber nicht, dass es sinnlos ist, √ºber sie Bescheid zu wissen. </font><font style="vertical-align: inherit;">Viele Gradientenabstiegsoptionen sind von der hessischen Optimierung inspiriert, sie vermeiden einfach das Problem √ºberm√§√üig gro√üer Matrizen. </font><font style="vertical-align: inherit;">Schauen wir uns eine solche Technik an, den Impulsgradientenabstieg.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Impulsbasierter Gradientenabstieg </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intuitiv besteht der Vorteil der hessischen Optimierung darin, dass sie nicht nur Informationen √ºber den Gradienten enth√§lt, sondern auch Informationen √ºber seine √Ñnderung. Der impulsbasierte Gradientenabstieg basiert auf einer √§hnlichen Intuition, vermeidet jedoch gro√üe Matrizen aus zweiten Ableitungen. Um die Impulstechnik zu verstehen, erinnern wir uns an unser erstes Bild mit Gef√§lle, in dem wir einen Ball untersucht haben, der ein Tal hinunter rollt. Dann haben wir gesehen, dass der Gef√§lleabstieg entgegen seinem Namen nur geringf√ºgig einer auf den Boden fallenden Kugel √§hnelt. Die Impulstechnik √§ndert den Gradientenabstieg an zwei Stellen, wodurch er eher einem physischen Bild √§hnelt. Zun√§chst f√ºhrt sie das Konzept der ‚ÄûGeschwindigkeit‚Äú f√ºr die Parameter ein, die wir optimieren m√∂chten. Der Gradient versucht, die Geschwindigkeit zu √§ndern, nicht den "Ort" direkt, √§hnlich wie physikalische Kr√§fte die Geschwindigkeit √§ndern.und nur indirekt den Standort beeinflussen. Zweitens ist die Impulsmethode eine Art Reibungsterm, der die Geschwindigkeit allm√§hlich verringert.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Geben wir eine mathematisch genauere Definition. </font><font style="vertical-align: inherit;">Wir f√ºhren die Geschwindigkeitsvariablen v = v1, v2, ... ein, eine f√ºr jede entsprechende Variable w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (im neuronalen Netzwerk enthalten diese Variablen nat√ºrlich alle Gewichte und Verschiebungen). </font><font style="vertical-align: inherit;">Dann √§ndern wir die Gradientenabstiegsaktualisierungsregel w ‚Üí w ‚Ä≤ = w - Œ∑‚àáC auf</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-208"><span class="MJXp-mtable" id="MJXp-Span-209"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-210" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-211" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-212"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">v </font></font></span><span class="MJXp-mo" id="MJXp-Span-213" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font></span><span class="MJXp-msup" id="MJXp-Span-214"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">v </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-216" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-217" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-218"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œº </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-219"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">v </font></font></span><span class="MJXp-mo" id="MJXp-Span-220" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-221"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Œ∑ </font></font></span><span class="MJXp-mi" id="MJXp-Span-222"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚àá </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-223"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C.</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-8"> v \rightarrow v' = \mu v - \eta \nabla C \tag{107} </script></p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-224"><span class="MJXp-mtable" id="MJXp-Span-225"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-226" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-227" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-229" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font></span><span class="MJXp-msup" id="MJXp-Span-230"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-231" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-232" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-233" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-234"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-235" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-msup" id="MJXp-Span-236"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-237" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">v </font></font></span><span class="MJXp-mo MJXp-script" id="MJXp-Span-238" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">'</font></font></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> w \rightarrow w' = w+v' \tag{108} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Gleichungen ist Œº ein Hyperparameter, der das Ausma√ü der Bremsung oder Reibung des Systems steuert. Um die Bedeutung der Gleichungen zu verstehen, ist es zun√§chst n√ºtzlich, den Fall zu betrachten, in dem Œº = 1 ist, d. H. Wenn keine Reibung vorliegt. In diesem Fall zeigt die Untersuchung der Gleichungen, dass nun die ‚ÄûKraft‚Äú ‚àáC die Geschwindigkeit v √§ndert und die Geschwindigkeit die √Ñnderungsrate w steuert. Intuitiv kann Geschwindigkeit gewonnen werden, indem st√§ndig Gradientenelemente hinzugef√ºgt werden. Dies bedeutet, dass wir eine ausreichend hohe Bewegungsgeschwindigkeit in diese Richtung erreichen k√∂nnen, wenn sich der Gradient w√§hrend mehrerer Trainingsphasen in ungef√§hr eine Richtung bewegt. Stellen Sie sich zum Beispiel vor, was passiert, wenn Sie bergab fahren:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/37e/5f9/af1/37e5f9af1985610568cba31157e35763.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mit jedem Schritt den Hang hinunter steigt die Geschwindigkeit und wir bewegen uns immer schneller zum Talboden. Dies erm√∂glicht es der Geschwindigkeitstechnik, viel schneller als beim normalen Gradientenabstieg zu laufen. Das Problem ist nat√ºrlich, dass wir, nachdem wir den Talboden erreicht haben, durch das Tal schl√ºpfen werden. Wenn sich der Gradient zu schnell √§ndert, kann sich herausstellen, dass wir uns in die entgegengesetzte Richtung bewegen. Dies ist der Punkt, an dem der Hyperparameter Œº in (107) eingef√ºhrt wird. Ich habe vorhin gesagt, dass Œº das Ausma√ü der Reibung im System steuert. genauer gesagt muss das Ausma√ü der Reibung als 1 &amp; mgr; m vorgestellt werden. Wenn Œº = 1 ist, gibt es, wie wir gesehen haben, keine Reibung, und die Geschwindigkeit wird vollst√§ndig durch den Gradienten ‚àáC bestimmt. Und umgekehrt, wenn Œº = 0 ist, gibt es viel Reibung, es wird keine Geschwindigkeit gewonnen und die Gleichungen (107) und (108) werden auf die √ºblichen Gradientenabstiegsgleichungen w ‚Üí w '= w - Œ∑‚àáC reduziert. In der PraxisDie Verwendung des Wertes von Œº im Intervall zwischen 0 und 1 kann uns den Vorteil bieten, dass wir schneller werden k√∂nnen, ohne dass die Gefahr eines Minimums besteht. Wir k√∂nnen einen solchen Wert f√ºr Œº unter Verwendung der ausstehenden Best√§tigungsdaten auf die gleiche Weise w√§hlen, wie wir die Werte f√ºr Œ∑ und Œª gew√§hlt haben.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bisher habe ich es vermieden, den Hyperparameter Œº zu benennen. Tatsache ist, dass der Standardname f√ºr Œº schlecht gew√§hlt wurde: Er wird als Impulskoeffizient bezeichnet. Dies kann verwirrend sein, da Œº dem Konzept des Impulses aus der Physik √ºberhaupt nicht entspricht. Es ist viel st√§rker mit Reibung verbunden. Der Begriff ‚ÄûImpulskoeffizient‚Äú ist jedoch weit verbreitet, daher werden wir ihn auch weiterhin verwenden.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein sch√∂nes Merkmal der Impulstechnik ist, dass fast nichts getan werden muss, um die Implementierung des Gradientenabfalls zu √§ndern und diese Technik in sie aufzunehmen. </font><font style="vertical-align: inherit;">Wir k√∂nnen die R√ºckausbreitung nach wie vor verwenden, um Gradienten wie zuvor zu berechnen und Ideen wie das √úberpr√ºfen stochastisch ausgew√§hlter Minipacks zu verwenden. </font><font style="vertical-align: inherit;">In diesem Fall k√∂nnen wir einige der Vorteile der hessischen Optimierung anhand von Informationen √ºber Gradienten√§nderungen nutzen. </font><font style="vertical-align: inherit;">All dies geschieht jedoch ohne Fehler und mit nur geringf√ºgigen Code√§nderungen. </font><font style="vertical-align: inherit;">In der Praxis ist die Impulstechnik weit verbreitet und beschleunigt h√§ufig das Lernen.</font></font><br><br><h3>  √úbungen </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Was wird schief gehen, wenn wir in der Impulstechnik Œº&gt; 1 verwenden? </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Was wird schief gehen, wenn wir in der Impulstechnik Œº &lt;0 verwenden? </font></font></li></ul><br><br><h3>  Herausforderung </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> F√ºgen Sie network2.py einen impulsbasierten stochastischen Gradientenabstieg hinzu. </font></font></li></ul><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Ans√§tze zur Minimierung der Kostenfunktion </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es wurden viele andere Ans√§tze entwickelt, um die Kostenfunktion zu minimieren, und es wurde keine Einigung √ºber den besten Ansatz erzielt. </font><font style="vertical-align: inherit;">Wenn Sie sich eingehender mit neuronalen Netzen befassen, ist es hilfreich, sich mit anderen Technologien zu befassen, zu verstehen, wie sie funktionieren, welche St√§rken und Schw√§chen sie haben und wie sie in die Praxis umgesetzt werden k√∂nnen. </font><font style="vertical-align: inherit;">In der zuvor erw√§hnten </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeit</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> werden verschiedene solche Techniken vorgestellt und verglichen, einschlie√ülich des gepaarten Gradientenabfalls und der BFGS-Methode (und auch der eng verwandten BFGS-Methode mit Speicherbeschr√§nkung oder </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">L-BFGS</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">Eine weitere Technologie, die k√ºrzlich </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">vielversprechende Ergebnisse</font></a><font style="vertical-align: inherit;"> gezeigt hat </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist Nesterovs beschleunigter Gradient, der die Impulstechnik verbessert. </font><font style="vertical-align: inherit;">Ein einfacher Gradientenabstieg eignet sich jedoch gut f√ºr viele Aufgaben, insbesondere bei Verwendung des Impulses. Daher bleiben wir bis zum Ende des Buches beim stochastischen Gradientenabstieg.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Andere Modelle k√ºnstlicher Neuronen </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bisher haben wir unsere NS mit Sigmoid-Neuronen erstellt. </font><font style="vertical-align: inherit;">Im Prinzip kann NS, das auf Sigmoidneuronen aufgebaut ist, jede Funktion berechnen. </font><font style="vertical-align: inherit;">In der Praxis sind Netzwerke, die auf anderen Neuronenmodellen basieren, manchmal den Sigmoid-Modellen voraus. </font><font style="vertical-align: inherit;">Je nach Anwendung k√∂nnen Netzwerke, die auf solchen alternativen Modellen basieren, schneller lernen, besser auf Verifizierungsdaten verallgemeinern oder beides tun. </font><font style="vertical-align: inherit;">Lassen Sie mich einige alternative Modelle von Neuronen erw√§hnen, um Ihnen eine Vorstellung von einigen h√§ufig verwendeten Optionen zu geben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die vielleicht einfachste Variante w√§re ein Tang-Neuron, das eine Sigmoidfunktion durch eine hyperbolische Tangente ersetzt. </font><font style="vertical-align: inherit;">Die Ausgabe eines Tang-Neurons mit der Eingabe x, einem Vektor der Gewichte w und einem Versatz b wird als angegeben</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-239"><span class="MJXp-mtable" id="MJXp-Span-240"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-241" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-242" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-243"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tanh </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mo" id="MJXp-Span-245" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-246"><font style="vertical-align: inherit;">w </font></span><span class="MJXp-mo" id="MJXp-Span-247" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">‚ãÖ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-249" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;">+ </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250"><font style="vertical-align: inherit;">b </font></span><span class="MJXp-mo" id="MJXp-Span-251" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">)</font></span></font><span class="MJXp-mo" id="MJXp-Span-244" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-245" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-246"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-247" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-249" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-251" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-10"> \tanh(w \cdot x+b) \tag{109} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wo Tanh nat√ºrlich </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hyperbolische Tangente ist</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Es stellt sich heraus, dass er sehr eng mit dem Sigmoid-Neuron verbunden ist. </font><font style="vertical-align: inherit;">Denken Sie daran, dass tanh definiert ist als</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-252"><span class="MJXp-mtable" id="MJXp-Span-253"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-254" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-255" style="text-align: center;"><span class="MJXp-mi" id="MJXp-Span-256"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tanh </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mo" id="MJXp-Span-258" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259"><font style="vertical-align: inherit;">z </font></span><span class="MJXp-mo" id="MJXp-Span-260" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">) </font></span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">‚â° </font></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-263"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">e </font></span></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-263"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-265" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;">z</font></span></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mo" id="MJXp-Span-266" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"> - </font></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-267"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-268" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">e </font></span></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-267"><span class="MJXp-mrow MJXp-script" id="MJXp-Span-269" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-270"><font style="vertical-align: inherit;">- </font></span></span></span></span></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-267"><span class="MJXp-mrow MJXp-script" id="MJXp-Span-269" style="vertical-align: 0.5em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271"><font style="vertical-align: inherit;">z</font></span></span></span></span></span></font><span class="MJXp-mo" id="MJXp-Span-257" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-258" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-259"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-260" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-261" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mfrac" id="MJXp-Span-262" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-263"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-265" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mo" id="MJXp-Span-266" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-267"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-268" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-269" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-270"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271"><font style="vertical-align: inherit;"></font></span></span></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-msubsup" id="MJXp-Span-272"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-274" style="vertical-align: 0.5em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">z</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-275" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-276"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-277" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">e </font></font></span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-278" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-279"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">z</font></font></span></span></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-11"> \tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}} \tag{110} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Mit ein wenig Algebra ist das leicht zu erkennen </font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-281"><span class="MJXp-mtable" id="MJXp-Span-282"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-283" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-284" style="text-align: center;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-285"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">œÉ </font></font></span><span class="MJXp-mo" id="MJXp-Span-286" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">z </font></font></span><span class="MJXp-mo" id="MJXp-Span-288" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) </font></font></span><span class="MJXp-mo" id="MJXp-Span-289" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= </font></font></span><span class="MJXp-mfrac" id="MJXp-Span-290" style="vertical-align: 0.25em;"><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-291"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1 </font></font></span><span class="MJXp-mo" id="MJXp-Span-292" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi" id="MJXp-Span-293"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">tanh </font></font></span><font style="vertical-align: inherit;"><span class="MJXp-mo" id="MJXp-Span-295" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296"><font style="vertical-align: inherit;">z </font></span><span class="MJXp-mrow" id="MJXp-Span-297"><span class="MJXp-mo" id="MJXp-Span-298" style="margin-left: 0.111em; margin-right: 0.111em;"><font style="vertical-align: inherit;">/</font></span></span><span class="MJXp-mn" id="MJXp-Span-299"><font style="vertical-align: inherit;"> 2 </font></span><span class="MJXp-mo" id="MJXp-Span-300" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">)</font></span></font><span class="MJXp-mo" id="MJXp-Span-294" style="margin-left: 0em; margin-right: 0em;"></span><span class="MJXp-mo" id="MJXp-Span-295" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mrow" id="MJXp-Span-297"><span class="MJXp-mo" id="MJXp-Span-298" style="margin-left: 0.111em; margin-right: 0.111em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mn" id="MJXp-Span-299"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-300" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-box" style="margin-top: -0.9em;"><span class="MJXp-denom"><span><span class="MJXp-rule" style="height: 1em; border-top: none; border-bottom: 1px solid; margin: 0.1em 0px;"></span></span><span><span class="MJXp-box"><span class="MJXp-mn" id="MJXp-Span-301"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></span></span></span></span></span></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-12"> \sigma(z) = \frac{1+\tanh(z/2)}{2} \tag{111} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das hei√üt, Tanh skaliert nur das Sigmoid. </font><font style="vertical-align: inherit;">Grafisch k√∂nnen Sie auch sehen, dass die Tanh-Funktion dieselbe Form wie das Sigmoid hat: </font></font><br><br><img src="https://habrastorage.org/webt/2d/al/jw/2daljwgmoaw7v8g4bz7jkiy8fwo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein Unterschied zwischen Tang-Neuronen und Sigmoid-Neuronen besteht darin, dass sich die Ausgabe der ersten von -1 bis 1 und nicht von 0 bis 1 erstreckt. Dies bedeutet Wenn Sie ein Netzwerk erstellen, das auf Tang-Neuronen basiert, m√ºssen Sie m√∂glicherweise Ihre Ausgaben (und, abh√§ngig von den Details der Anwendung, m√∂glicherweise die Eingaben) etwas anders normalisieren als in Sigmoid-Netzwerken. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie Sigmoid-Neuronen k√∂nnen Tang-Neuronen im Prinzip jede Funktion berechnen (obwohl es einige Tricks gibt) und Eingaben von -1 bis 1 markieren. Dar√ºber hinaus lassen sich die Ideen der R√ºckausbreitung und des stochastischen Gradientenabfalls genauso einfach auf Tang anwenden -neurons sowie zu Sigmoid.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> √úbung </font></font></h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Man beweise Gleichung (111). </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Welche Art von Neuron sollte in Netzwerken, Tang oder Sigmoid verwendet werden? Die Antwort ist, gelinde gesagt, nicht offensichtlich! Es </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">gibt jedoch </font></font></a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">theoretische Argumente</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und einige empirische Beweise daf√ºr, dass Tang-Neuronen manchmal besser funktionieren. Lassen Sie uns kurz eines der theoretischen Argumente f√ºr Tang-Neuronen durchgehen. Angenommen, wir verwenden Sigmoid-Neuronen und alle Aktivierungen im Netzwerk sind positiv. Betrachten Sie die Gewichte w </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">jk,</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die f√ºr das Neuron Nr. J in der Schicht Nr. 1 + 1 enthalten sind. Backpropagation-Regeln (BP4) sagen uns, dass der damit verbundene Gradient gleich a </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Œ¥ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j ist</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Da die Aktivierungen positiv sind, ist das Vorzeichen dieses Gradienten dasselbe wie das von Œ¥ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Dies bedeutet, dass, wenn Œ¥ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> positiv ist, alle Gewichte w </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">jk</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> w√§hrend des Gradientenabfalls abnehmen, und wenn Œ¥ </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">j</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> negativ ist, dann alle Gewichte w </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l + 1 </font></font></sup> <sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">jk</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">erh√∂ht sich w√§hrend des Gef√§lles. Mit anderen Worten, alle mit demselben Neuron verbundenen Gewichte nehmen zusammen zu oder ab. Dies ist ein Problem, da Sie m√∂glicherweise einige Gewichte erh√∂hen und andere reduzieren m√ºssen. Dies kann jedoch nur passieren, wenn einige Eingabeaktivierungen unterschiedliche Vorzeichen haben. Dies legt die Notwendigkeit nahe, das Sigmoid durch eine andere Aktivierungsfunktion zu ersetzen, beispielsweise eine hyperbolische Tangente, die es erm√∂glicht, dass Aktivierungen sowohl positiv als auch negativ sind. Da tanh in Bezug auf Null symmetrisch ist, tanh (‚àíz) = ‚àítanh (z), kann man erwarten, dass Aktivierungen in verborgenen Schichten grob gesagt gleichm√§√üig zwischen positiv und negativ verteilt sind. Dies wird dazu beitragen, sicherzustellen, dass die Aktualisierungen der Skalen in die eine oder andere Richtung nicht systematisch verzerrt sind.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie ernst sollte dieses Argument genommen werden? Immerhin ist es heuristisch, liefert keinen strengen Beweis daf√ºr, dass Tang-Neuronen Sigmoid-Neuronen √ºberlegen sind. Vielleicht haben Sigmoidneuronen einige Eigenschaften, die dieses Problem kompensieren? In der Tat zeigte die Tanh-Funktion in vielen F√§llen minimale bis keine Vorteile gegen√ºber dem Sigmoid. Leider haben wir keine einfachen und schnell implementierten Methoden, um zu √ºberpr√ºfen, welcher Neuronentyp schneller lernt oder sich bei der Verallgemeinerung f√ºr einen bestimmten Fall als effektiver erweist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eine andere Variante eines Sigmoidneurons ist ein gleichgerichtetes lineares Neuron oder eine gleichgerichtete lineare Einheit, ReLU. Die Ausgabe ReLU mit Eingabe x, dem Vektor der Gewichte w und Offset b wird wie folgt angegeben:</font></font><br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-307"><span class="MJXp-mtable" id="MJXp-Span-308"><span><span class="MJXp-mlabeledtr" id="MJXp-Span-309" style="vertical-align: baseline;"><span class="MJXp-mtd" id="MJXp-Span-310" style="text-align: center;"><span class="MJXp-mo" id="MJXp-Span-311" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">max </font></font></span><span class="MJXp-mo" id="MJXp-Span-312" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">( </font></font></span><span class="MJXp-mn" id="MJXp-Span-313"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">0 </font></font></span><span class="MJXp-mo" id="MJXp-Span-314" style="margin-left: 0em; margin-right: 0.222em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-315"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">w </font></font></span><span class="MJXp-mo" id="MJXp-Span-316" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ãÖ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mo" id="MJXp-Span-318" style="margin-left: 0.267em; margin-right: 0.267em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b </font></font></span><span class="MJXp-mo" id="MJXp-Span-320" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">)</font></font></span></span></span></span></span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> \max(0, w \cdot x+b) \tag{112} </script></p><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die grafische Richtfunktion max (0, z) sieht folgenderma√üen aus: </font></font><br><br><img src="https://habrastorage.org/webt/tu/1x/uv/tu1xuvrfyle3eismzxadvwjzjqq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Solche Neuronen unterscheiden sich offensichtlich stark von Sigmoid- und Tang-Neuronen. </font><font style="vertical-align: inherit;">Sie sind jedoch insofern √§hnlich, als sie auch zur Berechnung einer beliebigen Funktion verwendet werden k√∂nnen, und sie k√∂nnen unter Verwendung von R√ºckausbreitung und stochastischem Gradientenabstieg trainiert werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wann sollte ich ReLU anstelle von Sigmoid- oder Tang-Neuronen verwenden? </font><font style="vertical-align: inherit;">In neueren Arbeiten zur Bilderkennung ( </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) In fast dem gesamten Netzwerk wurden schwerwiegende Vorteile der Verwendung von ReLU festgestellt. Wie bei Tang-Neuronen haben wir jedoch noch kein wirklich tiefes Verst√§ndnis daf√ºr, wann genau welche ReLUs vorzuziehen sind und warum. Um sich ein Bild von einigen Problemen zu machen, denken Sie daran, dass Sigmoidneuronen nicht mehr lernen, wenn sie ges√§ttigt sind, dh wenn die Ausgabe nahe 0 oder 1 liegt. Wie wir in diesem Kapitel oft gesehen haben, besteht das Problem darin, dass die œÉ'-Mitglieder den Gradienten verringern das verlangsamt das Lernen. Tang-Neuronen leiden unter √§hnlichen Schwierigkeiten bei der S√§ttigung. Gleichzeitig wird eine Erh√∂hung der gewichteten Eingabe in ReLU niemals zu einer S√§ttigung f√ºhren, sodass keine entsprechende Verlangsamung des Trainings auftritt. Wenn andererseits die gewichtete Eingabe auf der ReLU negativ ist, verschwindet der Gradient und das Neuron h√∂rt √ºberhaupt auf zu lernen.Dies ist nur ein paar der vielen Probleme, die es nicht trivial machen zu verstehen, wann und wie sich ReLUs besser verhalten als Sigmoid- oder Tang-Neuronen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich habe ein Bild der Unsicherheit gemalt und betont, dass wir noch keine solide Theorie √ºber die Wahl der Aktivierungsfunktionen haben. In der Tat ist dieses Problem noch komplizierter als ich beschrieben habe, da es unendlich viele m√∂gliche Aktivierungsfunktionen gibt. Welches gibt uns das am schnellsten lernende Netzwerk? Welches ergibt die gr√∂√üte Genauigkeit bei den Tests? Ich bin √ºberrascht, wie wenige wirklich gr√ºndliche und systematische Studien zu diesen Themen durchgef√ºhrt wurden. Idealerweise sollten wir eine Theorie haben, die uns detailliert erkl√§rt, wie wir unsere Aktivierungsfunktionen ausw√§hlen (und m√∂glicherweise im laufenden Betrieb √§ndern). Andererseits sollten wir nicht durch das Fehlen einer vollst√§ndigen Theorie aufgehalten werden! Wir haben bereits leistungsstarke Tools und k√∂nnen mit ihrer Hilfe erhebliche Fortschritte erzielen. Bis zum Ende des Buches werde ich Sigmoidneuronen als Hauptneuronen verwenden.da sie gut funktionieren und konkrete Illustrationen der wichtigsten Ideen im Zusammenhang mit der Nationalversammlung geben. Beachten Sie jedoch, dass dieselben Ideen auch auf andere Neuronen angewendet werden k√∂nnen und diese Optionen ihre Vorteile haben.</font></font><br><br><h3>     </h3><br><blockquote> :          ,      ,   ?         ? <br><br> :   ,      .          ,     .    .      :         ,     ,     ? <br><br> ‚Äî         </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einmal auf einer Konferenz √ºber die Grundlagen der Quantenmechanik bemerkte ich, was wie eine lustige Sprachgewohnheit schien: Am Ende des Berichts begannen die Fragen des Publikums oft mit dem Satz: "Ich mag Ihren Standpunkt wirklich, aber ..." Quantengrundlagen sind nicht ganz mein √ºbliches Fachgebiet, und ich habe auf diese Art des Fragens aufmerksam gemacht, weil ich mich auf anderen wissenschaftlichen Konferenzen praktisch nicht getroffen habe, damit der Fragesteller Sympathie f√ºr den Standpunkt des Sprechers zeigt. Zu dieser Zeit entschied ich, dass die Verbreitung solcher Fragen darauf hindeutete, dass Fortschritte bei den Quantengrundlagen erzielt wurden und dass die Menschen gerade erst anfingen, an Dynamik zu gewinnen. Sp√§ter stellte ich fest, dass diese Einsch√§tzung zu hart war. Die Redner hatten mit einigen der schwierigsten Probleme zu k√§mpfen, auf die der menschliche Geist jemals gesto√üen ist. Der Fortschritt war nat√ºrlich langsam!Es war jedoch immer noch wertvoll, Nachrichten √ºber das Denken der Menschen √ºber diesen Bereich zu h√∂ren, auch wenn sie wenig bis gar nichts hatten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Buch haben Sie m√∂glicherweise eine ‚Äûnerv√∂se Zecke‚Äú bemerkt, die dem Satz ‚ÄûIch bin sehr beeindruckt‚Äú √§hnelt. Um zu erkl√§ren, was wir haben, habe ich oft auf W√∂rter wie ‚Äûheuristisch‚Äú oder ‚Äûgrob gesprochen‚Äú zur√ºckgegriffen, gefolgt von einer Erkl√§rung eines bestimmten Ph√§nomens. Diese Geschichten sind glaubw√ºrdig, aber empirische Beweise waren oft recht oberfl√§chlich. Wenn Sie die Forschungsliteratur studieren, werden Sie feststellen, dass Geschichten dieser Art in vielen Forschungsarbeiten √ºber neuronale Netze erscheinen, oft in Begleitung einer kleinen Menge von Beweisen, die sie unterst√ºtzen. Wie verhalten wir uns zu solchen Geschichten?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In vielen Bereichen der Wissenschaft - insbesondere wenn einfache Ph√§nomene betrachtet werden - kann man sehr strenge und verl√§ssliche Beweise f√ºr sehr allgemeine Hypothesen finden. In der Nationalversammlung gibt es jedoch eine Vielzahl von Parametern und Hyperparametern, und es bestehen √§u√üerst komplexe Beziehungen zwischen ihnen. In solch unglaublich komplexen Systemen ist es unglaublich schwierig, verl√§ssliche allgemeine Aussagen zu treffen. Das Verst√§ndnis des NS in seiner ganzen F√ºlle, wie Quantengrundlagen, testet die Grenzen des menschlichen Geistes. Oft m√ºssen wir auf Beweise f√ºr oder gegen mehrere bestimmte Einzelf√§lle einer allgemeinen Aussage verzichten. Infolgedessen m√ºssen diese Aussagen manchmal ge√§ndert oder aufgegeben werden, wenn neue Beweise auftauchen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Einer der Ans√§tze f√ºr diese Situation besteht darin, zu ber√ºcksichtigen, dass jede heuristische Geschichte √ºber die NS eine gewisse Herausforderung beinhaltet. Betrachten Sie zum Beispiel die Erkl√§rung, die ich zitiert habe, warum eine Ausnahme (Abbruch) von der </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Arbeit im Jahr 2012 funktioniert.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">: ‚ÄûDiese Technik reduziert die komplexe Gelenkanpassung von Neuronen, da sich ein Neuron nicht auf die Anwesenheit bestimmter Nachbarn verlassen kann. Am Ende muss er zuverl√§ssigere Eigenschaften lernen, die bei der Zusammenarbeit mit vielen verschiedenen zuf√§lligen Untergruppen von Neuronen n√ºtzlich sein k√∂nnen. ‚Äú Eine reichhaltige und provokative Aussage, auf deren Grundlage Sie ein ganzes Forschungsprogramm erstellen k√∂nnen, in dem Sie herausfinden m√ºssen, was wahr ist, wo es falsch ist und was gekl√§rt und ge√§ndert werden muss. Und jetzt gibt es wirklich eine ganze Branche von Forschern, die die Ausnahme (und ihre vielen Variationen) untersuchen und versuchen zu verstehen, wie sie funktioniert und welche Einschr√§nkungen sie hat. So haben wir mit vielen anderen heuristischen Ans√§tzen diskutiert. Jeder von ihnen ist nicht nur eine m√∂gliche Erkl√§rung,aber auch eine Herausforderung f√ºr die Forschung und ein detaillierteres Verst√§ndnis.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich wird nicht eine Person genug Zeit haben, um all diese heuristischen Erkl√§rungen gr√ºndlich genug zu untersuchen. Die gesamte Gemeinschaft von NS-Forschern wird Jahrzehnte brauchen, um eine wirklich leistungsf√§hige Theorie des NS-Trainings zu entwickeln, die auf Beweisen basiert. Bedeutet dies, dass es sich lohnt, heuristische Erkl√§rungen als lasch und ohne Beweise abzulehnen?</font></font> Nein!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir brauchen eine Heuristik, die unser Denken anregt. Dies √§hnelt der √Ñra gro√üer geografischer Entdeckungen: Fr√ºhe Gelehrte handelten (und machten Entdeckungen) oft auf der Grundlage von √úberzeugungen, die ernsthaft falsch waren. Sp√§ter haben wir diese Fehler korrigiert und unser geografisches Wissen wieder aufgef√ºllt. Wenn Sie etwas schlecht verstehen - wie die Forscher die Geographie verstanden haben und wie wir die NS heute verstehen -, ist es wichtiger, das Unbekannte k√ºhn zu studieren, als bei jedem Schritt Ihrer Argumentation gewissenhaft richtig zu sein. Daher sollten Sie diese Geschichten als n√ºtzliche Anweisungen betrachten, wie Sie √ºber NS nachdenken, ein gesundes Bewusstsein f√ºr ihre Grenzen bewahren und die Zuverl√§ssigkeit der Beweise in jedem Fall sorgf√§ltig √ºberwachen k√∂nnen. Mit anderen Worten, wir brauchen gute Geschichten f√ºr Motivation und Inspiration und sorgf√§ltige gr√ºndliche Untersuchungen - umechte Fakten entdecken.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460711/">https://habr.com/ru/post/de460711/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460699/index.html">Habr Weekly # 10 / Super Services und E-Pass, Smartphones und Russen, "Spionageger√§te", Leben ohne Satelliten</a></li>
<li><a href="../de460701/index.html">Kurs "Start in Data Science": der erste Schritt in der Arbeit mit Daten</a></li>
<li><a href="../de460703/index.html">Blauer Ozean der M√∂glichkeiten: Von null bis 400 tausend Videointerviews</a></li>
<li><a href="../de460707/index.html">Ist es Zeit f√ºr Spieleentwickler, ihren Fans nicht mehr zuzuh√∂ren?</a></li>
<li><a href="../de460709/index.html">√úberlegungen zu Agile</a></li>
<li><a href="../de460713/index.html">Anwendungsentwicklung auf SwiftUI. Teil 1: Datenfluss und Redux</a></li>
<li><a href="../de460717/index.html">W√∂chentliche Nachrichten: OneWeb-Satellitennetzwerktests, neuronale Schnittstellen der Ilona-Maske und elektronische Ger√§te ohne Spionage</a></li>
<li><a href="../de460719/index.html">Industry Foundation Klassen. Kurze Einf√ºhrung</a></li>
<li><a href="../de460723/index.html">NVIDIA Jetson Nano: Tests und erste Eindr√ºcke</a></li>
<li><a href="../de460725/index.html">Selbstdokumentierender Code ist (normalerweise) Unsinn</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>