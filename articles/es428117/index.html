<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåæ üöâ üöê Procesadores de tensor gratuitos de Google en la nube colaborativa ü§µüèΩ üèùÔ∏è üí´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Google recientemente proporcion√≥ acceso gratuito a su unidad de procesamiento de tensor (TPU) en la plataforma de aprendizaje autom√°tico basada en la ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Procesadores de tensor gratuitos de Google en la nube colaborativa</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428117/">  Google recientemente proporcion√≥ acceso gratuito a su unidad de procesamiento de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tensor</a> (TPU) en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">plataforma de</a> aprendizaje autom√°tico basada en la nube de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colaboratory</a> .  El procesador tensorial es un circuito integrado especializado (ASIC) desarrollado por Google para tareas de aprendizaje autom√°tico utilizando la biblioteca TensorFlow.  Decid√≠ intentar aprender una red convolucional de TPU en Keras que reconoce objetos en im√°genes CIFAR-10.  El c√≥digo completo de la soluci√≥n se puede ver y ejecutar en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">computadora port√°til</a> . <br><br><img src="https://habrastorage.org/webt/sl/ut/ho/slutho5dsyeduk2biql9rcsblnu.jpeg"><br>  <i>Foto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cloud.google.com</a></i> <br><a name="habracut"></a><br><h2>  Procesadores tensoriales </h2><br>  En Habr√© ya escribi√≥ c√≥mo se organizan los TPU ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> ), y tambi√©n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">por qu√© los TPU son adecuados para entrenar redes neuronales</a> .  Por lo tanto, no profundizar√© en los detalles de la arquitectura de TPU, sino que solo considerar√© las caracter√≠sticas que deben tenerse en cuenta al entrenar redes neuronales. <br><br>  Ahora hay tres generaciones de procesadores tensoriales, el rendimiento de TPU de √∫ltima y tercera generaci√≥n es de 420 TFlops (billones de operaciones de punto flotante por segundo), contiene 128 GB de memoria de alto ancho de banda.  Sin embargo, solo los TPU de segunda generaci√≥n est√°n disponibles en el Colaboratorio, que tienen 180 TFlops de rendimiento y 64 GB de memoria.  En el futuro, considerar√© estos TPU. <br><br>  El procesador tensor consiste en cuatro chips, cada uno de los cuales contiene dos n√∫cleos, un total de ocho n√∫cleos en TPU.  El entrenamiento de TPU se lleva a cabo en paralelo en todos los n√∫cleos utilizando la replicaci√≥n: cada n√∫cleo ejecuta una copia del gr√°fico TensorFlow con un octavo del volumen de datos. <br><br>  La base del procesador tensor es una unidad de matriz (MXU).  Utiliza la astuta estructura de datos de una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">matriz sist√≥lica de</a> 128x128 para la implementaci√≥n eficiente de operaciones matriciales.  Por lo tanto, para maximizar el uso de los recursos del equipo de TPU, la dimensi√≥n de la minimuestra o las caracter√≠sticas debe ser un m√∫ltiplo de 128 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">fuente</a> ).  Adem√°s, debido a la naturaleza del sistema de memoria TPU, es deseable que la dimensi√≥n de la mini-muestra y las caracter√≠sticas sean m√∫ltiplos de 8. <br><br><h2>  Plataforma colaborativa </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Colaboratory</a> es la plataforma en la nube de Google para avanzar en la tecnolog√≠a de aprendizaje autom√°tico.  Puede obtener una m√°quina virtual con las bibliotecas populares instaladas TensorFlow, Keras, sklearn, pandas, etc. de forma gratuita.  Lo m√°s conveniente es que puede ejecutar computadoras port√°tiles similares a Jupyter en el Colaboratorio.  Las computadoras port√°tiles se almacenan en Google Drive, puede distribuirlas e incluso organizar la colaboraci√≥n.  As√≠ es como se ve la computadora port√°til en el Colaboratorio ( <i>se puede hacer clic en</i> la <i>imagen</i> ): <br><br> <a href=""><img src="https://habrastorage.org/webt/4b/gp/sn/4bgpsnbpkhwyqrid6fixnaurcbw.png"></a> <br><br>  Escribe el c√≥digo en un navegador en una computadora port√°til, se ejecuta en una m√°quina virtual en Google Cloud.  El auto se le entrega por 12 horas, luego de lo cual se detiene.  Sin embargo, nada le impide iniciar otra m√°quina virtual y trabajar otras 12 horas.  Solo tenga en cuenta que despu√©s de que la m√°quina virtual se detiene, se eliminan todos los datos que contiene.  Por lo tanto, no olvide guardar los datos necesarios en su computadora o Google Drive, y despu√©s de reiniciar la m√°quina virtual, vuelva a descargarlos. <br><br>  Las instrucciones detalladas para trabajar en la plataforma de colaboraci√≥n est√°n <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br><h2>  Conecte el procesador tensor al Colaboratorio </h2><br>  De forma predeterminada, Colaboratory no utiliza aceleradores de c√°lculo de GPU o TPU.  Puede conectarlos en el men√∫ Tiempo de ejecuci√≥n -&gt; Cambiar tipo de tiempo de ejecuci√≥n -&gt; Acelerador de hardware.  En la lista que aparece, seleccione "TPU": <br><img src="https://habrastorage.org/webt/1f/7r/vt/1f7rvtfjvdowdjwrz0ctgyyly7s.png" alt="imagen"><br><br>  Despu√©s de elegir el tipo de acelerador, la m√°quina virtual a la que est√° conectada la computadora port√°til Colaboratory se reiniciar√° y TPU estar√° disponible. <br><br>  Si descarg√≥ alg√∫n dato a la m√°quina virtual, durante el proceso de reinicio se eliminar√°.  Tienes que descargar los datos nuevamente. <br><br><h2>  Red neuronal Keras para el reconocimiento CIFAR-10 </h2><br>  Como ejemplo, intentemos entrenar una red neuronal Keras en TPU que reconozca im√°genes del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">conjunto de datos CIFAR-10</a> .  Este es un conjunto de datos popular que contiene peque√±as im√°genes de objetos de 10 clases: avi√≥n, autom√≥vil, p√°jaro, gato, venado, perro, rana, caballo, barco y cami√≥n.  Las clases no se cruzan, cada objeto en la imagen pertenece a una sola clase. <br><br>  Descargue el conjunto de datos CIFAR-10 usando Keras: <br><br><pre><code class="python hljs">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</code> </pre> <br>  Para crear una red neuronal, obtuve una funci√≥n separada.  Crearemos el mismo modelo dos veces: la primera versi√≥n del modelo para TPU, en la que entrenaremos, y la segunda para la CPU, donde reconoceremos objetos. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> input_layer = Input(shape=(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), dtype=tf.float32, name=<span class="hljs-string"><span class="hljs-string">'Input'</span></span>) x = BatchNormalization()(input_layer) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = BatchNormalization()(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = Flatten()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) output_layer = Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[input_layer], outputs=[output_layer]) model.compile( optimizer=tf.train.AdamOptimizer(<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[<span class="hljs-string"><span class="hljs-string">'sparse_categorical_accuracy'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br>  Hasta ahora, los optimizadores Keras no se pueden usar en TPU, por lo tanto, al compilar un modelo, se especifica el optimizador de TensorFlow. <br><br>  Creamos un modelo Keras para la CPU, que en el siguiente paso convertiremos a un modelo para TPU: <br><br><pre> <code class="python hljs">cpu_model = create_model()</code> </pre> <br><h2>  Convierta la red neuronal Keras al modelo TPU </h2><br>  Los modelos en Keras y TensorFlow se pueden entrenar en la GPU sin ning√∫n cambio.  Hasta ahora no puede hacer esto en TPU, por lo que debe convertir el modelo que creamos en un modelo para TPU. <br><br>  Primero debe averiguar d√≥nde se encuentra el TPU disponible para nosotros.  En la plataforma de colaboraci√≥n, esto se puede hacer con el siguiente comando: <br><br><pre> <code class="python hljs">TPU_WORKER = <span class="hljs-string"><span class="hljs-string">'grpc://'</span></span> + os.environ[<span class="hljs-string"><span class="hljs-string">'COLAB_TPU_ADDR'</span></span>]</code> </pre> <br>  En mi caso, la direcci√≥n de TPU result√≥ ser as√≠: <code>grpc://10.102.233.146:8470</code> .  Las direcciones fueron diferentes para diferentes lanzamientos. <br><br>  Ahora puede obtener el modelo para TPU utilizando la funci√≥n <code>keras_to_tpu_model</code> : <br><br><pre> <code class="python hljs">tf.logging.set_verbosity(tf.logging.INFO) tpu_model = tf.contrib.tpu.keras_to_tpu_model( cpu_model, strategy=tf.contrib.tpu.TPUDistributionStrategy( tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))</code> </pre> <br>  La primera l√≠nea incluye el registro en el nivel de informaci√≥n.  Aqu√≠ est√° el registro de conversi√≥n del modelo: <br><br> <code>INFO:tensorflow:Querying Tensorflow master (b'grpc://10.102.233.146:8470') for TPU system metadata. <br> INFO:tensorflow:Found TPU system: <br> INFO:tensorflow:*** Num TPU Cores: 8 <br> INFO:tensorflow:*** Num TPU Workers: 1 <br> INFO:tensorflow:*** Num TPU Cores Per Worker: 8 <br> ... <br> WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.</code> <br> <br>  Puede ver que se encontr√≥ TPU en la direcci√≥n que especificamos anteriormente, tiene 8 n√∫cleos.  Tambi√©n vemos una advertencia de que <code>tpu_model</code> es experimental y se puede cambiar o eliminar en cualquier momento.  Espero que con el tiempo sea posible entrenar modelos Keras directamente en TPU sin ninguna conversi√≥n. <br><br><h2>  Entrenamos modelo en TPU </h2><br>  El modelo para TPU se puede entrenar de la manera habitual para Keras llamando al m√©todo de <code>fit</code> : <br><br><pre> <code class="python hljs">history = tpu_model.fit(x_train, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>*<span class="hljs-number"><span class="hljs-number">8</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">50</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  ¬øCu√°les son las caracter√≠sticas aqu√≠?  Recordamos que para usar TPU de manera efectiva, el tama√±o de la mini-muestra debe ser un m√∫ltiplo de 128. Adem√°s, se realiza capacitaci√≥n en cada n√∫cleo de TPU utilizando un octavo de todos los datos de la mini-muestra.  Por lo tanto, establecemos el tama√±o de la mini muestra durante el entrenamiento en 128 * 8, obtenemos 128 im√°genes para cada n√∫cleo de TPU.  Puede usar un tama√±o m√°s grande, por ejemplo, 256 o 512, luego el rendimiento ser√° mayor. <br><br>  En mi caso, el entrenamiento de una era requiere un promedio de 6 s. <br><br>  La calidad de la educaci√≥n en la era 50: <br> <code>Epoch 50/50 <br> - 6s - loss: 0.2727 - sparse_categorical_accuracy: 0.9006</code> <br> <br>  La proporci√≥n de respuestas correctas a los datos para la capacitaci√≥n fue del 90.06%.  Verificamos la calidad de los datos de prueba usando TPU: <br><br><pre> <code class="python hljs">scores = tpu_model.evaluate(x_test, y_test, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, batch_size=batch_size * <span class="hljs-number"><span class="hljs-number">8</span></span>) print(<span class="hljs-string"><span class="hljs-string">"     : %.2f%%"</span></span> % (scores[<span class="hljs-number"><span class="hljs-number">1</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>))</code> </pre> <br> <code>     : 80.79%</code> <br> <br>  Ahora guarde los pesos del modelo entrenado: <br><br><pre> <code class="python hljs">tpu_model.save_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  TensorFlow nos dar√° un mensaje de que los pesos se transfieren desde la TPU a la CPU: <br> <code>INFO:tensorflow:Copying TPU weights to the CPU</code> <br> <br>  Cabe se√±alar que los pesos de la red capacitada se guardaron en el disco de la m√°quina virtual colaborativa.  Cuando la m√°quina virtual se detiene, todos los datos de ella se borrar√°n.  Si no desea perder pesos entrenados, gu√°rdelos en su computadora: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br><h2>  Reconociendo objetos en la CPU </h2><br>  Ahora intentemos usar un modelo entrenado en TPU para reconocer objetos en im√°genes usando la CPU.  Para hacer esto, cree el modelo nuevamente y cargue los pesos entrenados en TPU en √©l: <br><br><pre> <code class="python hljs">model = create_model() model.load_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  El modelo est√° listo para usar en el procesador central.  Tratemos de reconocer con su ayuda una de las im√°genes del conjunto de pruebas CIFAR-10: <br><br><pre> <code class="python hljs">index=<span class="hljs-number"><span class="hljs-number">111</span></span> plt.imshow(toimage(x_test[index])) plt.show()</code> </pre> <br><img src="https://habrastorage.org/webt/za/z3/f-/zaz3f-jatj-5crlgsih84gsakg0.png"><br><br>  La imagen es peque√±a, pero puedes entender que este es un avi√≥n.  Comenzamos el reconocimiento: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      CIFAR-10 classes=['', '', '', '', '', '', '', '', '', ''] x = x_test[index] #  , .. Keras    x = np.expand_dims(x, axis=0) #   prediction = model.predict(x) #       print(prediction) #     prediction = np.argmax(prediction) print(classes[prediction])</span></span></code> </pre> <br>  Obtenemos una lista de valores de salida de las neuronas, casi todas est√°n cerca de cero, excepto el primer valor, que corresponde al plano. <br><br> <code>[[9.81738389e-01 2.91262069e-07 1.82225723e-02 9.78524668e-07 <br> 5.89265142e-07 6.76223244e-10 1.03252004e-10 9.23009047e-09 <br> 3.71878523e-05 3.16599618e-08]] <br> </code> <br> <br>  El reconocimiento fue exitoso! <br><br><h2>  Resumen </h2><br>  Fue posible demostrar la operatividad de TPU en la plataforma de colaboraci√≥n, se puede utilizar para entrenar redes neuronales en Keras.  Sin embargo, el conjunto de datos CIFAR-10 es demasiado peque√±o; no es suficiente para cargar completamente los recursos de TPU.  La aceleraci√≥n en comparaci√≥n con la GPU result√≥ ser peque√±a (puede comprobarlo usted mismo eligiendo la GPU como acelerador en lugar de la TPU y volviendo a entrenar el modelo nuevamente). <br><br>  En Habr√© hay un art√≠culo en el que se <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mide el rendimiento de TPU y GPU V100 en el entrenamiento de la red ResNet-50</a> .  En esta tarea, la TPU mostr√≥ el mismo rendimiento que las cuatro GPU V100.  ¬°Es bueno que Google proporcione un acelerador de aprendizaje de redes neuronales tan poderoso de forma gratuita! <br><br>  Video que muestra el entrenamiento de la red neuronal Keras en TPU. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/60xbDEpA49M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Enlaces utiles </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Computadora port√°til colaborativa con c√≥digo de aprendizaje modelo Keras TPU completo</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cuaderno colaborativo con ejemplo de capacitaci√≥n Keras TPU para reconocer ropa y zapatos de Fashion MNIST</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Procesadores tensoriales en Google Cloud</a> . </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Caracter√≠sticas de la arquitectura y uso de procesadores tensoriales</a> . </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es428117/">https://habr.com/ru/post/es428117/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es428107/index.html">Contenedorizaci√≥n de aplicaciones Angular 6 SPA Template ASP .NET Core 2.1</a></li>
<li><a href="../es428109/index.html">Muro corporativo</a></li>
<li><a href="../es428111/index.html">Aritm√©tica de precisi√≥n arbitraria en Erlang</a></li>
<li><a href="../es428113/index.html">A la pregunta de las curvas de Bezier, la velocidad de Arduino y un sitio interesante, o c√≥mo pas√© el fin de semana</a></li>
<li><a href="../es428115/index.html">Desarrollo web para comercio electr√≥nico: 5 tendencias tecnol√≥gicas para 2019</a></li>
<li><a href="../es428119/index.html">"Clase-campos-propuesta" o "¬øQu√© sali√≥ mal en tc39 commit"</a></li>
<li><a href="../es428121/index.html">Stan Drapkin. Trampas de criptograf√≠a de alto nivel en .NET</a></li>
<li><a href="../es428123/index.html">Semana de la seguridad 41: buenas noticias</a></li>
<li><a href="../es428125/index.html">¬øQui√©nes son los an√°lisis de productos y por qu√© se necesitan en un equipo?</a></li>
<li><a href="../es428127/index.html">Cach√© de Nginx: todo nuevo - bien olvidado viejo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>