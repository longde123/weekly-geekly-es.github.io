<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üí¥ üï∫ ü§üüèª Einf√ºhrung von Airflow zur Verwaltung von Spark-Jobs in ivi: Hoffnungen und Kr√ºcken üö£üèø üßï üë©‚Äçüè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Aufgabe, Modelle f√ºr maschinelles Lernen in der Produktion einzusetzen, ist immer schmerzhaft und schmerzhaft, da es sehr unangenehm ist, aus eine...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Einf√ºhrung von Airflow zur Verwaltung von Spark-Jobs in ivi: Hoffnungen und Kr√ºcken</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ivi/blog/456630/">  Die Aufgabe, Modelle f√ºr maschinelles Lernen in der Produktion einzusetzen, ist immer schmerzhaft und schmerzhaft, da es sehr unangenehm ist, aus einem gem√ºtlichen Jupyter-Notebook in die Welt der √úberwachung und Fehlertoleranz zu gelangen. <br><br>  Wir haben bereits √ºber die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste Iteration der √úberarbeitung des</a> Empfehlungssystems des Online-Kinos ivi geschrieben.  Im vergangenen Jahr haben wir die Anwendungsarchitektur fast nicht fertiggestellt (von global - nur von veraltetem Python 2.7 und Python 3.4 zu ‚Äûfrischem‚Äú Python 3.6), aber wir haben einige neue ML-Modelle hinzugef√ºgt und sind sofort auf das Problem gesto√üen, neue Algorithmen in der Produktion einzuf√ºhren.  In dem Artikel werde ich √ºber unsere Erfahrungen bei der Implementierung eines solchen Tools f√ºr das Task-Flow-Management wie Apache Airflow berichten: Warum das Team diesen Bedarf hatte, was nicht zu der vorhandenen L√∂sung passte, welche Kr√ºcken auf dem Weg geschnitten werden mussten und was daraus wurde. <br><br>  ‚Üí Die Videoversion des Berichts kann hier auf YouTube (ab 03:00:00 Uhr) angesehen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qc/bw/tt/qcbwttjzivyzuedihk88u6nwn5o.png"></div><br><a name="habracut"></a><br><br><h2>  <font color="#fd004c">Hydra Team</font> </h2><br>  Ich erz√§hle Ihnen ein wenig √ºber das Projekt: ivi besteht aus mehreren Zehntausenden von Inhaltseinheiten, wir haben eines der gr√∂√üten legalen Verzeichnisse in RuNet.  Die Hauptseite der ivi-Webversion ist ein personalisierter Ausschnitt aus dem Katalog, der dem Benutzer basierend auf seinem Feedback (Ansichten, Bewertungen usw.) den umfangreichsten und relevantesten Inhalt bietet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vj/-g/nh/vj-gnheaviq-z7tibuzawo-qdqs.png"></div><br>  Der Online-Teil des Empfehlungssystems ist eine Flask-Backend-Anwendung mit einer Last von bis zu 600 RPS.  Offline wird das Modell auf mehr als 250 Millionen Inhaltsaufrufe pro Monat trainiert.  Die Datenaufbereitungs-Pipelines f√ºr das Training sind in Spark implementiert, das √ºber dem Hive-Repository ausgef√ºhrt wird. <br><br>  Das Team hat jetzt 7 Entwickler, die sowohl Modelle erstellen als auch in die Produktion einf√ºhren - dies ist ein ziemlich gro√ües Team, das praktische Tools f√ºr die Verwaltung von Aufgabenabl√§ufen ben√∂tigt. <br><br><h2>  <font color="#fd004c">Offline-Architektur</font> </h2><br>  Unten sehen Sie das Diagramm der Datenflussinfrastruktur f√ºr das Empfehlungssystem. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xs/10/nv/xs10nvc8r3mz9bd8osjrn6sqn3u.png"></div><br>  Hier sind zwei Datenspeicher dargestellt: Hive f√ºr Benutzerfeedback (Ansichten, Bewertungen) und Postgres f√ºr verschiedene Gesch√§ftsinformationen (Arten der Monetarisierung von Inhalten usw.), w√§hrend die √úbertragung von Postgres zu Hive angepasst wurde.  Ein Paket von Spark-Anwendungen saugt Daten aus Hive: und schult unsere Modelle anhand dieser Daten (ALS f√ºr pers√∂nliche Empfehlungen, verschiedene kollaborative Modelle f√ºr die √Ñhnlichkeit von Inhalten). <br><br>  Spark-Anwendungen werden traditionell von einer dedizierten virtuellen Maschine verwaltet, die wir Hydra-Updater mit einer Reihe von Cron + Shell-Skripten nennen.  Dieses Bundle wurde seit jeher in der ivi-Betriebsabteilung erstellt und hat hervorragend funktioniert.  Das Shell-Skript war ein einziger Einstiegspunkt f√ºr das Starten von Spark-Anwendungen - das hei√üt, jedes neue Modell begann sich erst im Produkt zu drehen, nachdem die Administratoren dieses Skript fertiggestellt hatten. <br><br>  Einige der Artefakte des Modelltrainings werden in HDFS zur ewigen Speicherung gespeichert (und warten darauf, dass jemand sie von dort herunterl√§dt und auf den Server √ºbertr√§gt, auf dem sich der Online-Teil dreht), und andere werden direkt vom Spark-Treiber in den Redis-Schnellspeicher geschrieben, den wir allgemein verwenden Speicher f√ºr mehrere Dutzend Python-Prozesse des Online-Teils. <br><br>  Eine solche Architektur hat im Laufe der Zeit eine Reihe von Nachteilen angeh√§uft: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/x2/b0/rix2b0qjxi1qa0igp-bpl23bn04.png"></div><br>  Das Diagramm zeigt, dass Datenfl√ºsse eine ziemlich komplizierte und komplizierte Struktur haben - ohne ein einfaches und klares Werkzeug zur Verwaltung dieses Gutes werden Entwicklung und Betrieb zu Horror, Verfall und Leiden. <br><br>  Neben der Verwaltung von Spark-Anwendungen bietet das Administrationsskript viele n√ºtzliche Funktionen: Neustarten von Diensten im Kampf, Redis-Dump und andere Systemaufgaben.  Offensichtlich ist das Skript √ºber einen langen Zeitraum mit vielen Funktionen √ºberf√ºllt, da jedes neue Modell von uns ein paar Dutzend Zeilen darin generiert hat.  Das Skript sah in Bezug auf die Funktionalit√§t zu √ºberladen aus. Daher wollten wir als Team des Empfehlungssystems irgendwo einen Teil der Funktionalit√§t entfernen, die das Starten und Verwalten von Spark-Anwendungen betrifft.  F√ºr diese Zwecke haben wir uns f√ºr Airflow entschieden. <br><br><h2>  <font color="#fd004c">Kr√ºcken f√ºr Luftstrom</font> </h2><br>  Neben der L√∂sung all dieser Probleme haben wir nat√ºrlich auch neue f√ºr uns erstellt. Die Bereitstellung von Airflow zum Starten und √úberwachen von Spark-Anwendungen erwies sich als schwierig. <br><br>  Die Hauptschwierigkeit bestand darin, dass niemand die gesamte Infrastruktur f√ºr uns umbauen w√ºrde, weil  Devops Ressource ist eine knappe Sache.  Aus diesem Grund mussten wir Airflow nicht nur implementieren, sondern in das bestehende System integrieren, was von Grund auf viel schwieriger zu erkennen ist. <br><br>  Ich m√∂chte √ºber die Schmerzen sprechen, die wir w√§hrend des Implementierungsprozesses hatten, und √ºber die Kr√ºcken, die wir einschlagen mussten, um den Luftstrom zu erhalten. <br><br>  <b>Der erste und wichtigste Punkt</b> : Wie integriert man Airflow in ein gro√ües Shell-Skript der Betriebsabteilung? <br><br>  Hier ist die L√∂sung am offensichtlichsten: Wir haben begonnen, Diagramme direkt aus dem Shell-Skript mithilfe der Luftstrom-Bin√§rdatei mit dem Schl√ºssel trigger_dag auszul√∂sen.  Bei diesem Ansatz verwenden wir nicht den Airflow-Sheduler, und tats√§chlich wird die Spark-Anwendung mit derselben Krone gestartet - dies ist religi√∂s nicht sehr korrekt.  Wir haben jedoch eine nahtlose Integration in eine vorhandene L√∂sung erhalten.  So sieht der Start aus dem Shell-Skript unserer Hauptanwendung Spark aus, die historisch als Hydramatrices bezeichnet wurde. <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$FUNCNAME</span></span></span><span class="hljs-string"> started"</span></span> <span class="hljs-built_in"><span class="hljs-built_in">local</span></span> RETVAL=0 <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_CONFIG=/opt/airflow/airflow.cfg AIRFLOW_API=api/dag_last_run/hydramatrices/all <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"run /var/www/airflow/bin/airflow trigger_dag hydramatrices"</span></span> /var/www/airflow/bin/airflow trigger_dag hydramatrices 2&gt;&amp;1 | tee -a <span class="hljs-variable"><span class="hljs-variable">$LOGFILE</span></span></code> </pre> <br>  <b>Schmerz: Das</b> Shell-Skript der Betriebsabteilung muss irgendwie den Status des Luftstromdiagramms bestimmen, um seinen eigenen Ausf√ºhrungsfluss zu steuern. <br><br>  Crutch: Wir haben die Airflow REST-API um einen Endpunkt f√ºr die DAG-√úberwachung direkt in Shell-Skripten erweitert.  Jetzt hat jeder Graph drei Zust√§nde: RUNNING, SUCCEED, FAILED. <br><br>  Nachdem wir die Berechnungen in Airflow gestartet haben, rufen wir einfach regelm√§√üig das laufende Diagramm ab: Wir schreiben die GET-Anforderung auf, um festzustellen, ob die DAG abgeschlossen ist oder nicht.  Wenn der √úberwachungsendpunkt auf die erfolgreiche Ausf√ºhrung des Diagramms antwortet, f√ºhrt das Shell-Skript seinen Ablauf weiter aus. <br>  Ich m√∂chte sagen, dass die Airflow REST-API nur eine feurige Sache ist, mit der Sie Ihre Pipelines flexibel konfigurieren k√∂nnen - zum Beispiel k√∂nnen Sie POST-Parameter an Diagramme weiterleiten. <br><br>  Die Airflow-API-Erweiterung ist nur eine Python-Klasse, die ungef√§hr so ‚Äã‚Äãaussieht: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> settings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DagBag, DagRun <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Blueprint, request, Response airflow_api_blueprint = Blueprint(<span class="hljs-string"><span class="hljs-string">'airflow_api'</span></span>, __name__, url_prefix=<span class="hljs-string"><span class="hljs-string">'/api'</span></span>) AIRFLOW_DAGS = <span class="hljs-string"><span class="hljs-string">'{}/dags'</span></span>.format( os.path.dirname(os.path.dirname(os.path.abspath(__file__))) ) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ApiResponse</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    GET """</span></span> STATUS_OK = <span class="hljs-number"><span class="hljs-number">200</span></span> STATUS_NOT_FOUND = <span class="hljs-number"><span class="hljs-number">404</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">standard_response</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(status: int, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> json_data = json.dumps(payload) resp = Response(json_data, status=status, mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> resp <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">success</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, payload: dict)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(self.STATUS_OK, payload) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, status: int, message: str)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.standard_response(status, {<span class="hljs-string"><span class="hljs-string">'error'</span></span>: message}) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">not_found</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, message: str = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Resource not found'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> -&gt; Response:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.error(self.STATUS_NOT_FOUND, message)</code> </pre><br>  Wir verwenden die API im Shell-Skript - wir fragen den Endpunkt alle 10 Minuten ab: <br><br><pre> <code class="bash hljs"> TRIGGER=$? [ <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TRIGGER</span></span></span><span class="hljs-string">"</span></span> -eq <span class="hljs-string"><span class="hljs-string">"0"</span></span> ] &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG succeeded"</span></span> || { <span class="hljs-built_in"><span class="hljs-built_in">log</span></span> <span class="hljs-string"><span class="hljs-string">"trigger airflow DAG failed"</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> 1; } CMD=<span class="hljs-string"><span class="hljs-string">"curl -s http://</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HYDRA_SERVER</span></span></span><span class="hljs-string">/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$AIRFLOW_API</span></span></span><span class="hljs-string"> | jq .dag_last_run.state"</span></span> STATE=$(<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> <span class="hljs-variable"><span class="hljs-variable">$CMD</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> [ <span class="hljs-variable"><span class="hljs-variable">$STATE</span></span> == \<span class="hljs-string"><span class="hljs-string">"running\" ]; do log "</span></span>Generating matrices <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> progress...<span class="hljs-string"><span class="hljs-string">" sleep 600 STATE=</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(eval $CMD)</span></span></span><span class="hljs-string"> done [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$STATE</span></span></span><span class="hljs-string"> == \"success\" ] &amp;&amp; RETVAL=0 || RETVAL=1 [ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span><span class="hljs-string"> -eq 0 ] &amp;&amp; log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> succeeded<span class="hljs-string"><span class="hljs-string">" || log "</span></span><span class="hljs-variable"><span class="hljs-variable">$FUNCNAME</span></span> failed<span class="hljs-string"><span class="hljs-string">" return </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$RETVAL</span></span></span></span></code> </pre><br>  <b>Schmerz</b> : Wenn Sie jemals einen Spark-Job mit Spark-Submit im Cluster-Modus ausf√ºhren, wissen Sie, dass die Protokolle in STDOUT ein nicht informatives Blatt mit den Zeilen "SPARK APPLICATION_ID IS RUNNING" sind.  Die Protokolle der Spark-Anwendung selbst k√∂nnen beispielsweise mit dem Befehl Garnprotokolle angezeigt werden.  In einem Shell-Skript wurde dieses Problem einfach gel√∂st: Ein SSH-Tunnel wurde f√ºr einen der Cluster-Computer ge√∂ffnet und die Funken√ºbermittlung wurde im Client-Modus f√ºr diesen Computer ausgef√ºhrt.  In diesem Fall verf√ºgt STDOUT √ºber lesbare und verst√§ndliche Protokolle.  In Airflow haben wir uns f√ºr die Cluster-Entscheidung entschieden, und eine solche Nummer funktioniert nicht. <br><br>  Crutch: Nachdem Spark-Submit funktioniert hat, ziehen wir die Treiberprotokolle mit application_id aus HDFS und zeigen sie in der Airflow-Oberfl√§che einfach √ºber den Python print () -Operator an.  Das einzig Negative - in der Airflow-Oberfl√§che werden die Protokolle erst angezeigt, nachdem die Funken√ºbermittlung funktioniert hat. Sie m√ºssen die Echtzeit an anderen Stellen verfolgen - beispielsweise an der YARN-Webm√ºndung. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_logs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(config: BaseConfig, app_id: str)</span></span></span><span class="hljs-function"> -&gt; </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">None</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   :param config: :param app_id: """</span></span> hdfs = HDFSInteractor(config) logs_path = <span class="hljs-string"><span class="hljs-string">'/tmp/logs/{username}/logs/{app_id}'</span></span>.format(username=config.CURRENT_USERNAME, app_id=app_id) logs_files = hdfs.files_in_folder(logs_path) logs_files = [file <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file[<span class="hljs-number"><span class="hljs-number">-4</span></span>:] != <span class="hljs-string"><span class="hljs-string">'.tmp'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> logs_files: <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> hdfs.hdfs_client.read(os.path.join(logs_path, file), encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, delimiter=<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> reader: print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> reader: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stdout'</span></span>, line) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> len(line) &gt; <span class="hljs-number"><span class="hljs-number">30</span></span>: print_line = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'stderr'</span></span>, line): print_line = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> print_line: print(line)</code> </pre><br>  <b>Schmerz</b> : F√ºr Tester und Entwickler w√§re es sch√∂n, einen Airflow-Pr√ºfstand zu haben, aber wir sparen Entwicklungsressourcen, daher haben wir lange dar√ºber nachgedacht, wie die Testumgebung bereitgestellt werden kann. <br><br>  Crutch: Wir haben Airflow in einen Docker-Container gepackt und Dockerfile hat es mit Spark-Jobs direkt in das Repository gestellt.  Somit kann jeder Entwickler oder Tester seinen eigenen Luftstrom auf einem lokalen Computer erh√∂hen.  Aufgrund der Tatsache, dass Anwendungen im Cluster-Modus ausgef√ºhrt werden, sind lokale Ressourcen f√ºr Docker fast nicht erforderlich. <br><br>  Eine lokale Installation des Funkens wurde im Docker-Container und seiner gesamten Konfiguration √ºber Umgebungsvariablen versteckt - Sie m√ºssen nicht mehr mehrere Stunden damit verbringen, die Umgebung einzurichten.  Unten habe ich ein Beispiel mit einem Docker-Dateifragment f√ºr einen Container mit Airflow gegeben, in dem Sie sehen k√∂nnen, wie Airflow mithilfe von Umgebungsvariablen konfiguriert wird: <br><br><pre> <code class="bash hljs">FROM ubuntu:16.04 ARG AIRFLOW_VERSION=1.9.0 ARG AIRFLOW_HOME ARG USERNAME=airflow ARG USER_ID ARG GROUP_ID ARG LOCALHOST ARG AIRFLOW_PORT ARG PIPENV_PATH ARG PROJECT_HYDRAMATRICES_DOCKER_PATH RUN apt-get update \ &amp;&amp; apt-get install -y \ python3.6 \ python3.6-dev \ &amp;&amp; update-alternatives --install /usr/bin/python3 python3.6 /usr/bin/python3.6 0 \ &amp;&amp; apt-get -y install python3-pip RUN mv /root/.pydistutils.cf /root/.pydistutils.cfg RUN pip3 install pandas==0.20.3 \ apache-airflow==<span class="hljs-variable"><span class="hljs-variable">$AIRFLOW_VERSION</span></span> \ psycopg2==2.7.5 \ ldap3==2.5.1 \ cryptography <span class="hljs-comment"><span class="hljs-comment">#   ,       ENV PROJECT_HYDRAMATRICES_DOCKER_PATH=${PROJECT_HYDRAMATRICES_DOCKER_PATH} ENV PIPENV_PATH=${PIPENV_PATH} ENV SPARK_HOME=/usr/lib/spark2 ENV HADOOP_CONF_DIR=$PROJECT_HYDRAMATRICES_DOCKER_PATH/etc/hadoop-conf-preprod ENV PYTHONPATH=${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib ENV PIP_NO_BINARY=numpy ENV AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW_DAGS=${AIRFLOW_HOME}/dags ENV AIRFLOW_LOGS=${AIRFLOW_HOME}/logs ENV AIRFLOW_PLUGINS=${AIRFLOW_HOME}/plugins #      Airflow (log url) BASE_URL="http://${AIRFLOW_CURRENT_HOST}:${AIRFLOW_PORT}" ; #   Airflow ENV AIRFLOW__WEBSERVER__BASE_URL=${BASE_URL} ENV AIRFLOW__WEBSERVER__ENDPOINT_URL=${BASE_URL} ENV AIRFLOW__CORE__AIRFLOW_HOME=${AIRFLOW_HOME} ENV AIRFLOW__CORE__DAGS_FOLDER=${AIRFLOW_DAGS} ENV AIRFLOW__CORE__BASE_LOG_FOLDER=${AIRFLOW_LOGS} ENV AIRFLOW__CORE__PLUGINS_FOLDER=${AIRFLOW_PLUGINS} ENV AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY=${AIRFLOW_LOGS}/scheduler</span></span></code> </pre><br>  Durch die Implementierung von Airflow haben wir folgende Ergebnisse erzielt: <br><br><ul><li>  Reduzierter Release-Zyklus: Beim Rollout eines neuen Modells (oder einer neuen Datenaufbereitungs-Pipeline) muss nun ein neues Airflow-Diagramm geschrieben werden. Die Diagramme selbst werden im Repository gespeichert und mit dem Code bereitgestellt.  Dieser Prozess liegt vollst√§ndig in den H√§nden des Entwicklers.  Admins sind gl√ºcklich, wir ziehen sie nicht mehr an Kleinigkeiten. </li><li>  Spark-Anwendungsprotokolle, die fr√ºher direkt zur H√∂lle gingen, werden jetzt in Aiflow mit einer praktischen Zugriffsoberfl√§che gespeichert.  Sie k√∂nnen die Protokolle f√ºr jeden Tag anzeigen, ohne sie in HDFS-Verzeichnissen auszuw√§hlen. </li><li>  Die fehlgeschlagene Berechnung kann mit einer Schaltfl√§che in der Benutzeroberfl√§che neu gestartet werden. Dies ist sehr praktisch, selbst June kann damit umgehen. </li><li>  Sie k√∂nnen Spark-Jobs √ºber die Benutzeroberfl√§che aufzeichnen, ohne die Spark-Einstellungen auf dem lokalen Computer ausf√ºhren zu m√ºssen.  Tester sind zufrieden - alle Einstellungen f√ºr die ordnungsgem√§√üe Funktion von Spark-Submit wurden bereits in Dockerfile vorgenommen </li><li>  Aiflow-Standardbr√∂tchen - Zeitpl√§ne, Neustart fehlgeschlagener Jobs, sch√∂ne Diagramme (z. B. Ausf√ºhrungszeit der Anwendung, Statistiken √ºber erfolgreiche und erfolglose Starts). </li></ul><br>  Wohin als n√§chstes?  Jetzt haben wir eine gro√üe Anzahl von Datenquellen und -senken, deren Anzahl zunehmen wird.  √Ñnderungen in einer Hydramatrices-Repository-Klasse k√∂nnen in einer anderen Pipeline (oder sogar im Online-Teil) abst√ºrzen: <br><br><ul><li>  Clickhouse l√§uft √ºber ‚Üí Hive </li><li>  Datenvorverarbeitung: Hive ‚Üí Hive </li><li>  Bereitstellen von c2c-Modellen: Hive ‚Üí Redis </li><li>  Erstellung von Verzeichnissen (wie die Art der Monetarisierung von Inhalten): Postgres ‚Üí Redis </li><li>  Modellvorbereitung: Lokale FS ‚Üí HDFS </li></ul><br>  In einer solchen Situation brauchen wir wirklich einen Stand f√ºr das automatische Testen von Pipelines bei der Datenaufbereitung.  Dies wird die Kosten f√ºr das Testen von √Ñnderungen im Repository erheblich senken, die Einf√ºhrung neuer Modelle in der Produktion beschleunigen und den Endorphinspiegel in Testern drastisch erh√∂hen.  Aber ohne Airflow w√§re es unm√∂glich, einen St√§nder f√ºr diese Art von Autotest aufzustellen! <br><br>  Ich habe diesen Artikel geschrieben, um √ºber unsere Erfahrungen bei der Implementierung von Airflow zu berichten, die f√ºr andere Teams in einer √§hnlichen Situation n√ºtzlich sein k√∂nnen. Sie haben bereits ein gro√ües Arbeitssystem und m√∂chten etwas Neues, Modisches und Jugendliches ausprobieren.  Sie m√ºssen keine Angst vor Aktualisierungen des Arbeitssystems haben, sondern m√ºssen versuchen, zu experimentieren. Solche Experimente er√∂ffnen normalerweise neue Horizonte f√ºr die weitere Entwicklung. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456630/">https://habr.com/ru/post/de456630/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456614/index.html">Wie eine Pestgeschichte: Unschuld wird gerendert</a></li>
<li><a href="../de456616/index.html">3 Millionen Rubel f√ºr diejenigen, die codieren k√∂nnen</a></li>
<li><a href="../de456618/index.html">Larabeer Moskau - 21. Juni</a></li>
<li><a href="../de456622/index.html">So erstellen Sie ein Betriebssystem, das gem√§√ü dem Schutz der Klasse I zertifiziert ist</a></li>
<li><a href="../de456624/index.html">N√ºtzliche Python-Tools</a></li>
<li><a href="../de456632/index.html">Wir erstellen die vierte Etage von C ++ - Vorlagen in RESTinio. Warum und wie?</a></li>
<li><a href="../de456634/index.html">Nginx-Rezepte: CAS (Central Authorization Service)</a></li>
<li><a href="../de456638/index.html">Vergleichen des gleichen Projekts in Rust, Haskell, C ++, Python, Scala und OCaml</a></li>
<li><a href="../de456640/index.html">Analyse des Competitive Intelligence Contest an PHDays 9</a></li>
<li><a href="../de456642/index.html">Der erste Abschluss des JetBrains Corporate Master-Programms und der ITMO University</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>