<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöâ üë®üèª‚Äçüî¨ üçÆ 1600bit / s Sprachcodierung mit dem neuronalen Vocoder LPCNet üôé ü§≤ üè≠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dies ist eine Fortsetzung des ersten Artikels √ºber LPCNet . In der ersten Demo haben wir eine Architektur vorgestellt , die Signalverarbeitung und Dee...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>1600bit / s Sprachcodierung mit dem neuronalen Vocoder LPCNet</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446656/"><img src="https://habrastorage.org/getpro/habr/post_images/6ba/d56/c2e/6bad56c2eecd2e1aad4190ba40d1be74.jpg"><br><br>  Dies ist eine Fortsetzung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikels √ºber LPCNet</a> .  In der ersten Demo haben wir eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Architektur vorgestellt</a> , die Signalverarbeitung und Deep Learning kombiniert, um die Effektivit√§t der neuronalen Sprachsynthese zu verbessern.  Dieses Mal werden wir LPCNet in einen neuronalen Sprachcodec mit einer sehr niedrigen Bitrate verwandeln (siehe den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">wissenschaftlichen Artikel</a> ).  Es kann auf aktuellen Ger√§ten und sogar auf Telefonen verwendet werden. <br><br>  Zum ersten Mal arbeitet ein neuronaler Vocoder in Echtzeit auf einem Prozessorkern des Telefons und nicht auf einer Hochgeschwindigkeits-GPU.  Die endg√ºltige Bitrate von 1600 Bit / s ist etwa zehnmal niedriger als die von normalen Breitband-Codecs.  Die Qualit√§t ist viel besser als bei vorhandenen Vocodern mit einer sehr niedrigen Bitrate und vergleichbar mit herk√∂mmlichen Codecs, die eine h√∂here Bitrate verwenden. <br><a name="habracut"></a><br><h3>  Wellenformcodierer und Vocoder </h3><br>  Es gibt zwei gro√üe Arten von Sprachcodecs: Wellenformcodierer und Vocoder.  Zu den Wellenformcodierern geh√∂ren Opus, AMR / AMR-WB und alle Codecs, die f√ºr Musik verwendet werden k√∂nnen.  Sie versuchen, eine decodierte Wellenform so nah wie m√∂glich am Original bereitzustellen - normalerweise unter Ber√ºcksichtigung einiger Wahrnehmungsmerkmale.  Vocoder hingegen sind eigentlich Synthesizer.  Der Codierer extrahiert Informationen √ºber die Tonh√∂he und Form des Sprachpfads, leitet diese Informationen an den Decodierer weiter und synthetisiert die Sprache neu.  Es ist fast wie bei der Spracherkennung, gefolgt vom Lesen von Text in einem Sprachsynthesizer, nur dass der Textcodierer viel einfacher / schneller als die Spracherkennung ist (und etwas mehr Informationen vermittelt). <br><br>  Vocoder gibt es seit den 70er Jahren, aber da ihre Decoder eine Sprachsynthese durchf√ºhren, k√∂nnen sie nicht viel besser sein als herk√∂mmliche Sprachsynthesesysteme, die bis vor kurzem einfach schrecklich klangen.  Aus diesem Grund wurden Vocoder typischerweise bei Geschwindigkeiten unter 3 kB / s verwendet.  Dar√ºber hinaus bieten Wellenformcodierer einfach die beste Qualit√§t.  Dies dauerte bis vor kurzem, als neuronale Sprachsynthesesysteme wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveNet auf den Markt kamen</a> .  Pl√∂tzlich klang die Synthese viel besser, und nat√ºrlich gab es Leute, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus WaveNet einen Vocoder machen wollten</a> . <br><br><h3>  LPCNet-√úbersicht </h3><br>  WaveNet erzeugt Sprache von sehr hoher Qualit√§t, erfordert jedoch Hunderte von Gigaflops an Rechenleistung.  LPCNet reduzierte den Rechenaufwand erheblich.  Der Vocoder basiert auf WaveRNN, das WaveNet mithilfe eines wiederkehrenden neuronalen Netzwerks (RNN) und sp√§rlicher Matrizen verbessert.  LPCNet erweitert WaveRNN weiter durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">lineare Vorhersage</a> (LPC), die bei √§lteren Vocodern eine gute Leistung erbrachte.  Es sagt eine Stichprobe aus einer linearen Kombination vorheriger Stichproben voraus und macht sie vor allem um ein Vielfaches schneller als ein neuronales Netzwerk.  Nat√ºrlich ist es nicht universell (sonst w√ºrden Vocoder der 70er Jahre gro√üartig klingen), aber es kann die Belastung des neuronalen Netzwerks ernsthaft reduzieren.  Auf diese Weise k√∂nnen Sie ein kleineres Netzwerk als WaveRNN verwenden, ohne die Qualit√§t zu beeintr√§chtigen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/652/8fc/7df/6528fc7df88256e797551173b11f5e1d.png"></div><br>  <i><font color="gray">Schauen wir uns LPCNet genauer an.</font></i>  <i><font color="gray">Der gelbe Teil links wird einmal pro Frame berechnet und seine Ausgabe wird f√ºr die Netzwerkabtastfrequenz rechts (blau) verwendet.</font></i>  <i><font color="gray">Die Recheneinheit sagt eine Stichprobe zum Zeitpunkt t basierend auf vorherigen Stichproben und linearen Vorhersagekoeffizienten voraus</font></i> <br><br><h1>  Kompressionseigenschaften </h1><br>  LPCNet synthetisiert 10 ms lang Sprache aus Vektoren mit 20 Zeichen pro Frame.  Von diesen sind 18 Zeichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cepstralkoeffizienten</a> , die die Form des Spektrums darstellen.  Die beiden verbleibenden beschreiben die H√∂he: einen Parameter f√ºr den Tonh√∂henschritt (Tonh√∂henperiode) und den anderen f√ºr die <i>St√§rke</i> (wie stark das Signal mit sich selbst korreliert, wenn Sie eine Verz√∂gerung durch die Tonh√∂he einf√ºhren).  Wenn Sie die Parameter in Form von Gleitkommawerten speichern, ben√∂tigen alle diese Informationen w√§hrend der Speicherung oder √úbertragung bis zu 64 kbit / s.  Dies ist zu viel, da selbst der Opus-Codec eine sehr hochwertige Sprachcodierung mit nur 16 kbit / s (f√ºr 16 kHz Mono) bietet.  Nat√ºrlich m√ºssen Sie hier eine starke Komprimierung anwenden. <br><br><h3>  H√∂he </h3><br>  Alle Codecs sind stark von der Tonh√∂he abh√§ngig, aber im Gegensatz zu Wellenformcodierern, bei denen die Tonh√∂he ‚Äûnur‚Äú zur Reduzierung der Redundanz beitr√§gt, haben Vocoder keinen Fallback.  Wenn Sie die falsche H√∂he w√§hlen, wird eine schlecht klingende (oder sogar unleserliche) Sprache erzeugt.  Ohne auf Details einzugehen (siehe den wissenschaftlichen Artikel), bem√ºht sich der LPCNet-Encoder, keinen H√∂henfehler zu machen.  Die Suche beginnt mit einer Suche nach <i>Zeitkorrelationen</i> in einem Sprachsignal.  Sehen Sie unten, wie eine typische Suche funktioniert. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3e4/024/a6a/3e4024a6aac9fcd8cd1fb8eb750918e6.gif"><br>  <i><font color="gray">Die Tonh√∂he ist der Zeitraum, in dem die Tonh√∂he wiederholt wird.</font></i>  <i><font color="gray">Die Animation sucht verz√∂gert nach dem Schritt, der der maximalen Korrelation zwischen dem Signal x (n) und seiner Kopie x (nT) entspricht.</font></i>  <i><font color="gray">Der T-Wert mit maximaler Korrelation ist ein H√∂henabstand</font></i> <br><br>  Diese Informationen m√ºssen mit so wenig Bits wie m√∂glich codiert werden, ohne das Ergebnis zu stark zu beeintr√§chtigen.  Da wir die Frequenz von Natur aus auf einer logarithmischen Skala wahrnehmen (zum Beispiel verdoppelt jede musikalische Oktave die vorherige Frequenz), ist dies bei der logarithmischen Codierung sinnvoll.  Die H√∂he des Sprachsignals liegt bei den meisten Menschen (wir versuchen hier nicht, die Sopranistin abzudecken) zwischen 62,5 und 500 Hz.  Mit sieben Bits (128 m√∂gliche Werte) erhalten wir eine Aufl√∂sung von ungef√§hr einem Viertelton (der Unterschied zwischen und vor und re ist ein Ton). <br><br>  Also, mit der H√∂he fertig?  Na ja, nicht so schnell.  Die Leute sprechen nicht wie Roboter aus Filmen der 1960er Jahre.  Die Tonh√∂he kann sogar innerhalb eines 40-Millisekunden-Pakets variieren.  Sie m√ºssen dies ber√ºcksichtigen und die Bits f√ºr den Parameter zum √Ñndern der H√∂he belassen: 3 Bits, um die Differenz von bis zu 2,5 Halbt√∂nen zwischen dem Anfang und dem Ende des Pakets zu codieren.  Schlie√ülich m√ºssen Sie die Korrelation der Tonh√∂henschritte codieren und zwischen Vokalen und Konsonanten (z. B. s und f) unterscheiden.  Zwei Bits reichen f√ºr die Korrelation aus. <br><br><h3>  Cepstrum </h3><br>  W√§hrend die Tonh√∂he die √§u√üeren Merkmale der Sprache enth√§lt (Prosodie, Emotion, Betonung, ...), bestimmt die spektrale Eigenschaft, <i>was</i> gesagt wurde (mit Ausnahme von Tonsprachen wie Chinesisch, bei denen die Tonh√∂he f√ºr die Bedeutung wichtig ist).  Die Stimmb√§nder erzeugen f√ºr jeden Vokal ungef√§hr den gleichen Klang, aber die Form des Stimmapparates bestimmt, welcher Klang gesprochen wird.  Der Sprachpfad fungiert als Filter, und die Aufgabe des Codierers besteht darin, diesen Filter auszuwerten und an den Decodierer weiterzuleiten.  Dies kann effektiv durchgef√ºhrt werden, wenn Sie das Spektrum in ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Cepstrum</a> konvertieren (ja, dies ist ein ‚ÄûSpektrum‚Äú mit einer ge√§nderten Reihenfolge der Buchstaben, das sind wir lustigen Typen in der digitalen Signalverarbeitung). <br><br>  F√ºr ein Eingangssignal bei 16 kHz repr√§sentiert das Cepstrum grunds√§tzlich alle 10 ms einen Vektor mit 18 Zahlen, die so weit wie m√∂glich komprimiert werden m√ºssen.  Da wir vier solcher Vektoren in einem 40-ms-Paket haben und sie normalerweise einander √§hnlich sind, m√∂chten wir die Redundanz so weit wie m√∂glich beseitigen.  Dies kann unter Verwendung benachbarter Vektoren als Pr√§diktoren erfolgen und nur die Differenz zwischen der Vorhersage und dem realen Wert √ºbermitteln.  Gleichzeitig m√∂chten wir uns nicht zu sehr auf fr√ºhere Pakete verlassen, wenn eines davon verschwindet.  Es sieht so aus, als ob das Problem bereits behoben wurde ... <br><br>  <font color="brown"><i>Wenn Sie nur einen Hammer haben, sieht alles aus wie ein Nagel - Abraham Maslow.</i></font> <br><br>  Wenn Sie viel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mit Video-Codecs gearbeitet haben</a> , sind Sie wahrscheinlich auf das Konzept der B-Frames gesto√üen.  Im Gegensatz zu Videocodecs, die einen Frame in viele Pakete unterteilen, haben wir im Gegenteil viele Frames in einem Paket.  Wir beginnen mit der Codierung des <i>Schl√ºsselrahmens</i> , d. H. Des unabh√§ngigen Vektors, und des <b>Endes des</b> Pakets.  Dieser Vektor wird ohne Vorhersage codiert und belegt 37 Bit: 7 f√ºr die Gesamtenergie (erster Cepstralkoeffizient) und 30 Bit f√ºr andere Parameter unter Verwendung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vektorquantisierung</a> (VQ).  Dann kommen die (hierarchischen) B-Frames.  Von den beiden Schl√ºsselw√∂rtern (eines aus dem aktuellen Paket und eines aus dem vorherigen) wird ein Cepstrum zwischen ihnen vorhergesagt.  Als Pr√§diktor f√ºr die Codierung der Differenz zwischen dem realen Wert und der Vorhersage k√∂nnen Sie entweder zwei Keyframes oder deren Durchschnittswert ausw√§hlen.  Wir verwenden wieder VQ und codieren diesen Vektor mit insgesamt 13 Bits, einschlie√ülich der Wahl des Pr√§diktors.  Jetzt haben wir nur noch zwei Vektoren und sehr wenige Bits.  Verwenden Sie die letzten 3 Bits, um einfach den Pr√§diktor f√ºr die verbleibenden Vektoren auszuw√§hlen.  Nat√ºrlich ist dies alles in der Abbildung viel einfacher zu verstehen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/530/395/c02/530395c02aba2079c8e82e79f98071f4.png"></div><br>  <i><font color="gray">Cepstrum-Vorhersage und Quantisierung f√ºr Paket k.</font></i>  <i><font color="gray">Gr√ºne Vektoren werden unabh√§ngig quantisiert, blaue Vektoren werden vorhergesagt und rote Vektoren verwenden eine Vorhersage ohne Restquantisierung.</font></i>  <i><font color="gray">Die Vorhersage wird durch Pfeile angezeigt.</font></i> <br><br><h3>  Alles zusammenf√ºgen </h3><br>  Wenn wir all das addieren, erhalten wir 64 Bit pro 40-Millisekunden-Paket oder 1600 Bit pro Sekunde.  Wenn Sie das Komprimierungsverh√§ltnis berechnen m√∂chten, betr√§gt die unkomprimierte Breitbandsprache 256 kbps (16 kHz bei 16 Bit pro Abtastung), was ein 160-faches Komprimierungsverh√§ltnis bedeutet!  Nat√ºrlich k√∂nnen Sie immer mit Quantisierern spielen und eine niedrigere oder h√∂here Bitrate erzielen (mit einem entsprechenden Effekt auf die Qualit√§t), aber Sie m√ºssen irgendwo anfangen.  Hier ist eine Tabelle mit dem Layout, in das diese Bits gehen. <br><br><table><tbody><tr><td align="center" colspan="2">  <b>Bitzuordnung</b> </td></tr><tr><td>  Parameter </td><td>  Bit </td></tr><tr><td>  Pitch Pitch </td><td>  6 </td></tr><tr><td>  H√∂henmodulation </td><td>  3 </td></tr><tr><td>  H√∂henkorrelation </td><td>  2 </td></tr><tr><td>  Energie </td><td>  7 </td></tr><tr><td>  Unabh√§ngiger Cepstrum VQ (40 ms) </td><td>  30 </td></tr><tr><td>  Voraussichtliches VQ Cepstrum (20 ms) </td><td>  13 </td></tr><tr><td>  Cepstrum-Interpolation (10 ms) </td><td>  3 </td></tr><tr><td>  Insgesamt </td><td>  64 </td></tr></tbody></table><br>  Bei 64 Bit pro Paket 40 ms, bei 25 Paketen pro Sekunde werden 1600 Bit / s erhalten. <br><br><h1>  Implementierung </h1><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der LPCNet-Quellcode</a> ist unter der BSD-Lizenz verf√ºgbar.  Es enth√§lt eine Bibliothek, die die Verwendung des Codecs vereinfacht.  Bitte beachten Sie, dass die Entwicklung noch nicht abgeschlossen ist: Sowohl das Format als auch die API m√ºssen sich √§ndern.  Das Repository verf√ºgt auch √ºber eine Demo-Anwendung <code>lpcnet_demo</code> in der der Codec einfach √ºber die Befehlszeile getestet werden kann.  Vollst√§ndige Anweisungen finden Sie in der Datei README.md. <br><br>  Wer tiefer graben m√∂chte, hat die M√∂glichkeit, neue Modelle zu trainieren und / oder LPCNet als Baustein f√ºr andere Anwendungen wie die Sprachsynthese zu verwenden (LPCNet ist nur eine Komponente des Synthesizers, f√ºhrt keine eigene Synthese durch). <br><br><h3>  Leistung </h3><br>  Die neuronale Sprachsynthese erfordert viele Ressourcen.  Auf der ICASSP-Konferenz im letzten Jahr pr√§sentierten Bastian Klein und Kollegen von Google / DeepMind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen 2400-Bit</a> / <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">s-Codec auf WaveNet-Basis</a> , der einen Bitstream von Codec2 erhielt.  Obwohl es erstaunlich klingt, bedeutet die rechnerische Komplexit√§t von Hunderten von Gigaflops, dass es nicht ohne teure GPU und ernsthaften Aufwand in Echtzeit gestartet werden kann. <br><br>  Im Gegenteil, unser 1600-Bit / s-Codec erzeugt nur 3 Gigaflops und ist so konzipiert, dass er in Echtzeit auf viel g√ºnstigeren Ger√§ten funktioniert.  Tats√§chlich kann es heute in realen Anwendungen verwendet werden.  F√ºr die Optimierung musste Code f√ºr die AVX2 / FMA- und Neon-Befehlss√§tze geschrieben werden (nur eingebetteter Code ohne Assembler).  Dank dessen k√∂nnen wir jetzt Sprache nicht nur auf einem PC, sondern auch auf mehr oder weniger modernen Telefonen in Echtzeit codieren (und insbesondere decodieren).  Nachfolgend finden Sie die Leistung auf x86- und ARM-Prozessoren. <br><br><table><tbody><tr><td colspan="4" align="center">  Leistung </td></tr><tr><td>  CPU </td><td>  Frequenz </td><td>  % eines Kerns </td><td>  In Echtzeit </td></tr><tr><td>  AMD 2990WX (Threadripper) </td><td>  3,0 GHz * </td><td>  14% </td><td>  7.0x </td></tr><tr><td>  Intel Xeon E5-2640 v4 (Broadwell) </td><td>  2,4 GHz * </td><td>  20% </td><td>  5,0x </td></tr><tr><td>  L√∂wenmaul 855 (Cortex-A76 auf <b>Galaxy S10</b> ) </td><td>  2,82 GHz </td><td>  31% </td><td>  3.2x </td></tr><tr><td>  L√∂wenmaul 845 (Cortex-A75 auf <b>Pixel 3</b> ) </td><td>  2,5 GHz </td><td>  68% </td><td>  1,47x </td></tr><tr><td>  AMD A1100 (Cortex-A57) </td><td>  1,7 GHz </td><td>  102% </td><td>  0,98x </td></tr><tr><td>  BCM2837 (Cortex-A53 auf Raspberry Pi 3) </td><td>  1,2 GHz </td><td>  310% </td><td>  0,32x </td></tr><tr><td>  * Turbomodus </td><td></td><td></td><td></td></tr></tbody></table><br><br>  Die Zahlen sind ziemlich interessant.  Obwohl nur Broadwell und Threadripper gezeigt werden, weisen Haswell- und Skylake-Prozessoren auf der x86-Plattform eine √§hnliche Leistung auf (unter Ber√ºcksichtigung der Taktfrequenz).  ARM-Prozessoren unterscheiden sich jedoch deutlich voneinander.  Selbst unter Ber√ºcksichtigung des Frequenzunterschieds ist A76 f√ºnf- bis sechsmal schneller als A53: Dies ist durchaus zu erwarten, da A53 haupts√§chlich zur Energieeffizienz eingesetzt wird (z. B. in big.LITTLE-Systemen).  Trotzdem kann LPCNet auf einem modernen Telefon mit nur einem Kern in Echtzeit arbeiten.  Es w√§re zwar sch√∂n, es in Echtzeit auf dem Raspberry Pi 3 laufen zu lassen. Nun ist das weit, aber nichts ist unm√∂glich. <br><br>  Unter x86 betr√§gt der Grund f√ºr die Leistungsbeschr√§nkung das F√ºnffache des theoretischen Maximums.  Wie Sie wissen, sind Operationen der Matrix-Vektor-Multiplikation weniger effizient als Matrix-Matrix-Operationen, da pro Operation mehr Downloads durchgef√ºhrt werden - insbesondere ein Matrix-Download f√ºr jede FMA-Operation.  Einerseits h√§ngt die Leistung mit dem L2-Cache zusammen, der nur 16 Bit pro Zyklus bereitstellt.  Auf der anderen Seite behauptet Intel, dass L2 auf Broadwell bis zu 32 Bit pro Zyklus und auf Skylake bis zu 64 Bit pro Zyklus liefern kann. <br><br><h1>  Ergebnisse </h1><br>  Wir haben Audiotests im MUSHRA-Stil durchgef√ºhrt, um die Codierungsqualit√§t zu vergleichen.  Testbedingungen: <br><br><ul><li>  <b>Beispiel</b> : Original (wenn Sie ein besseres Ergebnis als das Original erhalten, stimmt eindeutig etwas mit Ihrem Test nicht) <br></li><li>  <b>1600 bps LPCNet</b> : unsere Demo <br></li><li>  <b>Unkomprimiertes LPNet</b> : ‚ÄûLPNet mit 122 √§quivalenten Einheiten‚Äú aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Artikel</a> <br></li><li>  <b>Opus 9000 bps Breitband</b> : Niedrigste Bitrate, mit der Opus 1.3 Breitband-Audio codiert <br></li><li>  <b>MELP bei 2400 bps</b> : ein bekannter Vocoder mit einer niedrigen Bitrate (√§hnlich wie Codec2 in der Qualit√§t) <br></li><li>  <b>Speex 4000 bps</b> : Dieser Breitband-Vocoder sollte niemals verwendet werden, ist aber eine gute Referenz f√ºr den Boden </li></ul><br>  Im ersten Test (Satz 1) haben wir acht Sprachfragmente von Aussagen von zwei M√§nnern und zwei Frauen.  Dateien im ersten Satz geh√∂ren zu derselben Datenbank (d. H. Dieselben Aufzeichnungsbedingungen), die f√ºr das Training verwendet wurde, aber diese spezifischen Personen wurden aus dem Trainingssatz ausgeschlossen.  Im zweiten Test (Satz 2) haben wir einige Dateien aus dem Opus-Test (unkomprimiert) verwendet, um den Ton unter verschiedenen Bedingungen aufzunehmen, um sicherzustellen, dass LPCNet eine gewisse Verallgemeinerung aufweist.  In beiden Tests jeweils 100 Teilnehmer, daher sind die Fehler recht gering.  Siehe die Ergebnisse unten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc6/e7d/cc5/dc6e7dcc5b08735cb492ad07bf7894af.svg"></div><br>  <i><font color="gray">Subjektive Qualit√§t (MUSHRA) in zwei Tests</font></i> <br><br>  Im Allgemeinen sieht LPCNet mit 1600 Bit / s gut aus - viel besser als MELP mit 2400 Bit / s und nicht weit hinter Opus mit 9000 Bit / s.  Gleichzeitig ist unkomprimiertes LPCNet mit 9000 Bit / s etwas besser in der Qualit√§t als Opus.  Dies bedeutet, dass es m√∂glich ist, eine bessere Qualit√§t als Opus bei Bitraten im Bereich von 2000-6000 Bit / s bereitzustellen. <br><br><h3>  H√∂r dir selbst zu </h3><br>  Hier sind Beispiele aus dem Audiotest: <br><br>  Frau (Satz 1) <br><br><ul><li>  <a href="">Probe</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Unkomprimiertes LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br>  Mann (Satz 1) <br><br><ul><li>  <a href="">Probe</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Unkomprimiertes LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br>  Gemischt (Satz 2) <br><br><ul><li>  <a href="">Probe</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Unkomprimiertes LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br><br><h1>  Wo kann dies verwendet werden? </h1><br>  Wir glauben, dass dies eine coole Technologie f√ºr sich ist, aber sie hat auch praktische Anwendungen.  Hier sind nur einige Optionen. <br><br><h3>  VoIP in schlecht vernetzten L√§ndern </h3><br>  Nicht jeder hat immer eine Hochgeschwindigkeitsverbindung.  In einigen L√§ndern ist die Kommunikation sehr langsam und unzuverl√§ssig.  Ein 1600-Bit-Sprachcodec funktioniert unter solchen Bedingungen normal und √ºbertr√§gt Pakete aus Gr√ºnden der Zuverl√§ssigkeit sogar mehrmals.  Aufgrund des Overheads der Paket-Header (40 Byte f√ºr IP + UDP + RTP) ist es nat√ºrlich besser, gr√∂√üere Pakete zu erstellen: 40, 80 oder 120 ms. <br><br><h3>  Amateurfunk / HF-Radio </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">David Rowe</a> arbeitet seit zehn Jahren an der Sprachcodierung f√ºr die Funkkommunikation.  Er entwickelte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Codec2</a> , der Sprache mit einer Geschwindigkeit von 700 bis 3200 Bit / s √ºbertr√§gt.  Im vergangenen Jahr haben David und ich dar√ºber gesprochen, wie Codec2 mithilfe der neuronalen Synthese verbessert werden kann, und jetzt tun wir es endlich.  In seinem Blog schrieb David √ºber seine eigene Implementierung des LPCNet-basierten Codecs zur Integration in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FreeDV</a> . <br><br><h3>  Erh√∂hte Zuverl√§ssigkeit beim Paketverlust </h3><br>  Die F√§higkeit, einen Bitstrom mit anst√§ndiger Qualit√§t in einer kleinen Anzahl von Bits zu codieren, ist n√ºtzlich, um Redundanz auf einem unzuverl√§ssigen Kanal bereitzustellen.  Opus verf√ºgt √ºber einen Vorw√§rtsfehlerkorrekturmechanismus (FEC), der als LBRR bekannt ist und einen vorherigen Frame mit einer niedrigeren Bitrate codiert und in den aktuellen Frame sendet.  Es funktioniert gut, erh√∂ht aber den Overhead erheblich.  Die Duplizierung von 1600 Bit / s-Streams ist viel effizienter. <br><br><h1>  Pl√§ne </h1><br>  Es gibt viele weitere M√∂glichkeiten, LPCNet zu verwenden.  Zum Beispiel die Verbesserung bestehender Codecs (das gleiche Opus).  Wie bei anderen Codecs verschlechtert sich die Opus-Qualit√§t bei sehr niedrigen Bitraten (unter 8000 Bit / s) recht schnell, da dem Wellenform-Codec Bits fehlen, die dem Original entsprechen.  Die √ºbertragenen linearen Vorhersageinformationen reichen jedoch aus, damit LPCNet anst√§ndig klingende Sprache synthetisieren kann - besser als Opus bei dieser Bitrate.  Dar√ºber hinaus hilft der Rest der von Opus √ºbertragenen Informationen (Restprognose) LPCNet, ein noch besseres Ergebnis zu erzielen.  In gewissem Sinne kann LPCNet als ausgefallener Nachfilter verwendet werden, um die Qualit√§t von Opus (oder eines anderen Codecs) zu verbessern, ohne den Bitstrom zu √§ndern (d. H. Unter Beibehaltung der vollst√§ndigen Kompatibilit√§t). <br><br><h1>  Zus√§tzliche Ressourcen </h1><br><ol><li>  J.-M. Valin, J. Skoglund, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1,6 Kbit / s breitbandiger neuronaler Vocoder unter Verwendung von LPCNet</a> , <i>gesendet an Interspeech 2019</i> , arXiv: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1903.12087</a> . </li><li>  J.-M. Valin, J. Skoglund, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LPCNet: Advanced Neural Speech Synthesis Through Linear Prediction</a> , <i>Proc.</i>  <i>ICASSP, 2019</i> , arXiv: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1810.11846</a> . </li><li>  A. van den Oord, S. Dileman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalkhbrenner, E. Senor, K. Kavukuglu, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveNet: Generatives Modell f√ºr unverarbeiteten Klang</a> , 2016. </li><li>  N. Karlhbrenner, E. Elsen, C. Simonyan, S. Nouri, N. Casagrande, E. Lockhart, F. Stimberg, A. van den Oord, S. Dileman, K. Kavukuglu, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Effektive neuronale Klangsynthese</a> , 2018. </li><li>  V. B. Klein, F. S. K. Lim, A. Lyubs, J. Skoglund, F. Stimberg, K. Wang, T. S. Walters, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprachcodierung mit niedriger Bitrate basierend auf Wavenet</a> , 2018 </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Der Quellcode von</a> LPCNet. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Codec f√ºr FreeDV basierend auf LPCNet von</a> David Rowe. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nehmen Sie an</a> der Diskussion √ºber die Entwicklung auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">#opus unter irc.freenode.net teil</a> (‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Webinterface</a> ) </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446656/">https://habr.com/ru/post/de446656/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446646/index.html">InterSystems IRIS 2019.1 Release</a></li>
<li><a href="../de446648/index.html">Kubernetes Operator-Entwicklung mit Operator Framework</a></li>
<li><a href="../de446650/index.html">Wie viel kosten Tester und wovon h√§ngen ihre Geh√§lter ab? Erstellen eines Portr√§ts eines erfolgreichen QS-Spezialisten</a></li>
<li><a href="../de446652/index.html">MVCC-4. Datenschnappsch√ºsse</a></li>
<li><a href="../de446654/index.html">Wie wir die Code√ºberpr√ºfung gespeichert haben</a></li>
<li><a href="../de446658/index.html">Interview mit Andrei Stankevich √ºber Sportprogrammierung</a></li>
<li><a href="../de446660/index.html">KI, Sch√ºler und gro√üer Preis: Wie man maschinelles Lernen in der 8. Klasse macht</a></li>
<li><a href="../de446662/index.html">Transaktionen und Mechanismen zu ihrer Kontrolle</a></li>
<li><a href="../de446664/index.html">Das SAP Forum 2019 ist nur noch 2 Wochen entfernt! Was wird da sein?</a></li>
<li><a href="../de446666/index.html">Holen Sie das Beste aus Grafikrechnern heraus: Spiele auf dem TI-83</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>