<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üéé ‚è™ üßëüèΩ‚Äçü§ù‚Äçüßëüèª Intuitive Anwendung von Monte-Carlo-Methoden mit Markov-Ketten üë∂üèº üèûÔ∏è ‚òùüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ist es einfach Ich habe es versucht 
 Alexey Kuzmin, Direktor f√ºr Datenentwicklung und Arbeit bei DomKlik, Dozent f√ºr Datenwissenschaft in Netologie, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intuitive Anwendung von Monte-Carlo-Methoden mit Markov-Ketten</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  Ist es einfach  Ich habe es versucht </h4><br>  <i>Alexey Kuzmin, Direktor f√ºr Datenentwicklung und Arbeit bei DomKlik, Dozent f√ºr Datenwissenschaft in Netologie, hat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Artikel von</a> Rahul Agarwal √ºbersetzt, wie Monte-Carlo-Methoden mit Markov-Ketten funktionieren, um Probleme mit einem gro√üen Zustandsraum zu l√∂sen.</i> <br><a name="habracut"></a><br>  Jeder, der mit Data Science in Verbindung steht, hat jemals von Monte-Carlo-Methoden mit Markov-Ketten (MCMCs) geh√∂rt.  Manchmal wird das Thema beim Studium der Bayes'schen Statistik angesprochen, manchmal bei der Arbeit mit Werkzeugen wie dem Propheten. <br><br>  Aber MCMC ist schwer zu verstehen.  Wann immer ich √ºber diese Methoden las, bemerkte ich, dass die Essenz von MCMC in den tiefen Schichten des mathematischen Rauschens verborgen ist und es schwierig ist, hinter diesem Rauschen zu erkennen.  Ich musste viele Stunden damit verbringen, dieses Konzept zu verstehen. <br><br>  In diesem Artikel wird versucht, Monte-Carlo-Methoden mit Markov-Ketten zu erkl√§ren, damit klar wird, wof√ºr sie verwendet werden.  Ich werde mich in meinem n√§chsten Beitrag auf einige weitere M√∂glichkeiten konzentrieren, diese Methoden anzuwenden. <br><br>  Also fangen wir an.  MCMC besteht aus zwei Begriffen: Monte-Carlo- und Markov-Ketten.  Sprechen wir √ºber jeden von ihnen. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  Im einfachsten Sinne <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">k√∂nnen Monte-Carlo-Methoden</a> als einfache Simulationen definiert werden. <br><br>  Monte Carlo Methods erhielt ihren Namen vom Monte Carlo Casino in Monaco.  In vielen Kartenspielen m√ºssen Sie die Wahrscheinlichkeit kennen, den Dealer zu gewinnen.  Manchmal kann die Berechnung dieser Wahrscheinlichkeit mathematisch kompliziert oder unl√∂sbar sein.  Wir k√∂nnen jedoch jederzeit eine Computersimulation ausf√ºhren, um das gesamte Spiel viele Male zu spielen, und die Wahrscheinlichkeit als die Anzahl der Siege geteilt durch die Anzahl der gespielten Spiele betrachten. <br>  Dies ist alles, was Sie √ºber Monte-Carlo-Methoden wissen m√ºssen.  Ja, es ist nur eine einfache Modellierungstechnik mit einem ausgefallenen Namen. <br><br><h2>  Markov-Ketten </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Da der Begriff MCMC aus zwei Teilen besteht, m√ºssen Sie noch verstehen, was <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markov-Ketten sind</a> .  Bevor wir jedoch zu den Markov-Ketten √ºbergehen, lassen Sie uns ein wenig √ºber die Markov-Eigenschaften sprechen. <br><br>  Angenommen, es gibt ein System von M-m√∂glichen Zust√§nden, und Sie wechseln von einem Zustand in einen anderen.  Lass dich noch nicht verwirren.  Ein spezielles Beispiel f√ºr ein solches System ist das Wetter, das von hei√ü √ºber kalt bis m√§√üig wechselt.  Ein weiteres Beispiel ist der Aktienmarkt, der von einem b√§rischen zu einem bullischen und stagnierenden Staat springt. <br><br>  <i>Die Markov-Eigenschaft</i> legt nahe, dass f√ºr einen gegebenen Prozess, der sich zu einem bestimmten Zeitpunkt im Zustand X <sub>n befindet</sub> , die Wahrscheinlichkeit X <sub>n + 1</sub> = k (wobei k einer der M-Zust√§nde ist, in die der Prozess gehen kann) nur von abh√§ngt Was ist dieser Zustand im Moment.  Und nicht dar√ºber, wie es seinen aktuellen Zustand erreicht hat. <br>  In mathematischen Begriffen k√∂nnen wir dies in Form der folgenden Formel schreiben: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Aus Gr√ºnden der Klarheit ist es Ihnen egal, in welcher Reihenfolge der Markt bullisch wurde.  Die Wahrscheinlichkeit, dass der n√§chste Staat ‚Äûb√§risch‚Äú sein wird, wird nur durch die Tatsache bestimmt, dass sich der Markt derzeit in einem ‚Äûbullischen‚Äú Zustand befindet.  Das macht auch in der Praxis Sinn. <br><br>  Ein Prozess mit einer Markov-Eigenschaft wird als Markov-Prozess bezeichnet.  Warum ist die Markov-Kette wichtig?  Aufgrund seiner station√§ren Verteilung. <br><br><h3>  Was ist station√§re Verteilung? </h3><br>  Ich werde versuchen, die station√§re Verteilung zu erkl√§ren, indem ich sie f√ºr das folgende Beispiel berechne.  Angenommen, Sie haben einen Markov-Prozess f√ºr die B√∂rse, wie unten gezeigt. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Sie haben eine √úbergangswahrscheinlichkeitsmatrix, die die Wahrscheinlichkeit eines √úbergangs vom Zustand X <sub>i</sub> zum X <sub>j bestimmt</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  √úbergangswahrscheinlichkeitsmatrix, Q. <br><br>  In der transienten Wahrscheinlichkeitsmatrix Q ist die Wahrscheinlichkeit, dass der n√§chste Zustand "bull" ist, gegeben, wenn der aktuelle Zustand "bull" = 0,9 ist;  Die Wahrscheinlichkeit, dass der n√§chste Zustand "b√§risch" ist, wenn der aktuelle Zustand "bull" ist = 0,075.  Usw. <br><br>  Beginnen wir mit einem bestimmten Zustand.  Unser Zustand wird durch den Vektor [Stier, B√§r, Stagnation] bestimmt.  Wenn wir mit einem ‚Äûb√§rischen‚Äú Zustand beginnen, sieht der Vektor folgenderma√üen aus: [0,1,0].  Wir k√∂nnen die Wahrscheinlichkeitsverteilung f√ºr den n√§chsten Zustand berechnen, indem wir den aktuellen Zustandsvektor mit der √úbergangswahrscheinlichkeitsmatrix multiplizieren. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Beachten Sie, dass sich die Wahrscheinlichkeiten zu 1 addieren.</b> <br><br>  Die folgende Zustandsverteilung ergibt sich aus der Formel: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  Usw.  Am Ende erreichen Sie einen station√§ren Zustand, in dem sich der Zustand stabilisiert: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  F√ºr die oben beschriebene √úbergangswahrscheinlichkeitsmatrix Q ist die station√§re Verteilung s <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Sie k√∂nnen eine station√§re Verteilung mit dem folgenden Code erhalten: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Sie k√∂nnen auch von jedem anderen Zustand aus starten - erzielen Sie die gleiche station√§re Verteilung.  √Ñndern Sie den Ausgangszustand im Code, wenn Sie dies sicherstellen m√∂chten. <br><br>  Jetzt k√∂nnen wir die Frage beantworten, warum die station√§re Verteilung so wichtig ist. <br><br>  Die station√§re Verteilung ist wichtig, da damit die Wahrscheinlichkeit bestimmt werden kann, dass sich ein System zuf√§llig in einem bestimmten Zustand befindet. <br><br>  In unserem Beispiel k√∂nnen wir sagen, dass sich der Markt in 62,5% der F√§lle in einem ‚Äûbullischen‚Äú Zustand befindet, 31,25% in einem ‚Äûb√§rischen‚Äú Zustand und 6,25% in einer Stagnation. <br><br>  Intuitiv kann man dies als zuf√§lliges Wandern um die Kette sehen. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Zuf√§lliger Spaziergang <br><br>  Sie befinden sich an einem bestimmten Punkt und w√§hlen den n√§chsten Zustand aus, wobei Sie die Wahrscheinlichkeitsverteilung des n√§chsten Zustands unter Ber√ºcksichtigung des aktuellen Zustands beobachten.  Wir k√∂nnen einige Knoten h√§ufiger als andere besuchen, basierend auf den Wahrscheinlichkeiten dieser Knoten. <br><br>  So l√∂ste Google das Suchproblem zu Beginn des Internets.  Das Problem bestand darin, die Seiten nach ihrer Wichtigkeit zu sortieren.  Google hat das Problem mithilfe des Pagerank-Algorithmus gel√∂st.  Der Google Pagerank-Algorithmus sollte den Status als Seite und die Wahrscheinlichkeit einer Seite in einer station√§ren Verteilung als ihre relative Bedeutung betrachten. <br><br>  Nun wenden wir uns direkt der Betrachtung von MCMC-Methoden zu. <br><br><h2>  Was sind Monte-Carlo-Methoden mit Markov-Ketten (MCMC) </h2><br>  Lassen Sie mich eine Frage stellen, bevor ich beantworte, was MCMC ist.  Wir kennen die Beta-Distribution.  Wir kennen seine Wahrscheinlichkeitsdichtefunktion.  Aber k√∂nnen wir aus dieser Verteilung eine Stichprobe ziehen?  K√∂nnen Sie einen Weg finden, dies zu tun? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Denken Sie ... <br><br>  Mit MCMC k√∂nnen Sie aus einer beliebigen Wahrscheinlichkeitsverteilung ausw√§hlen.  Dies ist besonders wichtig, wenn Sie eine Auswahl aus der posterioren Verteilung treffen m√ºssen. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  Die Abbildung zeigt den Bayes-Satz. <br><br>  Beispielsweise m√ºssen Sie eine Probe aus einer posterioren Verteilung erstellen.  Aber ist es einfach, die hintere Komponente zusammen mit der Normalisierungskonstante (Evidenz) zu berechnen?  In den meisten F√§llen finden Sie sie in Form eines Produkts aus Wahrscheinlichkeit und a priori Wahrscheinlichkeit.  Die Berechnung der Normalisierungskonstante (p (D)) funktioniert jedoch nicht.  Warum?  Schauen wir uns das genauer an. <br><br>  Angenommen, H nimmt nur 3 Werte an: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  In diesem Fall ist p (D) leicht zu berechnen.  Was aber, wenn der Wert von H stetig ist?  W√§re es m√∂glich, dies so einfach zu berechnen, insbesondere wenn H unendliche Werte annehmen w√ºrde?  Dazu m√ºsste ein komplexes Integral gel√∂st werden. <br><br>  Wir wollen eine zuf√§llige Auswahl aus der posterioren Verteilung treffen, aber auch p (D) als Konstante betrachten. <br><br>  Wikipedia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schreibt</a> : <br><br>  Monte-Carlo-Methoden mit Markov-Ketten sind eine Klasse von Algorithmen zur Abtastung aus einer Wahrscheinlichkeitsverteilung, die auf dem Aufbau einer Markov-Kette basiert, die als station√§re Verteilung die gew√ºnschte Form hat.  Der Kettenzustand nach einer Reihe von Schritten wird dann als Auswahl aus der gew√ºnschten Verteilung verwendet.  Die Probenqualit√§t verbessert sich mit zunehmender Anzahl von Schritten. <br><br>  Schauen wir uns ein Beispiel an.  Angenommen, Sie ben√∂tigen ein Beispiel aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beta-Distribution</a> .  Seine Dichte: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  wobei C die Normalisierungskonstante ist.  Eigentlich ist dies eine Funktion von Œ± und Œ≤, aber ich m√∂chte zeigen, dass es f√ºr eine Stichprobe aus der Beta-Verteilung nicht ben√∂tigt wird, also nehmen wir es als Konstante. <br><br>  Das Beta-Verteilungsproblem ist wirklich schwierig, wenn nicht praktisch unl√∂sbar.  In der Realit√§t m√ºssen Sie m√∂glicherweise mit komplexeren Verteilungsfunktionen arbeiten, und manchmal kennen Sie die Normalisierungskonstanten nicht. <br><br>  MCMC-Methoden erleichtern das Leben, indem sie Algorithmen bereitstellen, mit denen eine Markov-Kette mit einer Beta-Verteilung als station√§re Verteilung erstellt werden kann, da wir aus einer gleichm√§√üigen Verteilung w√§hlen k√∂nnen (was relativ einfach ist). <br><br>  Wenn wir mit einem zuf√§lligen Zustand beginnen und auf der Grundlage eines Algorithmus mehrmals zum n√§chsten Zustand √ºbergehen, werden wir schlie√ülich eine Markov-Kette mit einer Beta-Verteilung als station√§re Verteilung erstellen.  Und die Zust√§nde, in denen wir uns seit langer Zeit befinden, k√∂nnen als Beispiel aus der Beta-Distribution verwendet werden. <br><br>  Einer dieser MCMC-Algorithmen ist der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Metropolis-Hastings-Algorithmus.</a> <br><br><h2>  Metropolis-Hastings-Algorithmus </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuition: </h3><br>  Was ist der Zweck? <br><br>  <i>Intuitiv m√∂chten wir ein St√ºck Oberfl√§che (unsere Markov-Kette) so entlang gehen, dass die Zeit, die wir an jedem Ort verbringen, proportional zur H√∂he der Oberfl√§che an diesem Ort ist (die gew√ºnschte Wahrscheinlichkeitsdichte, aus der wir eine Auswahl treffen m√∂chten).</i> <i><br><br></i>  <i>So m√∂chten wir beispielsweise doppelt so viel Zeit auf einem 100 Meter hohen H√ºgel verbringen wie auf einem benachbarten 50 Meter hohen H√ºgel.</i>  <i>Es ist gut, dass wir dies auch dann tun k√∂nnen, wenn wir die absoluten H√∂hen der Punkte auf der Oberfl√§che nicht kennen: Alles, was Sie wissen m√ºssen, sind die relativen H√∂hen.</i>  <i>Wenn beispielsweise die Spitze des H√ºgels A zweimal h√∂her ist als die Spitze des H√ºgels B, m√∂chten wir in A doppelt so viel Zeit verbringen wie auf B.</i> <i><br><br></i>  <i>Es gibt komplexere Schemata, um neue Orte und Regeln f√ºr ihre Annahme vorzuschlagen, aber die Hauptidee lautet wie folgt:</i> <i><br><br></i> <ol><li>  <i>W√§hlen Sie einen neuen "vorgeschlagenen" Ort.</i> </li><li>  <i>Finden Sie heraus, wie hoch oder niedriger dieser Ort im Vergleich zum aktuellen ist.</i> </li><li>  <i>An Ort und Stelle bleiben oder an einen neuen Ort ziehen, mit einer Wahrscheinlichkeit proportional zur H√∂he der Orte.</i> </li></ol> <i><br></i>  <i>Der Zweck des MCMC besteht darin, aus einer Wahrscheinlichkeitsverteilung auszuw√§hlen, ohne an irgendeinem Punkt seine genaue H√∂he kennen zu m√ºssen (C muss nicht bekannt sein).</i> <i><br></i>  <i>Wenn der Wanderprozess korrekt eingerichtet ist, k√∂nnen Sie sicherstellen, dass diese Proportionalit√§t (zwischen der aufgewendeten Zeit und der Verteilungsh√∂he) erreicht wird</i> . <br><br><h3>  Algorithmus: </h3><br>  Lassen Sie uns nun die Aufgabe formeller definieren und beschreiben.  Sei s = (s1, s2, ..., sM) die gew√ºnschte station√§re Verteilung.  Wir wollen eine Markov-Kette mit einer solchen station√§ren Verteilung erstellen.  Wir beginnen mit einer beliebigen Markov-Kette mit M-Zust√§nden mit der √úbergangsmatrix P, so dass pij die Wahrscheinlichkeit des √úbergangs vom Zustand i nach j darstellt. <br><br>  Intuitiv wissen wir, wie man die Markov-Kette durchstreift, aber die Markov-Kette hat nicht die erforderliche station√§re Verteilung.  Diese Kette hat eine station√§re Verteilung (die wir nicht brauchen).  Unser Ziel ist es, die Art und Weise, wie wir um die Markov-Kette herumwandern, so zu √§ndern, dass die Kette die gew√ºnschte station√§re Verteilung hat. <br><br>  Um dies zu tun: <br><br><ol><li>  Beginnen Sie mit einem zuf√§lligen Ausgangszustand i. </li><li>  W√§hlen Sie zuf√§llig einen neuen angenommenen Zustand aus, indem Sie die √úbergangswahrscheinlichkeiten in der i-ten Zeile der √úbergangsmatrix P betrachten. </li><li>  Berechnen Sie ein Ma√ü namens Entscheidungswahrscheinlichkeit, das definiert ist als: aij = min (sj.pji / si.pij, 1). </li><li>  Wirf nun eine M√ºnze, die mit der Wahrscheinlichkeit aij auf der Oberfl√§che des Adlers landet.  Wenn ein Adler f√§llt, nehmen Sie das Angebot an, gehen Sie zum n√§chsten Zustand √ºber, andernfalls lehnen Sie das Angebot ab, dh bleiben Sie im aktuellen Zustand. <br></li><li>  Wiederholen Sie viele Male. <br></li></ol><br>  Nach einer gro√üen Anzahl von Tests konvergiert diese Kette und hat eine station√§re Verteilung s.  Dann k√∂nnen wir Kettenzust√§nde als Stichprobe aus jeder Verteilung verwenden. <br><br>  Wenn Sie dies tun, um die Beta-Verteilung abzutasten, m√ºssen Sie nur die Wahrscheinlichkeitsdichte verwenden, um nach der Wahrscheinlichkeit zu suchen, eine Entscheidung zu treffen.  Teilen Sie dazu sj durch si (dh die Normalisierungskonstante C wird aufgehoben). <br><br><h3>  Beta-Auswahl </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Nun wenden wir uns dem Problem der Stichprobe aus der Beta-Distribution zu. <br><br>  Eine Beta-Verteilung ist eine kontinuierliche Verteilung auf [0,1] und kann auf [0,1] unendliche Werte haben.  Angenommen, eine beliebige Markov-Kette P mit unendlichen Zust√§nden auf [0,1] hat eine √úbergangsmatrix P, so dass pij = pji = alle Elemente in der Matrix. <br><br>  Wir brauchen keine Matrix P, wie wir sp√§ter sehen werden, aber ich m√∂chte, dass die Beschreibung des Problems dem von uns vorgeschlagenen Algorithmus so nahe wie m√∂glich kommt. <br><br><ul><li>  Beginnen Sie mit einem zuf√§lligen Anfangszustand i, der aus einer Gleichverteilung auf (0,1) erhalten wird. </li><li>  W√§hlen Sie zuf√§llig einen neuen angenommenen Zustand aus, indem Sie die √úbergangswahrscheinlichkeiten in der i-ten Zeile der √úbergangsmatrix P betrachten. Angenommen, wir w√§hlen einen anderen Zustand Unif (0,1) als den angenommenen Zustand j. </li><li>  Berechnen Sie das Ma√ü, das als Entscheidungswahrscheinlichkeit bezeichnet wird: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  Was vereinfacht zu: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Da pji = pij und wo <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Wirf jetzt eine M√ºnze.  Mit der Wahrscheinlichkeit aij wird ein Adler fallen.  Wenn ein Adler f√§llt, sollten Sie das Angebot annehmen, dh in den n√§chsten Zustand wechseln.  Andernfalls lohnt es sich, das Angebot abzulehnen, dh im selben Zustand zu bleiben. </li><li>  Wiederholen Sie den Test viele Male. </li></ul><br><h3>  Code: </h3><br>  Es ist Zeit, von der Theorie zur Praxis √ºberzugehen.  Wir werden unser Beta-Beispiel in Python schreiben. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Vergleichen Sie die Ergebnisse mit der tats√§chlichen Beta-Verteilung. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Wie Sie sehen k√∂nnen, sind die Werte der Beta-Distribution sehr √§hnlich.  Somit hat das MCMC-Netzwerk einen station√§ren Zustand erreicht <br><br>  Im obigen Code haben wir einen Beta-Sampler erstellt, aber das gleiche Konzept gilt f√ºr jede andere Distribution, aus der wir eine Auswahl treffen m√∂chten. <br><br><h2>  Schlussfolgerungen </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  Es war ein gro√üartiger Beitrag.  Herzlichen Gl√ºckwunsch, wenn Sie es bis zum Ende gelesen haben. <br><br>  MCMC-Methoden k√∂nnen im Wesentlichen komplex sein, bieten uns jedoch gro√üe Flexibilit√§t.  Sie k√∂nnen aus einer beliebigen Verteilungsfunktion ausw√§hlen, indem Sie die Auswahl √ºber die MCMC vornehmen.  Typischerweise werden diese Methoden verwendet, um Proben aus posterioren Verteilungen zu entnehmen. <br><br>  Sie k√∂nnen MCMC auch verwenden, um Probleme mit einem gro√üen Zustandsraum zu l√∂sen.  Zum Beispiel bei einem Rucksackproblem oder zur Entschl√ºsselung.  Ich werde versuchen, Ihnen im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">n√§chsten</a> Beitrag weitere interessante Beispiele zu liefern.  Bleib dran. <br><br><h2>  Von den Redakteuren </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-</a> Kurs <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zum Arbeiten mit Daten</a> </li><li>  Online-Kurs f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">maschinelles Lernen</a> </li><li>  Online-Kurs " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GROSSE DATEN von Grund auf neu</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460497/">https://habr.com/ru/post/de460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460485/index.html">Wie ein Online-Turnier das "Ziel n√§chste Woche" entmutigen kann</a></li>
<li><a href="../de460489/index.html">TOP 11 Fehler bei der Entwicklung von BCP</a></li>
<li><a href="../de460491/index.html">Arduino Temperatur- und Feuchtigkeitssensor mit Senden und Plotten (Teil 1)</a></li>
<li><a href="../de460493/index.html">"Killer Apps" f√ºr PC aus den 80ern: VisiCalc und WordStar</a></li>
<li><a href="../de460495/index.html">Container-to-Pipeline: CRI-O ist jetzt die Standardeinstellung in OpenShift Container Platform 4</a></li>
<li><a href="../de460499/index.html">Drei Gewinner des Dijkstra-Preises: Wie verliefen Hydra 2019 und SPTDC 2019?</a></li>
<li><a href="../de460501/index.html">Beispiel f√ºr die Implementierung einer kontinuierlichen Integration mit BuildBot</a></li>
<li><a href="../de460503/index.html">Drahtlose Konfiguration des Raspberry PI 3 B +</a></li>
<li><a href="../de460505/index.html">Verf√ºhren Sie drei Kreuze oder warum Projekte so schwer p√ºnktlich zu beenden sind</a></li>
<li><a href="../de460507/index.html">XEN und die Zukunft der Automobilindustrie: Wie ein Open-Source-Hypervisor zu einem Konkurrenten f√ºr kommerzielle Automobill√∂sungen wird</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>