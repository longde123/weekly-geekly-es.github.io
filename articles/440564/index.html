<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíáüèº üâë üòç Red neuronal GPT-2 de OpenAI. Inicio r√°pido üëß üë©üèæ‚Äçüé§ üë©‚Äçüíº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Apenas se mostraron las noticias sobre la red neuronal BERT de Google, mostrando resultados de √∫ltima generaci√≥n en una serie de tareas de aprendizaje...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Red neuronal GPT-2 de OpenAI. Inicio r√°pido</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440564/"><p><img src="https://habrastorage.org/getpro/habr/post_images/1cf/d63/ae6/1cfd63ae6b68d59325ef90cc4ea93f35.png" alt="imagen"></p><br><p>  Apenas se mostraron las noticias sobre la red neuronal BERT de Google, mostrando resultados de √∫ltima generaci√≥n en una serie de tareas de aprendizaje autom√°tico conversacionales (PNL), cuando OpenAI lanz√≥ un nuevo desarrollo: GPT-2.  Esta red neuronal con un n√∫mero r√©cord de par√°metros en este momento (1,5 mil millones, frente a los 100-300 millones com√∫nmente utilizados en tales casos) fue capaz de generar p√°ginas enteras de texto conectado. </p><br><p>  Es tan bueno generar que OpenAI se neg√≥ a publicar la versi√≥n completa, temiendo que usar√≠an esta red neuronal para crear noticias falsas, comentarios y rese√±as indistinguibles de las reales. </p><br><p>  Sin embargo, en OpenAI, se comparti√≥ una versi√≥n reducida de la red neuronal GPT-2 con 117 millones de par√°metros.  Lo lanzaremos a trav√©s del servicio Google Colab y experimentaremos con √©l. </p><a name="habracut"></a><br><h1 id="nemnogo-predystorii">  Un poco de historia </h1><br><p>  Para aquellos que no han seguido el desarrollo del progreso en el procesamiento del habla natural (PNL). </p><br><p>  En el verano de 2018, OpenAI entren√≥ previamente en un gran volumen de texto una red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GPT</a> construida sobre la arquitectura Transformer.  Result√≥ que si reemplaza un par de las √∫ltimas capas y lo vuelve a entrenar para una tarea espec√≠fica (este enfoque se llama Ajuste fino y se usa ampliamente en el aprendizaje autom√°tico), rompe los registros anteriores inmediatamente en una amplia gama de tareas de conversaci√≥n. </p><br><p>  Basado en este desarrollo, a finales de 2018, Google cre√≥ su propia red neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">BERT</a> .  Mejoraron seriamente el resultado al hacer que la red neuronal fuera bidireccional, a diferencia del GPT. </p><br><p>  Sin querer darse por vencido, en febrero de 2019, OpenAI aument√≥ inmediatamente su GPT en 10 veces y lo entren√≥ en una cantidad a√∫n mayor de texto, en 8 millones de p√°ginas web (un total de 40 GB de texto).  La red <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GPT-2</a> resultante es actualmente la red neuronal m√°s grande, con un n√∫mero de par√°metros sin precedentes de 1.500 millones (BERT ten√≠a 340 millones en el modelo m√°s grande y 110 millones en el BERT est√°ndar). </p><br><p>  Como resultado, GPT-2 pudo generar p√°ginas enteras de texto coherente.  Con referencias repetidas a los nombres de los personajes en el curso de la narraci√≥n, citas, referencias a eventos relacionados, etc.  No dar√© ejemplos aqu√≠, pero referir√© a aquellos que lo deseen al art√≠culo original en el blog de OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Modelos de mejores lenguajes y sus implicaciones</a> o los enlaces al final del art√≠culo. </p><br><p>  Generar un texto coherente de esta calidad es impresionante en s√≠ mismo, pero lo m√°s interesante es diferente.  GPT-2 sin ning√∫n entrenamiento adicional inmediatamente mostr√≥ resultados cercanos al estado de la t√©cnica en una serie de tareas de conversaci√≥n.  Repito, quien se perdi√≥ la importancia del momento, ¬°sin ning√∫n entrenamiento adicional para una tarea espec√≠fica! </p><br><p>  ¬øC√≥mo lograron esto?  Simplemente haciendo las redes neuronales las preguntas correctas. </p><br><h1 id="arhitektura-gpt-2">  Arquitectura GPT-2 </h1><br><p>  GPT-2 est√° entrenado para predecir la siguiente palabra en una oraci√≥n.  Este es un enfoque cl√°sico para generar texto.  Al principio, las redes de recurrencia (RNN), en particular, LSTM, ten√≠an la primac√≠a en esta √°rea.  Pero despu√©s de la invenci√≥n de la arquitectura <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformer</a> en el verano de 2017, gradualmente comenz√≥ a prevalecer en las tareas de conversaci√≥n.  Aunque el Transformer original tiene un problema para almacenar secuencias largas (los LSTM recuerdan las m√°s largas), la velocidad del entrenamiento y la profundidad de la red lo compensaron con creces.  Por cierto, ya han aparecido varias modificaciones del transformador, con la introducci√≥n de recurrencia ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformadores universales</a> ), una modificaci√≥n para secuencias m√°s largas ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformer-XL</a> ) y otras, pero hasta ahora solo se usa un Transformador original ligeramente sintonizado en Google y OpenAI. </p><br><p>  Recuerdo que BERT de Google aprendi√≥ un poco diferente: predecir no la siguiente palabra en una oraci√≥n, sino las palabras perdidas (enmascaradas) en una oraci√≥n.  Y tambi√©n para determinar si dos oraciones consecutivas son una continuaci√≥n l√≥gica entre s√≠, o si de ninguna manera est√°n conectadas por el significado.  Esto permiti√≥ que BERT fuera un modelo de lenguaje que comprende el significado de las palabras dependiendo de su entorno (contexto).  Lo que determin√≥ el √©xito de BERT en las tareas de NPL.  Pero solo despu√©s del reentrenamiento (ajuste fino) para una tarea espec√≠fica.  Simplemente predecir palabras en el modelo base no funciona muy bien en √©l.  Puede jugar con BERT en su navegador (a trav√©s de Google Colab): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://habr.com/en/post/436878</a> . </p><br><p>  GPT-2 no es necesario volver a entrenar.  Este no es solo un modelo de lenguaje como BERT, es un generador de texto.  Simplemente dale el comienzo de la frase, y luego ella complementar√° su palabra por palabra. </p><br><p>  Un detalle interesante: la investigaci√≥n de OpenAI ha demostrado que las matrices de textos de Wikipedia y libros literarios (que BERT estudi√≥ en particular) tienen un estilo sesgado.  Por lo tanto, entrenados solo en ellas las redes neuronales no generan texto muy bien.  Para diversificar los datos y estilos de entrada, OpenAI utiliz√≥ GPT-2 para capacitaci√≥n en p√°ginas web regulares recopiladas de 8 millones de sitios (un total de 40 GB de texto).  Y para descartar sitios de publicidad y spammers, se incluyeron en los sitios de muestra cuyos enlaces en el reddit tienen una buena calificaci√≥n.  Es decir, los sitios en los que viven usuarios que contienen informaci√≥n √∫til. </p><br><h1 id="pravilnyy-vopros-soderzhit-polovinu-otveta">  La pregunta correcta contiene la mitad de la respuesta. </h1><br><p>  Entonces, GPT-2, gracias a su tama√±o sin precedentes, pudo generar p√°ginas de texto coherente.  Pero lo m√°s sorprendente es que al hacerle la pregunta correcta (es decir, el comienzo correcto de una frase), ¬°pudo responder varias preguntas!  Solo porque la continuaci√≥n de tal comienzo es lo m√°s natural. </p><br><p>  Por ejemplo, para obtener una respuesta a la pregunta "¬øQu√© es la Tierra?", Puede aplicar a la entrada de esta red neuronal el comienzo de la frase: "La Tierra es ...".  Y ella completar√° esta frase hasta el final.  Porque la respuesta ser√° una continuaci√≥n natural de este comienzo. </p><br><p>  Adem√°s, al formar el comienzo de la frase de la manera correcta, puede obtener explicaciones para diferentes audiencias objetivo, teniendo en cuenta su inteligencia, edad y educaci√≥n.  Imagine frases continuas: "Yo, como cient√≠fico, creo que la Tierra es ...".  O: "Yo, como arado de tierras, afirmo que la Tierra es ...".  O: "Yo, como maestra en un jard√≠n de infantes, ahora les explicar√©, ni√±os, que la Tierra es ...". </p><br><p>  Como puede ver, al formar las preguntas correctas (el comienzo correcto de la frase), puede obtener respuestas de niveles completamente diferentes y detalles diferentes.  En cierto modo, sucede algo similar en las personas.  El m√©dico debe explicarle al paciente el curso de la enfermedad para que comprenda.  A nivel del paciente.  Si le preguntas a un ni√±o de cinco a√±os por qu√© hizo esto, entonces no puede responder de inmediato (que, naturalmente, los ni√±os viven con sentimientos y emociones).  Pero para dar la respuesta que se espera de √©l, el ni√±o comienza a inventarla: a generar texto.  Basado en el hecho de que la respuesta se adapta al padre y que al menos de alguna manera corresponde a lo que sucedi√≥.  Al principio, como muchos padres saben, estas ser√°n respuestas rid√≠culas.  Pero al alentar y castigar ("cu√©ntame m√°s", "no inventes excusas"), el ni√±o aprender√° a dar respuestas detalladas y completas. </p><br><p>  Este desarrollo de OpenAI y la capacidad de la red GPT-2 de proporcionar respuestas a tareas de conversaci√≥n sin capacitaci√≥n adicional especial para una tarea espec√≠fica, abren dos preguntas interesantes: </p><br><p>  1) ¬øSe puede lograr la interpretabilidad de las redes neuronales mediante un generador de texto tan elemental y el comienzo correcto de una frase?  Donde la respuesta ser√° una extensi√≥n natural.  Supongamos, por ejemplo, que una red neuronal no indica sellos en una fotograf√≠a mediante los n√∫meros de coordenadas x-y, sino que explica su posici√≥n en texto plano.  Luego, en el curso de la aclaraci√≥n, haci√©ndole la pregunta correcta, por ejemplo: "Llegu√© a esta conclusi√≥n porque ...", en teor√≠a, puede obtener una explicaci√≥n de c√≥mo encontr√≥ al gato en la foto.  Y esta explicaci√≥n en el caso extremo no puede ser peor que la humana.  Lo que resuelve el problema global de la interpretabilidad de las redes neuronales. </p><br><p>  2) ¬øPuede una red neuronal pre-entrenada en grandes vol√∫menes de texto ser universal, tener sentido com√∫n y no requerir entrenamiento adicional para tareas espec√≠ficas?  Aqu√≠, se entiende que cuando se trata de imitar el habla humana (respuestas humanas a preguntas), la red neuronal inevitablemente debe aprender el sentido com√∫n para dar respuestas muy similares a las humanas.  Dar respuestas ficticias monosil√°bicas, en general, no es t√≠pico de las personas.  En su mayor parte, las personas dan respuestas adecuadas detalladas, lo que significa que la red debe aprender a hacer lo mismo. </p><br><p>  Ambas preguntas permanecen abiertas, pero definitivamente se ha dado el primer paso para su aprobaci√≥n. </p><br><h1 id="a-tochnee">  O m√°s bien? </h1><br><p>  Si est√° de pie ahora, es mejor sentarse.  Porque as√≠ es como OpenAI usando la red neuronal GPT-2 obtuvo sus resultados en tareas de conversaci√≥n para diferentes dominios: </p><br><p>  <strong>Respuestas a preguntas sobre el texto.</strong> </p><br><p>  Bueno, eso es f√°cil.  O aliment√≥ a la red unos pocos p√°rrafos con una descripci√≥n que inclu√≠a en alg√∫n lugar en el medio, por ejemplo, "la manzana est√° en la mesa", y al final se le atribuy√≥: "la manzana est√° en ..." y la red se agreg√≥ a la "mesa".  Porque es capaz de recordar el contexto de varios p√°rrafos. </p><br><p>  O aliment√≥ la red como una frase inicial con algunos ejemplos del tipo "Pregunta: alguna pregunta, Respuesta: alguna respuesta", y al final despu√©s de la pregunta real, agregaron: "Respuesta:".  ¬°Y la red neuronal agreg√≥ la respuesta!  Dado que revel√≥ la estructura del documento en la Pregunta-Respuesta anterior.  Esto es asombroso. </p><br><p>  <strong>Versi√≥n corta (resumen) del texto</strong> </p><br><p>  La entrada es un texto largo de varios p√°rrafos o incluso p√°ginas, y la red neuronal debe escribir un contenido breve.  ¬øC√≥mo obtuvo este comportamiento de GPT-2?  Justo despu√©s del texto agregaron "TL; DR".  ¬°Y eso es todo!  ¬°Esto result√≥ ser suficiente para que el GPT-2 agregue un resumen del art√≠culo despu√©s de estos personajes!  Debido a que tales s√≠mbolos en Internet a menudo denotan el resumen de la publicaci√≥n. </p><br><p>  <strong>Traducci√≥n de textos</strong> </p><br><p>  La entrada GPT-2 recibi√≥ el texto en la forma: "hola = hola, perro = perro, viento = viento, gato = ...".  Y la red neuronal agreg√≥ la traducci√≥n de la √∫ltima palabra: "gato" (en el original en franc√©s).  Porque revel√≥ la estructura del documento y simplemente lo complement√≥ con la continuaci√≥n m√°s l√≥gica.  Si su mand√≠bula a√∫n no se ha ca√≠do de todo esto, entonces tengo dos noticias para usted, y ambas son malas =). </p><br><h1 id="zapusk-gpt-2-cherez-google-colab">  Lanzamiento de GPT-2 a trav√©s de Google Colab </h1><br><p>  Desafortunadamente, la versi√≥n completa de GPT-2 en OpenAI fue rechazada para ser compartida.  Motivando esto por el hecho de que usar esta red neuronal ser√° demasiado f√°cil generar noticias falsas y cr√≠ticas en las tiendas.  A juzgar por su declaraci√≥n, la discusi√≥n sobre la conveniencia de dise√±ar este modelo continuar√° durante los pr√≥ximos 6 meses, luego de OpenAI decidir√°n si lo suben o no.  Sin embargo, para una organizaci√≥n grande no es dif√≠cil repetir el modelo (parece que lo entrenaron para 256 TPU durante varios d√≠as y, seg√∫n estimaciones preliminares, les cost√≥ alrededor de $ 45 mil) </p><br><p>  Sin embargo, publicaron una versi√≥n reducida de GPT-2 con 117 millones de par√°metros (en lugar de 1.5 mil millones, como en el modelo completo): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/openai/gpt-2</a> .  Intentemos ejecutarlo y jugar con este modelo. </p><br><p>  Actualizaci√≥n 9 de noviembre de 2019: finalmente, se ha presentado toda la l√≠nea de modelos, incluidos 1.500 millones, y se han actualizado los archivos y las instrucciones para el lanzamiento. </p><br><p>  La forma m√°s f√°cil de hacerlo es a trav√©s de Google Colab: </p><br><ol><li>  Abrir el enlace </li></ol><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">http://colab.research.google.com/github/blade1780/bert/blob/master/Gpt-2.ipynb</a> </p><br><ol><li>  En el men√∫ <strong>Tiempo de ejecuci√≥n</strong> , seleccione <strong>Ejecutar todo</strong> , de modo que por primera vez se inicien todas las celdas, el modelo se descargue y se conecten las bibliotecas necesarias.  Acepte restablecer todo el tiempo de ejecuci√≥n si es necesario.  Ingrese el texto despu√©s de la aparici√≥n de "Solicitud de modelo &gt;&gt;&gt;" y presione Entrar. </li></ol><br><p>  Presta atenci√≥n a la l√≠nea desde el principio: </p><br><p>  nombre_modelo = '117M' </p><br><p>  Aqu√≠ puede especificar el tama√±o del modelo GPT-2 a utilizar.  Los siguientes modelos est√°n disponibles (sujetos a actualizaci√≥n): </p><br><p>  117 millones <br>  124 millones <br>  355 millones <br>  Los 774M <br>  El 1558M </p><br><p>  Aqu√≠, 117M es el modelo m√°s peque√±o que era el √∫nico disponible en el momento de escribir este art√≠culo.  OpenAI luego present√≥ modelos cada vez mayores, hasta el 5 de noviembre de 2019, present√≥ el m√°ximo de 1558 millones (con 1.500 millones de par√°metros). </p><br><div class="spoiler">  <b class="spoiler_title">Si algo sali√≥ mal ...</b> <div class="spoiler_text"><p>  Aseg√∫rese de que GPU y Python 3 est√©n seleccionados en el men√∫ Tiempo de ejecuci√≥n -&gt; Cambiar tipo de tiempo de ejecuci√≥n </p><br><p>  Si el bot√≥n de conexi√≥n no est√° activo, haga clic para conectarse. </p></div></div><br><p>  O cree todo el c√≥digo manualmente: </p><br><ol><li>  Vaya a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://colab.research.google.com</a> </li><li>  Presione el bot√≥n azul del CUADERNO NEW PYTHON 3 </li><li>  Desde el men√∫ Tiempo de ejecuci√≥n -&gt; Cambiar tipo de tiempo de ejecuci√≥n, seleccione Python 3 y la GPU (este √∫ltimo para ejecutar la red neuronal en la GPU) </li><li>  En la primera celda, escriba: </li></ol><br><pre><code class="python hljs">model_name = <span class="hljs-string"><span class="hljs-string">'117M'</span></span> !git clone https://github.com/openai/gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> %cd gpt<span class="hljs-number"><span class="hljs-number">-2</span></span> !pip3 install -r requirements.txt !python3 download_model.py $model_name</code> </pre> <br><p>  En lugar de 117M (el m√°s peque√±o), puede especificar cualquier modelo intermedio o m√°s grande: 1558M. </p><br><p>  Y haga clic en el √≠cono negro Play a la izquierda de la celda.  Esto descargar√° la red neuronal GPT-2 seleccionada e instalar√° las dependencias necesarias. </p><br><p>  En la segunda celda (puede agregarlo a trav√©s del men√∫ Insertar -&gt; Celda de c√≥digo o colocando el mouse debajo del centro de la celda actual, aparecer√°n los botones de agregar): </p><br><pre> <code class="python hljs">!python3 src/interactive_conditional_samples.py --model_name=$model_name</code> </pre> <br><p>  Esto iniciar√° el modo interactivo.  Espere hasta que la red neuronal se inicie y aparezca una ventana para ingresar texto con la inscripci√≥n "" Solicitud de modelo &gt;&gt;&gt; ". Ingrese el comienzo de la frase y presione Entrar. Despu√©s de un momento, el texto generado aparece debajo del encabezado MUESTRA. </p><br><p>  Tambi√©n puede iniciar el modo de generar texto completamente al azar.  El texto se generar√° infinitamente en peque√±as piezas de SAMPLE 1, SAMPLE 2, etc., hasta que haga clic en el bot√≥n Detener en la celda.  Para hacer esto, cree una nueva celda con el c√≥digo: </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name | tee samples.txt</code> </pre> <br><p>  El resultado se guardar√° en el archivo samples.txt.  Se puede descargar con los siguientes comandos (cree una nueva celda nuevamente y ejec√∫tela despu√©s de generar el texto): </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">'samples.txt'</span></span>)</code> </pre> <br><p>  Puede cambiar los par√°metros para generar texto (coeficiente de aleatoriedad, etc., consulte la descripci√≥n en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">trabajo original</a> ): </p><br><pre> <code class="python hljs">!python3 src/generate_unconditional_samples.py --model_name=$model_name --top_k <span class="hljs-number"><span class="hljs-number">40</span></span> --temperature <span class="hljs-number"><span class="hljs-number">0.7</span></span> | tee samples.txt</code> </pre> <br><p>  Dado que el 117M es un modelo <strong>muy</strong> reducido, no espere milagros (actualizaci√≥n: en el momento de escribir este art√≠culo, solo estaba disponible. Ahora todo est√° disponible, incluido el 1558M m√°s grande original, ver arriba).  La mayor√≠a de las muestras generadas ser√°n absurdas.  Pero tambi√©n hay secciones significativas.  El texto debe estar en ingl√©s, mientras que en otros idiomas GPT-2 a√∫n no puede funcionar. </p><br><h2 id="primery-generiruemogo-teksta">  Ejemplos de texto generado. </h2><br><p>  Muestras del texto generado por el modelo <strong>completo</strong> : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://blog.openai.com/better-language-models/#sample1</a> (en la parte superior de la barra para 8 historias). </p><br><p>  Tambi√©n hay un enorme archivo de texto de 2.4 Mb con muestras generadas al azar: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-samples.txt</a> </p><br><p>  Y uno m√°s, 2.27 MB, con otras configuraciones de aleatoriedad: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://raw.githubusercontent.com/openai/gpt-2/master/gpt2-topk40-samples.txt</a> </p><br><h1 id="ssylki">  Referencias </h1><br><ul><li>  Art√≠culo original del blog de OpenAI: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">mejores modelos de lenguaje y sus implicaciones</a> </li><li>  Github con todas las versiones pre-entrenadas de GPT-2: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">https://github.com/openai/gpt-2</a> </li><li>  Discusi√≥n sobre las <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">principales noticias de</a> reddit </li><li>  Discusi√≥n sobre reddit que se niega a publicar el modelo completo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">es hora de que OpenAI cambie el nombre de CloseAI</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cuaderno Google Colab para ejecutar GPT-2 (todos los modelos) en un navegador</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/440564/">https://habr.com/ru/post/440564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../440554/index.html">BEM conveniente</a></li>
<li><a href="../440556/index.html">Aprendiendo el dise√±o de diagramas de relaci√≥n de entidad</a></li>
<li><a href="../440558/index.html">Tecnolog√≠a que acercar√° las redes cu√°nticas.</a></li>
<li><a href="../440560/index.html">Alexander Belokrylov y Dmitry Chuyko sobre Liberica JDK en jug.msk.ru</a></li>
<li><a href="../440562/index.html">Windows Phone - TODO, es una o otra vez</a></li>
<li><a href="../440566/index.html">Acelerar sin obst√°culos o conocer SIMD</a></li>
<li><a href="../440568/index.html">Estamos escribiendo una aplicaci√≥n de aprendizaje en Go y Javascript para evaluar el rendimiento real de las acciones. Parte 2 - Prueba del backend</a></li>
<li><a href="../440570/index.html">Mapas de sombras reflectantes: Parte 2 - Implementaci√≥n</a></li>
<li><a href="../440574/index.html">Russian AI Cup 2018, historia 9 lugares</a></li>
<li><a href="../440576/index.html">Cambios importantes en CTE en PostgreSQL 12</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>