<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§¶üèæ üé® üëêüèΩ Wo eine Person Formen sieht, sieht KI Texturen üìõ üë∂üèΩ üë©‚Äçüëß‚Äçüë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√úberraschenderweise k√∂nnen Forscher mit tiefgreifenden Computer-Vision-Algorithmen Bilder h√§ufig nicht klassifizieren, da sie sich haupts√§chlich auf T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wo eine Person Formen sieht, sieht KI Texturen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462951/">  √úberraschenderweise k√∂nnen Forscher mit tiefgreifenden Computer-Vision-Algorithmen Bilder h√§ufig nicht klassifizieren, da sie sich haupts√§chlich auf Texturen und nicht auf Formen konzentrieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/444/41f/95344441f850333698aa94694c254d32.jpg"><br><br>  Wenn Sie sich ein Foto einer Katze ansehen, k√∂nnen Sie dieses Tier mit hoher Wahrscheinlichkeit erkennen, unabh√§ngig davon, ob es rot oder gestreift ist - oder selbst wenn das Foto schwarzwei√ü, fleckig, ramponiert oder angelaufen ist.  Sie werden wahrscheinlich eine Katze bemerken k√∂nnen, wenn sie sich hinter einem Kissen zusammenrollt oder auf einen Tisch springt, der nur eine verschwommene Form darstellt.  Sie haben nat√ºrlich gelernt, Katzen in fast jeder Situation zu erkennen.  Bildverarbeitungssysteme, die auf tiefen neuronalen Netzen basieren, k√∂nnen, obwohl sie Menschen manchmal unter festgelegten Bedingungen mit Katzenerkennungsaufgaben versorgen k√∂nnen, mit Bildern verwechselt werden, die sich zumindest geringf√ºgig von dem unterscheiden, was sie wissen, oder Rauschen oder zu viel enthalten starkes Korn. <br><a name="habracut"></a><br>  Und jetzt haben deutsche Forscher einen unerwarteten Grund daf√ºr entdeckt: Wenn Menschen auf die Formen der abgebildeten Objekte achten, h√§ngt Computer Vision mit tiefem Lernen an den Texturen von Objekten. <br><br>  Diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Entdeckung</a> , die im Mai auf einer internationalen Konferenz √ºber lernende Repr√§sentationen vorgestellt wurde, unterstreicht den scharfen Kontrast zwischen dem ‚ÄûDenken‚Äú von Menschen und Maschinen und zeigt, wie falsch wir sein k√∂nnen, wenn wir verstehen, wie KI funktioniert.  Und es kann uns auch sagen, warum unsere Vision als Ergebnis der Evolution so wurde. <br><br><h2>  Elfenbeinkatzen und beobachten Flugzeuge </h2><br>  Deep-Learning-Algorithmen steuern Tausende von Bildern durch ein neuronales Netzwerk, das entweder eine Katze hat oder nicht.  Das System sucht in diesen Daten nach Mustern, mit denen es das Bild, auf das es zuvor noch nicht gesto√üen ist, am besten markiert.  Die Netzwerkarchitektur √§hnelt ein wenig der Struktur des menschlichen visuellen Systems, da sie √ºber verbundene Schichten verf√ºgt, mit denen immer mehr abstrakte Merkmale aus dem Bild extrahiert werden k√∂nnen.  Der Prozess des Aufbaus eines Assoziationssystems, das zur richtigen Antwort f√ºhrt, ist jedoch eine Black Box, die die Menschen erst nachtr√§glich interpretieren k√∂nnen.  "Wir haben versucht zu verstehen, was zum Erfolg dieser tiefgreifenden Computer-Vision-Algorithmen f√ºhrt und warum sie so anf√§llig sind", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Thomas Ditterich</a> , ein IT-Spezialist an der University of Oregon, der nicht an dieser Studie beteiligt ist. <br><br>  Einige Forscher bevorzugen es zu untersuchen, was passiert, wenn sie versuchen, das Netzwerk durch geringf√ºgige √Ñnderung des Bildes auszutricksen.  Sie stellten fest, dass selbst kleine √Ñnderungen dazu f√ºhren k√∂nnen, dass das System das Bild falsch markiert - und dass gro√üe √Ñnderungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">m√∂glicherweise nicht dazu f√ºhren, dass sich</a> die Beschriftung √§ndert.  In der Zwischenzeit verfolgen andere Experten √Ñnderungen im System, um zu analysieren, wie einzelne Neuronen auf das Bild reagieren, und erstellen einen ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aktivierungsatlas</a> ‚Äú, der auf den vom System erlernten Attributen basiert. <br><br>  Eine Gruppe von Wissenschaftlern aus den Labors des Computational Neurobiologist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Matias Betge</a> und des Psychophysiologen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Felix Wichmann</a> von der Universit√§t T√ºbingen in Deutschland w√§hlten jedoch einen qualitativen Ansatz.  Letztes Jahr berichtete das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Team,</a> dass das Netzwerk beim Training von Bildern, die durch Rauschen einer bestimmten Art ver√§ndert wurden, Bilder besser erkannte als Menschen, die versuchten, dieselben verrauschten Bilder zu erkennen.  Dieselben Bilder, die leicht unterschiedlich modifiziert wurden, verwirrten das Netzwerk jedoch v√∂llig, obwohl die neue Verzerrung f√ºr die Menschen fast genauso aussah wie die alte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/418/8c4/c184188c4f088a155c652e51562c42f6.jpg" width="60%"><br>  <i>Robert Geyros, Doktorand in Computational Neurobiology an der Universit√§t T√ºbingen</i> <br><br>  Um dieses Ergebnis zu erkl√§ren, fragten sich die Forscher, welche Bildqualit√§t sich selbst mit etwas Rauschen am meisten √§ndert.  Die offensichtliche Wahl ist Textur.  "Die Form eines Objekts bleibt mehr oder weniger unversehrt, wenn Sie lange Zeit viel L√§rm hinzuf√ºgen", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Robert Geyros</a> , ein Doktorand in den Labors von Betge und Wichmann, dem Hauptautor der Studie.  "Die lokale Bildstruktur wird jedoch sehr schnell verzerrt, wenn ein geringes Rauschen hinzugef√ºgt wird."  Sie haben sich also eine schwierige Methode ausgedacht, um zu testen, wie die visuellen Systeme von Maschinen und Menschen Bilder verarbeiten. <br><br>  Geyros, Betge und ihre Kollegen haben Bilder mit zwei widerspr√ºchlichen Merkmalen erstellt, die die Form eines Objekts und die Textur eines anderen annehmen: zum Beispiel eine Katzensilhouette in grauer Elefantenhautstruktur oder ein B√§r aus Aluminiumdosen oder eine ebene Silhouette mit √úberlappung einander mit Bildern von Zifferbl√§ttern.  Fast jedes Mal beschrifteten Menschen Hunderte solcher Bilder anhand ihrer Formen - Katze, B√§r, Flugzeug - wie beabsichtigt.  Vier verschiedene Klassifizierungsalgorithmen neigten sich jedoch in die entgegengesetzte Richtung und verteilten Etiketten, die die Texturen von Objekten widerspiegelten: Elefanten, Dosen, Uhren. <br><br>  "Dies √§ndert unser Verst√§ndnis davon, wie tiefe neuronale Netze mit direkter Verteilung - ohne zus√§tzliche Einstellungen nach dem √ºblichen Lernprozess - Bilder erkennen", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nikolaus Kriegescorte</a> , ein Computational Neuroscientist an der Columbia University, der nicht an der Studie beteiligt war. <br><br>  Auf den ersten Blick mag die Bevorzugung von KI-Texturen gegen√ºber Formen seltsam erscheinen, aber es macht Sinn.  "Textur ist eine hochaufl√∂sende Form", sagte Kriegscorte.  F√ºr das System ist es einfacher, sich an eine solche Skala zu halten: Die Anzahl der Pixel mit Texturinformationen √ºbersteigt die Anzahl der Pixel, aus denen die Grenze des Objekts besteht, erheblich, und die ersten Schritte des Netzwerks beziehen sich auf die Erkennung lokaler Merkmale wie Linien und Fl√§chen.  "Genau das ist Textur", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">John Tsotsos</a> , ein Computer-Vision-Spezialist an der York University in Toronto, der nicht mit dieser Studie in Verbindung gebracht wird.  "Zum Beispiel eine Gruppierung von Segmenten, die auf die gleiche Weise aneinandergereiht sind." <br><br>  Geyros und Kollegen zeigten, dass diese lokalen Zeichen ausreichen, damit das Netzwerk die Klassifizierung durchf√ºhren kann.  Dies ist der Beweis von Betge und einem weiteren Autor der Studie, dem Postdoc <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wiland Brendel</a> , der die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> , die auch auf der Mai-Konferenz vorgestellt wurde, zum Abschluss gebracht hat.  In dieser Arbeit bauten sie ein Deep-Learning-System auf, das √§hnlich funktionierte wie Klassifizierungsalgorithmen vor der Verbreitung des Deep-Learning - basierend auf dem Prinzip ‚ÄûBag of Attribute‚Äú.  Der Algorithmus zerlegt das Bild in kleine Fragmente, wie die aktuellen Modelle (wie Geyros, die in seinem Experiment verwendet wurden), aber anstatt diese Informationen schrittweise zu integrieren, um Anzeichen einer h√∂heren Abstraktionsebene zu extrahieren, nimmt der Algorithmus sofort eine Annahme √ºber den Inhalt jedes St√ºcks vor ( "In diesem St√ºck gibt es Hinweise auf ein Fahrrad, in diesem - Hinweise auf einen Vogel").  Er faltete einfach alle Entscheidungen zusammen, um das Objekt zu bestimmen (‚Äûwenn mehr Teile Zeichen eines Fahrrads enthalten, dann ist dies ein Fahrrad‚Äú), ohne auf die r√§umlichen Beziehungen der Teile zu achten.  Und doch konnte er Objekte mit unerwartet hoher Genauigkeit erkennen. <br><br>  "Diese Arbeit stellt die Annahme in Frage, dass Deep Learning etwas v√∂llig anderes bewirkt", sagte Brendel.  ‚ÄûOffensichtlich wurde ein gro√üer Sprung gemacht.  Ich sage nur, dass es nicht so gro√ü war, wie manche gehofft hatten. " <br><br>  Laut Amir Rosenfeld, einem Postdoc der York University und der University of Toronto, der nicht an der Studie teilgenommen hat, gibt es ‚Äûeinen gro√üen Unterschied zwischen dem, was neuronale Netze unserer Meinung nach tun sollten und dem, was sie tun‚Äú, einschlie√ülich der Art und Weise, wie sie es schaffen menschliches Verhalten reproduzieren. <br><br>  Die Brezel sprach in der gleichen Richtung.  Es ist leicht anzunehmen, dass neuronale Netze Probleme auf die gleiche Weise l√∂sen wie Menschen, sagte er.  "Wir vergessen jedoch st√§ndig die Existenz anderer Methoden." <br><br><h2>  Eine Verschiebung hin zu einer menschlicheren Sichtweise </h2><br>  Moderne Deep-Learning-Methoden k√∂nnen lokale Merkmale wie Texturen in globalere Muster wie Formen integrieren.  "Was in diesen Arbeiten unerwartet und sehr √ºberzeugend gezeigt wird - obwohl die Architektur es Ihnen erm√∂glicht, Standardbilder zu klassifizieren, geschieht dies nicht automatisch, wenn Sie nur das Netzwerk dar√ºber schulen", sagte Kriegscorte. <br><br>  Geyros wollte sehen, was passiert, wenn das Team Modelle zwingt, Texturen zu ignorieren.  Das Team nahm die Bilder, die traditionell f√ºr das Training von Klassifizierungsalgorithmen verwendet wurden, und malte sie in verschiedenen Stilen, wobei ihnen n√ºtzliche Texturinformationen entzogen wurden.  Als sie jedes Modell in den neuen Bildern umschulten, st√ºtzten sich die Systeme auf gr√∂√üere, globale Muster und zeigten eine gr√∂√üere Tendenz zur Mustererkennung, die eher Menschen √§hnelte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c60/18d/44b/c6018d44bb8459f3b0496d19975c6c5d.jpg" width="60%"><br>  <i>Wieland Brendel, Computational Neuroscientist an der Universit√§t T√ºbingen</i> <br><br>  Danach begannen die Algorithmen, verrauschte Bilder besser zu klassifizieren, selbst wenn sie nicht darauf trainiert waren, mit solchen Verzerrungen umzugehen.  "Das Formerkennungsnetzwerk ist kostenlos v√∂llig zuverl√§ssiger geworden", sagte Geyros.  "Dies deutet darauf hin, dass die richtige Tendenz zur Ausf√ºhrung bestimmter Aufgaben, in unserem Fall die Neigung zur Verwendung von Formularen, dazu beitr√§gt, das Wissen auf neue Bedingungen zu √ºbertragen." <br><br>  Dies legt auch nahe, dass sich beim Menschen eine solche Tendenz auf nat√ºrliche Weise bilden k√∂nnte, da die Verwendung von Formen eine zuverl√§ssigere Methode ist, um zu erkennen, was wir unter neuen oder lauten Bedingungen sehen.  Menschen leben in einer dreidimensionalen Welt, in der Objekte aus vielen Blickwinkeln unter vielen verschiedenen Bedingungen sichtbar sind und in der unsere anderen Gef√ºhle wie Ber√ºhrungen optional die Erkennung von Objekten erg√§nzen k√∂nnen.  Daher ist es f√ºr unsere Vision sinnvoll, der Form eine vorrangige Textur zuzuweisen.  Dar√ºber hinaus haben einige Psychologen einen Zusammenhang zwischen Sprache, Lernen und der Tendenz zur Verwendung von Formen gezeigt: Als Kindern beigebracht wurde, Formen beim Studium bestimmter Kategorien von W√∂rtern mehr Aufmerksamkeit zu schenken, konnten sie sp√§ter ein viel umfangreicheres Vokabular von Substantiven entwickeln als andere. <br><br>  Diese Arbeit erinnert daran, dass ‚ÄûDaten einen st√§rkeren Einfluss auf die Vorurteile und Vorurteile von Modellen haben als wir dachten‚Äú, sagte Wichman.  Dies ist nicht das erste Mal, dass Forscher auf dieses Problem sto√üen: Es wurde bereits gezeigt, dass Gesichtserkennungsprogramme, die automatische Suche nach Lebensl√§ufen und andere neuronale Netze unerwarteten Anzeichen aufgrund von Vorurteilen, die tief in den Daten verwurzelt sind, auf denen sie trainiert werden, zu viel Bedeutung beimessen.  Die Beseitigung unerw√ºnschter Vorurteile aus dem Entscheidungsprozess erwies sich als schwierige Aufgabe, aber Wichman sagte, die neue Arbeit zeige, dass dies im Prinzip m√∂glich und ermutigend sei. <br><br>  Selbst Geyros 'Modelle, die sich auf Formen konzentrieren, k√∂nnen get√§uscht werden, indem den Bildern zu viel Rauschen hinzugef√ºgt wird oder bestimmte Pixel ge√§ndert werden. Dies bedeutet, dass sie noch einen langen Weg vor sich haben, um eine Qualit√§t zu erreichen, die mit dem menschlichen Sehen vergleichbar ist.  In gleicher Weise zeigt eine neue Arbeit von Rosenfeld, Tsotsos und Marcus Solbach, einem Doktoranden des Tsotsos-Labors, dass Algorithmen f√ºr maschinelles Lernen nicht in der Lage sind, die √Ñhnlichkeit verschiedener Bilder wie Menschen zu erfassen.  Dennoch helfen solche Arbeiten, ‚Äûgenau anzugeben, in welchen Aspekten diese Modelle wichtige Aspekte des menschlichen Gehirns noch nicht reproduzieren‚Äú, sagte Kriegscorte.  Und Wichman sagte, dass "in einigen F√§llen es wichtiger sein kann, den Datensatz zu untersuchen." <br><br>  Sanya Fiedler, eine IT-Spezialistin an der Universit√§t von Toronto, die nicht an der Studie teilgenommen hat, stimmt dem zu.  "Es ist unsere Aufgabe, intelligente Daten zu entwickeln", sagte sie.  Sie und ihre Kollegen untersuchen, wie Nebenaufgaben dazu beitragen k√∂nnen, dass neuronale Netze die Qualit√§t ihrer Kernaufgaben verbessern.  Inspiriert von den Entdeckungen von Geyros haben sie k√ºrzlich den Bildklassifizierungsalgorithmus trainiert, um nicht nur die Objekte selbst zu erkennen, sondern auch um zu bestimmen, welche Pixel zu ihren Konturen geh√∂ren.  Und das Netzwerk konnte Objekte automatisch besser erkennen.  "Wenn Sie nur eine Aufgabe erhalten, ist das Ergebnis selektive Aufmerksamkeit und Blindheit in Bezug auf viele andere Dinge", sagte Fiedler.  "Wenn ich Ihnen mehrere Aufgaben gebe, werden Sie etwas √ºber verschiedene Dinge lernen, und dies kann nicht passieren."  Bei diesen Algorithmen ist es genauso. "  Das L√∂sen verschiedener Probleme hilft ihnen dabei, ‚Äûeine Tendenz zu verschiedenen Informationen zu entwickeln‚Äú, √§hnlich wie beim Experiment von Geyros mit Formen und Texturen. <br><br>  Alle diese Studien sind ‚Äûein sehr interessanter Schritt zur Vertiefung unseres Verst√§ndnisses dessen, was mit tiefem Lernen geschieht, und vielleicht helfen sie uns, die Einschr√§nkungen zu √ºberwinden, mit denen wir konfrontiert sind‚Äú, sagte Dietrich.  "Deshalb liebe ich diese Reihe von Arbeiten." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462951/">https://habr.com/ru/post/de462951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462939/index.html">Top 10 C ++ Russia-Berichte und Open Access-Konferenz-Playlist</a></li>
<li><a href="../de462943/index.html">Jagen Sie den Wumpus oder erleben Sie das Schreiben eines klassischen Android-Spiels</a></li>
<li><a href="../de462945/index.html">Generieren Sie Einmalkennw√∂rter f√ºr 2FA in JS mithilfe der Web Crypto API</a></li>
<li><a href="../de462947/index.html">Die Geschichte, wie PVS-Studio einen Fehler in der in ... PVS-Studio verwendeten Bibliothek gefunden hat</a></li>
<li><a href="../de462949/index.html">Die Geschichte, wie PVS-Studio einen Fehler in der in ... PVS-Studio verwendeten Bibliothek gefunden hat</a></li>
<li><a href="../de462955/index.html">Digitale Transformation der Schulung und Zertifizierung von Au√üendienstmitarbeitern</a></li>
<li><a href="../de462957/index.html">Vor- und Nachteile: Die Preisschwelle f√ºr .org ist weiterhin aufgehoben</a></li>
<li><a href="../de462959/index.html">Verarbeitung von Online-Schecks in nat√ºrlicher Sprache: Ein Kurs f√ºr Zauberkurse f√ºr eine normale Katze und andere Probleme</a></li>
<li><a href="../de462961/index.html">Data Science Digest (August 2019)</a></li>
<li><a href="../de462963/index.html">Verwenden der Kontext-API in React zum Erstellen eines globalen Anwendungsthemas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>