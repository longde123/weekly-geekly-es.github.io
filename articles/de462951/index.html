<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤦🏾 🎨 👐🏽 Wo eine Person Formen sieht, sieht KI Texturen 📛 👶🏽 👩‍👧‍👦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Überraschenderweise können Forscher mit tiefgreifenden Computer-Vision-Algorithmen Bilder häufig nicht klassifizieren, da sie sich hauptsächlich auf T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wo eine Person Formen sieht, sieht KI Texturen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462951/">  Überraschenderweise können Forscher mit tiefgreifenden Computer-Vision-Algorithmen Bilder häufig nicht klassifizieren, da sie sich hauptsächlich auf Texturen und nicht auf Formen konzentrieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/444/41f/95344441f850333698aa94694c254d32.jpg"><br><br>  Wenn Sie sich ein Foto einer Katze ansehen, können Sie dieses Tier mit hoher Wahrscheinlichkeit erkennen, unabhängig davon, ob es rot oder gestreift ist - oder selbst wenn das Foto schwarzweiß, fleckig, ramponiert oder angelaufen ist.  Sie werden wahrscheinlich eine Katze bemerken können, wenn sie sich hinter einem Kissen zusammenrollt oder auf einen Tisch springt, der nur eine verschwommene Form darstellt.  Sie haben natürlich gelernt, Katzen in fast jeder Situation zu erkennen.  Bildverarbeitungssysteme, die auf tiefen neuronalen Netzen basieren, können, obwohl sie Menschen manchmal unter festgelegten Bedingungen mit Katzenerkennungsaufgaben versorgen können, mit Bildern verwechselt werden, die sich zumindest geringfügig von dem unterscheiden, was sie wissen, oder Rauschen oder zu viel enthalten starkes Korn. <br><a name="habracut"></a><br>  Und jetzt haben deutsche Forscher einen unerwarteten Grund dafür entdeckt: Wenn Menschen auf die Formen der abgebildeten Objekte achten, hängt Computer Vision mit tiefem Lernen an den Texturen von Objekten. <br><br>  Diese <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Entdeckung</a> , die im Mai auf einer internationalen Konferenz über lernende Repräsentationen vorgestellt wurde, unterstreicht den scharfen Kontrast zwischen dem „Denken“ von Menschen und Maschinen und zeigt, wie falsch wir sein können, wenn wir verstehen, wie KI funktioniert.  Und es kann uns auch sagen, warum unsere Vision als Ergebnis der Evolution so wurde. <br><br><h2>  Elfenbeinkatzen und beobachten Flugzeuge </h2><br>  Deep-Learning-Algorithmen steuern Tausende von Bildern durch ein neuronales Netzwerk, das entweder eine Katze hat oder nicht.  Das System sucht in diesen Daten nach Mustern, mit denen es das Bild, auf das es zuvor noch nicht gestoßen ist, am besten markiert.  Die Netzwerkarchitektur ähnelt ein wenig der Struktur des menschlichen visuellen Systems, da sie über verbundene Schichten verfügt, mit denen immer mehr abstrakte Merkmale aus dem Bild extrahiert werden können.  Der Prozess des Aufbaus eines Assoziationssystems, das zur richtigen Antwort führt, ist jedoch eine Black Box, die die Menschen erst nachträglich interpretieren können.  "Wir haben versucht zu verstehen, was zum Erfolg dieser tiefgreifenden Computer-Vision-Algorithmen führt und warum sie so anfällig sind", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Thomas Ditterich</a> , ein IT-Spezialist an der University of Oregon, der nicht an dieser Studie beteiligt ist. <br><br>  Einige Forscher bevorzugen es zu untersuchen, was passiert, wenn sie versuchen, das Netzwerk durch geringfügige Änderung des Bildes auszutricksen.  Sie stellten fest, dass selbst kleine Änderungen dazu führen können, dass das System das Bild falsch markiert - und dass große Änderungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">möglicherweise nicht dazu führen, dass sich</a> die Beschriftung ändert.  In der Zwischenzeit verfolgen andere Experten Änderungen im System, um zu analysieren, wie einzelne Neuronen auf das Bild reagieren, und erstellen einen „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aktivierungsatlas</a> “, der auf den vom System erlernten Attributen basiert. <br><br>  Eine Gruppe von Wissenschaftlern aus den Labors des Computational Neurobiologist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Matias Betge</a> und des Psychophysiologen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Felix Wichmann</a> von der Universität Tübingen in Deutschland wählten jedoch einen qualitativen Ansatz.  Letztes Jahr berichtete das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Team,</a> dass das Netzwerk beim Training von Bildern, die durch Rauschen einer bestimmten Art verändert wurden, Bilder besser erkannte als Menschen, die versuchten, dieselben verrauschten Bilder zu erkennen.  Dieselben Bilder, die leicht unterschiedlich modifiziert wurden, verwirrten das Netzwerk jedoch völlig, obwohl die neue Verzerrung für die Menschen fast genauso aussah wie die alte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/418/8c4/c184188c4f088a155c652e51562c42f6.jpg" width="60%"><br>  <i>Robert Geyros, Doktorand in Computational Neurobiology an der Universität Tübingen</i> <br><br>  Um dieses Ergebnis zu erklären, fragten sich die Forscher, welche Bildqualität sich selbst mit etwas Rauschen am meisten ändert.  Die offensichtliche Wahl ist Textur.  "Die Form eines Objekts bleibt mehr oder weniger unversehrt, wenn Sie lange Zeit viel Lärm hinzufügen", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Robert Geyros</a> , ein Doktorand in den Labors von Betge und Wichmann, dem Hauptautor der Studie.  "Die lokale Bildstruktur wird jedoch sehr schnell verzerrt, wenn ein geringes Rauschen hinzugefügt wird."  Sie haben sich also eine schwierige Methode ausgedacht, um zu testen, wie die visuellen Systeme von Maschinen und Menschen Bilder verarbeiten. <br><br>  Geyros, Betge und ihre Kollegen haben Bilder mit zwei widersprüchlichen Merkmalen erstellt, die die Form eines Objekts und die Textur eines anderen annehmen: zum Beispiel eine Katzensilhouette in grauer Elefantenhautstruktur oder ein Bär aus Aluminiumdosen oder eine ebene Silhouette mit Überlappung einander mit Bildern von Zifferblättern.  Fast jedes Mal beschrifteten Menschen Hunderte solcher Bilder anhand ihrer Formen - Katze, Bär, Flugzeug - wie beabsichtigt.  Vier verschiedene Klassifizierungsalgorithmen neigten sich jedoch in die entgegengesetzte Richtung und verteilten Etiketten, die die Texturen von Objekten widerspiegelten: Elefanten, Dosen, Uhren. <br><br>  "Dies ändert unser Verständnis davon, wie tiefe neuronale Netze mit direkter Verteilung - ohne zusätzliche Einstellungen nach dem üblichen Lernprozess - Bilder erkennen", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Nikolaus Kriegescorte</a> , ein Computational Neuroscientist an der Columbia University, der nicht an der Studie beteiligt war. <br><br>  Auf den ersten Blick mag die Bevorzugung von KI-Texturen gegenüber Formen seltsam erscheinen, aber es macht Sinn.  "Textur ist eine hochauflösende Form", sagte Kriegscorte.  Für das System ist es einfacher, sich an eine solche Skala zu halten: Die Anzahl der Pixel mit Texturinformationen übersteigt die Anzahl der Pixel, aus denen die Grenze des Objekts besteht, erheblich, und die ersten Schritte des Netzwerks beziehen sich auf die Erkennung lokaler Merkmale wie Linien und Flächen.  "Genau das ist Textur", sagte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">John Tsotsos</a> , ein Computer-Vision-Spezialist an der York University in Toronto, der nicht mit dieser Studie in Verbindung gebracht wird.  "Zum Beispiel eine Gruppierung von Segmenten, die auf die gleiche Weise aneinandergereiht sind." <br><br>  Geyros und Kollegen zeigten, dass diese lokalen Zeichen ausreichen, damit das Netzwerk die Klassifizierung durchführen kann.  Dies ist der Beweis von Betge und einem weiteren Autor der Studie, dem Postdoc <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wiland Brendel</a> , der die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> , die auch auf der Mai-Konferenz vorgestellt wurde, zum Abschluss gebracht hat.  In dieser Arbeit bauten sie ein Deep-Learning-System auf, das ähnlich funktionierte wie Klassifizierungsalgorithmen vor der Verbreitung des Deep-Learning - basierend auf dem Prinzip „Bag of Attribute“.  Der Algorithmus zerlegt das Bild in kleine Fragmente, wie die aktuellen Modelle (wie Geyros, die in seinem Experiment verwendet wurden), aber anstatt diese Informationen schrittweise zu integrieren, um Anzeichen einer höheren Abstraktionsebene zu extrahieren, nimmt der Algorithmus sofort eine Annahme über den Inhalt jedes Stücks vor ( "In diesem Stück gibt es Hinweise auf ein Fahrrad, in diesem - Hinweise auf einen Vogel").  Er faltete einfach alle Entscheidungen zusammen, um das Objekt zu bestimmen („wenn mehr Teile Zeichen eines Fahrrads enthalten, dann ist dies ein Fahrrad“), ohne auf die räumlichen Beziehungen der Teile zu achten.  Und doch konnte er Objekte mit unerwartet hoher Genauigkeit erkennen. <br><br>  "Diese Arbeit stellt die Annahme in Frage, dass Deep Learning etwas völlig anderes bewirkt", sagte Brendel.  „Offensichtlich wurde ein großer Sprung gemacht.  Ich sage nur, dass es nicht so groß war, wie manche gehofft hatten. " <br><br>  Laut Amir Rosenfeld, einem Postdoc der York University und der University of Toronto, der nicht an der Studie teilgenommen hat, gibt es „einen großen Unterschied zwischen dem, was neuronale Netze unserer Meinung nach tun sollten und dem, was sie tun“, einschließlich der Art und Weise, wie sie es schaffen menschliches Verhalten reproduzieren. <br><br>  Die Brezel sprach in der gleichen Richtung.  Es ist leicht anzunehmen, dass neuronale Netze Probleme auf die gleiche Weise lösen wie Menschen, sagte er.  "Wir vergessen jedoch ständig die Existenz anderer Methoden." <br><br><h2>  Eine Verschiebung hin zu einer menschlicheren Sichtweise </h2><br>  Moderne Deep-Learning-Methoden können lokale Merkmale wie Texturen in globalere Muster wie Formen integrieren.  "Was in diesen Arbeiten unerwartet und sehr überzeugend gezeigt wird - obwohl die Architektur es Ihnen ermöglicht, Standardbilder zu klassifizieren, geschieht dies nicht automatisch, wenn Sie nur das Netzwerk darüber schulen", sagte Kriegscorte. <br><br>  Geyros wollte sehen, was passiert, wenn das Team Modelle zwingt, Texturen zu ignorieren.  Das Team nahm die Bilder, die traditionell für das Training von Klassifizierungsalgorithmen verwendet wurden, und malte sie in verschiedenen Stilen, wobei ihnen nützliche Texturinformationen entzogen wurden.  Als sie jedes Modell in den neuen Bildern umschulten, stützten sich die Systeme auf größere, globale Muster und zeigten eine größere Tendenz zur Mustererkennung, die eher Menschen ähnelte. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c60/18d/44b/c6018d44bb8459f3b0496d19975c6c5d.jpg" width="60%"><br>  <i>Wieland Brendel, Computational Neuroscientist an der Universität Tübingen</i> <br><br>  Danach begannen die Algorithmen, verrauschte Bilder besser zu klassifizieren, selbst wenn sie nicht darauf trainiert waren, mit solchen Verzerrungen umzugehen.  "Das Formerkennungsnetzwerk ist kostenlos völlig zuverlässiger geworden", sagte Geyros.  "Dies deutet darauf hin, dass die richtige Tendenz zur Ausführung bestimmter Aufgaben, in unserem Fall die Neigung zur Verwendung von Formularen, dazu beiträgt, das Wissen auf neue Bedingungen zu übertragen." <br><br>  Dies legt auch nahe, dass sich beim Menschen eine solche Tendenz auf natürliche Weise bilden könnte, da die Verwendung von Formen eine zuverlässigere Methode ist, um zu erkennen, was wir unter neuen oder lauten Bedingungen sehen.  Menschen leben in einer dreidimensionalen Welt, in der Objekte aus vielen Blickwinkeln unter vielen verschiedenen Bedingungen sichtbar sind und in der unsere anderen Gefühle wie Berührungen optional die Erkennung von Objekten ergänzen können.  Daher ist es für unsere Vision sinnvoll, der Form eine vorrangige Textur zuzuweisen.  Darüber hinaus haben einige Psychologen einen Zusammenhang zwischen Sprache, Lernen und der Tendenz zur Verwendung von Formen gezeigt: Als Kindern beigebracht wurde, Formen beim Studium bestimmter Kategorien von Wörtern mehr Aufmerksamkeit zu schenken, konnten sie später ein viel umfangreicheres Vokabular von Substantiven entwickeln als andere. <br><br>  Diese Arbeit erinnert daran, dass „Daten einen stärkeren Einfluss auf die Vorurteile und Vorurteile von Modellen haben als wir dachten“, sagte Wichman.  Dies ist nicht das erste Mal, dass Forscher auf dieses Problem stoßen: Es wurde bereits gezeigt, dass Gesichtserkennungsprogramme, die automatische Suche nach Lebensläufen und andere neuronale Netze unerwarteten Anzeichen aufgrund von Vorurteilen, die tief in den Daten verwurzelt sind, auf denen sie trainiert werden, zu viel Bedeutung beimessen.  Die Beseitigung unerwünschter Vorurteile aus dem Entscheidungsprozess erwies sich als schwierige Aufgabe, aber Wichman sagte, die neue Arbeit zeige, dass dies im Prinzip möglich und ermutigend sei. <br><br>  Selbst Geyros 'Modelle, die sich auf Formen konzentrieren, können getäuscht werden, indem den Bildern zu viel Rauschen hinzugefügt wird oder bestimmte Pixel geändert werden. Dies bedeutet, dass sie noch einen langen Weg vor sich haben, um eine Qualität zu erreichen, die mit dem menschlichen Sehen vergleichbar ist.  In gleicher Weise zeigt eine neue Arbeit von Rosenfeld, Tsotsos und Marcus Solbach, einem Doktoranden des Tsotsos-Labors, dass Algorithmen für maschinelles Lernen nicht in der Lage sind, die Ähnlichkeit verschiedener Bilder wie Menschen zu erfassen.  Dennoch helfen solche Arbeiten, „genau anzugeben, in welchen Aspekten diese Modelle wichtige Aspekte des menschlichen Gehirns noch nicht reproduzieren“, sagte Kriegscorte.  Und Wichman sagte, dass "in einigen Fällen es wichtiger sein kann, den Datensatz zu untersuchen." <br><br>  Sanya Fiedler, eine IT-Spezialistin an der Universität von Toronto, die nicht an der Studie teilgenommen hat, stimmt dem zu.  "Es ist unsere Aufgabe, intelligente Daten zu entwickeln", sagte sie.  Sie und ihre Kollegen untersuchen, wie Nebenaufgaben dazu beitragen können, dass neuronale Netze die Qualität ihrer Kernaufgaben verbessern.  Inspiriert von den Entdeckungen von Geyros haben sie kürzlich den Bildklassifizierungsalgorithmus trainiert, um nicht nur die Objekte selbst zu erkennen, sondern auch um zu bestimmen, welche Pixel zu ihren Konturen gehören.  Und das Netzwerk konnte Objekte automatisch besser erkennen.  "Wenn Sie nur eine Aufgabe erhalten, ist das Ergebnis selektive Aufmerksamkeit und Blindheit in Bezug auf viele andere Dinge", sagte Fiedler.  "Wenn ich Ihnen mehrere Aufgaben gebe, werden Sie etwas über verschiedene Dinge lernen, und dies kann nicht passieren."  Bei diesen Algorithmen ist es genauso. "  Das Lösen verschiedener Probleme hilft ihnen dabei, „eine Tendenz zu verschiedenen Informationen zu entwickeln“, ähnlich wie beim Experiment von Geyros mit Formen und Texturen. <br><br>  Alle diese Studien sind „ein sehr interessanter Schritt zur Vertiefung unseres Verständnisses dessen, was mit tiefem Lernen geschieht, und vielleicht helfen sie uns, die Einschränkungen zu überwinden, mit denen wir konfrontiert sind“, sagte Dietrich.  "Deshalb liebe ich diese Reihe von Arbeiten." </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de462951/">https://habr.com/ru/post/de462951/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de462939/index.html">Top 10 C ++ Russia-Berichte und Open Access-Konferenz-Playlist</a></li>
<li><a href="../de462943/index.html">Jagen Sie den Wumpus oder erleben Sie das Schreiben eines klassischen Android-Spiels</a></li>
<li><a href="../de462945/index.html">Generieren Sie Einmalkennwörter für 2FA in JS mithilfe der Web Crypto API</a></li>
<li><a href="../de462947/index.html">Die Geschichte, wie PVS-Studio einen Fehler in der in ... PVS-Studio verwendeten Bibliothek gefunden hat</a></li>
<li><a href="../de462949/index.html">Die Geschichte, wie PVS-Studio einen Fehler in der in ... PVS-Studio verwendeten Bibliothek gefunden hat</a></li>
<li><a href="../de462955/index.html">Digitale Transformation der Schulung und Zertifizierung von Außendienstmitarbeitern</a></li>
<li><a href="../de462957/index.html">Vor- und Nachteile: Die Preisschwelle für .org ist weiterhin aufgehoben</a></li>
<li><a href="../de462959/index.html">Verarbeitung von Online-Schecks in natürlicher Sprache: Ein Kurs für Zauberkurse für eine normale Katze und andere Probleme</a></li>
<li><a href="../de462961/index.html">Data Science Digest (August 2019)</a></li>
<li><a href="../de462963/index.html">Verwenden der Kontext-API in React zum Erstellen eines globalen Anwendungsthemas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>