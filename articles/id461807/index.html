<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎤 🤑 💮 Bagaimana prioritas pod di Kubernetes menyebabkan downtime di Grafana Labs 🚨 🤲🏿 📙</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Catatan perev. : Kami hadir untuk perhatian Anda perincian teknis tentang alasan downtime layanan cloud baru-baru ini yang dilayani oleh pencipta Graf...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana prioritas pod di Kubernetes menyebabkan downtime di Grafana Labs</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/461807/">  <i><b>Catatan</b></i>  <i><b>perev.</b></i>  <i>: Kami hadir untuk perhatian Anda perincian teknis tentang alasan downtime layanan cloud baru-baru ini yang dilayani oleh pencipta Grafana.</i>  <i>Ini adalah contoh klasik tentang bagaimana fitur baru dan tampaknya sangat berguna yang dirancang untuk meningkatkan kualitas infrastruktur ... dapat sangat merugikan jika seseorang tidak melihat banyak nuansa penerapannya dalam realitas produksi.</i>  <i>Luar biasa ketika materi seperti itu muncul yang memungkinkan Anda belajar tidak hanya dari kesalahan Anda.</i>  <i>Rincian dalam terjemahan teks ini dari wakil presiden produk dari Grafana Labs.</i> <br><br><img src="https://habrastorage.org/webt/yb/jj/1h/ybjj1hh4m7ro1eym14eiercw7po.jpeg"><br><br>  Pada hari Jumat, 19 Juli, layanan Hosted Prometheus di Grafana Cloud berhenti bekerja selama sekitar 30 menit.  Saya minta maaf kepada semua klien yang menderita kegagalan.  Tugas kami adalah menyediakan alat yang diperlukan untuk pemantauan, dan kami memahami bahwa tidak dapat diaksesnya hal tersebut mempersulit hidup Anda.  Kami menangani insiden ini dengan sangat serius.  Catatan ini menjelaskan apa yang terjadi, bagaimana kami bereaksi terhadapnya, dan apa yang kami lakukan sehingga ini tidak terjadi lagi. <a name="habracut"></a><br><br><h2>  Latar belakang </h2><br>  Layanan Grafana Cloud Hosted Prometheus didasarkan pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Cortex</a> , sebuah proyek CNCF untuk membuat layanan Prometheus multi-tenant yang dapat diskalakan secara horizontal, sangat mudah diakses.  Arsitektur Cortex terdiri dari serangkaian layanan microser terpisah, yang masing-masing menjalankan fungsinya: replikasi, penyimpanan, permintaan, dll.  Cortex sedang dikembangkan secara aktif, terus-menerus memiliki peluang baru dan meningkatkan produktivitas.  Kami secara teratur menyebarkan rilis Cortex baru ke cluster sehingga pelanggan dapat memanfaatkan peluang ini - untungnya, Cortex dapat memperbarui tanpa downtime. <br><br>  Untuk pembaruan yang lancar, layanan Ingester Cortex memerlukan replika Ingester tambahan selama proses pembaruan.  <i>( <b>Catatan</b> : <a href="">Ingester</a> adalah komponen inti Cortex. Tugasnya adalah mengumpulkan aliran sampel yang konstan, mengelompokkannya menjadi potongan Prometheus dan menyimpannya dalam basis data seperti DynamoDB, BigTable atau Cassandra.)</i> Ini memungkinkan Ingesters yang lebih tua. meneruskan data saat ini ke Ingester baru.  Perlu dicatat bahwa Ingester menuntut sumber daya.  Untuk pekerjaan mereka perlu memiliki 4 core dan 15 GB memori per pod, mis.  25% dari daya prosesor dan memori mesin dasar dalam kasus kluster Kubernet kami.  Secara umum, kami biasanya memiliki lebih banyak sumber daya yang tidak terpakai dalam sebuah cluster dari 4 core dan memori 15 GB, sehingga kami dapat dengan mudah menjalankan Ingester tambahan ini selama pembaruan. <br><br>  Namun, sering terjadi bahwa selama operasi normal tidak satu pun dari mesin ini memiliki 25% dari sumber daya yang tidak diklaim.  Ya, kami tidak berusaha: CPU dan memori selalu berguna untuk proses lain.  Untuk mengatasi masalah ini, kami memutuskan untuk menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes Pod Priorities</a> .  Idenya adalah untuk memberikan Ingester prioritas yang lebih tinggi daripada layanan microser lainnya (stateless).  Saat kami perlu menjalankan Ingester tambahan (N +1), untuk sementara kami mengeluarkan pod lainnya yang lebih kecil.  Pod ini ditransfer ke sumber daya gratis di mesin lain, meninggalkan "lubang" yang cukup besar untuk meluncurkan Ingester tambahan. <br><br>  Pada hari Kamis, 18 Juli, kami meluncurkan empat tingkat prioritas baru dalam kelompok kami: <b>kritis</b> , <b>tinggi</b> , <b>sedang</b> dan <b>rendah</b> .  Mereka diuji pada cluster internal tanpa lalu lintas klien selama sekitar satu minggu.  Secara default, polong tanpa prioritas tertentu mendapat prioritas <b>sedang</b> , kelas dengan prioritas <b>tinggi</b> ditetapkan untuk Ingesters.  <b>Critical</b> dicadangkan untuk pemantauan (Prometheus, Alertmanager, simpul-eksportir, metrik kube-state, dll.).  Konfigurasi kami terbuka, dan lihat PR di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> . <br><br><h2>  Kecelakaan </h2><br>  Pada hari Jumat, 19 Juli, salah satu insinyur meluncurkan kluster Cortex khusus baru untuk klien besar.  Konfigurasi untuk klaster ini tidak termasuk prioritas pod baru, jadi semua pod baru diberi prioritas default - <b>medium</b> . <br><br>  Cluster Kubernetes tidak memiliki sumber daya yang cukup untuk cluster Cortex baru, dan cluster produksi Cortex yang ada tidak diperbarui (Ingester dibiarkan tanpa prioritas <b>tinggi</b> ).  Karena Ingesters dari cluster baru default ke prioritas <b>menengah</b> , dan polong yang ada dalam produksi bekerja tanpa prioritas sama sekali, Ingester dari cluster baru mengusir Ingesters dari cluster produksi Cortex yang ada. <br><br>  ReplicaSet untuk Ingester yang dikecualikan dalam klaster produksi mendeteksi pod yang dikecam dan membuat yang baru untuk mempertahankan jumlah salinan yang ditentukan.  Pod baru ditetapkan ke prioritas <b>sedang</b> secara default, dan Ingester "lama" berikutnya dalam produksi kehilangan sumber daya.  Hasilnya adalah <b>proses seperti longsoran salju</b> yang menyebabkan crowding out semua polong dari Ingester untuk cluster produksi Cortex. <br><br>  Ingester menyimpan status dan menyimpan data selama 12 jam sebelumnya.  Ini memungkinkan kita untuk mengompres mereka lebih efisien sebelum menulis ke penyimpanan jangka panjang.  Untuk melakukan ini, Cortex membuang data seri menggunakan Tabel Hash Terdistribusi (DHT), dan mereplikasi setiap seri menjadi tiga Ingester menggunakan konsistensi kuorum gaya Dynamo.  Cortex tidak menulis data ke Ingesters, yang dinonaktifkan.  Jadi, ketika sejumlah besar Ingester meninggalkan DHT, Cortex tidak dapat memberikan replikasi yang memadai dari catatan, dan mereka "jatuh". <br><br><h2>  Deteksi dan eliminasi </h2><br>  Pemberitahuan Prometheus baru berdasarkan "berbasis <i>kesalahan-anggaran</i> " (rincian <i>berbasis kesalahan</i> akan muncul di artikel mendatang) mulai membunyikan alarm 4 menit setelah dimulainya penutupan.  Selama sekitar lima menit berikutnya, kami melakukan diagnosa dan memperluas kluster Kubernetes yang mendasarinya untuk mengakomodasi kluster produksi baru dan yang sudah ada. <br><br>  Lima menit kemudian, Ingesters lama berhasil merekam data mereka, dan yang baru mulai, dan cluster Cortex menjadi tersedia lagi. <br><br>  Butuh 10 menit lagi untuk mendiagnosis dan memperbaiki kesalahan kehabisan memori (OOM) dari proksi otentikasi terbalik yang terletak di depan Cortex.  Kesalahan OOM disebabkan oleh peningkatan sepuluh kali lipat dalam QPS (seperti yang kami yakini, karena permintaan yang terlalu agresif dari server klien Prometheus). <br><br><h2>  Konsekuensinya </h2><br>  Total waktu henti adalah 26 menit.  Tidak ada data yang hilang.  Ingester telah berhasil mengunggah semua data dalam memori ke penyimpanan jangka panjang.  Selama shutdown, server klien Prometheus membuat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">buffer</a> entri menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">API remote_write</a> berbasis WAL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">baru</a> (ditulis oleh <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Callum Styan</a> dari Grafana Labs) dan mengulangi entri yang gagal setelah kegagalan. <br><br><img src="https://habrastorage.org/webt/ub/rv/3p/ubrv3po8fpxvn0r5ifuvwbcdogy.png"><br>  <i>Operasi Penulisan Klaster Produksi</i> <br><br><h2>  Kesimpulan </h2><br>  Penting untuk belajar dari kejadian ini dan mengambil langkah-langkah yang diperlukan untuk menghindari terulangnya kejadian tersebut. <br><br>  Melihat ke belakang, kita harus mengakui bahwa kita tidak boleh menetapkan prioritas <b>medium</b> default, sampai semua Ingester dalam produksi menerima prioritas <b>tinggi</b> .  Selain itu, mereka harus menjaga prioritas <b>tinggi</b> mereka terlebih dahulu.  Sekarang semuanya sudah diperbaiki.  Kami berharap pengalaman kami akan membantu organisasi lain mempertimbangkan penggunaan prioritas pod di Kubernetes. <br><br>  Kami akan menambahkan tingkat kontrol tambahan atas penyebaran objek tambahan apa pun yang konfigurasinya bersifat global untuk kluster.  Selanjutnya, perubahan tersebut akan dievaluasi oleh lebih banyak orang.  Selain itu, modifikasi yang menyebabkan kegagalan dianggap terlalu tidak signifikan untuk dokumen proyek yang terpisah - itu hanya dibahas dalam masalah GitHub.  Mulai sekarang, semua perubahan konfigurasi tersebut akan disertai dengan dokumentasi proyek yang sesuai. <br><br>  Terakhir, kami mengotomatiskan pengubahan ukuran proksi otentikasi terbalik untuk mencegah OOM selama kemacetan, yang telah kami saksikan, dan menganalisis pengaturan Prometheus default yang terkait dengan rollback dan penskalaan untuk mencegah masalah serupa di masa mendatang. <br><br>  Kegagalan yang dialami juga memiliki beberapa konsekuensi positif: setelah menerima sumber daya yang diperlukan, Cortex secara otomatis pulih tanpa intervensi tambahan.  Kami juga mendapatkan pengalaman berharga dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Grafana Loki</a> , sistem agregasi log baru kami, yang membantu memastikan bahwa semua Ingester berperilaku baik selama dan setelah kecelakaan. <br><br><h2>  PS dari penerjemah </h2><br>  Baca juga di blog kami: <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Penskalaan otomatis dan manajemen sumber daya di Kubernetes (laporan ulasan dan video)</a> ”; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kubernetes-adventure Dailymotion: membangun infrastruktur di awan + di tempat</a> ”; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tinder Migration to Kubernetes</a> "; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kisah sukses Kubernetes dalam produksi.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 10: Reddit</a> . " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id461807/">https://habr.com/ru/post/id461807/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id461793/index.html">Ivan Ponomarev tentang Kafka Streams API di pertemuan jug.msk.ru</a></li>
<li><a href="../id461797/index.html">Kisah-kisah pelayanan. Posting sembrono tentang pekerjaan serius</a></li>
<li><a href="../id461801/index.html">DisplayPort-LVDS</a></li>
<li><a href="../id461803/index.html">Data Version Control (DVC): versi data dan reproduksibilitas percobaan</a></li>
<li><a href="../id461805/index.html">Aplikasi Integrasi Monte Carlo dalam Rendering</a></li>
<li><a href="../id461813/index.html">Berita dari dunia OpenStreetMap No. 470 (07.16.2019 - 07.22.2019)</a></li>
<li><a href="../id461815/index.html">Sebuah revolusi dalam desain pasokan daya komputer setengah abad yang lalu</a></li>
<li><a href="../id461817/index.html">CMake dan C ++ - saudara selamanya</a></li>
<li><a href="../id461819/index.html">Mengapa desain situs web sederhana lebih baik secara ilmiah</a></li>
<li><a href="../id461821/index.html">Imunoterapi baru menghilangkan semua tumor pada wanita dengan kanker payudara metastatik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>