<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõë üåÇ üôÖüèø Evoluci√≥n de la infraestructura de la base de datos: de la base de datos y la aplicaci√≥n en un servidor a la replicaci√≥n de transmisi√≥n üòñ üë®üèº‚Äçüî¨ ü§πüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! 

 Mi nombre es Anton Markelov, soy ingeniero de operaciones en United Traders. Estamos involucrados en proyectos de una forma u otra relac...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Evoluci√≥n de la infraestructura de la base de datos: de la base de datos y la aplicaci√≥n en un servidor a la replicaci√≥n de transmisi√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/utex/blog/443530/"><img src="https://habrastorage.org/webt/9o/uy/bk/9ouybky85gzjccvemkc882t32ws.png"><br><br>  Hola Habr! <br><br>  Mi nombre es Anton Markelov, soy ingeniero de operaciones en United Traders.  Estamos involucrados en proyectos de una forma u otra relacionados con inversiones, intercambios y otros asuntos financieros.  No somos una empresa muy grande, unos 30 ingenieros de desarrollo, las escalas son apropiadas, un poco menos de cien servidores.  Durante el crecimiento cuantitativo y cualitativo de nuestra infraestructura, la soluci√≥n cl√°sica "mantenemos tanto la aplicaci√≥n como su base de datos en el mismo servidor" dej√≥ de satisfacernos tanto en t√©rminos de confiabilidad como de velocidad.  Por parte de los analistas, era necesario crear consultas entre bases de datos, el departamento de operaciones estaba cansado de perder el tiempo haciendo copias de seguridad y monitoreando una gran cantidad de servidores de bases de datos.  Adem√°s de eso, almacenar el estado en la misma m√°quina que la aplicaci√≥n en s√≠ misma redujo en gran medida la flexibilidad de la planificaci√≥n de recursos y la resistencia de la infraestructura. <br><br>  El proceso de transici√≥n a la arquitectura actual fue evolutivo, se probaron varias soluciones para proporcionar una interfaz conveniente para desarrolladores y analistas, y para aumentar la confiabilidad y la capacidad de administraci√≥n de toda esta econom√≠a.  Quiero hablar sobre las principales etapas de la modernizaci√≥n de nuestro DBMS, a qu√© nivel hemos llegado y qu√© decisiones hemos tomado, como resultado, un entorno independiente tolerante a fallas que proporciona formas convenientes de interacci√≥n para ingenieros de operaciones, desarrolladores y analistas.  Espero que nuestra experiencia sea √∫til para los ingenieros de empresas de nuestra escala. <br><br>  Este art√≠culo es un resumen de mi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">informe</a> en la conferencia UPTIMEDAY, quiz√°s el formato de video sea m√°s c√≥modo para alguien, aunque el escritor es un poco mejor con mis manos que un orador. <br><br>  El "Hombre del copo de nieve" con KDPV fue <i>prestado</i> descaradamente de Maxim Dorofeev. <br><a name="habracut"></a><br><h2>  Enfermedades de crecimiento </h2><br>  Tenemos una arquitectura de microservicio, los servicios se escriben principalmente en Java o Kotlin utilizando el marco Spring.  Al lado de cada microservicio hay una base PostgreSQL, todo est√° cubierto por nginx en la parte superior para proporcionar acceso.  Un microservicio t√≠pico es una aplicaci√≥n en Spring Boot que escribe sus datos en PostgreSQL (parte de las aplicaciones al mismo tiempo y en ClickHouse), se comunica con los vecinos a trav√©s de Kafka y tiene algunos puntos finales REST o GraphQL para comunicarse con el mundo exterior. <br><br><img src="https://habrastorage.org/webt/43/s1/aq/43s1aq-9ggmakbk3dygl8ox9bna.png"><br><br>  Anteriormente, cuando √©ramos muy peque√±os, solo ten√≠amos varios servidores en DigitalOcean, Kafka a√∫n no estaba all√≠, toda la comunicaci√≥n se realiz√≥ a trav√©s de REST.  Es decir, tomamos una gotita, instalamos Java, PostgreSQL, nginx all√≠, lanzamos Zabbix all√≠ para que monitoree los recursos del servidor y la disponibilidad de puntos finales de servicio.  Implementaron todo con la ayuda de Ansible, ten√≠amos libros de jugadas estandarizados, cuatro o cinco roles implementaron todo el servicio.  Mientras tuvi√©ramos, relativamente hablando, 6 servidores en producci√≥n y 3 en prueba, de alguna manera podr√≠as vivir con √©l. <br>  Luego comenz√≥ la fase de desarrollo activo, el n√∫mero de aplicaciones creci√≥, diez microservicios se convirtieron en cuarenta, su funcionalidad comenz√≥ a cambiar, adem√°s apareci√≥ la integraci√≥n con sistemas externos como CRM, sitios de clientes y similares.  Tenemos el primer dolor.  Algunas aplicaciones comenzaron a consumir m√°s recursos, dejaron de ingresar a los servidores existentes, obtuvimos gotitas, arrastramos aplicaciones de un lado a otro, tomamos muchas manos.  Me doli√≥ bastante, a nadie le gusta el est√∫pido trabajo mec√°nico, quer√≠a decidir r√°pidamente.  As√≠ que fuimos de frente: solo tomamos 3 servidores dedicados grandes en lugar de 10 gotas de nube.  Esto cerr√≥ el problema por un tiempo, pero se hizo evidente que era hora de encontrar opciones para alg√∫n tipo de orquestaci√≥n y reequilibrio del servidor.  Comenzamos a analizar de cerca las soluciones como DC / OS y Kubernetes, y aumentamos gradualmente nuestra experiencia en esta √°rea. <br><br>  Casi al mismo tiempo, ten√≠amos un departamento anal√≠tico, que necesitaba hacer solicitudes dif√≠ciles con regularidad, preparar informes, tener tableros hermosos, y esto nos trajo un segundo dolor.  En primer lugar, los analistas cargaron mucho la base y, en segundo lugar, necesitaban consultas entre bases de datos, porque  cada microservicio mantuvo un segmento de datos bastante estrecho.  Probamos varios sistemas, al principio intentamos resolverlo todo a trav√©s de la replicaci√≥n a nivel de tabla (estaba de vuelta en el noveno PostgreSQL, no hab√≠a una r√©plica l√≥gica lista para usar), pero las artesan√≠as resultantes basadas en pglogical, Presto, Slony-I y Bucardo no lo hicieron por completo. arreglado  Por ejemplo, pglogical no era compatible con la migraci√≥n: se implement√≥ una nueva versi√≥n del microservicio, la estructura de la base de datos cambi√≥, Java cambi√≥ la estructura usando Flyway y en las r√©plicas en pglogical todo debe cambiarse manualmente.  De lo contrario, faltaba algo o era demasiado dif√≠cil. <br><br><h2>  S√∫per esclavo </h2><br>  Como resultado de la investigaci√≥n, naci√≥ una soluci√≥n simple y brutal llamada Superslave: tomamos un servidor separado, configuramos un esclavo para cada servidor de producci√≥n en diferentes puertos y creamos una base de datos virtual que combina las bases de datos de los esclavos a trav√©s de postgres_fdw (contenedor de datos extranjeros).  Es decir, todo esto se implement√≥ por medios est√°ndar de postgres sin introducir entidades adicionales, de manera simple y confiable: con una sola solicitud fue posible obtener datos de varias bases de datos.  Dimos esta base cruzada virtual a los analistas.  Una ventaja adicional es que la r√©plica de solo lectura, incluso con un error con derechos de acceso, no pudo escribir nada all√≠. <br><br><img src="https://habrastorage.org/webt/uf/j7/2i/ufj72i581hu3b_s5cep3cy_dk60.png"><br><br>  Llevamos a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Redash</a> para visualizaci√≥n, sabe c√≥mo dibujar gr√°ficos, ejecutar solicitudes en un horario, por ejemplo, una vez al d√≠a, y tiene un sistema de derechos pesado, por lo que dejamos que los analistas y desarrolladores vayan all√≠. <br><br><img src="https://habrastorage.org/webt/wq/jo/gj/wqjogjc5ovjcfxz9b6nqa_g71nm.png"><br><br>  Paralelamente, el crecimiento continu√≥, Kafka apareci√≥ en la infraestructura como un bus y ClickHouse para el almacenamiento de an√°lisis.  Se agrupan f√°cilmente fuera de la caja, nuestro s√∫per esclavo contra su fondo parec√≠a un f√≥sil torpe.  Adem√°s, PostgreSQL, de hecho, segu√≠a siendo el √∫nico estado que ten√≠a que arrastrarse despu√©s de la aplicaci√≥n (si a√∫n ten√≠a que transferirse a otro servidor), y realmente quer√≠amos obtener una aplicaci√≥n sin estado para participar de cerca en experimentos con Kubernetes y √©l. plataformas similares <br><br>  Comenzamos a buscar una soluci√≥n que cumpla con los siguientes requisitos: <br><br><ul><li>  tolerancia a fallas: cuando caen N servidores, el cl√∫ster contin√∫a funcionando; </li><li>  para las aplicaciones, todo debe permanecer como antes, sin cambios en el c√≥digo; </li><li>  facilidad de despliegue y gesti√≥n; </li><li>  menos capas de abstracci√≥n sobre PostgreSQL regular; </li><li>  idealmente, balanceo de carga para que no todas las solicitudes vayan a un servidor; </li><li>  Idealmente, est√° escrito en un lenguaje familiar. </li></ul><br>  No hab√≠a muchos candidatos: <br><br><ul><li>  replicaci√≥n de transmisi√≥n est√°ndar (repmgr, Patroni, Stolon); </li><li>  replicaci√≥n basada en disparador (Londiste, Slony); </li><li>  replicaci√≥n de consultas de capa media (pgpool-II); </li><li>  replicaci√≥n sincr√≥nica con m√∫ltiples servidores centrales (Bucardo). </li></ul><br>  En gran parte, ya tuvimos malas experiencias durante la construcci√≥n de la base cruzada, por lo que Patroni y Stolon se quedaron.  Patroni est√° escrito en Python, Stolon in Go, tenemos suficiente experiencia en ambos idiomas.  Adem√°s, tienen una arquitectura y funcionalidad similares, por lo que la elecci√≥n se hizo por razones subjetivas: Patroni fue desarrollado por Zalando, y una vez intentamos trabajar con su proyecto Nakadi (REST API para Kafka), donde encontramos una grave falta de documentaci√≥n. <br><br><h2>  Estol√≥n </h2><br><img src="https://habrastorage.org/webt/z5/af/hf/z5afhfobk43vpt0x2dmryf0gt50.png"><br><br>  La arquitectura de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Stolon es</a> bastante simple: hay N servidores, el l√≠der se selecciona usando etcd / consul, PostgreSQL se inicia en modo asistente y se replica a otros servidores.  Luego, los proxies stolon van a este PostgreSQL-master, pretendiendo ser aplicaciones con postgres ordinarios, y los clientes van a estos proxies.  En caso de desaparici√≥n de un maestro, se realizan reelecciones, alguien m√°s se convierte en maestro, el resto se convierte en stand-by.  Hay pocas capas de abstracci√≥n, PostgreSQL se instala como de costumbre, la √∫nica advertencia es que la configuraci√≥n de PostgreSQL se almacena en etcd, y se configura de manera algo diferente. <br><br>  Al probar el cl√∫ster, detectamos bastantes problemas: <br><br><ul><li>  Stolon no sabe c√≥mo trabajar sobre ZooKeeper, solo c√≥nsul o etcd; </li><li>  etcd es muy sensible a IO.  Si mantiene PostgreSQL y etcd en el mismo servidor, definitivamente necesita SSD r√°pidos; </li><li>  incluso en SSD es necesario configurar tiempos de espera de etcd, de lo contrario todo se romper√° bajo carga: el cl√∫ster pensar√° que el maestro se ha ca√≠do y romper√° las conexiones constantemente; </li><li>  De forma predeterminada, max_ connections en PostgreSQL es peque√±o (200), debe aumentarlo seg√∫n sus necesidades; </li><li>  un grupo de tres, etc. sobrevivir√° a la muerte de un solo servidor, idealmente debe tener una configuraci√≥n, por ejemplo, 5 etcd + 3 Stolon; </li><li>  fuera de la caja, todas las conexiones van al maestro, los esclavos no son accesibles para la conexi√≥n. </li></ul><br>  Dado que todas las conexiones a PostgreSQL van al asistente, nuevamente nos encontramos con un problema con solicitudes de an√°lisis pesadas.  etcd a veces reaccion√≥ dolorosamente a la alta carga en el maestro y lo cambi√≥.  Y cambiar al asistente siempre es romper las conexiones.  La solicitud se reinici√≥, todo comenz√≥ de nuevo.  Para una soluci√≥n alternativa, se escribi√≥ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un script de Python</a> que solicit√≥ direcciones stolonctl de esclavos vivos y gener√≥ una configuraci√≥n para HAProxy, redirigiendo las solicitudes a ellos. <br><br><img src="https://habrastorage.org/webt/k6/er/ds/k6erds-_gyudfmyruuzpmw0oudm.png"><br><br>  Result√≥ la siguiente imagen: las solicitudes de las aplicaciones van al puerto stolon-proxy, que las redirige al maestro, y las solicitudes de los analistas (siempre son de solo lectura) van al puerto HAProxy, que las transfiere a alg√∫n esclavo. <br><br>  Adem√°s, literalmente hoy, se adopt√≥ un RP en Stolon, que permiti√≥ enviar informaci√≥n sobre instancias de Stolon a un descubrimiento de servicios de terceros. <br><br><img src="https://habrastorage.org/webt/o-/al/k_/o-alk_xqmlva498z1yav5nbooze.png"><br><br>  En cuanto a juzgar por las m√©tricas de velocidad de respuesta de la aplicaci√≥n, la transici√≥n a un cl√∫ster remoto no tuvo un impacto significativo en el rendimiento, el tiempo de respuesta promedio no ha cambiado.  La latencia de red resultante, aparentemente, fue compensada por el hecho de que la base de datos ahora est√° en un servidor dedicado. <br><br>  Stolon sin problemas sobrevive a un bloqueo del asistente (p√©rdida de servidor, p√©rdida de red, p√©rdida de disco), cuando el servidor cobra vida: restablece autom√°ticamente la r√©plica.  El punto m√°s d√©bil en Stolon es etcd, las fallas en √©l ponen el cl√∫ster.  Tuvimos un accidente t√≠pico: un grupo de tres nodos, etc., dos fueron cortados.  Todo, el qu√≥rum se rompi√≥, etcd entr√≥ en estado no saludable, el cl√∫ster Stolon no acepta ninguna conexi√≥n, incluidas las solicitudes de stolonctl.  Esquema de recuperaci√≥n: convierta etcd en el servidor sobreviviente en un cl√∫ster de nodo √∫nico, luego agregue los miembros nuevamente.  Conclusi√≥n: para sobrevivir a la muerte de dos servidores, debe tener al menos 5 instancias, etc. <br><br><h2>  Monitoreo y captura de errores </h2><br>  Con el crecimiento de la infraestructura y la complejidad de los microservicios, quer√≠a recopilar m√°s informaci√≥n sobre lo que est√° sucediendo dentro de la aplicaci√≥n y la m√°quina Java.  No pudimos adaptar Zabbix al nuevo entorno: es muy inconveniente en las condiciones de una infraestructura cambiante.  Tuve que moler muletas a trav√©s de su API o subir con mis manos, lo que es a√∫n peor.  Su base de datos est√° mal adaptada a cargas pesadas, y en general es muy inconveniente poner todo esto en una base de datos relacional. <br><br>  Como resultado, elegimos Prometheus para el monitoreo.  √âl tiene un actuador listo para usar para aplicaciones Spring para proporcionar m√©tricas, para Kafka atornillaron a JMX Exporter, que tambi√©n proporciona m√©tricas de una manera c√≥moda.  Aquellos exportadores que no se encontraron "en la caja", nos escribimos en Python, hay unos diez de ellos.  Visualizamos a Grafana, recopilamos los registros con Graylog (ya que ahora es compatible con Beats). <br><br>  Utilizamos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Sentry</a> para recopilar errores.  Escribe todo de forma estructurada, dibuja gr√°ficos, muestra lo que sucedi√≥ con m√°s frecuencia, con menos frecuencia.  Por lo general, los desarrolladores van inmediatamente a Sentry inmediatamente despu√©s de la implementaci√≥n, para ver si hay alg√∫n pico o si necesitan revertirlo urgentemente.  Resulta detectar r√°pidamente los errores sin recoger los registros. <br><br>  Eso es todo por ahora, si el formato de los art√≠culos se adapta a los lectores, seguiremos hablando m√°s sobre nuestra infraestructura, todav√≠a hay mucha diversi√≥n: Kafka y soluciones anal√≠ticas para eventos que lo atraviesan, canal CI / CD para aplicaciones de Windows y aventuras con Openshift. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/443530/">https://habr.com/ru/post/443530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443520/index.html">Elegir un autom√≥vil para un especialista en TI, o consejos para teteras de una tetera</a></li>
<li><a href="../443522/index.html">Hosting: opciones, comparaciones, estad√≠sticas de usuario</a></li>
<li><a href="../443524/index.html">Animaciones Flash de bricolaje en Unity3D. Primera parte, l√≠rica</a></li>
<li><a href="../443526/index.html">De los algoritmos al c√°ncer: conferencias de la Escuela de Bioinform√°tica</a></li>
<li><a href="../443528/index.html">Amazon lanz√≥ Open Distro para Elasticsearch</a></li>
<li><a href="../443532/index.html">5 caracter√≠sticas de los polvos met√°licos para impresi√≥n 3D</a></li>
<li><a href="../443534/index.html">Compute Express Link - Interconexi√≥n para Big Data</a></li>
<li><a href="../443542/index.html">Punto de verificaci√≥n gratuito Introducci√≥n R80.20 Curso gratuito</a></li>
<li><a href="../443544/index.html">Perfiles de memoria en STM32 y otros microcontroladores: an√°lisis de tama√±o de pila est√°tica</a></li>
<li><a href="../443546/index.html">Toyota y JAXA planean tener un rover tripulado en la luna en 2029</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>