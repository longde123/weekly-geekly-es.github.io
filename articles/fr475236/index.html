<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🥑 👨🏼‍🔬 🤜🏼 N'ignorez plus jamais l'entraînement de renforcement. 🚇 🤟🏿 💜</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous présente la traduction de l'article «Ne plus jamais ignorer le renforcement de l'apprentissage» par Michel Kana, Ph.D. 

 Appre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>N'ignorez plus jamais l'entraînement de renforcement.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Bonjour, Habr!  Je vous présente la traduction de l'article «Ne plus jamais ignorer le renforcement de l'apprentissage» par Michel Kana, Ph.D. <br><br>  Apprendre avec un enseignant et apprendre sans enseignant n'est pas tout.  Tout le monde le sait.  Commencez avec OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="image"></div><br>  <i>Allez-vous vaincre le champion du monde d'échecs, le backgammon ou y aller?</i> <br><br>  Il existe un moyen qui vous permettra de le faire - la formation de renforcement. <br><a name="habracut"></a><br><h2>  Qu'est-ce que l'apprentissage par renforcement? </h2><br>  L'apprentissage renforcé consiste à apprendre à prendre des décisions cohérentes dans un environnement avec la récompense maximale reçue qui est donnée pour chaque action. <br><br>  Il n'y a aucun enseignant en lui, seulement un signal de récompense de l'environnement.  Le temps compte et les actions affectent les données suivantes.  Ces conditions créent des difficultés d'apprentissage avec ou sans enseignant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="image"></div><br>  Dans l'exemple ci-dessous, la souris essaie de trouver autant de nourriture que possible et évite les chocs électriques lorsque cela est possible. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="image"></div><br>  Une souris peut être courageuse et obtenir une décharge pour se rendre dans un endroit avec beaucoup de fromage.  Ce sera mieux que de rester immobile et de ne rien recevoir. <br><br>  La souris ne veut pas prendre les meilleures décisions dans chaque situation spécifique.  Cela nécessiterait une grande dépense mentale de sa part, et ce ne serait pas universel. <br><br>  L'entraînement avec des renforts fournit des ensembles de méthodes magiques qui permettent à notre souris d'apprendre à éviter les chocs électriques et à obtenir le plus de nourriture possible. <br><br>  La souris est un agent.  Un labyrinthe avec des murs, du fromage et des pistolets paralysants est <b>l'environnement</b> .  La souris peut se déplacer vers la gauche, la droite, le haut, le bas - ce sont des <b>actions</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="image"></div><br>  La souris veut du fromage, pas un choc électrique.  Le fromage est une <b>récompense</b> .  La souris peut inspecter l'environnement - ce sont des <b>observations</b> . <br><br><h2>  Formation au renforcement des glaces </h2><br>  Laissons la souris dans le labyrinthe et passons à la glace.  «L'hiver est arrivé.  Vous et vos amis jetiez du frisbee dans le parc lorsque vous avez soudainement lancé un frisbee au milieu du lac.  Fondamentalement, l'eau du lac était gelée, mais il y avait quelques trous où la glace a fondu.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">source</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="image"></div><br>  «Si vous marchez sur l'un des trous, vous tomberez dans l'eau glacée.  De plus, il y a une énorme pénurie de frisbee dans le monde, il est donc absolument essentiel de faire le tour du lac et de trouver une route. »( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> ) <br><br><h2>  Comment vous sentez-vous dans une situation similaire? </h2><br>  C'est un défi pour l'apprentissage par renforcement.  L'agent contrôle les mouvements du personnage dans le monde de la grille.  Certaines tuiles de grille sont passables, tandis que d'autres font tomber le personnage dans l'eau.  L'agent reçoit une récompense pour avoir trouvé un chemin praticable vers l'objectif. <br><br>  Nous pouvons simuler un tel environnement en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI Gym</a> - une boîte à outils pour développer et comparer des algorithmes d'apprentissage avec des renforts.  Il donne accès à un ensemble normalisé d'environnements, comme dans notre exemple, qui s'appelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Frozen Lake</a> .  Il s'agit d'un support de texte qui peut être créé avec quelques lignes de code. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Il nous faut maintenant une structure qui nous permettra d'aborder systématiquement les problèmes d'apprentissage par renforcement. <br><br><h2>  Processus décisionnel de Markov </h2><br>  Dans notre exemple, l'agent contrôle le mouvement du personnage sur le monde de la grille, et cet environnement est appelé un environnement entièrement observable. <br><br>  Puisque la future tuile ne dépend pas des tuiles passées en tenant compte de la tuile actuelle <br>  (nous avons affaire à une séquence d'états aléatoires, c'est-à-dire à la <b>propriété Markov</b> ), nous avons donc affaire au soi-disant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processus de Markov</a> . <br><br>  L'état actuel encapsule tout ce qui est nécessaire pour décider quel sera le prochain mouvement; rien n'est nécessaire pour se souvenir. <br><br>  Sur chaque cellule suivante (c'est-à-dire une situation), l'agent choisit avec une certaine probabilité l'action qui mène à la cellule suivante, c'est-à-dire la situation, et l'environnement répond à l'agent avec observation et récompense. <br><br>  Nous ajoutons la fonction de récompense et le coefficient de remise au processus de Markov, et nous obtenons le soi-disant <i>processus de récompense de Markov</i> .  En ajoutant un ensemble d'actions, nous obtenons <i>le processus décisionnel de Markov</i> ( <b>MDP</b> ).  Les composants de MDP sont décrits plus en détail ci-dessous. <br><br><h2>  Condition </h2><br>  Un état est une partie de l'environnement, une représentation numérique de ce que l'agent observe à un certain moment dans l'environnement, l'état de la grille du lac.  S est le point de départ, G est la cible, F est la glace solide sur laquelle l'agent peut se tenir et H est le trou dans lequel l'agent tombera s'il marche dessus.  Nous avons 16 états dans un environnement de grille 4 x 4, ou 64 états dans un environnement 8 x 8. Ci-dessous, nous allons dessiner un exemple d'un environnement 4 x 4 utilisant OpenAI Gym. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Actions </h2><br>  L'agent a 4 actions possibles, qui sont représentées dans l'environnement comme 0, 1, 2, 3 pour gauche, droite, bas, haut, respectivement. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="image"></div><br><h2>  Modèle de transition d'état </h2><br>  Le modèle de transition d'état décrit comment l'état de l'environnement change lorsqu'un agent prend des mesures en fonction de son état actuel. <br><br>  Le modèle est généralement décrit par la probabilité de transition, qui est exprimée comme une matrice de transition carrée de taille N x N, où N est le nombre d'états de notre modèle.  L'illustration ci-dessous est un exemple d'une telle matrice pour les conditions météorologiques. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="image"></div><br>  Dans l'environnement du lac gelé, nous supposons que le lac n'est pas glissant.  Si nous allons à droite, nous allons définitivement à droite.  Par conséquent, toutes les probabilités sont égales. <br><br>  «Gauche» déplace la cellule de l'agent 1 vers la gauche ou la laisse dans la même position si l'agent est à la bordure gauche. <br><br>  «Droite» la déplace d'une cellule vers la droite ou la laisse dans la même position si l'agent est à la frontière droite. <br><br>  «Up» déplace la cellule d'agent 1 vers le haut, ou l'agent reste au même endroit s'il se trouve à la limite supérieure. <br><br>  «Bas» déplace la cellule d'agent 1 vers le bas, ou elle reste au même endroit si elle est à la limite inférieure. <br><br><h2>  Rémunération </h2><br>  Dans chaque état F, l'agent reçoit 0 récompense; dans l'état H, il reçoit -1, puisque, passé dans cet état, l'agent décède.  Et lorsque l'agent atteint l'objectif, il reçoit une récompense +1. <br><br>  Étant donné que les deux modèles, le modèle de transition et le modèle de récompense, sont des fonctions déterministes, cela rend l'environnement déterministe.  \ <br><br><h2>  Remise </h2><br>  La remise est un paramètre facultatif qui contrôle l'importance des futures récompenses.  Il est mesuré dans la plage de 0 à 1. Le but de ce paramètre est d'empêcher la récompense totale d'aller à l'infini. <br><br>  La remise modélise également le comportement de l'agent lorsque l'agent préfère la récompense immédiate à la récompense qui pourrait être reçue à l'avenir. <br><br><h2>  Valeur </h2><br>  La valeur de la fortune est le revenu à long terme attendu avec une remise pour la fortune. <br><br><h2>  Politique (π) </h2><br>  La stratégie utilisée par l'agent pour sélectionner l'action suivante est appelée stratégie.  Parmi toutes les politiques disponibles, la meilleure est celle qui maximise le montant de la rémunération reçue ou attendue au cours de l'épisode. <br><br><h2>  Épisode </h2><br>  L'épisode commence lorsque l'agent apparaît sur la cellule de départ et se termine lorsque l'agent tombe dans le trou ou atteint la cellule cible. <br><br><h2>  Visualisons tout cela </h2><br>  Après avoir passé en revue tous les concepts impliqués dans le processus décisionnel de Markov, nous pouvons maintenant modéliser plusieurs actions aléatoires dans un environnement 16x16 en utilisant OpenAI Gym.  Chaque fois, l'agent sélectionne une action aléatoire et l'exécute.  Le système calcule la récompense et affiche le nouvel état de l'environnement. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="image"></div><br><h2>  Conclusion </h2><br>  Dans cet article, nous avons brièvement passé en revue les concepts de base de l'apprentissage par renforcement.  Notre exemple a fourni une introduction à la boîte à outils OpenAI Gym, qui facilite l'expérimentation d'environnements pré-construits. <br><br>  Dans la partie suivante, nous présenterons comment concevoir et mettre en œuvre des politiques qui permettront à l'agent de prendre un ensemble d'actions afin d'atteindre l'objectif et de recevoir un prix tel que vaincre un champion du monde. <br><br>  Merci de votre attention. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475236/">https://habr.com/ru/post/fr475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475212/index.html">Rechercher des données et des objets dans la base de données MS SQL Server à l'aide de l'utilitaire gratuit de recherche dbForge</a></li>
<li><a href="../fr475214/index.html">Quand une entreprise décède: comment survivre à la faillite</a></li>
<li><a href="../fr475218/index.html">Protocoles cryptographiques: définitions, enregistrements, propriétés, classification, attaques</a></li>
<li><a href="../fr475226/index.html">Stage à la Fondation Haxe</a></li>
<li><a href="../fr475228/index.html">Fourchette de paie. Vous êtes programmeur pour maman</a></li>
<li><a href="../fr475240/index.html">Utilisation de modules stricts dans des projets Python à grande échelle: expérience Instagram. Partie 1</a></li>
<li><a href="../fr475242/index.html">Utilisation de modules stricts dans des projets Python à grande échelle: expérience Instagram. 2e partie</a></li>
<li><a href="../fr475244/index.html">Nouvelles fonctionnalités JavaScript attendues que vous devez connaître</a></li>
<li><a href="../fr475246/index.html">Programmation asynchrone Python: un bref aperçu</a></li>
<li><a href="../fr475248/index.html">L'utilisation de polyfills lors de l'écriture d'applications inter-navigateurs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>