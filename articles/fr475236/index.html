<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ë üë®üèº‚Äçüî¨ ü§úüèº N'ignorez plus jamais l'entra√Ænement de renforcement. üöá ü§üüèø üíú</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous pr√©sente la traduction de l'article ¬´Ne plus jamais ignorer le renforcement de l'apprentissage¬ª par Michel Kana, Ph.D. 

 Appre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>N'ignorez plus jamais l'entra√Ænement de renforcement.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Bonjour, Habr!  Je vous pr√©sente la traduction de l'article ¬´Ne plus jamais ignorer le renforcement de l'apprentissage¬ª par Michel Kana, Ph.D. <br><br>  Apprendre avec un enseignant et apprendre sans enseignant n'est pas tout.  Tout le monde le sait.  Commencez avec OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="image"></div><br>  <i>Allez-vous vaincre le champion du monde d'√©checs, le backgammon ou y aller?</i> <br><br>  Il existe un moyen qui vous permettra de le faire - la formation de renforcement. <br><a name="habracut"></a><br><h2>  Qu'est-ce que l'apprentissage par renforcement? </h2><br>  L'apprentissage renforc√© consiste √† apprendre √† prendre des d√©cisions coh√©rentes dans un environnement avec la r√©compense maximale re√ßue qui est donn√©e pour chaque action. <br><br>  Il n'y a aucun enseignant en lui, seulement un signal de r√©compense de l'environnement.  Le temps compte et les actions affectent les donn√©es suivantes.  Ces conditions cr√©ent des difficult√©s d'apprentissage avec ou sans enseignant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="image"></div><br>  Dans l'exemple ci-dessous, la souris essaie de trouver autant de nourriture que possible et √©vite les chocs √©lectriques lorsque cela est possible. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="image"></div><br>  Une souris peut √™tre courageuse et obtenir une d√©charge pour se rendre dans un endroit avec beaucoup de fromage.  Ce sera mieux que de rester immobile et de ne rien recevoir. <br><br>  La souris ne veut pas prendre les meilleures d√©cisions dans chaque situation sp√©cifique.  Cela n√©cessiterait une grande d√©pense mentale de sa part, et ce ne serait pas universel. <br><br>  L'entra√Ænement avec des renforts fournit des ensembles de m√©thodes magiques qui permettent √† notre souris d'apprendre √† √©viter les chocs √©lectriques et √† obtenir le plus de nourriture possible. <br><br>  La souris est un agent.  Un labyrinthe avec des murs, du fromage et des pistolets paralysants est <b>l'environnement</b> .  La souris peut se d√©placer vers la gauche, la droite, le haut, le bas - ce sont des <b>actions</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="image"></div><br>  La souris veut du fromage, pas un choc √©lectrique.  Le fromage est une <b>r√©compense</b> .  La souris peut inspecter l'environnement - ce sont des <b>observations</b> . <br><br><h2>  Formation au renforcement des glaces </h2><br>  Laissons la souris dans le labyrinthe et passons √† la glace.  ¬´L'hiver est arriv√©.  Vous et vos amis jetiez du frisbee dans le parc lorsque vous avez soudainement lanc√© un frisbee au milieu du lac.  Fondamentalement, l'eau du lac √©tait gel√©e, mais il y avait quelques trous o√π la glace a fondu.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">source</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="image"></div><br>  ¬´Si vous marchez sur l'un des trous, vous tomberez dans l'eau glac√©e.  De plus, il y a une √©norme p√©nurie de frisbee dans le monde, il est donc absolument essentiel de faire le tour du lac et de trouver une route. ¬ª( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Source</a> ) <br><br><h2>  Comment vous sentez-vous dans une situation similaire? </h2><br>  C'est un d√©fi pour l'apprentissage par renforcement.  L'agent contr√¥le les mouvements du personnage dans le monde de la grille.  Certaines tuiles de grille sont passables, tandis que d'autres font tomber le personnage dans l'eau.  L'agent re√ßoit une r√©compense pour avoir trouv√© un chemin praticable vers l'objectif. <br><br>  Nous pouvons simuler un tel environnement en utilisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI Gym</a> - une bo√Æte √† outils pour d√©velopper et comparer des algorithmes d'apprentissage avec des renforts.  Il donne acc√®s √† un ensemble normalis√© d'environnements, comme dans notre exemple, qui s'appelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Frozen Lake</a> .  Il s'agit d'un support de texte qui peut √™tre cr√©√© avec quelques lignes de code. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Il nous faut maintenant une structure qui nous permettra d'aborder syst√©matiquement les probl√®mes d'apprentissage par renforcement. <br><br><h2>  Processus d√©cisionnel de Markov </h2><br>  Dans notre exemple, l'agent contr√¥le le mouvement du personnage sur le monde de la grille, et cet environnement est appel√© un environnement enti√®rement observable. <br><br>  Puisque la future tuile ne d√©pend pas des tuiles pass√©es en tenant compte de la tuile actuelle <br>  (nous avons affaire √† une s√©quence d'√©tats al√©atoires, c'est-√†-dire √† la <b>propri√©t√© Markov</b> ), nous avons donc affaire au soi-disant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">processus de Markov</a> . <br><br>  L'√©tat actuel encapsule tout ce qui est n√©cessaire pour d√©cider quel sera le prochain mouvement; rien n'est n√©cessaire pour se souvenir. <br><br>  Sur chaque cellule suivante (c'est-√†-dire une situation), l'agent choisit avec une certaine probabilit√© l'action qui m√®ne √† la cellule suivante, c'est-√†-dire la situation, et l'environnement r√©pond √† l'agent avec observation et r√©compense. <br><br>  Nous ajoutons la fonction de r√©compense et le coefficient de remise au processus de Markov, et nous obtenons le soi-disant <i>processus de r√©compense de Markov</i> .  En ajoutant un ensemble d'actions, nous obtenons <i>le processus d√©cisionnel de Markov</i> ( <b>MDP</b> ).  Les composants de MDP sont d√©crits plus en d√©tail ci-dessous. <br><br><h2>  Condition </h2><br>  Un √©tat est une partie de l'environnement, une repr√©sentation num√©rique de ce que l'agent observe √† un certain moment dans l'environnement, l'√©tat de la grille du lac.  S est le point de d√©part, G est la cible, F est la glace solide sur laquelle l'agent peut se tenir et H est le trou dans lequel l'agent tombera s'il marche dessus.  Nous avons 16 √©tats dans un environnement de grille 4 x 4, ou 64 √©tats dans un environnement 8 x 8. Ci-dessous, nous allons dessiner un exemple d'un environnement 4 x 4 utilisant OpenAI Gym. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Actions </h2><br>  L'agent a 4 actions possibles, qui sont repr√©sent√©es dans l'environnement comme 0, 1, 2, 3 pour gauche, droite, bas, haut, respectivement. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="image"></div><br><h2>  Mod√®le de transition d'√©tat </h2><br>  Le mod√®le de transition d'√©tat d√©crit comment l'√©tat de l'environnement change lorsqu'un agent prend des mesures en fonction de son √©tat actuel. <br><br>  Le mod√®le est g√©n√©ralement d√©crit par la probabilit√© de transition, qui est exprim√©e comme une matrice de transition carr√©e de taille N x N, o√π N est le nombre d'√©tats de notre mod√®le.  L'illustration ci-dessous est un exemple d'une telle matrice pour les conditions m√©t√©orologiques. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="image"></div><br>  Dans l'environnement du lac gel√©, nous supposons que le lac n'est pas glissant.  Si nous allons √† droite, nous allons d√©finitivement √† droite.  Par cons√©quent, toutes les probabilit√©s sont √©gales. <br><br>  ¬´Gauche¬ª d√©place la cellule de l'agent 1 vers la gauche ou la laisse dans la m√™me position si l'agent est √† la bordure gauche. <br><br>  ¬´Droite¬ª la d√©place d'une cellule vers la droite ou la laisse dans la m√™me position si l'agent est √† la fronti√®re droite. <br><br>  ¬´Up¬ª d√©place la cellule d'agent 1 vers le haut, ou l'agent reste au m√™me endroit s'il se trouve √† la limite sup√©rieure. <br><br>  ¬´Bas¬ª d√©place la cellule d'agent 1 vers le bas, ou elle reste au m√™me endroit si elle est √† la limite inf√©rieure. <br><br><h2>  R√©mun√©ration </h2><br>  Dans chaque √©tat F, l'agent re√ßoit 0 r√©compense; dans l'√©tat H, il re√ßoit -1, puisque, pass√© dans cet √©tat, l'agent d√©c√®de.  Et lorsque l'agent atteint l'objectif, il re√ßoit une r√©compense +1. <br><br>  √âtant donn√© que les deux mod√®les, le mod√®le de transition et le mod√®le de r√©compense, sont des fonctions d√©terministes, cela rend l'environnement d√©terministe.  \ <br><br><h2>  Remise </h2><br>  La remise est un param√®tre facultatif qui contr√¥le l'importance des futures r√©compenses.  Il est mesur√© dans la plage de 0 √† 1. Le but de ce param√®tre est d'emp√™cher la r√©compense totale d'aller √† l'infini. <br><br>  La remise mod√©lise √©galement le comportement de l'agent lorsque l'agent pr√©f√®re la r√©compense imm√©diate √† la r√©compense qui pourrait √™tre re√ßue √† l'avenir. <br><br><h2>  Valeur </h2><br>  La valeur de la fortune est le revenu √† long terme attendu avec une remise pour la fortune. <br><br><h2>  Politique (œÄ) </h2><br>  La strat√©gie utilis√©e par l'agent pour s√©lectionner l'action suivante est appel√©e strat√©gie.  Parmi toutes les politiques disponibles, la meilleure est celle qui maximise le montant de la r√©mun√©ration re√ßue ou attendue au cours de l'√©pisode. <br><br><h2>  √âpisode </h2><br>  L'√©pisode commence lorsque l'agent appara√Æt sur la cellule de d√©part et se termine lorsque l'agent tombe dans le trou ou atteint la cellule cible. <br><br><h2>  Visualisons tout cela </h2><br>  Apr√®s avoir pass√© en revue tous les concepts impliqu√©s dans le processus d√©cisionnel de Markov, nous pouvons maintenant mod√©liser plusieurs actions al√©atoires dans un environnement 16x16 en utilisant OpenAI Gym.  Chaque fois, l'agent s√©lectionne une action al√©atoire et l'ex√©cute.  Le syst√®me calcule la r√©compense et affiche le nouvel √©tat de l'environnement. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="image"></div><br><h2>  Conclusion </h2><br>  Dans cet article, nous avons bri√®vement pass√© en revue les concepts de base de l'apprentissage par renforcement.  Notre exemple a fourni une introduction √† la bo√Æte √† outils OpenAI Gym, qui facilite l'exp√©rimentation d'environnements pr√©-construits. <br><br>  Dans la partie suivante, nous pr√©senterons comment concevoir et mettre en ≈ìuvre des politiques qui permettront √† l'agent de prendre un ensemble d'actions afin d'atteindre l'objectif et de recevoir un prix tel que vaincre un champion du monde. <br><br>  Merci de votre attention. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475236/">https://habr.com/ru/post/fr475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475212/index.html">Rechercher des donn√©es et des objets dans la base de donn√©es MS SQL Server √† l'aide de l'utilitaire gratuit de recherche dbForge</a></li>
<li><a href="../fr475214/index.html">Quand une entreprise d√©c√®de: comment survivre √† la faillite</a></li>
<li><a href="../fr475218/index.html">Protocoles cryptographiques: d√©finitions, enregistrements, propri√©t√©s, classification, attaques</a></li>
<li><a href="../fr475226/index.html">Stage √† la Fondation Haxe</a></li>
<li><a href="../fr475228/index.html">Fourchette de paie. Vous √™tes programmeur pour maman</a></li>
<li><a href="../fr475240/index.html">Utilisation de modules stricts dans des projets Python √† grande √©chelle: exp√©rience Instagram. Partie 1</a></li>
<li><a href="../fr475242/index.html">Utilisation de modules stricts dans des projets Python √† grande √©chelle: exp√©rience Instagram. 2e partie</a></li>
<li><a href="../fr475244/index.html">Nouvelles fonctionnalit√©s JavaScript attendues que vous devez conna√Ætre</a></li>
<li><a href="../fr475246/index.html">Programmation asynchrone Python: un bref aper√ßu</a></li>
<li><a href="../fr475248/index.html">L'utilisation de polyfills lors de l'√©criture d'applications inter-navigateurs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>