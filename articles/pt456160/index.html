<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßõüèø ‚úäüèª üë©üèº‚Äçü§ù‚Äçüë®üèΩ Aprendizado por refor√ßo ou estrat√©gias evolutivas? - Isso e outro üî≥ ü§ô ‚úîÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! 

 Raramente decidimos publicar aqui tradu√ß√µes de textos h√° dois anos, sem c√≥digo e foco claramente acad√™mico - mas hoje abriremos uma exce√ß...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizado por refor√ßo ou estrat√©gias evolutivas? - Isso e outro</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/456160/">  Ol√° Habr! <br><br>  Raramente decidimos publicar aqui tradu√ß√µes de textos h√° dois anos, sem c√≥digo e foco claramente acad√™mico - mas hoje abriremos uma exce√ß√£o.  Esperamos que o dilema apresentado no t√≠tulo do artigo excite muitos de nossos leitores, e voc√™ j√° leu o trabalho original ou agora l√™ o trabalho fundamental sobre estrat√©gias evolutivas com as quais este post √© polemizado.  Bem-vindo ao gato! <br><br><img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br><a name="habracut"></a><br>  Em mar√ßo de 2017, a OpenAI fez barulho na comunidade de aprendizagem profunda, publicando o artigo " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Estrat√©gias de evolu√ß√£o como uma alternativa escal√°vel ao aprendizado por refor√ßo</a> ".  Neste trabalho, resultados impressionantes foram descritos em favor do fato de a luz n√£o convergir no treinamento com refor√ßo (RL), e √© aconselh√°vel tentar outros m√©todos ao treinar redes neurais complexas.  Em seguida, surgiu uma discuss√£o sobre a import√¢ncia do aprendizado refor√ßado e o quanto ele merece o status da tecnologia "obrigat√≥ria" para aprender a resolver problemas.  Aqui, quero falar sobre o fato de que voc√™ n√£o deve considerar essas duas tecnologias como concorrentes, uma das quais √© claramente melhor que a outra;  pelo contr√°rio, eles acabam se complementando.  De fato, se voc√™ pensar um pouco sobre o que √© necess√°rio para criar uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">IA comum</a> e esses sistemas que, ao longo de sua exist√™ncia, seriam capazes de aprender, julgar e planejar, ent√£o quase certamente chegaremos √† conclus√£o de que essa ou aquela solu√ß√£o combinada ser√° necess√°ria .  A prop√≥sito, foi a natureza que chegou √† decis√£o combinada, que dotou a intelig√™ncia complexa de mam√≠feros e outros animais superiores durante a evolu√ß√£o. <br><br><h4>  Estrat√©gias evolutivas </h4><br>  A tese principal do artigo da OpenAI foi que, em vez de usar o aprendizado por refor√ßo combinado com a retropropaga√ß√£o tradicional, eles treinaram com sucesso a rede neural para resolver problemas complexos usando a chamada "estrat√©gia evolutiva" (ES).  Essa abordagem de ES consiste em manter a distribui√ß√£o dos valores de peso em uma balan√ßa de rede, com muitos agentes trabalhando em paralelo e usando par√¢metros selecionados nessa distribui√ß√£o.  Cada agente opera em seu pr√≥prio ambiente e, ap√≥s a conclus√£o de um determinado n√∫mero de epis√≥dios ou etapas de um epis√≥dio, o algoritmo retorna uma recompensa total, expressa como uma pontua√ß√£o de condicionamento f√≠sico.  Dado esse valor, a distribui√ß√£o dos par√¢metros pode ser deslocada para agentes mais bem-sucedidos, privando os menos bem-sucedidos.  Milh√µes de vezes repetindo essa opera√ß√£o envolvendo centenas de agentes, √© poss√≠vel mover a distribui√ß√£o de pesos para um espa√ßo que nos permita formular uma pol√≠tica de qualidade para os agentes resolverem sua tarefa.  De fato, os resultados apresentados no artigo s√£o impressionantes: √© mostrado que, se voc√™ executar mil agentes em paralelo, o movimento antropom√≥rfico em duas pernas poder√° ser estudado em menos de meia hora (enquanto at√© os m√©todos mais avan√ßados de RL exigem mais de uma hora).  Para uma revis√£o mais detalhada, recomendo a leitura de um excelente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">post</a> dos autores do experimento, bem como do pr√≥prio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo cient√≠fico</a> . <br><br><img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br><br>  <i>V√°rias estrat√©gias de aprendizado para postura ereta antropom√≥rfica, estudadas pelo m√©todo ES da OpenAI.</i> <br><br><h4>  Caixa preta </h4><br>  O grande benef√≠cio desse m√©todo √© que √© f√°cil paralelizar.  Embora os m√©todos RL, por exemplo, A3C, exijam a troca de informa√ß√µes entre fluxos de trabalho e o servidor de par√¢metros, o ES precisa apenas de estimativas de validade e informa√ß√µes generalizadas sobre a distribui√ß√£o de par√¢metros.  Gra√ßas a essa simplicidade, esse m√©todo ignora os m√©todos modernos de RL em escalabilidade.  No entanto, tudo isso n√£o √© em v√£o: voc√™ precisa otimizar a rede com base no princ√≠pio de uma caixa preta.  Nesse caso, a "caixa preta" significa que durante o treinamento a estrutura interna da rede √© completamente ignorada e apenas o resultado geral (recompensa pelo epis√≥dio) √© usado, e depende se os pesos de uma rede espec√≠fica ser√£o herdados pelas gera√ß√µes futuras.  Em situa√ß√µes em que n√£o obtemos um feedback pronunciado do ambiente - e na solu√ß√£o de muitas tarefas tradicionais relacionadas √† RL, o fluxo de recompensa √© muito rarefeito - o problema passa de uma "caixa preta parcialmente" para uma "caixa preta completamente".  Nesse caso, √© poss√≠vel aumentar seriamente a produtividade, portanto, √© claro, esse compromisso √© justificado.  "Quem precisa de gradientes se eles ainda s√£o irremediavelmente barulhentos?"  - esta √© a opini√£o geral. <br><br>  No entanto, em situa√ß√µes em que o feedback √© mais ativo, os assuntos de ES est√£o come√ßando a dar errado.  A equipe do OpenAI descreve como a rede simples de classifica√ß√£o MNIST foi treinada usando ES, e dessa vez o treinamento foi 1000 vezes mais lento.  O fato √© que o sinal de gradiente na classifica√ß√£o das imagens √© extremamente informativo sobre como ensinar √† rede uma melhor classifica√ß√£o.  Assim, o problema est√° associado n√£o tanto √† t√©cnica RL, como a recompensas esparsas em ambientes que produzem gradientes ruidosos. <br><br><h4>  Solu√ß√£o encontrada por natureza </h4><br>  Se voc√™ tentar aprender com o exemplo da natureza, pensando em maneiras de desenvolver a IA, em alguns casos a IA pode ser representada como uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">abordagem orientada</a> a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">problemas</a> .  No final, a natureza opera dentro de tais limita√ß√µes que os cientistas da computa√ß√£o simplesmente n√£o t√™m.  H√° uma opini√£o de que uma abordagem puramente te√≥rica para resolver um problema espec√≠fico pode fornecer solu√ß√µes mais eficazes do que alternativas emp√≠ricas.  Ainda assim, acho que seria aconselh√°vel verificar como um sistema din√¢mico que opera sob condi√ß√µes de certas restri√ß√µes (Terra) forma agentes (animais, em particular mam√≠feros), capazes de comportamentos flex√≠veis e complexos.  Embora algumas dessas limita√ß√µes n√£o sejam aplic√°veis ‚Äã‚Äãnos mundos simulados da ci√™ncia de dados, outras s√£o muito boas. <br><br>  Tendo examinado o comportamento intelectual dos mam√≠feros, vemos que ele √© formado como resultado da complexa intera√ß√£o de dois processos intimamente relacionados: <i>aprender com a experi√™ncia de outras pessoas</i> e <i>aprender com nossa pr√≥pria experi√™ncia</i> .  O primeiro √© frequentemente identificado com a evolu√ß√£o devido √† sele√ß√£o natural, mas aqui utilizo um termo mais amplo para levar em conta epigen√©tica, microbiomas e outros mecanismos que garantem a troca de experi√™ncias entre organismos que n√£o s√£o geneticamente relacionados entre si.  O segundo processo, o aprendizado em primeira m√£o, √© toda a informa√ß√£o que um animal consegue assimilar ao longo da vida, e essa informa√ß√£o est√° diretamente relacionada √† intera√ß√£o desse animal com o mundo exterior.  Esta categoria inclui tudo, desde aprender a reconhecer objetos at√© dominar a comunica√ß√£o inerente ao processo educacional. <br><br>  Grosso modo, esses dois processos que ocorrem na natureza podem ser comparados com duas op√ß√µes para otimizar redes neurais.  Estrat√©gias evolutivas, nas quais informa√ß√µes de gradiente s√£o usadas para atualizar informa√ß√µes sobre o corpo, aproximam-se do aprendizado da experi√™ncia de outra pessoa.  Da mesma forma, os m√©todos de gradiente, nos quais o recebimento de uma experi√™ncia espec√≠fica leva a uma ou outra mudan√ßa no comportamento do agente, s√£o compar√°veis ‚Äã‚Äãao aprendizado da experi√™ncia.  Se voc√™ pensar nas variedades de comportamento intelectual ou nas habilidades que cada uma dessas duas abordagens desenvolve nos animais, essa compara√ß√£o √© mais pronunciada.  Nos dois casos, os ‚Äúm√©todos evolutivos‚Äù contribuem para o estudo de comportamentos reativos que permitem o desenvolvimento de uma certa aptid√£o (suficiente para permanecer vivo).  Aprender a andar ou escapar do cativeiro em muitos casos √© equivalente a comportamentos mais "instintivos" que s√£o "conectados" em muitos animais no n√≠vel gen√©tico.  Al√©m disso, este exemplo confirma que os m√©todos evolutivos s√£o aplic√°veis ‚Äã‚Äãnos casos em que a recompensa do sinal √© extremamente rara (como, por exemplo, o fato da cria√ß√£o bem-sucedida de um filhote).  Nesse caso, √© imposs√≠vel correlacionar a recompensa com qualquer conjunto espec√≠fico de a√ß√µes que possam ter sido cometidas muitos anos antes do in√≠cio desse fato.  Por outro lado, se considerarmos o caso em que o SE falha, a classifica√ß√£o das imagens, os resultados ser√£o extraordinariamente compar√°veis ‚Äã‚Äãaos resultados do treinamento com animais obtido no decorrer de in√∫meras experi√™ncias psicol√≥gicas comportamentais realizadas em mais de 100 anos. <br><br><h4>  Treinamento animal </h4><br>  Os m√©todos utilizados no aprendizado por refor√ßo s√£o, em muitos casos, retirados diretamente da literatura psicol√≥gica sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">condicionamento operante</a> , e o condicionamento operante foi estudado com base na psicologia animal.  A prop√≥sito, Richard Sutton, um dos dois fundadores do treinamento por refor√ßo, √© bacharel em psicologia.  No contexto do condicionamento operante, os animais aprendem a associar recompensa ou puni√ß√£o a padr√µes comportamentais espec√≠ficos.  Formadores e pesquisadores podem, de alguma forma, manipular essa associa√ß√£o com recompensas, provocando animais a mostrar engenhosidade ou certos comportamentos.  No entanto, o condicionamento operante usado no estudo de animais nada mais √© do que uma forma mais pronunciada desse condicionamento, com base na qual os animais s√£o treinados ao longo da vida.  Recebemos constantemente sinais positivos de refor√ßo do ambiente e ajustamos nosso comportamento de acordo.  De fato, muitos neurofisiologistas e cientistas cognitivos acreditam que, de fato, pessoas e outros animais agem um n√≠vel acima e aprendem constantemente a prever os resultados de seu comportamento em situa√ß√µes futuras, contando com poss√≠veis recompensas. <br><br>  O papel central da previs√£o no auto-estudo est√° mudando a din√¢mica descrita acima da maneira mais significativa.  O sinal anteriormente considerado muito rarefeito (recompensa epis√≥dica) √© muito denso.  Teoricamente, a situa√ß√£o √© aproximadamente a seguinte: a cada momento, o c√©rebro dos mam√≠feros calcula os resultados com base em um fluxo complexo de est√≠mulos e a√ß√µes sensoriais, enquanto o animal √© simplesmente imerso nesse fluxo.  Nesse caso, o comportamento final do animal emite um sinal denso, que deve ser guiado pela corre√ß√£o das previs√µes e pelo desenvolvimento do comportamento.  O c√©rebro usa todos esses sinais para otimizar previs√µes (e, consequentemente, a qualidade das a√ß√µes tomadas) no futuro.  Uma vis√£o geral dessa abordagem √© apresentada no excelente livro ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Surf Incerteza</a> ‚Äù do cientista cognitivo e fil√≥sofo Andy Clark.  Se extrapolamos esses argumentos para o treinamento de agentes artificiais, o treinamento de refor√ßo revela uma falha fundamental: o sinal usado nesse paradigma √© irremediavelmente fraco em compara√ß√£o com o que poderia ser (ou deveria ser).  Nos casos em que √© imposs√≠vel aumentar a satura√ß√£o do sinal (talvez porque seja, por defini√ß√£o, fraco ou esteja associado √† reatividade de baixo n√≠vel) - provavelmente √© melhor preferir um m√©todo de treinamento bem paralelizado, por exemplo, ES. <br><br><h4>  Melhor aprendizado de redes neurais </h4><br>  Com base nos princ√≠pios de maior atividade nervosa inerente ao c√©rebro dos mam√≠feros, que est√° constantemente envolvido na previs√£o, ultimamente tem sido poss√≠vel obter certos sucessos no treinamento de refor√ßo, que agora leva em considera√ß√£o a import√¢ncia de tais previs√µes.  Posso recomendar dois trabalhos semelhantes: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendendo a agir prevendo o futuro</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado por refor√ßo com tarefas auxiliares n√£o supervisionadas</a> </li></ul><br>  Em ambos os artigos, os autores complementam as pol√≠ticas t√≠picas de redes neurais padr√£o com resultados previstos em rela√ß√£o a condi√ß√µes ambientais futuras.  No primeiro artigo, a previs√£o √© aplicada a uma variedade de vari√°veis ‚Äã‚Äãde medi√ß√£o e, no segundo, mudan√ßas no ambiente e no comportamento do agente como tal.  Nos dois casos, o sinal escasso associado ao refor√ßo positivo se torna muito mais saturado e informativo, proporcionando aprendizado acelerado e assimila√ß√£o de modelos comportamentais mais complexos.  Essas melhorias est√£o dispon√≠veis apenas ao trabalhar com m√©todos que usam o sinal de gradiente, mas n√£o com m√©todos que operam com o princ√≠pio de "caixa preta", como, por exemplo, ES. <br><br>  Al√©m disso, os m√©todos de aprendizado em primeira m√£o e gradiente s√£o muito mais eficazes.  Mesmo naqueles casos em que era poss√≠vel estudar um problema espec√≠fico usando o m√©todo ES em vez de usar treinamento de refor√ßo, o ganho foi alcan√ßado devido ao fato de muitas vezes mais dados estarem envolvidos na estrat√©gia ES do que com RL.  Pensando neste caso sobre os princ√≠pios do treinamento em animais, observamos que o resultado do treinamento em um exemplo estrangeiro se manifesta ap√≥s muitas gera√ß√µes, enquanto √†s vezes um √∫nico evento experimentado pela pr√≥pria experi√™ncia √© suficiente para o animal aprender a li√ß√£o para sempre.  Embora esse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">treinamento sem exemplos</a> ainda n√£o se encaixe totalmente nos m√©todos tradicionais de gradiente, √© muito mais intelig√≠vel que o ES.  Existem, por exemplo, abordagens como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">controle neural epis√≥dico</a> , onde os valores Q s√£o armazenados durante o processo de treinamento, ap√≥s o qual o programa √© verificado com eles antes de executar a√ß√µes.  Acontece que um m√©todo gradiente permite aprender a resolver problemas muito mais rapidamente do que antes.  No artigo sobre controle neural epis√≥dico, os autores mencionam o hipocampo humano, capaz de armazenar informa√ß√µes sobre o evento mesmo ap√≥s uma experi√™ncia vivida e, portanto, desempenha um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">papel cr√≠tico</a> no processo de recall.  Tais mecanismos requerem acesso √† organiza√ß√£o interna do agente, o que tamb√©m √©, por defini√ß√£o, imposs√≠vel no paradigma de SE. <br><br><h4>  Ent√£o, por que n√£o combin√°-los? </h4><br>  Provavelmente, a maior parte deste artigo poderia ter deixado a impress√£o de que eu estava defendendo m√©todos de RL nele.  No entanto, acredito que, a longo prazo, a melhor solu√ß√£o seria uma combina√ß√£o dos dois m√©todos, de modo que cada um seja usado nas situa√ß√µes em que √© mais adequado.  Obviamente, no caso de muitas pol√≠ticas reativas ou em situa√ß√µes com sinais muito escassos de refor√ßo positivo, o ES vence, especialmente se voc√™ tiver o poder de computa√ß√£o no qual pode executar um treinamento paralelo em massa.  Por outro lado, m√©todos gradientes que usam aprendizado refor√ßado ou treinamento de professores ser√£o √∫teis quando um feedback abrangente estiver dispon√≠vel, e a solu√ß√£o do problema precisa ser aprendida rapidamente e com menos dados. <br><br>  Voltando √† natureza, descobrimos que o primeiro m√©todo, em ess√™ncia, estabelece as bases para o segundo.  √â por isso que, durante a evolu√ß√£o, os mam√≠feros desenvolveram um c√©rebro que permite um aprendizado extremamente eficiente a partir do material de sinais complexos vindos do ambiente.  Ent√£o, a quest√£o permanece em aberto.  Talvez as estrat√©gias evolutivas nos ajudem a inventar arquiteturas eficazes de aprendizado que ser√£o √∫teis para m√©todos de aprendizado gradiente.  Afinal, a solu√ß√£o encontrada pela natureza √© realmente muito bem-sucedida. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456160/">https://habr.com/ru/post/pt456160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456150/index.html">Petty little joy # 4: Radon - qualidade do c√≥digo medida em n√∫meros</a></li>
<li><a href="../pt456152/index.html">Padr√µes de Design de N√≠vel para Jogos 2D</a></li>
<li><a href="../pt456154/index.html">Recursos principais do UX e MVP ao criar um produto</a></li>
<li><a href="../pt456156/index.html">√â por isso que a √°lgebra escolar √© necess√°ria.</a></li>
<li><a href="../pt456158/index.html">Um pouco sobre fontes de combust√≠vel nuclear</a></li>
<li><a href="../pt456162/index.html">Aurora, uma empresa fundada por imigrantes do Google, Tesla e Uber, come√ßou a trabalhar com preocupa√ß√µes com autom√≥veis</a></li>
<li><a href="../pt456164/index.html">Bal√µes Loon fornecem conex√£o de emerg√™ncia √† rede e √† Internet no Peru ap√≥s um terremoto de magnitude - 8.0</a></li>
<li><a href="../pt456168/index.html">Onde estava sua casa h√° milh√µes de anos?</a></li>
<li><a href="../pt456170/index.html">Como criar um aplicativo financeiro: 5 APIs para ajudar o desenvolvedor</a></li>
<li><a href="../pt456172/index.html">Parte 2: RocketChip: conectando RAM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>