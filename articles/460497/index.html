<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìë üßó ‚úçüèø Uso intuitivo de los m√©todos de Monte Carlo con cadenas de Markov. üé∑ üë©üèæ‚Äçüéì ü§≤üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="¬øEs f√°cil? Lo intent√© 
 Alexey Kuzmin, director de desarrollo de datos y trabajo en DomKlik, profesor de Data Science en Netology, tradujo un art√≠culo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Uso intuitivo de los m√©todos de Monte Carlo con cadenas de Markov.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  ¬øEs f√°cil?  Lo intent√© </h4><br>  <i>Alexey Kuzmin, director de desarrollo de datos y trabajo en DomKlik, profesor de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Data Science</a> en Netology, tradujo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un art√≠culo de</a> Rahul Agarwal sobre c√≥mo los m√©todos de Monte Carlo trabajan con las cadenas de Markov para resolver problemas con un gran espacio de estado.</i> <br><a name="habracut"></a><br>  Todos los asociados con Data Science han o√≠do hablar de los m√©todos de Monte Carlo con cadenas de Markov (MCMC).  A veces se aborda el tema al estudiar estad√≠sticas bayesianas, a veces al trabajar con herramientas como Prophet. <br><br>  Pero MCMC es dif√≠cil de entender.  Cada vez que leo sobre estos m√©todos, noto que la esencia de MCMC est√° oculta en las capas profundas de ruido matem√°tico, y es dif√≠cil distinguir detr√°s de este ruido.  Tuve que pasar muchas horas entendiendo este concepto. <br><br>  En este art√≠culo, est√° disponible un intento de explicar los m√©todos de Monte Carlo con cadenas de Markov, para que quede claro para qu√© se usan.  Me centrar√© en algunas formas m√°s de usar estos m√©todos en mi pr√≥xima publicaci√≥n. <br><br>  Entonces comencemos.  MCMC consta de dos t√©rminos: cadenas Monte Carlo y Markov.  Hablemos de cada uno de ellos. <br><br><h2>  Montecarlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  En los t√©rminos m√°s simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">, los m√©todos de Monte Carlo</a> se pueden definir como simulaciones simples. <br><br>  Monte Carlo Methods obtuvo su nombre del Casino Monte Carlo en M√≥naco.  En muchos juegos de cartas necesitas saber la probabilidad de ganar el crupier.  Algunas veces el c√°lculo de esta probabilidad puede ser matem√°ticamente complicado o intratable.  Pero siempre podemos ejecutar una simulaci√≥n por computadora para jugar todo el juego muchas veces y considerar la probabilidad como el n√∫mero de victorias dividido por el n√∫mero de juegos jugados. <br>  Esto es todo lo que necesita saber sobre los m√©todos de Monte Carlo.  S√≠, es solo una t√©cnica de modelado simple con un nombre elegante. <br><br><h2>  Cadenas de Markov </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Dado que el t√©rmino MCMC consta de dos partes, a√∫n debe comprender qu√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">son las cadenas de Markov</a> .  Pero antes de pasar a las cadenas de Markov, hablemos un poco sobre las propiedades de Markov. <br><br>  Supongamos que hay un sistema de estados M-posibles, y usted se mueve de un estado a otro.  Que nada te confunda todav√≠a.  Un ejemplo espec√≠fico de dicho sistema es el clima, que cambia de caliente a fr√≠o a moderado.  Otro ejemplo es el mercado de valores, que est√° saltando de un estado bajista a un estado alcista y estancado. <br><br>  <i>La propiedad de Markov</i> sugiere que para un proceso dado que se encuentra en el estado X <sub>n</sub> en un momento particular en el tiempo, la probabilidad X <sub>n + 1</sub> = k (donde k es cualquiera de los estados M a los que puede ir el proceso) depende solo de ¬øCu√°l es esta condici√≥n en este momento?  Y no sobre c√≥mo lleg√≥ a su estado actual. <br>  En t√©rminos matem√°ticos, podemos escribir esto en la forma de la siguiente f√≥rmula: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Para mayor claridad, no le importa la secuencia de condiciones que tom√≥ el mercado para volverse optimista.  La probabilidad de que el pr√≥ximo estado sea "bajista" est√° determinada solo por el hecho de que el mercado se encuentra actualmente en un estado "alcista".  Tambi√©n tiene sentido en la pr√°ctica. <br><br>  Un proceso con una propiedad de Markov se llama proceso de Markov.  ¬øPor qu√© es importante la cadena de Markov?  Debido a su distribuci√≥n estacionaria. <br><br><h3>  ¬øQu√© es la distribuci√≥n estacionaria? </h3><br>  Tratar√© de explicar la distribuci√≥n estacionaria al calcularla para el siguiente ejemplo.  Supongamos que tiene un proceso de Markov para el mercado de valores, como se muestra a continuaci√≥n. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Tiene una matriz de probabilidad de transici√≥n que determina la probabilidad de una transici√≥n del estado X <sub>i</sub> a X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Matriz de probabilidad de transici√≥n, Q <br><br>  En la matriz de probabilidad de transici√≥n dada Q, la probabilidad de que el pr√≥ximo estado sea "toro", dado el estado actual de "toro" = 0.9;  la probabilidad de que el pr√≥ximo estado sea "bajista" si el estado actual es "toro" = 0.075.  Y as√≠ sucesivamente. <br><br>  Bueno, comencemos con un estado particular.  Nuestro estado ser√° establecido por el vector [toro, oso, estancamiento].  Si comenzamos con un estado "bajista", el vector ser√° as√≠: [0,1,0].  Podemos calcular la distribuci√≥n de probabilidad para el siguiente estado multiplicando el vector de estado actual por la matriz de probabilidad de transici√≥n. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Tenga en cuenta que las probabilidades suman 1.</b> <br><br>  La f√≥rmula puede encontrar la siguiente distribuci√≥n de estados: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  Y as√≠ sucesivamente.  Al final, alcanzar√° un estado estacionario en el que el estado se estabiliza: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  Para la matriz de probabilidad de transici√≥n Q descrita anteriormente, la distribuci√≥n estacionaria s es <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Puede obtener una distribuci√≥n estacionaria con el siguiente c√≥digo: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Tambi√©n puede comenzar desde cualquier otro estado: logre la misma distribuci√≥n estacionaria.  Cambie el estado inicial en el c√≥digo si desea asegurarse de esto. <br><br>  Ahora podemos responder a la pregunta de por qu√© la distribuci√≥n estacionaria es tan importante. <br><br>  La distribuci√≥n estacionaria es importante porque puede usarse para determinar la probabilidad de que un sistema est√© en un cierto estado al azar. <br><br>  Para nuestro ejemplo, podemos decir que en el 62.5% de los casos el mercado estar√° en un estado "alcista", el 31.25% en un estado "bajista" y el 6.25% en estancamiento. <br><br>  Intuitivamente, puedes ver esto como un azar deambulando por la cadena. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Paseo al azar <br><br>  Est√° en un cierto punto y elige el siguiente estado, observando la distribuci√≥n de probabilidad del siguiente estado, teniendo en cuenta el estado actual.  Podemos visitar algunos nodos con m√°s frecuencia que otros, seg√∫n las probabilidades de estos nodos. <br><br>  As√≠ es como Google resolvi√≥ el problema de b√∫squeda en los albores de Internet.  El problema fue ordenar las p√°ginas, dependiendo de su importancia.  Google resolvi√≥ el problema utilizando el algoritmo Pagerank.  El algoritmo Google Pagerank deber√≠a considerar el estado como una p√°gina, y la probabilidad de una p√°gina en una distribuci√≥n estacionaria como su importancia relativa. <br><br>  Ahora pasamos directamente a la consideraci√≥n de los m√©todos MCMC. <br><br><h2>  ¬øCu√°les son los m√©todos de Monte Carlo con cadenas de Markov (MCMC) </h2><br>  Antes de responder qu√© es MCMC, d√©jame hacerte una pregunta.  Sabemos acerca de la distribuci√≥n beta.  Conocemos su funci√≥n de densidad de probabilidad.  ¬øPero podemos tomar una muestra de esta distribuci√≥n?  ¬øPuedes encontrar una manera de hacer esto? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Piensa ... <br><br>  MCMC le permite elegir entre cualquier distribuci√≥n de probabilidad.  Esto es especialmente importante cuando necesita hacer una selecci√≥n desde la distribuci√≥n posterior. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  La figura muestra el teorema de Bayes. <br><br>  Por ejemplo, necesita hacer una muestra de una distribuci√≥n posterior.  Pero, ¬øes f√°cil calcular el componente posterior junto con la constante de normalizaci√≥n (evidencia)?  En la mayor√≠a de los casos, puede encontrarlos en forma de un producto de probabilidad y probabilidad a priori.  Pero calcular la constante de normalizaci√≥n (p (D)) no funciona.  Por qu√©  Echemos un vistazo m√°s de cerca. <br><br>  Suponga que H solo toma 3 valores: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  En este caso, p (D) es f√°cil de calcular.  Pero, ¬øqu√© pasa si el valor de H es continuo?  ¬øSer√≠a posible calcular esto tan f√°cilmente, especialmente si H tomara valores infinitos?  Para hacer esto, una integral compleja tendr√≠a que ser resuelta. <br><br>  Queremos hacer una selecci√≥n aleatoria de la distribuci√≥n posterior, pero tambi√©n queremos considerar p (D) como una constante. <br><br>  Wikipedia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escribe</a> : <br><br>  Los m√©todos de Monte Carlo con cadenas de Markov son una clase de algoritmos para el muestreo de una distribuci√≥n de probabilidad, basada en la construcci√≥n de una cadena de Markov, que como distribuci√≥n estacionaria tiene la forma deseada.  El estado de la cadena despu√©s de una serie de pasos se usa como una selecci√≥n de la distribuci√≥n deseada.  La calidad de muestreo mejora con el n√∫mero creciente de pasos. <br><br>  Veamos un ejemplo.  Digamos que necesita una muestra de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">distribuci√≥n beta</a> .  Su densidad: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  donde C es la constante de normalizaci√≥n.  En realidad, esta es una funci√≥n de Œ± y Œ≤, pero quiero mostrar que no es necesaria para una muestra de la distribuci√≥n beta, por lo que la tomaremos como una constante. <br><br>  El problema de distribuci√≥n beta es realmente dif√≠cil, si no pr√°cticamente insoluble.  En realidad, es posible que deba trabajar con funciones de distribuci√≥n m√°s complejas y, a veces, no conocer√° las constantes de normalizaci√≥n. <br><br>  Los m√©todos MCMC facilitan la vida al proporcionar algoritmos que podr√≠an crear una cadena de Markov que tiene una distribuci√≥n beta como una distribuci√≥n estacionaria, dado que podemos elegir entre una distribuci√≥n uniforme (que es relativamente simple). <br><br>  Si comenzamos con un estado aleatorio y pasamos al siguiente estado sobre la base de alg√∫n algoritmo varias veces, finalmente crearemos una cadena de Markov con una distribuci√≥n beta como distribuci√≥n estacionaria.  Y los estados en los que nos encontramos durante mucho tiempo pueden usarse como muestra de la distribuci√≥n beta. <br><br>  Uno de estos algoritmos MCMC es el algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Metropolis-Hastings.</a> <br><br><h2>  Algoritmo Metr√≥polis-Hastings </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuici√≥n </h3><br>  Entonces, ¬øcu√°l es el prop√≥sito? <br><br>  <i>Intuitivamente, queremos caminar a lo largo de alg√∫n pedazo de superficie (nuestra cadena de Markov) de tal manera que la cantidad de tiempo que pasamos en cada ubicaci√≥n sea proporcional a la altura de la superficie en ese lugar (la densidad de probabilidad deseada de la que queremos hacer una selecci√≥n).</i> <i><br><br></i>  <i>Entonces, por ejemplo, nos gustar√≠a pasar el doble de tiempo en una colina de 100 metros de altura que en una colina vecina de 50 metros.</i>  <i>Es bueno que podamos hacer esto incluso si no conocemos las alturas absolutas de los puntos en la superficie: todo lo que necesita saber son las alturas relativas.</i>  <i>Por ejemplo, si la cima de la colina A es dos veces m√°s alta que la cima de la colina B, entonces nos gustar√≠a pasar el doble de tiempo en A que en B.</i> <i><br><br></i>  <i>Existen esquemas m√°s complejos para proponer nuevos lugares y reglas para su adopci√≥n, pero la idea principal es la siguiente:</i> <i><br><br></i> <ol><li>  <i>Elija una nueva ubicaci√≥n "sugerida".</i> </li><li>  <i>Descubra qu√© tan alta o baja es esta ubicaci√≥n en comparaci√≥n con la actual.</i> </li><li>  <i>Permanecer en el lugar o mudarse a un nuevo lugar con una probabilidad proporcional a las alturas de los lugares.</i> </li></ol> <i><br></i>  <i>El prop√≥sito del MCMC es seleccionar de alguna distribuci√≥n de probabilidad sin tener que saber su altura exacta en ning√∫n punto (no es necesario saber C).</i> <i><br></i>  <i>Si el proceso de "deambular" est√° configurado correctamente, puede asegurarse de que se logre esta proporcionalidad (entre el tiempo empleado y la altura de distribuci√≥n)</i> . <br><br><h3>  Algoritmo </h3><br>  Ahora definamos y describamos la tarea en t√©rminos m√°s formales.  Sea s = (s1, s2, ..., sM) la distribuci√≥n estacionaria deseada.  Queremos crear una cadena de Markov con una distribuci√≥n tan estacionaria.  Comenzamos con una cadena de Markov arbitraria con estados M con la matriz de transici√≥n P, de modo que pij representa la probabilidad de transici√≥n del estado i a j. <br><br>  Intuitivamente, sabemos c√≥mo recorrer la cadena de Markov, pero la cadena de Markov no tiene la distribuci√≥n estacionaria requerida.  Esta cadena tiene alguna distribuci√≥n estacionaria (que no necesitamos).  Nuestro objetivo es cambiar la forma en que deambulamos por la cadena de Markov para que la cadena tenga la distribuci√≥n estacionaria deseada. <br><br>  Para hacer esto: <br><br><ol><li>  Comience con un estado inicial aleatorio i. </li><li>  Seleccione aleatoriamente un nuevo estado asumido mirando las probabilidades de transici√≥n en la i-√©sima fila de la matriz de transici√≥n P. </li><li>  Calcule una medida llamada probabilidad de decisi√≥n, que se define como: aij = min (sj.pji / si.pij, 1). </li><li>  Ahora lanza una moneda, que aterriza en la superficie del √°guila con probabilidad aij.  Si un √°guila cae, acepte la oferta, es decir, vaya al siguiente estado, de lo contrario, rechace la oferta, es decir, permanezca en el estado actual. <br></li><li>  Repite muchas veces. <br></li></ol><br>  Despu√©s de una gran cantidad de pruebas, esta cadena converger√° y tendr√° una distribuci√≥n estacionaria.  Entonces podemos usar estados de cadena como muestra de cualquier distribuci√≥n. <br><br>  Al hacer esto para muestrear la distribuci√≥n beta, el √∫nico momento en que debe usar la densidad de probabilidad es buscar la probabilidad de tomar una decisi√≥n.  Para hacer esto, divida sj por si (es decir, la constante de normalizaci√≥n C se cancela). <br><br><h3>  Selecci√≥n Beta </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Ahora pasamos al problema del muestreo de la distribuci√≥n beta. <br><br>  Una distribuci√≥n beta es una distribuci√≥n continua en [0,1] y puede tener valores infinitos en [0,1].  Suponga que una cadena de Markov arbitraria P con estados infinitos en [0,1] tiene una matriz de transici√≥n P tal que pij = pji = todos los elementos en la matriz. <br><br>  No necesitamos la Matriz P, como veremos m√°s adelante, pero quiero que la descripci√≥n del problema sea lo m√°s cercana posible al algoritmo que propusimos. <br><br><ul><li>  Comience con un estado inicial aleatorio que obtuve de una distribuci√≥n uniforme en (0,1). </li><li>  Seleccione aleatoriamente un nuevo estado asumido observando las probabilidades de transici√≥n en la i-√©sima fila de la matriz de transici√≥n P. Supongamos que elegimos otro estado Unif (0,1) como el estado asumido j. </li><li>  Calcule la medida, que se llama la probabilidad de tomar una decisi√≥n: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  Lo que se simplifica a: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Desde pji = pij, y donde <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Ahora tira una moneda.  Con probabilidad aij caer√° un √°guila.  Si cae un √°guila, entonces debe aceptar la oferta, es decir, pasar al siguiente estado.  De lo contrario, vale la pena rechazar la oferta, es decir, permanecer en el mismo estado. </li><li>  Repita la prueba muchas veces. </li></ul><br><h3>  C√≥digo: </h3><br>  Es hora de pasar de la teor√≠a a la pr√°ctica.  Escribiremos nuestra muestra beta en Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Compare los resultados con la distribuci√≥n beta real. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Como puede ver, los valores son muy similares a la distribuci√≥n beta.  Por lo tanto, la red MCMC ha alcanzado un estado estacionario <br><br>  En el c√≥digo anterior, creamos una muestra beta, pero el mismo concepto se aplica a cualquier otra distribuci√≥n de la que queramos hacer una selecci√≥n. <br><br><h2>  Conclusiones </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  Fue una gran publicaci√≥n.  Felicitaciones si lo lees hasta el final. <br><br>  En esencia, los m√©todos MCMC pueden ser complejos, pero nos brindan una gran flexibilidad.  Puede seleccionar desde cualquier funci√≥n de distribuci√≥n utilizando la selecci√≥n a trav√©s de MCMC.  T√≠picamente, estos m√©todos se usan para tomar muestras de distribuciones posteriores. <br><br>  Tambi√©n puede usar MCMC para resolver problemas con un gran espacio de estado.  Por ejemplo, en un problema de mochila o para descifrar.  Intentar√© brindarte ejemplos m√°s interesantes en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pr√≥xima</a> publicaci√≥n.  Est√©n atentos. <br><br><h2>  De los editores </h2><br><ul><li>  Curso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Python para trabajar con datos</a> </li><li>  Curso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprendizaje autom√°tico en</a> l√≠nea </li><li>  Curso en l√≠nea " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">GRANDES DATOS desde cero</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/460497/">https://habr.com/ru/post/460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460485/index.html">C√≥mo un torneo en l√≠nea puede desalentar el "terminar la pr√≥xima semana"</a></li>
<li><a href="../460489/index.html">TOP 11 errores en el desarrollo de BCP</a></li>
<li><a href="../460491/index.html">Sensor de temperatura y humedad Arduino con env√≠o y trazado (Parte 1)</a></li>
<li><a href="../460493/index.html">"Aplicaciones asesinas" para PC de los a√±os 80: VisiCalc y WordStar</a></li>
<li><a href="../460495/index.html">Contenedor a canalizaci√≥n: CRI-O ahora es el valor predeterminado en OpenShift Container Platform 4</a></li>
<li><a href="../460499/index.html">Tres ganadores del Premio Dijkstra: ¬øc√≥mo fueron Hydra 2019 y SPTDC 2019?</a></li>
<li><a href="../460501/index.html">Ejemplo de implementaci√≥n de integraci√≥n continua con BuildBot</a></li>
<li><a href="../460503/index.html">Configuraci√≥n inal√°mbrica de Raspberry PI 3 B +</a></li>
<li><a href="../460505/index.html">Atrae tres cruces, o por qu√© los proyectos son tan dif√≠ciles de terminar a tiempo</a></li>
<li><a href="../460507/index.html">XEN y el futuro de la automoci√≥n: c√≥mo un hipervisor de c√≥digo abierto se convierte en un competidor de las soluciones automotrices comerciales</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>