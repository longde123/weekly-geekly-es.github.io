<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüë©‚Äçüëß‚Äçüë¶ üï¥üèº üéπ G√©n√©ration d'images √† partir de texte √† l'aide d'AttnGAN üë¥ ü§ôüèª üö®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous pr√©sente la traduction de l'article " AttnGAN: Texte √† grain fin √† la g√©n√©ration d'images avec des r√©seaux contradictoires g√©n√©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>G√©n√©ration d'images √† partir de texte √† l'aide d'AttnGAN</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/424957/">  Bonjour, Habr!  Je vous pr√©sente la traduction de l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AttnGAN: Texte √† grain fin √† la g√©n√©ration d'images avec des r√©seaux</a> contradictoires <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">g√©n√©ratifs attentionnels</a> " par Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He. <br><br>  Dans cette publication, je veux parler de mes exp√©riences avec l'architecture AttnGAN pour g√©n√©rer des images √† partir d'une description textuelle.  Cette architecture a d√©j√† √©t√© mentionn√©e sur Habr√© apr√®s la sortie de l'article original d√©but 2018, et je m'int√©ressais √† la question - √† quel point sera-t-il difficile de former soi-m√™me un tel mod√®le? <br><br><h3>  Description de l'architecture </h3><br>  Pour ceux qui ne connaissent pas AttnGAN et le GAN classique, je vais bri√®vement d√©crire l'essentiel.  Le GAN classique comprend au moins 2 r√©seaux de neurones - un g√©n√©rateur et un discriminateur.  La t√¢che du g√©n√©rateur est de g√©n√©rer des donn√©es (images, texte, audio, vid√©o, etc.) qui sont ¬´similaires¬ª aux donn√©es r√©elles de l'ensemble de donn√©es.  La t√¢che du discriminateur est d'√©valuer les donn√©es g√©n√©r√©es, une tentative de les comparer avec les vraies et de les rejeter.  Le r√©sultat rejet√© du travail du g√©n√©rateur le stimule √† g√©n√©rer le meilleur r√©sultat afin de ¬´tromper¬ª le discriminateur, qui, √† son tour, apprend √† mieux reconna√Ætre les contrefa√ßons. <br><br>  Il y a beaucoup de modifications du GAN, et les auteurs d'AttnGAN ont abord√© la question de l'architecture de mani√®re tr√®s inventive.  Le mod√®le se compose de 9 r√©seaux de neurones qui sont finement r√©gl√©s pour l'interaction.  Cela ressemble √† ceci: <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/tk/eq/nq/tkeqnqzoqcw9dt9giju0rdsh4zg.png"><br><br>  Les encodeurs de texte et d'image (encodeur de texte / image) convertissent la description originale du texte et les images r√©elles en une sorte de repr√©sentation interne.  De mani√®re caract√©ristique, dans ce cas, le texte est consid√©r√© comme une s√©quence de mots s√©par√©s, dont la pr√©sentation est trait√©e avec la repr√©sentation de l'image, ce qui vous permet de comparer des mots individuels √† des parties individuelles de l'image.  De cette fa√ßon, le m√©canisme d'attention appel√© par les auteurs de l'article DAMSM est impl√©ment√©. <br><br>  Fca - cr√©e une repr√©sentation concise de la sc√®ne globale dans l'image, bas√©e sur la description de texte enti√®re.  La valeur de C √† la sortie est concat√©n√©e avec un vecteur de la distribution normale de Z, qui d√©termine la variabilit√© de la sc√®ne.  Ces informations sont √† la base du fonctionnement du g√©n√©rateur. <br><br>  Le g√©n√©rateur est le plus grand r√©seau compos√© de trois niveaux.  Chaque niveau produit des images de r√©solution croissante, de 64 * 64 √† 256 * 256 pixels, et le r√©sultat du travail √† chaque niveau est ajust√© √† l'aide des r√©seaux d'attention Fattn, qui v√©hiculent des informations sur l'emplacement correct des objets individuels dans la sc√®ne.  De plus, les r√©sultats √† chaque niveau sont v√©rifi√©s par trois discriminateurs travaillant s√©par√©ment qui √©valuent le r√©alisme de l'image et sa coh√©rence avec l'id√©e g√©n√©rale de la sc√®ne. <br><br><h3>  La formation </h3><br>  Pour tester l'architecture, j'ai utilis√© l'ensemble de donn√©es CUB standard avec des photos et des descriptions textuelles d'oiseaux. <br><br>  L'ensemble du mod√®le est form√© en deux √©tapes.  La premi√®re √©tape est la pr√©-formation des r√©seaux DAMSM, qui consiste en un encodeur de texte et d'image.  Au cours de cette √©tape, comme d√©crit ci-dessus, une ¬´carte d'attention¬ª est cr√©√©e qui ressemble √† ceci: <br><br><img src="https://habrastorage.org/webt/of/w7/6f/ofw76fxdvl6bbuohon_okmsgfro.png"><br><br>  Comme le montre la figure, DAMSM parvient √† capturer tr√®s pr√©cis√©ment la relation entre les mots individuels de la description du texte et les √©l√©ments de l'image, il est particuli√®rement facile pour le mod√®le de reconna√Ætre les couleurs.  Je dois dire que le syst√®me ne dispose d'aucune information suppl√©mentaire sur ce qu'est ¬´rouge¬ª, ¬´jaune¬ª ou ¬´ailes¬ª, ¬´bec¬ª.  Il n'y a qu'un ensemble de textes et d'images. <br><br>  La formation DAMSM se d√©roule sans aucun probl√®me, le temps de formation sur cet ensemble de donn√©es est de 150-200 √®res, ce qui correspond √† plusieurs heures sur un GPU haute puissance. <br><br>  La deuxi√®me et principale √©tape est la formation du g√©n√©rateur √† l'aide du mod√®le DAMSM. <br>  Le g√©n√©rateur √† chaque niveau g√©n√®re une image de r√©solution plus √©lev√©e - cela ressemble √† ceci: <br><br><img src="https://habrastorage.org/webt/xz/go/cw/xzgocw0eswwfeku7kxogp2jhuqy.png"><br><br>  La formation des g√©n√©rateurs prend beaucoup plus de temps et n'est pas toujours aussi stable, le temps de formation recommand√© sur cet ensemble de donn√©es est de 300 √† 600 √©poques, ce qui correspond √† environ 4 √† 8 jours sur un GPU haute puissance. <br><br>  Le principal probl√®me dans la formation du g√©n√©rateur, √† mon avis, est le manque de m√©triques suffisamment bonnes qui nous permettraient d'√©valuer la qualit√© de la formation de mani√®re plus formelle.  J'ai √©tudi√© plusieurs impl√©mentations du score Inception, qui, en th√©orie, se positionne comme une m√©trique universelle pour de telles t√¢ches - mais elles ne m'ont pas sembl√© suffisamment convaincantes.  Si vous d√©cidez de former un tel g√©n√©rateur, vous devrez constamment surveiller visuellement la progression de la formation, en fonction des r√©sultats interm√©diaires.  Cependant, cette r√®gle est vraie pour de telles t√¢ches, un contr√¥le visuel est toujours n√©cessaire. <br><br><h3>  R√©sultats </h3><br>  Maintenant, la partie amusante.  En utilisant le mod√®le form√©, nous allons essayer de g√©n√©rer des images, nous commencerons par des phrases simples: <br><br><img src="https://habrastorage.org/webt/7g/ri/x-/7grix-945iwxoysnibzph4yjd0w.png"><br><br>  Essayons des descriptions plus complexes: <br><br><img src="https://habrastorage.org/webt/8n/kp/eu/8nkpeuqwf4wiqk_c6fn8bynmxiq.png"><br><br>  Toutes les descriptions de texte sont invent√©es, je n'ai intentionnellement pas utilis√© de phrases du jeu de donn√©es pour les tests.  Bien s√ªr, toutes ces images n'ont pas √©t√© obtenues du premier coup.  Le mod√®le est erron√©, les auteurs eux-m√™mes en parlent.  √Ä mesure que le texte de description et les √©l√©ments √† afficher augmentent, il devient de plus en plus difficile de maintenir le r√©alisme de l'ensemble de la sc√®ne.  Cependant, si vous souhaitez utiliser quelque chose de similaire dans la production, par exemple, g√©n√©rer des images de certains objets pour un concepteur, vous pouvez former et personnaliser le syst√®me selon vos besoins, ce qui peut √™tre assez strict. <br><br>  Pour chaque description de texte, vous pouvez g√©n√©rer de nombreuses options d'image (y compris celles qui ne sont pas r√©alistes), il y aura donc toujours beaucoup de choix. <br><br><h3>  D√©tails techniques </h3><br>  Dans ce travail, j'ai utilis√© un GPU basse puissance pour le prototypage et un serveur Google Cloud avec Tesla K80 install√© pendant la phase de formation. <br><br>  Le code source a √©t√© extrait du r√©f√©rentiel des auteurs de l'article et a subi une refactorisation s√©rieuse.  Le syst√®me a √©t√© test√© en Python 3.6 avec Pytorch 0.4.1 <br><br>  Merci de votre attention! <br><br>  <i>Article original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AttnGAN: Texte √† grain fin √† la g√©n√©ration d'images avec des r√©seaux</a> contradictoires <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">g√©n√©ratifs attentionnels</a> , 2018 - Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr424957/">https://habr.com/ru/post/fr424957/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr424945/index.html">Le smartphone conduit une petite voiture.</a></li>
<li><a href="../fr424947/index.html">Reconnaissance des gestes avec APDS-9960</a></li>
<li><a href="../fr424949/index.html">PHP Digest n ¬∞ 140 (17-30 septembre 2018)</a></li>
<li><a href="../fr424951/index.html">Hourra! Ce n'√©tait pas de la parano√Øa</a></li>
<li><a href="../fr424955/index.html">Le condens√© de mati√®res fra√Æches du monde du front-end de la derni√®re semaine n ¬∞ 332 (24-30 septembre 2018)</a></li>
<li><a href="../fr424961/index.html">MTA-STS pour Postfix</a></li>
<li><a href="../fr424963/index.html">Financement Zuckerberg: cr√©er ensemble des outils pour la science</a></li>
<li><a href="../fr424965/index.html">D√©veloppement d'applications React √† l'aide de ReasonReact</a></li>
<li><a href="../fr424967/index.html">Raccourcis JavaScript pour les d√©butants</a></li>
<li><a href="../fr424969/index.html">Guide Node.js, partie 9: Utilisation du syst√®me de fichiers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>