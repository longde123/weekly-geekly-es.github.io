<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩‍👩‍👧‍👦 🕴🏼 🎹 Génération d'images à partir de texte à l'aide d'AttnGAN 👴 🤙🏻 🚨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bonjour, Habr! Je vous présente la traduction de l'article " AttnGAN: Texte à grain fin à la génération d'images avec des réseaux contradictoires géné...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Génération d'images à partir de texte à l'aide d'AttnGAN</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/424957/">  Bonjour, Habr!  Je vous présente la traduction de l'article " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AttnGAN: Texte à grain fin à la génération d'images avec des réseaux</a> contradictoires <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">génératifs attentionnels</a> " par Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He. <br><br>  Dans cette publication, je veux parler de mes expériences avec l'architecture AttnGAN pour générer des images à partir d'une description textuelle.  Cette architecture a déjà été mentionnée sur Habré après la sortie de l'article original début 2018, et je m'intéressais à la question - à quel point sera-t-il difficile de former soi-même un tel modèle? <br><br><h3>  Description de l'architecture </h3><br>  Pour ceux qui ne connaissent pas AttnGAN et le GAN classique, je vais brièvement décrire l'essentiel.  Le GAN classique comprend au moins 2 réseaux de neurones - un générateur et un discriminateur.  La tâche du générateur est de générer des données (images, texte, audio, vidéo, etc.) qui sont «similaires» aux données réelles de l'ensemble de données.  La tâche du discriminateur est d'évaluer les données générées, une tentative de les comparer avec les vraies et de les rejeter.  Le résultat rejeté du travail du générateur le stimule à générer le meilleur résultat afin de «tromper» le discriminateur, qui, à son tour, apprend à mieux reconnaître les contrefaçons. <br><br>  Il y a beaucoup de modifications du GAN, et les auteurs d'AttnGAN ont abordé la question de l'architecture de manière très inventive.  Le modèle se compose de 9 réseaux de neurones qui sont finement réglés pour l'interaction.  Cela ressemble à ceci: <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/tk/eq/nq/tkeqnqzoqcw9dt9giju0rdsh4zg.png"><br><br>  Les encodeurs de texte et d'image (encodeur de texte / image) convertissent la description originale du texte et les images réelles en une sorte de représentation interne.  De manière caractéristique, dans ce cas, le texte est considéré comme une séquence de mots séparés, dont la présentation est traitée avec la représentation de l'image, ce qui vous permet de comparer des mots individuels à des parties individuelles de l'image.  De cette façon, le mécanisme d'attention appelé par les auteurs de l'article DAMSM est implémenté. <br><br>  Fca - crée une représentation concise de la scène globale dans l'image, basée sur la description de texte entière.  La valeur de C à la sortie est concaténée avec un vecteur de la distribution normale de Z, qui détermine la variabilité de la scène.  Ces informations sont à la base du fonctionnement du générateur. <br><br>  Le générateur est le plus grand réseau composé de trois niveaux.  Chaque niveau produit des images de résolution croissante, de 64 * 64 à 256 * 256 pixels, et le résultat du travail à chaque niveau est ajusté à l'aide des réseaux d'attention Fattn, qui véhiculent des informations sur l'emplacement correct des objets individuels dans la scène.  De plus, les résultats à chaque niveau sont vérifiés par trois discriminateurs travaillant séparément qui évaluent le réalisme de l'image et sa cohérence avec l'idée générale de la scène. <br><br><h3>  La formation </h3><br>  Pour tester l'architecture, j'ai utilisé l'ensemble de données CUB standard avec des photos et des descriptions textuelles d'oiseaux. <br><br>  L'ensemble du modèle est formé en deux étapes.  La première étape est la pré-formation des réseaux DAMSM, qui consiste en un encodeur de texte et d'image.  Au cours de cette étape, comme décrit ci-dessus, une «carte d'attention» est créée qui ressemble à ceci: <br><br><img src="https://habrastorage.org/webt/of/w7/6f/ofw76fxdvl6bbuohon_okmsgfro.png"><br><br>  Comme le montre la figure, DAMSM parvient à capturer très précisément la relation entre les mots individuels de la description du texte et les éléments de l'image, il est particulièrement facile pour le modèle de reconnaître les couleurs.  Je dois dire que le système ne dispose d'aucune information supplémentaire sur ce qu'est «rouge», «jaune» ou «ailes», «bec».  Il n'y a qu'un ensemble de textes et d'images. <br><br>  La formation DAMSM se déroule sans aucun problème, le temps de formation sur cet ensemble de données est de 150-200 ères, ce qui correspond à plusieurs heures sur un GPU haute puissance. <br><br>  La deuxième et principale étape est la formation du générateur à l'aide du modèle DAMSM. <br>  Le générateur à chaque niveau génère une image de résolution plus élevée - cela ressemble à ceci: <br><br><img src="https://habrastorage.org/webt/xz/go/cw/xzgocw0eswwfeku7kxogp2jhuqy.png"><br><br>  La formation des générateurs prend beaucoup plus de temps et n'est pas toujours aussi stable, le temps de formation recommandé sur cet ensemble de données est de 300 à 600 époques, ce qui correspond à environ 4 à 8 jours sur un GPU haute puissance. <br><br>  Le principal problème dans la formation du générateur, à mon avis, est le manque de métriques suffisamment bonnes qui nous permettraient d'évaluer la qualité de la formation de manière plus formelle.  J'ai étudié plusieurs implémentations du score Inception, qui, en théorie, se positionne comme une métrique universelle pour de telles tâches - mais elles ne m'ont pas semblé suffisamment convaincantes.  Si vous décidez de former un tel générateur, vous devrez constamment surveiller visuellement la progression de la formation, en fonction des résultats intermédiaires.  Cependant, cette règle est vraie pour de telles tâches, un contrôle visuel est toujours nécessaire. <br><br><h3>  Résultats </h3><br>  Maintenant, la partie amusante.  En utilisant le modèle formé, nous allons essayer de générer des images, nous commencerons par des phrases simples: <br><br><img src="https://habrastorage.org/webt/7g/ri/x-/7grix-945iwxoysnibzph4yjd0w.png"><br><br>  Essayons des descriptions plus complexes: <br><br><img src="https://habrastorage.org/webt/8n/kp/eu/8nkpeuqwf4wiqk_c6fn8bynmxiq.png"><br><br>  Toutes les descriptions de texte sont inventées, je n'ai intentionnellement pas utilisé de phrases du jeu de données pour les tests.  Bien sûr, toutes ces images n'ont pas été obtenues du premier coup.  Le modèle est erroné, les auteurs eux-mêmes en parlent.  À mesure que le texte de description et les éléments à afficher augmentent, il devient de plus en plus difficile de maintenir le réalisme de l'ensemble de la scène.  Cependant, si vous souhaitez utiliser quelque chose de similaire dans la production, par exemple, générer des images de certains objets pour un concepteur, vous pouvez former et personnaliser le système selon vos besoins, ce qui peut être assez strict. <br><br>  Pour chaque description de texte, vous pouvez générer de nombreuses options d'image (y compris celles qui ne sont pas réalistes), il y aura donc toujours beaucoup de choix. <br><br><h3>  Détails techniques </h3><br>  Dans ce travail, j'ai utilisé un GPU basse puissance pour le prototypage et un serveur Google Cloud avec Tesla K80 installé pendant la phase de formation. <br><br>  Le code source a été extrait du référentiel des auteurs de l'article et a subi une refactorisation sérieuse.  Le système a été testé en Python 3.6 avec Pytorch 0.4.1 <br><br>  Merci de votre attention! <br><br>  <i>Article original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AttnGAN: Texte à grain fin à la génération d'images avec des réseaux</a> contradictoires <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">génératifs attentionnels</a> , 2018 - Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He.</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr424957/">https://habr.com/ru/post/fr424957/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr424945/index.html">Le smartphone conduit une petite voiture.</a></li>
<li><a href="../fr424947/index.html">Reconnaissance des gestes avec APDS-9960</a></li>
<li><a href="../fr424949/index.html">PHP Digest n ° 140 (17-30 septembre 2018)</a></li>
<li><a href="../fr424951/index.html">Hourra! Ce n'était pas de la paranoïa</a></li>
<li><a href="../fr424955/index.html">Le condensé de matières fraîches du monde du front-end de la dernière semaine n ° 332 (24-30 septembre 2018)</a></li>
<li><a href="../fr424961/index.html">MTA-STS pour Postfix</a></li>
<li><a href="../fr424963/index.html">Financement Zuckerberg: créer ensemble des outils pour la science</a></li>
<li><a href="../fr424965/index.html">Développement d'applications React à l'aide de ReasonReact</a></li>
<li><a href="../fr424967/index.html">Raccourcis JavaScript pour les débutants</a></li>
<li><a href="../fr424969/index.html">Guide Node.js, partie 9: Utilisation du système de fichiers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>