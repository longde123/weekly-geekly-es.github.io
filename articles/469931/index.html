<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🚣🏾 ♌️ 🧑🏼‍🤝‍🧑🏼 ¿Qué es un factor de velocidad de aprendizaje y cómo mejora las características de aprendizaje profundo? 👌 🤷🏿 👶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este artículo es mi intento de expresar mi opinión sobre los siguientes aspectos: 



1. ¿Qué es un factor de velocidad de aprendizaje y cuál es su va...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>¿Qué es un factor de velocidad de aprendizaje y cómo mejora las características de aprendizaje profundo?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/469931/">  Este artículo es mi intento de expresar mi opinión sobre los siguientes aspectos: <br><br><ol><li>  ¿Qué es un factor de velocidad de aprendizaje y cuál es su valor? </li><li>  ¿Cómo elegir este coeficiente al entrenar modelos? </li><li>  ¿Por qué es necesario cambiar el coeficiente de la velocidad de aprendizaje durante el entrenamiento de modelos? </li><li>  ¿Qué hacer con un factor de velocidad de aprendizaje cuando se usa un modelo pre-entrenado? </li></ol><br>  La mayor parte de esta publicación se basa en materiales preparados por <i>fast.ai</i> : [1], [2], [5] y [3], que representan una versión concisa de su trabajo destinada a la comprensión más rápida de la esencia del problema.  Para familiarizarse con los detalles, se recomienda hacer clic en los enlaces que figuran a continuación. <br><a name="habracut"></a><br><h3>  <b>¿Qué es un factor de velocidad de aprendizaje?</b> </h3><br>  El coeficiente de velocidad de aprendizaje es un hiperparámetro que determina el orden de cómo ajustaremos nuestras escalas teniendo en cuenta la función de pérdida en el descenso de gradiente.  Cuanto más bajo es el valor, más lento nos movemos a lo largo de la inclinación.  Aunque cuando se utiliza un bajo coeficiente de velocidad de aprendizaje, podemos obtener un efecto positivo en el sentido de que no perdemos un mínimo local, esto también puede significar que tendremos que pasar mucho tiempo en la convergencia, especialmente si estamos en la región de la meseta. <br><br>  La relación se ilustra con la siguiente fórmula <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mo>&amp;#x2022;</mo><mi>n</mi><mi>u</mi><mi>e</mi><mi>v</mi><msub><mi>o</mi><mi>p</mi></msub><mi>e</mi><mi>s</mi><mi>o</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><msub><mi>e</mi><mi>p</mi></msub><mi>e</mi><mi>s</mi><mi>o</mi><mo>&amp;#x2212;</mo><mi>t</mi><mi>a</mi><mi>s</mi><mi>a</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>j</mi><mi>e</mi><mo>&amp;#x2217;</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="62.435ex" height="2.66ex" viewBox="0 -780.1 26881.5 1145.2" role="img" focusable="false" style="vertical-align: -0.848ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMAIN-2219" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-75" x="1101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="1673" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-76" x="2140" y="0"></use><g transform="translate(2625,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6F" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-70" x="686" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="3567" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-73" x="4033" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6F" x="4503" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMAIN-3D" x="5266" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="6322" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-78" x="6789" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-69" x="7361" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-73" x="7707" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-74" x="8176" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="8538" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6E" x="9004" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-74" x="9605" y="0"></use><g transform="translate(9966,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-70" x="659" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="10889" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-73" x="11355" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6F" x="11825" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMAIN-2212" x="12532" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-74" x="13533" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-61" x="13895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-73" x="14424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-61" x="14894" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-64" x="15423" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="15947" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-61" x="16413" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-70" x="16943" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-72" x="17446" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="17898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6E" x="18364" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-64" x="18965" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-69" x="19488" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-7A" x="19834" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-61" x="20302" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6A" x="20832" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="21244" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMAIN-2217" x="21933" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-67" x="22656" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-72" x="23136" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-61" x="23588" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-64" x="24117" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-69" x="24641" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="24986" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-6E" x="25453" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-74" x="26053" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/469931/&amp;usg=ALkJrhjImPzDJyS0XffE5FhnzsEgBjXcAw#MJMATHI-65" x="26415" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo>•</mo><mi>n</mi><mi>u</mi><mi>e</mi><mi>v</mi><msub><mi>o</mi><mi>p</mi></msub><mi>e</mi><mi>s</mi><mi>o</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><msub><mi>e</mi><mi>p</mi></msub><mi>e</mi><mi>s</mi><mi>o</mi><mo>−</mo><mi>t</mi><mi>a</mi><mi>s</mi><mi>a</mi><mi>d</mi><mi>e</mi><mi>a</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>z</mi><mi>a</mi><mi>j</mi><mi>e</mi><mo>∗</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>d</mi><mi>i</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> • nuevo_peso = existente_peso - tasa de aprendizaje * gradiente </script></p><br><img src="https://habrastorage.org/webt/dn/j1/nj/dnj1njm2womrahwlbv_dzs25xqs.jpeg"><br>  <b>Descenso de gradiente con factores de velocidad de aprendizaje pequeños (arriba) y grandes (abajo).</b>  <b>Fuente: curso de aprendizaje automático de Andrew Ng en Coursera</b> <b><br></b> <br>  Muy a menudo, el factor de velocidad de aprendizaje lo establece el usuario arbitrariamente.  En el mejor de los casos, para una comprensión intuitiva de qué valor es el más adecuado para determinar el coeficiente de velocidad de aprendizaje, puede confiar en experimentos previos (u otro tipo de material de capacitación). <br><br>  Esencialmente, es lo suficientemente difícil elegir el valor correcto.  El siguiente diagrama ilustra varios escenarios que pueden surgir cuando el usuario ajusta independientemente la velocidad de aprendizaje. <br><br><img src="https://habrastorage.org/webt/qm/uh/qd/qmuhqdtbcagnnzltgnfy53wuxxi.jpeg"><br>  <b>La influencia de varios factores de tasa de aprendizaje en la convergencia.</b>  <b>(Crédito Img: cs231n)</b> <b><br></b> <br>  Además, el factor de velocidad de aprendizaje afecta la rapidez con que nuestro modelo alcanza un mínimo local (también conocido como logrará la mejor precisión).  Por lo tanto, la elección correcta desde el principio garantiza menos pérdida de tiempo para entrenar el modelo.  Cuanto menos tiempo de entrenamiento, menos dinero se gasta en la potencia informática de la GPU en la nube. <br><br><h4>  ¿Hay alguna forma más conveniente de determinar la tasa de coeficiente de aprendizaje? <br></h4><br>  En el apartado 3.3.  " <i>Coeficientes de tasa de aprendizaje cíclico para redes neuronales</i> " Leslie Smith defendió lo siguiente: la eficiencia del aprendizaje se puede estimar entrenando un modelo con una velocidad de aprendizaje baja inicialmente establecida, que luego aumenta (lineal o exponencialmente) en cada iteración. <br><br><img src="https://habrastorage.org/webt/j9/zu/yi/j9zuyi5do_thph6ylxtaxetvm7q.jpeg"><br>  <b>El factor de velocidad de aprendizaje aumenta después de cada mini paquete.</b> <b><br></b> <br>  Al fijar los valores de los indicadores en cada iteración, veremos que a medida que aumenta la velocidad de aprendizaje, se alcanza un punto (en el cual) los valores de la función de pérdida dejan de disminuir y comienzan a aumentar.  En la práctica, nuestra velocidad de aprendizaje idealmente debería estar en algún lugar a la izquierda del punto inferior del gráfico (como se muestra en el gráfico a continuación).  En este caso (el valor será) de 0.001 a 0.01. <br><br><img src="https://habrastorage.org/webt/tq/sw/m7/tqswm7bda8qr9zed3h9s3fafbj4.jpeg"><br><br><h4>  Lo anterior parece útil.  ¿Cómo empezar a usarlo? </h4><br>  En este momento hay una función <i>lista</i> para <i>usar</i> en el paquete <i>fast.ia</i> desarrollado por Jeremy Howard, este es un tipo de abstracción / complemento en la parte superior de la biblioteca pytorch (similar a cómo se hace en el caso de Keras y Tensorflow). <br><br>  Solo es necesario ingresar el siguiente comando para comenzar la búsqueda del coeficiente óptimo de velocidad de aprendizaje antes de (comenzar) entrenar la red neuronal. <br><br><pre><code class="python hljs">learn.lr_find() learn.sched.plot_lr()</code> </pre> <br><br><h3>  <b>Mejorando el modelo</b> </h3><br>  Entonces, hablamos sobre cuál es el coeficiente de velocidad de aprendizaje, cuál es su valor y cómo lograr su valor óptimo antes de comenzar a entrenar el modelo en sí. <br>  Ahora nos centraremos en cómo se puede usar el factor de velocidad de aprendizaje para ajustar modelos. <br><br><h4>  Sabiduría Convencional </h4><br>  Por lo general, cuando el usuario establece su coeficiente de velocidad de aprendizaje y comienza a entrenar el modelo, debe esperar hasta que el coeficiente de velocidad de aprendizaje comience a caer y el modelo alcance el valor óptimo. <br><br>  Sin embargo, desde el momento en que el gradiente alcanza una meseta, se hace más difícil mejorar los valores de la función de pérdida cuando se entrena el modelo.  En [3], Dauphin expresa la opinión de que la dificultad para minimizar la función de pérdida proviene del punto de silla de montar, y no del mínimo local. <br><br><img src="https://habrastorage.org/webt/-t/jm/uw/-tjmuwg7a8etbhsc36cw97flhh8.png"><br>  <b>Un punto de silla de montar en la superficie de los errores.</b>  <b>Un punto de silla de montar es un punto del dominio de definición de una función que es estacionaria para una función determinada, pero no es su extremo local</b> .  (ImgCredit: safaribooksonline) <br><br><h4>  Entonces, ¿cómo se puede evitar esto? </h4><br>  Propongo considerar varias opciones.  Uno de ellos, general, usando la cita de [1], <br><blockquote>  ... en lugar de usar un valor fijo para el coeficiente de velocidad de aprendizaje y disminuirlo con el tiempo, si el entrenamiento ya no suaviza nuestra pérdida, vamos a cambiar el coeficiente de velocidad de aprendizaje en cada iteración de acuerdo con alguna función cíclica f.  Cada ciclo tiene, en términos del número de iteraciones, una longitud fija.  Este método permite que el coeficiente de velocidad de aprendizaje varíe entre valores límite razonables.  Esto realmente ayuda, porque, al quedarnos atascados en los puntos de silla, al aumentar el coeficiente de velocidad de aprendizaje obtenemos una intersección más rápida de la meseta de los puntos de silla. </blockquote><br>  En [2], Leslie propone el "método del triángulo", en el cual el coeficiente de velocidad de aprendizaje se revisa después de cada una de varias iteraciones. <br><br><img src="https://habrastorage.org/webt/j4/_w/ga/j4_wga1vdfg3qmiovtzdbtrmb4e.jpeg"><br><br><img src="https://habrastorage.org/webt/pn/tu/f5/pntuf5w2svpsbk9nsiyme8iqf98.jpeg"><br>  <b>"El método de triángulos" y "método de triángulos-2" son métodos para pruebas cíclicas de coeficientes de velocidad de aprendizaje, propuestos por Leslie N. Smith.</b>  <b>En el gráfico superior, el Ir mínimo y máximo se mantienen iguales.</b> <br><br>  Lonchilov &amp; Hutter [6] propuso otro método, que no es menos popular y se llama "descenso de gradiente estocástico con un reinicio en caliente".  Este método, que se basa en el uso de la función coseno como una función cíclica, reinicia el coeficiente de la velocidad de aprendizaje en el punto máximo de cada ciclo.  La aparición del bit "Caliente" se debe al hecho de que cuando se reinicia el coeficiente de velocidad de aprendizaje, no comienza desde el nivel cero, sino desde los parámetros a los que el modelo ha llegado al paso anterior. <br><br>  Dado que este método tiene variaciones, el siguiente gráfico muestra uno de los métodos de su aplicación, donde cada ciclo está vinculado al mismo intervalo de tiempo. <br><br><img src="https://habrastorage.org/webt/kb/8o/pe/kb8opexd3ppx8ynj2bj7dtaphzg.jpeg"><br>  <b>SGDR - gráfico, coeficiente de tasa de aprendizaje vs.</b>  <b>iteraciones</b> <b><br></b> <br>  Por lo tanto, obtenemos una forma de acortar la duración del entrenamiento simplemente saltando sobre los "picos" de vez en cuando (como se muestra a continuación). <br><br><img src="https://habrastorage.org/webt/85/cn/-b/85cn-blk82myspio7d5pxfwfeqk.png"><br>  <b>Comparación de coeficientes de tasa de aprendizaje fijo y cíclico</b> (img credit: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arxiv.org/abs/1704.00109</a> <br>  Además de ahorrar tiempo, este método, según los estudios, mejora la precisión de la clasificación sin ajustes y con menos iteraciones. <br><br><h3>  Transferir la tasa de aprendizaje en Transfer Learning </h3><br>  En el curso de <i>fast.ai, el</i> énfasis está en la gestión de un modelo pre-entrenado para resolver problemas de inteligencia artificial.  Por ejemplo, cuando se resuelven problemas de clasificación de imágenes, se les enseña a los estudiantes cómo usar modelos pre-entrenados como VGG y Resnet50 y vincularlos con la muestra de datos de imagen que necesita predecirse. <br>  Para resumir cómo se construye el modelo en el programa <i>fast.ai</i> (no debe confundirse con el <i>paquete fast. Ai</i> - el paquete del programa), a continuación se <i>detallan</i> los pasos que daremos en una situación ordinaria: <br><br><ol><li>  Habilitar aumento de datos y precomputar = True </li><li>  Use Ir_find () para encontrar el coeficiente de tasa de aprendizaje más alto, donde la pérdida todavía está mejorando claramente. </li><li>  Entrena la última capa de activaciones precalculadas para la era 1-2. </li><li>  Entrene la última capa con ganancia de datos (es decir, calcule = falso) durante 1-2 épocas con el ciclo _len 1. </li><li>  Descongele todas las capas. </li><li>  Coloque las capas anteriores en un factor de velocidad de aprendizaje que sea 3x-10x debajo de la siguiente capa alta </li><li>  Reutilizar Ir_find () </li><li>  Entrene una red completa con el ciclo _mult = 2 = 2 hasta que comience a reentrenar. </li></ol><br>  Puede notar que los pasos dos, cinco y siete (de los anteriores) están relacionados con la tasa de factor de aprendizaje.  En una parte anterior de nuestra publicación, destacamos el punto de los segundos pasos mencionados, donde mencionamos cómo obtener el mejor coeficiente de velocidad de aprendizaje antes de comenzar a entrenar el modelo. <br><br>  En el siguiente párrafo, hablamos sobre cómo puede reducir el tiempo de entrenamiento utilizando SGDR y reiniciando periódicamente el factor de velocidad de aprendizaje, mejorar la precisión para que pueda evitar áreas donde el gradiente es cercano a cero. <br>  En la última sección, tocaremos el concepto de aprendizaje diferenciado y explicaremos cómo se utiliza para determinar el coeficiente de velocidad de aprendizaje cuando un modelo entrenado se asocia con un pre-entrenado ... <br><br><h3>  ¿Qué es el aprendizaje diferencial? </h3><br>  Este es un método en el que varios factores de velocidad de entrenamiento se establecen en la red durante el entrenamiento.  Proporciona una alternativa a la forma en que los usuarios suelen ajustar los factores de velocidad de aprendizaje, es decir, usar el mismo factor de velocidad de aprendizaje a través de la red durante el entrenamiento. <br><br><img src="https://habrastorage.org/webt/xb/aw/-e/xbaw-e9-pehhvaeylpidgeykwwo.png"><br>  <b>La razón por la que amo Twitter es una respuesta directa de la persona misma.</b> <b><br></b>  (Al momento de escribir esta publicación, Jeremy publicó un artículo con Sebastian Ruder, quien se sumergió aún más en este tema. Entonces, creo, el coeficiente diferencial de velocidad de aprendizaje ahora tiene otro nombre: ajuste exacto discriminatorio :) <br><br>  Para demostrar el concepto más claramente, podemos referirnos al diagrama a continuación, en el que el modelo previamente entrenado se "divide" en 3 grupos, donde cada uno se ajusta con un valor creciente del coeficiente de velocidad de aprendizaje. <br><br><img src="https://habrastorage.org/webt/cv/3l/ax/cv3laxkfy-60oz9ftqnhotviqss.jpeg"><br>  <b>Ejemplo de CNN con coeficiente de tasa de aprendizaje diferenciado</b> .  Crédito de imagen de [3] <br><br>  Este método de configuración comprende lo siguiente: las primeras capas generalmente contienen detalles muy pequeños de los datos, como líneas y ángulos, desde los cuales no intentaremos cambiar mucho e intentar guardar la información contenida en ellos.  En general, no hay una necesidad seria de cambiar su peso a un gran número. <br><br>  Por el contrario, para las capas posteriores, como las de la imagen pintadas de verde, donde obtenemos signos detallados de los datos, como el blanco de los ojos, la boca o la nariz, la necesidad de salvarlos desaparece. <br><br><h4>  ¿Cómo se compara esto con otros métodos de ajuste fino? </h4><br>  En [9], se demostró que ajustar el modelo completo sería demasiado costoso, ya que los usuarios pueden obtener más de 100 capas.  Muy a menudo, las personas recurren a la optimización del modelo capa por capa. <br><br>  Sin embargo, esta es la razón de una serie de requisitos, los llamados  interferencia concurrente, y requiere múltiples entradas a través de un conjunto de datos, lo que lleva a un entrenamiento excesivo de conjuntos pequeños. <br><br>  También demostramos que los métodos presentados en [9] son ​​capaces de mejorar la precisión y reducir el número de errores en diversas tareas relacionadas con la clasificación de NRL. <br><br><img src="https://habrastorage.org/webt/by/no/yr/bynoyrrrl8edulvd8udbo8hv-uk.png"><br>  <b>Resultados tomados de la fuente [9]</b> <br><br>  Referencias <br>  [1] Mejorando la forma en que trabajamos con la tasa de aprendizaje. <br>  [2] La técnica del índice de aprendizaje cíclico. <br>  [3] Transferir el aprendizaje utilizando tasas de aprendizaje diferenciales. <br>  [4] Leslie N. Smith.  Tasas de aprendizaje cíclico para la formación de redes neuronales. <br>  [5] Estimación de una tasa de aprendizaje óptima para una red neuronal profunda <br>  [6] Descenso de gradiente estocástico con reinicios cálidos <br>  [7] Optimización para Deep Learning Highlights en 2017 <br>  [8] Cuaderno de la Lección 1, fast.ai Parte 1 V2 <br>  [9] Modelos lingüísticos ajustados para la clasificación de texto </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/469931/">https://habr.com/ru/post/469931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../469917/index.html">Texto rápido en PHP \ Python. Primeros pasos</a></li>
<li><a href="../469919/index.html">Verificación del código de Telegram Open Network por el analizador PVS-Studio</a></li>
<li><a href="../469921/index.html">[Caso] Monitoreo de la calidad del aire en un pueblo rural</a></li>
<li><a href="../469923/index.html">Vulnerabilidad inesperada en productos Apple. Totalmente inesperado</a></li>
<li><a href="../469925/index.html">"F # no es más difícil de dominar que Entity Framework o WPF": Entrevista con Scott Vlashin</a></li>
<li><a href="../469935/index.html">Curso "Fundamentos del trabajo efectivo con Wolfram Technologies": más de 13 horas de video conferencias, teoría y problemas</a></li>
<li><a href="../469939/index.html">Enrutador CNC casero como alternativa a una impresora 3D, parte cuatro. Conceptos generales de procesamiento</a></li>
<li><a href="../469945/index.html">¿Es importante que las computadoras y las personas vean el mundo de manera diferente?</a></li>
<li><a href="../469947/index.html">Pequeñas imágenes de Docker que creían en sí mismas *</a></li>
<li><a href="../469949/index.html">En el cumpleaños de Yuri Knorozov: estudiamos los conceptos básicos de la escritura maya</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>