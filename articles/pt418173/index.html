<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚öîÔ∏è ü•ù üôÜüèª Ida e volta para redes neurais ou uma revis√£o do uso de codificadores autom√°ticos na an√°lise de texto üëÇüèø üë®üèΩ‚Äçüè≠ üë©üèº‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="J√° escrevemos no primeiro artigo do blog corporativo sobre como funciona o algoritmo de detec√ß√£o de empr√©stimos transfer√≠veis. Apenas alguns par√°grafo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ida e volta para redes neurais ou uma revis√£o do uso de codificadores autom√°ticos na an√°lise de texto</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/antiplagiat/blog/418173/"> J√° escrevemos no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">primeiro artigo do blog corporativo</a> sobre como funciona o algoritmo de detec√ß√£o de empr√©stimos transfer√≠veis.  Apenas alguns par√°grafos desse artigo s√£o dedicados ao t√≥pico de comparar textos, embora a id√©ia mere√ßa uma descri√ß√£o muito mais detalhada.  No entanto, como voc√™ sabe, n√£o se pode contar imediatamente sobre tudo, embora realmente se queira.  Em uma tentativa de prestar homenagem a esse t√≥pico e √† arquitetura da rede chamada " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">codificador autom√°tico</a> ", para a qual temos sentimentos muito calorosos, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">Oleg_Bakhteev</a> e eu escrevemos essa resenha. <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deep Learning for PNL (sem Magia)</a> <br><br>  Como mencionamos naquele artigo, a compara√ß√£o dos textos era ‚Äúsem√¢ntica‚Äù - n√£o comparamos os fragmentos de texto em si, mas os vetores correspondentes a eles.  Tais vetores foram obtidos como resultado do treinamento de uma rede neural, que exibia um fragmento de texto de comprimento arbitr√°rio em um vetor de dimens√£o grande, mas fixa.  Como obter esse mapeamento e como ensinar a rede a produzir os resultados desejados √© uma quest√£o separada, que ser√° discutida abaixo. <br><a name="habracut"></a><br><h1>  O que √© um codificador autom√°tico? </h1><br>  Formalmente, uma rede neural √© chamada de codificador autom√°tico (ou codificador autom√°tico), que treina para restaurar objetos recebidos na entrada da rede. <br><img src="https://habrastorage.org/webt/jy/jw/ip/jyjwipnzwzlyidenzeovey3jba4.png"><br>  O codificador autom√°tico consiste em duas partes: um codificador <b>f</b> , que codifica a amostra <b>X</b> em sua representa√ß√£o interna <b>H</b> , e um decodificador <b>g</b> , que restaura a amostra original.  Portanto, o codificador autom√°tico tenta combinar a vers√£o restaurada de cada objeto de amostra com o objeto original. <br><br>  Ao treinar um codificador autom√°tico, a seguinte fun√ß√£o √© minimizada: <br><img src="https://habrastorage.org/webt/9f/ay/cm/9faycmmbldgcxvehefjrurcusyq.png"><br><br>  Onde <b>r</b> representa a vers√£o restaurada do objeto original: <br><img src="https://habrastorage.org/webt/zf/oe/oi/zfoeoiwfxnrscvv0n5cv4keku5k.png"><br><br>  Considere o exemplo fornecido em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">blog.keras.io</a> : <br><img src="https://habrastorage.org/webt/lw/wp/i_/lwwpi_kn0wyrduexhkqyrg9jttk.png"><br>  A rede recebe um objeto <b>x</b> como uma entrada (no nosso caso, o n√∫mero 2). <br><br>  Nossa rede codifica esse objeto em um estado oculto.  Ent√£o, de acordo com o estado latente, a reconstru√ß√£o do objeto <b>r √©</b> restaurada, que deve ser semelhante a x.  Como vemos, a imagem restaurada (√† direita) ficou mais emba√ßada.  Isso √© explicado pelo fato de tentarmos manter em uma vis√£o oculta apenas os sinais mais importantes do objeto, para que o objeto seja restaurado com perdas. <br><br>  O modelo do codificador autom√°tico √© treinado no princ√≠pio de um telefone danificado, em que uma pessoa (codificador) transmite informa√ß√µes <b>(x</b> ) para a segunda pessoa (decodificador <b>)</b> e, por sua vez, o informa √† terceira <b>(r (x))</b> . <br><br>  Um dos principais objetivos desses codificadores autom√°ticos √© reduzir a dimens√£o do espa√ßo de origem.  Quando estamos lidando com codificadores autom√°ticos, o pr√≥prio procedimento de treinamento de rede neural faz com que o codificador autom√°tico se lembre das principais caracter√≠sticas dos objetos a partir dos quais ser√° mais f√°cil restaurar os objetos de amostra originais. <br><br>  Aqui, podemos fazer uma analogia com o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">m√©todo dos componentes principais</a> : este √© um m√©todo para reduzir a dimens√£o, cujo resultado √© a proje√ß√£o da amostra em um subespa√ßo no qual a varia√ß√£o dessa amostra √© m√°xima. <br><br>  De fato, o codificador autom√°tico √© uma generaliza√ß√£o do m√©todo do componente principal: no caso em que nos restringimos √† considera√ß√£o de modelos lineares, o m√©todo do codificador autom√°tico e do componente principal fornece as mesmas representa√ß√µes vetoriais.  A diferen√ßa surge quando consideramos modelos mais complexos, por exemplo, redes neurais multicamadas totalmente conectadas, como codificador e decodificador. <br><br>  Um exemplo de compara√ß√£o do m√©todo do componente principal e do codificador autom√°tico √© apresentado no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Reduzindo a dimensionalidade dos dados com redes neurais</a> : <br><img src="https://habrastorage.org/webt/bj/ap/hq/bjaphq8tpjla39pbn2djsrm84y4.png"><br><br>  Aqui, s√£o demonstrados os resultados do treinamento do codificador autom√°tico e o m√©todo do componente principal para amostragem de imagens de rostos humanos.  A primeira linha mostra os rostos das pessoas da amostra de controle, ou seja,  de uma parte especialmente diferida da amostra que n√£o foi usada pelos algoritmos no processo de aprendizado.  Na segunda e terceira linhas est√£o as imagens restauradas dos estados ocultos do codificador autom√°tico e do m√©todo do componente principal, respectivamente, da mesma dimens√£o.  Aqui voc√™ pode ver claramente quanto melhor o codificador autom√°tico funcionou. <br><br>  No mesmo artigo, outro exemplo ilustrativo: comparando os resultados do codificador autom√°tico e o m√©todo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LSA</a> para a tarefa de recupera√ß√£o de informa√ß√µes.  O m√©todo LSA, como o m√©todo do componente principal, √© um m√©todo cl√°ssico de aprendizado de m√°quina e √© frequentemente usado em tarefas relacionadas ao processamento de linguagem natural. <br><img src="https://habrastorage.org/webt/di/h5/j3/dih5j3wsflfohomgzzrjo9e6n7k.png"><br>  A figura mostra uma proje√ß√£o 2D de v√°rios documentos obtidos usando o codificador autom√°tico e o m√©todo LSA.  As cores indicam o tema do documento.  Pode-se ver que a proje√ß√£o do codificador autom√°tico divide bem os documentos por t√≥pico, enquanto o LSA produz um resultado muito mais barulhento. <br><br>  Outra aplica√ß√£o importante dos auto- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">codificadores √© o pr√©-treinamento da rede</a> .  O pr√©-treinamento da rede √© usado quando a rede otimizada √© profunda o suficiente.  Nesse caso, o treinamento da rede "do zero" pode ser bastante dif√≠cil, portanto, primeiro toda a rede √© representada como uma cadeia de codificadores. <br><br>  O algoritmo de pr√©-treinamento √© bastante simples: para cada camada, treinamos nosso pr√≥prio codificador autom√°tico e depois definimos que a sa√≠da do pr√≥ximo codificador √© simultaneamente a entrada para a pr√≥xima camada de rede.  O modelo resultante consiste em uma cadeia de codificadores treinados para preservar ansiosamente os recursos mais importantes dos objetos, cada um em sua pr√≥pria camada.  O esquema de pr√©-treinamento √© apresentado abaixo: <br><img src="https://habrastorage.org/webt/yy/mc/ui/yymcuigpqgegoa_7gwndzfxulia.png"><br>  Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">psyyz10.github.io</a> <br><br>  Essa estrutura √© denominada Autoencoder empilhado e √© frequentemente usada como "overclocking" para treinar ainda mais o modelo de rede completo e profundo.  A motiva√ß√£o para esse treinamento de uma rede neural √© que uma rede neural profunda √© uma fun√ß√£o n√£o convexa: no processo de treinamento de uma rede, a otimiza√ß√£o dos par√¢metros pode "ficar presa" no m√≠nimo local.  O pr√©-treinamento ganancioso dos par√¢metros de rede permite encontrar um bom ponto de partida para o treinamento final e, assim, tentar evitar esses m√≠nimos locais. <br><br>  Obviamente, n√£o consideramos todas as estruturas poss√≠veis, porque existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Autoencodificadores Esparsos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Autoencodificadores Denoising</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Autoencoder Contrativo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Autoencoder</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Contrativo de Reconstru√ß√£o</a> .  Eles diferem entre si usando v√°rias fun√ß√µes de erro e termos de penalidade para eles.  Todas essas arquiteturas, em nossa opini√£o, merecem uma revis√£o separada.  Em nosso artigo, mostramos, antes de tudo, o conceito geral de codificadores autom√°ticos e as tarefas espec√≠ficas de an√°lise de texto que s√£o resolvidas usando-o. <br><br><h2>  Como isso funciona nos textos? </h2><br>  Passamos agora a exemplos espec√≠ficos do uso de autocoders para tarefas de an√°lise de texto.  Estamos interessados ‚Äã‚Äãem ambos os lados da aplica√ß√£o - nos dois modelos para obter representa√ß√µes internas e no uso dessas representa√ß√µes internas como atributos, por exemplo, no problema de classifica√ß√£o adicional.  Os artigos sobre esse t√≥pico costumam abordar tarefas como an√°lise de sentimentos ou detec√ß√£o de reformula√ß√£o, mas tamb√©m existem trabalhos que descrevem o uso de codificadores autom√°ticos para comparar textos em diferentes idiomas ou para tradu√ß√£o autom√°tica. <br><br>  Nas tarefas de an√°lise de texto, na maioria das vezes o objeto √© a frase, ou seja,  sequ√™ncia ordenada de palavras.  Assim, o codificador autom√°tico recebe exatamente essa sequ√™ncia de palavras, ou melhor, representa√ß√µes vetoriais dessas palavras extra√≠das de algum modelo previamente treinado.  O que s√£o representa√ß√µes vetoriais de palavras, foi considerado em Habr√© em detalhes suficientes, por exemplo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> .  Assim, o codificador autom√°tico, tomando uma sequ√™ncia de palavras como entrada, deve treinar alguma representa√ß√£o interna de toda a senten√ßa que atenda √†s caracter√≠sticas que s√£o importantes para n√≥s, com base na tarefa.  Em problemas de an√°lise de texto, precisamos mapear senten√ßas para vetores para que elas fiquem pr√≥ximas no sentido de alguma fun√ß√£o de dist√¢ncia, geralmente uma medida de cosseno: <br><br><img src="https://habrastorage.org/webt/fs/ka/ec/fskaecgqanvbmtzhf4hdqdd0bhw.png"><br>  Fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Deep Learning for PNL (sem Magia)</a> <br><br>  Um dos primeiros autores a mostrar o uso bem-sucedido de codificadores autom√°ticos na an√°lise de texto foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Richard Socher</a> . <br><br>  Em seu artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pool din√¢mico e desdobramento de auto-codificadores recursivos para detec√ß√£o de parafrase,</a> ele descreve uma nova estrutura de autocodifica√ß√£o - Desdobramento de autoencoder recursivo (desdobramento do RAE) (veja a figura abaixo). <br><img src="https://habrastorage.org/webt/zn/va/o9/znvao90juxs6ywwmm5ywwe6nrdm.png"><br>  Revela√ß√£o do RAE <br><br>  Sup√µe-se que a estrutura da senten√ßa seja definida por um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">analisador sint√°tico</a> .  A estrutura mais simples √© considerada - a estrutura de uma √°rvore bin√°ria.  Essa √°rvore consiste em folhas - palavras de um fragmento, n√≥s internos (n√≥s de ramifica√ß√£o) - frases e um v√©rtice terminal.  Tomando a sequ√™ncia de palavras (x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> ) como entrada (tr√™s representa√ß√µes vetoriais de palavras neste exemplo), o codificador autom√°tico sequencialmente codifica, nesse caso, da direita para a esquerda, representa√ß√µes vetoriais de palavras de senten√ßa em representa√ß√µes vetoriais de coloca√ß√µes e depois em vetor Apresenta√ß√£o de toda a oferta.  Especificamente neste exemplo, primeiro concatenamos os vetores x <sub>2</sub> e x <sub>3</sub> e depois os multiplicamos pela matriz <i>. Temos</i> a dimens√£o <i>oculta √ó 2 vis√≠vel</i> , onde <i>oculto</i> √© onde o tamanho da representa√ß√£o interna oculta, <i>vis√≠vel</i> , √© a dimens√£o do vetor de palavras.  Assim, reduzimos a dimens√£o e adicionamos n√£o linearidade usando a fun√ß√£o tanh.  No primeiro passo, obtemos uma representa√ß√£o vetorial oculta para a frase duas palavras <i>x <sub>2</sub></i> <i>ex</i> <i><sub>3</sub></i> : <i>h <sub>1</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [x <sub>2</sub> , x <sub>3</sub> ] + b <sub>e</sub> )</i> .  No segundo, combinamos a palavra restante <i>h <sub>2</sub></i> = <i>tanh‚Å° (W <sub>e</sub> [h <sub>1</sub> , x <sub>1</sub> ] + b <sub>e</sub> )</i> e obtemos uma representa√ß√£o vetorial para toda a senten√ßa - <i>h <sub>2</sub></i> .  Como mencionado acima, na defini√ß√£o de um codificador autom√°tico, precisamos minimizar o erro entre os objetos e suas vers√µes restauradas.  No nosso caso, s√£o palavras.  Portanto, tendo recebido a representa√ß√£o vetorial final de toda a senten√ßa <i>h <sub>2</sub></i> , decodificaremos suas vers√µes restauradas (x <sub>1</sub> ', x <sub>2</sub> ', x <sub>3</sub> ').  O decodificador aqui funciona com o mesmo princ√≠pio que o codificador, apenas a matriz de par√¢metros e o vetor shift s√£o diferentes aqui: <i>W <sub>d</sub></i> e <i>b <sub>d</sub></i> . <br><br>  Usando a estrutura de uma √°rvore bin√°ria, voc√™ pode codificar senten√ßas de qualquer comprimento em um vetor de dimens√£o fixa - sempre combinamos um par de vetores da mesma dimens√£o, usando a mesma matriz de par√¢metros <i>W <sub>e</sub></i> .  No caso de uma √°rvore n√£o bin√°ria, voc√™ s√≥ precisa inicializar as matrizes antecipadamente se quisermos combinar mais de duas palavras - 3, 4, ... n, nesse caso, a matriz ter√° apenas a dimens√£o <i>oculta √ó invis√≠vel</i> . <br><br>  Vale ressaltar que, neste artigo, representa√ß√µes vetoriais treinadas de frases s√£o usadas n√£o apenas para resolver o problema de classifica√ß√£o - algumas frases s√£o reformuladas ou n√£o.  Os dados de um experimento na pesquisa dos vizinhos mais pr√≥ximos tamb√©m s√£o apresentados - com base apenas no vetor de oferta recebido, s√£o pesquisados ‚Äã‚Äãos vetores mais pr√≥ximos da amostra que est√£o pr√≥ximos a ele no significado: <br><br><img src="https://habrastorage.org/webt/d6/pv/ae/d6pvaevnd18k2ouizgs3t9k0xbk.png"><br><br>  No entanto, ningu√©m nos incomoda em usar outras arquiteturas de rede para codificar e decodificar para combinar sequencialmente palavras em frases. <br><br>  Aqui est√° um exemplo de um artigo do NIPS 2017 - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado de representa√ß√£o de par√°grafos deconvolucionais</a> : <br><img src="https://habrastorage.org/webt/g9/u7/0m/g9u70mec8rpyrbnxuqtnflyxcfo.png"><br><br>  Vemos que a codifica√ß√£o da amostra <b>X</b> na representa√ß√£o oculta <b>h</b> ocorre usando uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">rede neural convolucional</a> , e o decodificador trabalha com o mesmo princ√≠pio. <br><br>  Ou aqui est√° um exemplo usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GRU-GRU</a> no artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Vetores de</a> reflex√£o. <br><br>  Uma caracter√≠stica interessante aqui √© que o modelo trabalha com triplas frases: ( <i>s <sub>i-1</sub> , s <sub>i</sub> , s <sub>i + 1</sub></i> ).  A senten√ßa si √© codificada usando f√≥rmulas GRU padr√£o, e o decodificador, usando as informa√ß√µes de representa√ß√£o interna si, tenta decodificar si <i><sub>-1</sub></i> e si <i><sub>+ 1</sub></i> , tamb√©m usando GRU. <br><br><img src="https://habrastorage.org/webt/ed/od/-r/edod-radj66y43mbsgu31zxo7nu.png"><br><br>  O princ√≠pio de opera√ß√£o neste caso se assemelha ao modelo padr√£o de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tradu√ß√£o autom√°tica de rede neural</a> , que funciona de acordo com o esquema de codificador-decodificador.  No entanto, aqui n√£o temos dois idiomas, enviamos uma frase em um idioma para a entrada da nossa unidade de codifica√ß√£o e tentamos restaur√°-la.  No processo de aprendizagem, h√° uma minimiza√ß√£o de alguma qualidade funcional interna (isso nem sempre √© um erro de reconstru√ß√£o); ent√£o, se necess√°rio, vetores pr√©-treinados s√£o usados ‚Äã‚Äãcomo caracter√≠sticas em outro problema. <br><br>  Outro artigo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Autoencodificadores recursivos por correspond√™ncia bil√≠ng√ºe para tradu√ß√£o autom√°tica estat√≠stica</a> , apresenta uma arquitetura que d√° uma nova olhada na tradu√ß√£o autom√°tica.  Primeiro, para dois idiomas, os autocoders recursivos s√£o treinados separadamente (de acordo com o princ√≠pio descrito acima - onde o Unfolding RAE foi introduzido).  Ent√£o, entre eles, um terceiro codificador autom√°tico √© treinado - um mapeamento entre dois idiomas.  Essa arquitetura tem uma clara vantagem - ao exibir textos em diferentes idiomas em um espa√ßo oculto comum, podemos compar√°-los sem usar a tradu√ß√£o autom√°tica como uma etapa intermedi√°ria. <br><br><img src="https://habrastorage.org/webt/tj/rq/un/tjrqunxjtnbgsivzz7iohnm8ibs.png"><br><br>  O treinamento de codificadores autom√°ticos em fragmentos de texto √© frequentemente encontrado em artigos sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">classifica√ß√£o de treinamento</a> .  Aqui, novamente, o fato de estarmos treinando a funcionalidade final da qualidade da classifica√ß√£o √© importante, primeiro treinamos o codificador autom√°tico para inicializar melhor os vetores de solicita√ß√µes e respostas enviadas √† entrada da rede. <br><br><img src="https://habrastorage.org/webt/5t/a1/qn/5ta1qnx9gqzl9dypb0t4nhgcjtg.jpeg"><br><br>  E, √© claro, n√£o podemos deixar de mencionar <a href="">Autoencoders Variacionais</a> , ou <a href="">VAEs</a> , como modelos generativos.  √â melhor, √© claro, apenas assistir a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">esta palestra da Yandex</a> .  Basta dizer o seguinte: se queremos <i>gerar</i> objetos a partir do espa√ßo oculto de um codificador autom√°tico convencional, a qualidade dessa gera√ß√£o ser√° baixa, pois n√£o sabemos nada sobre a distribui√ß√£o da vari√°vel oculta.  Mas voc√™ pode treinar imediatamente o codificador autom√°tico para gerar, introduzindo uma suposi√ß√£o de distribui√ß√£o. <br><br>  E, usando o VAE, voc√™ pode gerar textos a partir desse espa√ßo oculto, por exemplo, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">fazem</a> os autores do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gerando Senten√ßas a partir de um Espa√ßo Cont√≠nuo</a> ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Um Autoencoder Variacional Convolucional H√≠brido para Gera√ß√£o de Texto</a> . <br><br>  As propriedades generativas do VAE tamb√©m funcionam bem em tarefas que comparam textos em diferentes idiomas - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Uma abordagem de auto-codifica√ß√£o variacional para induzir intercambios de palavras</a> em v√°rios idiomas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√©</a> um excelente exemplo disso. <br><br>  Como conclus√£o, queremos fazer uma pequena previs√£o.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado de representa√ß√£o</a> - o treinamento em representa√ß√µes internas usando exatamente o VAE, especialmente em conjunto com as <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Redes Advers√°rias Generativas</a> , √© uma das abordagens mais desenvolvidas nos √∫ltimos anos - isso pode ser julgado pelo menos pelos t√≥picos mais comuns dos artigos nas √∫ltimas principais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">confer√™ncias de</a> aprendizado de m√°quina da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICLR 2018</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ICML 2018</a> .  Isso √© bastante l√≥gico - porque seu uso ajudou a melhorar a qualidade em v√°rias tarefas, e n√£o apenas em textos.  Mas este √© o t√≥pico de uma revis√£o completamente diferente ... </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt418173/">https://habr.com/ru/post/pt418173/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt418163/index.html">Testes de produ√ß√£o: Netflix Chaos Automation Platform</a></li>
<li><a href="../pt418165/index.html">Quasar, Sobaken e Vermin: revelando detalhes da campanha de espionagem cibern√©tica em andamento</a></li>
<li><a href="../pt418167/index.html">ScadaPy: adicione o protocolo IEC 60870-5-104</a></li>
<li><a href="../pt418169/index.html">O que h√° de novo na atualiza√ß√£o 1 do Veeam Availability Console 2.0?</a></li>
<li><a href="../pt418171/index.html">Em que m√©tricas confiar se os usu√°rios realizarem poucas convers√µes no site?</a></li>
<li><a href="../pt418177/index.html">Edi√ß√£o de imagens .heic sem perda de cor</a></li>
<li><a href="../pt418183/index.html">Aplica√ß√£o da an√°lise de fala nos neg√≥cios</a></li>
<li><a href="../pt418185/index.html">Uma hist√≥ria de aut√≥psia: como revertemos o Hancitor</a></li>
<li><a href="../pt418187/index.html">Na Am√©rica, eles sugeriram a substitui√ß√£o de todas as bibliotecas pelos hubs da Amazon. O p√∫blico est√° indignado</a></li>
<li><a href="../pt418189/index.html">Herdeiro de Zeus: por que o Trojan IcedID √© perigoso para clientes de bancos</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>