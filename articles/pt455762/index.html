<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òòÔ∏è üßúüèª ‚ô¶Ô∏è Uma breve introdu√ß√£o √†s cadeias de Markov üë®‚ÄçüöÄ üë©üèª üßîüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 1998, Lawrence Page, Sergey Brin, Rajiv Motwani e Terry Vinograd publicaram o artigo ‚ÄúO Ranking de Cita√ß√£o PageRank: Trazendo Ordem √† Web‚Äù, que des...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Uma breve introdu√ß√£o √†s cadeias de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/455762/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif" alt="imagem"></div><br>  Em 1998, Lawrence Page, Sergey Brin, Rajiv Motwani e Terry Vinograd publicaram o artigo ‚ÄúO Ranking de Cita√ß√£o PageRank: Trazendo Ordem √† Web‚Äù, que descrevia o agora famoso algoritmo PageRank, que se tornou a base do Google.  Depois de pouco menos de duas d√©cadas, o Google se tornou um gigante e, embora seu algoritmo tenha evolu√≠do fortemente, o PageRank ainda √© um ‚Äús√≠mbolo‚Äù dos algoritmos de classifica√ß√£o do Google (embora apenas algumas pessoas possam realmente dizer quanto peso ele tem no algoritmo hoje) . <br><br>  Do ponto de vista te√≥rico, √© interessante notar que uma das interpreta√ß√µes padr√£o do algoritmo PageRank √© baseada em um conceito simples, mas fundamental, das cadeias de Markov.  A partir do artigo, veremos que as cadeias de Markov s√£o ferramentas poderosas para modelagem estoc√°stica que podem ser √∫teis para qualquer cientista de dados.  Em particular, responderemos a perguntas b√°sicas: o que s√£o cadeias de Markov, que boas propriedades elas possuem e o que pode ser feito com a ajuda delas? <br><a name="habracut"></a><br><h4>  Breve revis√£o </h4><br>  Na primeira se√ß√£o, damos as defini√ß√µes b√°sicas necess√°rias para entender as cadeias de Markov.  Na segunda se√ß√£o, consideramos o caso especial de cadeias de Markov em um espa√ßo de estados finito.  Na terceira se√ß√£o, consideramos algumas das propriedades elementares das cadeias de Markov e ilustramos essas propriedades com muitos pequenos exemplos.  Finalmente, na quarta se√ß√£o, associamos as cadeias de Markov ao algoritmo PageRank e vemos com um exemplo artificial como as cadeias de Markov podem ser usadas para classificar os n√≥s de um gr√°fico. <br><br><blockquote>  <strong>Nota</strong>  A compreens√£o deste post requer conhecimento dos conceitos b√°sicos de probabilidade e √°lgebra linear.  Em particular, ser√£o utilizados os seguintes conceitos: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener">probabilidade condicional</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener">vetor pr√≥prio</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" rel="noopener">f√≥rmula de probabilidade total</a> . </blockquote><br><hr><br><h3>  O que s√£o cadeias de Markov? </h3><br><h4>  Vari√°veis ‚Äã‚Äãaleat√≥rias e processos aleat√≥rios </h4><br>  Antes de introduzir o conceito de cadeias de Markov, vamos relembrar brevemente os conceitos b√°sicos, mas importantes da teoria das probabilidades. <br><br>  Primeiro, fora da linguagem da matem√°tica, uma <strong>vari√°vel aleat√≥ria</strong> X √© uma quantidade determinada pelo resultado de um fen√¥meno aleat√≥rio.  Seu resultado pode ser um n√∫mero (ou "semelhan√ßa de um n√∫mero", por exemplo, vetores) ou outra coisa.  Por exemplo, podemos definir uma vari√°vel aleat√≥ria como resultado de uma rolagem (n√∫mero) ou como resultado de um sorteio (n√£o um n√∫mero, a menos que designemos, por exemplo, "√°guia" como 0, mas "caudas" como 1).  Tamb√©m mencionamos que o espa√ßo de resultados poss√≠veis de uma vari√°vel aleat√≥ria pode ser discreto ou cont√≠nuo: por exemplo, uma vari√°vel aleat√≥ria normal √© cont√≠nua e uma vari√°vel aleat√≥ria de Poisson √© discreta. <br><br>  Al√©m disso, podemos definir um <strong>processo aleat√≥rio</strong> (tamb√©m chamado estoc√°stico) como um conjunto de vari√°veis ‚Äã‚Äãaleat√≥rias indexadas pelo conjunto T, que frequentemente denota diferentes pontos no tempo (no que se segue, assumiremos isso).  Os dois casos mais comuns: T pode ser um conjunto de n√∫meros naturais (processo aleat√≥rio com tempo discreto) ou um conjunto de n√∫meros reais (processo aleat√≥rio com tempo cont√≠nuo).  Por exemplo, se jogarmos uma moeda todos os dias, definiremos um processo aleat√≥rio com tempo discreto, e o valor sempre vari√°vel de uma op√ß√£o na bolsa definir√° um processo aleat√≥rio com tempo cont√≠nuo.  Vari√°veis ‚Äã‚Äãaleat√≥rias em diferentes momentos no tempo podem ser independentes umas das outras (um exemplo com um sorteio) ou ter algum tipo de depend√™ncia (um exemplo com o valor da op√ß√£o);  al√©m disso, eles podem ter um espa√ßo de estado cont√≠nuo ou discreto (o espa√ßo de resultados poss√≠veis a cada momento no tempo). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31e/ada/2f8/31eada2f80d66f0df4c007ec8da11579.jpg"></div><br>  <i>Diferentes tipos de processos aleat√≥rios (discretos / cont√≠nuos no espa√ßo / tempo).</i> <br><br><h4>  Propriedade Markov e cadeia Markov </h4><br>  Existem fam√≠lias bem conhecidas de processos aleat√≥rios: processos gaussianos, processos de Poisson, modelos auto-regressivos, modelos de m√©dia m√≥vel, cadeias de Markov e outros.  Cada um desses casos individuais tem certas propriedades que nos permitem explorar e compreend√™-los melhor. <br><br>  Uma das propriedades que simplifica bastante o estudo de um processo aleat√≥rio √© a propriedade Markov.  Se o explicarmos em uma linguagem muito informal, a propriedade Markov nos dir√° que, se soubermos o valor obtido por algum processo aleat√≥rio em um determinado momento, n√£o receberemos nenhuma informa√ß√£o adicional sobre o comportamento futuro do processo, coletando outras informa√ß√µes sobre seu passado.  Em uma linguagem mais matem√°tica: a qualquer momento, a distribui√ß√£o condicional dos estados futuros de um processo com determinados estados atuais e passados ‚Äã‚Äãdepende apenas do estado atual, e n√£o dos estados passados ‚Äã‚Äã( <strong>propriedade da falta de mem√≥ria</strong> ).  Um processo aleat√≥rio com uma propriedade Markov √© chamado de <strong>processo Markov</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44f/1ba/f48/44f1baf48ceb669e0416489eea2bae35.png"></div><br>  <i>A propriedade Markov significa que, se conhecermos o estado atual em um determinado momento, n√£o precisaremos de informa√ß√µes adicionais sobre o futuro, coletadas do passado.</i> <br><br>  Com base nessa defini√ß√£o, podemos formular a defini√ß√£o de "cadeias de Markov homog√™neas com tempo discreto" (a seguir, por simplicidade, as chamaremos de "cadeias de Markov").  <strong>A cadeia de Markov</strong> √© um processo de Markov com tempo discreto e um espa√ßo de estado discreto.  Portanto, uma cadeia de Markov √© uma sequ√™ncia discreta de estados, cada um dos quais √© retirado de um espa√ßo de estados discreto (finito ou infinito), satisfazendo a propriedade Markov. <br><br>  Matematicamente, podemos denotar a cadeia de Markov da seguinte maneira: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/114/3b7/fc3/1143b7fc371f3142534c2b886bf3e69c.png"></div><br>  onde, a cada momento, o processo obt√©m seus valores de um conjunto E discreto, de modo que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/886/a22/d76/886a22d7671798102ee3d94fe9868b81.png"></div><br>  Ent√£o a propriedade Markov implica que temos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/edc/8bf/384/edc8bf38422705e72c9dd7d094b249db.png"></div><br>  Observe novamente que essa √∫ltima f√≥rmula reflete o fato de que, para a cronologia (onde estou agora e onde estava antes), a distribui√ß√£o de probabilidade do pr√≥ximo estado (onde estarei o pr√≥ximo) depende do estado atual, mas n√£o dos estados anteriores. <br><br><blockquote>  <strong>Nota</strong>  Neste post introdut√≥rio, decidimos falar apenas sobre cadeias simples de Markov homog√™neas com tempo discreto.  No entanto, tamb√©m existem cadeias de Markov n√£o homog√™neas (dependentes do tempo) e / ou cadeias de tempo cont√≠nuo.  Neste artigo, n√£o consideraremos essas varia√ß√µes do modelo.  Tamb√©m √© importante notar que a defini√ß√£o acima de uma propriedade de Markov √© extremamente simplificada: a verdadeira defini√ß√£o matem√°tica usa o conceito de filtragem, que vai muito al√©m do conhecimento introdut√≥rio do modelo. </blockquote><br><h4>  Caracterizamos a din√¢mica da aleatoriedade de uma cadeia de Markov </h4><br>  Na subse√ß√£o anterior, nos familiarizamos com a estrutura geral correspondente a qualquer cadeia de Markov.  Vamos ver o que precisamos para definir uma "inst√¢ncia" espec√≠fica de um processo t√£o aleat√≥rio. <br><br>  Primeiro, observamos que a determina√ß√£o completa das caracter√≠sticas de um processo aleat√≥rio com tempo discreto que n√£o satisfaz a propriedade de Markov pode ser dif√≠cil: a distribui√ß√£o de probabilidade em um determinado momento pode depender de um ou mais momentos no passado e / ou no futuro.  Todas essas depend√™ncias poss√≠veis de tempo podem potencialmente complicar a cria√ß√£o de uma defini√ß√£o de processo. <br><br>  No entanto, devido √† propriedade Markov, a din√¢mica da cadeia de Markov √© bastante simples de determinar.  E de fato.  precisamos determinar apenas dois aspectos: a <strong>distribui√ß√£o de probabilidade inicial</strong> (ou seja, a distribui√ß√£o de probabilidade no tempo n = 0), denotada por <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/595/90e/140/59590e140cdb348c943d9dcab0ea011d.png"></div><br>  e <strong>a matriz de probabilidade de transi√ß√£o</strong> (que nos d√° as probabilidades de que o estado no tempo n + 1 seja o pr√≥ximo para outro estado no tempo n para qualquer par de estados), indicado por <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/011/aee/574/011aee5747fe7e42fa09bf044c421e26.png"></div><br>  Se esses dois aspectos s√£o conhecidos, a din√¢mica completa (probabil√≠stica) do processo √© claramente definida.  E, de fato, a probabilidade de qualquer resultado do processo pode ser calculada ciclicamente. <br><br>  Exemplo: suponha que desejamos saber a probabilidade de que os tr√™s primeiros estados do processo tenham valores (s0, s1, s2).  Ou seja, queremos calcular a probabilidade <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c5/991/81b/6c599181b1fd3892391711f311878b72.png"></div><br>  Aqui aplicamos a f√≥rmula da probabilidade total, que afirma que a probabilidade de obter (s0, s1, s2) √© igual √† probabilidade de obter o primeiro s0 vezes a probabilidade de obter s1, j√° que anteriormente obtivemos s0 vezes a probabilidade de obter s2, levando em considera√ß√£o o fato de que chegamos mais cedo na ordem s0 e s1.  Matematicamente, isso pode ser escrito como <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a46/da3/64f/a46da364f28d5b69055700759db02663.png"></div><br>  E ent√£o a simplifica√ß√£o √© revelada, determinada pela suposi√ß√£o de Markov.  E, de fato, no caso de cadeias longas, obtemos probabilidades fortemente condicionais para os √∫ltimos estados.  No entanto, no caso das cadeias de Markov, podemos simplificar essa express√£o aproveitando o fato de que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/7f6/568/0ab7f6568e28bb562ebb287252422d51.png"></div><br>  ficando assim <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b6c/700/30a/b6c70030a8627d87c39dab96c147513a.png"></div><br>  Como eles caracterizam completamente a din√¢mica probabil√≠stica do processo, muitos eventos complexos podem ser calculados apenas com base na distribui√ß√£o de probabilidade inicial q0 e na matriz de probabilidade de transi√ß√£o p.  Tamb√©m vale mencionar mais uma conex√£o b√°sica: a express√£o da distribui√ß√£o de probabilidade no tempo n + 1, expressa em rela√ß√£o √† distribui√ß√£o de probabilidade no tempo n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/f31/a8a/5aef31a8ae120a25e5b43d6534dc20ff.png"></div><br><h3>  Cadeias de Markov em espa√ßos de estados finitos </h3><br><h4>  Representa√ß√£o de matrizes e gr√°ficos </h4><br>  Aqui assumimos que o conjunto E tem um n√∫mero finito de estados poss√≠veis N: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d57/788/f81/d57788f81d0a92aa1d94ef572bdf25e3.png"></div><br>  Ent√£o a distribui√ß√£o de probabilidade inicial pode ser descrita como <strong>um vetor de linha</strong> q0 do tamanho N e as probabilidades de transi√ß√£o podem ser descritas como uma matriz p do tamanho N por N, de modo que <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/100/6f8/6ae/1006f86aeff6699711058bd890190917.png"></div><br>  A vantagem dessa nota√ß√£o √© que, se denotarmos a distribui√ß√£o de probabilidade na etapa n pelo vetor de linha qn, de modo que seus componentes sejam especificados <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/19b/4ae/00619b4ae1601eacdb8fd7ce248f1738.png"></div><br>  ent√£o, rela√ß√µes simples de matriz s√£o preservadas <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a12/cdc/a7b/a12cdca7b75ef09ceaacef33a3667549.png"></div><br>  (aqui n√£o consideraremos a prova, mas √© muito simples reproduzi-la). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23c/3c6/a10/23c3c6a102bd7aa079c36a75f60a5e42.png"></div><br>  <i>Se multiplicarmos o vetor de linha √† direita, que descreve a distribui√ß√£o de probabilidade em um determinado est√°gio de tempo, pela matriz de probabilidade de transi√ß√£o, obteremos a distribui√ß√£o de probabilidade no pr√≥ximo est√°gio de tempo.</i> <br><br>  Portanto, como vemos, a transi√ß√£o da distribui√ß√£o de probabilidade de um determinado est√°gio para o pr√≥ximo √© simplesmente definida como a multiplica√ß√£o correta do vetor linha de probabilidades do passo inicial pela matriz p.  Al√©m disso, isso implica que temos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/35b/072/e0c/35b072e0c74397094e5bf6a9dab11417.png"></div><br>  A din√¢mica de aleatoriedade de uma cadeia de Markov em um espa√ßo de estados finito pode ser facilmente representada como um gr√°fico orientado normalizado, de modo que cada n√≥ do gr√°fico seja um estado, e para cada par de estados (ei, ej) existe uma aresta que vai de ei a ej se p (ei, ej )&gt; 0.  Ent√£o o valor da aresta ter√° a mesma probabilidade p (ei, ej). <br><br><h4>  Exemplo: um leitor do nosso site </h4><br>  Vamos ilustrar tudo isso com um exemplo simples.  Considere o comportamento cotidiano de um visitante fict√≠cio de um site.  Todos os dias ele tem tr√™s estados poss√≠veis: o leitor n√£o visita o site naquele dia (N), o leitor visita o site, mas n√£o l√™ o post inteiro (V), e o leitor visita o site e l√™ um post inteiro (R).  Portanto, temos o seguinte espa√ßo de estado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/242/b1c/627/242b1c62745a4a13f2a24b8f919c6820.png"></div><br>  Suponha que, no primeiro dia, esse leitor tenha 50% de chance de acessar apenas o site e 50% de chance de visitar o site e ler pelo menos um artigo.  O vetor que descreve a distribui√ß√£o de probabilidade inicial (n = 0) fica assim: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/362/a5a/045/362a5a0455c939f9d855d65605945c90.png"></div><br>  Imagine tamb√©m que as seguintes probabilidades sejam observadas: <br><br><ul><li>  quando o leitor n√£o o visita um dia, h√° uma probabilidade de 25% de n√£o visit√°-lo no dia seguinte, uma probabilidade de 50% apenas para visit√°-lo e 25% para visit√°-lo e ler o artigo </li><li>  quando o leitor visita o site um dia, mas n√£o l√™, ele tem 50% de chance de visit√°-lo novamente no dia seguinte e n√£o l√™ o artigo, e 50% de chance de visitar e ler </li><li>  quando um leitor visita e l√™ um artigo no mesmo dia, ele tem 33% de chance de n√£o fazer logon no dia seguinte <em>(espero que este post n√£o tenha esse efeito!)</em> , 33% de chance de fazer login apenas no site e 34% de visitar e ler o artigo novamente </li></ul><br>  Ent√£o temos a seguinte matriz de transi√ß√£o: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cf5/7e6/ba5/cf57e6ba5303d4e13cbb736e6115306d.png"></div><br>  Da subse√ß√£o anterior, sabemos como calcular para este leitor a probabilidade de cada estado no dia seguinte (n = 1) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c64/a77/76c/c64a7776cfc56a5af1a0ccf495469ef7.png"></div><br>  A din√¢mica probabil√≠stica dessa cadeia de Markov pode ser representada graficamente da seguinte forma: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/797/9a1/8327979a1aa6edd9d462a0b40a4c072d.png"></div><br>  <i>Apresenta√ß√£o em forma de gr√°fico da cadeia de Markov, modelando o comportamento do nosso visitante inventado no site.</i> <br><br><h3>  Propriedades das cadeias de Markov </h3><br>  Nesta se√ß√£o, falaremos apenas sobre algumas das propriedades ou caracter√≠sticas mais b√°sicas das cadeias de Markov.  N√£o entraremos em detalhes matem√°ticos, mas forneceremos uma breve vis√£o geral de pontos interessantes que devem ser estudados para usar as cadeias de Markov.  Como vimos, no caso de um espa√ßo de estados finito, a cadeia de Markov pode ser representada como um gr√°fico.  No futuro, usaremos a representa√ß√£o gr√°fica para explicar algumas propriedades.  No entanto, n√£o esque√ßa que essas propriedades n√£o est√£o necessariamente limitadas ao caso de um espa√ßo de estados finito. <br><br><h4>  Decomposi√ß√£o, periodicidade, irrevogabilidade e recupera√ß√£o </h4><br>  Nesta subse√ß√£o, vamos come√ßar com v√°rias maneiras cl√°ssicas de caracterizar um estado ou uma cadeia de Markov inteira. <br><br>  Primeiro, mencionamos que a cadeia de Markov √© <strong>indecompon√≠vel</strong> se for poss√≠vel alcan√ßar qualquer estado de qualquer outro estado (n√£o √© necess√°rio isso em uma etapa do tempo).  Se o espa√ßo de estados √© finito e a cadeia pode ser representada como um gr√°fico, podemos dizer que o gr√°fico de uma cadeia de Markov indecompon√≠vel est√° fortemente conectado (teoria dos grafos). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cef/a39/05c/cefa3905cced27b4e27a4e9547fbe846.png"></div><br>  <i>Ilustra√ß√£o da propriedade da indecomposi√ß√£o (irredutibilidade).</i>  <i>A corrente da esquerda n√£o pode ser reduzida: de 3 ou 4 n√£o podemos entrar em 1 ou 2. A corrente da direita (uma extremidade √© adicionada) pode ser reduzida: cada estado pode ser alcan√ßado a partir de qualquer outro.</i> <br><br>  Um estado possui um per√≠odo k se, ao sair, para qualquer retorno a esse estado, o n√∫mero de etapas de tempo for um m√∫ltiplo de k (k √© o maior divisor comum de todos os comprimentos poss√≠veis de caminhos de retorno).  Se k = 1, eles dizem que o estado √© aperi√≥dico, e toda a cadeia de Markov √© <strong>aperi√≥dica</strong> se todos os seus estados forem aperi√≥dicos.  No caso de uma cadeia de Markov irredut√≠vel, tamb√©m podemos mencionar que, se um estado √© aperi√≥dico, todos os outros tamb√©m s√£o aperi√≥dicos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbe/65f/a07/cbe65fa07e7b816d385408824ba0ff39.png"></div><br>  <i>Ilustra√ß√£o da propriedade periodicidade.</i>  <i>A corrente √† esquerda √© peri√≥dica com k = 2: ao sair de qualquer estado, o retorno a ele sempre exige o n√∫mero de etapas m√∫ltiplo de 2. A corrente √† direita tem um per√≠odo de 3.</i> <br><br>  Um estado √© <strong>irrevog√°vel</strong> se, ao sair do estado, houver uma probabilidade diferente de zero de que nunca mais voltaremos a ele.  Por outro lado, um estado √© considerado <strong>retorn√°vel</strong> se soubermos que depois de deixar o estado, podemos retornar a ele no futuro com probabilidade 1 (se n√£o for irrevog√°vel). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6a0/1ad/7b0/6a01ad7b05e96f5a6606a8a48d127233.png"></div><br>  <i>Ilustra√ß√£o da propriedade de retorno / irrevogabilidade.</i>  <i>A cadeia da esquerda tem as seguintes propriedades: 1, 2 e 3 s√£o irrevog√°veis ‚Äã‚Äã(ao deixar esses pontos, n√£o podemos ter certeza absoluta de que retornaremos a eles) e t√™m um per√≠odo de 3, e 4 e 5 s√£o retorn√°veis ‚Äã‚Äã(ao deixar esses pontos, temos certeza absoluta que um dia retornaremos a eles) e teremos um per√≠odo de 2. A corrente da direita tem outra nervura, tornando toda a corrente retorn√°vel e aperi√≥dica.</i> <br><br>  Para o estado de retorno, podemos calcular o tempo m√©dio de retorno, que √© o <strong>tempo esperado de retorno</strong> ao sair do estado.  Observe que mesmo a probabilidade de um retorno √© 1, isso n√£o significa que o tempo de retorno esperado seja finito.  Portanto, entre todos os estados de retorno, podemos distinguir entre <strong>estados de retorno positivo</strong> (com um tempo de retorno esperado finito) e <strong>estados de retorno zero</strong> (com um tempo de retorno esperado infinito). <br><br><h4>  Distribui√ß√£o estacion√°ria, comportamento marginal e ergodicidade </h4><br>  Nesta subse√ß√£o, consideramos propriedades que caracterizam alguns aspectos da din√¢mica (aleat√≥ria) descrita pela cadeia de Markov. <br><br>  A distribui√ß√£o de probabilidade œÄ no espa√ßo de estados E √© chamada de <strong>distribui√ß√£o estacion√°ria</strong> se satisfizer a express√£o <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/736/d70/e5d/736d70e5ddfc5aeb788d29ebfa79f9ec.png"></div><br>  Desde que n√≥s temos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b98/c4e/e6c/b98c4ee6c3b8032b074c7543db816c7e.png"></div><br>  Ent√£o a distribui√ß√£o estacion√°ria satisfaz a express√£o <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b8/df4/0a8/2b8df40a8a9a2ae90eb49a7c60dafb55.png"></div><br>  Por defini√ß√£o, a distribui√ß√£o estacion√°ria de probabilidade n√£o muda ao longo do tempo.  Ou seja, se a distribui√ß√£o inicial q for estacion√°ria, ser√° a mesma em todos os est√°gios subseq√ºentes do tempo.  Se o espa√ßo de estados for finito, ent√£o p pode ser representado como uma matriz e œÄ como um vetor de linha, e ent√£o obtemos <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee0/565/9fc/ee05659fc4ed268ee3189342a76d9311.png"></div><br>  Isso novamente expressa o fato de que a distribui√ß√£o estacion√°ria de probabilidade n√£o muda com o tempo (como vemos, multiplicar a distribui√ß√£o probabil√≠stica √† direita por p nos permite calcular a distribui√ß√£o probabil√≠stica no pr√≥ximo est√°gio do tempo).  Lembre-se de que uma cadeia de Markov indecompon√≠vel tem uma distribui√ß√£o de probabilidade estacion√°ria se e somente se um de seus estados tiver retorno positivo. <br><br>  Outra propriedade interessante relacionada √† distribui√ß√£o de probabilidade estacion√°ria √© a seguinte.      (      )  , ,      ,          : ,    <strong> </strong> ,    ,   .       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95b/fb9/1c6/95bfb91c67d024e2df40b0e6dcdaf747.png"></div><br>     ,          :        (  )     . <br><br> , <strong></strong> ‚Äî     ,     .    ,   ,   ¬´¬ª,      . ,     f(.),     E   (  , ,     ).     ,       ( ).  n-      <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af1/8c5/4a3/af18c54a3d7dc1e1ad4a4015ab7ad64c.png"></div><br>        f   E,     ( ),   <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04a/352/c77/04a352c77b0687ef3cc89f3b7e0edf38.png"></div><br>     ,      ,      (   ).     : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/37d/c4d/6cb37dc4dcf0a3e53cc8e6baec8f4b1a.png"></div><br>  ,  ,                  . <br><br><h4>       </h4><br>      .     ,   ,       . <br><br>  ,         ,        R ( ¬´    ¬ª).  ,      :            ,         ,       ?       ,    . <br><br>    <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/57c/c74/2a257cc74db27e5ac89ffc1e06bd9ed9.png"></div><br> ,    m(R,R).    ,     R,   <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f9/e1a/a6e/4f9e1aa6e04e736fde182693398a4dca.png"></div><br>    ,    m(R,R)   m(N,R)  m(V,R).       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e23/7cb/f2d/e237cbf2d81597544f800d38b5a59e91.png"></div><br> ,    3   3        m(N,R) = 2.67, m(V,R) = 2.00  m(R,R) = 2.54.       R   2.54.               R (      N  R      V  R). <br><br>     ,  ,      .    ,       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb3/73d/068/bb373d068a04d681c0501d8276731c0a.png"></div><br>         p,     1.   ,     : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0c6/abe/19e/0c6abe19e37b67af5f380eb3e5c0beb9.jpg"></div><br> <i>      .</i> <br><br>   ,  œÄ( R ) = 1/m(R,R),    ,      (       ). <br><br>     ,  ,           (   ).  ,        ,          ,    œÄ(N) ,         ,  œÄ(V) ,   ,    ,   œÄ¬Æ ,      .     ,     ,    ,       ()    : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/128/58e/a88/12858ea88a0e3bd05950b9d30096b776.jpg"></div><br> <i>  3       (,   )    ().</i> <br><br><h3>  :  PageRank </h3><br>     PageRank!     ,  ,   PageRank,    ,   ,              .     ,   . <br><br><h4>  - </h4><br> PageRank    :      (  ,     , ,  - )       ? <br><br>         , PageRank    .  ,             .        ,        ,        (,   ,    , ).          . <br><br>     :  ‚Äî   ,         (  ,           ),        .   ,        (      ),       ¬´ ¬ª    .        ,        ( )  ,      . <br><br>   PageRank   :             (    ,       ,       ).          PageRank. <br><br><h4>   </h4><br>     ,    . ,      -  7 ,   1  7,        . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif"></div><br>           .   ,  ¬´¬ª     (  ¬´ ¬ª),         :    K   (  K    )      1/K.       : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b30/3ec/a66/b303eca66763a4187d027842214ff529.png"></div><br>   0.0     ".".     ,   ,        ,          .   ,     ,       <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2f/8da/687/e2f8da6879f6f19fdc921803c8c7e371.png"></div><br>  ,     PageRank (  )    <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a8/b69/a5b/0a8b69a5b2916bca1f5fa45955af1b4b.jpg"></div><br> <i> PageRank,       7 .</i> <br><br>   PageRank   -   1 &gt; 7 &gt; 4 &gt; 2 &gt; 5 = 6 &gt; 3. <br><br><hr><br><h3>  Conclus√µes </h3><br>     : <br><br><ul><li>   ‚Äî    ,     (      ) </li><li>      ,           (    ¬´ ¬ª) </li><li>      ‚Äî       ,    </li><li>                 (  ,  ‚Ä¶) </li><li>     PageRank ( )    -,       ;          ( ,             ,  ,      ) </li></ul><br>     ,         ,    .         , ,    (   ,             ,     ),   (   -            ),   (   ),   (           ),     . <br><br> ,  ,         ,  ,      .   ,            ,           . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455762/">https://habr.com/ru/post/pt455762/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455746/index.html">Celesta 7.x: ORM, migra√ß√£o e teste ‚Äúem um pacote‚Äù</a></li>
<li><a href="../pt455754/index.html">Testes de um estratostato √† deriva. Lan√ßamento de Rogozin e LoRa na estratosfera</a></li>
<li><a href="../pt455756/index.html">√â a favor</a></li>
<li><a href="../pt455758/index.html">Hacking de crescimento no foguete de varejo: da pesquisa de hip√≥teses √†s t√©cnicas de teste</a></li>
<li><a href="../pt455760/index.html">A m√°gica do SwiftUI ou sobre os construtores de fun√ß√µes</a></li>
<li><a href="../pt455764/index.html">Pesquisa de c√≥digo de barras incrivelmente precisa, r√°pida e leve atrav√©s de segmenta√ß√£o sem√¢ntica</a></li>
<li><a href="../pt455768/index.html">Fatores essenciais de SEO no local</a></li>
<li><a href="../pt455770/index.html">AERODISCO: aguardando vs. realidade</a></li>
<li><a href="../pt455774/index.html">Motores de turbina a g√°s para aeronaves</a></li>
<li><a href="../pt455784/index.html">Por causa do cinza escuro ser mais claro que o cinza no CSS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>