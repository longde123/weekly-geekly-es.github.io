<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÜüèº üéõÔ∏è üë©üèΩ‚Äçüî¨ Passer de Redshift √† ClickHouse üßëüèæ üíÜüèª üì¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pendant longtemps, iFunny a utilis√© Redshift comme base de donn√©es pour les √©v√©nements qui se produisent dans les services backend et les applications...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Passer de Redshift √† ClickHouse</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/funcorp/blog/433346/"><img src="https://habrastorage.org/webt/s8/xo/0d/s8xo0dnodxojhff6jufnruyg660.jpeg"><br><br>  Pendant longtemps, iFunny a utilis√© Redshift comme base de donn√©es pour les √©v√©nements qui se produisent dans les services backend et les applications mobiles.  Il a √©t√© choisi parce qu'au moment de la mise en ≈ìuvre, il n'y avait, dans l'ensemble, aucune alternative comparable en termes de co√ªt et de commodit√©. <br><br>  Cependant, tout a chang√© apr√®s la sortie publique de ClickHouse.  Nous l'avons √©tudi√© pendant longtemps, compar√© le co√ªt, estim√© l'architecture approximative, et enfin, cet √©t√©, nous avons d√©cid√© de voir √† quel point il nous √©tait utile.  Dans cet article, vous d√©couvrirez le probl√®me que Redshift nous a aid√© √† r√©soudre et comment nous avons d√©plac√© cette solution vers ClickHouse. <br><a name="habracut"></a><br><h2>  Le probl√®me </h2><br>  iFunny avait besoin d'un service similaire √† Yandex.Metrica, mais exclusivement pour la consommation domestique.  Je vais vous expliquer pourquoi. <br><br>  Les clients externes √©crivent des √©v√©nements.  Il peut s'agir d'applications mobiles, de sites Web ou de services backend internes.  Il est tr√®s difficile pour ces clients d'expliquer que le service d'accueil √©v√©nementiel est actuellement indisponible, ¬´essayez de l'envoyer en 15 minutes ou en une heure¬ª.  Il y a beaucoup de clients, ils veulent envoyer des √©v√©nements tout le temps et ne peuvent pas attendre du tout. <br><br>  Contrairement √† eux, il existe des services internes et des utilisateurs assez tol√©rants √† cet √©gard: ils peuvent fonctionner correctement m√™me avec un service d'analyse inaccessible.  Et la plupart des m√©triques du produit et les r√©sultats des tests A / B sont g√©n√©ralement judicieux √† regarder une seule fois par jour, voire moins souvent.  Par cons√©quent, les exigences de lecture sont assez faibles.  En cas d'accident ou de mise √† jour, nous pouvons nous permettre d'√™tre inaccessibles ou incoh√©rents en lecture pendant plusieurs heures voire plusieurs jours (dans un cas particuli√®rement n√©glig√©). <br><br>  Si nous parlons de chiffres, nous devons prendre environ cinq milliards d'√©v√©nements (300 Go de donn√©es compress√©es) par jour, tout en stockant les donn√©es pendant trois mois sous une forme ¬´√† chaud¬ª, accessible pour les requ√™tes SQL et dans une ¬´froide¬ª pendant deux ans ou plus, mais pour qu'en quelques jours nous puissions les transformer en "chaud". <br><br>  Fondamentalement, les donn√©es sont une collection d'√©v√©nements class√©s par heure.  Il existe environ trois cents types d'√©v√©nements, chacun ayant son propre ensemble de propri√©t√©s.  Il existe encore des donn√©es provenant de sources tierces qui doivent √™tre synchronis√©es avec la base de donn√©es analytiques: par exemple, une collection d'installations d'applications √† partir de MongoDB ou un service AppsFlyer externe. <br><br>  Il s'av√®re que pour la base de donn√©es, nous avons besoin d'environ 40 To de disque et pour le stockage ¬´froid¬ª - environ 250 To de plus. <br><br><h2>  Redshift Solution </h2><br><img src="https://habrastorage.org/webt/f0/nq/dl/f0nqdl7cvriq9ygc3jlhelfdlqi.png"><br><br>  Il existe donc des clients mobiles et des services backend √† partir desquels vous devez recevoir des √©v√©nements.  Le service HTTP accepte les donn√©es, effectue la validation minimale, collecte les √©v√©nements sur le disque local dans des fichiers regroup√©s par minute, les comprime imm√©diatement et les envoie au compartiment S3.  La disponibilit√© de ce service d√©pend de la disponibilit√© des serveurs avec l'application et AWS S3.  Les applications ne stockent pas l'√©tat, elles sont donc facilement √©quilibr√©es, mises √† l'√©chelle et √©chang√©es.  S3 est un service de stockage de fichiers relativement simple avec une bonne r√©putation et une bonne disponibilit√©, vous pouvez donc vous y fier. <br><br>  Ensuite, vous devez en quelque sorte livrer les donn√©es √† Redshift.  Tout est assez simple ici: Redshift a un importateur S3 int√©gr√©, qui est la m√©thode recommand√©e pour charger les donn√©es.  Par cons√©quent, une fois toutes les 10 minutes, un script d√©marre qui se connecte √† Redshift et lui demande de t√©l√©charger les donn√©es en utilisant le pr√©fixe <code>s3://events-bucket/main/year=2018/month=10/day=14/10_3*</code> <br><br>  Afin de surveiller l'√©tat de la t√¢che de t√©l√©chargement, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apache Airflow</a> : il vous permet de r√©p√©ter l'op√©ration en cas d'erreurs et d'avoir un historique d'ex√©cution clair, ce qui est important pour un grand nombre de ces t√¢ches.  Et en cas de probl√®me, vous pouvez r√©p√©ter le t√©l√©chargement pendant quelques intervalles de temps ou t√©l√©charger les donn√©es ¬´froides¬ª de S3 il y a un an. <br><br>  Dans le m√™me Airflow, de la m√™me mani√®re, selon le planning, les scripts fonctionnent qui se connectent √† la base de donn√©es et effectuent des t√©l√©chargements p√©riodiques √† partir de r√©f√©rentiels externes, ou construisent des agr√©gations d'√©v√©nements sous la forme <code>INSERT INTO ... SELECT ...</code> <br><br>  Redshift a de faibles garanties de disponibilit√©.  Une fois par semaine, jusqu'√† une demi-heure (la fen√™tre de temps est sp√©cifi√©e dans les param√®tres), AWS peut arr√™ter la mise √† jour du cluster ou tout autre travail planifi√©.  En cas de panne sur un n≈ìud, le cluster devient √©galement indisponible jusqu'√† la restauration de l'h√¥te.  Cela prend g√©n√©ralement environ 15 minutes et se produit environ une fois tous les six mois.  Dans le syst√®me actuel, ce n'est pas un probl√®me, il a √©t√© initialement con√ßu pour que la base soit p√©riodiquement indisponible. <br><br>  Sous Redshift, 4 instances ds2.8xlarge ont √©t√© utilis√©es (36 CPU, 16 TB HDD), ce qui nous donne au total 64 TB d'espace disque. <br><br>  Le dernier point est la sauvegarde.  La planification de la sauvegarde peut √™tre sp√©cifi√©e dans les param√®tres du cluster et elle fonctionne correctement. <br><br><h2>  ClickHouse Transition Motivation </h2><br>  Bien s√ªr, s'il n'y avait pas de probl√®mes, personne n'aurait pens√© √† migrer vers ClickHouse.  Mais ils l'√©taient. <br><br>  Si vous regardez le sch√©ma de stockage ClickHouse avec le moteur MergeTree et Redshift, vous pouvez voir que leur id√©ologie est tr√®s similaire.  Les deux bases de donn√©es sont en colonnes, fonctionnent correctement avec un grand nombre de colonnes et compressent tr√®s bien les donn√©es sur le disque (et dans Redshift, vous pouvez configurer les types de compression pour chaque colonne individuelle).  M√™me les donn√©es sont stock√©es de la m√™me mani√®re: elles sont tri√©es par cl√© primaire, ce qui vous permet de lire uniquement des blocs sp√©cifiques et de ne pas conserver des index s√©par√©s en m√©moire, ce qui est important lorsque vous travaillez avec de grandes quantit√©s de donn√©es. <br><br>  La diff√©rence essentielle, comme toujours, r√©side dans les d√©tails. <br><br><h3>  Table journali√®re </h3><br>  Le tri des donn√©es sur le disque et leur suppression dans Redshift se produisent lorsque vous effectuez: <pre> <code class="xml hljs">VACUUM <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tablename</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre>  Dans ce cas, le processus de vide fonctionne avec toutes les donn√©es de ce tableau.  Si vous stockez des donn√©es pour les trois mois dans une table, ce processus prend un temps ind√©cent et vous devez l'ex√©cuter au moins quotidiennement, car les anciennes donn√©es sont supprim√©es et de nouvelles sont ajout√©es.  J'ai d√ª cr√©er des tableaux s√©par√©s pour chaque jour et les combiner via la vue, et ce n'est pas seulement la difficult√© de faire pivoter et de prendre en charge cette vue, mais aussi de ralentir les requ√™tes.  Sur demande, √† en juger par expliquer, toutes les tables ont √©t√© scann√©es.  Et bien que la num√©risation d'une table prenne moins d'une seconde, avec une quantit√© de 90 pi√®ces, il s'av√®re que toute requ√™te prend au moins une minute.  Ce n'est pas tr√®s pratique. <br><br><h3>  Doublons </h3><br>  Le probl√®me suivant est les doublons.  D'une mani√®re ou d'une autre, lors de la transmission de donn√©es sur un r√©seau, il existe deux options: soit perdre des donn√©es, soit recevoir des doublons.  Nous ne pouvions pas perdre de messages, par cons√©quent, nous nous sommes simplement r√©concili√©s avec le fait qu'un petit pourcentage des √©v√©nements seraient dupliqu√©s.  Vous pouvez supprimer des doublons par jour en cr√©ant une nouvelle table, en y ins√©rant des donn√©es de l'ancienne, o√π en utilisant la fonction de fen√™tre vous avez supprim√© des lignes avec un ID en double, en supprimant l'ancienne table et en renommant la nouvelle.  Puisqu'il y avait une vue au-dessus des tables quotidiennes, il fallait ne pas l'oublier et la supprimer pour le moment de renommer les tables.  Dans ce cas, il √©tait √©galement n√©cessaire de surveiller les verrous, sinon, dans le cas d'une requ√™te qui bloquait la vue ou l'une des tables, ce processus pouvait √™tre tra√Æn√© longtemps. <br><br><h3>  Surveillance et maintenance </h3><br>  Pas une seule requ√™te dans Redshift ne prend moins de quelques secondes.  M√™me si vous voulez simplement ajouter un utilisateur ou voir une liste des demandes actives, vous devrez attendre quelques dizaines de secondes.  Bien s√ªr, vous pouvez tol√©rer, et pour cette classe de bases de donn√©es, cela est acceptable, mais en fin de compte, cela se traduit par un tas de temps perdu. <br><br><h3>  Co√ªt </h3><br>  Selon nos calculs, le d√©ploiement de ClickHouse sur des instances AWS avec exactement les m√™mes ressources est exactement la moiti√© du prix.  Bien s√ªr, il devrait en √™tre ainsi, car en utilisant Redshift, vous obtenez une base de donn√©es pr√™te √† l'emploi √† laquelle vous pouvez vous connecter avec n'importe quel client PostgreSQL juste apr√®s avoir cliqu√© sur quelques boutons dans la console AWS, et AWS fera le reste pour vous.  Mais √ßa vaut le coup?  Nous avons d√©j√† l'infrastructure, nous semblons √™tre en mesure de faire des sauvegardes, la surveillance et la configuration, et nous le faisons pour un tas de services internes.  Pourquoi ne pas vous attaquer au support ClickHouse? <br><br><h2>  Processus de transition </h2><br>  Tout d'abord, nous avons cr√©√© une petite installation ClickHouse √† partir d'une seule machine, o√π nous avons commenc√© √† t√©l√©charger p√©riodiquement, √† l'aide des outils int√©gr√©s, des donn√©es √† partir de S3.  Ainsi, nous avons pu tester nos hypoth√®ses sur la vitesse et les capacit√©s de ClickHouse. <br><br>  Apr√®s quelques semaines de tests sur une petite copie des donn√©es, il est devenu clair que pour remplacer Redshift par Clickhouse, plusieurs probl√®mes devaient √™tre r√©solus: <br><br><ul><li>  sur quels types d'instances et de disques d√©ployer; </li><li>  utiliser la r√©plication? </li><li>  comment installer, configurer et ex√©cuter; </li><li>  comment faire le suivi; </li><li>  quel genre de r√©gime sera; </li><li>  comment fournir des donn√©es √† partir de S3; </li><li>  Comment r√©√©crire toutes les requ√™tes de SQL standard √† non standard? </li></ul><br>  <b>Types d'instances et de disques</b> .  En ce qui concerne le nombre de processeurs, disque et m√©moire, ils ont d√©cid√© de s'appuyer sur l'installation actuelle de Redshift.  Il y avait plusieurs options, y compris les instances i3 avec des disques NVMe locaux, mais nous avons d√©cid√© de nous arr√™ter √† r5.4xlarge et le stockage sous la forme de 8T ST1 EBS pour chaque instance.  Selon les estimations, cela aurait d√ª donner des performances comparables √† Redshift pour la moiti√© du co√ªt.  Dans le m√™me temps, en raison de l'utilisation de disques EBS, nous obtenons des sauvegardes et une restauration simples via des instantan√©s de disques, presque comme dans Redshift. <br><br>  <b>R√©plication</b> .  Comme nous sommes partis de ce qui est d√©j√† dans Redshift, nous avons d√©cid√© de ne pas utiliser de r√©plication.  De plus, cela ne nous oblige pas √† √©tudier imm√©diatement ZooKeeper, qui n'est pas encore dans l'infrastructure, mais c'est formidable qu'il soit d√©sormais possible de faire de la r√©plication √† la demande. <br><br>  <b>L'installation</b>  C'est la partie la plus simple.  Un r√¥le suffisamment petit pour Ansible, qui installera des packages RPM pr√™ts √† l'emploi et effectuera la m√™me configuration sur chaque h√¥te. <br><br>  <b>Suivi</b>  Pour surveiller tous les services, Prometheus est utilis√© avec Telegraf et Grafana.Par cons√©quent, ils ont simplement plac√© des agents Telegraf sur des h√¥tes avec ClickHouse, collect√© un tableau de bord √† Grafana, qui montrait la charge actuelle du serveur par processeur, m√©moire et disques.  Gr√¢ce au plugin pour Grafana, nous avons apport√© √† ce tableau de bord les demandes actives actuelles pour le cluster, le statut des importations depuis S3 et d'autres choses utiles.  Il s'est av√©r√© encore meilleur et plus informatif (et nettement plus rapide) que le tableau de bord qui a donn√© la console AWS. <br><br>  <b>Sch√©ma</b> .  L'une de nos principales erreurs dans Redshift a √©t√© de ne mettre que les principaux champs d'√©v√©nement dans des colonnes distinctes et d'ajouter les champs qui sont rarement utilis√©s pour ajouter <br>  dans une grande propri√©t√© de colonne.  D'une part, cela nous a donn√© la flexibilit√© de changer les champs aux √©tapes initiales, quand nous ne savions pas exactement quels √©v√©nements nous allions collecter, avec quelles propri√©t√©s, de plus, ils changeaient 5 fois par jour.  Et d'autre part, les demandes pour une grande colonne de propri√©t√©s ont pris de plus en plus de temps.  Dans ClickHouse, nous avons d√©cid√© de faire la bonne chose tout de suite, nous avons donc collect√© toutes les colonnes possibles et entr√© le type optimal pour elles.  Le r√©sultat est un tableau avec environ deux cents colonnes. <br><br>  La t√¢che suivante consistait √† choisir le bon moteur de stockage et de partitionnement. <br>  Ils n'ont plus pens√© au partitionnement, mais ont fait de m√™me que dans Redshift - une partition pour chaque jour, mais maintenant toutes les partitions sont une table, qui <br>  acc√©l√®re consid√©rablement les demandes et simplifie la maintenance.  Le moteur de stockage a √©t√© pris par ReplacingMergeTree, car il vous permet de supprimer les doublons d'une partition particuli√®re, simplement en faisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OPTIMIZE ... FINAL</a> .  De plus, le sch√©ma de partitionnement quotidien permet, en cas d'erreurs ou d'accidents, de ne travailler qu'avec des donn√©es pendant une journ√©e, pas un mois, ce qui est beaucoup plus rapide. <br><br>  <b>Livraison des donn√©es de s3 √† ClickHouse</b> .  Ce fut l'un des processus les plus longs.  Cela n'a tout simplement pas fonctionn√© en effectuant le chargement par les outils ClickHouse int√©gr√©s, car les donn√©es sur S3 sont en JSON, chaque champ doit √™tre extrait dans son propre jsonpath, comme nous l'avons fait dans Redshift, et parfois nous devons √©galement utiliser la transformation: par exemple, l'UUID d'un message provenant d'un enregistrement standard sous la forme <code>DD96C92F-3F4D-44C6-BCD3-E25EB26389E9</code> convertir en octets et mettre en type FixedString (16). <br><br>  Je voulais avoir un service sp√©cial similaire √† ce que nous avions dans Redshift en tant que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">commande COPY</a> .  Ils n'ont rien trouv√© de pr√™t, j'ai donc d√ª le faire.  Vous pouvez √©crire un article s√©par√© sur son fonctionnement, mais en bref, il s'agit d'un service HTTP d√©ploy√© sur chaque h√¥te avec ClickHouse.  Vous pouvez vous r√©f√©rer √† l'un d'eux.  Les param√®tres de demande sp√©cifient le pr√©fixe S3 √† partir duquel les fichiers sont extraits, la liste jsonpath pour la conversion de JSON en un ensemble de colonnes, ainsi qu'un ensemble de conversions pour chaque colonne.  Le serveur auquel la demande est arriv√©e commence √† analyser les fichiers sur S3 et √† distribuer le travail d'analyse aux autres h√¥tes.  Dans le m√™me temps, il est important pour nous que les lignes qui n'ont pas pu √™tre import√©es, ainsi que l'erreur, soient ajout√©es √† une table ClickHouse distincte.  Cela aide beaucoup √† enqu√™ter sur les probl√®mes et les bogues dans le service de r√©ception d'√©v√©nements et les clients qui g√©n√®rent ces √©v√©nements.  Avec le placement de l'importateur directement sur les h√¥tes de la base de donn√©es, nous avons utilis√© ces ressources qui, en r√®gle g√©n√©rale, sont inactives, car les demandes complexes ne tournent pas 24 heures sur 24.  Bien s√ªr, s'il y a plus de demandes, vous pouvez toujours prendre le service de l'importateur sur des h√¥tes s√©par√©s. <br><br>  Il n'y a pas eu de gros probl√®mes avec l'importation de donn√©es √† partir de sources externes.  Dans ces scripts qui l'√©taient, ils ont juste chang√© la destination de Redshift √† ClickHouse. <br><br>  Il y avait une option pour connecter MongoDB sous la forme d'un dictionnaire et ne pas faire de copies quotidiennes.  Malheureusement, cela ne convenait pas, car le dictionnaire doit √™tre plac√© en m√©moire, et la taille de la plupart des collections dans MongoDB ne le permet pas.  Mais les dictionnaires nous ont √©galement √©t√© utiles: leur utilisation est tr√®s pratique pour connecter les bases de donn√©es GeoIP de MaxMind et les utiliser dans les requ√™tes.  Pour cela, nous utilisons les fichiers de mise en page ip_trie et CSV fournis par le service.  Par exemple, la configuration du dictionnaire geoip_asn_blocks_ipv4 ressemble √† ceci: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>geoip_asn_blocks_ipv4<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span>GeoLite2-ASN-Blocks-IPv4.csv<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span>CSVWithNames<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">\</span></span></span><span class="hljs-tag">/</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span>300<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">ip_trie</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>prefix<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_number<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>UInt32<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>0<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_organization<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>?<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br>  Il suffit de mettre cette configuration dans <code>/etc/clickhouse-server/geoip_asn_blocks_ipv4_dictionary.xml</code> , apr√®s quoi vous pouvez faire des requ√™tes dans le dictionnaire pour obtenir le nom du fournisseur par adresse IP: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dictGetString(<span class="hljs-string"><span class="hljs-string">'geoip_asn_blocks_ipv4'</span></span>, <span class="hljs-string"><span class="hljs-string">'autonomous_system_organization'</span></span>, tuple(IPv4StringToNum(<span class="hljs-string"><span class="hljs-string">'192.168.1.1'</span></span>)));</code> </pre><br>  <b>Modifier le sch√©ma de donn√©es</b> .  Comme mentionn√© ci-dessus, nous avons d√©cid√© de ne pas encore utiliser la r√©plication, car nous pouvons maintenant nous permettre de devenir inaccessibles en cas d'accident ou de travail planifi√©, et une copie des donn√©es est d√©j√† sur s3 et nous pouvons la transf√©rer vers ClickHouse dans un d√©lai raisonnable.  S'il n'y a pas de r√©plication, ils n'ont pas d√©velopp√© ZooKeeper et l'absence de ZooKeeper conduit √©galement √† l'impossibilit√© d'utiliser l'expression ON CLUSTER dans les requ√™tes DDL.  Ce probl√®me a √©t√© r√©solu par un petit script python qui se connecte √† chaque h√¥te ClickHouse (il n'y en a que huit jusqu'√† pr√©sent) et ex√©cute la requ√™te SQL sp√©cifi√©e. <br><br>  <b>Prise en charge SQL incompl√®te dans ClickHouse</b> .  Le processus de transfert des requ√™tes de la syntaxe Redshift vers la syntaxe ClickHouse s'est d√©roul√© parall√®lement au d√©veloppement de l'importateur et a √©t√© principalement trait√© par une √©quipe d'analystes.  Curieusement, mais le probl√®me n'√©tait m√™me pas dans le JOIN, mais dans les fonctions de la fen√™tre.  Pour comprendre comment cela peut √™tre fait via des tableaux et des fonctions lambda, cela a pris plusieurs jours.  Il est bon que ce probl√®me soit souvent trait√© dans les rapports sur ClickHouse, dont il existe un grand nombre, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">events.yandex.ru/lib/talks/5420</a> .  √Ä ce stade, les donn√©es √©taient d√©j√† √©crites √† la fois √† deux endroits: √† la fois dans Redshift et dans le nouveau ClickHouse, donc lorsque nous avons transf√©r√© les demandes, nous avons compar√© les r√©sultats.  La comparaison de la vitesse a √©t√© probl√©matique, car nous avons supprim√© une grande colonne de propri√©t√©s, et la plupart des requ√™tes ont commenc√© √† fonctionner uniquement avec les colonnes n√©cessaires, ce qui, bien s√ªr, a consid√©rablement augment√©, mais les requ√™tes auxquelles la colonne des propri√©t√©s n'a pas particip√©, ont fonctionn√© de la m√™me mani√®re, ou un peu plus vite. <br><br>  En cons√©quence, nous avons obtenu le sch√©ma suivant: <br><br><img src="https://habrastorage.org/webt/tj/h8/ka/tjh8kagqccdbjgnswbmbm9wkloc.png"><br><br><h2>  R√©sultats </h2><br>  En fin de compte, nous avons obtenu les avantages suivants: <br><br><ul><li>  Une table au lieu de 90 </li><li>  Les demandes de service sont ex√©cut√©es en millisecondes </li><li>  Le co√ªt a diminu√© de moiti√© </li><li>  Suppression facile des √©v√©nements en double </li></ul><br>  Il existe √©galement des inconv√©nients pour lesquels nous sommes pr√™ts: <br><br><ul><li>  En cas d'accident, vous devrez r√©parer vous-m√™me le cluster </li><li>  Les modifications de sch√©ma doivent maintenant √™tre effectu√©es sur chaque h√¥te s√©par√©ment </li><li>  La mise √† jour vers de nouvelles versions devra se faire vous-m√™me </li></ul><br>  Nous ne pouvons pas comparer la vitesse des demandes de front, car le sch√©ma de donn√©es a consid√©rablement chang√©.  De nombreuses requ√™tes sont devenues plus rapides, simplement parce qu'elles lisent moins de donn√©es sur le disque.  Dans le bon sens, un tel changement a d√ª √™tre effectu√© dans Redshift, mais il a √©t√© d√©cid√© de le combiner avec la migration vers ClickHouse. <br><br>  La migration et la pr√©paration ont dur√© environ trois mois.  Elle a march√© du d√©but juillet √† la fin septembre et a exig√© la participation de deux personnes.  Le 27 septembre, nous avons d√©sactiv√© Redshift et depuis lors, nous ne travaillons que sur ClickHouse.  Il s'av√®re, d√©j√† un peu plus de deux mois.  Le terme est court, mais n'a jusqu'√† pr√©sent jamais rencontr√© de perte de donn√©es ou de bogue critique, √† cause duquel l'ensemble du cluster se l√®verait.  Nous attendons les mises √† jour des nouvelles versions! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr433346/">https://habr.com/ru/post/fr433346/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr433336/index.html">L'impl√©mentation de la biblioth√®que babylonienne</a></li>
<li><a href="../fr433338/index.html">Pr√©sentation du fabricant d'imprimantes 3D Creality</a></li>
<li><a href="../fr433340/index.html">Appareils sans fil Xiaomi dans la maison intelligente ioBroker</a></li>
<li><a href="../fr433342/index.html">Un autre processeur Verilog simple</a></li>
<li><a href="../fr433344/index.html">Deux succ√®s de l'espace priv√©</a></li>
<li><a href="../fr433348/index.html">DSL typ√© en TypeScript de JSX</a></li>
<li><a href="../fr433350/index.html">√âv√©nements num√©riques √† Moscou du 17 au 23 d√©cembre</a></li>
<li><a href="../fr433352/index.html">Le condens√© de mati√®res fra√Æches du monde du front-end de la derni√®re semaine n ¬∞ 343 (10-16 d√©cembre 2018)</a></li>
<li><a href="../fr433354/index.html">Nouvelles du monde d'OpenStreetMap n ¬∞ 438 (12/04/2018 - 12/10/2018)</a></li>
<li><a href="../fr433356/index.html">Les attaquants ont appris √† contourner l'authentification √† deux facteurs Yahoo Mail et Gmail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>