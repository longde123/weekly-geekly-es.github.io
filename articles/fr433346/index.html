<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🙆🏼 🎛️ 👩🏽‍🔬 Passer de Redshift à ClickHouse 🧑🏾 💆🏻 📦</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pendant longtemps, iFunny a utilisé Redshift comme base de données pour les événements qui se produisent dans les services backend et les applications...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Passer de Redshift à ClickHouse</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/funcorp/blog/433346/"><img src="https://habrastorage.org/webt/s8/xo/0d/s8xo0dnodxojhff6jufnruyg660.jpeg"><br><br>  Pendant longtemps, iFunny a utilisé Redshift comme base de données pour les événements qui se produisent dans les services backend et les applications mobiles.  Il a été choisi parce qu'au moment de la mise en œuvre, il n'y avait, dans l'ensemble, aucune alternative comparable en termes de coût et de commodité. <br><br>  Cependant, tout a changé après la sortie publique de ClickHouse.  Nous l'avons étudié pendant longtemps, comparé le coût, estimé l'architecture approximative, et enfin, cet été, nous avons décidé de voir à quel point il nous était utile.  Dans cet article, vous découvrirez le problème que Redshift nous a aidé à résoudre et comment nous avons déplacé cette solution vers ClickHouse. <br><a name="habracut"></a><br><h2>  Le problème </h2><br>  iFunny avait besoin d'un service similaire à Yandex.Metrica, mais exclusivement pour la consommation domestique.  Je vais vous expliquer pourquoi. <br><br>  Les clients externes écrivent des événements.  Il peut s'agir d'applications mobiles, de sites Web ou de services backend internes.  Il est très difficile pour ces clients d'expliquer que le service d'accueil événementiel est actuellement indisponible, «essayez de l'envoyer en 15 minutes ou en une heure».  Il y a beaucoup de clients, ils veulent envoyer des événements tout le temps et ne peuvent pas attendre du tout. <br><br>  Contrairement à eux, il existe des services internes et des utilisateurs assez tolérants à cet égard: ils peuvent fonctionner correctement même avec un service d'analyse inaccessible.  Et la plupart des métriques du produit et les résultats des tests A / B sont généralement judicieux à regarder une seule fois par jour, voire moins souvent.  Par conséquent, les exigences de lecture sont assez faibles.  En cas d'accident ou de mise à jour, nous pouvons nous permettre d'être inaccessibles ou incohérents en lecture pendant plusieurs heures voire plusieurs jours (dans un cas particulièrement négligé). <br><br>  Si nous parlons de chiffres, nous devons prendre environ cinq milliards d'événements (300 Go de données compressées) par jour, tout en stockant les données pendant trois mois sous une forme «à chaud», accessible pour les requêtes SQL et dans une «froide» pendant deux ans ou plus, mais pour qu'en quelques jours nous puissions les transformer en "chaud". <br><br>  Fondamentalement, les données sont une collection d'événements classés par heure.  Il existe environ trois cents types d'événements, chacun ayant son propre ensemble de propriétés.  Il existe encore des données provenant de sources tierces qui doivent être synchronisées avec la base de données analytiques: par exemple, une collection d'installations d'applications à partir de MongoDB ou un service AppsFlyer externe. <br><br>  Il s'avère que pour la base de données, nous avons besoin d'environ 40 To de disque et pour le stockage «froid» - environ 250 To de plus. <br><br><h2>  Redshift Solution </h2><br><img src="https://habrastorage.org/webt/f0/nq/dl/f0nqdl7cvriq9ygc3jlhelfdlqi.png"><br><br>  Il existe donc des clients mobiles et des services backend à partir desquels vous devez recevoir des événements.  Le service HTTP accepte les données, effectue la validation minimale, collecte les événements sur le disque local dans des fichiers regroupés par minute, les comprime immédiatement et les envoie au compartiment S3.  La disponibilité de ce service dépend de la disponibilité des serveurs avec l'application et AWS S3.  Les applications ne stockent pas l'état, elles sont donc facilement équilibrées, mises à l'échelle et échangées.  S3 est un service de stockage de fichiers relativement simple avec une bonne réputation et une bonne disponibilité, vous pouvez donc vous y fier. <br><br>  Ensuite, vous devez en quelque sorte livrer les données à Redshift.  Tout est assez simple ici: Redshift a un importateur S3 intégré, qui est la méthode recommandée pour charger les données.  Par conséquent, une fois toutes les 10 minutes, un script démarre qui se connecte à Redshift et lui demande de télécharger les données en utilisant le préfixe <code>s3://events-bucket/main/year=2018/month=10/day=14/10_3*</code> <br><br>  Afin de surveiller l'état de la tâche de téléchargement, nous utilisons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apache Airflow</a> : il vous permet de répéter l'opération en cas d'erreurs et d'avoir un historique d'exécution clair, ce qui est important pour un grand nombre de ces tâches.  Et en cas de problème, vous pouvez répéter le téléchargement pendant quelques intervalles de temps ou télécharger les données «froides» de S3 il y a un an. <br><br>  Dans le même Airflow, de la même manière, selon le planning, les scripts fonctionnent qui se connectent à la base de données et effectuent des téléchargements périodiques à partir de référentiels externes, ou construisent des agrégations d'événements sous la forme <code>INSERT INTO ... SELECT ...</code> <br><br>  Redshift a de faibles garanties de disponibilité.  Une fois par semaine, jusqu'à une demi-heure (la fenêtre de temps est spécifiée dans les paramètres), AWS peut arrêter la mise à jour du cluster ou tout autre travail planifié.  En cas de panne sur un nœud, le cluster devient également indisponible jusqu'à la restauration de l'hôte.  Cela prend généralement environ 15 minutes et se produit environ une fois tous les six mois.  Dans le système actuel, ce n'est pas un problème, il a été initialement conçu pour que la base soit périodiquement indisponible. <br><br>  Sous Redshift, 4 instances ds2.8xlarge ont été utilisées (36 CPU, 16 TB HDD), ce qui nous donne au total 64 TB d'espace disque. <br><br>  Le dernier point est la sauvegarde.  La planification de la sauvegarde peut être spécifiée dans les paramètres du cluster et elle fonctionne correctement. <br><br><h2>  ClickHouse Transition Motivation </h2><br>  Bien sûr, s'il n'y avait pas de problèmes, personne n'aurait pensé à migrer vers ClickHouse.  Mais ils l'étaient. <br><br>  Si vous regardez le schéma de stockage ClickHouse avec le moteur MergeTree et Redshift, vous pouvez voir que leur idéologie est très similaire.  Les deux bases de données sont en colonnes, fonctionnent correctement avec un grand nombre de colonnes et compressent très bien les données sur le disque (et dans Redshift, vous pouvez configurer les types de compression pour chaque colonne individuelle).  Même les données sont stockées de la même manière: elles sont triées par clé primaire, ce qui vous permet de lire uniquement des blocs spécifiques et de ne pas conserver des index séparés en mémoire, ce qui est important lorsque vous travaillez avec de grandes quantités de données. <br><br>  La différence essentielle, comme toujours, réside dans les détails. <br><br><h3>  Table journalière </h3><br>  Le tri des données sur le disque et leur suppression dans Redshift se produisent lorsque vous effectuez: <pre> <code class="xml hljs">VACUUM <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tablename</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre>  Dans ce cas, le processus de vide fonctionne avec toutes les données de ce tableau.  Si vous stockez des données pour les trois mois dans une table, ce processus prend un temps indécent et vous devez l'exécuter au moins quotidiennement, car les anciennes données sont supprimées et de nouvelles sont ajoutées.  J'ai dû créer des tableaux séparés pour chaque jour et les combiner via la vue, et ce n'est pas seulement la difficulté de faire pivoter et de prendre en charge cette vue, mais aussi de ralentir les requêtes.  Sur demande, à en juger par expliquer, toutes les tables ont été scannées.  Et bien que la numérisation d'une table prenne moins d'une seconde, avec une quantité de 90 pièces, il s'avère que toute requête prend au moins une minute.  Ce n'est pas très pratique. <br><br><h3>  Doublons </h3><br>  Le problème suivant est les doublons.  D'une manière ou d'une autre, lors de la transmission de données sur un réseau, il existe deux options: soit perdre des données, soit recevoir des doublons.  Nous ne pouvions pas perdre de messages, par conséquent, nous nous sommes simplement réconciliés avec le fait qu'un petit pourcentage des événements seraient dupliqués.  Vous pouvez supprimer des doublons par jour en créant une nouvelle table, en y insérant des données de l'ancienne, où en utilisant la fonction de fenêtre vous avez supprimé des lignes avec un ID en double, en supprimant l'ancienne table et en renommant la nouvelle.  Puisqu'il y avait une vue au-dessus des tables quotidiennes, il fallait ne pas l'oublier et la supprimer pour le moment de renommer les tables.  Dans ce cas, il était également nécessaire de surveiller les verrous, sinon, dans le cas d'une requête qui bloquait la vue ou l'une des tables, ce processus pouvait être traîné longtemps. <br><br><h3>  Surveillance et maintenance </h3><br>  Pas une seule requête dans Redshift ne prend moins de quelques secondes.  Même si vous voulez simplement ajouter un utilisateur ou voir une liste des demandes actives, vous devrez attendre quelques dizaines de secondes.  Bien sûr, vous pouvez tolérer, et pour cette classe de bases de données, cela est acceptable, mais en fin de compte, cela se traduit par un tas de temps perdu. <br><br><h3>  Coût </h3><br>  Selon nos calculs, le déploiement de ClickHouse sur des instances AWS avec exactement les mêmes ressources est exactement la moitié du prix.  Bien sûr, il devrait en être ainsi, car en utilisant Redshift, vous obtenez une base de données prête à l'emploi à laquelle vous pouvez vous connecter avec n'importe quel client PostgreSQL juste après avoir cliqué sur quelques boutons dans la console AWS, et AWS fera le reste pour vous.  Mais ça vaut le coup?  Nous avons déjà l'infrastructure, nous semblons être en mesure de faire des sauvegardes, la surveillance et la configuration, et nous le faisons pour un tas de services internes.  Pourquoi ne pas vous attaquer au support ClickHouse? <br><br><h2>  Processus de transition </h2><br>  Tout d'abord, nous avons créé une petite installation ClickHouse à partir d'une seule machine, où nous avons commencé à télécharger périodiquement, à l'aide des outils intégrés, des données à partir de S3.  Ainsi, nous avons pu tester nos hypothèses sur la vitesse et les capacités de ClickHouse. <br><br>  Après quelques semaines de tests sur une petite copie des données, il est devenu clair que pour remplacer Redshift par Clickhouse, plusieurs problèmes devaient être résolus: <br><br><ul><li>  sur quels types d'instances et de disques déployer; </li><li>  utiliser la réplication? </li><li>  comment installer, configurer et exécuter; </li><li>  comment faire le suivi; </li><li>  quel genre de régime sera; </li><li>  comment fournir des données à partir de S3; </li><li>  Comment réécrire toutes les requêtes de SQL standard à non standard? </li></ul><br>  <b>Types d'instances et de disques</b> .  En ce qui concerne le nombre de processeurs, disque et mémoire, ils ont décidé de s'appuyer sur l'installation actuelle de Redshift.  Il y avait plusieurs options, y compris les instances i3 avec des disques NVMe locaux, mais nous avons décidé de nous arrêter à r5.4xlarge et le stockage sous la forme de 8T ST1 EBS pour chaque instance.  Selon les estimations, cela aurait dû donner des performances comparables à Redshift pour la moitié du coût.  Dans le même temps, en raison de l'utilisation de disques EBS, nous obtenons des sauvegardes et une restauration simples via des instantanés de disques, presque comme dans Redshift. <br><br>  <b>Réplication</b> .  Comme nous sommes partis de ce qui est déjà dans Redshift, nous avons décidé de ne pas utiliser de réplication.  De plus, cela ne nous oblige pas à étudier immédiatement ZooKeeper, qui n'est pas encore dans l'infrastructure, mais c'est formidable qu'il soit désormais possible de faire de la réplication à la demande. <br><br>  <b>L'installation</b>  C'est la partie la plus simple.  Un rôle suffisamment petit pour Ansible, qui installera des packages RPM prêts à l'emploi et effectuera la même configuration sur chaque hôte. <br><br>  <b>Suivi</b>  Pour surveiller tous les services, Prometheus est utilisé avec Telegraf et Grafana.Par conséquent, ils ont simplement placé des agents Telegraf sur des hôtes avec ClickHouse, collecté un tableau de bord à Grafana, qui montrait la charge actuelle du serveur par processeur, mémoire et disques.  Grâce au plugin pour Grafana, nous avons apporté à ce tableau de bord les demandes actives actuelles pour le cluster, le statut des importations depuis S3 et d'autres choses utiles.  Il s'est avéré encore meilleur et plus informatif (et nettement plus rapide) que le tableau de bord qui a donné la console AWS. <br><br>  <b>Schéma</b> .  L'une de nos principales erreurs dans Redshift a été de ne mettre que les principaux champs d'événement dans des colonnes distinctes et d'ajouter les champs qui sont rarement utilisés pour ajouter <br>  dans une grande propriété de colonne.  D'une part, cela nous a donné la flexibilité de changer les champs aux étapes initiales, quand nous ne savions pas exactement quels événements nous allions collecter, avec quelles propriétés, de plus, ils changeaient 5 fois par jour.  Et d'autre part, les demandes pour une grande colonne de propriétés ont pris de plus en plus de temps.  Dans ClickHouse, nous avons décidé de faire la bonne chose tout de suite, nous avons donc collecté toutes les colonnes possibles et entré le type optimal pour elles.  Le résultat est un tableau avec environ deux cents colonnes. <br><br>  La tâche suivante consistait à choisir le bon moteur de stockage et de partitionnement. <br>  Ils n'ont plus pensé au partitionnement, mais ont fait de même que dans Redshift - une partition pour chaque jour, mais maintenant toutes les partitions sont une table, qui <br>  accélère considérablement les demandes et simplifie la maintenance.  Le moteur de stockage a été pris par ReplacingMergeTree, car il vous permet de supprimer les doublons d'une partition particulière, simplement en faisant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OPTIMIZE ... FINAL</a> .  De plus, le schéma de partitionnement quotidien permet, en cas d'erreurs ou d'accidents, de ne travailler qu'avec des données pendant une journée, pas un mois, ce qui est beaucoup plus rapide. <br><br>  <b>Livraison des données de s3 à ClickHouse</b> .  Ce fut l'un des processus les plus longs.  Cela n'a tout simplement pas fonctionné en effectuant le chargement par les outils ClickHouse intégrés, car les données sur S3 sont en JSON, chaque champ doit être extrait dans son propre jsonpath, comme nous l'avons fait dans Redshift, et parfois nous devons également utiliser la transformation: par exemple, l'UUID d'un message provenant d'un enregistrement standard sous la forme <code>DD96C92F-3F4D-44C6-BCD3-E25EB26389E9</code> convertir en octets et mettre en type FixedString (16). <br><br>  Je voulais avoir un service spécial similaire à ce que nous avions dans Redshift en tant que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">commande COPY</a> .  Ils n'ont rien trouvé de prêt, j'ai donc dû le faire.  Vous pouvez écrire un article séparé sur son fonctionnement, mais en bref, il s'agit d'un service HTTP déployé sur chaque hôte avec ClickHouse.  Vous pouvez vous référer à l'un d'eux.  Les paramètres de demande spécifient le préfixe S3 à partir duquel les fichiers sont extraits, la liste jsonpath pour la conversion de JSON en un ensemble de colonnes, ainsi qu'un ensemble de conversions pour chaque colonne.  Le serveur auquel la demande est arrivée commence à analyser les fichiers sur S3 et à distribuer le travail d'analyse aux autres hôtes.  Dans le même temps, il est important pour nous que les lignes qui n'ont pas pu être importées, ainsi que l'erreur, soient ajoutées à une table ClickHouse distincte.  Cela aide beaucoup à enquêter sur les problèmes et les bogues dans le service de réception d'événements et les clients qui génèrent ces événements.  Avec le placement de l'importateur directement sur les hôtes de la base de données, nous avons utilisé ces ressources qui, en règle générale, sont inactives, car les demandes complexes ne tournent pas 24 heures sur 24.  Bien sûr, s'il y a plus de demandes, vous pouvez toujours prendre le service de l'importateur sur des hôtes séparés. <br><br>  Il n'y a pas eu de gros problèmes avec l'importation de données à partir de sources externes.  Dans ces scripts qui l'étaient, ils ont juste changé la destination de Redshift à ClickHouse. <br><br>  Il y avait une option pour connecter MongoDB sous la forme d'un dictionnaire et ne pas faire de copies quotidiennes.  Malheureusement, cela ne convenait pas, car le dictionnaire doit être placé en mémoire, et la taille de la plupart des collections dans MongoDB ne le permet pas.  Mais les dictionnaires nous ont également été utiles: leur utilisation est très pratique pour connecter les bases de données GeoIP de MaxMind et les utiliser dans les requêtes.  Pour cela, nous utilisons les fichiers de mise en page ip_trie et CSV fournis par le service.  Par exemple, la configuration du dictionnaire geoip_asn_blocks_ipv4 ressemble à ceci: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>geoip_asn_blocks_ipv4<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span>GeoLite2-ASN-Blocks-IPv4.csv<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">path</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span>CSVWithNames<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">format</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">file</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">\</span></span></span><span class="hljs-tag">/</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">source</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span>300<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">lifetime</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">ip_trie</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">layout</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>prefix<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">key</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_number<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>UInt32<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>0<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span>autonomous_system_organization<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">name</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span>String<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">type</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span>?<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">null_value</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">attribute</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">structure</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionary</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dictionaries</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br>  Il suffit de mettre cette configuration dans <code>/etc/clickhouse-server/geoip_asn_blocks_ipv4_dictionary.xml</code> , après quoi vous pouvez faire des requêtes dans le dictionnaire pour obtenir le nom du fournisseur par adresse IP: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dictGetString(<span class="hljs-string"><span class="hljs-string">'geoip_asn_blocks_ipv4'</span></span>, <span class="hljs-string"><span class="hljs-string">'autonomous_system_organization'</span></span>, tuple(IPv4StringToNum(<span class="hljs-string"><span class="hljs-string">'192.168.1.1'</span></span>)));</code> </pre><br>  <b>Modifier le schéma de données</b> .  Comme mentionné ci-dessus, nous avons décidé de ne pas encore utiliser la réplication, car nous pouvons maintenant nous permettre de devenir inaccessibles en cas d'accident ou de travail planifié, et une copie des données est déjà sur s3 et nous pouvons la transférer vers ClickHouse dans un délai raisonnable.  S'il n'y a pas de réplication, ils n'ont pas développé ZooKeeper et l'absence de ZooKeeper conduit également à l'impossibilité d'utiliser l'expression ON CLUSTER dans les requêtes DDL.  Ce problème a été résolu par un petit script python qui se connecte à chaque hôte ClickHouse (il n'y en a que huit jusqu'à présent) et exécute la requête SQL spécifiée. <br><br>  <b>Prise en charge SQL incomplète dans ClickHouse</b> .  Le processus de transfert des requêtes de la syntaxe Redshift vers la syntaxe ClickHouse s'est déroulé parallèlement au développement de l'importateur et a été principalement traité par une équipe d'analystes.  Curieusement, mais le problème n'était même pas dans le JOIN, mais dans les fonctions de la fenêtre.  Pour comprendre comment cela peut être fait via des tableaux et des fonctions lambda, cela a pris plusieurs jours.  Il est bon que ce problème soit souvent traité dans les rapports sur ClickHouse, dont il existe un grand nombre, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">events.yandex.ru/lib/talks/5420</a> .  À ce stade, les données étaient déjà écrites à la fois à deux endroits: à la fois dans Redshift et dans le nouveau ClickHouse, donc lorsque nous avons transféré les demandes, nous avons comparé les résultats.  La comparaison de la vitesse a été problématique, car nous avons supprimé une grande colonne de propriétés, et la plupart des requêtes ont commencé à fonctionner uniquement avec les colonnes nécessaires, ce qui, bien sûr, a considérablement augmenté, mais les requêtes auxquelles la colonne des propriétés n'a pas participé, ont fonctionné de la même manière, ou un peu plus vite. <br><br>  En conséquence, nous avons obtenu le schéma suivant: <br><br><img src="https://habrastorage.org/webt/tj/h8/ka/tjh8kagqccdbjgnswbmbm9wkloc.png"><br><br><h2>  Résultats </h2><br>  En fin de compte, nous avons obtenu les avantages suivants: <br><br><ul><li>  Une table au lieu de 90 </li><li>  Les demandes de service sont exécutées en millisecondes </li><li>  Le coût a diminué de moitié </li><li>  Suppression facile des événements en double </li></ul><br>  Il existe également des inconvénients pour lesquels nous sommes prêts: <br><br><ul><li>  En cas d'accident, vous devrez réparer vous-même le cluster </li><li>  Les modifications de schéma doivent maintenant être effectuées sur chaque hôte séparément </li><li>  La mise à jour vers de nouvelles versions devra se faire vous-même </li></ul><br>  Nous ne pouvons pas comparer la vitesse des demandes de front, car le schéma de données a considérablement changé.  De nombreuses requêtes sont devenues plus rapides, simplement parce qu'elles lisent moins de données sur le disque.  Dans le bon sens, un tel changement a dû être effectué dans Redshift, mais il a été décidé de le combiner avec la migration vers ClickHouse. <br><br>  La migration et la préparation ont duré environ trois mois.  Elle a marché du début juillet à la fin septembre et a exigé la participation de deux personnes.  Le 27 septembre, nous avons désactivé Redshift et depuis lors, nous ne travaillons que sur ClickHouse.  Il s'avère, déjà un peu plus de deux mois.  Le terme est court, mais n'a jusqu'à présent jamais rencontré de perte de données ou de bogue critique, à cause duquel l'ensemble du cluster se lèverait.  Nous attendons les mises à jour des nouvelles versions! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr433346/">https://habr.com/ru/post/fr433346/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr433336/index.html">L'implémentation de la bibliothèque babylonienne</a></li>
<li><a href="../fr433338/index.html">Présentation du fabricant d'imprimantes 3D Creality</a></li>
<li><a href="../fr433340/index.html">Appareils sans fil Xiaomi dans la maison intelligente ioBroker</a></li>
<li><a href="../fr433342/index.html">Un autre processeur Verilog simple</a></li>
<li><a href="../fr433344/index.html">Deux succès de l'espace privé</a></li>
<li><a href="../fr433348/index.html">DSL typé en TypeScript de JSX</a></li>
<li><a href="../fr433350/index.html">Événements numériques à Moscou du 17 au 23 décembre</a></li>
<li><a href="../fr433352/index.html">Le condensé de matières fraîches du monde du front-end de la dernière semaine n ° 343 (10-16 décembre 2018)</a></li>
<li><a href="../fr433354/index.html">Nouvelles du monde d'OpenStreetMap n ° 438 (12/04/2018 - 12/10/2018)</a></li>
<li><a href="../fr433356/index.html">Les attaquants ont appris à contourner l'authentification à deux facteurs Yahoo Mail et Gmail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>