<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ù§Ô∏è ‚õπüèº ‚§µÔ∏è Was ist falsch an Reinforcement Learning? üöÖ üïë üéóÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Anfang 2018 wurde ein Artikel ver√∂ffentlicht. Deep Reinforcement Learning funktioniert noch nicht ("Lernen mit Verst√§rkung funktioniert noch nicht.")....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Was ist falsch an Reinforcement Learning?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/437020/"><p><img src="https://habrastorage.org/webt/hv/1l/vs/hv1lvsyszoctmnrbxex7valfo8a.jpeg"></p><br><p>  Anfang 2018 wurde ein Artikel ver√∂ffentlicht. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deep Reinforcement Learning funktioniert noch nicht</a> ("Lernen mit Verst√§rkung funktioniert noch nicht.").  Die Hauptbeschwerde war, dass moderne Lernalgorithmen mit Verst√§rkung ungef√§hr die gleiche Zeit ben√∂tigen, um ein Problem zu l√∂sen wie eine regul√§re Zufallssuche. </p><br><p>  Hat sich seitdem etwas ge√§ndert?  Nein. </p><br><p>  Verst√§rktes Lernen wird als einer der drei Hauptpfade zum Aufbau einer starken KI angesehen.  Die Schwierigkeiten in diesem Bereich des maschinellen Lernens und die Methoden, mit denen Wissenschaftler versuchen, mit diesen Schwierigkeiten umzugehen, lassen jedoch darauf schlie√üen, dass dieser Ansatz selbst m√∂glicherweise grundlegende Probleme aufweist. </p><a name="habracut"></a><br><h2 id="postoyte-chto-znachit-odin-iz-treh-a-ostalnye-dva-kakie">  Warten Sie, was bedeutet einer von drei?  Was sind die anderen beiden? </h2><br><p>  Angesichts des Erfolgs neuronaler Netze in den letzten Jahren und der Analyse, wie sie mit kognitiven F√§higkeiten auf hoher Ebene arbeiten, die fr√ºher nur f√ºr Menschen und h√∂here Tiere als charakteristisch angesehen wurden, gibt es heute in der wissenschaftlichen Gemeinschaft die Meinung, dass es drei Hauptans√§tze zur Schaffung einer starken KI gibt die Basis neuronaler Netze, die als mehr oder weniger realistisch angesehen werden k√∂nnen: </p><br><h2 id="1-obrabotka-tekstov">  1. Textverarbeitung </h2><br><p>  Die Welt hat eine gro√üe Anzahl von B√ºchern und Texten im Internet gesammelt, einschlie√ülich Lehrb√ºchern und Nachschlagewerken.  Der Text ist bequem und schnell f√ºr die Verarbeitung auf einem Computer.  Theoretisch sollte diese Reihe von Texten ausreichen, um eine starke Konversations-KI zu trainieren. </p><br><p>  Es wird impliziert, dass sich in diesen Textfeldern die gesamte Struktur der Welt widerspiegelt (zumindest wird sie in Lehrb√ºchern und Nachschlagewerken beschrieben).  Dies ist jedoch √ºberhaupt keine Tatsache.  Texte als Form der Darstellung von Informationen sind stark von der realen dreidimensionalen Welt und dem Zeitverlauf, in dem wir leben, getrennt. </p><br><p>  Gute Beispiele f√ºr KI, die auf Textarrays trainiert wurden, sind Chat-Bots und automatische √úbersetzer.  Um den Text zu √ºbersetzen, m√ºssen Sie die Bedeutung der Phrase verstehen und sie in neuen W√∂rtern (in einer anderen Sprache) nacherz√§hlen.  Es gibt ein weit verbreitetes Missverst√§ndnis, dass Grammatik- und Syntaxregeln, einschlie√ülich einer Beschreibung aller m√∂glichen Ausnahmen, eine bestimmte Sprache vollst√§ndig beschreiben.  Es ist nicht so.  Sprache ist nur ein Hilfsmittel im Leben, sie √§ndert sich leicht und passt sich neuen Situationen an. </p><br><p>  Das Problem bei der Textverarbeitung (auch durch Expertensysteme, sogar neuronale Netze) besteht darin, dass <strong>es keine</strong> Regeln gibt, welche Phrasen in welchen Situationen angewendet werden sollten.  Bitte beachten Sie - nicht die Regeln f√ºr die Erstellung der Phrasen selbst (was Grammatik und Syntax bewirken), sondern welche Phrasen in welchen Situationen.  In der gleichen Situation sprechen Menschen Phrasen in verschiedenen Sprachen aus, die im Allgemeinen in Bezug auf die Struktur der Sprache nicht miteinander verwandt sind.  Vergleichen Sie S√§tze mit √§u√üerster √úberraschung: "Oh Gott!"  und "o heilige Schei√üe!".  Nun, und wie kann man eine Korrespondenz zwischen ihnen herstellen, wenn man das Sprachmodell kennt?  Auf keinen Fall.  Es ist historisch zuf√§llig passiert.  Sie m√ºssen die Situation kennen und wissen, was sie normalerweise in einer bestimmten Sprache sprechen.  Aus diesem Grund sind automatische √úbersetzer so unvollkommen. </p><br><p>  Ob dieses Wissen nur von einer Reihe von Texten unterschieden werden kann, ist unbekannt.  Wenn automatische √úbersetzer jedoch perfekt √ºbersetzen, ohne dumme und l√§cherliche Fehler zu machen, ist dies ein Beweis daf√ºr, dass die Erstellung einer starken KI nur auf der Grundlage von Text m√∂glich ist. </p><br><h2 id="2-raspoznavanie-izobrazheniy">  2. Bilderkennung </h2><br><p>  Schau dir dieses Bild an </p><br><p><img src="https://habrastorage.org/webt/pa/od/nd/paodndrl6p5dkuhig3rwo68cu-q.jpeg"></p><br><p>  Wenn wir uns dieses Foto ansehen, verstehen wir, dass nachts geschossen wurde.  Den Fahnen nach zu urteilen, weht der Wind von rechts nach links.  Und nach dem Rechtsverkehr zu urteilen, ist der Fall in England oder Australien nicht der Fall.  Keine dieser Informationen wird explizit in den Pixeln des Bildes angegeben, dies ist externes Wissen.  Auf dem Foto gibt es nur Zeichen, anhand derer wir das Wissen aus anderen Quellen nutzen k√∂nnen. </p><br><div class="spoiler">  <b class="spoiler_title">Wissen Sie noch etwas, das dieses Bild betrachtet?</b> <div class="spoiler_text"><p>  Dar√ºber und √ºber die Rede ... Und finden Sie sich endlich ein M√§dchen </p></div></div><br><p>  Daher wird angenommen, dass ein neuronales Netzwerk, das Objekte in einem Bild erkennt, eine interne Vorstellung davon hat, wie die reale Welt funktioniert.  Und diese Ansicht, die sich aus den Fotografien ergibt, wird sicherlich unserer realen und realen Welt entsprechen.  Im Gegensatz zu Textfeldern, bei denen dies nicht garantiert ist. </p><br><p>  Der Wert neuronaler Netze, die auf einem ImageNet-Array von Fotos (und jetzt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenImages V4</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">COCO</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">KITTI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BDD100K</a> und anderen) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">trainiert wurden</a> , ist keineswegs die Tatsache, dass eine Katze auf einem Foto erkannt wird.  Und das ist in der vorletzten Ebene gespeichert.  Hier befinden sich eine Reihe von Funktionen auf hoher Ebene, die unsere Welt beschreiben.  Ein Vektor mit 1024 Zahlen reicht aus, um eine Beschreibung von 1000 verschiedenen Kategorien von Objekten mit einer Genauigkeit von 80% zu erhalten (und in 95% der F√§lle liegt die richtige Antwort in den 5 n√§chstgelegenen Optionen).  Denken Sie nur dar√ºber nach. </p><br><p>  Aus diesem Grund werden diese Funktionen aus der vorletzten Ebene bei v√∂llig unterschiedlichen Aufgaben in der Bildverarbeitung so erfolgreich eingesetzt.  Durch Transferlernen und Feinabstimmung.  Aus diesem Vektor in 1024 Zahlen k√∂nnen Sie beispielsweise eine Tiefenkarte aus dem Bild erhalten </p><br><p><img src="https://habrastorage.org/webt/vs/k6/lm/vsk6lmod2grqjzous7knxl5ekaq.jpeg"></p><br><p>  (Ein Beispiel aus der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit,</a> in der ein praktisch unver√§ndertes vorab trainiertes Densenet-169-Netzwerk verwendet wird) </p><br><p>  Oder bestimmen Sie die Pose einer Person.  Es gibt viele Anwendungen. </p><br><p><img src="https://habrastorage.org/webt/id/rs/sp/idrsspge5oaq0dae1-li5pghf3s.jpeg"></p><br><p>  Infolgedessen kann die Bilderkennung m√∂glicherweise verwendet werden, um eine starke KI zu erzeugen, da sie das Modell unserer realen Welt wirklich widerspiegelt.  Ein Schritt von der Fotografie zum Video und zum Video ist unser Leben, da wir ungef√§hr 99% der Informationen visuell erhalten. </p><br><p>  Auf dem Foto ist jedoch v√∂llig unverst√§ndlich, wie das neuronale Netzwerk zum Denken und Ziehen von Schlussfolgerungen motiviert werden kann.  Sie kann trainiert werden, um Fragen wie "Wie viele Stifte liegen auf dem Tisch?" Zu beantworten.  (Diese Aufgabenklasse hei√üt Visual Question Answering, ein Beispiel f√ºr einen solchen Datensatz: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://visualqa.org</a> ).  Oder geben Sie eine Textbeschreibung dar√ºber, was auf dem Foto passiert.  Dies ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Taskklasse f√ºr Bildunterschriften</a> . </p><br><p><img src="https://habrastorage.org/webt/mp/lz/0y/mplz0y9uleukwz68u-lyc35wlqk.jpeg"></p><br><p>  Aber ist das Intelligenz?  Nachdem sie diesen Ansatz entwickelt haben, k√∂nnen neuronale Netze in naher Zukunft Videofragen beantworten wie "Zwei Spatzen sa√üen auf den Dr√§hten, einer von ihnen flog weg, wie viele Spatzen waren noch √ºbrig?".  Dies ist echte Mathematik, in etwas komplizierteren F√§llen, f√ºr Tiere unzug√§nglich und auf der Ebene der menschlichen Schulbildung.  Vor allem, wenn au√üer Spatzen neben ihnen Titten sitzen, die aber nicht ber√ºcksichtigt werden m√ºssen, da es sich nur um Spatzen handelte.  Ja, es wird definitiv Intelligenz sein. </p><br><h2 id="3-obuchenie-s-podkrepleniem-reinforcement-learning">  3. Reinforcement Learning </h2><br><p>  Die Idee ist sehr einfach: Aktionen zu f√∂rdern, die zur Belohnung f√ºhren, und zu vermeiden, dass sie zum Scheitern f√ºhren.  Dies ist eine universelle Art des Lernens und kann nat√ºrlich definitiv zur Schaffung einer starken KI f√ºhren.  Daher gab es in den letzten Jahren so viel Interesse an Reinforcement Learning. </p><br><div class="spoiler">  <b class="spoiler_title">Mischen, aber nicht sch√ºtteln</b> <div class="spoiler_text"><p>  Nat√ºrlich ist es am besten, eine starke KI zu schaffen, indem Sie alle drei Ans√§tze kombinieren.  In Bildern und mit Verst√§rkungstraining k√∂nnen Sie KI auf Tierniveau erhalten.  Und theoretisch k√∂nnen Sie den Bildern Textnamen von Objekten hinzuf√ºgen (ein Witz nat√ºrlich - die KI dazu zwingen, Videos anzusehen, in denen Menschen interagieren und sprechen, wie beim Unterrichten eines Babys), und eine Umschulung in einem Textfeld vornehmen, um Wissen zu erlangen (ein Analogon unserer Schule und Universit√§t) KI auf menschlicher Ebene.  Kann reden. </p></div></div><br><p>  Verst√§rktes Lernen hat ein gro√ües Plus.  Im Simulator k√∂nnen Sie ein vereinfachtes Modell der Welt erstellen.  F√ºr eine menschliche Figur reichen also nur 17 Freiheitsgrade aus, anstatt 700 bei einer lebenden Person (ungef√§hre Anzahl von Muskeln).  Daher k√∂nnen Sie im Simulator das Problem in einer sehr kleinen Dimension l√∂sen. </p><br><p>  Mit Blick auf die Zukunft k√∂nnen moderne Reinforcement Learning-Algorithmen das Modell einer Person selbst bei 17 Freiheitsgraden nicht willk√ºrlich steuern.  Das hei√üt, sie k√∂nnen das Optimierungsproblem nicht l√∂sen, bei dem 44 Zahlen am Eingang und 17 am Eingang vorhanden sind. Dies ist nur in sehr einfachen F√§llen m√∂glich, wobei die Anfangsbedingungen und Hyperparameter genau eingestellt werden.  Und selbst in diesem Fall ben√∂tigen Sie mehrere Tage Berechnungen auf einer leistungsstarken GPU, um beispielsweise ein humanoides Modell mit 17 Freiheitsgraden zum Laufen zu unterrichten und von einer stehenden Position aus zu starten (was viel einfacher ist).  Und etwas kompliziertere F√§lle, zum Beispiel das Lernen, aus einer willk√ºrlichen Pose aufzustehen, lernen m√∂glicherweise √ºberhaupt nicht.  Dies ist ein Fehler. </p><br><p>  Dar√ºber hinaus arbeiten alle Reinforcement Learning-Algorithmen mit bedr√ºckend kleinen neuronalen Netzen, k√∂nnen jedoch keine gro√üen lernen.  Gro√üe Faltungsnetzwerke werden nur verwendet, um die Dimension des Bildes auf mehrere Merkmale zu reduzieren, die den Lernalgorithmen mit Verst√§rkung zugef√ºhrt werden.  Der gleiche laufende Humanoid wird von einem Feed Forward-Netzwerk mit zwei oder drei Schichten von 128 Neuronen gesteuert.  Wirklich?  Und versuchen wir auf dieser Grundlage, eine starke KI aufzubauen? </p><br><p>  Um zu verstehen, warum dies geschieht und was beim Lernen von Verst√§rkung falsch ist, m√ºssen Sie sich zun√§chst mit den grundlegenden Architekturen des modernen Lernens von Verst√§rkung vertraut machen. </p><br><p>  Die physische Struktur des Gehirns und des Nervensystems wird durch die Evolution auf den spezifischen Tiertyp und seine Lebensbedingungen abgestimmt.  Im Laufe der Evolution entwickelte eine Fliege ein solches Nervensystem und eine solche Arbeit von Neurotransmittern in den Ganglien (ein Analogon des Gehirns bei Insekten), um einer Fliegenklatsche schnell auszuweichen.  Nun, nicht von einer Fliegenklatsche, sondern von V√∂geln, die seit 400 Millionen Jahren gefischt haben (nur ein Scherz, die V√∂gel selbst erschienen vor 150 Millionen Jahren, h√∂chstwahrscheinlich von Fr√∂schen vor 360 Millionen Jahren).  Ein Nashorn hat genug Nervensystem und Gehirn, um sich langsam dem Ziel zuzuwenden und zu rennen.  Und dort, wie sie sagen, hat das Nashorn schlechtes Sehverm√∂gen, aber das ist nicht sein Problem. </p><br><p>  Zus√§tzlich zur Evolution arbeitet jedes einzelne Individuum von der Geburt an und w√§hrend des gesamten Lebens genau den √ºblichen Lernmechanismus mit Verst√§rkung.  Bei S√§ugetieren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">und auch bei Insekten</a> erledigt das Dopaminsystem diese Arbeit.  Ihre Arbeit ist voller Geheimnisse und Nuancen, aber alles l√§uft darauf hinaus, dass das Dopaminsystem im Falle einer Auszeichnung durch Ged√§chtnismechanismen die Verbindungen zwischen Neuronen, die unmittelbar zuvor aktiv waren, irgendwie repariert.  So entsteht assoziatives Ged√§chtnis. </p><br><p>  Was aufgrund seiner Assoziativit√§t dann bei der Entscheidungsfindung verwendet wird.  Einfach ausgedr√ºckt, wenn die aktuelle Situation (aktuelle aktive Neuronen in dieser Situation) durch assoziatives Ged√§chtnis Lustneuronen aktiviert, w√§hlt die Person die Aktionen aus, die sie in einer √§hnlichen Situation ausgef√ºhrt hat und an die sie sich erinnert.  "W√§hlt Aktionen" ist eine schlechte Definition.  Es gibt keine Wahl.  Einfach aktivierte Lustged√§chtnisneuronen, die vom Dopaminsystem f√ºr eine bestimmte Situation fixiert werden, aktivieren automatisch Motoneuronen, was zu einer Muskelkontraktion f√ºhrt.  Dies ist der Fall, wenn sofortige Ma√ünahmen erforderlich sind. </p><br><p>  K√ºnstliches Lernen mit Verst√§rkung als Wissensgebiet ist es notwendig, beide Probleme zu l√∂sen: </p><br><h3 id="1-podobrat-arhitekturu-neyroseti-chto-dlya-nas-uzhe-sdelala-evolyuciya">  1. W√§hlen Sie die Architektur des neuronalen Netzwerks (was die Evolution bereits f√ºr uns getan hat) </h3><br><p>  Die gute Nachricht ist, dass h√∂here kognitive Funktionen im Neokortex bei S√§ugetieren (und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">im Striatum bei Korviden</a> ) in einer ann√§hernd einheitlichen Struktur ausgef√ºhrt werden.  Anscheinend braucht dies keine streng vorgeschriebene "Architektur". </p><br><p>  Die Vielfalt der Hirnregionen ist wahrscheinlich auf rein historische Gr√ºnde zur√ºckzuf√ºhren.  Als sie sich weiterentwickelten, wuchsen neue Teile des Gehirns zus√§tzlich zu den grundlegenden Teilen, die von den ersten Tieren √ºbrig geblieben waren.  Nach dem Prinzip funktioniert es - nicht anfassen.  Andererseits reagieren bei verschiedenen Menschen dieselben Teile des Gehirns auf dieselben Situationen.  Dies kann sowohl durch die Assoziativit√§t (Merkmale und "Gro√ümutterneuronen", die sich an diesen Stellen w√§hrend des Lernprozesses auf nat√ºrliche Weise gebildet haben) als auch durch die Physiologie erkl√§rt werden.  Dass in Genen kodierte Signalwege genau zu diesen Bereichen f√ºhren.  Es gibt keinen Konsens, aber Sie k√∂nnen zum Beispiel diesen k√ºrzlich erschienenen Artikel lesen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"Biologische und k√ºnstliche Intelligenz</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">"</a> </p><br><h3 id="2-nauchitsya-obuchat-neyronnye-seti-po-principam-obucheniya-s-podkrepleniem">  2. Lernen Sie, wie Sie neuronale Netze nach den Prinzipien des Lernens mit Verst√§rkung trainieren </h3><br><p>  Dies ist, was modernes Reinforcement Learning haupts√§chlich tut.  Und was sind die Erfolge?  Nicht sehr. </p><br><h1 id="naivnyy-podhod">  Naiver Ansatz </h1><br><p>  Es scheint sehr einfach zu sein, ein neuronales Netzwerk mit Verst√§rkung zu trainieren: Wir f√ºhren zuf√§llige Aktionen aus, und wenn wir eine Belohnung erhalten, betrachten wir die ergriffenen Aktionen als ‚ÄûReferenz‚Äú.  Wir setzen sie als Standardbezeichnungen auf die Ausgabe des neuronalen Netzwerks und trainieren das neuronale Netzwerk durch die Methode der R√ºckausbreitung des Fehlers, so dass es genau eine solche Ausgabe erzeugt.  Nun, das h√§ufigste neuronale Netzwerktraining.  Und wenn die Aktionen zu einem Fehler gef√ºhrt haben, ignorieren Sie entweder diesen Fall oder unterdr√ºcken Sie diese Aktionen (wir legen einige andere als Ausgabe fest, z. B. jede andere zuf√§llige Aktion).  Im Allgemeinen wiederholt diese Idee das Dopaminsystem. </p><br><p>  Wenn Sie jedoch versuchen, ein neuronales Netzwerk auf diese Weise zu trainieren, egal wie komplex die Architektur ist, rekursiv, faltungsorientiert oder gew√∂hnliche direkte Verteilung, dann ... wird es nicht funktionieren! </p><br><p>  Warum?  Unbekannt </p><br><p>  Es wird angenommen, dass das Nutzsignal so klein ist, dass es vor dem Hintergrund von Rauschen verloren geht.  Daher lernt das Netzwerk nicht die Standardmethode zur R√ºck√ºbertragung des Fehlers.  Eine Belohnung kommt sehr selten vor, vielleicht einmal in Hunderten oder sogar Tausenden von Schritten.  Und selbst LSTM merkt sich maximal 100-500 Punkte in der Geschichte und dann nur bei sehr einfachen Aufgaben.  Bei komplexeren F√§llen ist es jedoch bereits gut, wenn es 10 bis 20 Punkte in der Geschichte gibt. </p><br><p>  Die Wurzel des Problems liegt jedoch gerade in sehr seltenen Belohnungen (zumindest bei Aufgaben von praktischem Wert).  Im Moment wissen wir nicht, wie man neuronale Netze trainiert, die sich an Einzelf√§lle erinnern w√ºrden.  Was das Gehirn mit Brillanz bew√§ltigt.  Sie k√∂nnen sich an etwas erinnern, das nur einmal im Leben passiert ist.  √úbrigens basiert der gr√∂√üte Teil der Ausbildung und Arbeit des Intellekts auf solchen F√§llen. </p><br><p>  Dies ist so etwas wie ein schreckliches Ungleichgewicht von Klassen aus dem Bereich der Bilderkennung.  Es gibt einfach keine M√∂glichkeiten, damit umzugehen.  Das Beste, was sie bisher finden konnten, ist einfach, dem Netzwerkeingang zusammen mit neuen Situationen erfolgreiche Situationen aus der Vergangenheit zu √ºbermitteln, die in einem k√ºnstlichen Spezialpuffer gespeichert sind.  Das hei√üt, st√§ndig nicht nur neue F√§lle zu lehren, sondern auch erfolgreiche alte.  Nat√ºrlich kann ein solcher Puffer nicht unendlich erh√∂ht werden, und es ist unklar, was genau darin gespeichert werden soll.  Es wird immer noch versucht, die Pfade innerhalb des neuronalen Netzwerks, die w√§hrend eines erfolgreichen Falls aktiv waren, vor√ºbergehend zu reparieren, damit sie durch nachfolgendes Training nicht √ºberschrieben werden.  Eine ziemlich enge Analogie zu dem, was meiner Meinung nach im Gehirn passiert, obwohl sie auch in dieser Richtung noch nicht viel Erfolg haben.  Da die neuen trainierten Aufgaben in ihrer Berechnung die Ergebnisse der Neuronen verwenden, die die eingefrorenen Pfade verlassen, st√∂rt das Signal nur die neuen eingefrorenen und die alten Aufgaben funktionieren nicht mehr.  Es gibt noch einen anderen merkw√ºrdigen Ansatz: Das Netzwerk mit neuen Beispielen / Aufgaben nur in orthogonaler Richtung zu fr√ºheren Aufgaben zu trainieren ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://arxiv.org/abs/1810.01256</a> ).  Dies √ºberschreibt nicht die bisherigen Erfahrungen, schr√§nkt jedoch die Netzwerkkapazit√§t drastisch ein. </p><br><p>  In Meta-Learning wird eine separate Klasse von Algorithmen entwickelt, die zur Bew√§ltigung dieser Katastrophe entwickelt wurden (und gleichzeitig Hoffnung auf eine starke KI geben).  Dies sind Versuche, einem neuronalen Netzwerk mehrere Aufgaben gleichzeitig beizubringen.  Nicht in dem Sinne, dass es unterschiedliche Bilder in einer Aufgabe erkennt, n√§mlich unterschiedliche Aufgaben in unterschiedlichen Bereichen (jede mit ihrer eigenen Verteilungs- und L√∂sungslandschaft).  Sagen Sie, erkennen Sie Bilder und fahren Sie gleichzeitig Fahrrad.  Bisher ist der Erfolg auch nicht sehr gut, da es normalerweise darauf ankommt, ein neuronales Netzwerk mit allgemeinen universellen Gewichten im Voraus vorzubereiten und es dann in nur wenigen Schritten des Gradientenabfalls schnell an eine bestimmte Aufgabe anzupassen.  Beispiele f√ºr Meta-Learning-Algorithmen sind MAML und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Reptile</a> . </p><br><p>  Im Allgemeinen beendet nur dieses Problem (die Unf√§higkeit, aus einzelnen erfolgreichen Beispielen zu lernen) das moderne Training mit Verst√§rkung.  Die ganze Kraft neuronaler Netze vor dieser traurigen Tatsache ist bisher machtlos. </p><br><p>  Diese Tatsache, dass der einfachste und offensichtlichste Weg nicht funktioniert, zwang die Forscher, zum klassischen tischbasierten Reinforcement Learning zur√ºckzukehren.  Was als Wissenschaft in der Antike auftauchte, als neuronale Netze noch nicht einmal im Projekt waren.  Aber anstatt die Werte in Tabellen und Formeln manuell zu berechnen, verwenden wir jetzt einen so leistungsstarken Approximator wie neuronale Netze als Zielfunktionen!  Dies ist die Essenz des modernen Reinforcement Learning.  Und sein Hauptunterschied zum √ºblichen Training neuronaler Netze. </p><br><h1 id="q-learning-i-dqn">  Q-Learning und DQN </h1><br><p>  Reinforcement Learning (noch vor den neuronalen Netzen) wurde als eine ziemlich einfache und originelle Idee geboren: Lassen Sie uns zuf√§llige Aktionen ausf√ºhren, und dann berechnen wir f√ºr jede Zelle in der Tabelle und jede Bewegungsrichtung nach einer speziellen Formel (genannt Bellman-Gleichung, dieses Wort werden Sie in fast jeder Arbeit mit Verst√§rkungstraining zu treffen), wie gut diese Zelle und die gew√§hlte Richtung sind.  Je h√∂her diese Zahl ist, desto wahrscheinlicher f√ºhrt dieser Weg zum Sieg. </p><br><p><img src="https://habrastorage.org/webt/nx/zm/-7/nxzm-7q1_oc-igaim3j0mrr7vki.png"></p><br><p>  Egal in welcher Zelle Sie erscheinen, bewegen Sie sich entlang des wachsenden Gr√ºns!  (in Richtung der maximalen Anzahl an den Seiten der aktuellen Zelle). </p><br><p>  Diese Zahl hei√üt Q (aus dem Wort Qualit√§t - Qualit√§t der Wahl nat√ºrlich), und die Methode ist Q-Learning.  Deepmind ersetzte die Formel zur Berechnung dieser Zahl durch ein neuronales Netzwerk oder lehrte das neuronale Netzwerk mithilfe dieser Formel (plus ein paar weitere Tricks, die ausschlie√ülich mit der Mathematik des Trainings neuronaler Netzwerke zusammenh√§ngen) und erhielt die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DQN-</a> Methode.  Dies ist, wer 2015 den Haufen von Atari-Spielen gewann und eine Revolution im Deep Reinforcement Learning einleitete. </p><br><p>  Leider funktioniert diese Methode in ihrer Architektur nur mit diskreten diskreten Aktionen.  Im DQN wird der aktuelle Zustand (die aktuelle Situation) dem Eingang des neuronalen Netzwerks zugef√ºhrt, und am Ausgang sagt das neuronale Netzwerk die Zahl Q voraus. Und da der Ausgang des Netzwerks alle m√∂glichen Aktionen auf einmal auflistet (jede mit ihrem eigenen vorhergesagten Q), stellt sich heraus, dass das neuronale Netzwerk im DQN die klassische Funktion Q implementiert (s, a) aus Q-Learning.  Q  state  action (  Q(s,a)    s  a).     argmax          Q   ,     . </p><br><p>        Q,      .        ,    Q- (..    Q   ,   ).    .      ,        (Exploration),       ,     ,        .         ,        . </p><br><p>   ,    ?    5     Atari,  continuous    ? ,    -1..1      0.1,          ,     Atari.         . ,        .       10    .  -      ,    10       .     .   DQN     ,      17     .  ,    ,  . </p><br><p>             DQN, ,   ,   continuous  (      ): DDQN, DuDQN, BDQN, CDQN, NAF, Rainbow. ,     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Direct Future Prediction (DFP)</a> ,    DQN     .    Q   , DFP          ,    .          .                     ,     . ,   ,       ,     . </p><br><p>    ,          Reinforcement Learning. </p><br><p><img src="https://habrastorage.org/webt/f3/lc/3t/f3lc3tno4mpvwren4rfocva9iv8.png"></p><br><h1 id="policy-gradient"> Policy Gradient </h1><br><p>       state,       (  ,        ).   ,  actions,  .   ,   R   .        (   ),   (  ).        .     . </p><br><p> ,    R   ,   ,        .       !       .   ""       labels (       ),      .     ,   ,      R. </p><br><p>   Policy Gradient.      ‚Äî    ,     R,        .    ‚Äî     ,       ,      .     ,    . </p><br><h1 id="actor-critic-ddpg"> Actor-critic, DDPG </h1><br><p>   ,       ‚Äî      ,       .  ,  Q-   ,    DQN.      state,    action(s).       state,     action,   ,      Q     : Q(s,a). </p><br><p> ,   Q(s,a),    (  critic, ),       ,      (  , actor),       R.       ,    .     actor-critic.       Policy Gradient,        ,    .   . </p><br><p>      DDPG.       actions,     continuous . DDPG   continuous  DQN    . </p><br><p><img src="https://habrastorage.org/webt/9b/th/fk/9bthfkh7cfpymc6_f6xrt7sica0.png"></p><br><h1 id="advantage-actor-critic-a3ca2c"> Advantage Actor Critic (A3C/A2C) </h1><br><p>             critic  Q(s,a) ‚Äî   ,   actor,     DDPG.         ,   . </p><br><p>     ,     .   ,           ,    <strong></strong> ,    . ,    ,    ,      (     ,   ). </p><br><p>          Q(s,a),    Advantage: A(s,a) = Q(s,a) ‚Äî V(s).  A(s,a)     Q(s,a)  ,    ‚Äî      ,    V(s).  A(s,a) &gt; 0,      ,    .  A(s,a) &lt; 0,      ,     , ..   . </p><br><p>    V(s)     state   ,     (    s,  a).         ‚Äî     state,   V(s).       ,      state,   V(s). </p><br><p>  ,    Q(s,a)     r,     ,         A = r ‚Äî V(s). </p><br><p>   ,    V(s) (          ),    ‚Äî actor  critic,    !     state,        head:    actions,    V(s).     c , ..       state. ,      . </p><br><p><img src="https://habrastorage.org/webt/eo/ph/5y/eoph5ypzawg11tachwn-nt_7nyg.png"></p><br><p>        V(s)      .     V(s),          action (     ),      .    Dueling Q-Network (DuDQN),  Q(s,a)      Q(s,a) = V(s) + A(a),    . </p><br><p> Asynchronous Advantage Actor Critic (A3C)   ,   ,     actor.        batch  .  ,     actor.     ,     ,   .   ,   A2C ‚Äî   A3C,         actor       ( ). A2C    ,    ,     . </p><br><h1 id="trpo-ppo-sac"> TRPO, PPO, SAC </h1><br><p> ,    . </p><br><p>        ,     .   Reinforcement Learning     ,      ,   ,          ‚Äî      ,  .   . </p><br><p>   ‚Äî TRPO  PPO,   state-of-the-art,   Actor-Critic.  PPO         RL.  ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI Five</a>    Dota 2. </p><br><p>   ,       TRPO  PPO ‚Äî        ,     .   ,   A3C/A2C   ,    .  ,   policy     ,     . -  gradient clipping        ,     .   ,         (       ,      ),      ,   ,    -  . </p><br><p> In letzter Zeit hat der SAC-Algorithmus (Soft-Actor-Critic) an Popularit√§t gewonnen.  Es unterscheidet sich nicht sehr von PPO, nur ein Ziel wurde hinzugef√ºgt, wenn gelernt wurde, die Entropie in der Politik zu erh√∂hen.  Machen Sie das Verhalten von Agenten zuf√§lliger.  Nein, nicht so.  Dass der Agent in eher zuf√§lligen Situationen handeln konnte.  Dies erh√∂ht automatisch die Zuverl√§ssigkeit der Richtlinie, sobald der Agent f√ºr zuf√§llige Situationen bereit ist.  Dar√ºber hinaus erfordert der SAC etwas weniger Trainingsbeispiele als PPO und reagiert weniger empfindlich auf Hyperparametereinstellungen, was ebenfalls von Vorteil ist.  Selbst mit SAC ben√∂tigen Sie jedoch ungef√§hr 20 Millionen Frames und ungef√§hr einen Berechnungstag auf einer GPU, um einen Humanoiden f√ºr das Laufen mit 17 Freiheitsgraden ausgehend von einer stehenden Position zu trainieren.  Schwierigere Anfangsbedingungen, um beispielsweise einem Humanoiden beizubringen, sich aus einer willk√ºrlichen Pose zu erheben, werden m√∂glicherweise √ºberhaupt nicht gelehrt. </p><br><p>  Insgesamt die allgemeine Empfehlung im modernen Reinforcement Learning: Verwenden Sie SAC, PPO, DDPG, DQN (in dieser Reihenfolge absteigend). </p><br><h1 id="model-based">  Modellbasiert </h1><br><p>  Es gibt einen anderen interessanten Ansatz, der indirekt mit dem verst√§rkten Lernen zusammenh√§ngt.  Dies dient dazu, ein Modell der Umgebung zu erstellen und damit vorherzusagen, was passieren wird, wenn wir Ma√ünahmen ergreifen. </p><br><p>  Ihr Nachteil ist, dass sie in keiner Weise sagt, welche Ma√ünahmen ergriffen werden sollten.  Nur √ºber ihr Ergebnis.  Ein solches neuronales Netzwerk ist jedoch einfach zu trainieren - trainieren Sie einfach alle Statistiken.  Es stellt sich so etwas wie ein Weltsimulator heraus, der auf einem neuronalen Netzwerk basiert. </p><br><p>  Danach generieren wir eine gro√üe Anzahl von zuf√§lligen Aktionen, und jede wird durch diesen Simulator (√ºber ein neuronales Netzwerk) gesteuert.  Und wir schauen, welches die maximale Belohnung bringt.  Es gibt eine kleine Optimierung - um nicht nur zuf√§llige Aktionen zu generieren, sondern nach dem Normalgesetz von der aktuellen Flugbahn abzuweichen.  Und tats√§chlich, wenn wir unsere Hand heben, m√ºssen wir sie mit hoher Wahrscheinlichkeit weiter heben.  Daher m√ºssen Sie zun√§chst die minimalen Abweichungen von der aktuellen Flugbahn √ºberpr√ºfen. </p><br><p>  Der Trick dabei ist, dass selbst ein primitiver physikalischer Simulator wie MuJoCo oder pyBullet etwa 200 FPS erzeugt.  Wenn Sie ein neuronales Netzwerk so trainieren, dass es mindestens einige Schritte vorw√§rts vorhersagt, k√∂nnen Sie in einfachen Umgebungen problemlos Stapel von 2000-5000 Vorhersagen gleichzeitig abrufen.  Abh√§ngig von der Leistung der GPU k√∂nnen Sie aufgrund der Parallelisierung in der GPU und der Rechengeschwindigkeit im neuronalen Netzwerk eine Prognose f√ºr Zehntausende von zuf√§lligen Aktionen pro Sekunde erhalten.  Das neuronale Netzwerk fungiert hier einfach als sehr schneller Simulator der Realit√§t. </p><br><p>  Da das neuronale Netzwerk die reale Welt vorhersagen kann (dies ist im Allgemeinen ein modellbasierter Ansatz), kann das Training sozusagen vollst√§ndig in der Vorstellungskraft durchgef√ºhrt werden.  Dieses Konzept im Reinforcement Learning wird als Traumwelten oder Weltmodelle bezeichnet.  Dies funktioniert gut, eine gute Beschreibung finden Sie hier: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://worldmodels.github.io</a> .  Dar√ºber hinaus hat es ein nat√ºrliches Gegenst√ºck - gew√∂hnliche Tr√§ume.  Und mehrfaches Scrollen der letzten oder geplanten Ereignisse im Kopf. </p><br><h1 id="imitation-learning">  Nachahmung lernen </h1><br><p>  Aufgrund der Ohnmacht, dass die Reinforcement Learning-Algorithmen nicht f√ºr gro√üe Dimensionen und komplexe Aufgaben geeignet sind, haben sich die Menschen vorgenommen, die Handlungen von Experten zumindest in Form von Personen zu wiederholen.  Hier wurden gute Ergebnisse erzielt (unerreichbar durch konventionelles Reinforcement Learning).  Es stellte sich heraus, dass OpenAI <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">das Spiel Montezumas Rache bestanden hat</a> .  Der Trick erwies sich als einfach - den Agenten sofort am Ende des Spiels (am Ende der von der Person angezeigten Flugbahn) zu platzieren.  Dort lernt der Agent mithilfe von PPO dank der N√§he der endg√ºltigen Belohnung schnell, auf der Flugbahn zu gehen.  Danach setzen wir ihn ein wenig zur√ºck, wo er schnell lernt, den Ort zu erreichen, den er bereits studiert hat.  Wenn der Agent den "Respawn" -Punkt entlang der Flugbahn schrittweise bis zum Beginn des Spiels verschiebt, lernt er, die Flugbahn des Experten w√§hrend des Spiels zu bestehen / zu simulieren. </p><br><p>  Ein weiteres beeindruckendes Ergebnis ist die Wiederholung von Bewegungen f√ºr Personen, die mit Motion Capture: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DeepMimic aufgenommen wurden</a> .  Das Rezept √§hnelt der OpenAI-Methode: Jede Episode beginnt nicht am Anfang des Pfades, sondern an einem zuf√§lligen Punkt entlang des Pfades.  Dann untersucht PPO erfolgreich die Umgebung dieses Punktes. </p><br><p>  Ich muss sagen, dass der sensationelle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Go-Explore-</a> Algorithmus von Uber, der Montezumas Revenge mit Rekordpunkten bestanden hat, √ºberhaupt kein Reinforcement Learning-Algorithmus ist.  Dies ist eine regul√§re zuf√§llige Suche, die jedoch mit einer zuf√§llig besuchten Zellzelle beginnt (einer groben Zelle, in die mehrere Zust√§nde fallen).  Und nur wenn die Flugbahn bis zum Ende des Spiels durch eine solche zuf√§llige Suche gefunden wird, wird das neuronale Netzwerk mithilfe von Imitation Learning trainiert.  In √§hnlicher Weise wie OpenAI, d.h.  beginnend am Ende der Flugbahn. </p><br><h1 id="curiosity-lyubopytstvo">  Neugier (Neugier) </h1><br><p>  Ein sehr wichtiges Konzept beim Reinforcement Learning ist die Neugier.  In der Natur ist es ein Motor f√ºr die Umweltforschung. </p><br><p>  Das Problem ist, dass Sie als Ma√ü f√ºr die Neugier keinen einfachen Netzwerkvorhersagefehler verwenden k√∂nnen, was als n√§chstes passieren wird.  Andernfalls h√§ngt ein solches Netzwerk mit schwankendem Laub vor dem ersten Baum.  Oder vor einem Fernseher mit zuf√§lliger Kanalumschaltung.  Da das Ergebnis aufgrund der Komplexit√§t nicht vorhersehbar ist und der Fehler immer gro√ü ist.  Dies ist jedoch genau der Grund, warum wir (Menschen) gerne Laub, Wasser und Feuer betrachten.  Und wie andere Leute arbeiten =).  Aber wir haben Schutzmechanismen, um nicht f√ºr immer zu h√§ngen. </p><br><p>  Einer dieser Mechanismen wurde als inverses Modell in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">neugierigen Erforschung von</a> erfunden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><br></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Selbst√ºberwachte Vorhersage</a> .  Kurz gesagt, ein Agent (neuronales Netzwerk) versucht nicht nur vorherzusagen, welche Aktionen in einer bestimmten Situation am besten ausgef√ºhrt werden, sondern auch vorherzusagen, was nach den ergriffenen Aktionen mit der Welt geschehen wird.  Und er verwendet diese Vorhersage der Welt f√ºr den n√§chsten Schritt, damit er und der aktuelle Schritt seine fr√ºher ergriffenen Ma√ünahmen vorhersagen k√∂nnen (ja, es ist schwierig, Sie k√∂nnen es nicht ohne ein Pint herausfinden). </p><br><p>  Dies f√ºhrt zu einem merkw√ºrdigen Effekt: Der Agent wird nur neugierig auf das, was er mit seinen Handlungen beeinflussen kann.  Er kann die schwingenden √Ñste eines Baumes nicht beeinflussen, so dass sie f√ºr ihn uninteressant werden.  Aber er kann durch das Viertel laufen, also ist er neugierig darauf, die Welt zu erkunden. </p><br><p>  Wenn der Agent jedoch eine TV-Fernbedienung hat, die zuf√§llige Kan√§le umschaltet, kann er dies beeinflussen!  Und er wird neugierig sein, ad infinitum auf die Kan√§le zu klicken (da er nicht vorhersagen kann, was der n√§chste Kanal sein wird, weil er zuf√§llig ist).  Ein Versuch, dieses Problem zu umgehen, wurde von Google in der Arbeit von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Episodic Curiosity through Reachability</a> unternommen. </p><br><p>  Das vielleicht beste Ergebnis auf dem neuesten Stand der Technik ist jedoch die Neugierde. OpenAI besitzt derzeit die Idee der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Random Network Distillation (RND)</a> .  Das Wesentliche ist, dass ein zweites, vollst√§ndig zuf√§llig initialisiertes Netzwerk ben√∂tigt wird und der aktuelle Status ihm zugef√ºhrt wird.  Und unser haupts√§chlich funktionierendes neuronales Netzwerk versucht, die Ausgabe dieses neuronalen Netzwerks zu erraten.  Das zweite Netzwerk ist nicht trainiert, es bleibt w√§hrend der Initialisierung immer fest. </p><br><p>  Was ist der Punkt?  Der Punkt ist, dass, wenn ein Staat bereits von unserem Arbeitsnetzwerk besucht und untersucht wurde, er die Ausgabe dieses zweiten Netzwerks mehr oder weniger erfolgreich vorhersagen kann.  Und wenn dies ein neuer Zustand ist, in dem wir noch nie waren, kann unser neuronales Netzwerk die Ausgabe dieses RND-Netzwerks nicht vorhersagen.  Dieser Fehler bei der Vorhersage der Ausgabe dieses zuf√§llig initialisierten Netzwerks wird als Indikator f√ºr die Neugier verwendet (er bietet hohe Belohnungen, wenn wir seine Ausgabe in dieser Situation nicht vorhersagen k√∂nnen). </p><br><p>  Warum dies funktioniert, ist nicht ganz klar.  Sie schreiben jedoch, dass dies das Problem beseitigt, wenn das Vorhersageziel stochastisch ist und wenn nicht gen√ºgend Daten vorhanden sind, um eine Vorhersage dar√ºber zu treffen, was als n√§chstes passieren wird (was bei gew√∂hnlichen Neugieralgorithmen einen gro√üen Vorhersagefehler ergibt).  Auf die eine oder andere Weise, aber RND zeigte wirklich hervorragende Forschungsergebnisse, die auf Neugierde in Spielen beruhten.  Und bew√§ltigt das Problem des Zufallsfernsehens. </p><br><p>  Mit RND hat die Neugier in OpenAI zum ersten Mal ehrlich (und nicht durch eine vorl√§ufige Zufallssuche wie in Uber) die erste Stufe von Montezumas Rache √ºberschritten.  Nicht jedes Mal und unzuverl√§ssig, aber von Zeit zu Zeit stellt sich heraus. </p><br><p><img src="https://habrastorage.org/webt/iw/jm/kb/iwjmkbze4r8efybc5-01pbqzjf8.png"></p><br><h1 id="chto-v-itoge">  Was ist das Ergebnis? </h1><br><p>  Wie Sie sehen, hat das Reinforcement Learning in nur wenigen Jahren einen langen Weg zur√ºckgelegt.  Nicht nur einige erfolgreiche L√∂sungen, wie in Faltungsnetzwerken, in denen Resudal- und Skip-Verbindungen es erm√∂glichten, Netzwerke mit einer Tiefe von Hunderten von Schichten zu trainieren, anstatt ein Dutzend Schichten mit Relu-Aktivierungsfunktion allein, die das Problem des Verschwindens von Gradienten in Sigmoid und Tanh √ºberwanden.  Beim Lernen mit Verst√§rkung wurden Fortschritte bei den Konzepten und beim Verst√§ndnis der Gr√ºnde erzielt, warum diese oder jene naive Version der Implementierung nicht funktioniert hat.  Das Schl√ºsselwort "hat nicht funktioniert." </p><br><p>  Aus technischer Sicht beruht jedoch immer noch alles auf den Vorhersagen aller gleichen Q-, V- oder A-Werte.  Es gibt keine Zeitabh√§ngigkeiten auf verschiedenen Ebenen, wie im Gehirn (Hierarchisches Reinforcement Learning z√§hlt nicht, die Hierarchie ist darin zu primitiv im Vergleich zur Assoziativit√§t im lebenden Gehirn).  Keine Versuche, eine Netzwerkarchitektur zu entwickeln, die speziell auf das verst√§rkte Lernen zugeschnitten ist, wie dies bei LSTM und anderen wiederkehrenden Netzwerken f√ºr Zeitsequenzen der Fall war.  Verst√§rkung Das Lernen stampft entweder auf der Stelle, freut sich √ºber kleine Erfolge oder bewegt sich in eine v√∂llig falsche Richtung. </p><br><p>  Ich w√ºrde gerne glauben, dass es in der Architektur neuronaler Netze einen Durchbruch in der Architektur neuronaler Netze geben wird, √§hnlich wie in Faltungsnetzen.  Und wir werden sehen, wie das Lernen zur St√§rkung wirklich funktioniert.  Lernen an isolierten Beispielen mit assoziativem Ged√§chtnis und Arbeiten auf verschiedenen Zeitskalen. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de437020/">https://habr.com/ru/post/de437020/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de437006/index.html">Wanhao Duplicator 10 3D-Drucker Bewertung</a></li>
<li><a href="../de437008/index.html">NLP. Die Grundlagen. Techniken. Selbstentwicklung. Teil 1</a></li>
<li><a href="../de437010/index.html">Echos der Vergangenheit: Young's Erfahrung auf der Basis der neuen R√∂ntgenspektroskopie-Methode</a></li>
<li><a href="../de437014/index.html">Die Aufgabe von N K√∂rpern oder wie man eine Galaxie in die Luft jagt, ohne die K√ºche zu verlassen</a></li>
<li><a href="../de437018/index.html">Einige Fallstricke bei der statischen Eingabe in Python</a></li>
<li><a href="../de437022/index.html">Noise Security Bit 0x22 (Fehlerinjektionsangriffe, 35C3 und Wallet.fail)</a></li>
<li><a href="../de437026/index.html">Google in Frankreich verurteilte die DSGVO wegen Missbrauchs personenbezogener Daten zu einer Geldstrafe von 50 Millionen Euro</a></li>
<li><a href="../de437030/index.html">Automatisierung der Infrastruktur eines Luxusb√ºros: wie es aussieht</a></li>
<li><a href="../de437032/index.html">Installationsanleitung f√ºr NGINX ModSecurity</a></li>
<li><a href="../de437034/index.html">Universal Whistles: Snom A230 und A210 USB Dongle Review</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>