<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🕗 🔣 🛒 Penguatan Pembelajaran dengan Python 🤤 🚱 👨🏾‍🌾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hai teman-teman! 



 Dalam publikasi terakhir tahun yang akan datang, kami ingin menyebutkan Reinforcement Learning - sebuah topik yang sudah kami te...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penguatan Pembelajaran dengan Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Hai teman-teman! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  Dalam publikasi terakhir tahun yang akan datang, kami ingin menyebutkan Reinforcement Learning - sebuah topik yang sudah kami terjemahkan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">buku</a> . <br><br>  Menilai sendiri: ada artikel dasar dengan Medium, yang menguraikan konteks masalah, menggambarkan algoritma paling sederhana dengan implementasi di Python.  Artikel ini memiliki beberapa gif.  Dan motivasi, penghargaan dan memilih strategi yang tepat di jalan menuju kesuksesan adalah hal-hal yang akan sangat berguna bagi kita masing-masing di tahun mendatang. <br><br>  Selamat membaca! <br><a name="habracut"></a><br>  Pembelajaran yang diperkuat adalah suatu bentuk pembelajaran mesin di mana agen belajar untuk bertindak di lingkungan, melakukan tindakan dan dengan demikian mengembangkan intuisi, setelah itu ia mengamati hasil dari tindakannya.  Pada artikel ini saya akan memberitahu Anda bagaimana memahami dan merumuskan masalah belajar dengan penguatan, dan kemudian menyelesaikannya dengan Python. <br><br><br>  Baru-baru ini, kita menjadi terbiasa dengan kenyataan bahwa komputer bermain game melawan manusia - baik sebagai bot dalam game multi-pemain, atau sebagai saingan dalam game satu-satu: katakanlah, di Dota2, PUB-G, Mario.  Perusahaan riset <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Deepmind</a> membuat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">keributan</a> tentang berita itu ketika program AlphaGo 2016 mereka mengalahkan juara Korea Selatan pada 2016.  Jika Anda seorang gamer yang rajin, Anda bisa mendengar tentang lima pertandingan Dota 2 OpenAI Five, di mana mobil-mobil bertarung melawan orang-orang dan mengalahkan para pemain terbaik di Dota2 dalam beberapa pertandingan.  (Jika Anda tertarik pada perinciannya, algoritme dianalisis secara terperinci di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> dan diperiksa bagaimana mesin dimainkan). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  Versi terbaru dari OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mengambil Roshan</a> . <br><br>  Jadi, mari kita mulai dengan pertanyaan sentral.  Mengapa kita perlu pelatihan yang diperkuat?  Apakah hanya digunakan dalam permainan, atau apakah itu berlaku dalam skenario realistis untuk menyelesaikan masalah yang diterapkan?  Jika ini adalah pelatihan penguatan bacaan pertama Anda, Anda tidak bisa membayangkan jawaban untuk pertanyaan-pertanyaan ini.  Memang, pembelajaran yang diperkuat adalah salah satu teknologi yang paling banyak digunakan dan berkembang pesat di bidang kecerdasan buatan. <br>  Berikut adalah sejumlah bidang studi di mana sistem pembelajaran penguatan sangat dibutuhkan: <br><br><ol><li>  Kendaraan tak berawak </li><li>  Industri game </li><li>  Robotika </li><li>  Sistem Rekomendasi </li><li>  Periklanan dan Pemasaran </li></ol><br>  <b>Gambaran umum dan latar belakang pembelajaran penguatan</b> <br><br>  Jadi, bagaimana fenomena pembelajaran dengan penguatan terbentuk ketika kita memiliki begitu banyak mesin dan metode pembelajaran mendalam yang kita miliki?  "Dia ditemukan oleh Rich Sutton dan Andrew Barto, supervisor penelitian Rich, yang membantunya mempersiapkan PhD."  Paradigma pertama terbentuk pada 1980-an dan kemudian kuno.  Selanjutnya, Rich percaya bahwa dia memiliki masa depan yang hebat, dan dia akhirnya akan menerima pengakuan. <br><br>  Pembelajaran yang diperkuat mendukung otomasi di lingkungan di mana ia digunakan.  Baik mesin dan pembelajaran mendalam beroperasi dalam cara yang kira-kira sama - mereka diatur secara strategis berbeda, tetapi kedua paradigma mendukung otomatisasi.  Jadi, mengapa pelatihan penguatan muncul? <br><br>  Ini sangat mengingatkan pada proses pembelajaran alami di mana proses / model bertindak dan menerima umpan balik tentang bagaimana dia berhasil mengatasi tugas: baik dan tidak. <br><br>  Mesin dan pembelajaran mendalam juga merupakan opsi pelatihan, namun, mereka lebih disesuaikan untuk mengidentifikasi pola dalam data yang tersedia.  Di sisi lain, dalam pembelajaran penguatan, pengalaman tersebut diperoleh melalui coba-coba;  sistem secara bertahap menemukan opsi yang tepat atau global optimal.  Keuntungan tambahan yang serius dari pembelajaran yang diperkuat adalah bahwa dalam hal ini, tidak perlu menyediakan satu set data pelatihan yang luas, seperti halnya mengajar dengan seorang guru.  Beberapa fragmen kecil sudah cukup. <br><br>  <b>Konsep pembelajaran penguatan</b> <br><br>  Bayangkan mengajarkan kucing Anda trik baru;  tetapi, sayangnya, kucing tidak mengerti bahasa manusia, jadi Anda tidak dapat mengambil dan memberi tahu mereka apa yang akan Anda mainkan dengan mereka.  Karena itu, Anda akan bertindak berbeda: meniru situasi, dan kucing akan berusaha merespons dengan satu atau lain cara dalam merespons.  Jika kucing bereaksi seperti yang Anda inginkan, maka Anda menuangkan susu ke dalamnya.  Apakah Anda mengerti apa yang akan terjadi selanjutnya?  Sekali lagi, dalam situasi yang sama, kucing akan melakukan tindakan yang diinginkan lagi, dan dengan antusiasme yang lebih besar, berharap bahwa ia akan diberi makan lebih baik lagi.  Ini adalah bagaimana pembelajaran terjadi pada contoh positif;  tetapi, jika Anda mencoba untuk "mendidik" kucing dengan insentif negatif, misalnya, perhatikan dengan seksama dan mengerutkan kening, ia biasanya tidak belajar dalam situasi seperti itu. <br><br>  Pembelajaran yang diperkuat bekerja dengan cara yang sama.  Kami memberi tahu mesin tersebut beberapa input dan tindakan, dan kemudian memberi hadiah kepada mesin tergantung pada hasilnya.  Tujuan utama kami adalah memaksimalkan imbalan.  Sekarang mari kita lihat bagaimana merumuskan kembali masalah di atas dalam hal pembelajaran penguatan. <br><br><ul><li>  Kucing bertindak sebagai "agen" yang terpapar pada "lingkungan". </li><li>  Lingkungan adalah rumah atau area bermain, tergantung pada apa yang Anda ajarkan pada kucing. </li><li>  Situasi yang timbul dari pelatihan disebut "keadaan".  Dalam kasus kucing, contoh kondisi adalah ketika kucing "berlari" atau "merangkak di bawah tempat tidur." </li><li>  Agen bereaksi dengan melakukan tindakan dan berpindah dari satu "keadaan" ke yang lain. </li><li>  Setelah keadaan berubah, agen menerima "hadiah" atau "denda" tergantung pada tindakan yang telah diambil. </li><li>  "Strategi" adalah metodologi untuk memilih tindakan untuk mendapatkan hasil terbaik. </li></ul><br>  Sekarang kita telah mengetahui apa itu pembelajaran penguatan, mari kita bicara secara rinci tentang asal-usul dan evolusi pembelajaran penguatan dan pembelajaran penguatan yang mendalam, bahas bagaimana paradigma ini memungkinkan kita untuk memecahkan masalah yang tidak mungkin untuk dipelajari dengan atau tanpa guru, dan perhatikan juga hal-hal berikut fakta yang aneh: saat ini, mesin pencari Google dioptimalkan menggunakan algoritma pembelajaran penguatan. <br><br>  <b>Memahami terminologi penguatan pembelajaran</b> <br><br>  Agen dan Lingkungan memainkan peran kunci dalam algoritma pembelajaran penguatan.  Lingkungan adalah dunia di mana Agen harus bertahan hidup.  Selain itu, Agen menerima sinyal penguatan dari Lingkungan (hadiah): ini adalah angka yang menggambarkan seberapa baik atau buruk kondisi dunia saat ini dapat dipertimbangkan.  Tujuan Agen adalah untuk memaksimalkan total hadiah, yang disebut "keuntungan".  Sebelum menulis algoritma pembelajaran penguatan pertama kami, Anda perlu memahami terminologi berikut. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>Negara</b> : Negara bagian adalah uraian lengkap tentang dunia di mana tidak ada satu bagian pun dari informasi yang mencirikan dunia ini yang hilang.  Itu bisa berupa posisi, tetap atau dinamis.  Sebagai aturan, negara-negara tersebut ditulis dalam bentuk array, matriks atau tensor orde tinggi. </li><li>  <b>Tindakan</b> : Tindakan biasanya tergantung pada kondisi lingkungan, dan di lingkungan yang berbeda agen akan mengambil tindakan yang berbeda.  Banyak tindakan agen yang valid dicatat dalam ruang yang disebut "ruang tindakan".  Biasanya, jumlah tindakan di ruang terbatas. </li><li>  <b>Lingkungan</b> : Ini adalah tempat di mana agen itu ada dan dengan mana ia berinteraksi.  Berbagai jenis penghargaan, strategi, dll. Digunakan untuk lingkungan yang berbeda. </li><li>  <b>Hadiah</b> dan <b>kemenangan</b> : Anda harus terus-menerus memonitor fungsi hadiah R saat berlatih dengan bala bantuan.  Sangat penting ketika mengatur algoritma, mengoptimalkannya, dan juga ketika Anda berhenti belajar.  Itu tergantung pada keadaan dunia saat ini, tindakan yang baru saja diambil dan keadaan dunia berikutnya. </li><li>  <b>Strategi</b> : strategi adalah aturan yang menurutnya agen memilih tindakan selanjutnya.  Himpunan strategi juga disebut sebagai "otak" agen. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Sekarang kita telah terbiasa dengan terminologi pembelajaran penguatan, mari kita selesaikan masalah menggunakan algoritma yang sesuai.  Sebelum ini, Anda perlu memahami bagaimana merumuskan masalah seperti itu, dan ketika memecahkan masalah ini, bergantung pada terminologi pelatihan dengan penguatan. <br><br>  <b>Solusi taksi</b> <br><br>  Jadi, kami beralih ke pemecahan masalah dengan menggunakan algoritma penguatan. <br>  Misalkan kita memiliki zona pelatihan untuk taksi tak berawak, yang kita latih untuk mengantarkan penumpang ke tempat parkir di empat titik berbeda ( <code>R,G,Y,B</code> ).  Sebelum itu, Anda perlu memahami dan mengatur lingkungan tempat kami memulai pemrograman dengan Python.  Jika Anda baru mulai mempelajari Python, saya merekomendasikan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel ini untuk Anda</a> . <br><br>  Lingkungan untuk memecahkan masalah taksi dapat dikonfigurasi menggunakan OpenAI's <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gym</a> - ini adalah salah satu perpustakaan paling populer untuk memecahkan masalah dengan pelatihan penguatan.  Nah, sebelum menggunakan gym, Anda harus menginstalnya di komputer Anda, dan manajer paket Python bernama pip nyaman untuk ini.  Berikut ini adalah perintah instalasi. <br><br> <code>pip install gym</code> <br> <br>  Selanjutnya, mari kita lihat bagaimana lingkungan kita akan ditampilkan.  Semua model dan antarmuka untuk tugas ini sudah dikonfigurasi di gym dan diberi nama di bawah <code>Taxi-V2</code> .  Cuplikan kode di bawah ini digunakan untuk menampilkan lingkungan ini. <br><br>  “Kami memiliki 4 lokasi (ditunjukkan dengan berbagai huruf);  tugas kami adalah untuk mengambil penumpang di satu titik dan mengantarnya di tempat lain.  Kami mendapatkan +20 poin untuk pendaratan penumpang yang sukses dan kehilangan 1 poin untuk setiap langkah yang dihabiskan untuk itu.  Ada juga penalti 10 poin untuk setiap penumpang yang tidak diinginkan dan turun dari penumpang. ”  (Sumber: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Ini adalah output yang akan kita lihat di konsol kita: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taksi V2 ENV <br><br>  Hebat, <code>env</code> adalah jantung dari OpenAi Gym, ini adalah antarmuka lingkungan terpadu.  Berikut ini adalah metode env yang kami anggap berguna: <br><br>  <code>env.reset</code> : mengatur ulang lingkungan dan mengembalikan keadaan awal acak. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> pengembangan lingkungan satu langkah dalam waktu. <br>  <code>env.step(action)</code> : mengembalikan variabel berikut <br><br><ul><li>  <code>observation</code> : Pengamatan lingkungan. </li><li>  <code>reward</code> : <code>reward</code> apakah tindakan Anda bermanfaat. </li><li>  <code>done</code> : Menunjukkan apakah kami berhasil mengambil dan menurunkan penumpang dengan benar, juga disebut sebagai "satu episode." </li><li>  <code>info</code> : Informasi tambahan seperti kinerja dan latensi yang diperlukan untuk tujuan debugging. </li><li>  <code>env.render</code> : Menampilkan satu bingkai lingkungan (berguna untuk rendering) </li></ul><br>  Jadi, setelah memeriksa lingkungannya, mari kita coba untuk lebih memahami masalahnya.  Taksi adalah satu-satunya mobil di tempat parkir ini.  Parkir dapat dibagi menjadi kotak <code>5x5</code> , di mana kami mendapatkan 25 kemungkinan lokasi taksi.  25 nilai ini adalah salah satu elemen ruang negara kita.  Harap dicatat: saat ini, taksi kami terletak di titik dengan koordinat (3, 1). <br><br>  Ada 4 titik di lingkungan di mana penumpang diizinkan naik: ini adalah: <code>R, G, Y, B</code> atau <code>[(0,0), (0,4), (4,0), (4,3)]</code> dalam koordinat ( horizontal; vertikal), jika mungkin untuk menafsirkan lingkungan di atas dalam koordinat Cartesius.  Jika Anda juga memperhitungkan satu lagi (1) keadaan penumpang: di dalam taksi, Anda dapat mengambil semua kombinasi lokasi penumpang dan tujuan mereka untuk menghitung jumlah negara bagian di lingkungan kami untuk pelatihan taksi: kami memiliki empat (4) tujuan dan lima (4+) 1) lokasi penumpang. <br><br>  Jadi, di lingkungan kita untuk taksi, ada 5 × 5 × 5 × 4 = 500 negara.  Seorang agen menangani salah satu dari 500 syarat dan mengambil tindakan.  Dalam kasus kami, opsinya adalah sebagai berikut: bergerak ke satu arah atau yang lain, atau keputusan untuk mengambil / menurunkan penumpang.  Dengan kata lain, kami memiliki enam kemungkinan tindakan yang kami miliki: <br>  pickup, drop, utara, timur, selatan, barat (Empat nilai terakhir adalah arah di mana taksi dapat bergerak.) <br><br>  Ini adalah <code>action space</code> : set dari semua tindakan yang agen kami dapat ambil dalam keadaan tertentu. <br><br>  Seperti yang jelas dari ilustrasi di atas, taksi tidak dapat melakukan tindakan tertentu dalam beberapa situasi (gangguan dinding).  Dalam kode yang menggambarkan lingkungan, kita cukup memberikan penalti -1 untuk setiap pukulan di dinding, dan taksi bertabrakan dengan dinding.  Dengan demikian, denda tersebut akan menumpuk, sehingga taksi akan berusaha untuk tidak menabrak dinding. <br><br>  Tabel hadiah: Saat membuat lingkungan taksi, tabel hadiah utama yang disebut P juga dapat dibuat. Anda dapat menganggapnya sebagai matriks, di mana jumlah negara sesuai dengan jumlah baris dan jumlah tindakan dengan jumlah kolom.  Artinya, kita berbicara tentang matriks <code>states × actions</code> . <br><br>  Karena benar-benar semua kondisi dicatat dalam matriks ini, Anda dapat melihat nilai hadiah default yang ditetapkan untuk keadaan yang telah kami pilih untuk menggambarkan: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  Struktur kamus ini adalah sebagai berikut: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Nilai 0–5 sesuai dengan tindakan (selatan, utara, timur, barat, pickup, dropoff) yang dapat dilakukan taksi dalam keadaan saat ini seperti yang ditunjukkan dalam ilustrasi. </li><li>  selesai memungkinkan Anda untuk menilai ketika kami berhasil menurunkan penumpang pada titik yang diinginkan. </li></ul><br>  Untuk mengatasi masalah ini tanpa pelatihan penguatan, Anda dapat mengatur status target, memilih ruang, dan kemudian, jika Anda dapat mencapai status target untuk sejumlah iterasi, asumsikan bahwa momen ini sesuai dengan hadiah maksimum.  Di negara-negara lain, nilai hadiah dapat mendekati maksimum jika program bertindak dengan benar (mendekati tujuan) atau mengakumulasi denda jika itu membuat kesalahan.  Selain itu, nilai denda bisa mencapai tidak lebih rendah dari -10. <br><br>  Mari kita menulis kode untuk menyelesaikan masalah ini tanpa pelatihan penguatan. <br>  Karena kami memiliki tabel-P dengan nilai hadiah default untuk setiap negara bagian, kami dapat mencoba mengatur navigasi taksi kami hanya berdasarkan tabel ini. <br><br>  Kami membuat lingkaran tanpa akhir, menggulir hingga penumpang mencapai tujuan (satu episode), atau, dengan kata lain, hingga tingkat hadiah mencapai 20. Metode <code>env.action_space.sample()</code> secara otomatis memilih tindakan acak dari serangkaian semua tindakan yang tersedia .  Pertimbangkan apa yang terjadi: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Kesimpulan: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  kredit: OpenAI <br><br>  Masalahnya dipecahkan, tetapi tidak dioptimalkan, atau algoritma ini tidak akan berfungsi dalam semua kasus.  Kami membutuhkan agen interaksi yang sesuai sehingga jumlah iterasi yang dihabiskan oleh mesin / algoritma untuk menyelesaikan masalah tetap minimal.  Di sini algoritma Q-learning akan membantu kita, implementasi yang akan kita bahas di bagian selanjutnya. <br><br>  <b>Memperkenalkan Q-Learning</b> <br><br>  Di bawah ini adalah yang paling populer dan salah satu algoritma pembelajaran penguatan paling sederhana.  Lingkungan menghargai agen untuk pelatihan bertahap dan untuk fakta bahwa dalam keadaan tertentu ia mengambil langkah paling optimal.  Dalam implementasi yang dibahas di atas, kami memiliki tabel hadiah “P”, yang akan dipelajari oleh agen kami.  Berdasarkan tabel hadiah, ia memilih tindakan berikutnya tergantung pada seberapa bermanfaatnya, dan kemudian memperbarui nilai lain, yang disebut nilai-Q.  Akibatnya, sebuah tabel baru dibuat, disebut Q-table, ditampilkan pada kombinasi (Status, Action).  Jika nilai-Q lebih baik, maka kami mendapatkan hadiah yang lebih optimal. <br><br>  Misalnya, jika taksi dalam keadaan di mana penumpang berada pada titik yang sama dengan taksi, sangat mungkin bahwa nilai Q untuk tindakan "penjemputan" lebih tinggi daripada tindakan lain, misalnya, "mengantar penumpang" atau "pergi ke utara" ". <br>  Nilai-Q diinisialisasi dengan nilai acak, dan ketika agen berinteraksi dengan lingkungan dan menerima berbagai penghargaan dengan melakukan tindakan tertentu, nilai-Q diperbarui sesuai dengan persamaan berikut: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Ini menimbulkan pertanyaan: bagaimana menginisialisasi nilai-Q dan bagaimana menghitungnya.  Saat tindakan dilakukan, nilai-Q dieksekusi dalam persamaan ini. <br><br>  Di sini, Alpha dan Gamma adalah parameter dari algoritma Q-learning.  Alpha adalah langkah belajar, dan gamma adalah faktor diskon.  Kedua nilai dapat berkisar dari 0 hingga 1 dan terkadang sama dengan satu.  Gamma dapat sama dengan nol, tetapi alpha tidak bisa, karena nilai kerugian selama pembaruan harus dikompensasi (tingkat pembelajaran positif).  Nilai alfa di sini sama dengan ketika mengajar dengan seorang guru.  Gamma menentukan seberapa penting kita ingin memberikan hadiah yang menanti kita di masa depan. <br><br>  Algoritma ini dirangkum di bawah ini: <br><br><ul><li>  Langkah 1: inisialisasi tabel-Q, mengisinya dengan nol, dan untuk nilai-Q kita tetapkan konstanta acak. </li><li>  Langkah 2: Sekarang biarkan agen merespons lingkungan dan mencoba tindakan yang berbeda.  Untuk setiap perubahan status, kami memilih salah satu dari semua tindakan yang dimungkinkan dalam kondisi ini (S). </li><li>  Langkah 3: Pergi ke status berikutnya (S ') berdasarkan hasil dari tindakan sebelumnya (a). </li><li>  Langkah 4: Untuk semua tindakan yang mungkin dari status (S '), pilih satu dengan nilai Q tertinggi. </li><li>  Langkah 5: Perbarui nilai-nilai Q-tabel sesuai dengan persamaan di atas. </li><li>  Langkah 6: Ubah status berikutnya menjadi yang sekarang. </li><li>  Langkah 7: Jika negara target tercapai, kami menyelesaikan proses, dan kemudian ulangi. </li></ul><br>  <b>Q-learning dengan Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Hebat, sekarang semua nilai Anda akan disimpan dalam variabel <code>q_table</code> . <br><br>  Jadi, model Anda dilatih dalam kondisi lingkungan, dan sekarang tahu cara memilih penumpang dengan lebih akurat.  Dan Anda berkenalan dengan fenomena pembelajaran penguatan, dan Anda dapat memprogram algoritma untuk memecahkan masalah baru. <br><br>  Teknik pembelajaran penguatan lainnya: <br><br><ul><li>  Markov proses pengambilan keputusan (MDP) dan persamaan Bellman </li><li>  Pemrograman Dinamis: RL Berbasis Model, Iterasi Strategi, dan Iterasi Nilai </li><li>  Pelatihan-Q yang mendalam </li><li>  Metode penurunan strategi gradien </li><li>  Sarsa </li></ul><br>  Kode untuk latihan ini terletak di: <br><br>  vihar / python-reinforcement-learning </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id434738/">https://habr.com/ru/post/id434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id434728/index.html">Orang dan proses: mengapa udalenka tidak cocok untuk setiap perusahaan?</a></li>
<li><a href="../id434730/index.html">Database dalam memori: aplikasi, penskalaan dan penambahan penting</a></li>
<li><a href="../id434732/index.html">Hidup di 6200 DPI. Ulasan Inti HyperX Pulsefire</a></li>
<li><a href="../id434734/index.html">Transformasi Fourier. Cepat dan geram</a></li>
<li><a href="../id434736/index.html">Menggunakan database log Mikrotik untuk menekan kekerasan</a></li>
<li><a href="../id434740/index.html">Jaringan saraf diajarkan untuk mendeteksi panel surya dalam gambar satelit dan memprediksi tingkat distribusinya</a></li>
<li><a href="../id434742/index.html">Bagian 2: Menggunakan pengontrol PSBC UDB Cypress untuk mengurangi jumlah interupsi dalam printer 3D</a></li>
<li><a href="../id434744/index.html">Samsung SSD 860 QVO 1 TB dan 4 TB: konsumen pertama SATA QLC (2 bagian)</a></li>
<li><a href="../id434746/index.html">BLE di bawah mikroskop 4</a></li>
<li><a href="../id434750/index.html">Bagaimana mengendalikan infrastruktur jaringan Anda. Bab Dua Pembersihan dan dokumentasi</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>