<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§æüèª üíñ üßïüèΩ PDDM - Nuevo algoritmo de aprendizaje de refuerzo basado en modelos con programador avanzado üî® üò≠ üà¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="El aprendizaje por refuerzo se divide en dos grandes clases: sin modelo y basado en modelo. En el primer caso, las acciones se optimizan directamente ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>PDDM - Nuevo algoritmo de aprendizaje de refuerzo basado en modelos con programador avanzado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470179/"><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  El aprendizaje por refuerzo se divide en dos grandes clases: sin modelo y basado en modelo.  En el primer caso, las acciones se optimizan directamente mediante la se√±al de recompensa, y en el segundo, la red neuronal es solo un modelo de realidad, y las acciones √≥ptimas se seleccionan utilizando un programador externo.  Cada enfoque tiene sus propias ventajas y desventajas. </p><br><p>  Los desarrolladores de Berkeley y Google Brain introdujeron el algoritmo PDDM basado en modelos con un programador mejorado que le permite aprender efectivamente movimientos complejos con una gran cantidad de grados de libertad con una peque√±a cantidad de ejemplos.  Para aprender a rotar bolas en un brazo rob√≥tico con articulaciones de dedos realistas con 24 grados de libertad, solo tom√≥ 4 horas de pr√°ctica en un robot f√≠sico real. </p><a name="habracut"></a><br><p>  El refuerzo de aprendizaje es el entrenamiento de robots con una se√±al de recompensa.  Esto es similar a c√≥mo aprenden los seres vivos.  Pero el problema se complica por el hecho de que no se sabe c√≥mo cambiar los pesos de la red neuronal, por lo que sus acciones propuestas conducen a un aumento de las recompensas.  Por lo tanto, en Reinforcement Learning, los m√©todos convencionales de entrenamiento de redes neuronales no son adecuados.  Despu√©s de todo, no se sabe exactamente qu√© deber√≠a dar a su salida, lo que significa que es imposible encontrar un error entre su predicci√≥n y el estado real de las cosas.  Para omitir esta diferencia a trav√©s de las capas de la red neuronal y cambiar los pesos entre las neuronas para minimizar este error.  Este es un algoritmo cl√°sico de propagaci√≥n hacia atr√°s ense√±ado por redes neuronales. </p><br><p>  Por lo tanto, los cient√≠ficos han inventado varias formas de resolver este problema. </p><br><h2 id="model-free">  Sin modelo </h2><br><p>  Uno de los enfoques m√°s efectivos fue el modelo actor-cr√≠tico.  Deje que una red neuronal (actor) en su entrada reciba el estado del entorno del estado y, en la salida, emita acciones que deber√≠an conducir a un aumento en las recompensas.  Hasta ahora, estas acciones son aleatorias y simplemente dependen del flujo de se√±al dentro de la red, ya que la red neuronal a√∫n no ha sido entrenada.  Y la segunda red neuronal (cr√≠tica), deja que la entrada tambi√©n reciba el estado del entorno del estado, pero tambi√©n las acciones de la salida de la primera red.  Y en la salida, deje solo la recompensa de recompensa pronosticada, que se recibir√° si se aplican estas acciones. </p><br><p>  Ahora mire sus manos: no sabemos cu√°les deber√≠an ser las mejores acciones en la salida de la primera red, lo que lleva a un aumento en la recompensa.  Por lo tanto, usando el algoritmo de propagaci√≥n hacia atr√°s, no podemos entrenarlo.  Pero la segunda red neuronal puede predecir muy bien el valor exacto de la recompensa recompensa (o m√°s bien, generalmente su cambio), que recibir√° si ahora se aplican acciones.  ¬°Entonces tomemos el gradiente de cambio de error de la segunda red y apliqu√©moslo a la primera!  Por lo tanto, puede entrenar la primera red neuronal mediante el m√©todo cl√°sico de propagaci√≥n hacia atr√°s del error.  Simplemente tomamos el error no de las salidas de la primera red, sino de las salidas de la segunda. </p><br><p>  Como resultado, la primera red neuronal aprende a emitir acciones √≥ptimas que conducen a un aumento de las recompensas.  Porque si el cr√≠tico cr√≠tico cometi√≥ un error y predijo una recompensa m√°s peque√±a de lo que result√≥ ser en realidad, entonces el gradiente de esta diferencia mover√° las acciones del actor actor en la direcci√≥n para que el cr√≠tico prediga la recompensa con mayor precisi√≥n.  Y eso significa acciones m√°s √≥ptimas (despu√©s de todo, conducir√°n al hecho de que el cr√≠tico predice con precisi√≥n un premio m√°s alto).  Un principio similar funciona en la direcci√≥n opuesta: si el cr√≠tico sobreestima la recompensa esperada, la diferencia entre la expectativa y la realidad disminuir√° las salidas de acciones de la primera red neuronal, lo que llev√≥ a esta indicaci√≥n de recompensa sobreestimada de la segunda red. </p><br><p>  Como puede ver, en este caso, las acciones se optimizan directamente por la se√±al de recompensa.  Esta es la esencia com√∫n de todos los algoritmos sin modelo en el aprendizaje por refuerzo.  Son el estado del arte en este momento. </p><br><p>  Su ventaja es que se buscan acciones √≥ptimas por el descenso del gradiente, por lo tanto, al final, se encuentran las m√°s √≥ptimas.  Lo que significa mostrar el mejor resultado.  Otra ventaja es la capacidad de usar redes neuronales peque√±as (y, por lo tanto, m√°s f√°ciles de aprender).  Si de toda la variedad de factores ambientales algunos espec√≠ficos son clave para resolver el problema, entonces el descenso de gradiente es bastante capaz de identificarlos.  Y usar para resolver el problema.  Estas dos ventajas han asegurado el √©xito con m√©todos directos sin modelo. </p><br><p>  Pero tambi√©n tienen desventajas.  Dado que las acciones son ense√±adas directamente por la se√±al de recompensa, se necesitan muchos ejemplos de capacitaci√≥n.  Decenas de millones, incluso para casos muy simples.  Trabajan mal en tareas con una gran cantidad de grados de libertad.  Si el algoritmo no logra identificar de inmediato factores clave entre el paisaje de alta dimensi√≥n, lo m√°s probable es que no aprenda en absoluto.  Adem√°s, los m√©todos sin modelo pueden explotar vulnerabilidades en el sistema, enfoc√°ndose en acciones no √≥ptimas (si el descenso de gradiente converge en √©l), ignorando otros factores ambientales.  Incluso para tareas libres de modelo ligeramente diferentes, los m√©todos deben ser entrenados completamente nuevamente. </p><br><h2 id="model-based">  Basado en modelos </h2><br><p>  Los m√©todos basados ‚Äã‚Äãen modelos en el aprendizaje por refuerzo son fundamentalmente diferentes del enfoque descrito anteriormente.  En Model-Based, una red neuronal solo predice lo que suceder√° despu√©s.  No ofrece ninguna acci√≥n.  Es decir, es simplemente un modelo de realidad (de ah√≠ el "Modelo" -basado en el nombre).  Y no es un sistema de toma de decisiones en absoluto. </p><br><p>  Las redes neuronales basadas en modelos se alimentan con el estado actual del entorno del estado y las acciones que queremos realizar.  Y la red neuronal predice c√≥mo cambiar√° el estado en el futuro despu√©s de aplicar estas acciones.  Tambi√©n puede predecir qu√© recompensa ser√° el resultado de estas acciones.  Pero esto no es necesario, ya que la recompensa generalmente se puede calcular a partir de un estado conocido.  Adem√°s, este estado de salida puede retroalimentarse a la entrada de la red neuronal (junto con nuevas acciones propuestas) y, por lo tanto, predecir recursivamente los cambios en el entorno externo muchos pasos adelante. </p><br><p>  Las redes neuronales basadas en modelos son muy f√°ciles de aprender.  Dado que simplemente predicen c√≥mo cambiar√° el mundo, sin hacer ninguna sugerencia, qu√© acciones √≥ptimas deber√≠an ser para que la recompensa aumente.  Por lo tanto, la red neuronal basada en modelos utiliza todos los ejemplos existentes para su entrenamiento, y no solo aquellos que conducen a un aumento o disminuci√≥n de las recompensas, como es el caso de Model-Free.  Esta es la raz√≥n por la cual las redes neuronales basadas en modelos necesitan muchos menos ejemplos de entrenamiento. </p><br><p>  El √∫nico inconveniente es que la red neuronal basada en el modelo debe estudiar la din√°mica real del sistema y, por lo tanto, debe tener la capacidad suficiente para ello.  Una red neuronal sin modelo puede converger en factores clave, ignorando el resto y, por lo tanto, ser una red peque√±a y simple (si la tarea se resuelve en principio con menos recursos). </p><br><p>  Otra gran ventaja, adem√°s de la capacitaci√≥n en un n√∫mero menor de ejemplos, es que, como modelo universal del mundo, se puede usar una √∫nica red neuronal basada en modelos para resolver cualquier n√∫mero de problemas en este mundo. </p><br><p>  El principal problema en el enfoque basado en modelos es qu√© acciones deben aplicarse a la entrada de redes neuronales.  Despu√©s de todo, la red neuronal en s√≠ misma no ofrece ninguna acci√≥n √≥ptima. </p><br><p>  La forma m√°s f√°cil es conducir a trav√©s de dicha red neuronal decenas de miles de acciones aleatorias y elegir aquellas para las cuales la red neuronal predecir√° la mayor recompensa.  Este es un cl√°sico aprendizaje basado en modelos de refuerzo.  Sin embargo, con grandes dimensiones y largas cadenas de tiempo, el n√∫mero de acciones posibles resulta ser demasiado grande para clasificarlas todas (o incluso adivinar al menos un poco √≥ptimo). </p><br><p>  Por esta raz√≥n, los m√©todos basados ‚Äã‚Äãen modelos suelen ser inferiores a los modelos libres, que por el descenso del gradiente convergen directamente con las acciones m√°s √≥ptimas. </p><br><p>  Una versi√≥n mejorada aplicable a los movimientos en rob√≥tica no es usar acciones aleatorias, sino mantener el movimiento anterior, agregando aleatoriedad a la distribuci√≥n normal.  Dado que los movimientos de los robots suelen ser suaves, esto reduce el n√∫mero de bustos.  Pero al mismo tiempo, se puede pasar por alto un cambio brusco importante. </p><br><p>  La opci√≥n de desarrollo final para este enfoque puede considerarse la opci√≥n CEM, que no utiliza una distribuci√≥n normal fija que introduce aleatoriedad en la ruta actual de acciones, sino que selecciona los par√°metros de la distribuci√≥n aleatoria mediante entrop√≠a cruzada.  Para hacer esto, se lanza una poblaci√≥n de c√°lculos de acciones y los mejores de ellos se utilizan para refinar la propagaci√≥n de par√°metros en la pr√≥xima generaci√≥n.  Algo as√≠ como un algoritmo evolutivo. </p><br><h2 id="pddm">  PDDM </h2><br><p>  Se necesitaba una introducci√≥n tan larga para explicar lo que est√° sucediendo en el nuevo algoritmo de aprendizaje de refuerzo basado en el modelo PDDM propuesto.  Despu√©s de leer un art√≠culo en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">blog de Berkeley AI</a> (o una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">versi√≥n extendida</a> ), e incluso el art√≠culo original <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">arxiv.org/abs/1909.11652</a> , esto podr√≠a no haber sido obvio. </p><br><p>  El m√©todo PDDM repite la idea de CEM al elegir acciones aleatorias que deben ejecutarse a trav√©s de una red neuronal basada en modelos para seleccionar acciones con la mayor recompensa predecible.  Solo en lugar de seleccionar par√°metros de distribuci√≥n aleatoria, como se hace en CEM, PDDM usa una correlaci√≥n temporal entre acciones y una regla m√°s suave para actualizar la distribuci√≥n aleatoria.  La f√≥rmula se da en el art√≠culo original.  Esto le permite verificar un mayor n√∫mero de acciones adecuadas a largas distancias, especialmente si los movimientos requieren una coordinaci√≥n precisa.  Adem√°s, los autores del algoritmo filtran candidatos para acciones, obteniendo as√≠ una trayectoria de movimientos m√°s suave. </p><br><p>  En pocas palabras, los desarrolladores simplemente propusieron una mejor f√≥rmula para elegir acciones aleatorias para probar en el cl√°sico aprendizaje basado en modelos de refuerzo. </p><br><p>  Pero el resultado fue muy bueno. </p><br><p>  En solo 4 horas de entrenamiento en un robot real, un robot con 24 grados de libertad aprendi√≥ a sostener dos bolas y rotarlas en las palmas sin dejarlas caer.  Un resultado inalcanzable para cualquier m√©todo moderno sin modelo con un n√∫mero tan peque√±o de ejemplos. </p><br><p>  Curiosamente, para el entrenamiento, utilizaron un segundo brazo rob√≥tico con 7 grados de libertad, que recogi√≥ las bolas y las devolvi√≥ al brazo rob√≥tico principal: </p><br><p><img src="https://habrastorage.org/webt/7s/yf/wq/7syfwqr2vg2-rlzupbccu-06mic.gif"></p><br><p>  Como resultado, despu√©s de 1-2 horas, el roboruk pudo sostener con confianza las bolas y moverlas en la palma de su mano, y 4 horas fueron suficientes para completar el entrenamiento. </p><br><p><img src="https://habrastorage.org/webt/az/gq/1u/azgq1uy2qpj7dwfvwxzq6qmsv2a.gif"></p><br><p>  Presta atenci√≥n a los movimientos de los dedos.  Esta es una caracter√≠stica de los enfoques basados ‚Äã‚Äãen modelos.  Dado que las acciones previstas se eligen al azar, no siempre coinciden con las √≥ptimas.  El algoritmo sin modelo podr√≠a potencialmente converger en movimientos suaves realmente √≥ptimos. </p><br><p>  Sin embargo, el enfoque basado en el modelo permite con una red neuronal capacitada modelar el mundo para resolver diferentes problemas sin necesidad de reentrenamiento.  Hay varios ejemplos en el art√≠culo, por ejemplo, puede cambiar f√°cilmente la direcci√≥n de rotaci√≥n de las bolas en la mano (en Model-Free, tendr√≠a que volver a entrenar la red neuronal para esto).  O sostenga la pelota en un punto espec√≠fico en la palma de su mano, siguiendo el punto rojo. </p><br><p><img src="https://habrastorage.org/webt/np/2a/4v/np2a4vibrlvwohr_kimp7w4o_1u.gif"></p><br><p>  Tambi√©n puede hacer que Roboruk dibuje trayectorias arbitrarias con un l√°piz, aprendiendo que para los m√©todos sin modelo es una tarea muy dif√≠cil. </p><br><p><img src="https://habrastorage.org/webt/iz/a_/v5/iza_v5a2jkohqpcl8vb99_yfwri.gif"></p><br><p>  Aunque el algoritmo propuesto no es una panacea, y ni siquiera es un algoritmo de IA en el sentido completo de la palabra (en PDDM, la red neuronal simplemente reemplaza el modelo anal√≠tico, y las decisiones se toman mediante b√∫squeda aleatoria con una regla dif√≠cil que reduce el n√∫mero de enumeraci√≥n de opciones), puede ser √∫til en rob√≥tica.  Dado que mostr√≥ una mejora notable en los resultados y est√° capacitado en un n√∫mero muy peque√±o de ejemplos. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/470179/">https://habr.com/ru/post/470179/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../470167/index.html">Conferencia para aquellos interesados ‚Äã‚Äãen la ciencia antes de que se generalizara</a></li>
<li><a href="../470169/index.html">C√≥mo evitar que la idea muera y reunir un equipo que no la mate</a></li>
<li><a href="../470171/index.html">Habr Weekly # 21 / Dobroshrift, technodom para un gato, el derecho a reparar electrodom√©sticos, la Uni√≥n Europea y cookies "transparentes"</a></li>
<li><a href="../470173/index.html">Plataforma de integraci√≥n como servicio</a></li>
<li><a href="../470175/index.html">Agregar Iniciar sesi√≥n con Apple en el back-end</a></li>
<li><a href="../470187/index.html">El rango de precios para el dise√±o y el dise√±o de un servicio en l√≠nea es de 100 mil a 5 millones de rublos. Razones</a></li>
<li><a href="../470189/index.html">Env√≠o de mensajes entre pares con PeerJS</a></li>
<li><a href="../470193/index.html">Protecci√≥n universal contra ataques xss e inyecciones sql</a></li>
<li><a href="../470195/index.html">F # 4: Let / Use / Do</a></li>
<li><a href="../470197/index.html">¬øPuedo hacer? Considere el patr√≥n Has</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>