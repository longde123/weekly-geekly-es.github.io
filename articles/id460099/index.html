<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â¡ï¸ ğŸ˜¥ â›ªï¸ Penerapan pembelajaran mesin otomatis ke jaringan saraf dengan arsitektur transformator ğŸ• ğŸ‘©ğŸ»â€ğŸ”§ ğŸ•´ğŸ»</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dari Google AI Blog 

 Sejak publikasi informasi tentang mereka pada tahun 2017, jaringan saraf arsitektur transformator telah diterapkan untuk berbag...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Penerapan pembelajaran mesin otomatis ke jaringan saraf dengan arsitektur transformator</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460099/"> <i>Dari Google AI Blog</i> <br><br>  Sejak <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">publikasi informasi</a> tentang mereka pada tahun 2017, jaringan saraf arsitektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">transformator</a> telah diterapkan untuk berbagai jenis tugas, dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menghasilkan teks bergaya fantasi</a> hingga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menulis harmoni musik</a> .  Yang penting, kualitas tinggi dari pekerjaan "transformer" telah menunjukkan bahwa ketika diterapkan pada tugas berurutan, seperti pemodelan dan terjemahan bahasa, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">distribusi langsung jaringan saraf</a> dapat seefektif yang berulang.  Meskipun popularitas transformator dan model distribusi langsung lainnya yang digunakan dalam tugas berurutan semakin meningkat, arsitektur mereka hampir selalu dibuat secara manual, berbeda dengan bidang visi komputer, di mana pendekatan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pembelajaran mesin</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">canggih</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">AOM</a> ) telah menemukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">model</a> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">model canggih</a> yang lebih maju daripada yang diekspos. pengaturan manual.  Secara alami, kami tertarik pada apakah penerapan AOM untuk tugas berurutan dapat mencapai kesuksesan yang sama. <br><a name="habracut"></a><br>  Setelah melakukan pencarian <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">evolusi</a> untuk pencarian arsitektur saraf (NAS), dan menggunakan terjemahan sebagai contoh tugas berurutan, kami menemukan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">transformator berevolusi</a> (ET) - arsitektur transformator baru yang menunjukkan peningkatan dalam berbagai tugas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pemrosesan bahasa alami</a> (OYA).  ET tidak hanya mencapai hasil mutakhir dalam terjemahan, tetapi juga menunjukkan peningkatan efisiensi dalam pemodelan bahasa dibandingkan dengan transformator asli.  Kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menerbitkan</a> model baru di perpustakaan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tensor2Tensor</a> , di mana ia dapat digunakan untuk tugas berurutan apa pun. <br><br><h2>  Pengembangan Teknisi </h2><br>  Untuk memulai pencarian evolusi untuk neuroarchitecture, kami perlu mengembangkan teknik baru, karena tugas yang digunakan untuk menilai "kebugaran" dari masing-masing arsitektur, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">terjemahan dari bahasa Inggris ke Jerman WMT'14</a> , menuntut sumber daya komputasi.  Akibatnya, pencarian ini ternyata lebih menuntut daripada pencarian serupa di bidang visi komputer, yang dapat beroperasi dengan basis data yang lebih kecil, misalnya, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CIFAR-10</a> .  Yang pertama dari teknik-teknik ini adalah awal yang hangat, menabur populasi evolusi asli dengan arsitektur tipe transformator bukan model acak.  Ini membantu memusatkan pencarian di area yang jelas kuat dari ruang pencarian, yang memungkinkan kami untuk dengan cepat menemukan model terbaik. <br><br>  Teknik kedua adalah metode baru yang dikembangkan oleh kami yang disebut Progressive Dynamic Hurdles (PDH).  Algoritma ini melengkapi pencarian evolusi, yang memungkinkan Anda mengalokasikan lebih banyak sumber daya ke kandidat terkuat, tidak seperti karya sebelumnya, di mana setiap model kandidat di NAS dialokasikan jumlah sumber daya yang sama.  PDH memungkinkan kita untuk selesai mengevaluasi suatu model lebih awal jika itu sangat buruk, sambil menghargai arsitektur yang menjanjikan dengan banyak sumber daya. <br><br><h2>  Transformer Berkembang </h2><br>  Dengan menggunakan metode ini, kami melakukan pencarian NAS skala besar pada tugas terjemahan kami dan menemukan ET.  Seperti kebanyakan arsitektur jaringan saraf dari tipe â€œsequence to sequenceâ€ (urutan ke urutan, seq2seq), ia memiliki encoder yang menyandikan urutan input ke dalam sisipan, dan decoder yang menggunakan sisipan ini untuk membuat urutan output.  Dalam kasus terjemahan, urutan input adalah penawaran terjemahan, dan urutan output adalah terjemahan. <br><br>  Fitur yang paling menarik dari ET adalah lapisan konvolusional di bagian bawah modul encoder dan decoder, ditambahkan dengan cara bercabang yang serupa untuk kedua tempat ini (yaitu, input melewati dua lapisan convolutional yang berbeda sebelum melipat). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8a/62d/1d1/b8a62d1d155203861756f0960becaaf0.png"><br>  <i>Perbandingan arsitektur encoder konvensional dan ET encoders.</i>  <i>Perhatikan struktur konvolusional bercabang di bagian bawah modul, yang terbentuk secara independen baik dalam enkoder maupun dalam dekoder.</i>  <i>Dekoder dijelaskan secara rinci dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pekerjaan kami</a> .</i> <br><br>  Ini sangat menarik karena encoder dan decoder selama NAS tidak berbagi arsitektur satu sama lain, dan utilitas arsitektur ini ditemukan secara independen dalam encoder dan decoder, yang berbicara mendukung skema semacam itu.  Jika transformator asli sepenuhnya mengandalkan penerapan perhatian pada data yang sama dengan yang ia hasilkan sendiri [perhatian-diri], ET adalah hibrida yang mengambil keuntungan dari perhatian-diri dan konvolusi luas. <br><br><h2>  Skor ET </h2><br>  Untuk menguji efektivitas arsitektur baru ini, pertama-tama kami membandingkannya dengan transformator asli, yang berfungsi dengan tugas menerjemahkan dari Bahasa Inggris ke Bahasa Jerman, yang kami gunakan selama pencarian.  Kami menemukan bahwa ET memiliki indikator dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">konektivitas</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">BLEU</a> terbaik pada semua ukuran parameter, dan peningkatan terbesar dalam ukuran dapat dibandingkan dengan perangkat seluler (~ 7 juta parameter), yang menunjukkan penggunaan parameter yang efisien.  Pada ukuran yang lebih besar, ET mencapai hasil mutakhir pada WMT '14 En-De dengan BLEU 29,8 dan SacreBLEU 29,2. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a2/4c4/c60/1a24c4c6085c167a5398fdcea218f75c.png"></div><br>  <i>Perbandingan ET dan transformator asli pada WMT'14 En-De dengan volume berbeda.</i>  <i>Keuntungan terbesar dicapai dengan ukuran kecil, sementara ET menunjukkan kinerja yang baik pada ukuran yang lebih besar, di depan transformator terbesar dengan parameter 37,6% lebih sedikit (model yang sebanding berada dalam lingkaran).</i> <br><br>  Untuk memeriksa generalisasi, kami membandingkan ET dengan transformator pada masalah tambahan pemrosesan bahasa alami.  Pertama, kami memeriksa terjemahan untuk pasangan bahasa yang berbeda, dan menemukan bahwa efektivitas ET lebih tinggi, dan pemisahannya kira-kira sama dengan yang ditunjukkan dalam terjemahan bahasa Inggris-Jerman;  dan sekali lagi, berkat penggunaan parameter yang efisien, celah terbesar diamati pada model berukuran sedang.  Kami juga membandingkan decoder dari kedua model pada pemodelan bahasa di <a href="">LM1B</a> , dan melihat peningkatan yang signifikan dalam konektivitas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c5/c96/1b6/6c5c961b6a09ba83905d9f886c9063ea.png"><br><br><h2>  Rencana masa depan </h2><br>  Hasil ini adalah langkah pertama dalam mengeksplorasi aplikasi pencarian arsitektur untuk model distribusi langsung berurutan.  ET didistribusikan sebagai <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sumber terbuka</a> dalam kerangka proyek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://www.google.com/url%3Fq%3D">Tensor2Tensor</a> , di mana ia dapat digunakan pada masalah yang berurutan.  Untuk meningkatkan reproduktifitas, kami juga membuka <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode ruang pencarian</a> yang kami gunakan dalam pencarian kami, dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Colab</a> dengan implementasi PDH.  Kami menantikan hasil dari komunitas riset, dipersenjatai dengan model-model baru, dan kami berharap orang lain dapat mengambil teknik pencarian baru ini sebagai dasar! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id460099/">https://habr.com/ru/post/id460099/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id460087/index.html">Sortir Piramida (HeapSort)</a></li>
<li><a href="../id460089/index.html">Pembaruan Aman Zimbra Collaboration Suite</a></li>
<li><a href="../id460091/index.html">Pencetakan langsung pada kaos dengan Epson SureColor SC - F dan perbedaannya dari pencetakan, decal, dan sublimasi layar sutra</a></li>
<li><a href="../id460095/index.html">Tertangkap larangan fork deepNude di gitlab.com</a></li>
<li><a href="../id460097/index.html">Matriks memiliki Anda: ikhtisar proyek menggunakan MITER ATT & CK</a></li>
<li><a href="../id460101/index.html">Operasi XSS berbasis cookie | $ 2300 cerita Bug Bounty</a></li>
<li><a href="../id460107/index.html">Sistem ISP, maafkan dan selamat tinggal! Mengapa dan bagaimana kami menulis panel kontrol server kami</a></li>
<li><a href="../id460109/index.html">Sudut: ketika Anda perlu melihat aplikasi, tetapi backend belum siap</a></li>
<li><a href="../id460111/index.html">Versi terbaru SAP Business One 9.3: apa yang telah berubah</a></li>
<li><a href="../id460113/index.html">Beberapa cerita dari kehidupan JSOC CERT, atau forensik Unbanal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>