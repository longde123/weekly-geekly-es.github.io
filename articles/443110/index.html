<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêí üê∏ üí≤ Configure el cl√∫ster Kubernetes HA en metal desnudo con GlusterFS y MetalLB. Parte 2/3 üö° ü¶É ‚öæÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Parte 1/3 aqu√≠ 
 Parte 3/3 aqu√≠ 


 Hola y bienvenido de nuevo! Esta es la segunda parte del art√≠culo sobre la configuraci√≥n de un cl√∫ster de Kubernet...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Configure el cl√∫ster Kubernetes HA en metal desnudo con GlusterFS y MetalLB. Parte 2/3</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/443110/"><p><img src="https://habrastorage.org/webt/oa/xl/av/oaxlavwz_atdglepw3r_vn6hmxm.jpeg"></p><br><p>  <strong>Parte 1/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>aqu√≠</strong></a> <br>  <strong>Parte 3/3</strong> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>aqu√≠</strong></a> </p><br><p>  Hola y bienvenido de nuevo!  Esta es la segunda parte del art√≠culo sobre la configuraci√≥n de un cl√∫ster de Kubernetes en metal desnudo.  Anteriormente, configuramos el cl√∫ster de Kubernetes HA mediante un etcd externo, maestro-maestro y equilibrio de carga.  Bueno, ahora es el momento de configurar un entorno y utilidades adicionales para hacer que el cl√∫ster sea m√°s √∫til y lo m√°s cercano posible al estado de trabajo. </p><br><p>  En esta parte del art√≠culo, nos centraremos en la configuraci√≥n del equilibrador de carga interno de los servicios de cl√∫ster: este ser√° MetalLB.  Tambi√©n instalaremos y configuraremos el almacenamiento de archivos distribuidos entre nuestros nodos de trabajo.  Usaremos GlusterFS para vol√∫menes persistentes que est√°n disponibles en Kubernetes. <br>  Despu√©s de completar todos los pasos, nuestro diagrama de cl√∫ster se ver√° as√≠: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/_v/yp/pe/_vyppenp91uzmkowqv1qcyomnrc.jpeg"></a> </p><a name="habracut"></a><br><h3 id="1-nastroyka-metallb-v-kachestve-vnutrennego-balansirovschika-nagruzki">  1. Configure MetalLB como un equilibrador de carga interno. </h3><br><p>  Algunas palabras sobre MetalLB, directamente desde la p√°gina del documento: </p><br><blockquote> MetalLB es una implementaci√≥n de equilibrador de carga para cl√∫steres de metal desnudo de Kubernetes con protocolos de enrutamiento est√°ndar. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Kubernetes</a> no ofrece la implementaci√≥n de equilibradores de carga de red ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">tipo de servicio LoadBalancer</a> ) para metal desnudo.  Todas las opciones de implementaci√≥n de Network LB que Kubernetes incluye son middleware, y accede a varias plataformas IaaS (GCP, AWS, Azure, etc.).  Si no trabaja en una plataforma compatible con IaaS (GCP, AWS, Azure, etc.), LoadBalancer permanecer√° en el estado "en espera" durante un per√≠odo indefinido tras la creaci√≥n. <br><br>  Los operadores de servidores BM tienen dos herramientas menos efectivas para ingresar el tr√°fico de usuarios en sus cl√∫steres, NodePort y servicios de IP externos.  Ambas opciones tienen importantes deficiencias en la producci√≥n, lo que convierte a los grupos de BM en ciudadanos de segunda clase en el ecosistema de Kubernetes. <br><br>  MetalLB busca corregir este desequilibrio ofreciendo una implementaci√≥n de Network LB que se integra con el equipo de red est√°ndar, por lo que los servicios externos en los cl√∫steres BM tambi√©n "funcionan" a la m√°xima velocidad. </blockquote><p>  Por lo tanto, utilizando esta herramienta, lanzamos servicios en el cl√∫ster de Kubernetes utilizando un equilibrador de carga, por lo que muchas gracias al equipo de MetalLB.  El proceso de configuraci√≥n es realmente simple y directo. </p><br><p>  Anteriormente en el ejemplo, seleccionamos la subred 192.168.0.0/24 para las necesidades de nuestro cl√∫ster.  Ahora tome parte de esta subred para el futuro equilibrador de carga. </p><br><p>  <strong>Ingresamos al</strong> sistema de la m√°quina con la utilidad <strong>kubectl</strong> configurada y ejecutamos: </p><br><pre><code class="plaintext hljs">control# kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code> </pre> <br><p>  Esto desplegar√° MetalLB en el cl√∫ster, en el <code>metallb-system</code> .  Aseg√∫rese de que todos los componentes de MetalLB funcionen correctamente: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod --namespace=metallb-system NAME READY STATUS RESTARTS AGE controller-7cc9c87cfb-ctg7p 1/1 Running 0 5d3h speaker-82qb5 1/1 Running 0 5d3h speaker-h5jw7 1/1 Running 0 5d3h speaker-r2fcg 1/1 Running 0 5d3h</code> </pre> <br><p>  Ahora configure MetalLB usando configmap.  En este ejemplo, estamos utilizando la personalizaci√≥n de Capa 2. Para obtener informaci√≥n sobre otras opciones de personalizaci√≥n, consulte la documentaci√≥n de MetalLB. </p><br><p>  Cree el <strong>archivo metallb-config.yaml</strong> en cualquier directorio dentro del rango de IP seleccionado de la subred de nuestro cl√∫ster: </p><br><pre> <code class="plaintext hljs">control# vi metallb-config.yaml apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.0.240-192.168.0.250</code> </pre> <br><p>  Y aplique esta configuraci√≥n: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f metallb-config.yaml</code> </pre> <br><p>  Verifique y modifique el mapa de configuraci√≥n m√°s adelante si es necesario: </p><br><pre> <code class="plaintext hljs">control# kubectl describe configmaps -n metallb-system control# kubectl edit configmap config -n metallb-system</code> </pre> <br><p>  Ahora tenemos nuestro propio equilibrador de carga local configurado.  Veamos c√≥mo funciona, utilizando el servicio Nginx como ejemplo. </p><br><pre> <code class="plaintext hljs">control# vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 control# vi nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx spec: type: LoadBalancer selector: app: nginx ports: - port: 80 name: http</code> </pre> <br><p>  Luego cree una implementaci√≥n de prueba y un servicio Nginx: </p><br><pre> <code class="plaintext hljs">control# kubectl apply -f nginx-deployment.yaml control# kubectl apply -f nginx-service.yaml</code> </pre> <br><p>  Y ahora, verifique el resultado: </p><br><pre> <code class="plaintext hljs">control# kubectl get po NAME READY STATUS RESTARTS AGE nginx-deployment-6574bd76c-fxgxr 1/1 Running 0 19s nginx-deployment-6574bd76c-rp857 1/1 Running 0 19s nginx-deployment-6574bd76c-wgt9n 1/1 Running 0 19s control# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.100.226.110 192.168.0.240 80:31604/TCP 107s</code> </pre> <br><p>  Cre√© 3 pods Nginx, como indicamos en la implementaci√≥n anterior.  El servicio Nginx dirigir√° el tr√°fico a todos estos pods de acuerdo con el esquema de equilibrio c√≠clico.  Y tambi√©n puede ver la IP externa recibida de nuestro equilibrador de carga MetalLB. </p><br><p>  Ahora intente pasar a la direcci√≥n IP 192.168.0.240 y ver√° la p√°gina Nginx index.html.  Recuerde eliminar la implementaci√≥n de prueba y el servicio Nginx. </p><br><pre> <code class="plaintext hljs">control# kubectl delete svc nginx service "nginx" deleted control# kubectl delete deployment nginx-deployment deployment.extensions "nginx-deployment" deleted</code> </pre> <br><p>  Bueno, eso es todo con MetalLB, sigamos adelante: configuraremos GlusterFS para los vol√∫menes de Kubernetes. </p><br><h3 id="2-nastroyka-glusterfs-s-heketi-na-rabochih-nodah">  2. Configuraci√≥n de GlusterFS con Heketi en nodos de trabajo. </h3><br><p>  De hecho, el cl√∫ster de Kubernetes no se puede usar sin vol√∫menes dentro de √©l.  Como saben, los hogares son ef√≠meros, es decir.  Se pueden crear y eliminar en cualquier momento.  Todos los datos dentro de ellos se perder√°n.  Por lo tanto, en un cl√∫ster real, se requiere almacenamiento distribuido para garantizar el intercambio de configuraciones y datos entre nodos y aplicaciones dentro de √©l. </p><br><p>  En Kubernetes, los vol√∫menes est√°n disponibles de varias maneras; elija los que desee.  En este ejemplo, demostrar√© c√≥mo crear almacenamiento GlusterFS para cualquier aplicaci√≥n interna, es como vol√∫menes persistentes.  Anteriormente, us√© la instalaci√≥n del "sistema" de GlusterFS en todos los nodos de trabajo de Kubernetes para esto, y luego simplemente cre√© vol√∫menes como hostPath en los directorios de GlusterFS. </p><br><p>  Ahora tenemos una nueva herramienta pr√°ctica de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><strong>Heketi</strong></a> . </p><br><p>  Algunas palabras de la documentaci√≥n de Heketi: </p><br><blockquote>  Infraestructura de gesti√≥n de volumen RESTful para GlusterFS. <br><br>  Heketi ofrece una interfaz de administraci√≥n RESTful que se puede utilizar para administrar el ciclo de vida de los vol√∫menes GlusterFS.  Gracias a Heketi, los servicios en la nube como OpenStack Manila, Kubernetes y OpenShift pueden proporcionar din√°micamente vol√∫menes GlusterFS con cualquier tipo de confiabilidad compatible.  Heketi determina autom√°ticamente la ubicaci√≥n de los bloques en un cl√∫ster, proporcionando la ubicaci√≥n de los bloques y sus r√©plicas en diferentes √°reas de falla.  Heketi tambi√©n admite cualquier cantidad de cl√∫steres de GlusterFS, lo que permite que los servicios en la nube ofrezcan almacenamiento de archivos en l√≠nea, no solo un cl√∫ster de GlusterFS. </blockquote><p>  Suena bien y, adem√°s, esta herramienta acercar√° nuestro cl√∫ster de VM a los grandes cl√∫steres de nube de Kubernetes.  Al final, podr√°s crear <strong>PersistentVolumeClaims</strong> , que se generar√° autom√°ticamente y mucho m√°s. </p><br><p>  Puede tomar discos duros del sistema adicionales para configurar GlusterFS o simplemente crear algunos dispositivos de bloques ficticios.  En este ejemplo, usar√© el segundo m√©todo. </p><br><p>  Cree dispositivos de bloques ficticios en los tres nodos de trabajo: </p><br><pre> <code class="plaintext hljs">worker1-3# dd if=/dev/zero of=/home/gluster/image bs=1M count=10000</code> </pre> <br><p>  Obtendr√° un archivo de aproximadamente 10 GB de tama√±o.  Luego use <strong>losetup</strong> - para agregarlo a estos nodos, como un dispositivo de bucle invertido: </p><br><pre> <code class="plaintext hljs">worker1-3# losetup /dev/loop0 /home/gluster/image</code> </pre> <br><blockquote>  <em>Tenga en cuenta: si ya tiene alg√∫n tipo de dispositivo de bucle invertido 0, deber√° elegir cualquier otro n√∫mero.</em> </blockquote><p>  Me tom√© el tiempo y descubr√≠ por qu√© Heketi no quiere trabajar correctamente.  Por lo tanto, para evitar cualquier problema en futuras configuraciones, primero aseg√∫rese de haber cargado el <strong>m√≥dulo del</strong> n√∫cleo <strong>dm_thin_pool</strong> e instalado el paquete <strong>glusterfs-client</strong> en todos los nodos de trabajo. </p><br><pre> <code class="plaintext hljs">worker1-3# modprobe dm_thin_pool worker1-3# apt-get update &amp;&amp; apt-get -y install glusterfs-client</code> </pre> <br><p>  Bueno, ahora necesita que el archivo <strong>/ home / gluster / image</strong> y el dispositivo <strong>/ dev / loop0</strong> est√©n presentes en todos los nodos de trabajo.  Recuerde crear un servicio systemd que autom√°ticamente inicie <strong>losetup</strong> y <strong>modprobe</strong> cada vez que estos servidores se inicien. </p><br><pre> <code class="plaintext hljs">worker1-3# vi /etc/systemd/system/loop_gluster.service [Unit] Description=Create the loopback device for GlusterFS DefaultDependencies=false Before=local-fs.target After=systemd-udev-settle.service Requires=systemd-udev-settle.service [Service] Type=oneshot ExecStart=/bin/bash -c "modprobe dm_thin_pool &amp;&amp; [ -b /dev/loop0 ] || losetup /dev/loop0 /home/gluster/image" [Install] WantedBy=local-fs.target</code> </pre> <br><p>  Y enci√©ndelo: </p><br><pre> <code class="plaintext hljs">worker1-3# systemctl enable /etc/systemd/system/loop_gluster.service Created symlink /etc/systemd/system/local-fs.target.wants/loop_gluster.service ‚Üí /etc/systemd/system/loop_gluster.service.</code> </pre> <br><p>  El trabajo preparatorio se ha completado y estamos listos para implementar GlusterFS y Heketi en nuestro cl√∫ster.  Para esto, usar√© esta <a href="">gu√≠a</a> genial.  La mayor√≠a de los comandos se inician desde una computadora de control externa, y se ejecutan comandos muy peque√±os desde cualquier nodo maestro dentro del cl√∫ster. </p><br><p>  Primero, copie el repositorio y cree DaemonSet GlusterFS: </p><br><pre> <code class="plaintext hljs">control# git clone https://github.com/heketi/heketi control# cd heketi/extras/kubernetes control# kubectl create -f glusterfs-daemonset.json</code> </pre> <br><p>  Ahora marquemos nuestros tres nodos de trabajo para GlusterFS;  despu√©s de etiquetarlos, se crear√°n pods GlusterFS: </p><br><pre> <code class="plaintext hljs">control# kubectl label node worker1 storagenode=glusterfs control# kubectl label node worker2 storagenode=glusterfs control# kubectl label node worker3 storagenode=glusterfs control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 1m6s glusterfs-hzdll 1/1 Running 0 1m9s glusterfs-p8r59 1/1 Running 0 2m1s</code> </pre> <br><p>  Ahora cree una cuenta de servicio Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-service-account.json</code> </pre> <br><p>  Brindamos a esta cuenta de servicio la capacidad de administrar c√°psulas de brillo.  Para hacer esto, cree una funci√≥n de cl√∫ster requerida para nuestra cuenta de servicio reci√©n creada: </p><br><pre> <code class="plaintext hljs">control# kubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account</code> </pre> <br><p>  Ahora creemos una clave secreta de Kubernetes que bloquea la configuraci√≥n de nuestra instancia de Heketi: </p><br><pre> <code class="plaintext hljs">control# kubectl create secret generic heketi-config-secret --from-file=./heketi.json</code> </pre> <br><p>  Cree la primera fuente en Heketi, que usamos para las primeras operaciones de configuraci√≥n y luego elimine: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f heketi-bootstrap.json service "deploy-heketi" created deployment "deploy-heketi" created control# kubectl get pod NAME READY STATUS RESTARTS AGE deploy-heketi-1211581626-2jotm 1/1 Running 0 2m glusterfs-5dtdj 1/1 Running 0 6m6s glusterfs-hzdll 1/1 Running 0 6m9s glusterfs-p8r59 1/1 Running 0 7m1s</code> </pre> <br><p>  Despu√©s de crear e iniciar el servicio Bootstrap Heketi, tendremos que cambiar a uno de nuestros nodos maestros, all√≠ ejecutaremos varios comandos, ya que nuestro nodo de control externo no est√° dentro de nuestro cl√∫ster, por lo que no podemos acceder a los pods de trabajo ni a la red interna del cl√∫ster. </p><br><p>  Primero, descarguemos la utilidad heketi-client y c√≥piela en la carpeta del sistema bin: </p><br><pre> <code class="plaintext hljs">master1# wget https://github.com/heketi/heketi/releases/download/v8.0.0/heketi-client-v8.0.0.linux.amd64.tar.gz master1# tar -xzvf ./heketi-client-v8.0.0.linux.amd64.tar.gz master1# cp ./heketi-client/bin/heketi-cli /usr/local/bin/ master1# heketi-cli heketi-cli v8.0.0</code> </pre> <br><p>  Ahora busque la direcci√≥n IP del pod heketi y exp√≥rtela como una variable del sistema: </p><br><pre> <code class="plaintext hljs">master1# kubectl --kubeconfig /etc/kubernetes/admin.conf describe pod deploy-heketi-1211581626-2jotm For me this pod have a 10.42.0.1 ip master1# curl http://10.42.0.1:57598/hello Handling connection for 57598 Hello from Heketi master1# export HEKETI_CLI_SERVER=http://10.42.0.1:57598</code> </pre> <br><p>  Ahora proporcionemos a Heketi informaci√≥n sobre el cl√∫ster GlusterFS que debe administrar.  Lo proporcionamos a trav√©s de un archivo de topolog√≠a.  Una topolog√≠a es un manifiesto JSON con una lista de todos los nodos, discos y cl√∫steres utilizados por GlusterFS. </p><br><blockquote>  NOTA  Aseg√∫rese de que <code>hostnames/manage</code> indique el nombre exacto, como en la secci√≥n de <code>kubectl get node</code> , y que <code>hostnames/storage</code> es la direcci√≥n IP de los nodos de almacenamiento. </blockquote><br><pre> <code class="plaintext hljs">master1:~/heketi-client# vi topology.json { "clusters": [ { "nodes": [ { "node": { "hostnames": { "manage": [ "worker1" ], "storage": [ "192.168.0.7" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker2" ], "storage": [ "192.168.0.8" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] }, { "node": { "hostnames": { "manage": [ "worker3" ], "storage": [ "192.168.0.9" ] }, "zone": 1 }, "devices": [ "/dev/loop0" ] } ] } ] }</code> </pre> <br><p>  Luego descargue este archivo: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli topology load --json=topology.json Creating cluster ... ID: e83467d0074414e3f59d3350a93901ef Allowing file volumes on cluster. Allowing block volumes on cluster. Creating node worker1 ... ID: eea131d392b579a688a1c7e5a85e139c Adding device /dev/loop0 ... OK Creating node worker2 ... ID: 300ad5ff2e9476c3ba4ff69260afb234 Adding device /dev/loop0 ... OK Creating node worker3 ... ID: 94ca798385c1099c531c8ba3fcc9f061 Adding device /dev/loop0 ... OK</code> </pre> <br><p>  Luego, utilizamos Heketi para proporcionar vol√∫menes para almacenar la base de datos.  El nombre del equipo es un poco extra√±o, pero todo est√° en orden.  Tambi√©n cree un repositorio heketi: </p><br><pre> <code class="plaintext hljs">master1:~/heketi-client# heketi-cli setup-openshift-heketi-storage master1:~/heketi-client# kubectl --kubeconfig /etc/kubernetes/admin.conf create -f heketi-storage.json secret/heketi-storage-secret created endpoints/heketi-storage-endpoints created service/heketi-storage-endpoints created job.batch/heketi-storage-copy-job created</code> </pre> <br><p>  Estos son todos los comandos que necesita ejecutar desde el nodo maestro.  Volvamos al nodo de control y continuemos desde all√≠;  En primer lugar, aseg√∫rese de que el √∫ltimo comando en ejecuci√≥n se ejecut√≥ correctamente: </p><br><pre> <code class="plaintext hljs">control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-storage-copy-job-txkql 0/1 Completed 0 69s</code> </pre> <br><p>  Y el trabajo heketi-storage-copy-job est√° hecho. </p><br><blockquote>  Si actualmente no hay <strong>un</strong> paquete <strong>glusterfs-client</strong> instalado instalado en sus nodos de trabajo, entonces se produce un error. </blockquote><p>  Es hora de eliminar el archivo de instalaci√≥n de Heketi Bootstrap y hacer una peque√±a limpieza: </p><br><pre> <code class="plaintext hljs">control# kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"</code> </pre> <br><p>  En la √∫ltima etapa, necesitamos crear una copia a largo plazo de Heketi: </p><br><pre> <code class="plaintext hljs">control# cd ./heketi/extras/kubernetes control:~/heketi/extras/kubernetes# kubectl create -f heketi-deployment.json secret/heketi-db-backup created service/heketi created deployment.extensions/heketi created control# kubectl get pod NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 39h glusterfs-hzdll 1/1 Running 0 39h glusterfs-p8r59 1/1 Running 0 39h heketi-b8c5f6554-knp7t 1/1 Running 0 22m</code> </pre> <br><p>  Si actualmente no hay un paquete glusterfs-client instalado instalado en sus nodos de trabajo, entonces se produce un error.  Y casi hemos terminado, ahora la base de datos Heketi se almacena en el volumen GlusterFS y no se restablece cada vez que se reinicia el hogar Heketi. </p><br><p>  Para comenzar a usar el cl√∫ster GlusterFS con asignaci√≥n din√°mica de recursos, necesitamos crear una StorageClass. </p><br><p>  Primero, busquemos el punto final de almacenamiento de Gluster, que se pasar√° a StorageClass como par√°metro (heketi-storage-endpoints): </p><br><pre> <code class="plaintext hljs">control# kubectl get endpoints NAME ENDPOINTS AGE heketi 10.42.0.2:8080 2d16h ....... ... ..</code> </pre> <br><p>  Ahora crea algunos archivos: </p><br><pre> <code class="plaintext hljs">control# vi storage-class.yml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/glusterfs parameters: resturl: "http://10.42.0.2:8080" control# vi test-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: "slow" spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi</code> </pre> <br><p>  Use estos archivos para crear clase y pvc: </p><br><pre> <code class="plaintext hljs">control# kubectl create -f storage-class.yaml storageclass "slow" created control# kubectl get storageclass NAME PROVISIONER AGE slow kubernetes.io/glusterfs 2d8h control# kubectl create -f test-pvc.yaml persistentvolumeclaim "gluster1" created control# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE gluster1 Bound pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO slow 2d8h</code> </pre> <br><p>  Tambi√©n podemos ver el volumen PV: </p><br><pre> <code class="plaintext hljs">control# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-27f733cd-1c77-11e9-bb07-7efe6b0e6fa5 1Gi RWO Delete Bound default/gluster1 slow 2d8h</code> </pre> <br><p>  Ahora tenemos un volumen GlusterFS creado din√°micamente asociado con <strong>PersistentVolumeClaim</strong> , y podemos usar esta declaraci√≥n en cualquier subtrama. </p><br><p>  Cree uno simple en Nginx y pru√©belo: </p><br><pre> <code class="plaintext hljs">control# vi nginx-test.yml apiVersion: v1 kind: Pod metadata: name: nginx-pod1 labels: name: nginx-pod1 spec: containers: - name: nginx-pod1 image: gcr.io/google_containers/nginx-slim:0.8 ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1 control# kubectl create -f nginx-test.yaml pod "nginx-pod1" created</code> </pre> <br><p>  Examine en (espere unos minutos, es posible que deba descargar la imagen si a√∫n no existe): </p><br><pre> <code class="plaintext hljs">control# kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs-5dtdj 1/1 Running 0 4d10h glusterfs-hzdll 1/1 Running 0 4d10h glusterfs-p8r59 1/1 Running 0 4d10h heketi-b8c5f6554-knp7t 1/1 Running 0 2d18h nginx-pod1 1/1 Running 0 47h</code> </pre> <br><p>  Ahora vaya al contenedor y cree el archivo index.html: </p><br><pre> <code class="plaintext hljs">control# kubectl exec -ti nginx-pod1 /bin/sh # cd /usr/share/nginx/html # echo 'Hello there from GlusterFS pod !!!' &gt; index.html # ls index.html # exit</code> </pre> <br><p>  Necesitar√° encontrar la direcci√≥n IP interna del hogar y enroscarse desde cualquier nodo maestro: </p><br><pre> <code class="plaintext hljs">master1# curl 10.40.0.1 Hello there from GlusterFS pod !!!</code> </pre> <br><p>  Al hacerlo, simplemente probamos nuestro nuevo volumen persistente. </p><br><blockquote>  Algunos comandos √∫tiles para verificar el nuevo cl√∫ster de GlusterFS son: <code>heketi-cli cluster list</code> <code>heketi-cli volume list</code> .  Se pueden ejecutar en su computadora si <strong>heketi-cli est√° instalado</strong> .  En este ejemplo, este es el nodo <strong>master1</strong> . </blockquote><br><pre> <code class="plaintext hljs">master1# heketi-cli cluster list Clusters: Id:e83467d0074414e3f59d3350a93901ef [file][block] master1# heketi-cli volume list Id:6fdb7fef361c82154a94736c8f9aa53e Cluster:e83467d0074414e3f59d3350a93901ef Name:vol_6fdb7fef361c82154a94736c8f9aa53e Id:c6b69bd991b960f314f679afa4ad9644 Cluster:e83467d0074414e3f59d3350a93901ef Name:heketidbstorage</code> </pre> <br><p>  En esta etapa, configuramos con √©xito un equilibrador de carga interno con almacenamiento de archivos, y nuestro cl√∫ster ahora est√° m√°s cerca del estado operativo. </p><br><p>  En la siguiente parte del art√≠culo, nos enfocaremos en crear un sistema de monitoreo de cl√∫ster, y tambi√©n lanzaremos un proyecto de prueba para usar todos los recursos que configuramos. </p><br><p>  ¬°Mant√©ngase en contacto y todo lo mejor! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/443110/">https://habr.com/ru/post/443110/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443098/index.html">Los datos se escriben en el disco utilizando imanes y l√°seres.</a></li>
<li><a href="../443100/index.html">Contando errores en la calculadora de Windows</a></li>
<li><a href="../443102/index.html">Cambio de comportamiento como producto: ¬øpor qu√© Marie Kondo est√° recaudando una ronda de $ 40 millones con Sequoia Capital?</a></li>
<li><a href="../443104/index.html">Calcular expresiones simb√≥licas con n√∫meros triangulares difusos en python</a></li>
<li><a href="../443106/index.html">USB4 anunciado: lo que se sabe sobre el est√°ndar</a></li>
<li><a href="../443112/index.html">¬øEst√°s seguro de que puedes confiar en tu VPN?</a></li>
<li><a href="../443114/index.html">Premio DevProject: Mi discurso en DeveloperWeek 2019</a></li>
<li><a href="../443120/index.html">La Duma del Estado continuar√° la lucha contra la venta ilegal de tarjetas SIM.</a></li>
<li><a href="../443122/index.html">Fuga de 809 millones de direcciones de correo electr√≥nico del servicio Verifications.io debido a MongoDB abierto p√∫blicamente</a></li>
<li><a href="../443124/index.html">Reaccionar perezoso? Pero, ¬øy si no tienes un componente?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>