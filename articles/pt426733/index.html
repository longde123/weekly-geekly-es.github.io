<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õàÔ∏è üç¶ ‚ú¥Ô∏è O ferro n√£o falhar√°. Como eu preparo dezenas de servidores por dia para a batalha üë®üèΩ‚Äçüíª ‚è±Ô∏è ‚úãüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Verificar um servidor n√£o √© um problema. Voc√™ pega a lista de verifica√ß√£o e faz a ordem: processador, mem√≥ria, discos. Mas com cem servidores, √© impro...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>O ferro n√£o falhar√°. Como eu preparo dezenas de servidores por dia para a batalha</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/426733/"> Verificar um servidor n√£o √© um problema.  Voc√™ pega a lista de verifica√ß√£o e faz a ordem: processador, mem√≥ria, discos.  Mas com cem servidores, √© improv√°vel que esse m√©todo funcione bem.  Para excluir o fator humano, para tornar as verifica√ß√µes mais confi√°veis ‚Äã‚Äãe r√°pidas, √© necess√°rio automatizar o processo.  Quem precisa saber como fazer isso melhor do que um provedor de hospedagem?  Artyom Artemyev, no HighLoad ++ Siberia, disse que m√©todos podem ser usados, o que √© melhor executar com as m√£os e o que funciona bem para automatizar.  A seguir, √© apresentada uma vers√£o em texto do relat√≥rio com dicas que podem ser repetidas por quem trabalha com ferro e precisa verificar regularmente seu desempenho. <br><br><img src="https://habrastorage.org/webt/eg/iq/xc/egiqxczvizqa8elpiks7967fg3y.png"><br><br>  <strong>Sobre o palestrante:</strong> Artyom Artemyev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">artemirk</a> ), diretor t√©cnico de um grande provedor de hospedagem FirstVDS, trabalha com ferro. <br><a name="habracut"></a><br>  O FirstVDS possui dois data centers.  A primeira √© a deles, eles constru√≠ram seu pr√≥prio pr√©dio, trouxeram e instalaram seus racks, eles mesmos mant√™m, se preocupam com a corrente e o resfriamento do data center.  O segundo data center √© uma sala grande em um grande data center alugado, tudo fica mais f√°cil com ele, mas tamb√©m existe.  No total, s√£o 60 racks e cerca de 3000 servidores de ferro.  Havia algo para treinar e testar diferentes abordagens, o que significa que estamos aguardando recomenda√ß√µes praticamente confirmadas.  Vamos come√ßar a visualizar ou ler o relat√≥rio. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eQjNQ2RnjUY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  H√° cerca de 6 a 7 anos, percebemos que simplesmente colocar o sistema operacional no servidor n√£o √© suficiente.  O sistema operacional est√° ativado, o servidor est√° acordado e pronto para a batalha.  N√≥s o lan√ßamos em produ√ß√£o - reinicializa√ß√µes e congelamentos incompreens√≠veis come√ßam.  O que fazer, n√£o est√° claro - o processo est√° em andamento, transferir todo o calado de trabalho para um novo peda√ßo de metal √© dif√≠cil, caro e doloroso.  Para onde correr? <br><br>  Os m√©todos modernos de implanta√ß√£o nos permitem evitar isso e transportar o servidor em 5 segundos, mas nossos clientes (especialmente h√° 6 anos) simplesmente n√£o voavam nas nuvens, caminhavam no ch√£o e usavam pe√ßas comuns de ferro. <br><br><img src="https://habrastorage.org/webt/li/kc/fa/likcfazjalyfvnjz_vq17bqqjpm.png"><br><br>  Neste artigo, mostrarei quais m√©todos tentamos, quais criamos raiz, quais n√£o criaram raiz, quais s√£o bons para executar com as m√£os e como automatizar tudo isso.  Vou dar um conselho e voc√™ pode repeti-lo em sua empresa se trabalhar com ferro e tiver essa necessidade. <br><br><h2>  Qual √© o problema? <br></h2><br>  Em teoria, verificar o servidor n√£o √© um problema.  Inicialmente, tivemos um processo, como na figura abaixo.  Um homem se senta, faz uma lista de verifica√ß√£o, verifica: processador, mem√≥ria, discos, enruga a testa, toma uma decis√£o. <br><br><img src="https://habrastorage.org/webt/bq/75/ac/bq75acnj5b9-wk-mhonnk06vtbc.png"><br><br>  Em seguida, 3 servidores foram instalados por m√™s.  Mas, quando h√° mais e mais servidores, essa pessoa come√ßa a chorar e reclamar que est√° morrendo no trabalho.  Uma pessoa est√° cada vez mais enganada, porque a verifica√ß√£o se tornou uma rotina. <br><br>  <strong>Tomamos uma decis√£o: automatizamos!</strong>  Uma pessoa far√° coisas mais √∫teis. <br><br><h3>  Excurs√£o curta <br></h3><br><img src="https://habrastorage.org/webt/yp/xx/ar/ypxxarfkx9huiayenmyjsiauh2g.png"><br><br>  Esclarecerei o que quero dizer quando falo sobre o servidor hoje.  N√≥s, como todo mundo, economizamos espa√ßo em rack e usamos servidores de alta densidade.  Hoje, s√£o 2 unidades, que podem acomodar 12 n√≥s de servidores de processador √∫nico ou 4 n√≥s de servidores de processador duplo.  Ou seja, cada servidor recebe 4 discos - tudo honestamente.  Al√©m disso, existem duas fontes de alimenta√ß√£o no rack, ou seja, tudo √© redundante e todo mundo gosta. <br><br><h2>  De onde √© o ferro? <br></h2><br>  O ferro √© trazido para o nosso centro de dados pelos nossos fornecedores - geralmente Supermicro e Intel.  No datacenter, nossos operadores instalam servidores em um espa√ßo vazio no rack e conectam duas conex√µes, uma rede e energia.  Tamb√©m √© responsabilidade dos operadores configurar o BIOS no servidor.  Ou seja, conecte o teclado, monitore e configure dois par√¢metros: <code>Restore on AC/Power Loss ‚Äî [Power On]</code> , para que o servidor sempre ligue assim que a energia aparecer.  Deve funcionar sem parar.  O segundo <code>First boot device ‚Äî [PXE]</code> , ou seja, colocamos o primeiro dispositivo de rede na rede, caso contr√°rio n√£o conseguiremos acessar o servidor, pois n√£o √© fato que ele tenha discos imediatamente etc. <br><br><img src="https://habrastorage.org/webt/ab/hf/i2/abhfi2f6a412ead4dsnxyjjmhe8.png"><br><br>  Depois disso, o operador abre o painel cont√°bil dos servidores iron, no qual voc√™ precisa registrar o fato de instalar o servidor, para o qual √© indicado: <br><br><ul><li>  rack; </li><li>  adesivo </li><li>  portas de rede </li><li>  portas de energia </li><li>  n√∫mero da unidade. </li></ul><br>  Depois disso, a porta de rede em que o operador instalou o novo servidor, por motivos de seguran√ßa, vai para uma VLAN de quarentena especial, que tamb√©m trava DHCP, Pxe, TFtp.  Em seguida, o servidor carrega o nosso Linux favorito, que possui todos os utilit√°rios necess√°rios, e o processo de diagn√≥stico √© iniciado. <br><br>  Como o servidor ainda possui o primeiro dispositivo de inicializa√ß√£o na rede, para servidores que entram em produ√ß√£o, a porta muda para outra VLAN.  N√£o h√° DHCP em outra VLAN e n√£o temos medo de reinstalar acidentalmente nosso servidor de produ√ß√£o.  Para isso, temos uma VLAN separada. <br><br>  Acontece que o servidor foi instalado, est√° tudo bem, mas n√£o inicializou no sistema de diagn√≥stico.  Isso acontece, em regra, devido ao fato de que, com um atraso na troca de VLANs, nem todos os comutadores de rede trocam rapidamente VLANs, etc. <br><br><img src="https://habrastorage.org/webt/q0/qa/wl/q0qawlis6oq5tykqjri3hif8bwg.png"><br><br>  Em seguida, o operador recebe a tarefa de reiniciar o servidor com as m√£os.  Anteriormente, n√£o havia IPMI, configuramos soquetes remotos e fixamos em que porta os soquetes do servidor, puxamos o soquete pela rede e o servidor foi reinicializado. <br><br>  Mas as tomadas gerenciadas tamb√©m nem sempre funcionam bem, por isso agora gerenciamos a energia do servidor sobre IPMI.  Mas quando o servidor √© novo, o IPMI n√£o est√° configurado, ele pode ser reinicializado apenas subindo e pressionando o bot√£o.  Portanto, um homem senta, espera - a luz se acende - corre e pressiona o bot√£o.  Esse √© o trabalho dele. <br><br>  Se depois disso o servidor n√£o inicializar, ele ser√° inserido em uma lista especial para reparo.  Esta lista inclui servidores nos quais o diagn√≥stico n√£o foi iniciado ou seus resultados n√£o foram satisfat√≥rios.  Uma pessoa individual - que adora ferro - senta e desmonta todos os dias - coleta, olha, por que n√£o funciona. <br><br><h2>  CPU <br></h2><br>  Est√° tudo bem, o servidor foi iniciado, estamos come√ßando a testar.  Primeiro, testamos o processador como um dos elementos mais importantes. <br><br><img src="https://habrastorage.org/webt/8k/6m/n0/8k6mn0cwmprfgrqoanohbc1lyyo.png"><br><br>  O primeiro impulso foi usar o aplicativo do fornecedor.  Temos quase todos os processadores Intel - eles foram ao site, baixaram a Ferramenta de diagn√≥stico do processador Intel - est√° tudo bem, mostra muitas informa√ß√µes interessantes, incluindo o hor√°rio de funcionamento do servidor em horas e o gr√°fico do consumo de energia. <br><br>  Mas o problema √© que o Intel PTD funciona no Windows, do qual n√£o gostamos mais.  Para iniciar um teste, basta mover o mouse, pressionar o bot√£o "INICIAR" e o teste come√ßar√°.  O resultado √© exibido na tela, mas n√£o h√° como export√°-lo para qualquer lugar.  Isso n√£o nos conv√©m, porque o processo n√£o √© automatizado. <br><br><img src="https://habrastorage.org/webt/ax/zo/t1/axzot1gbcsfefqbkta32bixrxk4.png"><br><br>  Fomos ler os f√≥runs e descobrimos as duas maneiras mais f√°ceis. <br><br><ol><li>  <strong>O la√ßo eterno cat / dev / zero&gt; / dev / null</strong> .  Voc√™ pode verificar no topo - 100% de um n√∫cleo √© consumido.  Contamos o n√∫mero de n√∫cleos, executamos o n√∫mero necess√°rio de cat / dev / zero, multiplicado pelo n√∫mero desejado de n√∫cleos.  Tudo funciona muito bem! <br></li><li>  <strong>Utilit√°rio / compartimento / estresse</strong> .  Ela constr√≥i matrizes na mem√≥ria e come√ßa a entreg√°-las constantemente.  Est√° tudo bem tamb√©m - o processador est√° aquecendo, h√° uma carga. <br></li></ol><br>  Damos os servidores em produ√ß√£o, os usu√°rios voltam e dizem que o processador √© inst√°vel.  Verificado - o processador est√° inst√°vel.  Eles come√ßaram a investigar, pegaram um servidor que passou nas verifica√ß√µes, mas ele travou em uma batalha, ativou o kernel de depura√ß√£o no Linux e coletou o Core dump.  O servidor antes da reinicializa√ß√£o libera para o arquivo tudo o que estava na mem√≥ria antes da falha. <br><br><img src="https://habrastorage.org/webt/i1/iu/z0/i1iuz0zoafq9lm8aaimgyqzyiic.png"><br><br>  V√°rias otimiza√ß√µes s√£o incorporadas aos processadores para opera√ß√µes frequentes.  Podemos ver sinalizadores refletindo quais otimiza√ß√µes o processador suporta, por exemplo, otimiza√ß√µes para trabalhar com n√∫meros de ponto flutuante, otimiza√ß√µes de multim√≠dia etc.  Mas nosso / bin / stress e o ciclo eterno apenas queimam o processador em uma opera√ß√£o e n√£o usam recursos adicionais.  A investiga√ß√£o mostrou que a CPU falha ao tentar usar a funcionalidade de um dos sinalizadores internos. <br><br>  O primeiro impulso foi deixar / bin / stress - deixar o processador aquecer.  Ent√£o, em um ciclo, percorremos todas as bandeiras e as puxamos.  Enquanto pensamos em como implementar isso, quais comandos chamar para chamar as fun√ß√µes de cada sinalizador, lemos os f√≥runs. <br><br>  No f√≥rum de overclockers, deparamos com um projeto interessante para procurar n√∫meros primos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Great Internet Mersenne Prime Search</a> .  Os cientistas criaram uma rede distribu√≠da √† qual qualquer pessoa pode se conectar e ajudar a encontrar um n√∫mero primo.  Os cientistas n√£o acreditam em ningu√©m, ent√£o o programa funciona de maneira inteligente: primeiro voc√™ o executa, calcula os n√∫meros primos que j√° conhece e compara o resultado com o que sabe.  Se o resultado n√£o corresponder, o processador est√° com defeito.  N√≥s realmente gostamos desta propriedade: com qualquer absurdo, √© prov√°vel que caia. <br><br>  Al√©m disso, o objetivo do projeto √© encontrar o maior n√∫mero poss√≠vel de n√∫meros primos; portanto, o programa √© constantemente otimizado para as propriedades dos novos processadores, resultando em muitas bandeiras. <br><br>  Mprime n√£o tem limite de tempo; se n√£o for parado, funciona para sempre.  Executamos por 30 minutos. <br><br><pre> <code class="hljs powershell">/usr/bin/timeout <span class="hljs-number"><span class="hljs-number">30</span></span>m /opt/mprime <span class="hljs-literal"><span class="hljs-literal">-t</span></span> /bin/grep <span class="hljs-literal"><span class="hljs-literal">-i</span></span> error /root/result.txt</code> </pre><br>  Ap√≥s concluir o trabalho, verificamos se n√£o h√° erros em result.txt e observamos os logs do kernel, em particular no arquivo / proc / kmsg, que procuramos por erros. <br><br><h3>  Outra excurs√£o <br></h3><br><img src="https://habrastorage.org/webt/es/ic/kj/esickjc00tooyxznjt_6lyumimg.png"><br><br>  Em 3 de janeiro de 2018, eles encontraram o 50¬∫ n√∫mero primo de Mersenne (2 <sup>p</sup> -1).  Desse n√∫mero, apenas 23 milh√µes de d√≠gitos.  Voc√™ pode baix√°-lo para visualiz√°-lo - <a href="">este</a> √© <a href="">um arquivo zip de 12 Mb</a> . <br><br>  Por que precisamos de n√∫meros primos?  Primeiro, qualquer criptografia RSA usa n√∫meros primos.  Quanto mais primos soubermos, mais confi√°vel ser√° sua chave SSH.  Em segundo lugar, os cientistas testam suas hip√≥teses e teoremas matem√°ticos, e n√£o nos importamos em ajudar os cientistas - n√£o nos custa nada.  Acontece que a hist√≥ria ganha-ganha. <br><br><img src="https://habrastorage.org/webt/v0/nd/cl/v0ndcljwlkwops-dtp9nwk9bsru.png"><br><br>  Ent√£o, o processador est√° funcionando, est√° tudo bem.  Resta descobrir que tipo de processador √© esse.  Usamos o processador dmidecode -t e vemos todos os slots que est√£o na placa-m√£e e quais processadores est√£o nesses slots.  Esta informa√ß√£o entra no nosso sistema cont√°bil, n√≥s a interpretaremos mais tarde. <br><br><h3>  Catch <br></h3><br>  Assim, surpreendentemente, pernas quebradas podem ser encontradas.  / bin / stress e o ciclo perp√©tuo funcionaram, e o Mprime caiu.  Eles dirigiram por um longo tempo, procuraram, descobriram - o resultado na imagem abaixo - tudo est√° claro aqui. <br><br><img src="https://habrastorage.org/webt/13/ae/gc/13aegcfaw9ozlrzzzgog3povm5w.jpeg"><br><br>  Esse processador simplesmente n√£o foi iniciado.  O operador era muito forte, pegou o processador errado - mas conseguiu entregar. <br><br><img src="https://habrastorage.org/webt/gj/xr/_w/gjxr_wu8lxf_cgsf4ehxs84-yb8.jpeg"><br><br>  Outro caso bonito.  A linha preta na foto abaixo √© a ventoinha, a seta mostra como o ar sopra.  Vemos: o radiador fica do outro lado da corrente.  Claro, tudo superaqueceu e desligou. <br><br><img src="https://habrastorage.org/webt/oh/-c/hp/oh-chp3arnnkslirn8elylfcfww.jpeg"><br><br><h2>  A mem√≥ria <br></h2><br>  Com a mem√≥ria, tudo √© bem simples.  S√£o c√©lulas nas quais escrevemos informa√ß√µes e depois de um tempo as lemos novamente.  Se permanecer o mesmo que anotamos, essa c√©lula estar√° funcionando. <br><br><img src="https://habrastorage.org/webt/91/gl/_s/91gl_slwgtws8webfnnjxfajui8.png"><br><br>  Todo mundo conhece o bom programa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Memtest86 +</a> , diretamente cl√°ssico, que √© executado a partir de qualquer m√≠dia, pela rede ou at√© mesmo de um disquete.  √â feito para verificar o maior n√∫mero poss√≠vel de c√©lulas de mem√≥ria.  Qualquer c√©lula ocupada n√£o pode mais ser verificada.  Portanto, o memtest86 + tem um tamanho m√≠nimo para n√£o ocupar mem√≥ria.  Infelizmente, o <strong>memtest86 + exibe apenas suas estat√≠sticas na tela</strong> .  Tentamos expandi-lo de alguma forma, mas tudo se resumiu ao fato de que dentro do programa n√£o havia nem mesmo uma pilha de rede.  Para expandi-lo, seria necess√°rio trazer o kernel do Linux e tudo mais. <br><br>  Existe uma vers√£o paga deste programa que j√° sabe como soltar informa√ß√µes no disco.  Mas nossos servidores nem sempre t√™m um disco e nem sempre existe um sistema de arquivos nesses discos.  Mas a unidade de rede, como j√° descobrimos, n√£o pode ser conectada. <br><br>  Come√ßamos a cavar ainda mais e encontramos um programa similar do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Memtester</a> .  Este programa funciona no n√≠vel do SO do Linux.  O maior ponto negativo √© que o sistema operacional e o Memtester ocupam algumas c√©lulas de mem√≥ria e essas c√©lulas n√£o ser√£o verificadas. <br><br>  O Memtester √© iniciado com o comando: memtester `cat / proc / meminfo | grep MemFree |  awk '{print $ 2-1024}' 'k 5 <br><br>  Aqui, transferimos a quantidade de mem√≥ria livre menos 1 MB.  Isso √© feito porque, caso contr√°rio, o Memtester ocupa toda a mem√≥ria e o assassino mata-a.  Realizamos este teste por 5 ciclos, na sa√≠da temos uma placa com OK ou com falha. <br><br><table width="300"><tbody><tr><td>  Endere√ßo bloqueado </td><td>  ok </td></tr><tr><td>  Valor aleat√≥rio </td><td>  ok </td></tr><tr><td>  Compare o XOR </td><td>  ok </td></tr><tr><td>  Comparar SUB </td><td>  ok </td></tr><tr><td>  Comparar MUL </td><td>  ok </td></tr><tr><td>  Compare DIV </td><td>  ok </td></tr><tr><td>  Compare OU </td><td>  ok </td></tr><tr><td>  Compare E </td><td>  ok </td></tr></tbody></table><br><br>  Salvamos o resultado final e analisamos mais as falhas. <br><br><img src="https://habrastorage.org/webt/ah/55/kt/ah55kt9fxuqwkmqsz8-dozgunic.png"><br><br>  Para entender a extens√£o do problema - nosso menor servidor possui 32 GB de mem√≥ria, nossa imagem do Linux com Memtester ocupa 60 MB, <strong>n√£o verificamos 2% da mem√≥ria</strong> .  Mas, de acordo com as estat√≠sticas dos √∫ltimos 6 anos, n√£o houve tal coisa que a mem√≥ria francamente batida entrou em produ√ß√£o.  Este √© o compromisso com o qual concordamos, e que √© caro para n√≥s consertar, e vivemos com ele. <br><br>  Ao longo do caminho, tamb√©m coletamos a mem√≥ria dmidecode -t, que fornece todos os bancos de mem√≥ria que temos na placa-m√£e (geralmente at√© 24 pe√ßas) e quais dados est√£o em cada banco.  Esta informa√ß√£o √© √∫til se quisermos atualizar o servidor - saberemos onde adicionar o qu√™, quantas tiras levar e para qual servidor ir. <br><br><h2>  Dispositivos de armazenamento <br></h2><br>  H√° 6 anos, todos os discos estavam com panquecas que giravam.  Uma hist√≥ria separada era reunir apenas uma lista de todos os discos.  Havia v√°rias abordagens diferentes, pois n√£o se acreditava que voc√™ pudesse apenas olhar ls / dev / sd.  Mas no final, paramos de olhar para ls / dev / sd * e ls / dev / cciss / c0d *.  No primeiro caso, √© um dispositivo SATA, no segundo - SCSI e SAS. <br><br><img src="https://habrastorage.org/webt/e7/gm/br/e7gmbraujo_ko4qmfanfwnru71y.png"><br><br>  Literalmente este ano, eles come√ßaram a vender discos nvme e adicionaram a lista nvme aqui. <br><br>  Depois que a lista de discos √© compilada, tentamos ler 0 bytes para entender que este √© um dispositivo de bloco e est√° tudo bem.  Se voc√™ n√£o conseguiu ler, acreditamos que isso √© algum tipo de fantasma, e n√£o temos e nunca tivemos esse disco. <br><br>  A primeira abordagem para verificar discos foi a √≥bvia: ‚ÄúVamos gravar dados aleat√≥rios no disco e ver a velocidade‚Äù - <code>dd -o nocache -o direct if=/dev/urandom of=${disk}</code> .  Como regra, os discos de panqueca fornecem 130-150 Mb / s.  Apertamos os olhos e decidimos por n√≥s mesmos que 90 MB / s √© a figura ap√≥s a qual existem discos que podem ser reparados, e todos os menores s√£o defeituosos. <br><br>  Mas, novamente, os usu√°rios come√ßaram a retornar e dizer que as unidades s√£o ruins.  Aconteceu que a f√≠sica insidiosa estava brincando conosco novamente. <br><br><img src="https://habrastorage.org/webt/m_/uh/tf/m_uhtfarwgyskpslnxxkufxynty.png"><br><br>  Existe velocidade angular e, como regra, quando voc√™ executa -dd, ele grava pr√≥ximo ao eixo.  Se, por algum motivo, a velocidade do eixo diminuiu, isso ser√° menos percept√≠vel do que se voc√™ escrever da borda do disco. <br><br>  Eu tive que mudar o princ√≠pio da verifica√ß√£o.  Agora, verificamos tr√™s locais: perto do eixo, no meio e fora.  Provavelmente, s√≥ pode ser verificado de fora, mas √© assim que historicamente aconteceu.  E o que funciona, n√£o toque. <br><br>  Voc√™ pode usar o <strong>smartctl</strong> para perguntar ao disco como est√°.  Acreditamos que uma boa unidade: <br><br><ul><li>  N√£o h√° setores realocados ( <strong>contagem de setores realocados = 0)</strong> , ou seja, todos os setores que deixaram o trabalho de f√°brica. </li><li>  N√£o <strong>usamos discos com mais de 4 anos</strong> , embora estejam funcionando bem.  Antes de introduzirmos essa pr√°tica, t√≠nhamos discos por 7 anos.  Agora acreditamos que, ap√≥s 4 anos, o disco valeu a pena e n√£o estamos prontos para aceitar o risco de desgaste. </li><li>  N√£o h√° setores que ser√£o realocados ( <strong>Current_Pending_Sector = 0</strong> ). </li><li>  <strong>Contagem de erros do UltraDMA CRC = 0</strong> - s√£o erros no cabo SATA.  Se houver um erro, voc√™ s√≥ precisa trocar o fio, n√£o precisa trocar o disco. </li></ul><br><img src="https://habrastorage.org/webt/01/jy/48/01jy48l5jyw-x1yx7wse-yawhdq.png"><br><br>  Os SSDs distribu√≠dos geralmente s√£o unidades excelentes, funcionam rapidamente, n√£o fazem barulho, n√£o esquentam.  Acreditamos que um bom SSD tem uma velocidade de grava√ß√£o superior a 200 MB / s.  Nossos clientes adoram pre√ßos baixos, e os modelos de servidor que emitem 320-350 MB / s nem sempre chegam at√© n√≥s. <br><br>  Para SSDs, tamb√©m procuramos smartctl.  O mesmo Realocado, Power_On_Hours, Current_Pending_Sector.  Todos os SSDs podem exibir o grau de desgaste, mostra o par√¢metro Media_Wearout_Indicator.  Limpamos os discos at√© 5% da vida √∫til e s√≥ ent√£o os tiramos.  Esses discos √†s vezes encontram uma segunda vida nas necessidades pessoais dos funcion√°rios.  Por exemplo, descobri recentemente que em 2 anos esse disco se desgastou em mais 1% no laptop de um funcion√°rio, embora em nosso pa√≠s o cache do SSD se esgote em 95% em cerca de 10 meses. <br><br>  Mas o problema √© que nem todos os fabricantes de disco concordaram com os nomes dos par√¢metros, e este Media_Wearout_Indicator, por exemplo, √© chamado Percent_Lifetime_Used para Toshiba, outra contagem de n√≠vel de desgaste, porcentagem de tempo restante para outros fabricantes ou simplesmente. <br><br>  Crucial n√£o tem essa op√ß√£o.  Depois, consideramos a quantidade de reescritas do disco - ‚Äúbyte writed‚Äù - quantos bytes j√° gravamos nesse disco.  Al√©m disso, de acordo com a especifica√ß√£o, estamos tentando descobrir quantas reescritas esse disco √© calculado pelo fabricante.  Pela matem√°tica elementar, determinamos quanto mais ele viver√°.  Se √© hora de mudar - mude. <br><br><h2>  RAID <br></h2><br>  N√£o sei por que, no mundo moderno, nossos clientes ainda querem RAIDs.  As pessoas compram RAID, colocam 4 SSDs l√°, que s√£o muito mais r√°pidos que este RAID (6 Gb).  Eles t√™m algum tipo de instru√ß√£o e a coletam.  Eu acho que isso √© uma coisa quase desnecess√°ria. <br><br>  Havia 3 fabricantes: Adaptec;  3ware;  Intel  T√≠nhamos 3 utilit√°rios, nos preocupamos, mas executamos diagn√≥sticos para todos.  Agora, a LSI comprou a todos - resta apenas um utilit√°rio. <br><br>  Quando nosso sistema de diagn√≥stico v√™ o RAID, ele analisa o volume l√≥gico em discos separados, para que voc√™ possa medir a velocidade de cada disco e ler seu Smart.  Depois disso, resta ao RAID verificar a bateria.  Quem n√£o sabe - h√° baterias suficientes no RAID para girar todos os discos por mais 2 horas.  Ou seja, voc√™ desliga o servidor, retira-o e ele gira o disco por mais 2 horas para concluir todas as grava√ß√µes. <br><br><h2>  Rede <br></h2><br>  Com a rede, tudo √© bem simples - deve haver menos de 300 Mbit dentro do data center.  Se menos, voc√™ precisa corrigi-lo.  Tamb√©m analisamos os erros na interface.  <strong>Os erros na interface de rede n√£o devem ser</strong> , e se forem, tudo est√° ruim. <br><br><img src="https://habrastorage.org/webt/ad/m2/yw/adm2yw4vmjpztcicdovupo84l20.png"><br><br>  Estamos tentando atualizar o firmware do BIOS e IPMI ao longo do caminho.  Aconteceu que n√£o gostamos de todos os BIOS.  Ainda temos BIOS que n√£o sabem como UEFI e outros recursos que usamos.  Tentamos atualiz√°-lo automaticamente, mas isso nem sempre funciona, nem tudo √© muito simples l√°.  Se n√£o funcionar, a pessoa vai e atualiza com as m√£os. <br><br>  N√≥s n√£o fornecemos o IPMI Supermicro para o mundo, temos em endere√ßos cinza atrav√©s do OpenVPN.  No entanto, temos medo de que um dia outra vulnerabilidade apare√ßa e sofreremos.  Portanto, tentamos manter o firmware IPMI sempre o √∫ltimo.  Caso contr√°rio, atualize. <br><br>  De uma coisa estranha, descobriu-se recentemente que a Intel em placas de rede de 10 e 40 gigabit n√£o inclui inicializa√ß√£o PXE.  Acontece que, se o servidor estiver em um rack no qual h√° apenas uma placa de 40 gigabit, √© imposs√≠vel inicializar pela rede, porque voc√™ precisa inicializar em uma placa de gigabit.  Flash separadamente as placas de rede em 40G para que elas tenham PXE e possam continuar funcionando. <br><br>  <strong>Depois que tudo √© verificado, o servidor entra imediatamente √† venda</strong> .  Seu pre√ßo √© calculado, no qual √© colocado no site e vendido. <br><br><img src="https://habrastorage.org/webt/by/ca/ah/bycaah2szao5f0-lcrkk5zcwzyo.png"><br><br>  No total, realizamos aproximadamente 350 verifica√ß√µes por m√™s, 69% dos servidores podem ser reparados, 31% n√£o podem ser reparados.  Isso se deve ao fato de termos uma hist√≥ria rica, alguns servidores permanecem em p√© h√° 10 anos.  A maioria dos servidores que n√£o passaram no teste, apenas jogamos. <br><br><blockquote>  Para os curiosos: temos 3 clientes que ainda vivem no Pentium IV e n√£o querem sair de lugar nenhum.  Eles t√™m 512 MB de RAM. <br></blockquote><br>  O futuro chegou!  Se eu cercasse esse sistema hoje ... <br><br><img src="https://habrastorage.org/webt/4v/bb/e_/4vbbe_vodbx8hzm_zk-nf2oejf4.png"><br><br>  Um utilit√°rio maravilhoso, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hardware Lister</a> (lshw), foi lan√ßado, o qual pode se comunicar com o kernel, exibindo lindamente que tipo de hardware existe no kernel, o que o kernel pode detectar.  Nem todas essas dan√ßas s√£o necess√°rias.  Se voc√™ repetir - eu recomendo fortemente que voc√™ olhe para este utilit√°rio e use-o.  Tudo se tornar√° muito mais simples. <br><br><h1>  Resumo: <br></h1><br><ul><li>  O compromisso n√£o √© ruim, √© apenas uma quest√£o de pre√ßo.  Se a solu√ß√£o for muito cara, √© necess√°rio procurar um n√≠vel em que a confiabilidade e o pre√ßo sejam aceit√°veis. </li><li>  √Äs vezes, programas n√£o essenciais s√£o interessantes para testes.  Resta apenas encontr√°-los. </li><li>  Teste tudo o que voc√™ alcan√ßa! </li></ul><br><blockquote>  O pr√≥ximo grande <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">HighLoad ++</a> j√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">est√°</a> nos dias 8 e 9 de novembro em Moscou.  O programa inclui especialistas famosos e novos nomes, tarefas tradicionais e novas.  Na se√ß√£o DevOps, por exemplo, o seguinte j√° √© aceito: <br><br><ul><li>  David O'Brien (Xirus), que discutir√° o eterno - ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">M√©tricas!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">M√©tricas!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">M√©tricas!</a>  " <br></li><li>  Vladimir Kolobaev (Avito) com um apelo de relat√≥rio ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitorando - para desenvolvedores!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tecnologia para a comunidade!</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lucro - para todos!</a>  " <br></li><li>  Elena Grahovac (N26) apresentar√° " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores pr√°ticas para servi√ßos de nuvem nativa</a> ". <br></li></ul><br>  Estude a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">lista de</a> relat√≥rios e corra para participar.  Ou <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">assine</a> nossa newsletter e voc√™ receber√° an√°lises regulares de relat√≥rios, relat√≥rios de novos artigos e v√≠deos. <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt426733/">https://habr.com/ru/post/pt426733/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt426723/index.html">Microsoft e parceiros esperam criar uma c√°psula do tempo na lua</a></li>
<li><a href="../pt426725/index.html">Como fazer as coisas quando voc√™ n√£o deseja faz√™-las</a></li>
<li><a href="../pt426727/index.html">EME? Cdm? DRM? CENC? IDK! O que voc√™ precisa para criar seu pr√≥prio player de v√≠deo em um navegador</a></li>
<li><a href="../pt426729/index.html">Escola de Magia TypeScript: extens√£o de tipos e gen√©ricos</a></li>
<li><a href="../pt426731/index.html">CSS: recursos interessantes do raio da borda</a></li>
<li><a href="../pt426735/index.html">Bem-vindo ao JETHACK Hackathon</a></li>
<li><a href="../pt426737/index.html">Brevemente sobre a arquitetura dos processadores neurom√≥rficos: uma vis√£o interna</a></li>
<li><a href="../pt426739/index.html">Arquivos de proxy da AWS S3 usando nginx</a></li>
<li><a href="../pt426741/index.html">Um memorando para quem planeja recrutar estagi√°rios pela primeira vez</a></li>
<li><a href="../pt426743/index.html">As maneiras de usar o blockchain mudaram para outro lugar. Sony anuncia sistema DRM baseado em blockchain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>