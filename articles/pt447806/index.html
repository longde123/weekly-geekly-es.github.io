<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê´ üßê üîΩ Acelerando o desempenho de redes neurais usando hash üíÉüèº üó£Ô∏è üßìüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O setor se concentrou em acelerar a multiplica√ß√£o de matrizes, mas a melhoria do algoritmo de pesquisa pode levar a um aumento mais s√©rio no desempenh...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Acelerando o desempenho de redes neurais usando hash</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/447806/"><h3>  O setor se concentrou em acelerar a multiplica√ß√£o de matrizes, mas a melhoria do algoritmo de pesquisa pode levar a um aumento mais s√©rio no desempenho </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/8e7/9a0/8e3/8e79a08e31fdeadd8258ab001cc23ec5.jpg"><br><br>  Nos √∫ltimos anos, a ind√∫stria de computadores tem estado ocupada tentando acelerar os c√°lculos necess√°rios para redes neurais artificiais - tanto para treinamento quanto para tirar conclus√µes de seu trabalho.  Em particular, foi feito um grande esfor√ßo no desenvolvimento de ferro especial, no qual esses c√°lculos podem ser realizados.  O Google desenvolveu a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tensor Processing Unit</a> , ou TPU, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">introduzida</a> pela primeira <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vez</a> ao p√∫blico em 2016.  Mais tarde, a Nvidia introduziu a Unidade de processamento gr√°fico <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">V100</a> , descrevendo-a como um chip projetado especificamente para treinamento e uso de IA, bem como para outras necessidades de computa√ß√£o de alto desempenho.  Cheio de outras startups, concentrando-se em outros tipos de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aceleradores</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hardware</a> . <br><a name="habracut"></a><br>  Talvez todos cometam um grande erro. <br><br>  Essa ideia foi expressa no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trabalho</a> , que apareceu em meados de mar√ßo no site arXiv.  Nela, seus autores, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Beidi Chen</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tarun Medini</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Anshumali Srivastava,</a> da Rice University, argumentam que talvez o equipamento especial desenvolvido para a opera√ß√£o de redes neurais esteja sendo otimizado para o algoritmo errado. <br><br>  O fato √© que o trabalho das redes neurais geralmente depende da rapidez com que o equipamento pode executar a multiplica√ß√£o de matrizes usadas para determinar os par√¢metros de sa√≠da de cada n√™utron artificial - sua ‚Äúativa√ß√£o‚Äù - para um determinado conjunto de valores de entrada.  As matrizes s√£o usadas porque cada valor de entrada para um neur√¥nio √© multiplicado pelo par√¢metro de peso correspondente, e ent√£o todos s√£o somados - e essa multiplica√ß√£o com adi√ß√£o √© a opera√ß√£o b√°sica da multiplica√ß√£o de matrizes. <br><br>  Pesquisadores da Rice University, como alguns outros cientistas, perceberam que a ativa√ß√£o de muitos neur√¥nios em uma camada espec√≠fica da rede neural √© muito pequena e n√£o afeta o valor de sa√≠da calculado pelas camadas subseq√ºentes.  Portanto, se voc√™ souber o que s√£o esses neur√¥nios, poder√° simplesmente ignor√°-los. <br><br>  Pode parecer que a √∫nica maneira de descobrir quais neur√¥nios em uma camada n√£o est√£o ativados √© primeiro executar todas as opera√ß√µes de multiplica√ß√£o de matrizes para essa camada.  Mas os pesquisadores perceberam que voc√™ pode realmente decidir dessa maneira mais eficiente se observar o problema de um √¢ngulo diferente.  "Abordamos esse problema como uma solu√ß√£o para o problema de pesquisa", diz Srivastava. <br><br>  Ou seja, em vez de calcular multiplica√ß√µes de matrizes e observar quais neur√¥nios foram ativados para uma determinada entrada, √© poss√≠vel ver apenas que tipo de neur√¥nios est√£o no banco de dados.  A vantagem dessa abordagem no problema √© que voc√™ pode usar uma estrat√©gia generalizada que h√° muito tempo foi aprimorada por cientistas da computa√ß√£o para acelerar a pesquisa de dados no banco de dados: hash. <br><br>  O hash permite verificar rapidamente se existe um valor na tabela do banco de dados, sem precisar passar por cada linha em uma linha.  Voc√™ usa um hash, facilmente calculado aplicando uma fun√ß√£o de hash ao valor desejado, indicando onde esse valor deve ser armazenado no banco de dados.  Depois, voc√™ pode verificar apenas um local para descobrir se esse valor est√° armazenado l√°. <br><br>  Os pesquisadores fizeram algo semelhante para c√°lculos relacionados a redes neurais.  O exemplo a seguir ajudar√° a ilustrar sua abordagem: <br><br>  Suponha que tenhamos criado uma rede neural que reconhece a entrada manuscrita de n√∫meros.  Suponha que a entrada seja pixels cinza em uma matriz de 16x16, ou seja, um total de 256 n√∫meros.  N√≥s alimentamos esses dados em uma camada oculta de 512 neur√¥nios, cujos resultados de ativa√ß√£o s√£o alimentados pela camada de sa√≠da de 10 neur√¥nios, um para cada um dos n√∫meros poss√≠veis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e73/dec/941/e73dec9413ed37b2235bbd1ee8369ca6.jpg"><br><br>  <i>Tabelas de redes: antes de calcular a ativa√ß√£o de neur√¥nios em camadas ocultas, usamos hashes para nos ajudar a determinar quais neur√¥nios ser√£o ativados.</i>  <i>Aqui, o hash dos valores de entrada H1 √© usado para procurar os neur√¥nios correspondentes na primeira camada oculta - nesse caso, ser√£o os neur√¥nios 2 e 4. O segundo hash H2 mostra quais neur√¥nios da segunda camada oculta contribuir√£o.</i>  <i>Essa estrat√©gia reduz o n√∫mero de ativa√ß√µes que precisam ser calculadas.</i> <br><br>  √â muito dif√≠cil treinar essa rede, mas por enquanto vamos omitir esse momento e imaginar que j√° ajustamos todos os pesos de cada neur√¥nio para que a rede neural reconhe√ßa n√∫meros manuscritos perfeitamente.  Quando um n√∫mero leg√≠vel escrito chega √† sua entrada, a ativa√ß√£o de um dos neur√¥nios de sa√≠da (correspondente a esse n√∫mero) ser√° pr√≥xima de 1. A ativa√ß√£o dos outros nove ser√° pr√≥xima de 0. Classicamente, a opera√ß√£o dessa rede requer uma multiplica√ß√£o de matrizes para cada um dos 512 neur√¥nios ocultos, e mais um para cada fim de semana - o que nos d√° muitas multiplica√ß√µes. <br><br>  Os pesquisadores adotam uma abordagem diferente.  O primeiro passo √© misturar os pesos de cada um dos 512 neur√¥nios na camada oculta usando o "hash sens√≠vel √† localidade", uma das propriedades das quais dados de entrada semelhantes fornecem valores de hash semelhantes.  Voc√™ pode agrupar neur√¥nios com hashes semelhantes, o que significa que esses neur√¥nios t√™m conjuntos de pesos semelhantes.  Cada grupo pode ser armazenado em um banco de dados e determinado pelo hash dos valores de entrada que levar√£o √† ativa√ß√£o desse grupo de neur√¥nios. <br><br>  Depois de todo esse hash, √© f√°cil determinar quais neur√¥nios ocultos ser√£o ativados por alguma nova entrada.  Voc√™ precisa executar 256 valores de entrada por meio de fun√ß√µes hash facilmente calcul√°veis ‚Äã‚Äãe usar o resultado para pesquisar no banco de dados os neur√¥nios que ser√£o ativados.  Dessa forma, voc√™ ter√° que calcular os valores de ativa√ß√£o para apenas alguns neur√¥nios importantes.  N√£o √© necess√°rio calcular a ativa√ß√£o de todos os outros neur√¥nios da camada apenas para descobrir que eles n√£o contribuem para o resultado. <br><br>  A entrada dessa rede neural de dados pode ser representada como a execu√ß√£o de uma consulta de pesquisa em um banco de dados que pede para encontrar todos os neur√¥nios que seriam ativados por contagem direta.  Voc√™ obt√©m a resposta rapidamente porque usa hashes para pesquisar.  E ent√£o voc√™ pode simplesmente calcular a ativa√ß√£o de um pequeno n√∫mero de neur√¥nios que realmente importam. <br><br>  Os pesquisadores usaram essa t√©cnica, que eles chamaram de SLIDE (Sub-Linear Deep learning Engine), para treinar uma rede neural - para um processo que possui mais solicita√ß√µes computacionais do que para o objetivo a que se destina.  Eles ent√£o compararam o desempenho do algoritmo de aprendizado com uma abordagem mais tradicional usando uma GPU poderosa - especificamente, a GPU Nvidia V100.  Como resultado, eles obtiveram algo surpreendente: "Nossos resultados mostram que, em m√©dia, a tecnologia SLIDE da CPU pode executar ordens de magnitude mais r√°pidas que a melhor alternativa poss√≠vel, implementada nos melhores equipamentos e com qualquer precis√£o". <br><br>  √â muito cedo para tirar conclus√µes sobre se esses resultados (que os especialistas ainda n√£o avaliaram) resistir√£o aos testes e se for√ßar√£o os fabricantes de chips a dar uma olhada diferente no desenvolvimento de equipamentos especiais para aprendizado profundo.  Mas o trabalho enfatiza definitivamente o perigo do arrastamento de um certo tipo de ferro nos casos em que existe a possibilidade de um novo e melhor algoritmo para a opera√ß√£o de redes neurais. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt447806/">https://habr.com/ru/post/pt447806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt447792/index.html">Padr√µes sombrios e a lei: como os reguladores dos EUA tentam controlar a mec√¢nica dos produtos e reduzir a influ√™ncia de empresas de tecnologia</a></li>
<li><a href="../pt447794/index.html">Sobre coisas simples, complicadas. Uma carta de um qu√≠mico para uma impressora 3D. Solventes para pl√°sticos e prote√ß√£o contra eles</a></li>
<li><a href="../pt447798/index.html">Como gerar um enorme gr√°fico financeiro com padr√µes de lavagem de dinheiro?</a></li>
<li><a href="../pt447802/index.html">Isabella 2</a></li>
<li><a href="../pt447804/index.html">Dwarf Fortress abandona gr√°ficos de texto, mas n√£o sua ess√™ncia</a></li>
<li><a href="../pt447808/index.html">Aprendendo a escrever contratos inteligentes do Waves no RIDE e RIDE4DAPPS. Parte 2 (DAO - Organiza√ß√£o Aut√¥noma Descentralizada)</a></li>
<li><a href="../pt447810/index.html">O Analytics for Azure DevOps Services agora est√° dispon√≠vel ao p√∫blico</a></li>
<li><a href="../pt447812/index.html">Como implementamos a entrega cont√≠nua de atualiza√ß√µes na plataforma do cliente</a></li>
<li><a href="../pt447814/index.html">Onde e como abrir um centro de desenvolvimento?</a></li>
<li><a href="../pt447816/index.html">Um pouco de magia de modelo C ++ e CRTP para controlar a corre√ß√£o das a√ß√µes do programador em tempo de compila√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>