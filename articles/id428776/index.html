<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘„ â™ï¸ â˜ºï¸ Sebuah realisasi baru rasa ingin tahu pada AI. Pelatihan dengan hadiah itu tergantung pada sulitnya memprediksi hasilnya ğŸ‘©ğŸ¿â€ğŸ¤â€ğŸ‘¨ğŸ¼ ğŸ¥’ ğŸŒ†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kemajuan dalam permainan "Pembalasan Montezuma" dianggap oleh banyak orang sebagai sinonim untuk prestasi dalam studi lingkungan yang tidak dikenal 

...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sebuah realisasi baru rasa ingin tahu pada AI. Pelatihan dengan hadiah itu tergantung pada sulitnya memprediksi hasilnya</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428776/"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">Kemajuan dalam permainan "Pembalasan Montezuma" dianggap oleh banyak orang sebagai sinonim untuk prestasi dalam studi lingkungan yang tidak dikenal</font></i> <br><br>  Kami telah mengembangkan metode Random Network Distillation (RND) berbasis prediksi yang mendorong agen pembelajaran yang diperkuat untuk menjelajahi lingkungan melalui rasa ingin tahu.  Metode ini untuk pertama kalinya melebihi hasil rata-rata manusia dalam permainan komputer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">"Montezuma's Revenge"</a> (kecuali untuk <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">aplikasi</a> anonim di ICLR, di mana hasilnya lebih buruk daripada kita).  <b>RND menunjukkan efisiensi ultra modern, secara berkala menemukan semua 24 kamar dan melewati tingkat pertama tanpa demonstrasi awal dan tanpa akses ke keadaan dasar permainan.</b> <br><a name="habracut"></a><br>  Metode RND merangsang transisi agen ke keadaan asing dengan mengukur kompleksitas memprediksi hasil superimposis jaringan saraf acak acak pada data keadaan.  Jika kondisinya tidak dikenal, maka hasil akhirnya sulit untuk diprediksi, yang berarti bahwa imbalannya tinggi.  Metode ini dapat diterapkan pada algoritma pembelajaran penguatan apa pun, mudah diterapkan dan efektif untuk penskalaan.  Di bawah ini adalah tautan ke implementasi RND, yang mereproduksi hasil dari artikel kami. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Teks artikel ilmiah</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode</a> </blockquote><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Hasil dalam Pembalasan Montezuma </h1><br>  Untuk mencapai tujuan yang diinginkan, agen pertama-tama harus mempelajari tindakan apa yang mungkin dilakukan di lingkungan dan apa yang merupakan kemajuan menuju tujuan.  Banyak sinyal hadiah dalam permainan menyediakan kurikulum, sehingga bahkan strategi penelitian sederhana cukup untuk mencapai tujuan.  Dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">karya awal dengan presentasi DQN</a> , Montezuma's Revenge adalah <b>satu</b> - <b>satunya permainan di mana DQN menunjukkan hasil 0% dari rata-rata skor manusia (4700)</b> .  Strategi intelijen sederhana tidak mungkin mengumpulkan imbalan apa pun dan menemukan tidak lebih dari beberapa kamar di level tersebut.  Sejak saat itu, kemajuan dalam permainan Montezuma's Revenge telah dianggap oleh banyak orang sebagai identik dengan kemajuan dalam studi lingkungan yang tidak dikenal. <br><br>  Kemajuan yang signifikan dicapai pada tahun <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">2016</a> dengan menggabungkan DQN dengan bonus di atas meja, di mana agen berhasil menemukan 15 kamar dan mendapatkan skor tertinggi 6.600 dengan rata-rata sekitar 3.700. Sejak itu, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">peningkatan yang</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">signifikan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dalam</a> hasil hanya dicapai melalui <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">demonstrasi</a> dari orang-orang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ahli</a> atau dengan mengakses status dasar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">emulator</a> . <br><br>  Kami melakukan percobaan RND skala besar dengan 1024 pekerja, mendapatkan <b>hasil rata</b> - <b>rata 10.000 lebih dari 9 awal</b> dan <b>hasil rata-rata terbaik dari 14.500</b> .  Dalam setiap kasus, agen menemukan 20-22 kamar.  Selain itu, dalam satu peluncuran yang lebih kecil, tetapi lebih lama (dari 10), <b>hasil maksimum adalah 17.500, yang sesuai dengan melewati level pertama dan menemukan semua 24 kamar</b> .  Grafik di bawah membandingkan dua percobaan ini, yang menunjukkan nilai rata-rata tergantung pada parameter pembaruan. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  Visualisasi di bawah ini menunjukkan kemajuan percobaan pada skala yang lebih kecil.  Agen, di bawah pengaruh rasa ingin tahu, membuka kamar baru dan menemukan cara untuk mencetak poin Selama pelatihan, hadiah eksternal ini memaksanya untuk kembali ke kamar ini nanti. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Kamar ditemukan oleh agen dan hasil rata-rata selama pelatihan.</font></i>  <i><font color="gray">Tingkat transparansi ruangan sesuai dengan berapa kali dari 10 lintasan agen yang terdeteksi.</font></i>  <i><font color="gray"><a href="">Video</a></font></i> <br><br><h1>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Studi pembelajaran berskala besar berbasis rasa ingin tahu</a> </h1><br>  Sebelum mengembangkan RND, kami, bersama dengan staf dari University of California di Berkeley, mengeksplorasi pembelajaran tanpa imbalan lingkungan.  Keingintahuan menyediakan cara yang lebih mudah untuk mengajar agen untuk berinteraksi dengan lingkungan <i>apa pun</i> , daripada menggunakan fungsi hadiah yang dirancang khusus untuk tugas tertentu, yang belum menjadi fakta yang sesuai dengan solusi masalah.  Dalam proyek-proyek seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">ALE</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Universe</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Malmo</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gym</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Gym Retro</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Unity</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">DeepMind Lab</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">CommAI</a> , sejumlah besar lingkungan simulasi dibuka untuk agen melalui antarmuka standar.  Agen yang menggunakan fungsi hadiah umum yang tidak spesifik untuk lingkungan tertentu dapat memperoleh tingkat kompetensi dasar dalam berbagai lingkungan.  Ini memungkinkan dia untuk menentukan perilaku yang berguna bahkan tanpa adanya imbalan yang rumit. <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Teks artikel ilmiah</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">kode</a> </blockquote><br>  Dalam pengaturan pelatihan standar dengan penguatan pada setiap langkah waktu diskrit, agen mengirimkan tindakan ke lingkungan, dan bereaksi, memberikan agen pengamatan baru, hadiah untuk transisi dan indikator akhir episode.  Dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel</a> kami <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sebelumnya,</a> kami mengatur lingkungan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">untuk menghasilkan</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hanya</a> pengamatan berikut.  Di sana, agen mempelajari model prediktor keadaan berikutnya berdasarkan pengalamannya dan menggunakan kesalahan prediksi sebagai hadiah internal.  Akibatnya, ia tertarik pada ketidakpastian.  Misalnya, perubahan akun game hanya dihargai jika akun ditampilkan di layar dan perubahan itu sulit diprediksi.  Agen, sebagai suatu peraturan, menemukan interaksi yang bermanfaat dengan objek baru, karena hasil interaksi seperti itu biasanya lebih sulit untuk diprediksi daripada aspek lingkungan lainnya. <br><br>  Seperti <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">peneliti</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">lain</a> , kami mencoba menghindari pemodelan semua aspek lingkungan, terlepas dari apakah mereka relevan atau tidak, memilih fitur pengamatan untuk pemodelan.  Anehnya, kami menemukan bahwa bahkan fungsi acak bekerja dengan baik. <br><br><h1>  Apa yang dilakukan agen penasaran? </h1><br>  Kami menguji agen kami di lebih dari 50 lingkungan yang berbeda dan mengamati berbagai kompetensi mulai dari tindakan acak hingga interaksi sadar dengan lingkungan.  Yang mengejutkan kami, dalam beberapa kasus, agen berhasil melewati permainan, meskipun dia tidak diberitahu tujuan melalui hadiah eksternal. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Remunerasi internal pada awal pelatihan</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Lompatan dalam hadiah internal di bagian pertama tingkat</font></i> <br><br>  <b>Breakout</b> - melompat dalam hadiah internal ketika agen melihat konfigurasi baru blok pada tahap awal pelatihan dan ketika level berlalu untuk pertama kalinya setelah pelatihan selama beberapa jam. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - kami melatih agen untuk mengontrol kedua platform secara bersamaan, dan dia belajar untuk menjaga bola dalam permainan, yang menyebabkan perkelahian yang berkepanjangan.  Bahkan ketika berlatih melawan AI dalam gim, agen berusaha memaksimalkan gim, dan tidak menang. <br><br>  <b><a href="">Bowling</a></b> - agen belajar bermain game lebih baik daripada agen lain yang dilatih langsung untuk memaksimalkan hadiah eksternal.  Kami pikir ini terjadi karena agen tertarik oleh kedipan papan skor yang sulit diprediksi setelah lemparan. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - Hadiah internal sangat selaras dengan tujuan permainan: tingkat perkembangan.  Agen dihargai untuk mencari area baru, karena detail area yang baru ditemukan tidak dapat diprediksi.  Akibatnya, agen menemukan 11 tingkat, menemukan ruang rahasia dan bahkan mengalahkan bos. <br><br><h1>  Masalah TV yang bising </h1><br>  Sebagai penjudi di mesin slot, tertarik dengan hasil acak, agen kadang-kadang jatuh ke dalam perangkap keingintahuannya sebagai akibat dari "masalah TV berisik".  Agen menemukan sumber keacakan dalam lingkungan dan terus mengamatinya, selalu mengalami imbalan internal yang tinggi untuk transisi tersebut.  Contoh jebakan tersebut adalah menonton televisi yang menghasilkan suara statis.  Kami mendemonstrasikan ini secara harfiah dengan menempatkan agen di labirin Persatuan dengan TV yang memutar saluran acak. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agen dalam labirin dengan TV berisik</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agen dalam labirin tanpa TV berisik</font></i> <br><br>  Secara teoritis, masalah TV yang bising benar-benar serius, tetapi kami masih berharap bahwa di lingkungan yang banyak deterministik seperti Pembalasan Montezuma, rasa ingin tahu akan menyebabkan agen menemukan kamar dan berinteraksi dengan objek.  Kami mencoba beberapa opsi untuk memprediksi keadaan berikutnya berdasarkan rasa ingin tahu, menggabungkan bonus penelitian dengan akun game. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  Dalam percobaan ini, agen mengontrol lingkungan melalui pengontrol derau, yang dengan beberapa kemungkinan mengulangi tindakan terakhir alih-alih yang sekarang.  Pengaturan ini dengan tindakan "lengket" berulang-ulang telah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">diusulkan</a> sebagai praktik terbaik bagi agen pelatihan dalam permainan yang sepenuhnya deterministik, seperti Atari, untuk mencegah menghafal.  Tindakan "Sticky" membuat transisi dari kamar ke kamar tidak dapat diprediksi. <br><br><h1>  Distilasi jaringan acak </h1><br>  Karena memprediksi keadaan selanjutnya rentan terhadap masalah TV yang bising, kami telah mengidentifikasi sumber kesalahan prediksi yang relevan berikut: <br><br><ul><li>  <b>Faktor 1</b> .  Kesalahan peramalan tinggi jika prediktor gagal menggeneralisasi dari contoh yang dipertimbangkan sebelumnya.  Pengalaman baru terkait dengan kesalahan prediksi yang tinggi. </li><li>  <b>Faktor 2</b> .  Kesalahan peramalan tinggi karena tujuan peramalan stokastik. </li><li>  <b>Faktor 3</b> .  Kesalahan peramalan tinggi karena kurangnya informasi yang diperlukan untuk peramalan, atau karena kelas model peramal terlalu terbatas untuk memenuhi kompleksitas fungsi tujuan. </li></ul><br>  Kami menentukan bahwa faktor 1 adalah sumber kesalahan yang berguna karena ia mengukur kebaruan pengalaman, sementara faktor 2 dan 3 mengarah pada masalah TV yang bising.  Untuk menghindari faktor 2 dan 3, kami mengembangkan RND - bonus penelitian baru berdasarkan <b>prediksi penerbitan jaringan saraf yang konstan dan diinisialisasi secara acak di negara bagian berikutnya, dengan mempertimbangkan keadaan berikut itu sendiri</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  Intuisi menunjukkan bahwa model prediksi memiliki kesalahan rendah dalam memprediksi kondisi di mana dia dilatih.  Secara khusus, prediksi agen tentang mengeluarkan jaringan saraf yang diinisialisasi secara acak akan kurang akurat di negara-negara baru daripada di negara-negara bahwa agen sering bertemu sebelumnya.  Keuntungan menggunakan masalah peramalan sintetik adalah bahwa ia dapat bersifat deterministik (melewati faktor 2), dan dalam kelas fungsi, prediktor dapat memilih prediktor dari arsitektur yang sama dengan jaringan target (melewati faktor 3).  Ini menghilangkan masalah RND dari TV berisik. <br><br>  Kami menggabungkan bonus penelitian dengan hadiah eksternal melalui bentuk optimisasi kebijakan terdekat - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Optimalisasi Kebijakan Proximal</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">PPO</a> ), yang menggunakan <b>dua nilai nilai untuk dua aliran hadiah</b> .  Ini memungkinkan Anda untuk menggunakan diskon yang berbeda untuk hadiah yang berbeda dan untuk menggabungkan imbalan episodik dan non-episodik.  <b>Karena fleksibilitas tambahan seperti itu, agen terbaik kami sering menemukan 22 dari 24 kamar di tingkat pertama dalam Pembalasan Montezuma, dan kadang-kadang melewati tingkat pertama setelah menemukan dua kamar yang tersisa.</b>  Metode yang sama menunjukkan kinerja rekaman dalam game Venture dan Gravitar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  Visualisasi di bawah ini menunjukkan grafik hadiah internal dalam episode Pembalasan Montezuma, di mana agen pertama kali menemukan obor. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  Implementasi yang kompeten adalah penting </h1><br>  Untuk memilih algoritma yang baik, penting untuk mempertimbangkan pertimbangan umum, seperti kerentanan terhadap masalah TV yang bising.  Namun, kami menemukan bahwa perubahan yang sangat kecil pada algoritma sederhana kami sangat memengaruhi efektivitasnya: dari agen yang tidak dapat meninggalkan ruangan pertama ke agen yang melewati level pertama.  Untuk menambah stabilitas pada pelatihan, kami menghindari kejenuhan sifat dan membawa hadiah internal ke kisaran yang dapat diprediksi.  Kami juga melihat <b>peningkatan signifikan dalam efektivitas RND setiap kali kami menemukan dan memperbaiki bug</b> (favorit kami termasuk pengenaan secara acak array, yang mengarah pada fakta bahwa penghargaan eksternal dianggap sebagai non-episodik; kami menyadari ini hanya setelah memikirkan fungsi nilai eksternal , yang tampak mencurigakan secara berkala).  Memperbaiki rincian ini telah menjadi bagian penting untuk mencapai kinerja tinggi bahkan ketika menggunakan algoritma yang secara konseptual mirip dengan pekerjaan sebelumnya.  Ini adalah salah satu alasan mengapa yang terbaik adalah memilih algoritma sederhana bila memungkinkan. <br><br><h1>  Pekerjaan di masa depan </h1><br>  Kami menawarkan bidang-bidang berikut untuk penelitian lebih lanjut: <br><br><ul><li>  Analisis keuntungan dari berbagai metode penelitian dan pencarian cara baru untuk menggabungkannya. </li><li>  Melatih agen penasaran dalam berbagai lingkungan tanpa imbalan dan belajar mentransfer ke lingkungan target dengan imbalan. </li><li>  Kecerdasan global, termasuk solusi terkoordinasi selama jangka waktu yang lama. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id428776/">https://habr.com/ru/post/id428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id428766/index.html">Intisari bahan-bahan segar dari dunia front-end untuk minggu terakhir No. 337 (29 Oktober - 4 November 2018)</a></li>
<li><a href="../id428768/index.html">Dalam tiga artikel tentang kuadrat terkecil: program pendidikan tentang teori probabilitas</a></li>
<li><a href="../id428770/index.html">Makro keyboard untuk tugas sehari-hari</a></li>
<li><a href="../id428772/index.html">Demokratisasi data Uber</a></li>
<li><a href="../id428774/index.html">Firewall GPS untuk pusat data - mengapa diperlukan dan bagaimana cara kerjanya</a></li>
<li><a href="../id428778/index.html">Lihat yang tak terlihat. Near Infrared (0,9-1,7Î¼m)</a></li>
<li><a href="../id428786/index.html">Prosesor kuantum berdasarkan resonansi putaran dan manipulasi dengan sistem singlet-triplet</a></li>
<li><a href="../id428788/index.html">Di bawah naungan Bitfury Clarke - cara kerja chip penambangan baru kami</a></li>
<li><a href="../id428790/index.html">Kami menulis obrolan bot untuk VKontakte di python menggunakan longpoll. Bagian Dua Loop ganda, pengecualian, dan ajaran sesat lainnya</a></li>
<li><a href="../id428792/index.html">Chip Apple T2 baru membuatnya sulit untuk mendengarkan melalui mikrofon bawaan laptop</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>