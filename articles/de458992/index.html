<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚òùüèº üòÆ üö£üèº Achtung f√ºr Dummies und Implementierung in Keras üè¨ üíñ üòÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√úber Artikel zur k√ºnstlichen Intelligenz in russischer Sprache 
 Trotz der Tatsache, dass der Aufmerksamkeitsmechanismus in der englischen Literatur b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Achtung f√ºr Dummies und Implementierung in Keras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2>  √úber Artikel zur k√ºnstlichen Intelligenz in russischer Sprache </h2><br>  Trotz der Tatsache, dass der Aufmerksamkeitsmechanismus in der englischen Literatur beschrieben ist, habe ich im russischsprachigen Sektor noch keine anst√§ndige Beschreibung dieser Technologie gesehen.  Es gibt viele Artikel √ºber k√ºnstliche Intelligenz (KI) in unserer Sprache.  Die gefundenen Artikel enth√ºllen jedoch nur die einfachsten KI-Modelle, zum Beispiel Faltungsnetzwerke, generative Netzwerke.  Nach den neuesten Entwicklungen auf dem Gebiet der KI gibt es im russischsprachigen Sektor jedoch nur sehr wenige Artikel. <br><br><a name="habracut"></a>  Das Fehlen von Artikeln in russischer Sprache √ºber die neuesten Entwicklungen wurde f√ºr mich zu einem Problem, als ich mich mit dem Thema befasste und den aktuellen Stand der Dinge auf dem Gebiet der KI studierte.  Ich kann gut Englisch, ich lese Artikel auf Englisch zu AI-Themen.  Wenn jedoch ein neues Konzept oder ein neues Prinzip der KI herauskommt, ist das Verst√§ndnis in einer Fremdsprache schmerzhaft und langwierig.  Englischkenntnisse, um in einem komplexen Objekt in einen Nicht-Muttersprachler einzudringen, sind immer noch viel mehr Zeit und M√ºhe wert.  Nachdem Sie die Beschreibung gelesen haben, stellen Sie sich die Frage: Wie viele Prozent verstehen Sie?  Wenn es einen Artikel auf Russisch g√§be, w√ºrde ich nach der ersten Lesung 100% verstehen.  Dies geschah mit generativen Netzwerken, f√ºr die es eine hervorragende Artikelserie gibt: Nach dem Lesen wurde alles klar.  In der Welt der Netzwerke gibt es jedoch viele Ans√§tze, die nur auf Englisch beschrieben werden und die tagelang behandelt werden mussten. <br><br>  Ich werde regelm√§√üig Artikel in meiner Muttersprache schreiben und Wissen in unser Sprachfeld bringen.  Wie Sie wissen, ist der beste Weg, ein Thema zu verstehen, es jemandem zu erkl√§ren.  Also, wer anders als ich sollte eine Reihe von Artikeln √ºber die modernste, komplexeste und fortschrittlichste Architektur-KI beginnen.  Am Ende des Artikels werde ich selbst einen 100% igen Ansatz verstehen, und er wird f√ºr jemanden n√ºtzlich sein, der sein Verst√§ndnis liest und verbessert (√ºbrigens, ich liebe Gesser, aber besser ** Blanche de bruxelles **). <br><br>  Wenn Sie das Thema verstehen, gibt es 4 Ebenen des Verst√§ndnisses: <br><br><ol><li>  Sie verstehen das Prinzip und die Ein- und Ausg√§nge des Algorithmus / Levels </li><li>  Sie verstehen die Sammelausg√§nge und allgemein, wie es funktioniert </li><li>  Sie verstehen alle oben genannten Punkte sowie das Ger√§t jeder Netzwerkebene (zum Beispiel haben Sie im VAE-Modell das Prinzip verstanden und auch die Essenz des Reparametrisierungstricks verstanden). </li><li>  Ich habe alles verstanden, einschlie√ülich aller Ebenen, ich habe auch verstanden, warum alles lernt, und gleichzeitig kann ich Hyperparameter f√ºr meine Aufgabe ausw√§hlen, anstatt vorgefertigte L√∂sungen zu kopieren und einzuf√ºgen. </li></ol><br>  F√ºr neue Architekturen ist der √úbergang von Stufe 1 zu Stufe 4 oft schwierig: Die Autoren betonen, dass sie verschiedene wichtige Details oberfl√§chlich n√§her beschreiben (haben sie sie selbst verstanden?).  Oder Ihr Gehirn enth√§lt keine Konstruktionen, sodass es selbst nach dem Lesen der Beschreibung nicht entschl√ºsselt und nicht zu F√§higkeiten wurde.  Dies passiert, wenn Sie w√§hrend Ihrer Studienzeit nach einer Nachtparty, bei der Sie die richtige Matte gegeben haben, in derselben Matan-Stunde geschlafen haben.  Apparate.  Und genau hier brauchen wir Artikel in unserer Muttersprache, die die Nuancen und Feinheiten jeder Operation enth√ºllen. <br><br><h2>  Aufmerksamkeitskonzept und Anwendung </h2><br>  Das Obige ist ein Szenario von Verst√§ndnisebenen.  Um die Aufmerksamkeit zu analysieren, beginnen wir mit Stufe eins.  Bevor wir die Ein- und Ausg√§nge beschreiben, werden wir das Wesentliche analysieren: Auf welchen Grundkonzepten, die selbst f√ºr ein Kind verst√§ndlich sind, basiert dieses Konzept.  In dem Artikel werden wir den englischen Begriff Attention verwenden, da es sich in dieser Form auch um einen Aufruf der Keras-Bibliotheksfunktion handelt (sie ist nicht direkt darin implementiert, ein zus√§tzliches Modul ist erforderlich, aber mehr dazu weiter unten).  Um weiterlesen zu k√∂nnen, m√ºssen Sie die Keras- und Python-Bibliotheken kennen, da der Quellcode bereitgestellt wird. <br><br><img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right">  Aufmerksamkeit √ºbersetzt aus dem Englischen als "Aufmerksamkeit".  Dieser Begriff beschreibt das Wesentliche des Ansatzes korrekt: Wenn Sie Autofahrer sind und der Verkehrspolizei-General auf dem Foto abgebildet ist, legen Sie unabh√§ngig vom Kontext des Fotos intuitiv Wert darauf.  Sie werden sich den General wahrscheinlich genauer ansehen.  Sie belasten Ihre Augen, schauen sich die Schultergurte genau an: wie viele Sterne hat er speziell dort.  Wenn der General nicht sehr gro√ü ist, ignorieren Sie ihn.  Andernfalls betrachten Sie es als einen Schl√ºsselfaktor f√ºr Entscheidungen.  So funktioniert unser Gehirn.  In der russischen Kultur wurden wir von Generationen darauf trainiert, auf hohe R√§nge zu achten. Unser Gehirn legt automatisch gro√üen Wert auf solche Objekte. <br><br>  Aufmerksamkeit ist eine M√∂glichkeit, dem Netzwerk mitzuteilen, worauf Sie mehr achten sollten, dh die Wahrscheinlichkeit eines bestimmten Ergebnisses in Abh√§ngigkeit vom Zustand der Neuronen und den Eingabedaten zu melden.  Die in Keras selbst implementierte Aufmerksamkeitsschicht identifiziert Faktoren basierend auf dem Trainingssatz, dessen Aufmerksamkeit den Netzwerkfehler verringert.  Die Identifizierung wichtiger Faktoren erfolgt durch das Verfahren der R√ºckausbreitung von Fehlern, √§hnlich wie dies f√ºr Faltungsnetzwerke erfolgt. <br><br>  Im Training zeigt Attention seinen probabilistischen Charakter.  Der Mechanismus selbst bildet eine Matrix von Wichtigkeitsskalen.  Wenn wir die Aufmerksamkeit nicht geschult h√§tten, h√§tten wir die Bedeutung beispielsweise empirisch festlegen k√∂nnen (der General ist wichtiger als der F√§hnrich).  Wenn wir jedoch ein Netzwerk auf Daten trainieren, wird die Wichtigkeit eine Funktion der Wahrscheinlichkeit eines bestimmten Ergebnisses, abh√§ngig von den Daten, die am Eingang des Netzwerks empfangen werden.  Wenn wir zum Beispiel einen im zaristischen Russland lebenden General treffen w√ºrden, w√§re die Wahrscheinlichkeit hoch, Stulpen zu bekommen.  Nachdem dies festgestellt worden w√§re, w√§re es m√∂glich, durch mehrere pers√∂nliche Treffen Statistiken zu sammeln.  Danach wird unser Gehirn die Tatsache des Treffens dieses Themas angemessen belasten und Markierungen auf Schultergurten und Streifen anbringen.  Es sollte beachtet werden, dass der gesetzte Marker nicht wahrscheinlich ist: Jetzt hat das Treffen des Generals v√∂llig andere Konsequenzen f√ºr Sie als damals, au√üerdem kann das Gewicht mehr als eins sein.  Das Gewicht kann jedoch durch Normalisierung auf die Wahrscheinlichkeit reduziert werden. <br><br>  Der probabilistische Charakter des Aufmerksamkeitsmechanismus beim Lernen manifestiert sich in maschinellen √úbersetzungsaufgaben.  Lassen Sie uns beispielsweise das Netzwerk dar√ºber informieren, dass bei der √úbersetzung vom Russischen ins Englische das Wort Liebe in 90% der F√§lle als Liebe, in 9% der F√§lle als Geschlecht und in 1% der F√§lle als sonst √ºbersetzt wird.  Das Netzwerk markiert sofort viele Optionen und zeigt die beste Qualit√§t des Trainings.  Beim √úbersetzen sagen wir dem Netzwerk: "Achten Sie beim √úbersetzen des Wortes Liebe besonders auf das englische Wort Love und pr√ºfen Sie, ob es noch Sex sein kann." <br><br>  Der Attention-Ansatz wird angewendet, um mit Text sowie Ton- und Zeitreihen zu arbeiten.  F√ºr die Textverarbeitung werden h√§ufig wiederkehrende neuronale Netze (RNN, LSTM, GRU) verwendet.  Aufmerksamkeit kann sie entweder erg√§nzen oder ersetzen und das Netzwerk auf einfachere und schnellere Architekturen verlagern. <br><br>  Eine der bekanntesten Anwendungen von Attention ist die Verwendung, um das Wiederholungsnetzwerk zu verlassen und zu einem vollst√§ndig verbundenen Modell zu wechseln.  Wiederkehrende Netzwerke weisen eine Reihe von M√§ngeln auf: die Unf√§higkeit, Schulungen auf der GPU anzubieten, eine schnelle Umschulung.  Mit dem Attention-Mechanismus k√∂nnen wir ein Netzwerk aufbauen, das Sequenzen auf der Basis eines vollst√§ndig verbundenen Netzwerks lernen kann, es auf der GPU trainieren und Droput verwenden. <br><br>  Aufmerksamkeit wird h√§ufig verwendet, um die Leistung von Wiederholungsnetzwerken zu verbessern, beispielsweise im Bereich der √úbersetzung von Sprache zu Sprache.  Bei Verwendung des Codierungs- / Decodierungsansatzes, der in der modernen KI h√§ufig verwendet wird (z. B. Variationsautocodierer).  Wenn eine Aufmerksamkeitsschicht zwischen dem Codierer und dem Decodierer hinzugef√ºgt wird, verbessert sich das Ergebnis des Netzwerkbetriebs merklich. <br><br>  In diesem Artikel zitiere ich keine spezifischen Netzwerkarchitekturen, die Attention verwenden. Dies wird Gegenstand separater Arbeiten sein.  Eine Auflistung aller m√∂glichen Verwendungszwecke der Aufmerksamkeit verdient einen gesonderten Artikel. <br><br><h2>  Implementierung von Aufmerksamkeit in Keras Out of the Box </h2><br>  Wenn Sie verstehen, welche Art von Ansatz Sie verwenden, ist es sehr n√ºtzlich, das Grundprinzip zu lernen.  Ein vollst√§ndiges Verst√§ndnis entsteht jedoch oft nur durch die Betrachtung einer technischen Implementierung.  Wenn Sie die Datenstr√∂me sehen, aus denen sich die Funktion der Operation zusammensetzt, wird klar, was genau berechnet wird.  Aber zuerst m√ºssen Sie es ausf√ºhren und "Achtung Hallo Wort" schreiben. <br><br>  Achtung ist derzeit in Keras selbst nicht implementiert.  Es gibt jedoch bereits Implementierungen von Drittanbietern, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufmerksamkeits-Keras,</a> die mit Github installiert werden k√∂nnen.  Dann wird Ihr Code extrem einfach: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> attention_keras.layers.attention <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AttentionLayer attn_layer = AttentionLayer(name=<span class="hljs-string"><span class="hljs-string">'attention_layer'</span></span>) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</code> </pre> <br>  Diese Implementierung unterst√ºtzt die Visualisierungsfunktion der Aufmerksamkeitsskala.  Nachdem Sie die Aufmerksamkeit geschult haben, k√∂nnen Sie eine Matrixsignalisierung erhalten, die laut Netzwerk f√ºr diesen Typ besonders wichtig ist (Bild von Github auf der Seite der Aufmerksamkeits-Keras-Bibliothek). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br>  Grunds√§tzlich brauchen Sie nichts anderes: F√ºgen Sie diesen Code als eine der Ebenen in Ihr Netzwerk ein und genie√üen Sie das Lernen Ihres Netzwerks.  Jedes Netzwerk, jeder Algorithmus wird in den ersten Phasen auf konzeptioneller Ebene entworfen (wie √ºbrigens die Datenbank), wonach die Implementierung vor der Implementierung in einer logischen und physischen Darstellung angegeben wird.  Diese Entwurfsmethode wurde noch nicht f√ºr neuronale Netze entwickelt (oh ja, dies wird das Thema meines n√§chsten Artikels sein).  Sie verstehen nicht, wie Faltungsschichten im Inneren funktionieren?  Das Prinzip wird beschrieben, Sie verwenden sie. <br><br><h2>  Keras-Implementierung von Attention low </h2><br>  Um das Thema endlich zu verstehen, werden wir im Folgenden die Implementierung von Attention under the Hood im Detail analysieren.  Das Konzept ist gut, aber wie genau funktioniert es und warum wird das Ergebnis genau wie angegeben erzielt? <br><br>  Die einfachste Implementierung des Attention-Mechanismus in Keras dauert nur drei Zeilen: <br><br><pre> <code class="python hljs">inputs = Input(shape=(input_dims,)) attention_probs = Dense(input_dims, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_probs'</span></span>)(inputs) attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number"><span class="hljs-number">32</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_mul'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'mul'</span></span></code> </pre><br>  In diesem Fall wird die Eingabeschicht in der ersten Zeile deklariert, dann kommt eine vollst√§ndig verbundene Schicht mit der Softmax-Aktivierungsfunktion, wobei die Anzahl der Neuronen gleich der Anzahl der Elemente in der ersten Schicht ist.  Die dritte Schicht multipliziert das Ergebnis der vollst√§ndig verbundenen Schicht mit den Eingabedaten Element f√ºr Element. <br><br>  Unten finden Sie die gesamte Attention-Klasse, die einen etwas komplexeren Selbstaufmerksamkeitsmechanismus implementiert, der als vollwertige Ebene im Modell verwendet werden kann. Die Klasse erbt die Keras-Layer-Klasse. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Attention class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim</span></span></code> </pre><br>  Hier sehen wir ungef√§hr dasselbe, was oben durch eine vollst√§ndig verbundene Keras-Schicht implementiert wurde, die nur durch eine tiefere Logik auf einer niedrigeren Ebene ausgef√ºhrt wurde.  In der Funktion wird eine parametrische Ebene (self.W) erstellt, die dann skalar mit dem Eingabevektor multipliziert wird (K.dot).  Die verdrahtete Logik in dieser Variante ist etwas komplizierter: Verschiebung (wenn der Bias-Parameter angegeben ist), hyperbolische Tangente, Belichtung, Maske (falls angegeben), Normalisierung werden auf den Eingangsvektor mal self.W angewendet, dann wird der Eingangsvektor erneut mit gewichtet das erhaltene Ergebnis.  Ich habe keine Beschreibung der in diesem Beispiel festgelegten Logik, ich reproduziere die Operationen des Lesens des Codes.  √úbrigens, bitte schreiben Sie in die Kommentare, wenn Sie eine mathematische Funktion auf hoher Ebene in dieser Logik erkennen. <br><br>  Die Klasse hat einen Parameter "Bias", dh  Voreingenommenheit.  Wenn der Parameter aktiviert ist, wird nach dem Anwenden der dichten Ebene der endg√ºltige Vektor zum Vektor der Ebenenparameter "self.b" hinzugef√ºgt, wodurch nicht nur die "Gewichte" f√ºr unsere Aufmerksamkeitsfunktion bestimmt werden k√∂nnen, sondern auch die Aufmerksamkeitsstufe um eine Zahl verschoben werden kann.  Lebensbeispiel: Wir haben Angst vor Geistern, haben sie aber nie getroffen.  Daher korrigieren wir die Angst um -100 Punkte.  Das hei√üt, nur wenn die Angst um 100 Punkte nachl√§sst, werden wir Entscheidungen √ºber den Schutz vor Geistern treffen, eine Ghostbusting-Agentur anrufen, Angstger√§te kaufen usw. <br><br><h2>  Fazit </h2><br>  Der Aufmerksamkeitsmechanismus weist Variationen auf.  Die einfachste in der obigen Klasse implementierte Attention-Option hei√üt Self-Attention.  Selbstaufmerksamkeit ist ein Mechanismus zur Verarbeitung sequentieller Daten unter Ber√ºcksichtigung des Kontexts jedes Zeitstempels.  Es wird am h√§ufigsten zum Arbeiten mit Textinformationen verwendet.  Die Implementierung der Selbstaufmerksamkeit kann durch Importieren der Keras-Selbstaufmerksamkeitsbibliothek aus der Box genommen werden.  Es gibt andere Varianten der Aufmerksamkeit.  Beim Studium englischsprachiger Materialien konnten mehr als 5 Variationen gez√§hlt werden. <br><br>  Beim Schreiben dieses relativ kurzen Artikels habe ich mehr als 10 englischsprachige Artikel studiert.  Nat√ºrlich konnte ich nicht alle Daten aus all diesen Artikeln auf 5 Seiten herunterladen. Ich habe nur einen Druck gemacht, um einen ‚ÄûLeitfaden f√ºr Dummies‚Äú zu erstellen.  Um alle Nuancen des Aufmerksamkeitsmechanismus zu verstehen, ben√∂tigen Sie ein Buch mit den Seiten 150-200.  Ich hoffe wirklich, dass ich die grundlegende Essenz dieses Mechanismus enth√ºllen konnte, damit diejenigen, die gerade erst anfangen, maschinelles Lernen zu verstehen, verstehen, wie das alles funktioniert. <br><br><h2>  Quellen </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufmerksamkeitsmechanismus in neuronalen Netzen mit Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Achtung in tiefen Netzwerken mit Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufmerksamkeitsbasierte Sequenz-zu-Sequenz in Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Textklassifizierung mithilfe des Aufmerksamkeitsmechanismus in Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Durchdringende Aufmerksamkeit: 2D-Faltungs-Neuronale Netze f√ºr die Vorhersage von Sequenz zu Sequenz</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wie implementiere ich die Aufmerksamkeitsschicht in Keras?</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Achtung?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Achtung!</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neuronale maschinelle √úbersetzung mit Aufmerksamkeit</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458992/">https://habr.com/ru/post/de458992/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458982/index.html">Nginx-Rezepte: Konvertieren von HTML und URLs in PDF und PS</a></li>
<li><a href="../de458984/index.html">So erstellen Sie die erste Anwendung f√ºr den Handel an der B√∂rse: 3 erste Schritte</a></li>
<li><a href="../de458986/index.html">PostgreSQL-Rezepte: Konvertieren von HTML und URLs in PDF und PS</a></li>
<li><a href="../de458988/index.html">Texturierung oder was Sie wissen m√ºssen, um ein Oberfl√§chenk√ºnstler zu werden. Teil 4. Modelle, Normalen und Sweep</a></li>
<li><a href="../de458990/index.html">H√∂r auf eifrig mit Kommentaren im Code</a></li>
<li><a href="../de458994/index.html">Raspberry Pi + CentOS = WLAN-Hotspot (oder Himbeer-Router in einem roten Hut)</a></li>
<li><a href="../de458996/index.html">User Inyerface - wie man den Benutzer nicht qu√§lt</a></li>
<li><a href="../de459000/index.html">Wie ich versucht habe, Halo 2 zu verbessern, es aber fast ruiniert habe</a></li>
<li><a href="../de459002/index.html">So konfigurieren Sie HTTPS - SSL Configuration Generator hilft</a></li>
<li><a href="../de459004/index.html">Grasshopper kryptografischer Algorithmus: fast der Komplex</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>