<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìπ üçµ üîö El t√≠tulo "Leer art√≠culos para usted". Octubre - diciembre 2019 üëè üë©üèø‚Äçü§ù‚Äçüë©üèæ üö≤</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Continuamos publicando rese√±as de art√≠culos cient√≠ficos de miembros de la comunidad Open Data Science del canal #article_essense. Si quiere...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>El t√≠tulo "Leer art√≠culos para usted". Octubre - diciembre 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/485122/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Hola Habr!  Continuamos publicando rese√±as de art√≠culos cient√≠ficos de miembros de la comunidad Open Data Science del canal #article_essense.  Si quieres recibirlos antes que los dem√°s, ¬°√∫nete a la <a href="http://ods.ai/">comunidad</a> ! </p><br><p>  Art√≠culos para hoy: </p><br><ol><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Poly-encoders: arquitecturas de transformadores y estrategias de pre-entrenamiento para un puntaje de m√∫ltiples oraciones r√°pido y preciso (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Discriminador impl√≠cito en Autoencoder Variacional (Indian Institute of Technology Ropar, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">El autoaprendizaje con Noisy Student mejora la clasificaci√≥n de ImageNet (Google Research, Carnegie Mellon University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Contraste de momento para el aprendizaje de representaci√≥n visual sin supervisi√≥n (Facebook, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Evaluaci√≥n comparativa de la robustez de la red neuronal frente a las corrupciones y perturbaciones comunes (Universidad de California, Oregon State University, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">DistilBERT, una versi√≥n destilada de BERT: m√°s peque√±a, m√°s r√°pida, m√°s barata y m√°s ligera (Hugging Face, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Modelos de lenguaje Plug and Play: un enfoque simple para la generaci√≥n de texto controlado (Uber AI, Caltech, HKUST, 2019)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">Representaci√≥n de gran relevancia para la estimaci√≥n de F0 en m√∫sica polif√≥nica (Universidad de Nueva York, EE. UU., 2017)</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/485122/">An√°lisis y mejora de la calidad de imagen de StyleGAN (NVIDIA, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Enlaces a colecciones pasadas de la serie:</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/company/ods/blog/472672/">Julio - septiembre 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/471514/">Enero - junio 2019</a> </li><li>  <a href="https://habr.com/ru/company/ods/blog/352518/">Febrero - marzo 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/352508/">Diciembre de 2017 - enero de 2018</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/343822/">Octubre - noviembre 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/339094/">Septiembre de 2017</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/336624/">Agosto de 2017</a> </li></ul></div></div><br><h3 id="1-poly-encoders-transformer-architectures-and-pre-training-strategies-for-fast-and-accurate-multi-sentence-scoring">  1. Poly-encoders: arquitecturas de transformadores y estrategias de preentrenamiento para puntuaci√≥n r√°pida y precisa de m√∫ltiples oraciones </h3><br><p>  Autores: Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, Jason Weston (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1905.01969">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Alexey (en slack zhirzemli) </p><br><p>  <strong>TLDR</strong> </p><br><p>  El art√≠culo propone un nuevo enfoque para calificar pares de oraciones (declaraciones).  Este procedimiento es relevante en las tareas de predecir si una respuesta coincide con un contexto condicional, as√≠ como en tareas como la predicci√≥n de la pr√≥xima oraci√≥n.  El m√©todo propuesto de Poly-Encoder se compara con las estrategias de Bi-Encoder y Cross-Encoder.  El m√©todo combina la ventaja de Bi-Encoder (la capacidad de almacenar en cach√© la presentaci√≥n de respuestas) y Cross-Encoder (entrenamiento no incondicional de codificadores de contexto y respuesta) </p><br><img src="https://habrastorage.org/webt/ax/qd/nl/axqdnlibzffxcyjtbfzeguhsdja.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Puntuaci√≥n de varias oraciones</strong> </p><br><p>  (Un peque√±o recordatorio sobre los enfoques de Bi y Cross Encoder. Para aquellos que est√°n familiarizados, puede omitir) </p><br><p>  La tarea de determinar la correspondencia del contexto (solicitud o declaraci√≥n del usuario) con el conjunto de respuestas existentes es principalmente relevante en los sistemas de di√°logo y recuperaci√≥n de informaci√≥n.  Se resuelve encontrando una cierta velocidad (producto de puntos) entre las representaciones codificadas del contexto y la respuesta, o codificando conjuntamente el contexto y la respuesta en un vector con la posterior transformaci√≥n lineal en un escalar. </p><br><p>  El primer enfoque se llama Bi-Encoder y la ventaja obvia de este m√©todo es la capacidad de contar sin conexi√≥n las representaciones de todas las respuestas disponibles.  Estas vistas se almacenan en cach√©, y durante la inferencia solo necesita encontrar el vector de consulta, hacer un producto de puntos con vectores de respuesta y organizar el resultado.  Adem√°s, este enfoque permite un muestreo negativo m√°s eficiente en la etapa de capacitaci√≥n.  Es decir, dentro de cada lote, se consideran representaciones para muestras positivas, y se pueden tomar ejemplos negativos directamente del mismo lote.  En esencia, reutilice el pase directo para ejemplos positivos y negativos.  La desventaja del enfoque de Bi-Encoder es el hecho de que las representaciones de contexto y respuesta aprenden casi de forma independiente.  El √∫nico punto en el que es posible al menos alg√∫n tipo de flujo de informaci√≥n entre las vistas de solicitud y respuesta es la botnet en forma del producto de punto final.  Al nivel de cualquier caracter√≠stica textual, la informaci√≥n no se pierde. </p><br><p>  El segundo enfoque es Cross-Encoder.  Implica una interacci√≥n m√°s poderosa de contexto y respuesta en el proceso de aprendizaje e inferencia.  Aqu√≠, las secuencias de token de solicitud y respuesta se concatenan en una.  Se coloca un token separador especial entre ellos y se agrega una incrustaci√≥n especial a cada parte (solicitud, respuesta).  De hecho, esta incorporaci√≥n desplaza las representaciones de entrada de los tokens de respuesta por alguna constante, de modo que el modelo puede distinguirlos m√°s f√°cilmente de los tokens de solicitud.  Como resultado, el modelo aprende a encontrar una representaci√≥n conjunta de la solicitud y la respuesta, de modo que la capa lineal final (vector -&gt; escalar) devuelve un valor logits grande para pares de oraciones que coinciden entre s√≠ y un valor peque√±o de lo contrario.  La desventaja de este enfoque es la imposibilidad de contar sin conexi√≥n las representaciones de respuestas: deben evaluarse en la etapa de inferencia, junto con un conjunto condicional de tokens de solicitud.  Adem√°s, el truco para reutilizar las ideas de ejemplos negativos y positivos en la etapa de capacitaci√≥n ya no funcionar√° aqu√≠.  Tendr√° que recoger muestras negativas antes de la formaci√≥n del lote. </p><br><p>  <strong>Motivaci√≥n</strong> <br>  La siguiente es una soluci√≥n que le permite mitigar las deficiencias y combinar las ventajas de los enfoques Bi y Cross Encoder.  La idea es que queramos entrenar un codificador que, por un lado, tenga en cuenta la dependencia condicional de los tokens de respuesta en los tokens de solicitud, y por otro lado, la utilizaci√≥n de esta dependencia deber√≠a ocurrir en representaciones previamente evaluadas de la respuesta y la solicitud.  Geom√©tricamente, personalmente me imagino algo como esto: mover la botnet (el producto de punto final de las dos presentaciones) un poco m√°s abajo a la red.  Cree cierta interacci√≥n entre las vistas de solicitud y respuesta.  Al mismo tiempo, implementar tal interacci√≥n no est√° demasiado lejos de la capa final, de modo que la parte principal del codificador de solicitud permanece independiente del codificador de respuesta. </p><br><p>  <strong>Implementaci√≥n</strong> <br>  La implementaci√≥n de tal idea es bastante simple: el codificador de candidatos funciona como en el caso de Bi-Encoder: obtenemos la representaci√≥n de la secuencia en forma de vector (token [CLS]) usando el modelo basado en transformador (BERT).  Almacenamos estas representaciones en cach√© despu√©s de entrenar el modelo. </p><br><p>  El codificador de contexto, a su vez, no comprime la representaci√≥n de la secuencia de entrada en un solo vector.  Aqu√≠ dejamos todos los vectores de secuencia codificados por el modelo. </p><br><p>  Para obtener una evaluaci√≥n de la conformidad del contexto (un conjunto de vectores) y el candidato (un vector), se utiliza el mecanismo de atenci√≥n.  El vector candidato en este caso es una solicitud, y el vector de contexto son las claves.  Se considera producto de punto y m√°s - softmax de acuerdo con los valores resultantes.  Los vectores de contexto se ponderan por la distribuci√≥n resultante y se suman.  Como resultado, obtenemos la representaci√≥n de contexto en forma de un solo vector.  Y adem√°s, como en el Bi-Encoder habitual, consideramos el producto escalar del contexto y el candidato. </p><br><p>  Adem√°s, el art√≠culo propuso varias formas de acelerar la ponderaci√≥n de los vectores de contexto.  La opci√≥n m√°s funcional era un proceso de contar la atenci√≥n, en el que solo se tomaban los primeros m vectores de la secuencia de contexto. </p><br><p>  <strong>Resultados</strong> <br>  Como resultado, result√≥ que Cross-Encoder todav√≠a funciona mejor.  Pero Poly-Encoder no est√° muy lejos en t√©rminos de m√©tricas de calidad, y en t√©rminos de velocidad de inferencia, funciona cientos de veces m√°s r√°pido. </p><br><h3 id="2-implicit-discriminator-in-variational-autoencoder">  2. Discriminador impl√≠cito en Autoencoder Variacional </h3><br><p>  Autores: Prateek Munjal, Akanksha Paul, Narayanan C. Krishnan (Instituto Indio de Tecnolog√≠a Ropar, 2019) <br>  <a href="https://arxiv.org/abs/1909.13062">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Alex Chiron (en sliron shiron8bit) </p><br><p>  En el art√≠culo, los autores propusieron una arquitectura que intenta combinar las ventajas de los enfoques VAE y GAN para la generaci√≥n de im√°genes, evitando las desventajas inherentes a cada enfoque: desenfoque en el caso de autoencoders, falta de modo / modo en caso de entrenamiento adversario.  Lo logran debido a los pesos totales entre el codificador y el discriminador y el generador / decodificador com√∫n, que, en primer lugar, reduce el n√∫mero de pesos de red, y en segundo lugar, nos permite obtener informaci√≥n √∫til del discriminador a trav√©s de gradientes si el generador / decodificador no cae en la distribuci√≥n de datos real. </p><br><p>  <strong>Introduccion</strong> <br>  En los problemas de generaci√≥n, la coincidencia de la distribuci√≥n de los datos generados Q con la distribuci√≥n de los datos reales P, que se mide a trav√©s de la divergencia Kullback-Leibler, juega un papel importante.  Una caracter√≠stica distintiva de esta medida de la lejan√≠a de las distribuciones es que es asim√©trica.  En consecuencia, obtendremos diferentes im√°genes dependiendo de si consideramos Div_KL (P || Q) o Div_KL (Q || P).  Si consideramos dos opciones para comparar distribuciones (en la imagen a continuaci√≥n), entonces con Div_KL (P || Q) (tambi√©n conocido como forward-KL, tambi√©n conocido como cero evitando) la segunda opci√≥n dar√° un valor m√°s bajo, y para Div_KL (Q || P) (es KL hacia atr√°s, tambi√©n es de forzamiento cero) las distribuciones de la primera opci√≥n se considerar√°n distribuciones m√°s cercanas.  En realidad, los resultados de VAE y GAN son muy diferentes: la p√©rdida de reconstrucci√≥n (L2) ayuda a minimizar la divergencia de KL hacia adelante (y, por lo tanto, conservamos todos los modos, pero obtenemos im√°genes borrosas), y el entrenamiento con un discriminador ayuda a minimizar la divergencia de KL hacia atr√°s (las im√°genes se obtienen m√°s claro, pero existe el riesgo de omitir el mod) </p><br><img src="https://habrastorage.org/webt/y9/7k/cd/y97kcdipocff08h4dsoaqly3udq.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Arquitectura, P√©rdidas y Entrenamiento</strong> <br>  Como se mencion√≥ anteriormente, los autores proponen tener en cuenta las deficiencias de ambos modos y combinar ambas minimizaciones debido a la arquitectura de red (en la imagen a continuaci√≥n), en la que la mayor√≠a de los pesos del codificador y discriminador son comunes (solo los cabezales completamente conectados que predicen la 'realidad' de la imagen y los par√°metros est√°n separados mu, sigma de la capa latente VAE), y tambi√©n debido al modo de entrenamiento.  El codificador y el generador son iguales. La mayor√≠a de las p√©rdidas utilizadas son bastante est√°ndar: en el codificador L_enc los, se utiliza el error de recuperaci√≥n L2 y la divergencia de Kullback-Leibler a N (0,1) (L_prior), el resto es entrenamiento de confrontaci√≥n (minimizamos la salida del discriminador al entrenar al discriminador, maxim√≠celo) al aprender un decodificador / generador), pero hay 2 caracter√≠sticas distintivas: </p><br><ul><li><p>  En la p√©rdida relacionada con el entrenamiento de confrontaci√≥n, se alimentan al discriminador 2 tipos diferentes de datos generados: recuperados mediante un codificador / decodificador y generados por un generador / decodificador a partir de muestras de N (0,1) </p><br></li><li><p>  En la p√©rdida del decodificador L_dec, hay un miembro en el que las caracter√≠sticas de la pen√∫ltima capa del discriminador (nuevamente, esta es la √∫ltima capa com√∫n entre el discriminador y el codificador) se comparan para im√°genes reales y restauradas. </p><br></li></ul><br><img src="https://habrastorage.org/webt/-d/n5/jh/-dn5jh_obvrbb4am3ujm37hd9qs.png" width="500" height="250"><br><p>  <strong>Resultados</strong> <br>  Los autores compararon los resultados con VAE y otros trabajos, de una forma u otra tratando de combinar VAE y GAN (VAE-GAN, alpha-GAN y AGE de Dmitry Ulyanov y Victor Lempitsky) en conjuntos de datos celeba y cifar10 (gracias por no mnist), recibi√≥ casi los mejores indicadores sobre el error de reconstrucci√≥n y la m√©trica de distancia de inicio de Frechet (compara las estad√≠sticas de activaci√≥n de la malla pre-entrenada para im√°genes reales y generadas).  Se observ√≥ por separado que la clasificaci√≥n por FID depende en gran medida de la arquitectura elegida, por lo que el resultado es mejor verificar el conjunto de 'expertos' (diferentes arquitecturas). </p><br><h3 id="3-self-training-with-noisy-student-improves-imagenet-classification">  3. El autoaprendizaje con Noisy Student mejora la clasificaci√≥n de ImageNet </h3><br><p>  Autores: Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le (Google Research, Carnegie Mellon University, 2019) <br>  <a href="https://arxiv.org/abs/1911.04252">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Alexander Belsky (en slack belskikh) </p><br><p>  Google recibi√≥ un impresionante 87.4% de top1 y 98.2% de top5 de precisi√≥n en la imagen.  Zayuzali oculta redes de pseudo-atenuaci√≥n y muy audaces.  El enfoque se llamaba Estudiante ruidoso. </p><br><img src="https://habrastorage.org/webt/es/s8/tm/ess8tmsezy4cjwydsqqfhjxclcu.png"><br><p><br></p><br><p>  <strong>El algoritmo es</strong> algo como esto: </p><br><ol><li>  Tomamos un modelo de maestro, ense√±amos una imagen normal. </li><li>  Generamos psudo etiquetas suaves en im√°genes del conjunto de datos JFT. </li><li>  Ense√±amos el modelo del estudiante en pseudo-etiquetas suaves, e interferimos tan pronto como podemos: fuertes abusos, abandonos y profundidad estoc√°stica. </li><li>  Tome el modelo del alumno, √∫selo como maestro en el paso 2 y repita el proceso.El conjunto de datos se equilibra de acuerdo con las clases de la siguiente manera.  Para empezar, tomamos EfficientNet-B0, entrenado en la imagen, expulsamos sus predicciones en el conjunto de datos JFT.  Luego tomaron esos ejemplos para los cuales la confianza m√°xima es superior a 0.3.  Para cada clase, se tomaron 130K im√°genes (si despu√©s de filtrar por 0.3 trashhold eran menos, duplicadas, si es que m√°s, tomadas de acuerdo con los alcances de predicado m√°s altos).  Recibi√≥ 130 millones de im√°genes, emisiones duplicadas, 81 millones restantes </li></ol><br><p>  <strong>Arquitectura:</strong> <br>  EfficeintNet, adem√°s, el modelo de estudiante toma un modelo de maestro mucho m√°s gordo.  Tambi√©n escanearon EfficientNet a EfficientNet-L0 / L1 / L2, lo que result√≥ en un modelo L2 con par√°metros de 480M (Resnet50 tiene 26M par√°metros, en comparaci√≥n) </p><br><p>  <strong>Proceso de aprendizaje:</strong> <br>  Butchesize 2048. El modelo Sota L2 ense√±√≥ 350 eras.  El modelo L2 m√°s grande estudiado en este modo durante 3.5 d√≠as en Cloud TPU v3 Pod con 2048 n√∫cleos. </p><br><p>  <strong>Procedimiento de aprendizaje iterativo:</strong> <br>  Al principio ense√±aron B7 tanto como estudiante como profesor.  Luego, usando a B7 como maestros, le ense√±aron al gordo L0 como estudiante.  Luego, cambiando sus lugares de esta manera, llegamos al modelo L2, que al final utilizamos como maestro para el mismo modelo L2. Resultado :: sota: con 2 veces menos par√°metros del modelo en comparaci√≥n con la celda anterior (FixRes ResNeXt-101 WSL 829 millones de par√°metros) </p><br><p>  Tambi√©n obtuve muy buenos <strong>resultados</strong> en ImageNet-A / C / P </p><br><img src="https://habrastorage.org/webt/ht/me/ce/htmeceti9jluqoyj84uqcy2fibo.png"><br><p><br></p><br><h3 id="4-momentum-contrast-for-unsupervised-visual-representation-learning">  4. Contraste de momento para el aprendizaje de la representaci√≥n visual sin supervisi√≥n </h3><br><p>  Autores del art√≠culo: Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick (Facebook, 2019) <br>  <a href="https://arxiv.org/abs/1911.05722">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Arseny Kravchenko (en slack arsenyinfo) </p><br><p>  SotA es un pre-entrenamiento sin supervisi√≥n para varias tareas de visi√≥n por computadora (desde la clasificaci√≥n hasta la estimaci√≥n de pose densa), probado en diferentes conjuntos de datos (imagenet, instagram) y tareas principales (imagenet, COCO, paisajes urbanos, LVIS, etc.). </p><br><img src="https://habrastorage.org/webt/li/5p/bn/li5pbnzez-zowzce2movvxthfea.png"><br><p><br></p><br><p>  ¬øC√≥mo se realiza el preentrenamiento sin supervisi√≥n?  Se nos ocurre alg√∫n tipo de tarea para la que no se necesitan etiquetas, aprender el codificador, congelarlo y luego resolver el problema principal agregando las capas faltantes (lineal para clasificaci√≥n, decodificadores para segmentaci√≥n, etc.).  Una de las tareas m√°s populares en este nicho es la discriminaci√≥n de instancias, basada en la p√©rdida de contraste, es decir.  queremos que las caracter√≠sticas de diferentes aumentos de la misma imagen est√©n cerca una de la otra (por ejemplo, en t√©rminos de distancia del coseno), y las caracter√≠sticas de las diferentes est√°n muy lejos. </p><br><p>  Puede intentar ense√±ar esta tarea de principio a fin, pero mucho depende del tama√±o del lote: la calidad depende en gran medida de la variedad de ejemplos dentro del lote.  Los experimentos muestran que al aumentar el tama√±o del lote, la calidad final mejora.  Pero el lote es algo similar a Mosc√∫: no es de goma, no funcionar√° durante mucho tiempo para aumentarlo en la frente. </p><br><p>  Los tipos anteriores tipos cercanos a la celda arruinaron un banco de memoria: las caracter√≠sticas de lotes anteriores se almacenaron por separado en la memoria y tambi√©n se usaron para generar los negativos, es decir  Muestras diferentes.  Esto ayud√≥ en parte, pero tambi√©n de manera imperfecta: durante el entrenamiento, los pesos del codificador cambian y las caracter√≠sticas antiguas se deterioran. </p><br><p>  Finalmente, la idea del art√≠culo: </p><br><ol><li>  Reemplacemos un banco de memoria simple con una cola donde se encontrar√°n caracter√≠sticas bastante nuevas; </li><li>  Mantendremos dos versiones del codificador: una se usa para el lote actual y est√° entrenada, y la otra es m√°s estable, sus pesos se actualizan desde la primera versi√≥n, pero con un gran impulso; </li><li>  Las caracter√≠sticas del lote se consideran el primer codificador, las caracter√≠sticas en la cola se cuentan por el segundo codificador. </li></ol><br><p>  Este enfoque permite acercarse a la calidad de la capacitaci√≥n de extremo a extremo, pero, gracias a la larga alineaci√≥n, logra los resultados potenciales de un lote irrealmente grande.  De esta forma obtienes m√©tricas interesantes para diferentes tareas, incluidas  en algunos lugares es incluso un poco mejor que la imagen tradicional supervisada en el imaginet. </p><br><h3 id="5-benchmarking-neural-network-robustness-to-common-corruptions-and-perturbations">  5. Evaluaci√≥n de la robustez de la red neuronal frente a las corrupciones y perturbaciones comunes </h3><br><p>  Autores: Dan Hendrycks, Thomas Dietterich (Universidad de California, Oregon State University, 2019) <br>  <a href="https://arxiv.org/abs/1903.12261">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Vladimir Iglovikov (en flojo ternaus) </p><br><img src="https://habrastorage.org/webt/fy/p3/zn/fyp3znumddg9tstty7aukiiuvwg.png" width="500" height="250"><br><p><br></p><br><p>  Fue aceptado en ICLR 2019 y, seg√∫n tengo entendido, este es uno de esos trabajos de DL que no ha sido capacitado en ninguna red. </p><br><p>  La tarea era as√≠, pero intentemos el aumento para la validaci√≥n de ImageNet, pero entrenaremos en la continua.  Adem√°s, a diferencia del adevrsarial, no tenemos la tarea de hacer que las transformaciones sean peque√±as e invisibles a la vista. </p><br><p>  <strong>Lo que se ha hecho:</strong> </p><br><ol><li>  Se ha seleccionado un conjunto de aumentos.  Los autores dicen que este es el m√°s com√∫n, pero, en mi opini√≥n, mienten. <br>  Utilizaron: ruido gaussiano, ISONoise, Downscale, Defocus, MotionBlur, ZoomBlur, FrostedGlassBlur, JpegCompression, Snow, Fog, Rain, Elastic transoform, etc. </li><li>  Todas estas transformaciones se han aplicado a la validaci√≥n de ImageNet.  El conjunto de datos resultante se denomin√≥ ImageNet-C </li><li>  Tambi√©n se propuso una variaci√≥n llamada ImageNet-P en la que se aplicaron conjuntos de transformaciones de diferentes fuerzas a cada imagen. </li><li>  Se propuso una m√©trica para evaluar la estabilidad del modelo. </li><li>  Se evaluaron varios modelos en el contexto de esta m√©trica: AlexNet, VGG-11, VGG-19, Resnet-50, Resnet-18, VGG-19 + BN, etc. </li></ol><br><p>  <strong>Conclusiones:</strong> </p><br><ol><li>  Cuanto m√°s fuerte es el aumento, m√°s sufre la precisi√≥n del modelo.  : capitan_obvious: </li><li>  Cuanto m√°s complejo es el modelo, m√°s estable. </li><li>  Aplicar CLAHE en im√°genes antes de la inferencia ayuda un poco. </li><li>  cuentan con bloques de agregaci√≥n como la ayuda de DenseNet o Resnext. </li><li>  Las redes que tienen multiescala son m√°s estables.  Un ejemplo de tales redes es MSDNet, Multigrid (no he o√≠do hablar de tales redes) </li></ol><br><p>  <a href="https://github.com/hendrycks/robustness">C√≥digo</a> </p><br><h3 id="6-distilbert-a-distilled-version-of-bert-smaller-faster-cheaper-and-lighter">  6. DistilBERT, una versi√≥n destilada de BERT: m√°s peque√±a, m√°s r√°pida, m√°s barata y m√°s ligera. </h3><br><p>  Autores: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf (Hugging Face, 2019) <br>  <a href="https://arxiv.org/abs/1910.01108">‚Üí Art√≠culo original</a> <br>  Autor de la revisi√≥n: Yuri Kashnitsky (en yorko slack) </p><br><p>  El art√≠culo es corto, es muy f√°cil de leer.  Al principio, algunas palabras generales sobre la carrera armamentista en la PNL y la huella ambiental.  Adem√°s, la idea de la destilaci√≥n (y Hinton tambi√©n lo hizo aqu√≠.) En la tarea de modelar el lenguaje, predecimos de manera est√°ndar la siguiente palabra en contexto.  Por lo general, la p√©rdida de entrop√≠a cruzada compara el vector de probabilidades pronosticadas (la longitud del diccionario completo) con un vector binario, donde solo hay una unidad que indica la palabra real en un lugar determinado en el conjunto de entrenamiento.  Es decir, el segundo, el tercero, etc.  La palabra ignora la palabra que el modelo considera apropiada.  Se da un ejemplo en el art√≠culo: "Creo que este es el comienzo de una bella [M√ÅSCARA]", en lugar de [M√ÅSCARA] BERT quiere sustituir en primer lugar todo el d√≠a o la vida, pero las palabras predichas por futuro futuro, historia y mundo tambi√©n son buenas.  ¬øPodemos de alguna manera tener en cuenta el hecho de que el modelo produce una buena distribuci√≥n de probabilidad?  En t√©rminos generales, premiar al modelo por el hecho de que la parte superior no tiene Murdock, tolerancia, maternidad y otras pocas palabras adecuadas. </p><br><img src="https://habrastorage.org/webt/wg/xd/rx/wgxdrxth-vykjuhkxszaxakxdke.png" width="500" height="250"><br><p><br></p><br><p>  <strong>La idea de la destilaci√≥n.</strong> <br>  La idea de un esquema espec√≠fico de maestro-alumno es que tenemos un gran modelo de <strong>maestro</strong> ( <strong>maestro</strong> , BERT) y un modelo m√°s peque√±o ( <strong>alumno</strong> , DistilBERT), que transmitir√° el "conocimiento" del modelo de maestro.  El modelo del alumno optimizar√° la p√©rdida de destilaci√≥n, es decir, la p√©rdida de entrop√≠a cruzada, definida para las distribuciones de probabilidad del profesor y el alumno: L = Œ£ t_i * log (s_i).  Es decir, para una palabra espec√≠fica borrada por el s√≠mbolo [M√ÅSCARA], y que debe predecirse por contexto, comparamos dos distribuciones de probabilidad de la aparici√≥n de cada palabra del diccionario: {t_i} y {s_i} - predichas, respectivamente, por el modelo y modelo del maestro estudiante  Por lo tanto, se obtiene una se√±al rica de entrenamiento: el modelo de estudiante en cada palabra recibe una se√±al calculada no solo al comparar su vector de pron√≥stico con la palabra real en la muestra de entrenamiento, sino al compararlo con el vector de pron√≥stico del modelo de maestro. </p><br><p>  <strong>Modelo DistilBERT</strong> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La idea es que el alumno es un modelo m√°s peque√±o que el profesor. </font><font style="vertical-align: inherit;">Entonces DistilBERT es generalmente el mismo BERT, pero la mitad de las capas se han tirado. </font><font style="vertical-align: inherit;">Tambi√©n arrojaron incrustaciones de tipo token y agrupadores, sin embargo, no hay detalles sobre esto. </font><font style="vertical-align: inherit;">Pero lo principal es que DistilBERT es un 40% menos: 66 millones de par√°metros en comparaci√≥n con 110 millones para BERT</font></font></p><br><p> <strong> DistilBERT</strong> <br>  DistilBERT  distillation loss     ‚Äî   masked language modeling loss,    BERT   cosine embedding loss ‚Äî           ( ,  ,      "" -   ,  "" ). :   ablation studies, ,   masked language modeling loss,    , ..    distillation loss  cosine embedding loss.   ,    RoBERTa   next sentence prediction   dynamic masking. </p><br><p>      ,  BERT (eng. wiki + Toronto Book Corpus) 90   8 V100 (16 GB).   RoBERTa    1024 V100 (32 GB). </p><br><p> <strong></strong> <br>     BERT ‚Äî "it performed surprisingly well",        DistilBERT ‚Äî  GLUE  surprisingly well ‚Äî     5  9   ,  BERT ,     SQuAD  IMDb ‚Äî  .   ,    DistilBERT   60% ‚Äî  . </p><br><p> <strong> </strong> <br>   DistilBERT  iPhone 7 Plus.   70% ,  BERT-base (  ),     200 .  ablation studies:     ,      ‚Äî distillation loss  cosine embedding loss. </p><br><p>      3          ,  DistilBERT ‚Äî     BERT,   40%  ,   60%    "97%   "    BERT (        ML). </p><br><p> -,      BERT,     . </p><br><p>  <strong>Materiales adicionales:</strong> <br> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"> Jay Alammar</a> <br> <a href="https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews">  , DistilBERT + Catalyst:   </a> </p><br><h3 id="7-plug-and-play-language-models-a-simple-approach-to-controlled-text-generation"> 7. Plug and Play Language Models: A Simple Approach To Controlled Text Generation </h3><br><p>  : Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu (Uber AI, Caltech, HKUST, 2019) <br> <a href="https://arxiv.org/abs/1912.02164">‚Üí  </a> <br>  :   (  Egor Timofeev) </p><br><p>               . ,           / /      (, .  <a href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a> ).     ,         ,     , ,      . </p><br><p> <strong></strong> <br>       (   x_prev    ),        p(x),      conditional LM (,    ‚Äî CTRL)    p(x|a). </p><br><p>       : p(x|a) ‚àù p(x)p(a|x),  p(x)  ,    (, GPT2),  p(a|x) ‚Äî     .       ‚Äî       ,   /.     ,       ,    . </p><br><p>   <strong></strong> : </p><br><ol><li>    ,  log(p(a|x)) ( ).     hidden state  . </li><li>      ,  hidden state      log(p(a|x)).   H_new. </li><li>   :           p(x).    ,    : -,        KL(H, H_new),  -,  .. post-norm fusion ( <a href="https://arxiv.org/pdf/1809.00125.pdf">https://arxiv.org/pdf/1809.00125.pdf</a> ),   p(x)   non conditional LM  ,     . </li><li>      . </li></ol><br><p>           ,  p(a|x). </p><br><p> <strong></strong> <br>       ,   -            topic relevance.    :  (GPT2) &lt;  +    &lt;&lt;       &lt;    + . </p><br><img src="https://habrastorage.org/webt/jx/zm/ye/jxzmyeaubsu6wtcp2np1zda32tk.png" width="500" height="250"><br><p><br></p><br><h3 id="8-deep-salience-representation-for-f0-estimation-in-polyphonic-music"> 8. Deep Salience Representation for F0 Estimation in Polyphonic Music </h3><br><p>  : Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan Pablo Bello ( New York University, USA, 2017) <br> <a href="https://bmcfee.github.io/papers/ismir2017_salience.pdf">‚Üí  </a> <br>  :   (  nglaz) </p><br><p>    .  ,                .        ,     ‚Äì    .       ,   -   .   constant-Q ,          (      )          . </p><br><img src="https://habrastorage.org/webt/7l/6y/c0/7l6yc0irzsti2kbks7avmvrb7w4.png" width="500" height="250"><br><p>     .  constant-Q     -   f_min  -    F.    f_min   f_min * h,      ,    ,     .    h   {0.5, 1, 2, 3, 4, 5},        .   ,          3- ,        2-  3-    (, ,  ). ,    ,     ,    ,   (0.5f, f, 2f, 3f, 4f, 5f),    .     ( 55)      .         ,           ,  dilated-. </p><br><p>  , ,     constant-Q       F,           . </p><br><p>    F0 estimation,    ,          .  2017 ,   ,   state-of-the-art.           ,      . </p><br><h3 id="9-analyzing-and-improving-the-image-quality-of-stylegan"> 9. Analyzing and Improving the Image Quality of StyleGAN </h3><br><p>  : Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila (NVIDIA, 2019) <br> <a href="http://arxiv.org/abs/1912.04958">‚Üí  </a> <br>  :   (  shiron8bit) </p><br><p> GAN-      ,     ,         .     ,   ,      ,   ,    ( FID)   : </p><br><ul><li>   droplet-like  (    / ),  AdaIN. </li><li>   ,   ProGAN-    /       end-to-end     MSG-GAN.     ,        /,            . </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Se agreg√≥ la regularizaci√≥n de longitud de ruta. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sugirieron un procedimiento para detectar im√°genes generadas: encontramos la proyecci√≥n de la imagen en el espacio latente W, si la imagen reconstruida est√° cerca del original, entonces es m√°s probable que se genere a trav√©s de stylegan2. </font></font></li></ul><br><img src="https://habrastorage.org/webt/f6/v3/7p/f6v37pcy3wcpw0epu5rz1-r24qk.png" width="500" height="250"><br><p><br></p><br><p>  <strong>Artefactos de gotas y AdaIN</strong> <br>  Los autores del art√≠culo dan el siguiente argumento en contra del uso de la capa AdaIN: adain normaliza cada mapa de caracter√≠sticas, destruyendo as√≠ la informaci√≥n sobre los valores de magnitud relativos entre s√≠, y la gotita es un intento del generador de impulsar esta informaci√≥n de una manera diferente.  Como una opci√≥n para debilitar AdaIN, se propuso lo siguiente: haremos todo el escalado (modulaci√≥n / demodulaci√≥n) directamente en la convoluci√≥n, en funci√≥n del estilo proveniente del bloque A, y el desplazamiento de la se√±al saliente (en lugar de mu (y) / y_ {b, i} en AdaIN) Deje que el bloque B transforme el ruido.  Esta innovaci√≥n al mismo tiempo permiti√≥ acelerar el entrenamiento en las mismas condiciones. </p><br><p>  <strong>Fracaso de ProGAN</strong> <br>  En el art√≠culo sobre MSG-GAN, se propuso utilizar conexiones de omisi√≥n, conectando bloques generadores coincidentes y bloques discriminadores por resoluci√≥n.  Los autores de Stylegan desarrollaron esta idea resumiendo las salidas de los bloques generadores de todas las resoluciones (con muestreo ascendente) y alimentando la versi√≥n de la imagen con submuestreo correspondiente a la entrada de cada bloque discriminador.  Se sugiri√≥ que los bloques residuales se usaran como la segunda opci√≥n, mientras que las conexiones omitidas en el generador y los bloques residuales en el discriminador mostraron los mejores resultados (el discriminador es similar a LAPGAN, pero sin discriminadores para cada resoluci√≥n, los mapas de caracter√≠sticas se reenv√≠an a√∫n m√°s). Como en el caso de ProGAN, en las iteraciones iniciales, las partes de la cuadr√≠cula responsables de las resoluciones m√°s bajas y la imagen general hacen una mayor contribuci√≥n, y luego el √©nfasis se transfiere a peque√±os detalles. </p><br><p>  <strong>Regularizaci√≥n de longitud de ruta</strong> <br>  Observando que los valores bajos de FID no siempre dan im√°genes de alta calidad, y tambi√©n observando una correlaci√≥n entre la calidad de imagen y la m√©trica PPL (Longitud de ruta perceptual: inicialmente la diferencia entre las caracter√≠sticas vgg de las im√°genes con peque√±os pasos en Z, pero la diferencia se reemplaz√≥ con LPIPS), los autores propusieron Path Regularizaci√≥n de longitud, que es minimizar la funcionalidad. </p><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy=&quot;false&quot;>(</mo><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>w</mi><mo stretchy=&quot;false&quot;>)</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.258ex" height="2.78ex" viewBox="0 -883.9 9583.3 1197.1" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-4A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-54" x="929" y="488"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-77" x="785" y="-212"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-79" x="1255" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMAIN-3D" x="2030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-6E" x="3336" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-61" x="3937" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-62" x="4466" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-6C" x="4896" y="0"></use><g transform="translate(5194,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-77" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMAIN-28" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-67" x="6720" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMAIN-28" x="7200" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-77" x="7590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMAIN-29" x="8306" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMATHI-79" x="8696" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/company/ods/blog/485122/&amp;xid=25657,15700022,15700186,15700190,15700256,15700259,15700262,15700265,15700271&amp;usg=ALkJrhjlOVoW1lOD6A0SMHiH8aQbcjKp0Q#MJMAIN-29" x="9193" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>J</mi><mi>w</mi><mi>T</mi></msubsup><mi>y</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>n</mi><mi>a</mi><mi>b</mi><mi>l</mi><msub><mi>a</mi><mi>w</mi></msub><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> J ^ T_w y = \ nabla_w (g (w) y) </script></p><br>  donde g es el generador en s√≠, J_w es el jacobiano en variables de espacio latente.  Al mismo tiempo, los c√°lculos jacobianos se pueden hacer a trav√©s de backprop, y tambi√©n se dice que para facilitar los c√°lculos, el regularizador se puede contar solo por cada 16 lotes.  El n√∫mero a se calcula como el promedio m√≥vil exponencial de la norma jacobiana. El uso de la regularizaci√≥n de la longitud de la ruta permite una interpolaci√≥n m√°s 'uniforme' del espacio oculto W, que, adem√°s de mejorar la calidad de la imagen, puede mejorar la reversibilidad (es decir, encontrar w que proporciona una imagen dada despu√©s de pasar por el generador), y tambi√©n abre perspectivas en t√©rminos de animaci√≥n e interpolaci√≥n entre fotogramas clave (en la nueva arquitectura, entre proyecciones de im√°genes similares deber√≠a haber puntos responsables de im√°genes cercanas  I).  La introducci√≥n de esta regularizaci√≥n tambi√©n ha jugado un papel en la simplificaci√≥n de la detecci√≥n de im√°genes generadas por esta arquitectura. <br><p>  El tiempo de entrenamiento para 8 GPU a una resoluci√≥n de 1024 * 1024 fue de 2 a 9 d√≠as para diferentes configuraciones. </p></div></div><p>Source: <a href="https://habr.com/ru/post/485122/">https://habr.com/ru/post/485122/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../485104/index.html">Hacer una llave RFID universal para intercomunicadores</a></li>
<li><a href="../485108/index.html">Estad√≠sticas de especialistas certificados de PMI en Rusia el 10/01/2020</a></li>
<li><a href="../485110/index.html">Mi experiencia de trabajo remoto efectivo</a></li>
<li><a href="../485118/index.html">C√≥digo limpio de Robert Martin. Resumen ¬øC√≥mo escribir un c√≥digo claro y hermoso?</a></li>
<li><a href="../485120/index.html">Agregue una API JSON muy r√°pida a nuestra aplicaci√≥n.</a></li>
<li><a href="../485124/index.html">Pruebas puras en PHP y PHPUnit</a></li>
<li><a href="../485126/index.html">Mu-mu, woof-woof, quack-quack: evoluci√≥n de la comunicaci√≥n ac√∫stica</a></li>
<li><a href="../485128/index.html">Ahorre en licencias Mikrotik CHR</a></li>
<li><a href="../485132/index.html">√önete al Festival de juegos independientes de Google Play</a></li>
<li><a href="../485136/index.html">Rastreo y monitoreo de Istio: microservicios y el principio de incertidumbre</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>