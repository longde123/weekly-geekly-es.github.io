<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöÉ üö≥ üßíüèª Wie hat sich der Kosaken-Retro-Wettbewerb entschieden? üé∞ ‚è∏Ô∏è üßóüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Fr√ºhjahr fand ein bedeutender OpenAI Retro Contest statt, der sich dem verst√§rkten Lernen, Meta-Lernen und nat√ºrlich Sonic widmete. Unser Te...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie hat sich der Kosaken-Retro-Wettbewerb entschieden?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/421585/"><p>  In diesem Fr√ºhjahr fand ein bedeutender OpenAI Retro Contest statt, der sich dem verst√§rkten Lernen, Meta-Lernen und nat√ºrlich Sonic widmete.  Unser Team belegte den 4. Platz von √ºber 900 Teams.  Das Trainingsfeld mit Verst√§rkung unterscheidet sich geringf√ºgig vom normalen maschinellen Lernen, und dieser Wettbewerb unterschied sich von einem typischen RL-Wettbewerb.  Ich frage nach Details unter Katze. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sx/pt/0r/sxpt0rdvj5g3xn6eohh6vmusj1o.jpeg" alt="Bild"></div><br><hr><a name="habracut"></a><br><h2 id="tldr">  TL; DR </h2><br><p>  Eine richtig abgestimmte Grundlinie ben√∂tigt keine zus√§tzlichen Tricks ... praktisch. </p><br><h2 id="intro-v-obuchenie-s-podkrepleniem">  Einf√ºhrung in das Verst√§rkungstraining </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/nb/vn/yxnbvndwcscdelo4q_m7zxxhfsu.png" alt="Bild"></div><br><p>  Verst√§rktes Lernen ist ein Bereich, der die Theorie der optimalen Kontrolle, die Spieltheorie, die Psychologie und die Neurobiologie kombiniert.  In der Praxis wird das verst√§rkte Lernen verwendet, um Entscheidungsprobleme zu l√∂sen und nach optimalen Verhaltensstrategien oder Richtlinien zu suchen, die f√ºr eine ‚Äûdirekte‚Äú Programmierung zu komplex sind.  In diesem Fall wird der Agent in der Geschichte der Interaktionen mit der Umgebung geschult.  Die Umgebung, die wiederum die Aktionen des Agenten bewertet, bietet ihm eine Belohnung (Skalar) - je besser sich der Agent verh√§lt, desto h√∂her ist die Belohnung.  Infolgedessen wird die beste Richtlinie von dem Agenten gelernt, der gelernt hat, die Gesamtbelohnung f√ºr die gesamte Zeit der Interaktion mit der Umgebung zu maximieren. </p><br><p>  Als einfaches Beispiel k√∂nnen Sie BreakOut spielen.  In diesem guten alten Spiel der Atari-Serie muss eine Person / ein Agent die untere horizontale Plattform kontrollieren, den Ball schlagen und damit allm√§hlich alle oberen Bl√∂cke brechen.  Je mehr niedergeschlagen - desto gr√∂√üer die Belohnung.  Dementsprechend sieht eine Person / ein Agent ein Bildschirmbild und es ist notwendig, eine Entscheidung zu treffen, in welche Richtung die untere Plattform bewegt werden soll. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/an/ke/wiankeye_8wb0cxtrhpsfdf5sdm.gif" alt="Bild"></div><br><p>  Wenn Sie sich f√ºr das Thema Verst√§rkungstraining interessieren, empfehle ich Ihnen einen coolen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Einf√ºhrungskurs von HSE</a> sowie dessen detaillierteres <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Open-Source-Gegenst√ºck</a> .  Wenn Sie etwas wollen, das Sie lesen k√∂nnen, aber mit Beispielen - ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch,</a> das von diesen beiden Kursen inspiriert ist.  Ich habe all diese Kurse √ºberpr√ºft / abgeschlossen / mitgestaltet und wei√ü daher aus eigener Erfahrung, dass sie eine hervorragende Grundlage bieten. </p><br><h2 id="pro-zadachu">  √úber die Aufgabe </h2><br><p>  Das Hauptziel dieses Wettbewerbs war es, einen Agenten zu finden, der in der SEGA-Reihe gut spielen kann - Sonic The Hedgehog.  OpenAI begann gerade damit, Spiele von SEGA in seine Plattform f√ºr die Schulung von RL-Agenten zu importieren, und beschloss daher, diesen Moment ein wenig zu f√∂rdern.  Sogar der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel wurde</a> mit dem Ger√§t von allem und einer detaillierten Beschreibung der grundlegenden Methoden ver√∂ffentlicht. </p><br><p>  Alle 3 Sonic-Spiele wurden mit jeweils 9 Levels unterst√ºtzt, bei denen Sie durch Schnippen einer tr√§nenreichen Tr√§ne sogar spielen und sich an Ihre Kindheit erinnern konnten (nachdem Sie sie zuerst bei Steam gekauft hatten). </p><br><p>  Als Zustand der Umgebung (was der Agent sah) war das Bild vom Simulator ein RGB-Bild, und als Aktion wurde der Agent aufgefordert, auszuw√§hlen, welche Taste auf dem virtuellen Joystick gedr√ºckt werden soll - springen / links / rechts und so weiter.  Der Agent erhielt Belohnungspunkte sowie im urspr√ºnglichen Spiel, d.h.  zum Sammeln von Ringen sowie f√ºr die Geschwindigkeit des Passierens des Levels.  Tats√§chlich hatten wir einen Original-Schall vor uns, nur war es notwendig, ihn mit Hilfe unseres Agenten durchzugehen. </p><br><p>  Der Wettbewerb fand vom 5. April bis 5. Juni statt, d.h.  nur 2 Monate, was ziemlich klein genug scheint.  Unser Team konnte erst im Mai zusammenkommen und am Wettbewerb teilnehmen, wodurch wir unterwegs viel lernen konnten. </p><br><h2 id="baselines">  Baselines </h2><br><p>  Als Basis wurden vollst√§ndige Schulungsleitf√§den f√ºr das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Regenbogentraining</a> (DQN-Ansatz) und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PPO</a> (Policy Gradient-Ansatz) auf einer der m√∂glichen Ebenen in Sonic und die √úbermittlung des resultierenden Agenten angegeben. </p><br><p>  Die Rainbow-Version basierte auf dem wenig bekannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anyrl-</a> Projekt, aber PPO verwendete die guten alten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Baselines</a> von OpenAI und schien uns viel vorzuziehen. </p><br><p>  Die ver√∂ffentlichten Basislinien unterschieden sich von den im Artikel beschriebenen Ans√§tzen durch ihre gr√∂√üere Einfachheit und kleinere Mengen von ‚ÄûHacks‚Äú, um das Lernen zu beschleunigen.  So haben die Organisatoren Ideen geworfen und die Richtung festgelegt, aber die Entscheidung √ºber die Verwendung und Umsetzung dieser Ideen wurde dem Teilnehmer des Wettbewerbs √ºberlassen. </p><br><p>  In Bezug auf Ideen m√∂chte ich OpenAI f√ºr die Offenheit und John Schulman f√ºr die Ratschl√§ge, Ideen und Vorschl√§ge danken, die er zu Beginn dieses Wettbewerbs ge√§u√üert hat.  Wir, wie viele Teilnehmer (und vor allem Neulinge in der RL-Welt), konnten uns so besser auf das Hauptziel des Wettbewerbs konzentrieren - Meta-Lernen und Verbesserung der Generalisierung von Agenten, √ºber die wir jetzt sprechen werden. </p><br><h2 id="osobennosti-ocenivaniya-resheniy">  Merkmale der Entscheidungsbewertung </h2><br><p>  Das Interessanteste begann zum Zeitpunkt der Bewertung der Agenten.  In typischen RL-Wettbewerben / Benchmarks werden Algorithmen in derselben Umgebung getestet, in der sie trainiert wurden. Dies tr√§gt zu Algorithmen bei, die sich gut erinnern k√∂nnen und viele Hyperparameter aufweisen.  Im selben Wettbewerb wurde der Test des Algorithmus auf den neuen Sonic-Levels durchgef√ºhrt (die niemandem gezeigt wurden), die vom OpenAI-Team speziell f√ºr diesen Wettbewerb entwickelt wurden.  Die Kirsche auf dem Kuchen war die Tatsache, dass das Mittel w√§hrend des Testprozesses auch w√§hrend des Durchgangs des Levels eine Belohnung erhielt, die es erm√∂glichte, direkt im Testprozess umzuschulen.  In diesem Fall sollte jedoch beachtet werden, dass die Tests sowohl zeitlich - 24 Stunden als auch in Spielzecken - auf 1 Million begrenzt waren.  Gleichzeitig unterst√ºtzte OpenAI nachdr√ºcklich die Erstellung von Agenten, die schnell neue Ebenen erreichen k√∂nnen.  Wie bereits erw√§hnt, war das Erhalten und Studieren solcher L√∂sungen das Hauptziel von OpenAI w√§hrend dieses Wettbewerbs. </p><br><p>  Im akademischen Umfeld wird die Richtung des Studiums von Richtlinien, die sich schnell an neue Bedingungen anpassen k√∂nnen, als Meta-Lernen bezeichnet und hat sich in den letzten Jahren aktiv weiterentwickelt. </p><br><p>  Im Gegensatz zu den √ºblichen Kaggle-Wettbewerben, bei denen die gesamte Einreichung auf das Senden Ihrer Antwortdatei hinausl√§uft, musste das Team bei diesem Wettbewerb (und in der Tat bei RL-Wettbewerben) seine L√∂sung in einen Docker-Container mit der angegebenen API einwickeln, sammeln und senden Docker-Bild.  Dies erh√∂hte die Schwelle f√ºr die Teilnahme am Wettbewerb, machte den Entscheidungsprozess jedoch viel ehrlicher - die Ressourcen und die Zeit f√ºr das Docker-Image waren begrenzt, zu schwere und / oder langsame Algorithmen haben die Auswahl einfach nicht bestanden.  Es scheint mir, dass dieser Ansatz zur Bewertung viel vorzuziehen ist, da er Forschern ohne einen ‚ÄûHeimatcluster aus DGX und AWS‚Äú erm√∂glicht, mit Liebhabern von Glas-50000-Modellen auf Augenh√∂he zu konkurrieren.  Ich hoffe, in Zukunft mehr von dieser Art von Wettbewerb zu sehen. </p><br><h2 id="komanda">  Das Team </h2><br><p>  Kolesnikov Sergey ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Scitator</a> ) <br>  RL-Enthusiast.  Zum Zeitpunkt des Wettbewerbs schrieb und verteidigte ein Student am Moskauer Institut f√ºr Physik und Technologie (MIPT) ein Diplom des letztj√§hrigen NIPS: Learning to Run-Wettbewerbs (ein Artikel, √ºber den auch geschrieben werden sollte). <br>  Senior Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dbrain</a> - Wir bringen produktionsbereite Wettbewerbe mit Docker und begrenzten Ressourcen in die reale Welt. </p><br><p>  Pawlow Michail ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fgvbrt</a> ) <br>  Senior Research <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Developer DiphakLab</a> .  Wiederholte Teilnahme und gewann Preise bei Hackathons und verst√§rkten Trainingswettbewerben. </p><br><p>  Sergeev Ilya ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sergeevii123</a> ) <br>  RL-Enthusiast.  Ich habe einen von Deephacks RL-Hackathons geschlagen und alles begann.  Data Scientist @ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Avito.ru</a> - Computer Vision f√ºr verschiedene interne Projekte. </p><br><p>  Sorokin Ivan ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">1ytic</a> ) <br>  Spracherkennung in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">languagepro.ru</a> . </p><br><h2 id="podhody-i-reshenie">  Ans√§tze und L√∂sung </h2><br><p>  Nach einem schnellen Test der vorgeschlagenen Baselines fiel unsere Wahl auf den OpenAI-Ansatz - PPO - als eine formellere und interessantere Option f√ºr die Entwicklung unserer L√∂sung.  Dar√ºber hinaus hat der PPO-Agent nach seinem Artikel f√ºr diesen Wettbewerb die Aufgabe etwas besser gemeistert.  Aus demselben Artikel wurden die ersten Verbesserungen geboren, die wir in unserer L√∂sung verwendet haben, aber das Wichtigste zuerst: </p><br><ol><li><p>  Kollaboratives PPO-Training auf allen verf√ºgbaren Ebenen </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/oc/lx/iaoclxlyzkgmdgiqkkksiwzcoli.png" alt="Bild"></div><br><p>  Die angelegte Basislinie wurde nur auf einer der verf√ºgbaren 27 Sonic-Ebenen trainiert.  Mit Hilfe kleiner Modifikationen war es jedoch m√∂glich, das Training sofort auf alle 27 Stufen zu parallelisieren.  Aufgrund der gr√∂√üeren Vielfalt im Training hatte der resultierende Agent eine viel gr√∂√üere Verallgemeinerung und ein besseres Verst√§ndnis des Ger√§ts der Sonic-Welt und kam daher um eine Gr√∂√üenordnung besser zurecht. </p><br></li><li><p>  Zus√§tzliches Training w√§hrend des Testens <br>  Um auf die Hauptidee des Wettbewerbs, das Meta-Lernen, zur√ºckzukommen, war es notwendig, einen Ansatz zu finden, der die maximale Verallgemeinerung aufweist und sich leicht an neue Umgebungen anpassen l√§sst.  Und zur Anpassung war es notwendig, den vorhandenen Agenten f√ºr die Testumgebung neu zu schulen, was tats√§chlich durchgef√ºhrt wurde (auf jeder Teststufe unternahm der Agent 1 Million Schritte, was ausreichte, um sich an eine bestimmte Ebene anzupassen).  Am Ende jedes Testspiels bewertete der Agent die erhaltene Auszeichnung und optimierte seine Richtlinien anhand der gerade erhaltenen Geschichte.  Es ist wichtig anzumerken, dass es bei diesem Ansatz wichtig ist, nicht alle Ihre bisherigen Erfahrungen zu vergessen und sich unter bestimmten Bedingungen nicht zu verschlechtern, was im Wesentlichen das Hauptinteresse des Meta-Lernens ist, da ein solcher Agent sofort seine gesamte vorhandene F√§higkeit zur Verallgemeinerung verliert. </p><br></li><li><p>  Explorationsboni <br>  Der Agent ging tief in die Verg√ºtungsbedingungen f√ºr ein Level ein und erhielt eine Belohnung daf√ºr, dass er sich entlang der x-Koordinate vorw√§rts bewegte. Er konnte auf einigen Levels stecken bleiben, wenn man zuerst vorw√§rts und dann zur√ºck musste.  Es wurde beschlossen, die Belohnung f√ºr den Agenten, die sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">z√§hlbasierte Erkundung</a> , zu erg√§nzen, wenn der Agent eine kleine Belohnung erhielt, wenn er sich in einem Zustand befand, in dem er sich noch nicht befand.  Es wurden zwei Arten von Explorationsboni implementiert: basierend auf dem Bild und basierend auf der x-Koordinate des Agenten.  Eine Belohnung basierend auf einem Bild wurde wie folgt berechnet: F√ºr jede Pixelposition im Bild wurde gez√§hlt, wie oft jeder Wert f√ºr eine Episode auftrat, die Belohnung war umgekehrt proportional zum Produkt √ºber alle Pixelpositionen hinweg, wie oft die Werte an diesen Positionen f√ºr eine Episode √ºbereinstimmten.  Die auf der x-Koordinate basierende Belohnung wurde auf √§hnliche Weise ber√ºcksichtigt: F√ºr jede x-Koordinate (mit einer bestimmten Genauigkeit) wurde gez√§hlt, wie oft sich der Agent in dieser Koordinate f√ºr die Episode befand. Die Belohnung ist umgekehrt proportional zu diesem Betrag f√ºr die aktuelle x-Koordinate. </p><br></li><li><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verwechslungsexperimente</a> <br>  Beim ‚ÄûUnterrichten mit einem Lehrer‚Äú wurde k√ºrzlich eine einfache, aber effektive Methode zur Datenerweiterung, die sogenannte  Verwechslung.  Die Idee ist sehr einfach: Das Hinzuf√ºgen von zwei beliebigen Eingabebildern erfolgt und eine gewichtete Summe der entsprechenden Beschriftungen wird diesem neuen Bild zugewiesen (z. B. 0,7 <em>Hund + 0,3</em> Katze).  Bei Aufgaben wie Bildklassifizierung und Spracherkennung zeigt die Verwechslung gute Ergebnisse.  Daher war es interessant, diese Methode f√ºr RL zu testen.  Die Augmentation wurde in jeder gro√üen Charge durchgef√ºhrt, die aus mehreren Episoden bestand.  Eingabebilder wurden in Pixel gemischt, aber mit Tags war nicht alles so einfach.  Die Werte return, values ‚Äã‚Äãund neglogpacs wurden durch eine gewichtete Summe gemischt, aber die Aktion (Aktionen) wurde aus dem Beispiel mit dem maximalen Koeffizienten ausgew√§hlt.  Eine solche L√∂sung zeigte keinen sp√ºrbaren Anstieg (obwohl es anscheinend einen Anstieg der Verallgemeinerung geben sollte), verschlechterte jedoch nicht die Basislinie.  Die folgenden Grafiken vergleichen den PPO-Algorithmus mit Verwechslung (rot) und ohne Verwechslung (blau): Oben ist die Belohnung w√§hrend des Trainings, unten ist die L√§nge der Episode. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bk/h-/jb/bkh-jbhhzowcl3gpazkbn8mun2a.jpeg" alt="Bild"></div><br></li><li><p>  Auswahl der besten Anfangspolitik <br>  Diese Verbesserung war eine der letzten und trug wesentlich zum Endergebnis bei.  Auf der Trainingsebene wurden verschiedene Richtlinien mit unterschiedlichen Hyperparametern trainiert.  Auf der Testebene wurde f√ºr die ersten Episoden jede von ihnen getestet, und f√ºr die weitere Schulung wurde die Richtlinie ausgew√§hlt, die die maximale Testbelohnung f√ºr die Episode ergab. </p><br></li></ol><br><h2 id="bloopers">  Patzer </h2><br><p>  Und nun zur Frage, was versucht wurde, aber "nicht geflogen".  Immerhin ist dies kein neuer SOTA-Artikel, um etwas zu verbergen. </p><br><ol><li>  √Ñnderung der Netzwerkarchitektur: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SELU-Aktivierung</a> , Selbstaufmerksamkeit, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SE-Bl√∂cke</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neuroevolution</a> </li><li>  Erstellen Sie Ihre eigenen Sonic-Levels - alles wurde vorbereitet, aber es war nicht genug Zeit </li><li>  Meta-Training durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MAML</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">REPTILE</a> </li><li>  Zusammenstellung mehrerer Modelle und Weiterbildung w√§hrend des Testens jedes Modells unter Verwendung von Wichtigkeitsstichproben </li></ol><br><h2 id="itogi">  Zusammenfassung </h2><br><p>  3 Wochen nach Ende des Wettbewerbs ver√∂ffentlichte OpenAI die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ergebnisse</a> .  Bei 11 zus√§tzlichen, zus√§tzlich erstellten Levels erhielt unser Team einen ehrenwerten 4. Platz, nachdem es in einem √∂ffentlichen Test vom 8. aufgesprungen war und die verdeckten Basislinien von OpenAI √ºberholt hatte. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/om/iw/yvomiwj1jmffvmlicjmhe7vjp9y.png" alt="Bild"></div><br><p>  Die Hauptunterscheidungsmerkmale, die im ersten 3ki ‚Äûgeflogen‚Äú sind: </p><br><ol><li>  Verbessertes Aktionssystem (eigene Schaltfl√§chen, zus√§tzliche entfernt); </li><li>  Untersuchung von Zust√§nden durch Hash aus dem Eingabebild; </li><li>  Mehr Ausbildungsniveau; </li></ol><br><p>  Dar√ºber hinaus m√∂chte ich darauf hinweisen, dass in diesem Wettbewerb neben dem Gewinn auch die Beschreibung ihrer Entscheidungen sowie Materialien, die anderen Teilnehmern geholfen haben, aktiv gef√∂rdert wurden - es gab auch eine separate Nominierung daf√ºr.  Was wiederum den Lampenwettbewerb erh√∂hte. </p><br><h2 id="posleslovie">  Nachwort </h2><br><p>  Ich pers√∂nlich mochte diesen Wettbewerb ebenso wie das Meta-Learning-Thema sehr.  W√§hrend der Teilnahme lernte ich eine gro√üe Liste von Artikeln kennen (einige habe ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht</a> einmal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vergessen</a> ) und lernte eine Vielzahl verschiedener Ans√§tze, die ich hoffentlich in Zukunft anwenden werde. </p><br><p>  In der besten Tradition der Teilnahme am Wettbewerb ist der gesamte Code verf√ºgbar und wird auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">github ver√∂ffentlicht</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de421585/">https://habr.com/ru/post/de421585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de421573/index.html">Food Design Digest August 2018</a></li>
<li><a href="../de421575/index.html">Die Konfrontation zwischen Yandex und Roskomnadzor braut sich zusammen, an einem Tag k√∂nnte die Suchmaschine teilweise blockiert sein</a></li>
<li><a href="../de421577/index.html">Ein Exploit f√ºr eine nicht geschlossene Sicherheitsanf√§lligkeit im Windows Task Scheduler wird ver√∂ffentlicht (√úbersetzung)</a></li>
<li><a href="../de421579/index.html">Organisation der effektiven Interaktion von Mikrodiensten</a></li>
<li><a href="../de421583/index.html">Wo kann man aufs College gehen, um f√ºr einen IT-Spezialisten zu studieren? + Umfrage</a></li>
<li><a href="../de421587/index.html">[Jekaterinburg, Ank√ºndigung] Java Mitap - JUG.EKB</a></li>
<li><a href="../de421589/index.html">Metamorphosen: molekulare Programmierung der Form</a></li>
<li><a href="../de421591/index.html">Budget-System f√ºr die drahtlose (Wi-Fi) autonome Video√ºberwachung (ohne Batterie)</a></li>
<li><a href="../de421593/index.html">SandboxEscaper / PoC-LPE: Was ist drin?</a></li>
<li><a href="../de421595/index.html">Wie IT-Mitarbeiter in den USA und in der EU Arbeit finden: 9 beste Ressourcen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>