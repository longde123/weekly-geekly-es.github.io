<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>„ÄΩÔ∏è üè® üê¢ Combinando projetos em diferentes data centers üëéüèæ ‚õπÔ∏è üéõÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neste artigo, examinaremos por que a abordagem tradicional de combinar redes locais no n√≠vel L2 √© ineficaz com um aumento significativo no n√∫mero de e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Combinando projetos em diferentes data centers</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/selectel/blog/436648/"><img src="https://habrastorage.org/webt/zy/ry/ey/zyryeyqpveem2dyaws3drxmvori.png"><br><br>  Neste artigo, examinaremos por que a abordagem tradicional de combinar redes locais no n√≠vel L2 √© ineficaz com um aumento significativo no n√∫mero de equipamentos, al√©m de mostrar quais problemas conseguimos resolver no processo de combinar projetos localizados em locais diferentes. <br><br><h2>  Circuito L2 normal </h2><br>  √Ä medida que a infraestrutura de TI cresce no data center, os clientes precisam combinar servidores, armazenamento e firewalls em uma √∫nica rede.  Para isso, a Selectel sugere inicialmente o uso de uma rede local. <br><br>  A rede local √© organizada como uma rede cl√°ssica "campus" dentro do mesmo data center, apenas os switches de acesso est√£o localizados diretamente nos racks com os servidores.  Os switches de acesso s√£o ainda combinados em um √∫nico switch de camada de agrega√ß√£o.  Cada cliente pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">solicitar uma conex√£o</a> com a rede local para qualquer dispositivo que ele alugue ou coloque conosco no data center. <br><a name="habracut"></a><br>  Para organizar uma rede local, s√£o usados ‚Äã‚Äãcomutadores de acesso e agrega√ß√£o dedicados, para que problemas na rede da Internet n√£o afetem a rede local. <br><br><img src="https://habrastorage.org/webt/j5/a5/re/j5a5re8shkxu356b54gjja2-mbg.png" title="Esquema de exemplo para um cliente"><br>  N√£o importa em qual rack o pr√≥ximo servidor est√° localizado - o Selectel combina os servidores em uma √∫nica rede local, e voc√™ n√£o precisa pensar em switches nem na localiza√ß√£o do servidor.  Voc√™ pode <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">solicitar um servidor</a> quando necess√°rio e ele ser√° conectado √† rede local. <br><br>  L2 funciona muito bem enquanto o tamanho do data center √© pequeno, quando nem todos os racks est√£o cheios.  Por√©m, √† medida que o n√∫mero de racks, servidores em racks, comutadores e clientes aumenta, o circuito se torna muito mais dif√≠cil de manter. <br><br><img src="https://habrastorage.org/webt/tg/kw/im/tgkwimba1eybjn6vngl8yohqsm0.png" title="Esquema de exemplo para v√°rios clientes"><br>  Os servidores de um cliente podem estar localizados em v√°rios <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">data centers</a> para garantir a toler√¢ncia a desastres ou se for imposs√≠vel colocar o servidor em um data center existente (por exemplo, todos os racks e todos os locais est√£o ocupados).  Entre v√°rios data centers, a conectividade tamb√©m √© necess√°ria - entre servidores em uma rede local. <br><br>  √Ä medida que o n√∫mero de data centers, racks e servidores aumenta, o circuito se torna mais complicado.  Inicialmente, a conectividade entre os servidores de diferentes data centers era realizada simplesmente no n√≠vel dos switches de agrega√ß√£o usando a tecnologia VLAN. <br><br><img src="https://habrastorage.org/webt/dj/jd/rf/djjdrfihfysh2jo3tblzpa4evmk.png" title="Um esquema de exemplo para um √∫nico cliente em v√°rios datacenters"><br>  Mas o espa√ßo de identifica√ß√£o da VLAN √© muito limitado (4095 IDs da VLAN).  Portanto, para cada data center, voc√™ deve usar seu pr√≥prio conjunto de VLANs, o que reduz o n√∫mero de identificadores poss√≠veis que podem ser usados ‚Äã‚Äãentre os data centers. <br><br><h2>  Problemas L2 </h2><br>  Ao usar um esquema no n√≠vel L2 usando VLAN, a opera√ß√£o incorreta de um dos servidores no datacenter pode levar a interrup√ß√µes no fornecimento de servi√ßos em outros servidores.  Os problemas mais comuns incluem: <br><br><ul><li> Problemas com STP (Spanning-Tree Protocol) </li><li>  Problemas com tempestades de transmiss√£o </li><li>  Problemas com o processamento multicast incorreto </li><li>  Fator humano (transfer√™ncia de link, transfer√™ncia de VLAN) </li><li>  Problemas com a organiza√ß√£o da reserva para L2 </li><li>  Problemas com tr√°fego de unicast desconhecido </li><li>  Problemas com o n√∫mero de endere√ßos MAC </li></ul><br>  Os problemas com o STP geralmente est√£o relacionados √†s configura√ß√µes dos servidores ou equipamentos do cliente.  Ao contr√°rio dos pontos populares de troca de tr√°fego, n√£o podemos filtrar completamente o STP nas portas de acesso e fechar as portas quando um STP PDU chega.  Na STP, v√°rios fabricantes de equipamentos de rede implementam funcionalidades b√°sicas de switches de data center, como a detec√ß√£o de loops na rede. <br><br>  Se o STP n√£o funcionar corretamente no lado do cliente, todo o dom√≠nio STP de pelo menos um comutador de acesso poder√° ser afetado.  O uso de extens√µes STP, como o MSTP, tamb√©m n√£o √© uma solu√ß√£o, porque o n√∫mero de portas, VLANs, comutadores geralmente excede a escalabilidade arquitetural do protocolo STP. <br><br><h3>  Difus√£o </h3><br>  A rede no centro de dados pode ser constru√≠da em dispositivos de diferentes fabricantes.  √Äs vezes, at√© as diferen√ßas na vers√£o do software do switch s√£o suficientes para que os switches lidem com o STP de maneira diferente.  Assim, por exemplo, no data center de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Dubrovka 3</a> , existem 280 racks, o que excede o n√∫mero m√°ximo poss√≠vel de comutadores em um dom√≠nio STP. <br><br>  Com o amplo uso do STP em uma rede assim, o tempo de resposta a quaisquer altera√ß√µes, em particular, simplesmente ligar ou desligar a porta, exceder√° todos os limites de espera.  Voc√™ n√£o deseja que, quando um dos clientes ligue a porta, sua conectividade de rede desapare√ßa por alguns minutos? <br><br>  Os problemas com o tr√°fego de transmiss√£o geralmente surgem devido a a√ß√µes incorretas no servidor (por exemplo, criando uma ponte entre v√°rias portas do servidor), bem como devido √† configura√ß√£o incorreta do software nos servidores.  Tentamos nivelar poss√≠veis problemas com a quantidade de tr√°fego de broadcast que chega √† nossa rede.  Mas podemos fazer isso em uma porta de conex√£o do servidor e, se 5 servidores estiverem inclu√≠dos em um comutador, cada um dos quais n√£o excede os limites definidos, juntos eles podem gerar tr√°fego suficiente para acionar o controle no comutador de agrega√ß√£o.  De nossa pr√≥pria pr√°tica, os problemas com uma tempestade de transmiss√£o do lado do servidor podem ser causados ‚Äã‚Äãpor uma falha espec√≠fica da placa de rede do servidor. <br><br>  Ao proteger toda a rede, o switch de agrega√ß√£o ‚Äúcolocar√°‚Äù a porta na qual ocorreu a anomalia na rede.  Infelizmente, isso levar√° √† inoperabilidade dos cinco servidores que causaram esse incidente, bem como √† inoperabilidade de outros servidores (at√© v√°rios racks no data center). <br><br><h3>  Multicast </h3><br>  Problemas com o processamento incorreto do tr√°fego multicast s√£o problemas muito espec√≠ficos que surgem no complexo devido √† opera√ß√£o incorreta do software no servidor e do switch.  Por exemplo, o Corosync √© configurado no modo multicast entre v√°rios servidores.  Regularmente, a troca de pacotes Hello √© realizada em pequenos volumes.  Mas, em alguns casos, os servidores com o Corosync instalado podem encaminhar muitos pacotes.  Esse volume j√° requer uma configura√ß√£o especial dos comutadores ou o uso dos mecanismos de processamento corretos (jun√ß√£o IGMP e outros).  No caso de opera√ß√£o incorreta dos mecanismos ou quando os limites s√£o acionados, pode haver interrup√ß√µes no servi√ßo que afetam outros clientes.  Obviamente, quanto menos clientes no switch, menor a probabilidade de ocorrerem problemas de outro cliente. <br><br>  O fator humano √© um tipo de problema bastante imprevisto que pode surgir ao trabalhar com equipamentos de rede.  Quando o administrador da rede est√° sozinho e ele constr√≥i seu trabalho com compet√™ncia, documenta as a√ß√µes e pondera as consequ√™ncias de suas a√ß√µes, a probabilidade de um erro √© muito pequena.  Mas quando a quantidade de equipamentos em opera√ß√£o do data center est√° aumentando, quando h√° muitos funcion√°rios, quando h√° muitas tarefas, √© necess√°ria uma abordagem completamente diferente para organizar o trabalho. <br><br>  Alguns tipos de a√ß√µes t√≠picas s√£o automatizados para evitar erros humanos, mas muitos tipos de a√ß√µes n√£o podem ser automatizados no momento, ou o pre√ßo da automa√ß√£o de tais a√ß√µes √© excessivamente alto.  Por exemplo, a altern√¢ncia f√≠sica de patch cords em um painel de conex√µes, conectando novos links, substituindo os existentes.  Tudo relacionado ao contato f√≠sico com o SCS.  Sim, existem pain√©is de corre√ß√£o que permitem alternar remotamente, mas s√£o muito caros, exigem muito trabalho preparat√≥rio e s√£o muito limitados em suas capacidades. <br><br>  Nenhum patch panel autom√°tico instalar√° um novo cabo, se necess√°rio.  Voc√™ pode cometer um erro ao configurar um switch ou roteador.  Indique que o n√∫mero da porta incorreto, n√∫mero da VLAN, seja selado ao inserir um valor num√©rico.  Ao especificar configura√ß√µes adicionais, n√£o leve em considera√ß√£o sua influ√™ncia na configura√ß√£o existente.  Com a complexidade crescente do esquema, complicando o esquema de redund√¢ncia (por exemplo, devido ao esquema atual atingir o limite de escala), a probabilidade de erros humanos aumenta.  Qualquer pessoa pode ter um erro humano, independentemente de o dispositivo estar no est√°gio de configura√ß√£o, em um servidor, em um switch, em um roteador ou em algum tipo de dispositivo de tr√¢nsito. <br><br>  A organiza√ß√£o da redund√¢ncia em L2, √† primeira vista, parece uma tarefa simples para pequenas redes.  O curso Cisco ICND aborda os conceitos b√°sicos do uso do STP como um protocolo que foi originalmente projetado para fornecer redund√¢ncia L2.  O STP possui muitas restri√ß√µes chamadas "por design" neste protocolo.  N√£o devemos esquecer que qualquer dom√≠nio STP tem uma "largura" muito limitada, ou seja, o n√∫mero de dispositivos em um dom√≠nio STP √© bastante pequeno comparado ao n√∫mero de racks no data center.  O protocolo STP em sua vers√£o original divide os links em usados ‚Äã‚Äãe de backup, o que n√£o fornece a utiliza√ß√£o completa dos uplinks durante a opera√ß√£o normal. <br><br>  O uso de outros protocolos de reserva L2 imp√µe suas limita√ß√µes.  Por exemplo, ERPS (Ethernet Ring Protection Switching) - para a topologia f√≠sica usada, para o n√∫mero de toques em um dispositivo, para a utiliza√ß√£o de todos os links.  O uso de outros protocolos, via de regra, est√° associado a altera√ß√µes propriet√°rias de diferentes fabricantes ou limita a constru√ß√£o da rede a uma tecnologia selecionada (por exemplo, a f√°brica TRILL / SPBm usando equipamentos Avaya). <br><br><h3>  Unicast desconhecido </h3><br>  Gostaria especialmente de destacar o subtipo de problemas com o tr√°fego unicast desconhecido.  O que √© isso  O tr√°fego destinado a um endere√ßo IP espec√≠fico via L3, mas √© transmitido na rede via L2, ou seja, √© transmitido a todas as portas pertencentes a esta VLAN.  Essa situa√ß√£o pode surgir por v√°rios motivos, por exemplo, ao receber DDoS em um endere√ßo IP n√£o ocupado.  Ou se, durante um erro de digita√ß√£o na configura√ß√£o do servidor, um endere√ßo que n√£o existe na rede foi especificado como backup, e no servidor historicamente houver um registro ARP est√°tico nesse endere√ßo.  O unicast desconhecido aparece quando todas as entradas nas tabelas ARP est√£o presentes, mas na aus√™ncia do endere√ßo MAC do receptor nas tabelas de comuta√ß√£o dos comutadores de tr√¢nsito. <br><br>  Por exemplo, a porta atr√°s da qual o host da rede com o endere√ßo fornecido est√° frequentemente entra no estado desligado.  Esse tipo de tr√°fego √© limitado por comutadores de tr√¢nsito e geralmente √© servido da mesma maneira que difus√£o ou multicast.  Mas, diferentemente deles, o tr√°fego unicast desconhecido pode ser iniciado "pela Internet" e n√£o apenas pela rede do cliente.  O risco de tr√°fego unicast desconhecido √© especialmente alto quando as regras de filtragem nos roteadores de borda permitem a falsifica√ß√£o de endere√ßos IP de fora. <br><br>  Mesmo o grande n√∫mero de endere√ßos MAC √†s vezes pode ser um problema.  Parece que, com um tamanho de data center de 200 racks, 40 servidores por rack, √© improv√°vel que o n√∫mero de endere√ßos MAC exceda muito o n√∫mero de servidores no data center.  Mas isso n√£o √© mais uma afirma√ß√£o verdadeira, j√° que um dos sistemas de virtualiza√ß√£o pode ser iniciado nos servidores e cada m√°quina virtual pode ser representada por seu endere√ßo MAC ou at√© v√°rios (ao emular v√°rias placas de rede em uma m√°quina virtual, por exemplo).  No total, podemos obter mais de v√°rios milhares de endere√ßos MAC leg√≠timos de um rack em 40 servidores. <br><br>  Esse n√∫mero de endere√ßos MAC pode afetar a plenitude da tabela de comuta√ß√£o em alguns modelos de comutador.  Al√©m disso, para determinados modelos de comutador, ao preencher a tabela de comuta√ß√£o, o hash √© usado e alguns endere√ßos MAC podem causar colis√µes de hash, levando ao aparecimento de tr√°fego unicast desconhecido.  A pesquisa aleat√≥ria de endere√ßos MAC em um servidor alugado a uma velocidade de, digamos, 4.000 endere√ßos por segundo, pode causar um estouro na tabela de comuta√ß√£o no comutador de acesso.  Naturalmente, os comutadores aplicam restri√ß√µes ao n√∫mero de endere√ßos MAC que podem ser aprendidos nas portas do comutador, mas, dependendo da implementa√ß√£o espec√≠fica desse mecanismo, os dados podem ser interpretados de diferentes maneiras. <br><br>  Novamente, o envio de tr√°fego para o endere√ßo MAC filtrado por esse mecanismo leva ao aparecimento de tr√°fego unicast desconhecido.  A coisa mais desagrad√°vel nessa situa√ß√£o √© que os interruptores raramente s√£o testados pelo fabricante para autocorre√ß√£o ap√≥s casos com excesso da tabela de comuta√ß√£o.  Um √∫nico estouro da tabela, causado, digamos, por um erro de um cliente nos par√¢metros hping ou na escrita de um script que monitora sua infraestrutura, pode levar √† reinicializa√ß√£o do switch e √† interrup√ß√£o da comunica√ß√£o de todos os servidores localizados no rack.  Se esse estouro ocorrer no comutador de n√≠vel de agrega√ß√£o, a reinicializa√ß√£o do comutador poder√° resultar em um tempo de inatividade de 15 minutos em toda a rede local do datacenter. <br><br>  Gostaria de dizer que o uso de L2 √© justificado apenas em casos limitados e imp√µe muitas restri√ß√µes.  O tamanho do segmento, o n√∫mero de segmentos L2 - esses s√£o todos os par√¢metros que devem ser avaliados toda vez que voc√™ adiciona uma nova VLAN com conectividade L2.  E quanto menores os segmentos L2, mais simples e, como resultado, mais confi√°vel a rede estar√° em servi√ßo. <br><br><h2>  Casos de uso t√≠picos de L2 </h2><br>  Como j√° mencionado, com o desenvolvimento gradual da infraestrutura em um data center, uma rede local L2 √© usada.  Infelizmente, esse uso tamb√©m est√° impl√≠cito no desenvolvimento de projetos em outro data center ou em outra tecnologia (por exemplo, m√°quinas virtuais na nuvem). <br><br><h3>  Vincular frente e verso, backup </h3><br>  Como regra, o uso de uma rede local come√ßa com a separa√ß√£o da funcionalidade dos servi√ßos de front-end e back-end, alocando o DBMS a um servidor separado (para melhorar o desempenho, separar o tipo de SO no servidor de aplicativos e no DBMS).  Inicialmente, o uso de L2 para esses fins parece justificado, no segmento existem poucos servidores, muitas vezes eles est√£o localizados no mesmo rack. <br><br><img src="https://habrastorage.org/webt/je/gf/1p/jegf1phbpite5kxigvt3vxmyj6a.png" title="Segmento L2 simples"><br>  Os servidores est√£o inclu√≠dos em uma VLAN, em um ou v√°rios comutadores.  √Ä medida que a quantidade de equipamentos aumenta, mais e mais novos servidores s√£o inclu√≠dos nos comutadores de novos racks no data center, a partir dos quais o dom√≠nio L2 come√ßa a crescer em largura. <br><br><img src="https://habrastorage.org/webt/j5/a5/re/j5a5re8shkxu356b54gjja2-mbg.png" title="Extens√£o do segmento L2"><br>  Novos servidores aparecem, incluindo servidores de banco de dados de backup, servidores de backup e similares.  Enquanto o projeto residir em um data center, geralmente n√£o ocorrem problemas de dimensionamento.  Os desenvolvedores de aplicativos acostumam-se ao fato de que, no pr√≥ximo servidor da rede local, o endere√ßo IP muda apenas no √∫ltimo octeto, e voc√™ n√£o precisa escrever nenhuma regra de roteamento separada. <br><br>  Os desenvolvedores devem aplicar um esquema semelhante quando o projeto crescer, quando os seguintes servidores j√° estiverem alugados em outro data center ou quando parte do projeto for transferida para as m√°quinas virtuais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na nuvem</a> .  Na foto, tudo parece muito simples e bonito: <br><br><img src="https://habrastorage.org/webt/bx/2y/ho/bx2yholhikhhkjpdaavttkfgucm.png" title="Um exemplo de combina√ß√£o de dois sites usando L2"><br>  Parece que voc√™ s√≥ precisa conectar dois comutadores de agrega√ß√£o no DC1 e DC2 com uma VLAN.  Mas o que est√° por tr√°s dessa a√ß√£o simples? <br><br><h3>  Reserva de Recursos </h3><br>  Primeiro, aumentamos a largura do dom√≠nio L2, para que todos os poss√≠veis problemas da rede local do DC1 possam surgir no DC2.  Quem gostaria que seus servidores estivessem localizados no DC2 e o incidente relacionado √† inacessibilidade da rede local ocorrer√° devido a a√ß√µes incorretas no DC1? <br><br>  Em segundo lugar, voc√™ precisa fazer o backup dessa VLAN.  A op√ß√£o de agrega√ß√£o em cada data center √© o ponto de falha.  O cabo entre os data centers √© outro ponto de falha.  Cada ponto de falha deve ser reservado.  Dois comutadores de agrega√ß√£o, dois cabos de comutadores de agrega√ß√£o para acessar comutadores, dois cabos entre data centers ... Cada vez que o n√∫mero de componentes aumenta e o circuito se torna mais complicado. <br><br><img src="https://habrastorage.org/webt/nq/ku/ws/nqkuws41ips807benbzzemxfzry.png" title="Exemplo de esquema L2 redundante"><br>  A complexidade do esquema √© causada pela necessidade de reservar cada elemento no sistema.  Para um backup completo de dispositivos e links, voc√™ precisa duplicar quase todos os elementos.  Em uma rede t√£o grande, n√£o √© mais poss√≠vel usar o STP para organizar backups.  Seria poss√≠vel apresentar todos os elementos de rede, em particular os comutadores de acesso, como componentes da nuvem MPLS, e a redund√¢ncia seria obtida devido √† funcionalidade do protocolo MPLS. <br><br>  Mas os dispositivos MPLS geralmente s√£o duas vezes mais caros que os dispositivos n√£o MPLS.  E deve-se notar que a op√ß√£o MPU em 1U, que possui um bom grau de escalabilidade, a implementa√ß√£o da funcionalidade MPLS completa no plano de controle, na pr√°tica, n√£o existia at√© recentemente.  Como resultado, quero me livrar ou minimizar o impacto dos problemas de L2 na rede existente, mas, ao mesmo tempo, manter a capacidade de reservar recursos. <br><br><h2>  Transi√ß√£o para L3 </h2><br>  Se cada link na rede √© representado como um segmento IP separado e cada dispositivo como um roteador separado, n√£o precisamos de redund√¢ncia no n√≠vel L2.  A redund√¢ncia de link e roteador √© garantida por protocolos de roteamento din√¢mico e redund√¢ncia de roteamento na rede. <br><br>  Dentro do datacenter, podemos salvar os esquemas de intera√ß√£o do servidor existentes via L2, e o acesso aos servidores em outro datacenter ser√° via L3. <br><br><img src="https://habrastorage.org/webt/hc/2q/n4/hc2qn4wcnecwsrrkxh-daualss8.png" title="Conectividade do site com L3"><br>  Assim, os data centers s√£o interconectados pela conectividade L3.  Ou seja, emula-se que um roteador esteja instalado entre os datacenters (na verdade, v√°rios, para backup).  Isso permite que voc√™ divida os dom√≠nios L2 entre os data centers, use sua pr√≥pria VLAN em cada data center e se comunique entre eles.  Para cada cliente, voc√™ pode usar intervalos repetidos de endere√ßos IP, as redes s√£o completamente isoladas umas das outras e n√£o pode entrar na rede de outro cliente a partir da rede de um cliente (exceto quando os dois clientes concordam com essa conex√£o). <br><br>  Recomendamos que voc√™ use segmentos IP de 10.0.0.0/8 para redes locais.  Para o primeiro datacenter, a rede ser√°, por exemplo, 10.0.1.0/24, para o segundo - 10.0.2.0/24.  Selectel no roteador prescreve o endere√ßo IP desta sub-rede.  Normalmente, os endere√ßos .250-.254 s√£o reservados para dispositivos de rede Selectel, e um endere√ßo .254 serve como gateway para outras LANs.  A rota √© atribu√≠da a todos os dispositivos em todos os datacenters: <br><br> <code>route add 10.0.0.0 mask 255.0.0.0 gw 10.0.x.254</code> <br> <br>  Onde x √© o n√∫mero do datacenter.  Devido a essa rota, os servidores nos datacenters ‚Äúse veem‚Äù roteando. <br><br><img src="https://habrastorage.org/webt/hf/x6/h1/hfx6h13labuqr97rfirrzwtw8nk.png" title="Exemplo de roteamento para combinar dois sites no L3"><br>  A presen√ßa dessa rota simplifica o dimensionamento do esquema no caso, por exemplo, da apar√™ncia de um terceiro data center.  Em seguida, para servidores no terceiro data center, os endere√ßos IP do pr√≥ximo intervalo, 10.0.3.0/24, s√£o registrados no roteador - o endere√ßo 10.0.3.254. <br><br><img src="https://habrastorage.org/webt/r_/lg/n5/r_lgn5-xrch-wfcytnqvyy7ujiy.png" title="Exemplo de roteamento para combinar tr√™s sites no L3"><br>  Uma caracter√≠stica distintiva da implementa√ß√£o de um esquema desse tipo √© que ele n√£o requer reserva adicional em caso de falha do datacenter ou dos canais de comunica√ß√£o externos.  Portanto, por exemplo, se o datacenter 1 falhar, a conex√£o entre o datacenter 2 e o datacenter 3 ser√° completamente preservada e, ao implementar o esquema com o feed L2 entre os datacenters por um deles, como na figura: <br><br><img src="https://habrastorage.org/webt/dj/jd/rf/djjdrfihfysh2jo3tblzpa4evmk.png" title="Opera√ß√£o de roteamento ao salvar esquemas L2"><br>  A conex√£o entre o data center 2 e o data center 3 depende da efici√™ncia do data center 1. Ou, a organiza√ß√£o de links adicionais e o uso de esquemas complexos de reserva L2 s√£o necess√°rios.  E, ao salvar o esquema L2, toda a rede ainda √© muito sens√≠vel √† comuta√ß√£o incorreta, √† forma√ß√£o de loops de comuta√ß√£o, v√°rias tempestades de tr√°fego e outros problemas. <br><br><h3>  Segmentos L3 nos projetos </h3><br>  Al√©m de usar diferentes segmentos L3 em diferentes datacenters, faz sentido alocar uma rede L3 separada para servidores em diferentes projetos, geralmente criados com diferentes tecnologias.  Por exemplo, servidores de hardware no datacenter em uma sub-rede IP, servidores virtuais no mesmo datacenter, mas na nuvem VMware, em outra sub-rede IP, alguns servidores relacionados √† prepara√ß√£o na terceira sub-rede IP .  Portanto, erros aleat√≥rios em um dos segmentos n√£o levam a uma falha completa de todos os servidores inclu√≠dos na rede local. <br><br><img src="https://habrastorage.org/webt/v3/zi/gd/v3zigd00-7klicdt31likdem460.png" title="Segmenta√ß√£o de redes de diferentes projetos no n√≠vel L3"><br><h2>  Reserva de roteador </h2><br>  Tudo isso √© impressionante, mas h√° um √∫nico ponto de falha entre os projetos - este √© o roteador.  O que fazer neste caso?  De fato, o roteador n√£o est√° sozinho.  Dois roteadores s√£o alocados para cada datacenter e, para cada cliente, eles formam o IP virtual .254 usando o protocolo VRRP. <br><br>  O uso de VRRP entre dois dispositivos adjacentes com um segmento L2 comum √© justificado.  Para data centers geograficamente distribu√≠dos, roteadores diferentes s√£o usados ‚Äã‚Äãe o MPLS √© organizado entre eles.  Assim, cada cliente que se conecta √† rede local dessa maneira √© conectado a um L3VPN separado criado para esses roteadores MPLS.  E o esquema, em aproxima√ß√£o √† realidade, fica assim: <br><br><img src="https://habrastorage.org/webt/9y/ae/s8/9yaes8vvilol5-zrhikhychdvs4.png" title="Organiza√ß√£o de Reservas"><br>  O endere√ßo do gateway para cada segmento .254 √© reservado pelo VRRP entre os dois roteadores. <br><br><h2>  Conclus√£o </h2><br>  Resumindo tudo isso, alterar o tipo de rede local de L2 para L3 nos permitiu manter a capacidade de escalar, aumentou o n√≠vel de confiabilidade e toler√¢ncia a falhas e tamb√©m nos permitiu implementar esquemas de redund√¢ncia adicionais.  Al√©m disso, isso contornou as restri√ß√µes e ‚Äúarmadilhas‚Äù existentes de L2. <br><br>  √Ä medida que os projetos e os data centers crescem, as solu√ß√µes padr√£o existentes atingem seu limite de escalabilidade.  Isso significa que eles n√£o s√£o mais adequados para a solu√ß√£o eficaz de problemas.  Os requisitos de confiabilidade e estabilidade do sistema como um todo est√£o aumentando constantemente, o que, por sua vez, afeta o processo de planejamento.  √â importante levar em considera√ß√£o o fato de que previs√µes de crescimento otimistas devem ser consideradas para que, no futuro, n√£o exista um sistema que n√£o possa ser escalado. <br><br>  Diga-nos - voc√™ j√° est√° usando o L3VPN?  Vejo voc√™ nos coment√°rios. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt436648/">https://habr.com/ru/post/pt436648/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt436638/index.html">Redistribuindo janelas entre monitores ap√≥s acordar do modo de suspens√£o</a></li>
<li><a href="../pt436640/index.html">Minha experi√™ncia em publicidade e desenvolvimento de aplicativos Android e iOS</a></li>
<li><a href="../pt436642/index.html">Ticket to Ride.Europa - etapas modestas na aritm√©tica do jogo</a></li>
<li><a href="../pt436644/index.html">S√≠mbolos e m√≥dulos sint√©ticos (WinDbg / DbgEng)</a></li>
<li><a href="../pt436646/index.html">Transl√∫cido no Android e AdjustResize</a></li>
<li><a href="../pt436650/index.html">3 estrat√©gias vencedoras de monetiza√ß√£o de aplicativos em 2019</a></li>
<li><a href="../pt436652/index.html">MPS 2018.3: planos de gera√ß√£o, melhorias na linguagem assembly e de empacotamento e na linguagem editor, interface atualizada</a></li>
<li><a href="../pt436654/index.html">Desenvolvimento de uma equipe para consultar dados do banco de dados - parte 4, concluindo</a></li>
<li><a href="../pt436656/index.html">Automa√ß√£o para aut√¥nomos: como integrar impostos a um projeto de TI</a></li>
<li><a href="../pt436658/index.html">O futuro do varejo: principais tend√™ncias digitais baseadas no Big Show 2019 da NRF Retail</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>