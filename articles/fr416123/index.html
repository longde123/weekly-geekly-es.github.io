<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äçüëß‚Äçüë¶ üßôüèº üçä Reconnaissance de marchandises sur des √©tag√®res √† l'aide de r√©seaux de neurones √† l'aide des technologies Keras et API de d√©tection d'objets Tensorflow üíí ü§õüèø üç±</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans l'article, nous parlerons de l'utilisation de r√©seaux de neurones convolutifs pour r√©soudre une t√¢che commerciale pratique de restauration d'un r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconnaissance de marchandises sur des √©tag√®res √† l'aide de r√©seaux de neurones √† l'aide des technologies Keras et API de d√©tection d'objets Tensorflow</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/true_engineering/blog/416123/">  Dans l'article, nous parlerons de l'utilisation de r√©seaux de neurones convolutifs pour r√©soudre une t√¢che commerciale pratique de restauration d'un r√©alogramme √† partir de photographies d'√©tag√®res avec des marchandises.  En utilisant l'API de d√©tection d'objets Tensorflow, nous formerons le mod√®le de recherche / localisation.  Nous am√©liorerons la qualit√© de la recherche de petits produits dans les photographies haute r√©solution en utilisant une fen√™tre flottante et un algorithme de suppression non maximum.  Chez Keras, nous mettons en ≈ìuvre un classificateur de produits par marque.  En parall√®le, nous comparerons les approches et les r√©sultats avec les d√©cisions d'il y a 4 ans.  Toutes les donn√©es utilis√©es dans l'article sont disponibles pour t√©l√©chargement, et le code enti√®rement fonctionnel est sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub</a> et est con√ßu comme un tutoriel. <br><br><img src="https://habrastorage.org/webt/aw/27/ar/aw27argaeseqsgos5cbpwdwt89s.jpeg"><br><a name="habracut"></a><br><h3>  Pr√©sentation </h3><br>  Qu'est-ce qu'un planogramme?  Le sch√©ma de disposition de l'affichage des marchandises sur l'√©quipement commercial de b√©ton du magasin. <br><br>  Qu'est-ce qu'un r√©alogramme?  La disposition des marchandises sur un √©quipement commercial sp√©cifique existant dans le magasin ici et maintenant. <br><br>  Planogramme - comme il se doit, r√©alogramme - ce que nous avons. <br><br><img src="https://habrastorage.org/webt/ym/oo/ew/ymooewlirqpaosatarrenno8caw.jpeg"><br><br>  Jusqu'√† pr√©sent, dans de nombreux magasins, la gestion du reste des marchandises sur des √©tag√®res, des √©tag√®res, des comptoirs, des √©tag√®res est exclusivement du travail manuel.  Des milliers d'employ√©s v√©rifient la disponibilit√© des produits manuellement, calculent le solde, v√©rifient l'emplacement avec les exigences.  C'est cher et des erreurs sont tr√®s probables.  Un affichage incorrect ou un manque de marchandises entra√Æne une baisse des ventes. <br><br>  En outre, de nombreux fabricants concluent des accords avec des d√©taillants pour afficher leurs produits.  Et comme il y a beaucoup de fabricants, entre eux commence la lutte pour le meilleur endroit sur l'√©tag√®re.  Tout le monde veut que son produit se situe au centre face aux yeux de l'acheteur et occupe la plus grande surface possible.  Il faut un audit continu. <br><br>  Des milliers de marchandiseurs se d√©placent d'un magasin √† l'autre pour s'assurer que les produits de leur entreprise sont en rayon et pr√©sent√©s conform√©ment au contrat.  Parfois, ils sont paresseux: il est beaucoup plus agr√©able de faire un rapport sans sortir de chez soi que de se rendre dans un point de vente.  Un audit permanent des auditeurs est n√©cessaire. <br><br>  Naturellement, la t√¢che d'automatisation et de simplification de ce processus est r√©solue depuis longtemps.  L'une des parties les plus difficiles a √©t√© le traitement d'image: trouver et reconna√Ætre des produits.  Et ce n'est que relativement r√©cemment que cette t√¢che a √©t√© tellement simplifi√©e que pour un cas particulier sous une forme simplifi√©e, sa solution compl√®te peut √™tre d√©crite dans un article.  Voil√† ce que nous allons faire. <br><br>  L'article contient un minimum de code (uniquement pour les cas o√π le code est plus clair que le texte).  La solution compl√®te est disponible sous forme de tutoriel illustr√© dans les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cahiers jupyter</a> .  L'article ne contient pas de description de l'architecture des r√©seaux de neurones, des principes des neurones, des formules math√©matiques.  Dans l'article, nous les utilisons comme un outil d'ing√©nierie, sans trop entrer dans les d√©tails de son appareil. <br><br><h3>  Donn√©es et approche </h3><br>  Comme pour toute approche bas√©e sur les donn√©es, les solutions de r√©seau neuronal n√©cessitent des donn√©es.  Vous pouvez √©galement les assembler manuellement: pour capturer plusieurs centaines de compteurs et les marquer √† l'aide, par exemple, de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">LabelImg</a> .  Vous pouvez commander un balisage, par exemple, sur Yandex.Tolok. <br><br><img src="https://habrastorage.org/webt/ji/he/lv/jihelvxh9vkmixjzsxink1nknya.jpeg"><br><br>  Nous ne pouvons pas divulguer les d√©tails d'un projet r√©el, nous allons donc expliquer la technologie sur les donn√©es ouvertes.  Faire du shopping et prendre des photos √©tait trop paresseux (et nous n'aurions pas √©t√© compris l√†-bas), et le d√©sir de faire nous-m√™mes le balisage des photos trouv√©es sur Internet s'est arr√™t√© apr√®s le centi√®me objet class√©.  Heureusement, par hasard, je suis tomb√© sur les archives de l'ensemble de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">donn√©es d'√©picerie</a> . <br><br>  En 2014, les employ√©s d'Idea Teknoloji, √† Istanbul, en Turquie, ont t√©l√©charg√© 354 photos de 40 magasins prises sur 4 cam√©ras.  Sur chacune de ces photographies, ils ont mis en √©vidence par des rectangles un total de plusieurs milliers d'objets, dont certains ont √©t√© class√©s en 10 cat√©gories. <br><br>  Ce sont des photos de paquets de cigarettes.  Nous ne faisons pas la promotion ou la promotion du tabagisme.  Il n'y avait tout simplement rien de plus neutre.  Nous promettons que partout dans l'article, lorsque la situation le permettra, nous utiliserons des photos de chats. <br><br><img src="https://habrastorage.org/webt/04/8x/ek/048xekrylrnspiwd7vefbux_kgu.jpeg"><br><br>  En plus des photos √©tiquet√©es des √©tag√®res, ils ont √©crit un article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Vers la reconnaissance des produits de d√©tail sur les √©tag√®res des √©piceries</a> avec une solution au probl√®me de localisation et de classification.  Cela a √©tabli une sorte de point de r√©f√©rence: notre solution utilisant de nouvelles approches devrait s'av√©rer plus simple et plus pr√©cise, sinon ce n'est pas int√©ressant.  Leur approche consiste en une combinaison d'algorithmes: <br><br><img src="https://habrastorage.org/webt/j5/p4/t_/j5p4t_ul9ungv_luvlzxa5aa1gi.jpeg"><br><br>  R√©cemment, les r√©seaux de neurones convolutifs (CNN) ont r√©volutionn√© le domaine de la vision par ordinateur et ont compl√®tement chang√© l'approche pour r√©soudre ces probl√®mes.  Au cours des derni√®res ann√©es, ces technologies sont devenues disponibles pour un large √©ventail de d√©veloppeurs, et des API de haut niveau comme Keras ont consid√©rablement abaiss√© leur seuil d'entr√©e.  D√©sormais, presque tous les d√©veloppeurs peuvent utiliser toute la puissance des r√©seaux de neurones convolutifs apr√®s seulement quelques jours de datation.  L'article d√©crit l'utilisation de ces technologies √† l'aide d'un exemple, montrant comment une cascade compl√®te d'algorithmes peut facilement √™tre remplac√©e par seulement deux r√©seaux de neurones sans perte de pr√©cision. <br><br>  Nous allons r√©soudre le probl√®me par √©tapes: <br><br><ul><li>  Pr√©paration des donn√©es.  Nous pompons les archives et les transformons en une vue pratique pour le travail. </li><li>  Classification des marques.  Nous r√©solvons le probl√®me de classification en utilisant un r√©seau de neurones. </li><li>  Recherchez des produits sur la photo.  Nous formons le r√©seau neuronal √† la recherche de biens. </li><li>  Impl√©mentation de la recherche.  Nous am√©liorerons la qualit√© de la d√©tection √† l'aide d'une fen√™tre flottante et d'un algorithme de suppression des non-maximums. </li><li>  Conclusion  Expliquez bri√®vement pourquoi la vie r√©elle est beaucoup plus compliqu√©e que cet exemple. </li></ul><br><h3>  La technologie </h3><br>  Les principales technologies que nous utiliserons: Tensorflow, Keras, API de d√©tection d'objets Tensorflow, OpenCV.  Bien que Windows et Mac OS conviennent √† l'utilisation de Tensorflow, nous recommandons toujours d'utiliser Ubuntu.  M√™me si vous n'avez jamais travaill√© avec ce syst√®me d'exploitation auparavant, son utilisation vous fera √©conomiser une tonne de temps.  L'installation de Tensorflow pour fonctionner avec le GPU est un sujet qui m√©rite un article s√©par√©.  Heureusement, de tels articles existent d√©j√†.  Par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Installation de TensorFlow sur Ubuntu 16.04 avec un GPU Nvidia</a> .  Certaines instructions peuvent √™tre obsol√®tes. <br><br>  <b>√âtape 1. Pr√©paration des donn√©es ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien github</a> )</b> <br><br>  Cette √©tape, en r√®gle g√©n√©rale, prend beaucoup plus de temps que la simulation elle-m√™me.  Heureusement, nous utilisons des donn√©es pr√©d√©finies, que nous convertissons sous la forme dont nous avons besoin. <br><br>  Vous pouvez t√©l√©charger et d√©compresser de cette fa√ßon: <br><br><pre><code class="bash hljs">wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part1.tar.gz wget https://github.com/gulvarol/grocerydataset/releases/download/1.0/GroceryDataset_part2.tar.gz tar -xvzf GroceryDataset_part1.tar.gz tar -xvzf GroceryDataset_part2.tar.gz</code> </pre> <br>  Nous obtenons la structure de dossiers suivante: <br><br><img src="https://habrastorage.org/webt/n1/yi/8n/n1yi8n3faxzmxia70bxsee-b69u.jpeg"><br><br>  Nous utiliserons les informations des r√©pertoires ShelfImages et ProductImagesFromShelves. <br>  ShelfImages contient des photos des √©tag√®res elles-m√™mes.  Dans le nom, l'identifiant du rack avec l'identifiant de l'image est encod√©.  Il peut y avoir plusieurs photos d'un rack.  Par exemple, une photographie dans son int√©gralit√© et 5 photographies en parties avec intersections. <br><br>  Fichier C1_P01_N1_S2_2.JPG (rack C1_P01, snapshot N1_S2_2): <br><br><img src="https://habrastorage.org/webt/nv/jd/or/nvjdorlqwj1qc7asuzktk7dqccs.jpeg"><br><br>  Nous parcourons tous les fichiers et collectons des informations dans le cadre de donn√©es pandas photos_df: <br><br><img src="https://habrastorage.org/webt/us/zq/zq/uszqzqw3haortnmdvscxp0sq1cq.png"><br>  ProductImagesFromShelves contient des photos d√©coup√©es de marchandises provenant des √©tag√®res dans 11 sous-r√©pertoires: 0 - non class√©, 1 - Marlboro, 2 - Kent, etc.  Afin de ne pas en faire la publicit√©, nous n'utiliserons que des num√©ros de cat√©gorie sans sp√©cifier de noms.  Les fichiers dans les noms contiennent des informations sur le rack, la position et la taille du pack. <br><br>  Fichier C1_P01_N1_S3_1.JPG_1276_1828_276_448.png du r√©pertoire 1 (cat√©gorie 1, rack C1_P01, image N1_S3_1, coordonn√©es du coin sup√©rieur gauche (1276, 1828), largeur 276, hauteur 448): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/-x/pc/f0-xpc4nqkymbj-ycs1my_sldhi.jpeg"></div><br>  Nous n'avons pas besoin des photos des packs individuels eux-m√™mes (nous les couperons des photos des √©tag√®res), et nous collectons des informations sur leur cat√©gorie et leur position dans les produits de la trame de donn√©es pandas_df: <br><br><img src="https://habrastorage.org/webt/0x/nh/rv/0xnhrvv5nibludwh7svn54hj_ba.png"><br>  √Ä la m√™me √©tape, nous divisons toutes nos informations en deux sections: formation pour la formation et validation pour le suivi de la formation.  Bien s√ªr, cela ne vaut pas la peine de le faire dans de vrais projets.  Et aussi ne faites pas confiance √† ceux qui font cela.  Vous devez au moins allouer un autre test pour le test final.  Mais m√™me avec cette approche pas tr√®s honn√™te, il est important pour nous de ne pas trop nous tromper. <br><br>  Comme nous l'avons d√©j√† not√©, il peut y avoir plusieurs photos d'un rack.  En cons√©quence, le m√™me pack peut se d√©cliner en plusieurs images.  Par cons√©quent, nous vous conseillons de ne pas d√©composer par images, et encore plus non par packs, mais par racks.  Cela est n√©cessaire pour qu'il ne se produise pas que le m√™me objet, pris sous des angles diff√©rents, se retrouve √† la fois en train et en validation. <br><br>  Nous effectuons une r√©partition 70/30 (30% des racks vont pour la validation, le reste pour la formation): <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># get distinct shelves shelves = list(set(photos_df['shelf_id'].values)) # use train_test_split from sklearn shelves_train, shelves_validation, _, _ = train_test_split(   shelves, shelves, test_size=0.3, random_state=6) # mark all records in data frames with is_train flag def is_train(shelf_id): return shelf_id in shelves_train photos_df['is_train'] = photos_df.shelf_id.apply(is_train) products_df['is_train'] = products_df.shelf_id.apply(is_train)</span></span></code> </pre> <br>  Nous nous assurerons que lorsque nous nous s√©parons, il y a suffisamment de repr√©sentants de chaque classe pour la formation et la validation: <br><img src="https://habrastorage.org/webt/9n/w_/xj/9nw_xjw1qiqc21sbi5q3s0pv3_q.jpeg"><br>  La couleur bleue indique le nombre de produits de la cat√©gorie pour la validation et l'orange pour la formation.  La situation n'est pas tr√®s bonne avec la cat√©gorie 3 pour la validation, mais en principe il y a peu de ses repr√©sentants. <br><br>  Au stade de la pr√©paration des donn√©es, il est important de ne pas se tromper, car tout travail ult√©rieur est bas√© sur ses r√©sultats.  Nous avons encore fait une erreur et pass√© de nombreuses heures heureuses √† essayer de comprendre pourquoi la qualit√© des mod√®les est tr√®s m√©diocre.  D√©j√† ressenti comme un perdant pour les technologies de la ¬´vieille √©cole¬ª, jusqu'√† ce que vous remarquiez accidentellement que certaines des photos originales ont √©t√© tourn√©es de 90 degr√©s, et certaines ont √©t√© faites √† l'envers. <br><br>  Dans le m√™me temps, le balisage est effectu√© comme si les photos √©taient correctement orient√©es.  Apr√®s une solution rapide, les choses sont devenues beaucoup plus amusantes. <br><br>  Nous enregistrerons nos donn√©es dans des fichiers pkl pour les utiliser dans les √©tapes suivantes.  Au total, nous avons: <br><br><ul><li>  Un annuaire de photographies de racks et de leurs pi√®ces avec des bundles, </li><li>  Un bloc de donn√©es avec une description de chaque rack avec une note s'il est destin√© √† la formation, </li><li>  Un cadre de donn√©es avec des informations sur tous les produits sur les √©tag√®res, indiquant leur position, leur taille, leur cat√©gorie et indiquant s'ils sont destin√©s √† la formation. </li></ul><br>  Pour v√©rification, nous affichons un rack selon nos donn√©es: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function to display shelf photo with rectangled products def draw_shelf_photo(file):   file_products_df = products_df[products_df.file == file]   coordinates = file_products_df[['xmin', 'ymin', 'xmax', 'ymax']].values   im = cv2.imread(f'{shelf_images}{file}')   im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)      for xmin, ymin, xmax, ymax in coordinates:       cv2.rectangle(im, (xmin, ymin), (xmax, ymax), (0, 255, 0), 5)   plt.imshow(im) # draw one photo to check our data fig = plt.gcf() fig.set_size_inches(18.5, 10.5) draw_shelf_photo('C3_P07_N1_S6_1.JPG')</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/1m/7a/xr/1m7axrc3gcdvgt1sg0sc9-dbs0y.png"><br><br>  <b>√âtape 2. Classification par marque ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien sur github</a> )</b> <br><br>  La classification des images est la t√¢che principale dans le domaine de la vision par ordinateur.  Le probl√®me est le ¬´foss√© s√©mantique¬ª: la photographie n'est qu'une grande matrice de nombres [0, 255].  Par exemple, 800x600x3 (3 canaux RVB). <br><br><img src="https://habrastorage.org/webt/0w/mi/rx/0wmirxot0tx0_dl_b-m_gpdunse.jpeg"><br><br>  Pourquoi cette t√¢che est difficile: <br><br><img src="https://habrastorage.org/webt/gx/il/fh/gxilfhn6woijdjngbgrbgfzgjmo.png"><br><br>  Comme nous l'avons d√©j√† dit, les auteurs des donn√©es que nous utilisons ont identifi√© 10 marques.  C'est une t√¢che extr√™mement simplifi√©e, car il y a beaucoup plus de marques de cigarettes sur les √©tag√®res.  Mais tout ce qui ne rentre pas dans ces 10 cat√©gories a √©t√© envoy√© √† 0 - non class√©: <br><br><img src="https://habrastorage.org/webt/wv/-j/hm/wv-jhmn18kt8ta1zhsai4fzdxbq.png">  " <br><br>  Leur article propose un tel algorithme de classification avec une pr√©cision totale de 92%: <br><img src="https://habrastorage.org/webt/vv/d3/ub/vvd3ubvr_tvjxasrwvs7jap6h7o.jpeg"><br>  Que ferons-nous: <br><br><ul><li>  Nous pr√©parerons les donn√©es pour la formation, </li><li>  Nous formons un r√©seau neuronal convolutionnel avec l'architecture ResNet v1, </li><li>  V√©rifiez les photos pour validation. </li></ul><br>  Cela semble ¬´volumineux¬ª, mais nous venons d'utiliser l'exemple de Keras ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Former un ResNet sur le jeu de donn√©es CIFAR10</a> ¬ª en lui reprenant la fonction de cr√©er ResNet v1. <br><br>  Pour d√©marrer le processus de formation, vous devez pr√©parer deux tableaux: x - photos de packs avec une dimension (nombre de packs, hauteur, largeur, 3) et y - leurs cat√©gories avec une dimension (nombre de packs, 10).  Le tableau y contient les vecteurs dits chauds.  Si la cat√©gorie d'un pack de formation porte le num√©ro 2 (de 0 √† 9), alors cela correspond au vecteur [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]. <br><br>  Une question importante est de savoir quoi faire avec la largeur et la hauteur, car toutes les photos ont √©t√© prises avec diff√©rentes r√©solutions √† diff√©rentes distances.  Nous devons choisir une taille fixe, √† laquelle nous pouvons apporter toutes nos photos des packs.  Cette taille fixe est un m√©ta-param√®tre qui d√©termine comment notre r√©seau de neurones va s'entra√Æner et fonctionner. <br><br>  D'une part, je veux rendre cette taille aussi grande que possible afin qu'aucun d√©tail de l'image ne passe inaper√ßu.  D'un autre c√¥t√©, avec notre maigre quantit√© de donn√©es d'entra√Ænement, cela peut conduire √† un recyclage rapide: le mod√®le fonctionnera parfaitement sur les donn√©es d'entra√Ænement, mais mal sur les donn√©es de validation.  Nous avons choisi la taille 120x80, peut-√™tre sur une taille diff√©rente nous obtiendrions un meilleur r√©sultat.  Fonction zoom: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># resize pack to fixed size SHAPE_WIDTH x SHAPE_HEIGHT def resize_pack(pack):   fx_ratio = SHAPE_WIDTH / pack.shape[1]   fy_ratio = SHAPE_HEIGHT / pack.shape[0]      pack = cv2.resize(pack, (0, 0), fx=fx_ratio, fy=fy_ratio)   return pack[0:SHAPE_HEIGHT, 0:SHAPE_WIDTH]</span></span></code> </pre> <br>  Mettez √† l'√©chelle et affichez un pack pour v√©rification.  Le nom de la marque est difficile √† lire par une personne, voyons comment le r√©seau neuronal va faire face √† la t√¢che de classification: <br><br><img src="https://habrastorage.org/webt/z_/8p/0f/z_8p0f5kuxx27mneryilqv81ols.png"><br><br>  Apr√®s pr√©paration selon le flag obtenu √† l'√©tape pr√©c√©dente, on d√©compose les tableaux x et y en x_train / x_validation et y_train / y_validation, on obtient: <br><br><pre> <code class="bash hljs">x_train shape: (1969, 120, 80, 3) y_train shape: (1969, 10) 1969 train samples 775 validation samples</code> </pre><br>  Les donn√©es sont pr√©par√©es, nous copions la fonction du constructeur de r√©seau neuronal de l'architecture ResNet v1 √† partir de l'exemple Keras: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">resnet_v1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_shape, depth, num_classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   ‚Ä¶</code> </pre> <br>  Nous construisons un mod√®le: <br><br><pre> <code class="python hljs">model = resnet_v1(input_shape=x_train.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:], depth=depth, num_classes=num_classes) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>,             optimizer=Adam(lr=lr_schedule(<span class="hljs-number"><span class="hljs-number">0</span></span>)), metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Nous avons un ensemble de donn√©es assez limit√©.  Par cons√©quent, afin d'emp√™cher le mod√®le de voir la m√™me photo √† chaque fois pendant la formation, nous utilisons l'augmentation: d√©caler l'image au hasard et la faire pivoter un peu.  Keras fournit cet ensemble d'options pour cela: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator(   featurewise_center=False,  # set input mean to 0 over the dataset   samplewise_center=False,  # set each sample mean to 0   featurewise_std_normalization=False,  # divide inputs by std of the dataset   samplewise_std_normalization=False,  # divide each input by its std   zca_whitening=False,  # apply ZCA whitening   rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)   width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)   height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)   horizontal_flip=False,  # randomly flip images   vertical_flip=False)  # randomly flip images datagen.fit(x_train)</span></span></code> </pre> <br>  Nous commen√ßons le processus de formation. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's run training process, 20 epochs is enough batch_size = 50 epochs = 15 model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),                   validation_data=(x_validation, y_validation),                   epochs=epochs, verbose=1, workers=4,                   callbacks=[LearningRateScheduler(lr_schedule)])</span></span></code> </pre> <br>  Apr√®s formation et √©valuation, nous obtenons une pr√©cision de l'ordre de 92%.  Vous pouvez obtenir une pr√©cision diff√©rente: il y a tr√®s peu de donn√©es, donc la pr√©cision d√©pend beaucoup du succ√®s de la partition.  Sur cette partition, nous n'avons pas obtenu une pr√©cision significativement sup√©rieure √† celle qui √©tait indiqu√©e dans l'article, mais nous n'avons pratiquement rien fait nous-m√™mes et avons √©crit peu de code.  De plus, nous pouvons facilement ajouter une nouvelle cat√©gorie, et la pr√©cision devrait (en th√©orie) augmenter consid√©rablement si nous pr√©parons plus de donn√©es. <br><br>  Pour l'int√©r√™t, comparez les matrices de confusion: <br><img src="https://habrastorage.org/webt/ee/ns/tm/eenstmjcconudxyjjkmtpzcdmow.jpeg"><br>  Presque toutes les cat√©gories que notre r√©seau neuronal d√©finit mieux, √† l'exception des cat√©gories 4 et 7. Il est √©galement utile de regarder les repr√©sentants les plus brillants de chaque cellule de matrice de confusion: <br><img src="https://habrastorage.org/webt/qv/tm/lw/qvtmlwxgqvbdhut73zgsbbtfqqo.jpeg"><br>  Vous pouvez √©galement comprendre pourquoi le Parlement a √©t√© confondu avec Camel, mais pourquoi Winston a √©t√© confondu avec Lucky Strike est compl√®tement incompr√©hensible, mais ils n'ont rien en commun.  C'est le principal probl√®me des r√©seaux de neurones - l'opacit√© totale de ce qui se passe √† l'int√©rieur.  Vous pouvez bien s√ªr visualiser certaines couches, mais pour nous, cette visualisation ressemble √† ceci: <br><br><img src="https://habrastorage.org/webt/w1/tn/et/w1tnetor61yz-uwvmjlserwvh3m.jpeg"><br><br>  Une opportunit√© √©vidente d'am√©liorer la qualit√© de la reconnaissance dans nos conditions est d'ajouter plus de photos. <br><br>  Ainsi, le classificateur est pr√™t.  Allez au d√©tecteur. <br><br>  <b>√âtape 3. Recherchez des produits sur la photo ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien sur github</a> )</b> <br><br>  Les t√¢ches importantes suivantes dans le domaine de la vision par ordinateur sont la segmentation s√©mantique, la localisation, la recherche d'objets et la segmentation d'instances. <br><br><img src="https://habrastorage.org/webt/z0/y9/bf/z0y9bfe7f6qp143c3wubj8rvnd8.jpeg"><br><br>  Notre t√¢che n√©cessite la d√©tection d'objets.  L'article de 2014 propose une approche bas√©e sur la m√©thode Viola-Jones et HOG avec une pr√©cision visuelle: <br><br><img src="https://habrastorage.org/webt/11/jt/8j/11jt8jcggyutkrxyelswr8psm8s.jpeg"><br><br>  Gr√¢ce √† l'utilisation de restrictions statistiques suppl√©mentaires, leur pr√©cision est tr√®s bonne: <br><br><img src="https://habrastorage.org/webt/od/nb/yl/odnbylwn0yo-k92q_nu6mazz8fq.jpeg"><br><br>  Maintenant, la t√¢che de reconnaissance d'objets est r√©solue avec succ√®s √† l'aide de r√©seaux de neurones.  Nous utiliserons le syst√®me API de d√©tection d'objets Tensorflow et formerons un r√©seau de neurones avec l'architecture SSD Mobilenet V1.  La formation d'un tel mod√®le √† partir de z√©ro n√©cessite beaucoup de donn√©es et peut prendre des jours, nous utilisons donc un mod√®le form√© sur les donn√©es COCO selon le principe de l'apprentissage par transfert. <br><br>  Le concept cl√© de cette approche est le suivant.  Pourquoi un enfant n'a-t-il pas besoin de montrer des millions d'objets pour qu'il apprenne √† trouver et √† distinguer une balle d'un cube?  Parce que l'enfant a 500 millions d'ann√©es de d√©veloppement du cortex visuel.  L'√©volution a fait de la vision le plus grand syst√®me sensoriel.  Pr√®s de 50% (mais ce n'est pas exact) des neurones du cerveau humain sont responsables du traitement de l'image.  Les parents ne peuvent montrer que le ballon et le cube, puis corriger l'enfant plusieurs fois pour qu'il trouve et se distingue parfaitement l'un de l'autre. <br><br>  D'un point de vue philosophique (avec des diff√©rences techniques plus que g√©n√©rales), l'apprentissage par transfert dans les r√©seaux de neurones fonctionne de mani√®re similaire.  Les r√©seaux de neurones convolutifs sont constitu√©s de niveaux, chacun d√©finissant des formes de plus en plus complexes: il identifie les points cl√©s, les combine en lignes, qui √† leur tour se combinent en figures.  Et ce n'est qu'au dernier niveau de la totalit√© des signes trouv√©s que d√©termine l'objet. <br><br>  Les objets du monde r√©el ont beaucoup en commun.  Lors du transfert d'apprentissage, nous utilisons les niveaux d√©j√† d√©finis de d√©finition des fonctionnalit√©s de base et ne formons que les couches responsables de l'identification des objets.  Pour ce faire, quelques centaines de photos et quelques heures de fonctionnement d'un GPU ordinaire nous suffisent.  Le r√©seau a √©t√© initialement form√© sur le jeu de donn√©es COCO (Microsoft Common Objects in Context), qui comprend 91 cat√©gories et 2 500 000 images!  Beaucoup, mais pas 500 millions d'ann√©es d'√©volution. <br><br>  En regardant un peu en avant, cette animation gif (un peu lente, ne d√©file pas imm√©diatement) de tensorboard visualise le processus d'apprentissage.  Comme vous pouvez le voir, le mod√®le commence √† produire un r√©sultat de haute qualit√© presque imm√©diatement, puis vient le broyage: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2d/356/aa4/d2d356aa49bedf31aa918930d8bf1545.gif" alt="image"><br><br>  Le ¬´formateur¬ª du syst√®me API de d√©tection d'objets Tensorflow peut effectuer ind√©pendamment une augmentation, d√©couper des parties al√©atoires des images pour la formation et s√©lectionner des exemples ¬´n√©gatifs¬ª (sections de photos qui ne contiennent aucun objet).  En th√©orie, aucun pr√©traitement photo n'est n√©cessaire.  Cependant, sur un ordinateur personnel avec un disque dur et une petite quantit√© de RAM, il a refus√© de travailler avec des images haute r√©solution: au d√©but, il a suspendu pendant longtemps, a bruiss√© avec un disque, puis s'est envol√©. <br><br>  En cons√©quence, nous avons compress√© les photos √† une taille de 1000x1000 pixels tout en conservant le rapport d'aspect.  Mais comme lors de la compression d'une grande photo, beaucoup de signes sont perdus, d'abord plusieurs carr√©s de taille al√©atoire ont √©t√© coup√©s de chaque photo du rack et compress√©s en 1000x1000.  En cons√©quence, les packs en haute r√©solution (mais pas assez) et en petits (mais beaucoup) sont tomb√©s dans les donn√©es d'entra√Ænement.  Nous r√©p√©tons: cette √©tape est forc√©e et, tr√®s probablement, compl√®tement inutile et peut-√™tre nuisible. <br><br>  Les photos pr√©par√©es et compress√©es sont enregistr√©es dans des r√©pertoires s√©par√©s (eval et train), et leur description (avec les bundles qu'ils contiennent) est form√©e sous la forme de deux donn√©es pandas (train_df et eval_df): <br><br><img src="https://habrastorage.org/webt/dl/lx/pn/dllxpn03xu5us7wr6h7y3rgzayg.png"><br>  Le syst√®me API de d√©tection d'objets Tensorflow requiert que les entr√©es soient pr√©sent√©es sous forme de fichiers tfrecord.  Vous pouvez les former √† l'aide de l'utilitaire, mais nous en ferons un code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">class_text_to_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(row_label)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> row_label == <span class="hljs-string"><span class="hljs-string">'pack'</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:       <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">split</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, group)</span></span></span><span class="hljs-function">:</span></span>   data = namedtuple(<span class="hljs-string"><span class="hljs-string">'data'</span></span>, [<span class="hljs-string"><span class="hljs-string">'filename'</span></span>, <span class="hljs-string"><span class="hljs-string">'object'</span></span>])   gb = df.groupby(group)   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [data(filename, gb.get_group(x))           <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename, x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(gb.groups.keys(), gb.groups)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_tf_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(group, path)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.gfile.GFile(os.path.join(path, <span class="hljs-string"><span class="hljs-string">'{}'</span></span>.format(group.filename)), <span class="hljs-string"><span class="hljs-string">'rb'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> fid:       encoded_jpg = fid.read()   encoded_jpg_io = io.BytesIO(encoded_jpg)   image = Image.open(encoded_jpg_io)   width, height = image.size   filename = group.filename.encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>)   image_format = <span class="hljs-string"><span class="hljs-string">b'jpg'</span></span>   xmins = []   xmaxs = []   ymins = []   ymaxs = []   classes_text = []   classes = []   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index, row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> group.object.iterrows():       xmins.append(row[<span class="hljs-string"><span class="hljs-string">'xmin'</span></span>] / width)       xmaxs.append(row[<span class="hljs-string"><span class="hljs-string">'xmax'</span></span>] / width)       ymins.append(row[<span class="hljs-string"><span class="hljs-string">'ymin'</span></span>] / height)       ymaxs.append(row[<span class="hljs-string"><span class="hljs-string">'ymax'</span></span>] / height)       classes_text.append(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>].encode(<span class="hljs-string"><span class="hljs-string">'utf8'</span></span>))       classes.append(class_text_to_int(row[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]))   tf_example = tf.train.Example(features=tf.train.Features(feature={       <span class="hljs-string"><span class="hljs-string">'image/height'</span></span>: dataset_util.int64_feature(height),       <span class="hljs-string"><span class="hljs-string">'image/width'</span></span>: dataset_util.int64_feature(width),       <span class="hljs-string"><span class="hljs-string">'image/filename'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/source_id'</span></span>: dataset_util.bytes_feature(filename),       <span class="hljs-string"><span class="hljs-string">'image/encoded'</span></span>: dataset_util.bytes_feature(encoded_jpg),       <span class="hljs-string"><span class="hljs-string">'image/format'</span></span>: dataset_util.bytes_feature(image_format),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmin'</span></span>: dataset_util.float_list_feature(xmins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/xmax'</span></span>: dataset_util.float_list_feature(xmaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymin'</span></span>: dataset_util.float_list_feature(ymins),       <span class="hljs-string"><span class="hljs-string">'image/object/bbox/ymax'</span></span>: dataset_util.float_list_feature(ymaxs),       <span class="hljs-string"><span class="hljs-string">'image/object/class/text'</span></span>: dataset_util.bytes_list_feature(classes_text),       <span class="hljs-string"><span class="hljs-string">'image/object/class/label'</span></span>: dataset_util.int64_list_feature(classes),   }))   <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf_example <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert_to_tf_records</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(images_path, examples, dst_file)</span></span></span><span class="hljs-function">:</span></span>   writer = tf.python_io.TFRecordWriter(dst_file)   grouped = split(examples, <span class="hljs-string"><span class="hljs-string">'filename'</span></span>)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> group <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grouped:       tf_example = create_tf_example(group, images_path)       writer.write(tf_example.SerializeToString())   writer.close() convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">train/'</span></span>, train_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">train.record'</span></span>) convert_to_tf_records(<span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{cropped_path}</span></span></span><span class="hljs-string">eval/'</span></span>, eval_df, <span class="hljs-string"><span class="hljs-string">f'</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{detector_data_path}</span></span></span><span class="hljs-string">eval.record'</span></span>)</code> </pre><br>  Il nous reste √† pr√©parer un r√©pertoire sp√©cial et √† d√©marrer les processus: <br><br><img src="https://habrastorage.org/webt/ld/yd/rb/ldydrbk3ivkixsx9kep69b5eyoy.jpeg"><br><br>  La structure peut √™tre diff√©rente, mais nous la trouvons tr√®s pratique. <br><br>  Le r√©pertoire de donn√©es contient les fichiers que nous avons cr√©√©s avec tfrecords (train.record et eval.record), ainsi que pack.pbtxt avec les types d'objets pour lesquels nous formerons le r√©seau neuronal.  Nous n'avons qu'un seul type d'objet √† d√©finir, donc le fichier est tr√®s court: <br><br><img src="https://habrastorage.org/webt/pc/jw/x0/pcjwx0l3fc-wuzhiyt7lfo9xtcq.png"><br><br>  Le r√©pertoire des mod√®les (il peut y avoir de nombreux mod√®les pour r√©soudre un probl√®me) dans le r√©pertoire enfant ssd_mobilenet_v1 contient les param√®tres de formation dans le fichier .config, ainsi que deux r√©pertoires vides: train et eval.  En train, le ¬´formateur¬ª enregistrera les points de contr√¥le du mod√®le, ¬´l'√©valuateur¬ª les r√©cup√©rera, les ex√©cutera sur les donn√©es pour √©valuation et les placera dans le r√©pertoire eval.  Tensorboard gardera une trace de ces deux r√©pertoires et affichera les informations sur le processus. <br><br>  Description d√©taill√©e de la structure des fichiers de configuration, etc.  peut √™tre trouv√© <a href="">ici</a> et <a href="">ici</a> .  Les instructions d'installation de l'API de d√©tection d'objets Tensorflow peuvent √™tre trouv√©es <a href="">ici</a> . <br><br>  Nous allons dans le r√©pertoire models / research / object_detection et d√©gonflons le mod√®le pr√©-form√©: <br><br><pre> <code class="bash hljs">wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz tar -xvzf ssd_mobilenet_v1_coco_2017_11_17.tar.gz</code> </pre><br>  Nous y copions le r√©pertoire pack_detector que nous avons pr√©par√©. <br><br>  Commencez d'abord le processus de formation: <br><br><pre> <code class="bash hljs">python3 train.py --logtostderr \   --train_dir=pack_detector/models/ssd_mobilenet_v1/train/ \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config</code> </pre><br>  Nous commen√ßons le processus d'√©valuation.  Nous n'avons pas de deuxi√®me carte vid√©o, nous la lan√ßons donc sur le processeur (en utilisant l'instruction CUDA_VISIBLE_DEVICES = "").  Pour cette raison, il sera tr√®s en retard concernant le processus de formation, mais ce n'est pas si mal: <br><br><pre> <code class="bash hljs">CUDA_VISIBLE_DEVICES=<span class="hljs-string"><span class="hljs-string">""</span></span> python3 eval.py \   --logtostderr \   --checkpoint_dir=pack_detector/models/ssd_mobilenet_v1/train \   --pipeline_config_path=pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --eval_dir=pack_detector/models/ssd_mobilenet_v1/<span class="hljs-built_in"><span class="hljs-built_in">eval</span></span></code> </pre> <br>  Nous commen√ßons le processus de tensorboard: <br><br><pre> <code class="bash hljs">tensorboard --logdir=pack_detector/models/ssd_mobilenet_v1</code> </pre> <br>  Apr√®s cela, nous pouvons voir de beaux graphiques, ainsi que le travail r√©el du mod√®le sur les donn√©es estim√©es (gif au d√©but): <br><br><img src="https://habrastorage.org/webt/qc/wt/rl/qcwtrlmdugb4zgyhy6gnnaoia9w.jpeg"><br><br>  Le processus de formation peut √™tre interrompu et repris √† tout moment.  Lorsque nous pensons que le mod√®le est suffisamment bon, nous enregistrons le point de contr√¥le sous la forme d'un graphique d'inf√©rence: <br><br><pre> <code class="bash hljs">python3 export_inference_graph.py \   --input_type image_tensor \   --pipeline_config_path pack_detector/models/ssd_mobilenet_v1/ssd_mobilenet_v1_pack.config \   --trained_checkpoint_prefix pack_detector/models/ssd_mobilenet_v1/train/model.ckpt-13756 \   --output_directory pack_detector/models/ssd_mobilenet_v1/pack_detector_2018_06_03</code> </pre> <br>  Ainsi, √† cette √©tape, nous avons obtenu un graphe d'inf√©rence, que nous pouvons utiliser pour rechercher des objets group√©s.  Nous passons √† son utilisation. <br><br>  <b>√âtape 4. Impl√©mentation de la recherche ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien github</a> )</b> <br><br>  Le code de chargement et d'initialisation du graphe d'inf√©rence se trouve sur le lien ci-dessus.  Fonctions de recherche cl√©s: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's write function that executes detection def run_inference_for_single_image(image, image_tensor, sess, tensor_dict):   # Run inference   expanded_dims = np.expand_dims(image, 0)   output_dict = sess.run(tensor_dict, feed_dict={image_tensor: expanded_dims})   # all outputs are float32 numpy arrays, so convert types as appropriate   output_dict['num_detections'] = int(output_dict['num_detections'][0])   output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)   output_dict['detection_boxes'] = output_dict['detection_boxes'][0]   output_dict['detection_scores'] = output_dict['detection_scores'][0]   return output_dict # it is useful to be able to run inference not only on the whole image, # but also on its parts # cutoff - minimum detection score needed to take box def run_inference_for_image_part(image_tensor, sess, tensor_dict,                                image, cutoff, ax0, ay0, ax1, ay1):   boxes = []   im = image[ay0:ay1, ax0:ax1]   h, w, c = im.shape   output_dict = run_inference_for_single_image(im, image_tensor, sess, tensor_dict)   for i in range(100):       if output_dict['detection_scores'][i] &lt; cutoff:           break       y0, x0, y1, x1, score = *output_dict['detection_boxes'][i], \                               output_dict['detection_scores'][i]       x0, y0, x1, y1, score = int(x0*w), int(y0*h), \                               int(x1*w), int(y1*h), \                               int(score * 100)       boxes.append((x0+ax0, y0+ay0, x1+ax0, y1+ay0, score))   return boxes</span></span></code> </pre> <br>  La fonction trouve des bo√Ætes d√©limit√©es pour les packs non pas dans toute la photo, mais dans sa partie.  La fonction filtre √©galement les rectangles trouv√©s avec un faible score de d√©tection sp√©cifi√© dans le param√®tre de coupure. <br><br>  Cela s'av√®re √™tre un dilemme.  D'une part, avec une coupure √©lev√©e, nous perdons beaucoup d'objets, d'autre part, avec une coupure basse, nous commen√ßons √† trouver de nombreux objets qui ne sont pas des paquets.  En m√™me temps, on ne trouve toujours pas tout et pas id√©alement: <br><img src="https://habrastorage.org/webt/-u/ao/7y/-uao7ylwycrzrn3xqd1kfh2q0bs.jpeg"><br>  Cependant, notez que si nous ex√©cutons la fonction pour un petit morceau de la photo, la reconnaissance est presque parfaite avec cutoff = 0.9: <br><br><img src="https://habrastorage.org/webt/gg/qn/ia/ggqniasnrkggb6bjnarwum_6i48.jpeg"><br><br>  Cela est d√ª au fait que le mod√®le SSD MobileNet V1 accepte des photos 300x300 en entr√©e.  Naturellement, avec une telle compression, beaucoup de signes sont perdus. <br><br>  Mais ces signes persistent si l'on d√©coupe un petit carr√© contenant plusieurs packs.  Cela sugg√®re l'id√©e d'utiliser une fen√™tre flottante: nous parcourons un petit rectangle sur une photo et nous nous souvenons de tout ce que nous avons trouv√©. <br><br><img src="https://habrastorage.org/webt/zn/7s/dz/zn7sdzl4wcb9dwugzkxilg2d2hk.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Un probl√®me se pose: on retrouve plusieurs fois les m√™mes packs, parfois dans une version tr√®s tronqu√©e. </font><font style="vertical-align: inherit;">Ce probl√®me peut √™tre r√©solu √† l'aide d'un algorithme de suppression non maximal. </font><font style="vertical-align: inherit;">L'id√©e est extr√™mement simple: en une seule √©tape, nous trouvons un rectangle avec un score de d√©tection maximal, rappelez-vous, supprimez tous les autres rectangles qui ont une zone d'intersection avec elle plus que overlapTresh (une impl√©mentation a √©t√© trouv√©e sur Internet avec quelques modifications):</font></font><br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># function for non-maximum suppression def non_max_suppression(boxes, overlapThresh):   if len(boxes) == 0:       return np.array([]).astype("int")   if boxes.dtype.kind == "i":       boxes = boxes.astype("float")   pick = []   x1 = boxes[:,0]   y1 = boxes[:,1]   x2 = boxes[:,2]   y2 = boxes[:,3]   sc = boxes[:,4]   area = (x2 - x1 + 1) * (y2 - y1 + 1)   idxs = np.argsort(sc)   while len(idxs) &gt; 0:       last = len(idxs) - 1       i = idxs[last]       pick.append(i)       xx1 = np.maximum(x1[i], x1[idxs[:last]])       yy1 = np.maximum(y1[i], y1[idxs[:last]])       xx2 = np.minimum(x2[i], x2[idxs[:last]])       yy2 = np.minimum(y2[i], y2[idxs[:last]])       w = np.maximum(0, xx2 - xx1 + 1)       h = np.maximum(0, yy2 - yy1 + 1)       #todo fix overlap-contains...       overlap = (w * h) / area[idxs[:last]]              idxs = np.delete(idxs, np.concatenate(([last],           np.where(overlap &gt; overlapThresh)[0])))     return boxes[pick].astype("int")</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le r√©sultat est visuellement presque parfait: </font></font><br><br><img src="https://habrastorage.org/webt/go/hd/yz/gohdyzvwuyu8ja4w893big9c1yy.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le r√©sultat du travail sur une photo de mauvaise qualit√© avec un grand nombre de packs: </font></font><br><br><img src="https://habrastorage.org/webt/jm/oi/da/jmoida7irvm4u3drey8nu6g00kw.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Comme nous pouvons le voir, le nombre d'objets et la qualit√© des photos ne nous ont pas emp√™ch√©s de reconna√Ætre correctement tous les colis, ce que nous visions.</font></font><br><br><h3>  Conclusion </h3><br>       ¬´¬ª:         ,       . ,    ,         ..    . <br><br>       ,    ,    : <br><br><ol><li>  150  ,     ,   , </li><li>        3-7  , </li><li>   100    , </li><li>        , </li><li>        (), </li><li>    (,  ), </li><li>    ,        ¬´¬ª, </li><li>  ,   ,     (SSD  ), </li><li>      ,  , </li><li>  . </li></ol><br>         ,      ,    . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416123/">https://habr.com/ru/post/fr416123/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416111/index.html">Conversion de donn√©es GraphQL pour le composant CustomTreeData de DevExtreme-Reactive</a></li>
<li><a href="../fr416113/index.html">Steven Wolfram: Souvenirs de Steve Jobs</a></li>
<li><a href="../fr416115/index.html">10 petites erreurs de conception que nous faisons encore</a></li>
<li><a href="../fr416119/index.html">Vendredi post mercredi: top des packages NPM les plus ¬´essentiels¬ª</a></li>
<li><a href="../fr416121/index.html">Fujitsu Artificial Intelligence calcule la g√©om√©trie des mat√©riaux magn√©tiques</a></li>
<li><a href="../fr416125/index.html">Installation, configuration du syst√®me et contr√¥le des cam√©ras</a></li>
<li><a href="../fr416127/index.html">CUDA et GPU √† distance</a></li>
<li><a href="../fr416129/index.html">Comment l'IA apprend √† g√©n√©rer des images de chats</a></li>
<li><a href="../fr416131/index.html">Comment g√©rer la MP dans la F√©d√©ration de Russie et ne pas enfreindre la loi</a></li>
<li><a href="../fr416133/index.html">Centre de donn√©es √† l'√©tranger: Equinix LD8</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>