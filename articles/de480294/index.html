<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöã üõåüèΩ üîù Hochgeschwindigkeits-Bildverarbeitung im vielseitigen LEGO Teilsortierger√§t üëö ‚öíÔ∏è üññ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In den letzten Jahren habe ich eine Maschine entwickelt und hergestellt, mit der LEGO Teile erkannt und sortiert werden k√∂nnen. Der wichtigste Teil de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hochgeschwindigkeits-Bildverarbeitung im vielseitigen LEGO Teilsortierger√§t</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480294/"> In den letzten Jahren habe ich eine Maschine entwickelt und hergestellt, mit der LEGO Teile erkannt und sortiert werden k√∂nnen.  Der wichtigste Teil der Maschine ist die <strong>Erfassungseinheit</strong> , ein kleines, fast vollst√§ndig geschlossenes Fach, in dem sich ein F√∂rderband, eine Beleuchtung und eine Kamera befinden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/6c7/5d8/6416c75d85738aceecf1162f1d48718d.jpg"></div><br>  <i>Die Beleuchtung sehen Sie etwas tiefer.</i> <br><br>  Die Kamera nimmt Fotos der LEGO-Teile auf, die durch den F√∂rderer kommen, und √ºbertr√§gt die Bilder dann drahtlos an einen Server, auf dem ein Algorithmus f√ºr k√ºnstliche Intelligenz ausgef√ºhrt wird, um das Teil unter Tausenden von m√∂glichen LEGO-Elementen zu erkennen.  In zuk√ºnftigen Artikeln werde ich Ihnen mehr √ºber den AI-Algorithmus erz√§hlen. Dieser Artikel wird sich auf die Verarbeitung konzentrieren, die zwischen der Rohausgabe der Videokamera und der Eingabe in das neuronale Netzwerk durchgef√ºhrt wird. <br><br>  Das Hauptproblem, das ich l√∂sen musste, bestand darin, den Videostream vom F√∂rderer in separate Bilder von Teilen zu konvertieren, die ein neuronales Netzwerk verwenden konnte. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca8/feb/2ea/ca8feb2eafab64d7d835d50e090e7bc5.gif"></div><br>  <i>Das ultimative Ziel: Wechseln Sie von einem Rohvideo (links) zu einer Reihe von Bildern derselben Gr√∂√üe (rechts), um sie in ein neuronales Netzwerk zu √ºbertragen.</i>  <i>(im vergleich zur realen arbeit ist gif etwa halb so langsam)</i> <br><br>  Dies ist ein gro√üartiges Beispiel f√ºr eine Aufgabe, die oberfl√§chlich betrachtet einfach zu sein scheint, aber tats√§chlich viele einzigartige und interessante Hindernisse aufwirft, von denen viele f√ºr Bildverarbeitungsplattformen einzigartig sind. <br><br>  Das Abrufen der richtigen Teile eines Bildes auf diese Weise wird h√§ufig als Objekterkennung bezeichnet.  Genau das muss ich tun: um das Vorhandensein von Objekten, deren Position und Gr√∂√üe zu erkennen, damit Sie f√ºr jedes Teil in jedem Frame <strong>Begrenzungsrechtecke erstellen</strong> k√∂nnen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfc/265/c31/cfc265c31250940a6170ad404f89de08.gif"></div><br>  <i>Das Wichtigste ist, gute Begrenzungsrahmen zu finden (oben in Gr√ºn dargestellt)</i> <br><br>  Ich werde drei Aspekte bei der L√∂sung des Problems ber√ºcksichtigen: <br><br><ul><li>  Vorbereiten, um unn√∂tige Variablen zu entfernen </li><li>  Erstellen eines Prozesses aus einfachen Bildverarbeitungsvorg√§ngen </li><li>  Aufrechterhaltung einer ausreichenden Leistung auf einer Raspberry Pi-Plattform mit begrenzten Ressourcen </li></ul><br><h2>  Eliminierung unn√∂tiger Variablen </h2><br>  Bei solchen Aufgaben ist es am besten, so viele Variablen wie m√∂glich zu entfernen, bevor Sie Bildverarbeitungstechniken anwenden.  Zum Beispiel sollte ich mir keine Gedanken √ºber Umgebungsbedingungen, unterschiedliche Kamerapositionen und Informationsverlust machen, da einige Teile von anderen √ºberlappen.  Nat√ºrlich ist es m√∂glich (wenn auch sehr schwierig), alle diese Variablen programmgesteuert aufzul√∂sen, aber zum Gl√ºck wurde diese Maschine von Grund auf neu erstellt.  Ich selbst kann mich auf eine erfolgreiche L√∂sung vorbereiten und alle St√∂rungen beseitigen, noch bevor ich anfing, Code zu schreiben. <br><br>  Der erste Schritt besteht darin, die Position, den Winkel und den Fokus der Kamera fest festzulegen.  Damit ist alles einfach - im System ist die Kamera √ºber dem F√∂rderer montiert.  Ich muss mich nicht um St√∂rungen durch andere Teile k√ºmmern.  Unerw√ºnschte Objekte haben fast keine Chance, in die Erfassungseinheit zu gelangen.  Ein bisschen komplizierter, aber es ist sehr wichtig, <strong>konstante Lichtverh√§ltnisse</strong> zu gew√§hrleisten.  Ich brauche den Objekterkenner nicht, um den Schatten eines sich bewegenden Teils auf dem Band f√§lschlicherweise als ein physisches Objekt zu interpretieren.  Gl√ºcklicherweise ist die Erfassungseinheit sehr klein (das gesamte Sichtfeld der Kamera ist kleiner als ein Laib Brot), sodass ich mehr als genug Kontrolle √ºber die Umgebungsbedingungen hatte. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8ec/9da/652/8ec9da6525ba0a6f906d3e1ef6309647.jpg"></div><br>  <i>Aufnahmeeinheit, Innenansicht.</i>  <i>Die Kamera befindet sich im oberen Drittel des Rahmens.</i> <br><br>  Eine L√∂sung besteht darin, das Fach vollst√§ndig geschlossen zu halten, damit keine Au√üenbeleuchtung eintritt.  Ich habe diesen Ansatz mit LED-Streifen als Lichtquelle ausprobiert.  Leider hat sich das System als sehr launisch herausgestellt - nur ein kleines Loch im Geh√§use reicht aus, und das Licht dringt in das Fach ein, sodass Objekte nicht erkannt werden k√∂nnen. <br><br>  Am Ende bestand die beste L√∂sung darin, alle anderen Lichtquellen zu ‚Äûverstopfen‚Äú, indem das kleine Fach mit starkem Licht gef√ºllt wurde.  Es hat sich herausgestellt, dass die Lichtquellen, mit denen Wohnr√§ume beleuchtet werden k√∂nnen, sehr billig und einfach zu handhaben sind. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/16a/031/025/16a031025c08531ff3ef2f0ef20a2394.jpg"></div><br>  <i>Holen Sie sich die Schatten!</i> <br><br>  Wenn die Quelle in das winzige Fach gerichtet ist, werden alle potenziellen externen Lichtst√∂rungen vollst√§ndig blockiert.  Ein solches System hat auch einen praktischen Nebeneffekt: Aufgrund der gro√üen Lichtmenge in der Kamera k√∂nnen Sie eine sehr hohe Verschlusszeit verwenden, um perfekte Bilder von Teilen zu erhalten, selbst wenn Sie sich schnell auf dem F√∂rderband bewegen. <br><br><h2>  Objekterkennung </h2><br>  Wie habe ich es geschafft, dieses sch√∂ne Video mit gleichm√§√üiger Beleuchtung in die Bounding-Boxen zu verwandeln, die ich brauchte?  Wenn Sie mit AI arbeiten, k√∂nnen Sie mir vorschlagen, ein neuronales Netzwerk zur Objekterkennung wie <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow">YOLO</a> oder <a href="https://arxiv.org/abs/1506.01497" rel="nofollow">Faster R-CNN zu</a> implementieren.  Diese neuronalen Netze k√∂nnen die Aufgabe leicht bew√§ltigen.  Leider f√ºhre ich Objekterkennungscode auf <a href="https://www.raspberrypi.org/" rel="nofollow">Raspberry Pi aus</a> .  Sogar ein leistungsf√§higer Computer w√ºrde Probleme haben, diese Faltungs-Neuronalen Netze mit der Frequenz auszuf√ºhren, die ich etwa 90 FPS ben√∂tigte.  Und Raspberry pi, das keine AI-kompatible GPU hat, k√∂nnte mit einer sehr reduzierten Version eines solchen AI-Algorithmus nicht fertig werden.  Ich kann Videos von einem Pi auf einen anderen Computer streamen, aber die Echtzeit-Video√ºbertragung ist ein sehr launischer Prozess. Verz√∂gerungen und Bandbreitenbeschr√§nkungen verursachen schwerwiegende Probleme, insbesondere, wenn Sie eine hohe Daten√ºbertragungsgeschwindigkeit ben√∂tigen. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/K9a6mGNmhbc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>YOLO ist sehr cool!</i>  <i>Aber ich brauche nicht alle Funktionen.</i> <br><br>  Gl√ºcklicherweise konnte ich eine schwierige AI-basierte L√∂sung vermeiden, indem ich die Bildverarbeitungstechniken der ‚Äûalten Schule‚Äú verwendete.  Die erste Technik ist die <strong>Hintergrundsubtraktion</strong> , bei der versucht wird, alle ver√§nderten Teile des Bildes zu isolieren.  In meinem Fall sind die LEGO-Details das einzige, was sich im Sichtfeld der Kamera bewegt.  (Nat√ºrlich bewegt sich das Band auch, aber da es eine einheitliche Farbe hat, scheint es station√§r zur Kamera zu sein).  Trenne diese LEGO-Details vom Hintergrund und die H√§lfte des Problems ist gel√∂st. <br><br>  Damit die Hintergrundsubtraktion funktioniert, m√ºssen sich die Vordergrundobjekte erheblich vom Hintergrund unterscheiden.  LEGO-Details haben eine gro√üe Auswahl an Farben, daher musste ich die Hintergrundfarbe sehr sorgf√§ltig ausw√§hlen, damit sie so weit wie m√∂glich von LEGO-Farben entfernt war.  Deshalb besteht das Klebeband unter der Kamera aus Papier - es muss nicht nur sehr homogen sein, sondern kann auch nicht aus LEGO bestehen, sonst hat es die Farbe eines der Teile, die ich erkennen muss!  Ich habe mich f√ºr Hellrosa entschieden, aber jede andere Pastellfarbe, im Gegensatz zu normalen LEGO-Farben, reicht aus. <br><br>  Die wunderbare OpenCV-Bibliothek verf√ºgt bereits √ºber mehrere Algorithmen zur Hintergrundsubtraktion.  Der MOG2 Background Subtractor ist der komplexeste von ihnen und arbeitet gleichzeitig unglaublich schnell, sogar auf Himbeer-Pi.  Das direkte Zuf√ºhren von Videobildern zu MOG2 funktioniert jedoch nicht sehr gut.  Hellgraue und wei√üe Figuren kommen der Helligkeit eines blassen Hintergrunds zu nahe und gehen darauf verloren.  Ich musste einen Weg finden, um das Klebeband klarer von den darauf befindlichen Teilen zu trennen, und dem Hintergrund-Subtrahierer befehlen, die <i>Farbe</i> und nicht die <i>Helligkeit</i> genauer zu betrachten.  Dazu hat es mir gereicht, die Bilds√§ttigung zu erh√∂hen, bevor ich sie auf einen Hintergrund-Subtrahierer √ºbertrug.  Die Ergebnisse haben sich deutlich verbessert. <br><br>  Nachdem ich den Hintergrund subtrahiert hatte, musste ich morphologische Operationen anwenden, um so viel Rauschen wie m√∂glich zu beseitigen.  Um die Konturen von wei√üen Bereichen zu finden, k√∂nnen Sie die Funktion findContours () der OpenCV-Bibliothek verwenden.  Indem Sie verschiedene Heuristiken anwenden, um rauschhaltige Schleifen abzulenken, k√∂nnen Sie diese Schleifen einfach in vordefinierte Begrenzungsrahmen konvertieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/214/785/f54/214785f5468b2b84ae29f212a66d5abd.gif"></div><br><h2>  Leistung </h2><br>  Ein neuronales Netzwerk ist eine gefr√§√üige Kreatur.  F√ºr beste Ergebnisse bei der Klassifizierung ben√∂tigt sie Bilder mit maximaler Aufl√∂sung und in m√∂glichst gro√üen Mengen.  Dies bedeutet, dass ich sie mit einer sehr hohen Bildrate aufnehmen muss, w√§hrend die Bildqualit√§t und Aufl√∂sung erhalten bleiben.  Ich muss das Maximum aus der Kamera und der GPU Raspberry PI herausholen. <br><br>  Eine sehr ausf√ºhrliche <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">Dokumentation f√ºr Picamera</a> besagt, dass der V2- <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">Kamerachip</a> Bilder mit einer Gr√∂√üe von 1280 <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">x</a> 720 Pixeln mit einer maximalen Frequenz von 90 Bildern pro Sekunde erzeugen kann.  Dies ist eine unglaubliche Datenmenge, und obwohl die Kamera sie erzeugen kann, bedeutet dies nicht, dass ein Computer damit umgehen kann.  Wenn ich rohe 24-Bit-RGB-Bilder verarbeiten w√ºrde, m√ºsste ich Daten mit einer Geschwindigkeit von ungef√§hr 237 MB / s √ºbertragen, was f√ºr die schlechte GPU des Pi-Computers und f√ºr SDRAM zu viel ist.  Selbst wenn die GPU-beschleunigte Komprimierung in JPEG verwendet wird, k√∂nnen 90 fps nicht erreicht werden. <br><br>  Der Raspberry Pi kann rohe, ungefilterte YUV-Bilder anzeigen.  Obwohl es schwieriger ist, mit RGB zu arbeiten als mit RGB, verf√ºgt YUV tats√§chlich √ºber viele praktische Eigenschaften.  Das wichtigste ist, dass nur 12 Bit pro Pixel gespeichert werden (f√ºr RGB sind es 24 Bit). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d50/28a/686/d5028a6862f79b8caf4d6f895dd46d84.png"></div><br>  <i>Alle vier Bytes von Y haben ein Byte U und ein Byte V, dh 1,5 Bytes pro Pixel.</i> <br><br>  Dies bedeutet, dass ich im Vergleich zu RGB-Frames <strong>doppelt so viele</strong> YUV-Frames verarbeiten kann, und dies z√§hlt nicht die zus√§tzliche Zeit, die die GPU beim Konvertieren in ein RGB-Bild einspart. <br><br>  Dieser Ansatz f√ºhrt jedoch zu eindeutigen Einschr√§nkungen des Verarbeitungsprozesses.  Die meisten Vorg√§nge mit einem Videobild in voller Gr√∂√üe verbrauchen extrem viel Speicher und CPU-Ressourcen.  Innerhalb meiner strengen Fristen ist es nicht einmal m√∂glich, ein YUV-Vollbild zu dekodieren. <br><br>  Zum Gl√ºck muss ich nicht den gesamten Frame verarbeiten!  F√ºr die Erkennung von Objekten m√ºssen Begrenzungsrechtecke nicht genau sein. Die ungef√§hre Genauigkeit ist ausreichend, sodass der gesamte Prozess der Erkennung von Objekten mit einem viel kleineren Rahmen ausgef√ºhrt werden kann.  Beim Verkleinern m√ºssen nicht alle Pixel eines Vollbilds ber√ºcksichtigt werden, sodass Bilder sehr schnell und kostenlos verkleinert werden k√∂nnen.  Dann vergr√∂√üert sich der Ma√üstab der resultierenden Begrenzungsrechtecke erneut und wird zum Ausschneiden von Objekten aus einem YUV-Frame in voller Gr√∂√üe verwendet.  Dank dessen muss ich nicht den gesamten hochaufl√∂senden Frame dekodieren oder anderweitig verarbeiten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fcd/d6a/9e4/fcdd6a9e466f0b87be4dbb349f19e402.png"></div><br>  Gl√ºcklicherweise ist es dank der Speichermethode dieses YUV-Formats (siehe oben) sehr einfach, schnelle Beschneidungs- und Zoomvorg√§nge zu implementieren, die direkt mit dem YUV-Format arbeiten.  Dar√ºber hinaus kann der gesamte Prozess problemlos auf vier Pi-Kerne parallelisiert werden.  Ich habe jedoch herausgefunden, dass nicht alle Kerne ihr volles Potenzial aussch√∂pfen, und dies zeigt, dass die Speicherbandbreite immer noch der Engpass ist.  Trotzdem schaffte ich es, in der Praxis 70-80 FPS zu erreichen.  Eine genauere Analyse der Speichernutzung kann dazu beitragen, die Dinge noch weiter zu beschleunigen. <br><br><hr><br>  Wenn Sie mehr √ºber das Projekt erfahren m√∂chten, lesen Sie meinen vorherigen Artikel: <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">‚ÄûWie ich mehr als 100.000 LEGO-Bilder zum Lernen erstellt habe</a> . <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">‚Äú</a> <br><br>  Video der Bedienung der gesamten Sortiermaschine: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/04JkdHEX3Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de480294/">https://habr.com/ru/post/de480294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de480280/index.html">TurboConf-Sicherheitsstudie</a></li>
<li><a href="../de480282/index.html">Die Geschichte des Fitness-Startups Peloton: von einer Bewertung von 8 Milliarden US-Dollar bis zu erfolgloser Werbung und Prognosen f√ºr einen 85-prozentigen R√ºckgang der Lagerbest√§nde</a></li>
<li><a href="../de480284/index.html">Meine (Nano-) Erfahrung mit der Yandex.Maps-API oder warum ben√∂tige ich Anweisungen?</a></li>
<li><a href="../de480288/index.html">Ist es m√∂glich, Informationen ohne Einschr√§nkung der Entfernung und Lichtgeschwindigkeit zu senden und zu empfangen?</a></li>
<li><a href="../de480290/index.html">Selbst gemachter Laptop ZedRipper auf sechzehn Z80</a></li>
<li><a href="../de480296/index.html">Reaktive Entwicklung des Telegrammbots</a></li>
<li><a href="../de480300/index.html">Im Jahr 2011 war die Frage, ob Nginx Igor Sysoev oder Rambler geh√∂rt</a></li>
<li><a href="../de480306/index.html">Warum die geschlossene T√ºr schlagen?</a></li>
<li><a href="../de480310/index.html">Habra-Detektiv: Das Geheimnis der Nachrichtenredakteure</a></li>
<li><a href="../de480316/index.html">So reduzieren Sie den Verbrauch von WLAN-Modulen um das Zehnfache oder mehr</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>