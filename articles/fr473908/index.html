<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∑üèº üßöüèº üõ§Ô∏è Assurer les ressources √† Cuba üí™üèª üöâ üåô</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La gestion des ressources de cluster est toujours un sujet complexe. Comment expliquer la n√©cessit√© de configurer des ressources pour l'utilisateur qu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Assurer les ressources √† Cuba</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473908/"><p><img src="https://habrastorage.org/webt/bp/bf/_p/bpbf_pwx_8zuvlwptjwkfnbxb5y.jpeg" alt="image"></p><br><p>  La gestion des ressources de cluster est toujours un sujet complexe.  Comment expliquer la n√©cessit√© de configurer des ressources pour l'utilisateur qui d√©ploie ses applications sur le cluster?  Peut-√™tre qu'il est plus facile d'automatiser cela? </p><a name="habracut"></a><br><h3 id="opisanie-problemy">  Description du probl√®me </h3><br><p>  La gestion des ressources est une t√¢che importante dans le contexte de l'administration du cluster Kubernetes.  Mais pourquoi est-ce important si Kubernetes fait tout le travail dur pour vous?  Parce que non.  Kubernetes vous fournit des outils pratiques pour r√©soudre de nombreux probl√®mes ... si vous utilisez ces outils.  Pour chaque module de votre cluster, vous pouvez sp√©cifier les ressources n√©cessaires pour ses conteneurs.  Et Kubernetes utilisera ces informations pour distribuer les instances de votre application sur les n≈ìuds de cluster. </p><br><p> Peu de gens prennent au s√©rieux la gestion des ressources chez Kubernetes.  Ceci est normal pour un cluster l√©g√®rement charg√© avec quelques applications statiques.  Mais que faire si vous avez un cluster tr√®s dynamique?  O√π vont et viennent les applications, o√π l'espace de noms est-il cr√©√© et supprim√© tout le temps?  Un cluster avec un grand nombre d'utilisateurs qui peuvent cr√©er leur propre espace de noms et d√©ployer des applications?  Eh bien, dans ce cas, au lieu d'une orchestration stable et pr√©visible, vous aurez un tas de plantages al√©atoires dans les applications, et parfois m√™me dans les composants de Kubernetes lui-m√™me! </p><br><p>  Voici un exemple d'un tel cluster: </p><br><p><img src="https://habrastorage.org/webt/0y/ar/2p/0yar2pe_8-bk-8cwrkd95bz8leu.png" alt="image"></p><br><p>  Vous voyez 3 foyers √† l'√©tat "Termin√©".  Mais ce n'est pas la suppression habituelle des foyers - ils sont bloqu√©s dans cet √©tat parce que le d√©mon containerd sur leur n≈ìud a √©t√© touch√© par quelque chose de tr√®s gourmand en ressources. </p><br><p>  De tels probl√®mes peuvent √™tre r√©solus en g√©rant correctement le manque de ressources, mais ce n'est pas le sujet de cet article (il y a un bon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> ), ainsi qu'une solution miracle pour r√©soudre tous les probl√®mes de ressources. </p><br><p>  La principale raison de ces probl√®mes est incorrecte ou le manque de gestion des ressources dans le cluster.  Et si ce type de probl√®me n'est pas un d√©sastre pour les d√©ploiements, car ils en cr√©eront facilement un nouveau, alors pour des entit√©s comme DaemonSet, ou plus encore pour StatefulSet, de tels gels seront fatals et n√©cessiteront une intervention manuelle. </p><br><p>  Vous pouvez avoir un √©norme cluster avec beaucoup de CPU et de m√©moire.  Lorsque vous ex√©cutez de nombreuses applications dessus sans param√®tres de ressources appropri√©s, il est possible que tous les pods gourmands en ressources soient plac√©s sur le m√™me n≈ìud.  Ils se battront pour des ressources, m√™me si les n≈ìuds restants du cluster restent pratiquement libres. </p><br><p>  Vous pouvez √©galement souvent voir des cas moins critiques o√π certaines applications sont affect√©es par leurs voisins.  M√™me si les ressources de ces applications "innocentes" √©taient correctement configur√©es, un sous-marin errant peut venir les tuer.  Un exemple d'un tel sc√©nario: </p><br><ol><li>  Votre application demande 4 Go de m√©moire, mais ne prend initialement que 1 Go. </li><li>  Une errance sous, sans configuration de ressource, est affect√©e au m√™me noeud. </li><li>  L'errance sous consomme toute la m√©moire disponible. </li><li>  Votre application tente d'allouer plus de m√©moire et se bloque car il n'y en a plus. </li></ol><br><p>  La r√©√©valuation est un autre cas assez populaire.  Certains d√©veloppeurs font d'√©normes demandes dans des manifestes ¬´au cas o√π¬ª et n'utilisent jamais ces ressources.  Le r√©sultat est un gaspillage d'argent. </p><br><h3 id="teoriya-resheniya">  Th√©orie de la d√©cision </h3><br><p>  Horreur!  Non? <br>  Heureusement, Kubernetes offre un moyen d'imposer certaines restrictions aux pods en sp√©cifiant les configurations de ressources par d√©faut ainsi que les valeurs minimales et maximales.  Ceci est impl√©ment√© √† l'aide de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">objet LimitRange</a> .  LimitRange est un outil tr√®s pratique lorsque vous avez un nombre limit√© d'espaces de noms ou un contr√¥le total sur le processus de cr√©ation.  M√™me sans la configuration appropri√©e des ressources, vos applications seront limit√©es dans leur utilisation.  Les foyers ¬´innocents¬ª correctement r√©gl√©s seront s√ªrs et prot√©g√©s des voisins nuisibles.  Si quelqu'un d√©ploie une application gourmande sans configuration de ressources, cette application recevra les valeurs par d√©faut et se bloquera probablement.  Et c'est tout!  L'application ne tra√Ænera plus personne. </p><br><p>  Ainsi, nous avons un outil pour contr√¥ler et forcer la configuration des ressources pour les foyers, maintenant il semble que nous soyons en s√©curit√©.  Alors?  Pas vraiment.  Le fait est que, comme nous l'avons d√©crit pr√©c√©demment, nos espaces de noms peuvent √™tre cr√©√©s par les utilisateurs, et par cons√©quent, LimitRange peut ne pas √™tre pr√©sent dans ces espaces de noms, car il doit √™tre cr√©√© s√©par√©ment dans chaque espace de noms.  Par cons√©quent, nous avons besoin de quelque chose non seulement au niveau de l'espace de noms, mais aussi au niveau du cluster.  Mais il n'y a pas encore de telle fonction dans Kubernetes. </p><br><p>  C'est pourquoi j'ai d√©cid√© d'√©crire ma solution √† ce probl√®me.  Permettez-moi de vous pr√©senter - Op√©rateur de limite.  Il s'agit d'un op√©rateur cr√©√© sur la base du framework <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Operator SDK</a> , qui utilise la ressource personnalis√©e ClusterLimit et permet de s√©curiser toutes les applications "innocentes" du cluster.  En utilisant cet op√©rateur, vous pouvez contr√¥ler les valeurs par d√©faut et les limites de ressources pour tous les espaces de noms en utilisant la quantit√© minimale de configuration.  Il vous permet √©galement de choisir exactement o√π appliquer la configuration √† l'aide de namespaceSelector. </p><br><div class="spoiler">  <b class="spoiler_title">Exemple de ClusterLimit</b> <div class="spoiler_text"><pre><code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: default-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"limited"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"800m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"1Gi"</span></span> min: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"99Mi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"700m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"900Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"110m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"111Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Avec cette configuration, l'op√©rateur cr√©era un LimitRange uniquement dans l'espace de noms avec le label <code>limit: limited</code> .  Cela sera utile pour fournir des restrictions plus strictes dans un groupe sp√©cifique d'espaces de noms.  Si namespaceSelector n'est pas sp√©cifi√©, l'op√©rateur appliquera un LimitRange √† tous les espaces de noms.  Si vous souhaitez configurer LimitRange manuellement pour un espace de noms sp√©cifique, vous pouvez utiliser l'annotation <code>"limit.myafq.com/unlimited": true</code> cela indiquera √† l'op√©rateur de sauter cet espace de noms et de ne pas cr√©er LimitRange automatiquement. </p><br><p>  Exemple de script pour utiliser l'op√©rateur: </p><br><ul><li>  Cr√©ez ClusterLimit par d√©faut avec des restrictions lib√©rales et sans namespaceSelector - il sera appliqu√© partout. </li><li>  Pour un ensemble d'espaces de noms avec des applications l√©g√®res, cr√©ez un ClusterLimit suppl√©mentaire et plus rigoureux avec namespaceSelector.  Mettez des √©tiquettes sur ces espaces de noms en cons√©quence. </li><li>  Sur un espace de noms avec des applications tr√®s gourmandes en ressources, placez l'annotation "limit.myafq.com/unlimited": true et configurez LimitRange manuellement avec des limites beaucoup plus larges que celles sp√©cifi√©es dans le ClusteLimit par d√©faut. </li></ul><br><blockquote>  <strong>La chose importante √† savoir sur plusieurs LimitRange dans un m√™me espace de noms:</strong> <br>  Lorsqu'un sous est cr√©√© dans un espace de noms avec plusieurs LimitRange, les valeurs par d√©faut les plus importantes seront prises pour configurer ses ressources.  Mais les valeurs maximales et minimales seront v√©rifi√©es selon la plus stricte de LimitRange. </blockquote><br><h3 id="prakticheskiy-primer">  Exemple pratique </h3><br><p>  L'op√©rateur suivra toutes les modifications dans tous les espaces de noms, ClusterLimits, enfants LimitRanges et initiera la coordination de l'√©tat du cluster avec toute modification des objets surveill√©s.  Voyons comment cela fonctionne dans la pratique. </p><br><p>  Pour commencer, cr√©ez sous sans aucune restriction: </p><br><div class="spoiler">  <b class="spoiler_title">kubectl ex√©cuter / obtenir la sortie</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash pod/bash created ‚ùØ() kubectl get pod bash -o yaml apiVersion: v1 kind: Pod metadata: labels: run: bash name: bash namespace: default spec: containers: - image: bash name: bash resources: {}</code> </pre> <br><p>  <em>Remarque: une partie de la sortie de la commande a √©t√© omise pour simplifier l'exemple.</em> </p></div></div><br><p>  Comme vous pouvez le voir, le champ ¬´ressources¬ª est vide, ce qui signifie que ce sous peut √™tre lanc√© n'importe o√π. <br>  Nous allons maintenant cr√©er le ClusterLimit par d√©faut pour l'ensemble du cluster avec des valeurs assez lib√©rales: </p><br><div class="spoiler">  <b class="spoiler_title">default-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: default-limit spec: limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"4"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"5Gi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"700m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"900Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"500m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"512Mi"</span></span></code> </pre> </div></div><br><p>  Et aussi plus rigoureux pour un sous-ensemble d'espaces de noms: </p><br><div class="spoiler">  <b class="spoiler_title">restrictive-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: restrictive-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"800m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"1Gi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"128Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"50m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"64Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Cr√©ez ensuite les espaces de noms et les pods pour voir comment cela fonctionne. <br>  Espace de noms normal avec restriction par d√©faut: </p><br><pre> <code class="python hljs">apiVersion: v1 kind: Namespace metadata: name: regular</code> </pre> <br><p>  Et un espace de noms l√©g√®rement plus limit√©, selon la l√©gende - pour les applications l√©g√®res: </p><br><pre> <code class="python hljs">apiVersion: v1 kind: Namespace metadata: labels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> name: lightweight</code> </pre> <br><p>  Si vous regardez les journaux de l'op√©rateur imm√©diatement apr√®s avoir cr√©√© l'espace de noms, vous pouvez trouver quelque chose comme √ßa sous le spoiler: </p><br><div class="spoiler">  <b class="spoiler_title">journaux d'op√©rateur</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{...,"msg":"Reconciling ClusterLimit","Triggered by":"/regular"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"regular","LimitRange":"default-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"regular","Name":"default-limit"} {...,"msg":"Reconciling ClusterLimit","Triggered by":"/lightweight"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"lightweight","LimitRange":"default-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"lightweight","Name":"default-limit"} {...,"msg":"Creating new namespace LimitRange.","Namespace":"lightweight","LimitRange":"restrictive-limit"} {...,"msg":"Updating namespace LimitRange.","Namespace":"lightweight","Name":"restrictive-limit"}</code> </pre> <br><p>  <em>La partie manquante du journal contient 3 autres champs qui ne sont pas pertinents pour le moment</em> </p></div></div><br><p>  Comme vous pouvez le voir, la cr√©ation de chaque espace de noms a commenc√© la cr√©ation de nouveaux LimitRange.  Un espace de noms plus limit√© a obtenu deux LimitRange - par d√©faut et plus strict. </p><br><p>  Essayons maintenant de cr√©er une paire de foyers dans ces espaces de noms. </p><br><div class="spoiler">  <b class="spoiler_title">kubectl ex√©cuter / obtenir la sortie</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n regular pod/bash created ‚ùØ() kubectl get pod bash -o yaml -n regular apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container bash; cpu, memory limit for container bash' labels: run: bash name: bash namespace: regular spec: containers: - image: bash name: bash resources: limits: cpu: 700m memory: 900Mi requests: cpu: 500m memory: 512Mi</code> </pre> </div></div><br><p>  Comme vous pouvez le voir, malgr√© le fait que nous n'avons pas chang√© la fa√ßon dont le pod est cr√©√©, le champ de ressource est maintenant rempli.  Vous pouvez √©galement remarquer l'annotation cr√©√©e automatiquement par LimitRanger. </p><br><p>  Maintenant, cr√©ez sous dans un espace de noms l√©ger: </p><br><div class="spoiler">  <b class="spoiler_title">kubectl ex√©cuter / obtenir la sortie</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n lightweight pod/bash created ‚ùØ() kubectl get pods -n lightweight bash -o yaml apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container bash; cpu, memory limit for container bash' labels: run: bash name: bash namespace: lightweight spec: containers: - image: bash name: bash resources: limits: cpu: 700m memory: 900Mi requests: cpu: 500m memory: 512Mi</code> </pre> </div></div><br><p>  Veuillez noter que les ressources dans l'√¢tre sont les m√™mes que dans l'exemple pr√©c√©dent.  En effet, dans le cas de plusieurs LimitRange, des valeurs par d√©faut moins strictes seront utilis√©es lors de la cr√©ation de pods.  Mais alors pourquoi avons-nous besoin d'une LimitRange plus limit√©e?  Il sera utilis√© pour v√©rifier les valeurs maximales et minimales des ressources.  Pour d√©montrer, nous allons rendre notre ClusterLimit limit√© encore plus limit√©: </p><br><div class="spoiler">  <b class="spoiler_title">restrictive-limit.yaml</b> <div class="spoiler_text"><pre> <code class="python hljs">apiVersion: limit.myafq.com/v1alpha1 kind: ClusterLimit metadata: name: restrictive-limit spec: namespaceSelector: matchLabels: limit: <span class="hljs-string"><span class="hljs-string">"restrictive"</span></span> limitRange: limits: - type: Container max: cpu: <span class="hljs-string"><span class="hljs-string">"200m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"250Mi"</span></span> default: cpu: <span class="hljs-string"><span class="hljs-string">"100m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"128Mi"</span></span> defaultRequest: cpu: <span class="hljs-string"><span class="hljs-string">"50m"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"64Mi"</span></span> - type: Pod max: cpu: <span class="hljs-string"><span class="hljs-string">"2"</span></span> memory: <span class="hljs-string"><span class="hljs-string">"2Gi"</span></span></code> </pre> </div></div><br><p>  Faites attention √† la section: </p><br><pre> <code class="plaintext hljs">- type: Container max: cpu: "200m" memory: "250Mi"</code> </pre> <br><p>  Maintenant, nous avons d√©fini un processeur de 200 m et 250 Mo de m√©moire comme maximum pour le conteneur dans l'√¢tre.  Et maintenant, essayez de cr√©er sous: </p><br><pre> <code class="plaintext hljs">‚ùØ() kubectl run --generator=run-pod/v1 --image=bash bash -n lightweight Error from server (Forbidden): pods "bash" is forbidden: [maximum cpu usage per Container is 200m, but limit is 700m., maximum memory usage per Container is 250Mi, but limit is 900Mi.]</code> </pre> <br><p>  Notre sous-marin a de grandes valeurs d√©finies par le LimitRange par d√©faut et il n'a pas pu d√©marrer car il n'a pas r√©ussi la v√©rification des ressources maximales autoris√©es. </p><br><hr><br><p>  C'√©tait un exemple d'utilisation de l'op√©rateur de limite.  Essayez-le vous-m√™me et jouez avec ClusterLimit dans votre instance Kubernetes locale. </p><br><p>  Dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©f√©rentiel d'op√©rateur</a> GitHub <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Limit,</a> vous pouvez trouver le manifeste pour le d√©ploiement de l'op√©rateur, ainsi que le code source.  Si vous souhaitez √©tendre les fonctionnalit√©s de l'op√©rateur, les qu√™tes de tirage et les qu√™tes de fonctionnalit√© sont les bienvenues! </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  La gestion des ressources chez Kubernetes est essentielle √† la stabilit√© et √† la fiabilit√© de vos applications.  Personnalisez vos ressources de foyer chaque fois que possible.  Et utilisez LimitRange pour vous assurer contre les cas o√π cela n'est pas possible.  Automatisez la cr√©ation de LimitRange √† l'aide de l'op√©rateur de limite. </p><br><p>  Suivez ces conseils et votre cluster sera toujours √† l'abri du chaos sans ressources des foyers errants. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr473908/">https://habr.com/ru/post/fr473908/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr473888/index.html">Nouvelles du monde d'OpenStreetMap n ¬∞ 483 (15/10/2019 - 21/10/2019)</a></li>
<li><a href="../fr473890/index.html">Op√©ration Calypso: un nouveau groupe APT attaque des bureaux gouvernementaux dans le monde</a></li>
<li><a href="../fr473894/index.html">Six t√¢ches pour le d√©veloppeur frontal</a></li>
<li><a href="../fr473904/index.html">Vivaldi 2.9 - Am√©liorations Am√©liorations</a></li>
<li><a href="../fr473906/index.html">7 cours gratuits pour les d√©veloppeurs de Microsoft</a></li>
<li><a href="../fr473910/index.html">Code anglais</a></li>
<li><a href="../fr473916/index.html">Contes du centre de donn√©es: histoires d'horreur d'Halloween sur les moteurs diesel, la diplomatie et les vis auto-taraudeuses dans le radiateur</a></li>
<li><a href="../fr473918/index.html">Voyager sur Internet en Russie: rapidit√© et opportunit√©</a></li>
<li><a href="../fr473922/index.html">Conf√©rence de la Selectel Networking Academy</a></li>
<li><a href="../fr473924/index.html">Non seulement SMS et jeton: authentification multifacteur bas√©e sur le service d'authentification SafeNet</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>