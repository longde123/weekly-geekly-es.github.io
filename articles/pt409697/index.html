<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👊🏼 👨🏼‍🎤 🧙🏿 Estilo de música com redes neurais 👩🏿‍🍳 💪🏾 🚲</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Na última década, as Redes Neurais Profundas (DNNs) tornaram-se uma excelente ferramenta para várias tarefas de IA, como classificação de imagens, rec...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Estilo de música com redes neurais</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/409697/"><p><img src="https://habrastorage.org/webt/qu/r7/29/qur729mzadpog93xrhjwkj9c51i.jpeg"></p><br><p>  Na última década, as Redes Neurais Profundas (DNNs) tornaram-se uma excelente ferramenta para várias tarefas de IA, como classificação de imagens, reconhecimento de fala e até participação em jogos.  Como os desenvolvedores tentaram mostrar o que causou o sucesso do DNN no campo da classificação de imagens e criaram ferramentas de visualização (por exemplo, Deep Dream, Filters) que ajudam a entender “o que” exatamente “estuda” o modelo DNN, surgiu uma nova aplicação interessante : extrair “estilo” de uma imagem e aplicar a outro conteúdo diferente.  Isso foi chamado de "transferência de estilo de imagem". </p><a name="habracut"></a><br><p><img src="https://habrastorage.org/webt/ma/w_/uy/maw_uyr88ft3g7oiwpx8yp6vfwi.jpeg"><br>  <em>Esquerda: imagem com conteúdo útil, no centro: imagem com estilo, direita: conteúdo + estilo (fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Blog de pesquisa do Google</a> )</em> </p><br><p>  Isso não apenas despertou o interesse de muitos outros pesquisadores (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">1</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">2</a> ), mas também levou ao surgimento de vários aplicativos móveis bem-sucedidos.  Nos últimos anos, esses métodos de transferência de estilo visual melhoraram bastante. </p><br><p><img src="https://habrastorage.org/webt/ij/oz/7-/ijoz7-xp5fibweiwdt3skobj_cy.jpeg"><br>  <em>Invólucro no estilo Adobe (fonte: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Engadget</a> )</em> </p><br><p><img src="https://habrastorage.org/webt/rx/sx/gc/rxsxgcb9dxcf7gk5iaq6e2dkjbo.jpeg"><br>  <em>Exemplo do site da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Prisma</a></em> </p><br><p>  Uma breve introdução a esses algoritmos: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/WHmp26bh0tI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  No entanto, apesar dos avanços no trabalho com imagens, a aplicação dessas técnicas em outras áreas, por exemplo, para o processamento de música, era muito limitada (ver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">3</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">4</a> ), e os resultados não são tão impressionantes quanto no caso das imagens.  Isso sugere que é muito mais difícil transferir estilo na música.  Neste artigo, examinaremos o problema com mais detalhes e discutiremos algumas abordagens possíveis. </p><br><h2 id="pochemu-tak-trudno-perenosit-stil-v-muzyke">  Por que é tão difícil transferir estilo na música? </h2><br><p>  Vamos primeiro responder à pergunta: o <strong>que é "transferência de estilo" na música</strong> ?  A resposta não é tão óbvia.  Nas imagens, os conceitos de "conteúdo" e "estilo" são intuitivos.  “Conteúdo da imagem” descreve os objetos representados, por exemplo, cães, casas, rostos etc., e “estilo da imagem” refere-se a cores, iluminação, pinceladas e textura. </p><br><p>  No entanto, a música é <strong>semanticamente abstrata e multidimensional</strong> por natureza.  "Conteúdo musical" pode significar coisas diferentes em diferentes contextos.  Freqüentemente, o conteúdo da música está associado a uma melodia e o estilo a um arranjo ou harmonização.  No entanto, o conteúdo pode ser a letra e diferentes melodias usadas para cantar podem ser interpretadas como estilos diferentes.  Na música clássica, o conteúdo pode ser considerado a partitura (que inclui harmonização), enquanto o estilo é a interpretação das notas pelo intérprete, que traz sua própria expressão (variando e acrescentando alguns sons dele mesmo).  Para entender melhor a essência da transferência de estilo na música, confira alguns desses vídeos: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/S75gYhODS0M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><iframe width="560" height="315" src="https://www.youtube.com/embed/buXqNqBFd6E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  No segundo vídeo, várias <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">técnicas de</a> aprendizado de máquina <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">são</a> usadas. </p><br><p>  Portanto, a transferência de estilo na música é, por definição, difícil de formalizar.  Existem outros fatores principais que complicam a tarefa: </p><br><ol><li>  <strong>As máquinas BAD entendem de música</strong> (por enquanto): o sucesso na transferência de estilo em imagens decorre do sucesso da DNN em tarefas relacionadas à compreensão de imagens, como reconhecimento de objetos.  Como os DNNs podem aprender propriedades que variam entre objetos, técnicas de retropaginação podem ser usadas para modificar a imagem de destino para corresponder às propriedades do conteúdo.  Embora tenhamos feito um <a href="">progresso</a> significativo na criação de modelos baseados em DNN, capazes de entender tarefas musicais (por exemplo, transcrever melodias, definir um gênero etc.), ainda estamos longe das alturas alcançadas no processamento de imagens.  Este é um sério obstáculo para a transferência de estilo na música.  Os modelos existentes simplesmente não conseguem aprender as propriedades “excelentes” que permitem classificar a música, o que significa que a aplicação direta dos algoritmos de transferência de estilo usados ​​ao trabalhar com imagens não produz o mesmo resultado. </li><li>  A música é <strong>passageira</strong> : são dados que representam séries dinâmicas, ou seja, um fragmento musical muda com o tempo.  Isso complica o aprendizado.  Embora as redes neurais recorrentes e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">LSTM</a> (Long Short-Term Memory) permitam que você aprenda mais com dados transitórios, ainda precisamos criar modelos confiáveis ​​que aprendam a reproduzir a estrutura da música a longo prazo (nota: essa é uma área real de pesquisa e cientistas da equipe do Google Magenta alcançaram algum sucesso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nisso</a> ). </li><li>  A música é <strong>discreta</strong> (pelo menos no nível simbólico): simbólica, ou música gravada no papel, é de natureza discreta.  No <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">temperamento uniforme</a> , o sistema de afinação de instrumentos musicais mais popular da atualidade, os tons de som ocupam posições discretas em uma escala de frequência contínua.  Ao mesmo tempo, a duração dos tons também está no espaço discreto (geralmente tons de um quarto, tons completos e assim por diante).  Portanto, é muito difícil adaptar os métodos de propagação de pixel back (usados ​​para trabalhar com imagens) no campo da música simbólica. </li></ol><br><p><img src="https://habrastorage.org/webt/tb/cj/a0/tbcja0feucuwkgawqqlxqvfqtna.png"><br>  <em>A natureza discreta das notas musicais em um temperamento uniforme.</em> </p><br><p>  Portanto, as técnicas usadas para transferir estilo nas imagens não são diretamente aplicáveis ​​à música.  Para fazer isso, eles precisam ser processados ​​com ênfase em conceitos e idéias musicais. </p><br><h2 id="dlya-chego-nuzhen-perenos-stilya-v-muzyke">  Para que serve a transferência de estilo na música? </h2><br><p>  Por que você precisa resolver esse problema?  Como nas imagens, os usos potenciais da transferência de estilos na música são bastante interessantes.  Por exemplo, <strong>desenvolvendo uma ferramenta para ajudar compositores</strong> .  Por exemplo, um instrumento automático capaz de transformar uma melodia usando arranjos de diferentes gêneros será extremamente útil para compositores que precisam experimentar rapidamente idéias diferentes.  Os DJs também estarão interessados ​​em tais instrumentos. </p><br><p>  Um resultado indireto de tal pesquisa será uma melhoria significativa nos sistemas de informática musical.  Como explicado acima, para que a transferência de estilo funcione na música, os modelos que criamos devem aprender a <strong>"entender"</strong> diferentes aspectos. </p><br><h2 id="uproschenie-zadachi-perenosa-stilya-v-muzyke">  Simplifique a tarefa de transferir estilo na música </h2><br><p>  Vamos começar com uma tarefa muito simples de analisar melodias monofônicas em diferentes gêneros.  Melodias monofônicas são seqüências de notas, cada uma determinada pelo tom e duração.  A progressão do tom depende, em grande parte, da escala da melodia, e a progressão da duração depende do ritmo.  Então, primeiro, <strong>separamos</strong> claramente <strong>“</strong> conteúdo de afinação” e <strong>“estilo rítmico”</strong> como duas entidades com as quais você pode reformular a tarefa de transferir estilo.  Além disso, ao trabalhar com melodias monofônicas, agora evitaremos as tarefas associadas ao arranjo e ao texto. </p><br><p>  Na ausência de modelos pré-treinados que possam distinguir com sucesso entre progressões de tons e ritmos de melodias monofônicas, primeiro recorremos a uma abordagem muito simples para transferir estilos.  Em vez de tentar alterar o conteúdo do som aprendido na melodia-alvo com o estilo rítmico aprendido no ritmo-alvo, tentaremos ensinar individualmente os padrões de tons e durações de diferentes gêneros, e tentar combiná-los.  Esquema aproximado da abordagem: </p><br><p><img src="https://habrastorage.org/webt/ek/i8/3o/eki83obepvf1nd44pgzupr-czmu.png"><br>  <em>Esquema do método de transferência de estilo entre gêneros.</em> </p><br><h2 id="obuchaem-otdelno-tonovym-i-ritmovym-progressiyam">  Ensinamos separadamente progressões de tom e ritmo </h2><br><p>  <strong><em>Apresentação dos dados</em></strong> </p><br><p>  Apresentaremos as melodias monofônicas como uma sequência de notas musicais, cada uma com um índice de tons e uma sequência.  Para que a nossa chave de apresentação seja independente, usaremos a apresentação com base em intervalos: o tom da próxima nota será apresentado como um desvio (± semitom) do tom da nota anterior.  Vamos criar dois dicionários para tons e durações em que cada estado discreto (para tom: +1, -1, +2, -2 e assim por diante; para durações: semínima, semínima, semínima com ponto e assim por diante) recebe um índice dicionário. </p><br><p><img src="https://habrastorage.org/webt/v2/h6/z6/v2h6z6pbrq6ezu9br3jv_actjia.png"><br>  <em>Apresentação de dados.</em> </p><br><p>  <strong><em>Arquitetura de modelo</em></strong> </p><br><p>  Usaremos a mesma arquitetura que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Colombo e colegas usaram</a> - eles ensinaram simultaneamente duas redes neurais LSTM para o mesmo gênero musical: a) a rede de tons aprendida para prever o próximo tom com base na nota anterior e na duração anterior, b) a rede de duração aprendida para prever a próxima duração com base na próxima nota e duração anterior.  Além disso, antes das redes LSTM, adicionaremos camadas de incorporação para comparar índices e durações de tons de entrada em espaços de incorporação memorizados.  A arquitetura da rede neural é mostrada na figura: </p><br><p><img src="https://habrastorage.org/webt/6q/fg/nk/6qfgnkiake_rls5yymzbvxdvzg0.png"></p><br><p>  <strong><em>Procedimento de treinamento</em></strong> </p><br><p>  Para cada gênero, as redes responsáveis ​​por tons e durações são treinadas ao mesmo tempo.  Usaremos dois conjuntos de dados: a) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Norbeck Folk Dataset</a> , cobrindo cerca de 2.000 músicas folclóricas irlandesas e suecas, b) um conjunto de dados de jazz (não disponível ao público), cobrindo cerca de 500 músicas de jazz. </p><br><p>  <strong><em>Mesclando modelos treinados</em></strong> </p><br><p>  Durante o teste, a melodia é gerada primeiro usando a rede de tons e a duração da rede treinada no primeiro gênero (por exemplo, folk).  Em seguida, a sequência de tons da melodia gerada é usada na entrada de uma rede de sequências treinadas em outro gênero (por exemplo, jazz), e o resultado é uma nova sequência de durações.  Portanto, uma melodia criada usando uma combinação de duas redes neurais possui uma sequência de tons correspondente ao primeiro gênero (folk) e uma sequência de durações correspondendo ao segundo gênero (jazz). </p><br><h2 id="predvaritelnye-rezultaty">  Resultados Preliminares </h2><br><p>  Pequenos trechos de algumas das músicas resultantes: <br>  <a href="">Tons Folk e Durações Folk</a> </p><br><p><img src="https://habrastorage.org/webt/c9/ic/zv/c9iczvv4qkjjv6lve-sj5wnb3dk.png"><br>  <em>Extrato da notação musical.</em> </p><br><p>  <a href="">Tons folclóricos e durações de jazz</a> </p><br><p><img src="https://habrastorage.org/webt/-p/oz/57/-poz57oqe5kpunzo3jqpikcjjju.png"><br>  <em>Extrato da notação musical.</em> </p><br><p>  <a href="">Tons e sequências de jazz</a> </p><br><p><img src="https://habrastorage.org/webt/8s/ia/4x/8sia4xupwzfbnz2tyzof4xw14li.png"><br>  <em>Extrato da notação musical</em> . </p><br><p>  <a href="">Tons de jazz e sequências folclóricas</a> </p><br><p><img src="https://habrastorage.org/webt/hf/mh/8p/hfmh8p9kyorpupmmvydj_psxb9u.png"><br>  <em>Extrato da notação musical.</em> </p><br><h2 id="zaklyuchenie">  Conclusão </h2><br><p>  Embora o algoritmo atual não seja ruim para começar, ele possui várias desvantagens críticas: </p><br><ol><li>  <strong>É impossível "transferir estilo" com base em uma melodia de destino específica</strong> .  Os modelos aprendem padrões de tons e durações em um gênero, o que significa que todas as transformações são determinadas pelo gênero.  Seria ideal modificar uma peça de música no estilo de uma música ou peça de destino específica. </li><li>  <strong>Não é possível controlar o grau de</strong> mudança <strong>de</strong> estilo.  Seria muito interessante obter uma "alça" que governasse esse aspecto. </li><li>  Ao mesclar gêneros, <strong>é impossível preservar a estrutura musical</strong> em uma melodia transformada.  Uma estrutura de longo prazo é importante para a avaliação musical em geral e, para que as melodias geradas sejam musicalmente estéticas, a estrutura deve ser preservada. </li></ol><br><p>  Nos artigos a seguir, examinaremos maneiras de contornar essas deficiências. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt409697/">https://habr.com/ru/post/pt409697/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt409687/index.html">Formato sexta-feira: Mulheres na música eletrônica - Wendy Carlos e Susan Chani</a></li>
<li><a href="../pt409689/index.html">Os sortudos e perdedores do mundo do bitcoin: 7 histórias vêm de 2017</a></li>
<li><a href="../pt409691/index.html">Mídia: EUA deixarão de financiar a ISS em 2025</a></li>
<li><a href="../pt409693/index.html">Ficção de temperatura e pressão, 2/3</a></li>
<li><a href="../pt409695/index.html">486º da República da China</a></li>
<li><a href="../pt409699/index.html">Os experimentos científicos mais esperados da próxima década</a></li>
<li><a href="../pt409701/index.html">As autoridades vão identificar mineiros na Federação Russa nas contas de eletricidade</a></li>
<li><a href="../pt409703/index.html">IA e veículos não tripulados - bem-vindo ao falar</a></li>
<li><a href="../pt409705/index.html">Kingston Nucleum USB hub - transformando um laptop sem porta em uma máquina de trabalho completa</a></li>
<li><a href="../pt409707/index.html">Teclado de bolso ou design sem sentido, mas bonito</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>