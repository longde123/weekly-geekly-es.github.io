<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚èèÔ∏è üõÄüèº üèπ Vorschl√§ge f√ºr Sicherheitsl√ºcken und den Schutz von Modellen f√ºr maschinelles Lernen üë©üèª‚Äçüî¨ ü•™ ü§¥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In j√ºngster Zeit befassen sich Experten zunehmend mit dem Thema Sicherheit von Modellen des maschinellen Lernens und bieten verschiedene Schutzmethode...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Vorschl√§ge f√ºr Sicherheitsl√ºcken und den Schutz von Modellen f√ºr maschinelles Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/458892/"><img src="https://habrastorage.org/webt/gn/da/kl/gndaklzm6lwmn9ijp2pb9sxnmza.jpeg"><br><br>  In j√ºngster Zeit befassen sich Experten zunehmend mit dem Thema Sicherheit von Modellen des maschinellen Lernens und bieten verschiedene Schutzmethoden an.  Es ist an der Zeit, potenzielle Schwachstellen und Abwehrmechanismen im Kontext g√§ngiger traditioneller Modellierungssysteme wie Linear- und Baummodelle, die auf statischen Datens√§tzen trainiert wurden, im Detail zu untersuchen.  Obwohl der Autor dieses Artikels kein Sicherheitsexperte ist, verfolgt er Themen wie Debugging, Erkl√§rungen, Fairness, Interpretierbarkeit und Datenschutz beim maschinellen Lernen sorgf√§ltig. <br><br>  In diesem Artikel stellen wir einige wahrscheinliche Angriffsmethoden auf ein typisches maschinelles Lernsystem in einer typischen Organisation vor, bieten vorl√§ufige L√∂sungen f√ºr den Schutz und betrachten einige h√§ufig auftretende Probleme und die vielversprechendsten Praktiken. <br><a name="habracut"></a><br><h2>  1. Angriffe auf Datenkorruption </h2><br>  Datenverzerrung bedeutet, dass jemand Trainingsdaten systematisch √§ndert, um die Vorhersagen Ihres Modells zu manipulieren (solche Angriffe werden auch als ‚Äûkausale‚Äú Angriffe bezeichnet).  Um Daten zu verzerren, muss ein Angreifer Zugriff auf einige oder alle Ihrer Trainingsdaten haben.  Und wenn in vielen Unternehmen keine ordnungsgem√§√üe Kontrolle vorhanden ist, k√∂nnen verschiedene Mitarbeiter, Berater und Auftragnehmer einen solchen Zugang haben.  Ein unbefugter Zugriff auf einige oder alle Trainingsdaten kann auch von einem Angreifer au√üerhalb des Sicherheitsbereichs erhalten werden. <br><br>  Ein direkter Angriff auf besch√§digte Daten kann das √Ñndern von Dataset-Labels umfassen.  Unabh√§ngig von der kommerziellen Verwendung Ihres Modells kann ein Angreifer seine Prognosen verwalten, indem er beispielsweise die Bezeichnungen √§ndert, sodass Ihr Modell lernen kann, wie man gro√üe Kredite, gro√üe Rabatte gew√§hrt oder kleine Versicherungspr√§mien f√ºr Angreifer festlegt.  Das Erzwingen falscher Vorhersagen durch ein Modell im Interesse eines Angreifers wird manchmal als Verletzung der "Integrit√§t" des Modells bezeichnet. <br><br>  Ein Angreifer kann Datenkorruption auch verwenden, um Ihr Modell zu trainieren, um eine Gruppe von Personen absichtlich zu diskriminieren und ihnen einen gro√üen Kredit, gro√üe Rabatte oder niedrige Versicherungspr√§mien zu entziehen, auf die sie Anspruch haben.  Im Kern √§hnelt dieser Angriff DDoS.  Das Erzwingen falscher Vorhersagen durch ein Modell, um anderen Schaden zuzuf√ºgen, wird manchmal als Versto√ü gegen die ‚ÄûZug√§nglichkeit‚Äú des Modells bezeichnet. <br><br>  Obwohl es den Anschein hat, dass es einfacher ist, Daten zu verzerren, als Werte in vorhandenen Zeilen eines Datasets zu √§ndern, k√∂nnen Sie auch Verzerrungen einf√ºhren, indem Sie dem Dataset scheinbar harmlose oder zus√§tzliche Spalten hinzuf√ºgen.  Ge√§nderte Werte in diesen Spalten k√∂nnen dann dazu f√ºhren, dass sich Modellvorhersagen √§ndern. <br><br>  Schauen wir uns nun einige m√∂gliche Schutz- und Expertenl√∂sungen (forensische L√∂sungen) f√ºr den Fall von Datenkorruption an: <br><br><ul><li>  <b>Differenzierte Wirkungsanalyse</b> .  Viele Banken f√ºhren bereits eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Analyse der unterschiedlichen</a> Auswirkungen f√ºr eine faire Kreditvergabe durch, um festzustellen, ob ihr Modell von verschiedenen Personengruppen diskriminiert wird.  Viele andere Organisationen sind jedoch noch nicht so weit gekommen.  Es gibt mehrere hervorragende Open-Source-Tools zur Erkennung von Diskriminierung und zur Durchf√ºhrung von Differential Impact Analysis.  Zum Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aequitas,</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Themis</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AIF360</a> . <br></li><li>  <b>Faire oder private Models</b> .  Modelle wie das Lernen fairer Repr√§sentationen (LFR) und die private Aggregation von Lehrerensembles (PATE) ber√ºcksichtigen bei der Erstellung von Prognosen tendenziell weniger die einzelnen demografischen Eigenschaften.  Au√üerdem sind diese Modelle m√∂glicherweise weniger anf√§llig f√ºr diskriminierende Angriffe, um Daten zu verzerren. <br></li><li>  <b>Ablehnung bei negativen Auswirkungen (RONI)</b> .  RONI ist eine Methode zum Entfernen von Datenzeilen aus einem Datensatz, die die Vorhersagegenauigkeit verringern.  Weitere Informationen zu RONI finden Sie in Abschnitt 8, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sicherheit beim maschinellen Lernen</a> . <br></li><li>  <b>Restanalyse</b> .  Suchen Sie nach seltsamen, auff√§lligen Mustern in den Residuen Ihrer Modellprognosen, insbesondere in Bezug auf Mitarbeiter, Berater oder Auftragnehmer. <br></li><li>  <b>Selbstreflexion</b> .  Bewerten Sie Modelle Ihrer Mitarbeiter, Berater und Auftragnehmer, um ungew√∂hnlich g√ºnstige Prognosen zu ermitteln. <br></li></ul><br>  W√§hrend des Trainings und im Rahmen der Echtzeit√ºberwachung von Modellen k√∂nnen differenzierte Wirkungsanalysen, Restanalysen und Selbstreflexionen durchgef√ºhrt werden. <br><br><h2>  2. Wasserzeichenangriffe </h2><br>  Ein Wasserzeichen ist ein Begriff aus der Literatur zur Sicherheit des tiefen Lernens, der sich h√§ufig auf das Hinzuf√ºgen spezieller Pixel zum Bild bezieht, um das gew√ºnschte Ergebnis aus Ihrem Modell zu erzielen.  Dasselbe ist mit Kunden- oder Transaktionsdaten durchaus m√∂glich. <br><br>  Stellen Sie sich ein Szenario vor, in dem ein Mitarbeiter, Berater, Auftragnehmer oder Angreifer von au√üerhalb Zugriff auf den Code f√ºr die produktive Verwendung Ihres Modells hat, der Echtzeitprognosen erstellt.  Eine solche Person kann den Code √§ndern, um eine seltsame oder unwahrscheinliche Kombination von Eingabevariablenwerten zu erkennen und das gew√ºnschte Vorhersageergebnis zu erhalten.  Wie bei der Besch√§digung von Daten k√∂nnen Wasserzeichenangriffe verwendet werden, um die Integrit√§t oder Zug√§nglichkeit Ihres Modells zu verletzen.  Um beispielsweise die Integrit√§t zu verletzen, kann ein Angreifer eine ‚ÄûNutzlast‚Äú in den Bewertungscode f√ºr die produktive Verwendung des Modells einf√ºgen, wodurch eine Kombination von 0 Jahren an Adresse 99 erkannt wird, was zu einer positiven Prognose f√ºr den Angreifer f√ºhrt.  Um die Verf√ºgbarkeit des Modells zu blockieren, kann er eine k√ºnstliche Diskriminierungsregel in den Bewertungscode einf√ºgen, die es dem Modell nicht erm√∂glicht, f√ºr eine bestimmte Personengruppe positive Ergebnisse zu erzielen. <br><br>  Schutz- und Expertenans√§tze f√ºr Angriffe mit Wasserzeichen k√∂nnen Folgendes umfassen: <br><br><ul><li>  <b>Anomalieerkennung</b> .  Autocoders ist ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Betrugserkennungsmodell</a> , das Eingaben identifizieren kann, die komplex und seltsam sind oder anderen Daten nicht √§hneln.  Potenziell k√∂nnen Auto-Encoder alle Wasserzeichen erkennen, die zum Ausl√∂sen b√∂sartiger Mechanismen verwendet werden. <br></li><li>  <b>Einschr√§nkungen der Datenintegrit√§t</b> .  Viele Datenbanken erlauben keine seltsamen oder unrealistischen Kombinationen von Eingabevariablen, die m√∂glicherweise Wasserzeichenangriffe verhindern k√∂nnten.  Der gleiche Effekt kann f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Integrit√§tsbeschr√§nkungen</a> f√ºr Datenstr√∂me gelten, die in Echtzeit empfangen werden. <br></li><li>  <b>Differenzierte Expositionsanalyse</b> : siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 1</a> . <br></li><li>  <b>Versionskontrolle</b> .  Der Evaluierungscode f√ºr die Produktionsanwendung des Modells muss wie jedes andere kritische Softwareprodukt versioniert und gesteuert werden. <br></li></ul><br>  Die Erkennung von Anomalien, Einschr√§nkungen der Datenintegrit√§t und die Analyse der unterschiedlichen Auswirkungen k√∂nnen w√§hrend des Trainings und als Teil der Echtzeitmodell√ºberwachung verwendet werden. <br><br><h2>  3. Inversion von Ersatzmodellen </h2><br>  Normalerweise wird ‚ÄûInversion‚Äú als Abrufen nicht autorisierter Informationen von einem Modell bezeichnet, anstatt Informationen darin zu platzieren.  Die Inversion kann auch ein Beispiel f√ºr einen ‚ÄûReverse Engineering-Angriff der Aufkl√§rung‚Äú sein.  Wenn ein Angreifer in der Lage ist, viele Vorhersagen √ºber die API Ihres Modells oder eines anderen Endpunkts (Website, Anwendung usw.) zu erhalten, kann er sein eigenes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ersatzmodell</a> trainieren.  Einfach ausgedr√ºckt ist dies eine Simulation Ihres Vorhersagemodells!  Theoretisch kann ein Angreifer ein Ersatzmodell zwischen den Eingabedaten, die zum Generieren der empfangenen Prognosen verwendet werden, und den Prognosen selbst trainieren.  Abh√§ngig von der Anzahl der Vorhersagen, die empfangen werden k√∂nnen, kann das Ersatzmodell zu einer ziemlich genauen Simulation Ihres Modells werden.  Nach dem Training des Ersatzmodells verf√ºgt der Angreifer √ºber eine ‚ÄûSandbox‚Äú, aus der er eine Unpers√∂nlichkeit (dh Nachahmung) oder einen Angriff mit einem Wettbewerbsbeispiel auf die Integrit√§t Ihres Modells planen oder das Potenzial erhalten kann, einige Aspekte Ihrer vertraulichen Trainingsdaten wiederherzustellen.  Ersatzmodelle k√∂nnen auch mit externen Datenquellen trainiert werden, die in gewisser Weise mit Ihren Vorhersagen √ºbereinstimmen, wie dies beispielsweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ProPublica</a> mit dem R√ºckfallmodell des COMPAS-Autors getan hat. <br><br>  Um Ihr Modell mithilfe eines Ersatzmodells vor Inversion zu sch√ºtzen, k√∂nnen Sie sich auf folgende Ans√§tze verlassen: <br><br><ul><li>  <b>Autorisierter Zugriff</b> .  Fordern Sie eine zus√§tzliche Authentifizierung an (z. B. zwei Faktoren), um eine Prognose zu erhalten. <br></li><li>  <b>Gasvorhersagen</b>  Begrenzen Sie eine gro√üe Anzahl schneller Prognosen einzelner Benutzer.  Betrachten Sie die M√∂glichkeit, die Vorhersageverz√∂gerungen k√ºnstlich zu erh√∂hen. <br></li><li>  <b>Ersatzmodelle "Wei√ü" (wei√üer Hut)</b> .  Versuchen Sie als White-Hacker-√úbung Folgendes: Trainieren Sie Ihre eigenen Ersatzmodelle zwischen Ihren Eingabe- und Modellvorhersagen f√ºr eine Produktionsanwendung und beachten Sie die folgenden Aspekte sorgf√§ltig: <br><ul><li>  Genauigkeitsgrenzen verschiedener Arten von "wei√üen" Ersatzmodellen;  Versuchen Sie zu verstehen, inwieweit das Ersatzmodell tats√§chlich verwendet werden kann, um unerw√ºnschte Daten √ºber Ihr Modell zu erhalten. <br></li><li>  Arten von Datentrends, die aus Ihrem ‚Äûwei√üen‚Äú Ersatzmodell gelernt werden k√∂nnen, z. B. lineare Trends, die durch lineare Modellkoeffizienten dargestellt werden. <br></li><li>  Arten von Segmenten oder demografischen Verteilungen, die untersucht werden k√∂nnen, indem die Anzahl der Personen analysiert wird, die bestimmten Knoten des ‚Äûwei√üen‚Äú Ersatzentscheidungsbaums zugeordnet sind. <br></li><li>  die Regeln, die aus dem ‚Äûwei√üen‚Äú Ersatzentscheidungsbaum gelernt werden k√∂nnen, zum Beispiel, wie eine Person, die eine positive Prognose erh√§lt, genau dargestellt wird. <br></li></ul><br></li></ul><br><h2>  4. Rivalit√§tsangriffe </h2><br>  Theoretisch kann ein engagierter Hacker lernen - beispielsweise Versuch und Irrtum (dh ‚ÄûIntelligenz‚Äú oder ‚ÄûSensitivit√§tsanalyse‚Äú) - ein Ersatzmodell oder Social Engineering umkehren, wie Sie mit Ihrem Modell spielen, um das gew√ºnschte Vorhersageergebnis zu erhalten, oder das Unerw√ºnschte vermeiden Prognose.  Der Versuch, solche Ziele mit einer speziell entwickelten Datenzeichenfolge zu erreichen, wird als gegnerischer Angriff bezeichnet.  (manchmal ein Angriff, um die Integrit√§t zu untersuchen).  Ein Angreifer kann einen gegnerischen Angriff nutzen, um ein gro√ües Darlehen oder eine niedrige Versicherungspr√§mie zu erhalten oder um die Verweigerung der Bew√§hrung mit einer hohen Einsch√§tzung des kriminellen Risikos zu vermeiden.  Einige Leute nennen die Verwendung von Wettbewerbsbeispielen, um ein unerw√ºnschtes Ergebnis von einer Prognose auszuschlie√üen, ‚ÄûAusweichen‚Äú. <br><br>  Probieren Sie die unten beschriebenen Methoden aus, um einen Angriff anhand eines Wettbewerbsbeispiels zu verteidigen oder zu erkennen: <br><br><ul><li>  <b>Aktivierungsanalyse</b> .  Die Aktivierungsanalyse erfordert, dass Ihre Vorhersagemodelle √ºber vergleichende interne Mechanismen verf√ºgen, z. B. die durchschnittliche Aktivierung von Neuronen in Ihrem neuronalen Netzwerk oder den Anteil der Beobachtungen, die sich auf jeden Endknoten in Ihrem zuf√§lligen Wald beziehen.  Anschlie√üend vergleichen Sie diese Informationen mit dem Verhalten des Modells mit realen eingehenden Datenstr√∂men.  Einer meiner Kollegen sagte: " <i>Es ist dasselbe, als w√ºrde man einen Endknoten in einer zuf√§lligen Gesamtstruktur sehen, der 0,1% der Trainingsdaten entspricht, aber f√ºr 75% der Bewertungslinien pro Stunde geeignet ist</i> ." <br></li><li>  <b>Anomalieerkennung</b> .  siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 2</a> . <br></li><li>  <b>Autorisierter Zugriff</b> .  siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 3</a> . <br></li><li>  <b>Vergleichsmodelle</b> .  Verwenden Sie bei der Auswertung neuer Daten zus√§tzlich zu einem komplexeren Modell ein Vergleichsmodell mit hoher Transparenz.  Interpretierte Modelle sind schwerer zu knacken, weil ihre Mechanismen transparent sind.  Vergleichen Sie bei der Auswertung neuer Daten das neue Modell mit einem zuverl√§ssigen transparenten Modell oder einem Modell, das auf verifizierten Daten und einem vertrauensw√ºrdigen Prozess trainiert wurde.  Wenn der Unterschied zwischen dem komplexeren und undurchsichtigen Modell und dem interpretierten (oder verifizierten) Modell zu gro√ü ist, kehren Sie zu den konservativen Modellvorhersagen zur√ºck oder verarbeiten Sie die Datenzeile manuell.  Nehmen Sie diesen Vorfall auf, es k√∂nnte sich um einen Angriff mit einem Wettbewerbsbeispiel handeln. <br></li><li>  <b>Gasvorhersagen</b> : siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 3</a> . <br></li><li>  <b>Sensitivit√§tsanalyse "Wei√ü"</b> .  Verwenden Sie die Sensitivit√§tsanalyse, um Ihre eigenen Forschungsangriffe durchzuf√ºhren und zu verstehen, welche variablen Werte (oder Kombinationen davon) gro√üe Schwankungen in den Prognosen verursachen k√∂nnen.  Suchen Sie bei der Auswertung neuer Daten nach diesen Werten oder Wertekombinationen.  Um eine ‚Äûwei√üe‚Äú Forschungsanalyse durchzuf√ºhren, k√∂nnen Sie das Open-Source-Paket <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">cleverhans verwenden</a> . <br></li><li>  Wei√üe Ersatzmodelle: siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 3</a> . <br></li></ul><br>  Aktivierungsanalysen oder Vergleichsmodelle k√∂nnen w√§hrend des Trainings und als Teil der Echtzeit√ºberwachung von Modellen verwendet werden. <br><br><h2>  5. Unpers√∂nlichkeit </h2><br>  Ein zielgerichteter Hacker kann - wiederum durch Versuch und Irrtum, durch Inversion mit einem Ersatzmodell oder Social Engineering - herausfinden, welche Eingabedaten oder bestimmte Personen das gew√ºnschte Vorhersageergebnis erhalten.  Ein Angreifer kann sich dann als diese Person ausgeben, um von der Prognose zu profitieren.  Unpers√∂nlichkeitsangriffe werden manchmal als ‚Äûsimulierte‚Äú Angriffe bezeichnet, und aus Sicht des Modells erinnert dies an Identit√§tsdiebstahl.  Wie im Fall eines Beispielangriffs im Wettbewerb werden bei der Unpers√∂nlichkeit die Eingabedaten gem√§√ü Ihrem Modell k√ºnstlich ge√§ndert.  Im Gegensatz zu demselben Angriff mit einem Wettbewerbsbeispiel, bei dem eine potenziell zuf√§llige Kombination von Werten verwendet werden kann, um bei der Unpers√∂nlichkeit die mit diesem Objekttyp verbundene Prognose zu t√§uschen, Informationen, die mit einem anderen modellierten Objekt (z. B. einem verurteilten Kunden) verkn√ºpft sind , Mitarbeiter, Finanztransaktion, Patient, Produkt usw.).  Angenommen, ein Angreifer kann herausfinden, von welchen Merkmalen Ihres Modells die Bereitstellung gro√üer Rabatte oder Vorteile abh√§ngt.  Dann kann er die Informationen f√§lschen, die Sie verwenden, um einen solchen Rabatt zu erhalten.  Ein Angreifer kann seine Strategie mit anderen teilen, was zu gro√üen Verlusten f√ºr Ihr Unternehmen f√ºhren kann. <br><br>  Wenn Sie ein zweistufiges Modell verwenden, achten Sie auf einen ‚Äûallergischen‚Äú Angriff: Ein Angreifer kann eine Reihe normaler Eingabedaten f√ºr die erste Stufe Ihres Modells simulieren, um die zweite Stufe anzugreifen. <br><br>  Schutz- und Expertenans√§tze f√ºr Angriffe mit Unpers√∂nlichkeit k√∂nnen Folgendes umfassen: <br><br><ul><li>  Aktivierungsanalyse.  siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 4</a> . <br></li><li>  Autorisierter Zugriff.  siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Abschnitt 3</a> . <br></li><li>  Suchen Sie nach Duplikaten.  Verfolgen Sie in der Bewertungsphase die Anzahl √§hnlicher Datens√§tze, f√ºr die Ihr Modell verf√ºgbar ist.  Dies kann in einem reduzierten Dimensionsraum unter Verwendung von Autocodierern, mehrdimensionaler Skalierung (MDS) oder √§hnlichen Dimensionsreduktionsmethoden erfolgen.  Wenn in einem bestimmten Zeitraum zu viele √§hnliche Zeilen vorhanden sind, ergreifen Sie Korrekturma√ünahmen. <br></li><li><script type="text/javascript">function gtElInit() {var lib = new google.translate.TranslateService();lib.translatePage('ru', 'de', function () {});}</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=gtElInit&amp;client=wt"></script> Funktionen zur Benachrichtigung √ºber Bedrohungen.  Speichern Sie die Funktion <code>num_similar_queries</code> in Ihrer Pipeline. <code>num_similar_queries</code> Funktion kann unmittelbar nach dem Training oder der Implementierung Ihres Modells unbrauchbar sein, kann jedoch w√§hrend der Evaluierung (oder w√§hrend einer zuk√ºnftigen Umschulung) verwendet werden, um das Modell oder die Pipeline √ºber Bedrohungen zu informieren.  Wenn beispielsweise zum Zeitpunkt der Bewertung der Wert von <code>num_similar_queries</code> gr√∂√üer als Null ist, kann die Anforderung zur Bewertung zur manuellen Analyse gesendet werden.  Wenn Sie das Modell in Zukunft neu trainieren, k√∂nnen Sie ihm beibringen, negative Vorhersageergebnisse f√ºr Eingabezeilen mit hohen <code>num_similar_queries</code> . <br></li></ul><br>  Aktivierungsanalyse, doppelte √úberpr√ºfung und Benachrichtigung √ºber potenzielle Bedrohungen k√∂nnen w√§hrend des Trainings und bei der √úberwachung von Modellen in Echtzeit verwendet werden. <br><br><h2>  6. H√§ufige Probleme </h2><br>  Einige g√§ngige Anwendungen f√ºr maschinelles Lernen werfen auch allgemeinere Sicherheitsprobleme auf. <br><br>  <b>Black Boxes und unn√∂tige Komplexit√§t</b> .  Obwohl die j√ºngsten Fortschritte bei interpretierten Modellen und Modellerkl√§rungen die Verwendung genauer und transparenter nichtlinearer Klassifikatoren und Regressoren erm√∂glichen, konzentrieren sich viele maschinelle Lernprozesse weiterhin auf Black-Box-Modelle.  Sie sind nur eine Art von oft unn√∂tiger Komplexit√§t im Standardworkflow des kommerziellen maschinellen Lernens.  Andere Beispiele f√ºr potenziell sch√§dliche Komplexit√§t k√∂nnen √ºberm√§√üig exotische Spezifikationen oder eine gro√üe Anzahl von Paketabh√§ngigkeiten sein.  Dies kann aus mindestens zwei Gr√ºnden ein Problem sein: <br><br><ol><li>  Ein hartn√§ckiger und motivierter Hacker kann mehr √ºber Ihr √ºberm√§√üig komplexes Black-Box-Simulationssystem erfahren als Sie oder Ihr Team (insbesondere in dem modernen √ºberhitzten und sich schnell √§ndernden Markt f√ºr die ‚ÄûAnalyse‚Äú von Daten).  Zu diesem Zweck kann ein Angreifer neben vielen anderen g√§ngigen Hacking-Tools viele neue modellunabh√§ngige <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkl√§rungsmethoden</a> und eine klassische Sensitivit√§tsanalyse verwenden.  Dieses Ungleichgewicht des Wissens kann m√∂glicherweise verwendet werden, um die in den Abschnitten 1 bis 5 beschriebenen Angriffe auszuf√ºhren, oder f√ºr andere Arten von Angriffen, die noch unbekannt sind. <br></li><li>  Maschinelles Lernen in Forschungs- und Entwicklungsumgebungen h√§ngt stark von einem vielf√§ltigen √ñkosystem von Open Source-Softwarepaketen ab.  Einige dieser Pakete haben viele Teilnehmer und Benutzer, andere sind hochspezialisiert und werden von einem kleinen Kreis von Forschern und Praktikern ben√∂tigt.  Es ist bekannt, dass viele Pakete von brillanten Statistikern und Forschern des maschinellen Lernens unterst√ºtzt werden, die sich eher auf Mathematik oder Algorithmen als auf Softwareentwicklung und sicherlich nicht auf Sicherheit konzentrieren.  Es gibt h√§ufige F√§lle, in denen die Pipeline f√ºr maschinelles Lernen von Dutzenden oder sogar Hunderten externer Pakete abh√§ngt, von denen jedes gehackt werden kann, um eine b√∂swillige ‚ÄûNutzlast‚Äú zu verbergen. <br></li></ol><br>  <b>Verteilte Systeme und Modelle</b> .  Gl√ºcklicherweise oder ungl√ºcklicherweise leben wir in einem Zeitalter von Big Data.  Viele Unternehmen verwenden heute verteilte Datenverarbeitungs- und maschinelle Lernsysteme.  Distributed Computing kann ein gro√ües Ziel f√ºr Angriffe von innen oder au√üen sein.  Daten k√∂nnen nur auf einem oder mehreren Arbeitsknoten eines gro√üen verteilten Datenspeicher- oder -verarbeitungssystems verzerrt werden.  Die Hintert√ºr f√ºr Wasserzeichen kann in ein Modell eines gro√üen Ensembles codiert werden.  Anstatt ein einfaches Dataset oder Modell zu debuggen, sollten Praktiker jetzt Daten oder Modelle untersuchen, die √ºber gro√üe Computercluster verteilt sind. <br><br>  <b>DDoS-Angriffe (Distributed Denial of Service)</b> .  Wenn ein Vorhersagemodellierungsdienst eine Schl√ºsselrolle bei den Aktivit√§ten Ihres Unternehmens spielt, stellen Sie sicher, dass Sie mindestens die beliebtesten verteilten DDoS-Angriffe ber√ºcksichtigen, wenn Angreifer einen Vorhersagedienst mit einer unglaublich gro√üen Anzahl von Anforderungen angreifen, um Prognosen f√ºr legitime Benutzer zu verz√∂gern oder zu beenden. <br><br><h2>  7. Allgemeine Entscheidungen </h2><br>  Sie k√∂nnen verschiedene g√§ngige, alte und neue, effektivste Methoden verwenden, um Sicherheitsl√ºcken in Sicherheitssystemen zu verringern und Fairness, Kontrollierbarkeit, Transparenz und Vertrauen in maschinelle Lernsysteme zu erh√∂hen. <br><br>  <b>Prognose f√ºr autorisierten Zugang und Frequenzregelung (Drosselung)</b> .  Standard-Sicherheitsfunktionen wie zus√§tzliche Authentifizierung und Anpassung der Vorhersagefrequenz k√∂nnen beim Blockieren einer Reihe von Angriffsvektoren, die in den Abschnitten 1 bis 5 beschrieben sind, sehr effektiv sein. <br><br>  <b>Vergleichsmodelle</b> .  Als Vergleichsmodell zur Bestimmung, ob mit der Prognose Manipulationen vorgenommen wurden, k√∂nnen Sie die alte und bew√§hrte Modellierungspipeline oder ein anderes interpretiertes Prognosewerkzeug mit hoher Transparenz verwenden.  Die Manipulation umfasst Datenkorruption, Wasserzeichenangriffe oder Wettbewerbsbeispiele.  Wenn der Unterschied zwischen der Prognose Ihres getesteten Modells und der Prognose eines komplexeren und undurchsichtigen Modells zu gro√ü ist, schreiben Sie solche F√§lle auf.  Senden Sie sie an Analysten oder ergreifen Sie andere Ma√ünahmen, um die Situation zu analysieren oder zu korrigieren.  Es m√ºssen ernsthafte Vorkehrungen getroffen werden, um sicherzustellen, dass Ihr Benchmark und Ihr F√∂rderband sicher und unver√§ndert von ihrem urspr√ºnglichen, zuverl√§ssigen Zustand bleiben. <br><br>  <b>Interpretierte, faire oder private Modelle</b> .  Derzeit gibt es Verfahren (z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">monotones GBM (M-GBM),</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">skalierbare Bayes'sche Regellisten (SBRLs)</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erkl√§rungen f√ºr neuronale Netze (XNNs)</a> ), die sowohl Genauigkeit als auch Interpretierbarkeit bieten.  Diese genauen und interpretierbaren Modelle sind einfacher zu dokumentieren und zu debuggen als klassische Black Boxes des maschinellen Lernens.  Neuere Arten von fairen und privaten Modellen (z. B. LFR, PATE) k√∂nnen auch darin geschult werden, extern sichtbaren demografischen Merkmalen, die zur Beobachtung zur Verf√ºgung stehen, weniger Aufmerksamkeit zu schenken, indem Social Engineering w√§hrend eines Angriffs mit einem Wettbewerbsbeispiel verwendet wird, oder Unpers√∂nlichkeit.  Denken Sie dar√ºber nach, in Zukunft einen neuen maschinellen Lernprozess zu schaffen?  Erw√§gen Sie, es auf der Grundlage weniger riskant interpretierter privater oder fairer Modelle aufzubauen.  Sie sind einfacher zu debuggen und m√∂glicherweise resistent gegen √Ñnderungen der Eigenschaften einzelner Objekte. <br><br>  <b>Debuggen eines Sicherheitsmodells</b> .  Ein neuer Bereich f√ºr das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Debuggen von Modellen ist</a> der Erkennung und Korrektur von Fehlern in Mechanismen und Vorhersagen von Modellen f√ºr maschinelles Lernen gewidmet.  Debugging-Tools wie Ersatzmodelle, Residuenanalyse und Sensitivit√§tsanalyse k√∂nnen in wei√üen Studien verwendet werden, um Ihre Schwachstellen zu identifizieren, oder in Analyse√ºbungen, um m√∂gliche Angriffe zu identifizieren, die auftreten k√∂nnen oder k√∂nnen. <br><br>  <b>Modelldokumentation und Erkl√§rungsmethoden</b> .  Die Modelldokumentation ist eine Strategie zur Risikominderung, die seit Jahrzehnten im Bankwesen eingesetzt wird.  Sie k√∂nnen Wissen √ºber komplexe Modellierungssysteme speichern und √ºbertragen, wenn sich die Zusammensetzung der Modellbesitzer √§ndert.  Die Dokumentation wurde traditionell f√ºr lineare Modelle mit hoher Transparenz verwendet.  Mit dem Aufkommen leistungsf√§higer, genauer Erkl√§rungswerkzeuge (wie dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SHAP-Baum</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">abgeleiteten Attributen lokaler Funktionen</a> f√ºr neuronale Netze) k√∂nnen bereits vorhandene Black-Box-Modell-Workflows zumindest ein wenig erkl√§rt, debuggt und dokumentiert werden.  Offensichtlich sollte die Dokumentation jetzt alle Sicherheitsziele enthalten, einschlie√ülich bekannter, behobener oder erwarteter Sicherheitsl√ºcken. <br><br>  <b>√úberwachen und verwalten Sie Modelle aus Sicherheitsgr√ºnden direkt</b> .  Seri√∂se Praktiker verstehen, dass die meisten Modelle auf statischen "Schnappsch√ºssen" der Realit√§t in Form von Datens√§tzen trainiert werden und dass die Genauigkeit von Prognosen in Echtzeit abnimmt, da sich der aktuelle Stand der Dinge von den zuvor gesammelten Informationen entfernt.  Heutzutage zielt die √úberwachung der meisten Modelle darauf ab, eine solche Verzerrung bei der Verteilung der Eingabevariablen zu identifizieren, die letztendlich zu einer Verringerung der Genauigkeit f√ºhren wird.  Die Modell√ºberwachung sollte so konzipiert sein, dass die in den Abschnitten 1 bis 5 beschriebenen Angriffe und alle anderen potenziellen Bedrohungen, die beim Debuggen Ihres Modells auftreten, verfolgt werden.  Obwohl dies nicht immer direkt mit der Sicherheit zusammenh√§ngt, sollten Modelle auch in Echtzeit auf differenzierte Effekte hin bewertet werden.  Zusammen mit der Modelldokumentation m√ºssen alle Modellierungsartefakte, der Quellcode und die zugeh√∂rigen Metadaten verwaltet, versioniert und auf Sicherheit sowie die wertvollen kommerziellen Ressourcen √ºberpr√ºft werden, die sie sind. <br><br>  <b>Funktionen zur Benachrichtigung √ºber Bedrohungen</b> .  Funktionen, Regeln und Phasen der vorl√§ufigen oder nachfolgenden Verarbeitung k√∂nnen in Ihren Modellen oder Prozessen enthalten sein, die mit Mitteln zur Benachrichtigung √ºber m√∂gliche Bedrohungen ausgestattet sind: z. B. die Anzahl √§hnlicher Zeilen im Modell;  ob die aktuelle Zeile einen Mitarbeiter, Auftragnehmer oder Berater darstellt;  Entsprechen die Werte in der aktuellen Zeile denen, die mit wei√üen Angriffen mit einem Wettbewerbsbeispiel erzielt wurden?  Diese Funktionen k√∂nnen w√§hrend des ersten Trainings des Modells ben√∂tigt werden oder nicht.  Das Einsparen von Platz f√ºr sie kann jedoch eines Tages sehr n√ºtzlich sein, um neue Daten auszuwerten oder das Modell anschlie√üend neu zu trainieren. <br><br>  <b>Erkennung von Systemanomalien</b> .  Trainieren Sie den Metamode zum Erkennen von Anomalien basierend auf einem Autocoder anhand der Betriebsstatistik Ihres gesamten Vorhersagemodellierungssystems (Anzahl der Prognosen f√ºr einen bestimmten Zeitraum, Verz√∂gerungen, CPU-, Speicher- und Festplattenladen, Anzahl der gleichzeitigen Benutzer usw.) und √ºberwachen Sie dieses Metamodell sorgf√§ltig auf Anomalien.  Eine Anomalie kann erkennen, ob etwas schief geht.  Nachuntersuchungen oder spezielle Mechanismen sind erforderlich, um die Ursache des Problems genau zu verfolgen. <br><br><h2>  8. Referenzen und Informationen zur weiteren Lekt√ºre </h2><br>  Ein gro√üer Teil der modernen akademischen Literatur zur Sicherheit des maschinellen Lernens konzentriert sich auf adaptives Lernen, tiefes Lernen und Verschl√ºsselung.  Bisher kennt der Autor jedoch nicht die Praktizierenden, die dies alles tats√§chlich tun w√ºrden.  Daher pr√§sentieren wir neben k√ºrzlich ver√∂ffentlichten Artikeln und Beitr√§gen Artikel der 1990er und fr√ºhen 2000er Jahre zu Netzwerkverletzungen, Virenerkennung, Spamfilterung und verwandten Themen, die ebenfalls n√ºtzliche Quellen waren.  Wenn Sie mehr √ºber das faszinierende Thema des Schutzes von Modellen f√ºr maschinelles Lernen erfahren m√∂chten, finden Sie hier die wichtigsten Links - aus der Vergangenheit und der Gegenwart -, die zum Schreiben des Beitrags verwendet wurden. <br><br><ul><li>  Bareno, Marco et al. Sicherheit beim maschinellen Lernen.  Machine Learning 81.2 (2010): 121 &amp; ndash; 148.  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://people.eecs.berkeley.edu/ <sub>~</sub> adj / publications / paper-files / SecML-MLJ2010.pdf</a> <br></li><li>  Kumar, Agites.  "Sicherheitsangriffe: Eine Analyse von Modellen des maschinellen Lernens."  DZone (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://dzone.com/articles/security-attacks-analysis-of-machine-learning-mode</a> <br></li><li>  Lorica, Ben und Lucidis, Mike.  ‚ÄûSie haben eine Anwendung f√ºr maschinelles Lernen erstellt.  Stellen Sie jetzt sicher, dass es sicher ist. "  O'Reilly Ideas (2019).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure</a> <br></li><li>  Paperno, Nicholas.  "Karte der Sicherheits- und Datenschutz-Pl√ºnderer beim maschinellen Lernen: Ein √úberblick √ºber aktuelle und zuk√ºnftige Forschungstrends beim Erreichen von Sicherheit und Vertraulichkeit beim maschinellen Lernen"  Vortr√§ge des 11. ACM-Workshops zu k√ºnstlicher Intelligenz und Sicherheit.  ACM (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://arxiv.org/pdf/1811.01134.pdf</a> <br></li></ul><br><h2>  Fazit </h2><br>  Diejenigen, die sich f√ºr die Wissenschaft und Praxis des maschinellen Lernens interessieren, sind besorgt dar√ºber, dass die Gefahr des Hackens mit maschinellem Lernen in Verbindung mit der wachsenden Gefahr von Verst√∂√üen gegen die Vertraulichkeit und algorithmischer Diskriminierung die wachsende √∂ffentliche und politische Skepsis gegen√ºber maschinellem Lernen und k√ºnstlicher Intelligenz erh√∂hen kann.  Wir alle m√ºssen uns an die schwierigen Zeiten f√ºr KI in der j√ºngeren Vergangenheit erinnern.  Sicherheitsl√ºcken, Datenschutzverletzungen und algorithmische Diskriminierung k√∂nnten m√∂glicherweise kombiniert werden, was zu einer Reduzierung der Mittel f√ºr Forschung auf dem Gebiet der Computerschulung oder zu drakonischen Ma√ünahmen zur Regulierung dieses Bereichs f√ºhren k√∂nnte.  Lassen Sie uns die Diskussion und L√∂sung dieser wichtigen Fragen fortsetzen, um eine Krise zu verhindern und ihre Folgen nicht zu st√∂ren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458892/">https://habr.com/ru/post/de458892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458882/index.html">√ñffentliche Rede. Kurz zur Hauptsache</a></li>
<li><a href="../de458884/index.html">Ein bisschen √ºber Weltraumkommunikationsstandards</a></li>
<li><a href="../de458886/index.html">Am n√ºtzlichsten Mail.ru Design Conf √ó Dribbble Meetup 2019 Papers von True Engineering</a></li>
<li><a href="../de458888/index.html">Summer Droid Meetup</a></li>
<li><a href="../de458890/index.html">Stichproben- und Berechnungsgenauigkeit</a></li>
<li><a href="../de458894/index.html">Typische Menschen und die Netzwerke, in denen sie leben</a></li>
<li><a href="../de458896/index.html">Funktionales JavaScript: Was sind Funktionen h√∂herer Ordnung und warum werden sie ben√∂tigt?</a></li>
<li><a href="../de458900/index.html">Konsolenpatronen als Modems</a></li>
<li><a href="../de458902/index.html">5 H√§ufige Python-Anf√§ngerfehler</a></li>
<li><a href="../de458904/index.html">Visualisierung der Anzahl der Siege f√ºr NBA-Teams mithilfe animierter Balkendiagramme in R.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>