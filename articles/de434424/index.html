<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë® üßëüèæ‚Äçü§ù‚Äçüßëüèª üìñ JAVA SOUND API-Grundlagen ‚òéÔ∏è üë®‚Äçüë©‚Äçüëß‚Äçüëß ü§üüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Habr! Ich pr√§sentiere Ihnen die √úbersetzung des Artikels ‚ÄûJava Sound, Erste Schritte, Teil 1, Wiedergabe‚Äú . 

 Sound in JAVA, Teil Eins, Der Anf...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>JAVA SOUND API-Grundlagen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/434424/">  Hallo Habr!  Ich pr√§sentiere Ihnen die √úbersetzung des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûJava Sound, Erste Schritte, Teil 1, Wiedergabe‚Äú</a> . <br><br><h3>  Sound in JAVA, Teil Eins, Der Anfang.  Sound abspielen </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1JZnj4eNHXE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Dies ist der Beginn einer Reihe von acht Lektionen, die Sie mit der Java Sound-API vertraut machen. <br><a name="habracut"></a><br>  Was ist Klang in der menschlichen Wahrnehmung?  Dies ist das Gef√ºhl, das wir erleben, wenn eine √Ñnderung des Luftdrucks auf die winzigen sensorischen Bereiche in unseren Ohren √ºbertragen wird. <br><br>  Das Hauptziel bei der Erstellung der Sound-API besteht darin, Ihnen Mittel zum Schreiben von Code zur Verf√ºgung zu stellen, mit deren Hilfe Druckwellen zur richtigen Zeit auf die Ohren des richtigen Motivs √ºbertragen werden k√∂nnen. <br><br>  Arten von Sound in Java: <br><br><ol><li>  Die Java Sound API unterst√ºtzt zwei Haupttypen von Audio (Sound). </li><li>  Ton digitalisiert und direkt als Datei aufgezeichnet </li><li>  Aufnahme als MIDI-Datei.  Sehr weit entfernt, aber √§hnlich der Notenschrift, bei der Musikinstrumente in der gew√ºnschten Reihenfolge gespielt werden. </li></ol><br>  Diese Typen unterscheiden sich in ihrem Wesen erheblich, und wir werden uns auf den ersten konzentrieren, da es sich in den meisten F√§llen um Ton handelt, der entweder digitalisiert werden muss, um von einer externen Quelle in eine Datei aufzunehmen, oder umgekehrt, um zuvor aus einer solchen Datei aufgenommenen Ton wiederzugeben. <br><br><h3>  Vorschau </h3><br>  Die Java Sound API basiert auf dem Konzept von <i>Linien und Mischern.</i> <br><br>  Weiter: <br>  Wir werden die physikalischen und elektrischen Eigenschaften der analogen Klangdarstellung beschreiben, die auf einen <i>Audiomischer</i> angewendet wird. <br><br>  Wir wenden uns dem Szenario der beginnenden Rockband zu, die in diesem Fall sechs Mikrofone und zwei Stereolautsprecher verwendet.  Wir brauchen dies, um die Funktionsweise des Audiomischers zu verstehen. <br><br>  Als N√§chstes betrachten wir eine Reihe von Java Sound-Themen f√ºr die Programmierung, z. B. Linien, Mixer, Formate f√ºr Audiodaten und mehr. <br><br>  Wir werden die Beziehungen zwischen den Objekten SourceDataLine, Clip, Mixer und AudioFormat verstehen und ein einfaches Programm erstellen, das Audio wiedergibt. <br><br>  Im Folgenden finden Sie ein Beispiel f√ºr dieses Programm, mit dem Sie den aufgenommenen Ton aufnehmen und dann wiedergeben k√∂nnen. <br><br>  In Zukunft werden wir den zu diesem Zweck verwendeten Programmcode vollst√§ndig erl√§utern.  Aber keineswegs vollst√§ndig in dieser Lektion. <br><br><h3>  Codebeispiel und √úberlegung </h3><br>  <b>Physikalische und elektrische Eigenschaften von analogem Klang</b> <br><br>  In unserer Lektion m√∂chten wir Ihnen die Grundlagen der Java-Programmierung mithilfe der Java Sound-API vorstellen. <br><br>  Die Java Sound API basiert auf dem Konzept eines Audio-Mixers, der h√§ufig zum Abspielen von Sound fast √ºberall verwendet wird: von Rockkonzerten bis zum H√∂ren von CDs zu Hause.  Bevor Sie jedoch eine ausf√ºhrliche Erl√§uterung der Funktionsweise des Audiomischers vornehmen, sollten Sie sich mit den physikalischen und elektrischen Eigenschaften des analogen Klangs selbst vertraut machen. <br><br>  <i>Schauen Sie sich Abb.</i>  <i>1</i> <br><br><img src="https://habrastorage.org/webt/ez/tu/sq/eztusq7byax0l9nu-5r6vj3vkxe.gif"><br><br>  Vasya Pupyrkin dr√ºckt eine Rede. <br><br>  Diese Abbildung zeigt, wie Vasya eine Rede mit einem System h√§lt, das als Wide-Address-System bekannt ist.  Ein solches System umfasst typischerweise ein Mikrofon, einen Verst√§rker und einen Lautsprecher.  Der Zweck dieses Systems ist es, Vasyas Stimme zu st√§rken, so dass er auch in einer gro√üen Menge geh√∂rt werden kann. <br><br>  <b>Wackeln in der Luft</b> <br><br>  Kurz gesagt, wenn Vasya spricht, lassen seine Stimmb√§nder Luftpartikel in seinem Kehlkopf vibrieren.  Dies f√ºhrt zur Entstehung von Schallwellen, die wiederum dazu f√ºhren, dass die Mikrofonmembran vibriert und sich dann in elektrische Schwingungen mit sehr kleiner Amplitude verwandelt, die genau die Schallschwingungen von Vasyas Original simulieren.  Ein Verst√§rker verst√§rkt, wie der Name schon sagt, diese elektrischen Schwingungen.  Dann gelangen sie zum Lautsprecher, der die inverse Umwandlung verst√§rkter elektrischer Schwingungen in sehr verst√§rkte Schallwellen durchf√ºhrt, aber dennoch genau die gleichen Wellen wiederholt, die in Vasya Pupyrkins Stimmb√§ndern erzeugt werden. <br><br>  <b>Dynamisches Mikrofon</b> <br><br>  Schauen wir uns nun Abb.  2, die ein schematisches Diagramm eines Mikrofons zeigt, das als dynamisch bezeichnet wird. <br><br><img src="https://habrastorage.org/webt/hz/1v/ui/hz1vui2-yqnq4cg3xdpi5iy-1w0.gif"><br>  <i>Abb.</i>  <i>2 Dynamische Mikrofonschaltung</i> <br><br>  <b>Schallschwingungen beeinflussen die Membran</b> <br><br>  Der Druck der Schallschwingungen wirkt auf eine flexible Membran im Mikrofon.  Dies bewirkt, dass die Membran vibriert, w√§hrend die Vibrationen der Membran die Vibrationen von Schallwellen wiederholen. <br><br>  <b>Bewegliche Spule</b> <br><br>  Eine Spule aus d√ºnnem Draht ist an der Mikrofonmembran angebracht.  Wenn die Membran schwingt, macht die Spule auch Hin- und Herbewegungen im Magnetfeld des Kerns, der aus einem starken Permanentmagneten besteht.  Und wie auch Faraday feststellte, entsteht in der Spule ein elektrischer Strom. <br><br>  <b>Ein elektrisches Signal folgt der Form von Schallwellen.</b> <br><br>  Somit wird aus einem sehr schwachen Strom, der in der Spule induziert wird, ein elektrisches Wechselsignal erhalten, das die Form von Schallwellen wiederholt, die auf die Mikrofonmembran wirken.  Ferner wird dieses Signal in Form einer Wechselspannung dem Eingang des Verst√§rkers aus Fig. 1 zugef√ºhrt.  1. <br><br>  <b>Lautsprecher</b> <br><br>  Tats√§chlich wiederholt das Funktionsprinzip des Lautsprechers das Ger√§t eines dynamischen Mikrofons, das nur in die entgegengesetzte Richtung eingeschaltet ist.  <i>(In diesem Fall sind die Wicklungsdr√§hte nat√ºrlich viel dicker und die Membran ist viel gr√∂√üer, um den Betrieb mit einem verst√§rkten Signal sicherzustellen.)</i> <i><br></i> <br><br><img src="https://habrastorage.org/webt/0e/ec/4x/0eec4xwyiyp2icsx69azgymv78c.gif"><br><br>  Schwingungen der Lautsprechermembran wirken sich auf Luftpartikel aus und erzeugen starke Schallwellen.  Die Form dieser Wellen wiederholt genau die Form von Schallwellen mit viel geringerer Intensit√§t, die durch Vasyas Stimmb√§nder erzeugt werden.  Aber die Intensit√§t der neuen Wellen reicht jetzt aus, um sicherzustellen, dass die Schallschwingungen von Vasya die Ohren von Menschen erreichen, die selbst in den hinteren Reihen einer gro√üen Menge stehen. <br><br>  <b>Rockkonzert</b> <br><br>  Zu diesem Zeitpunkt fragen Sie sich vielleicht, was dies alles mit der Java Sound API zu tun hat.  Aber warten Sie etwas l√§nger, wir f√ºhren Sie zu den Grundlagen des Audiomischers. <br><br>  Die oben beschriebene Schaltung war recht einfach.  Es bestand aus Vasya Pupyrkin, einem Mikrofon, einem Verst√§rker und einem Lautsprecher.  Betrachten Sie nun die Schaltung mit Abb.  4, die die B√ºhne pr√§sentiert, die f√ºr das Rockkonzert der beginnenden Musikgruppe vorbereitet wurde. <br><br><img src="https://habrastorage.org/webt/jh/zh/qo/jhzhqouio0xa25axcr164jch4du.gif"><br><br>  <b>Sechs Mikrofone und zwei Lautsprecher</b> <br><br>  In Abb.  4 Sechs Mikrofone befinden sich auf der B√ºhne.  An den Seiten der B√ºhne befinden sich zwei Lautsprecher.  Zu Beginn des Konzerts singen oder spielen die Darsteller in jedem der sechs Mikrofone Musik.  Dementsprechend werden wir sechs elektrische Signale haben, die einzeln verst√§rkt und dann beiden Lautsprechern zugef√ºhrt werden m√ºssen.  Dar√ºber hinaus k√∂nnen Interpreten verschiedene Sound-Spezialeffekte verwenden, z. B. Hall, die ebenfalls in elektrische Signale umgewandelt werden m√ºssen, bevor sie an die Lautsprecher angelegt werden. <br><br>  Zwei Lautsprecher an den Seiten der B√ºhne erzeugen den Effekt von Stereoklang.  Das hei√üt, das elektrische Signal, das vom rechts auf der B√ºhne befindlichen Mikrofon kommt, sollte in den ebenfalls rechts befindlichen Lautsprecher fallen.  Ebenso sollte das Signal vom Mikrofon links dem Lautsprecher links von der Szene zugef√ºhrt werden.  Elektrische Signale von anderen Mikrofonen, die sich n√§her an der B√ºhnenmitte befinden, sollten jedoch bereits in angemessenen Anteilen an beide Lautsprecher √ºbertragen werden.  Und zwei Mikrofone direkt in der Mitte sollten ihr Signal gleicherma√üen an beide Lautsprecher √ºbertragen. <br><br>  <b>Audiomischer</b> <br><br>  Die oben diskutierte Aufgabe wird nur von einem elektronischen Ger√§t ausgef√ºhrt, das als Audiomischer bezeichnet wird. <br><br>  <b>Audio-Leitung (Kanal)</b> <br><br>  Obwohl der Autor kein Experte f√ºr Audiomischer ist, hat ein typischer Audiomischer nach seinem bescheidenen Verst√§ndnis die F√§higkeit, am Eingang eine bestimmte Anzahl von voneinander unabh√§ngigen elektrischen Signalen zu empfangen, von denen jedes das urspr√ºngliche Tonsignal oder die urspr√ºngliche Leitung <i>(Kanal) darstellt.</i> <br><br>  (Das Konzept eines Audiokanals wird sehr wichtig, wenn wir beginnen, die Java Sound-API im Detail zu verstehen. <br><br>  <b>Unabh√§ngige Verarbeitung jedes Audiokanals</b> <br><br>  In jedem Fall kann der Standard-Audiomischer jede Audiolinie unabh√§ngig von den anderen anderen Kan√§len verst√§rken.  Au√üerdem kann der Mixer normalerweise Sound-Spezialeffekte wie z. B. Hall auf eine der Audio-Linien √ºbertragen.  Am Ende kann der Mischer, wie der Name schon sagt, alle einzelnen elektrischen Signale in den Ausgangskan√§len so mischen, wie er eingestellt wird, um den Beitrag jeder Audioleitung zu den Ausgangskan√§len zu steuern. (Dieser Regler wird normalerweise als Pan oder Pan bezeichnet. Verteilung im Raum). <br><br>  <b>Zur√ºck zum Stereoton</b> <br><br>  So ist im Diagramm mit Abb.  In 4 hat der Toningenieur des Audiomischers die F√§higkeit, Signale von sechs Mikrofonen zu kombinieren, um zwei Ausgangssignale zu erhalten, von denen jedes zu seinem Lautsprecher √ºbertragen wird. <br><br>  F√ºr einen erfolgreichen Betrieb muss das Signal von jedem Mikrofon in einem angemessenen Verh√§ltnis geliefert werden, abh√§ngig von der physischen Position des Mikrofons auf der B√ºhne.  (Durch √Ñndern des Schwenks kann ein qualifizierter Tontechniker bei Bedarf den Beitrag jedes Mikrofons √§ndern, wenn sich beispielsweise der S√§nger w√§hrend eines Konzerts auf der B√ºhne bewegt.) <br><br>  <b>Zeit, in die Welt der Programmierung zur√ºckzukehren</b> <br><br>  Kehren wir nun von der physischen Welt in die Welt der Programmierung zur√ºck.  Laut Sun: <i>‚ÄûJava Sound beinhaltet keine spezielle Hardwarekonfiguration.</i>  <i>Es erm√∂glicht die Installation verschiedener Audiokomponenten auf dem System und die Bereitstellung f√ºr den Benutzer √ºber die API.</i>  <i>Java Sound unterst√ºtzt die Standard-Ein- und Ausgabefunktionen einer Soundkarte (z. B. zum Aufnehmen und Abspielen von Audiodateien) sowie die M√∂glichkeit, mehrere Audiostreams zu mischen. ‚Äú</i> <br><br>  <b>Mischer und Kan√§le</b> <br><br>  Wie bereits erw√§hnt, basiert die Java Sound API auf dem Konzept von Mixern und Kan√§len.  Wenn Sie von der physischen Welt in die Programmierwelt wechseln, schreibt Sun Folgendes zum Mixer: <br><br>  <i>‚ÄûEin Mixer ist ein Audioger√§t mit einem oder mehreren Kan√§len.</i>  <i>Der Mixer, der das Audiosignal wirklich mischt, muss jedoch mehrere Eingangskan√§le von Quellquellen und mindestens einen Ausgangszielkanal haben. "</i> <br><br>  Eingabezeilen k√∂nnen Instanzen von Klassen mit SourceDataLine-Objekten sein, und Ausgabezeilen k√∂nnen TargetDataLine-Objekte sein.  Der Mixer kann auch aufgezeichneten und geloopten Sound als Eingang empfangen und seine Eingangsquellenkan√§le als Instanzen von Klassenobjekten definieren, die die Clip-Schnittstelle implementieren. <br><br>  Kanalleitungsschnittstelle. <br><br>  Sun meldet Folgendes √ºber die Line-Schnittstelle: ‚Äû <i>Line ist ein Element einer digitalen Audio-Pipeline, z. B. ein Audioeingang oder -ausgang, ein Mixer oder ein Audiopfad zu oder von einem Mixer.</i>  <i>Die Audiodaten, die durch den Kanal geleitet werden, k√∂nnen ein- oder mehrkanalig sein (z. B. Stereo).</i>  <i>... Ein Kanal kann Steuerelemente wie Gain, Pan und Reverb haben. ‚Äú</i> <br><br>  <b>Begriffe zusammenf√ºgen</b> <br><br>  Die obigen Zitate von Sun bezeichneten also die folgenden Begriffe <br><br>  Sourcedataline <br>  Targetgetataline <br>  Hafen <br>  Clip <br>  Kontrollen <br><br>  <i>Abb.</i>  <i>5 zeigt ein Beispiel f√ºr die Verwendung dieser Begriffe zum Erstellen eines einfachen Audioausgabeprogramms.</i> <br><br><img src="https://habrastorage.org/webt/e1/5r/gh/e15rghejgy0b2reeciyvircdvua.gif"><br><br>  <b>Programm Skript</b> <br><br>  Aus Sicht der Software  5 zeigt ein Mixer-Objekt, das mit einem Clip-Objekt und zwei SourceDataLine-Objekten erhalten wurde. <br><br>  <b>Was ist Clip?</b> <br><br>  Clip ist ein Objekt am Eingang des Mischpults, dessen Inhalt sich mit der Zeit nicht √§ndert.  Mit anderen Worten, Sie laden die Audiodaten in das Clip-Objekt, bevor Sie es abspielen.  Der Audioinhalt des Clip-Objekts kann ein- oder mehrmals abgespielt werden.  Sie k√∂nnen den Clip zur√ºckschleifen und dann wird der Inhalt immer wieder abgespielt. <br><br>  <b>Eingabestream</b> <br><br>  Das SourceDataLine-Objekt ist dagegen ein Stream-Objekt am Eingang des Mixers.  Ein Objekt dieses Typs kann einen Strom von Audiodaten empfangen und in Echtzeit an den Mixer senden.  Die erforderlichen Audiodaten k√∂nnen aus verschiedenen Quellen wie Audiodateien, Netzwerkverbindung oder Speicherpuffer bezogen werden. <br><br>  <b>Verschiedene Arten von Kan√§len</b> <br><br>  Somit k√∂nnen die Objekte Clip und SourceDataLine als Eingangskan√§le f√ºr das Mixer-Objekt betrachtet werden.  Jeder dieser Eingangskan√§le kann seinen eigenen haben: Pan, Gain und Reverb. <br><br>  <b>Audioinhalte abspielen</b> <br><br>  In einem solch einfachen System liest der Mixer Daten von den Eingangsleitungen, verwendet die Steuerung zum Mischen der Eingangssignale und liefert die Ausgabe an einen oder mehrere Ausgangskan√§le, wie z. B. einen Lautsprecher, einen Leitungsausgang, eine Kopfh√∂rerbuchse usw. <br><br>  Listing 11 zeigt ein einfaches Programm, das Audiodaten von einem Mikrofonanschluss erfasst, diese Daten im Speicher speichert und dann √ºber den Lautsprecheranschluss wiedergibt. <br><br>  Wir werden nur die Aufnahme und Wiedergabe diskutieren.  Der gr√∂√üte Teil des oben genannten Programms besteht darin, ein Fenster und eine grafische Oberfl√§che f√ºr den Benutzer zu erstellen, damit die Aufnahme und Wiedergabe gesteuert werden kann.  Wir werden diesen Teil nicht als √ºber das Ziel hinausgehend er√∂rtern.  Aber dann werden wir die Erfassung und Wiedergabe von Daten betrachten.  Wir werden in dieser Lektion √ºber das Verlieren sprechen und in der n√§chsten festhalten.  Auf dem Weg werden wir die Verwendung des Audiokanals mit der Java Sound API veranschaulichen. <br><br>  Die erfassten Daten werden in einem ByteArrayOutputStream-Objekt gespeichert. <br><br>  Ein Code-Snippet-Snippet erm√∂glicht das Lesen von Audiodaten von einem Mikrofon und das Speichern als ByteArrayOutputStream-Objekt. <br><br>  Die Methode mit dem Namen playAudio, die in Listing 1 beginnt, spielt die Audiodaten ab, die im ByteArrayOutputStream-Objekt erfasst und gespeichert wurden. <br><br><pre><code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">playAudio</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> audioData[] = byteArrayOutputStream. toByteArray(); InputStream byteArrayInputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ByteArrayInputStream( audioData);</code> </pre> <br>  <i>Listing 1</i> <br><br>  <b>Wir beginnen mit dem Standardcode.</b> <br><br>  Das Programm-Snippet in Listing 1 ist noch nicht mit Java Sound verwandt. <br><br>  Sein Zweck ist: <br><br><ul><li>  Konvertieren Sie zuvor gespeicherte Daten in ein Array vom Typ Byte. </li><li>  Ruft den Eingabestream f√ºr ein Byte-Datenarray ab. </li></ul><br>  Wir ben√∂tigen dies, um Audiodaten f√ºr die sp√§tere Wiedergabe verf√ºgbar zu machen. <br><br>  <b>Gehen Sie zur Sound-API</b> <br><br>  Die Codezeile in Listing 2 bezieht sich bereits auf die Java Sound API. <br><br><pre> <code class="java hljs"> AudioFormat audioFormat = getAudioFormat();</code> </pre><br>  <i>Listing 2:</i> <br><br>  Hier gehen wir kurz auf das Thema ein, das in der n√§chsten Lektion ausf√ºhrlich besprochen wird. <br><br>  <b>Zwei unabh√§ngige Formate</b> <br><br>  Meist handelt es sich um zwei unabh√§ngige Formate f√ºr Audiodaten. <br><br>  Dateiformat (beliebig), das Audiodaten enth√§lt (in unserem Programm noch nicht, da die Daten im Speicher gespeichert sind) <br><br>  Das Format der √ºbermittelten Audiodaten ist an sich. <br><br>  <b>Was ist ein Audioformat?</b> <br><br>  Hier ist, was Sun dar√ºber schreibt: <br><br>  <i>‚ÄûJeder Datenkanal hat ein eigenes Audioformat, das seinem Datenstrom zugeordnet ist.</i>  <i>Das Format (eine Instanz von AudioFormat) bestimmt die Bytereihenfolge des Audiostreams.</i>  <i>Die Formatparameter k√∂nnen die Anzahl der Kan√§le, die Abtastfrequenz, das Quantisierungsbit, das Codierungsverfahren usw. sein. Die √ºblichen Codierungsverfahren k√∂nnen die lineare Pulscodemodulation des PCM und seiner Varianten sein. ‚Äú</i> <br><br>  <b>Bytefolge</b> <br><br>  Die Quell-Audiodaten sind eine Bytefolge von Bin√§rdaten.  Es gibt verschiedene M√∂glichkeiten, wie Sie diese Sequenz organisieren und interpretieren k√∂nnen.  Wir werden uns nicht im Detail mit all diesen Optionen befassen, aber wir werden ein wenig auf das Audioformat eingehen, das wir hier in unserem Programm verwenden. <br><br>  <b>Kleiner Exkurs</b> <br><br>  Hier verlassen wir zun√§chst die playAudio-Methode und sehen uns die getAudioFormat-Methode aus Listing 2 an. <br><br>  <i>Die vollst√§ndige getAudioFormat-Methode ist in Listing 3 dargestellt.</i> <br><br><pre> <code class="java hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> AudioFormat </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getAudioFormat</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> sampleRate = <span class="hljs-number"><span class="hljs-number">8000.0F</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> sampleSizeInBits = <span class="hljs-number"><span class="hljs-number">16</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> channels = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> signed = <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> bigEndian = <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioFormat( sampleRate, sampleSizeInBits, channels, signed, bigEndian); }<span class="hljs-comment"><span class="hljs-comment">//end getAudioFormat</span></span></code> </pre><br>  <i>Listing 3:</i> <br><br>  Zus√§tzlich zur Deklaration initialisierter Variablen enth√§lt der Code in Listing 3 einen ausf√ºhrbaren Ausdruck. <br><br>  <b>AudioFormat-Objekt</b> <br><br>  Die Methode getAudioFormat erstellt eine Instanz eines Objekts der AudioFormat-Klasse und gibt sie zur√ºck.  Folgendes schreibt Sun √ºber diese Klasse: <br><br>  <i>‚ÄûDie AudioFormat-Klasse definiert die spezifische Reihenfolge der Daten in einem Audiostream.</i>  <i>Wenn Sie sich den Feldern des AudioFormat-Objekts zuwenden, erhalten Sie Informationen zur korrekten Interpretation von Bits in einem bin√§ren Datenstrom. ‚Äú</i> <br><br>  <b>Wir verwenden den einfachsten Konstruktor</b> <br><br>  Die AudioFormat-Klasse verf√ºgt √ºber zwei Arten von Konstruktoren (wir werden den trivialsten nehmen).  Die folgenden Parameter sind f√ºr diesen Konstruktor erforderlich: <br><br><ul><li>  Abtastrate oder Abtastrate pro Sekunde (Verf√ºgbare Werte: 8000, 11025, 16000, 22050 und 44100 Abtastungen pro Sekunde) </li><li>  Bittiefe der Daten (8 und 16 Bit pro Z√§hlung sind verf√ºgbar) </li><li>  Anzahl der Kan√§le (ein Kanal f√ºr Mono und zwei f√ºr Stereo) </li><li>  Signierte oder nicht signierte Daten, die im Stream verwendet werden (z. B. variiert der Wert zwischen 0 und 255 oder zwischen -127 und +127). </li><li>  Die Bytereihenfolge von Big-Endian oder Little-Endian.  (Wenn Sie einen Byte-Stream mit 16-Bit-Werten √ºbertragen, ist es wichtig zu wissen, welches Byte zuerst kommt - niedrig oder hoch, da es beide Optionen gibt). </li></ul><br>  Wie Sie in Listing 3 sehen k√∂nnen, haben wir in unserem Fall die folgenden Parameter f√ºr eine Instanz des AudioFormat-Objekts verwendet. <br><br><ul><li>  8000 Proben pro Sekunde </li><li>  16 Datengr√∂√üe </li><li>  signifikante Daten </li><li>  Little-Endian-Ordnung </li></ul><br>  Standardm√§√üig werden Daten von linearem PCM codiert. <br><br>  Der von uns verwendete Konstruktor erstellt eine Instanz des AudioFormat-Objekts unter Verwendung der linearen Pulscodemodulation und der oben angegebenen Parameter (wir werden in den folgenden Lektionen auf lineares PCM und andere Codierungsmethoden zur√ºckkommen). <br><br>  <b>Zur√ºck zur playAudio-Methode</b> <br><br>  Nachdem wir nun verstanden haben, wie das Audiodatenformat in Java-Sound funktioniert, kehren wir zur playAudio-Methode zur√ºck.  Sobald wir die verf√ºgbaren Audiodaten abspielen wollen, ben√∂tigen wir ein Objekt der Klasse AudioInputStream.  Wir erhalten eine Instanz davon in Listing 4. <br><br><pre> <code class="java hljs"> audioInputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioInputStream( byteArrayInputStream, audioFormat, audioData.length/audioFormat. getFrameSize());</code> </pre><br>  <i>Listing 4:</i> <br><br>  <b>Parameter f√ºr den AudioInputStream-Konstruktor</b> <br><br><ul><li>  Der Konstruktor f√ºr die AudioInputStream-Klasse erfordert die folgenden drei Parameter: </li><li>  Der Stream, auf dem die Instanz des AudioInputStream-Objekts basiert (wie wir zu diesem Zweck sehen, verwenden wir die zuvor erstellte Instanz des ByteArrayInputStream-Objekts). </li><li>  Das Audiodatenformat f√ºr diesen Stream (zu diesem Zweck haben wir bereits eine Instanz des AudioFormat-Objekts erstellt) </li><li>  Die Gr√∂√üe des Frames (Frames) f√ºr die Daten in diesem Stream (siehe Beschreibung unten) </li><li>  Die ersten beiden Parameter sind aus dem Code in Listing 4 ersichtlich. Der dritte Parameter ist jedoch an sich nicht so offensichtlich. </li></ul><br>  <b>Holen Sie sich die Rahmengr√∂√üe</b> <br><br>  Wie wir aus Listing 4 sehen k√∂nnen, wird der Wert des dritten Parameters mithilfe von Berechnungen erstellt.  Dies ist nur eines der Attribute des Audioformats, die wir zuvor nicht erw√§hnt haben, und es wird als Frame bezeichnet. <br><br>  <b>Was ist ein Rahmen?</b> <br><br>  F√ºr ein einfaches lineares PCM, das in unserem Programm verwendet wird, enth√§lt der Frame eine Reihe von Samples f√ºr alle Kan√§le zu einem bestimmten Zeitpunkt. <br><br>  Somit ist die Rahmengr√∂√üe gleich der Gr√∂√üe der Anzahl in Bytes mal der Anzahl von Kan√§len. <br><br>  Wie Sie vielleicht vermutet haben, gibt eine Methode namens getFrameSize die Frame-Gr√∂√üe in Bytes zur√ºck. <br><br>  <b>Berechnung der Rahmengr√∂√üe</b> <br><br>  Somit kann die L√§nge von Audiodaten in einem Rahmen berechnet werden, indem die Gesamtzahl der Bytes in der Audiodatensequenz durch die Anzahl der Bytes in einem Rahmen dividiert wird.  Diese Berechnung wird f√ºr den dritten Parameter in Listing 4 verwendet. <br><br>  <b>Abrufen eines SourceDataLine-Objekts</b> <br><br>  Der n√§chste Teil des Programms, den wir diskutieren werden, ist ein einfaches Audioausgabesystem.  Wie wir aus dem Diagramm in Abb. 5 sehen k√∂nnen, ben√∂tigen wir zur L√∂sung dieses Problems ein SourceDataLine-Objekt. <br><br>  Es gibt verschiedene M√∂glichkeiten, eine Instanz des SourceDataLine-Objekts abzurufen, die alle sehr schwierig sind.  Der Code in Listing 5 ruft einen Verweis auf eine Instanz des SourceDataLine-Objekts ab und speichert ihn. <br><br>  (Beachten Sie, dass dieser Code nicht nur das SourceDataLine-Objekt instanziiert. Er wird auf ziemlich umst√§ndliche Weise abgerufen.) <br><br><pre> <code class="java hljs"> DataLine.Info dataLineInfo = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> DataLine.Info( SourceDataLine.class, audioFormat); sourceDataLine = (SourceDataLine) AudioSystem.getLine( dataLineInfo);</code> </pre><br>  <i>Listing 5:</i> <br><br>  Was ist ein SourceDataLine-Objekt? <br><br>  Dar√ºber schreibt Sun Folgendes: <br><br>  <i>‚ÄûSourceDataLine ist ein Datenkanal, in den Daten geschrieben werden k√∂nnen.</i>  <i>Es dient als Eingang f√ºr einen Mixer.</i>  <i>Eine Anwendung schreibt eine Byte-Sequenz in eine SourceDataLine, die die Daten puffert und an ihren Mixer liefert.</i>  <i>Der Mischer kann die Daten, die er f√ºr die n√§chste Stufe verarbeitet, beispielsweise an den Ausgangsport √ºbertragen.</i> <i><br><br></i>  <i>Beachten Sie, dass die Namenskonvention f√ºr diese Paarung die Beziehung zwischen dem Kanal und seinem Mixer widerspiegelt. ‚Äú</i> <br><br>  <b>GetLine-Methode f√ºr die AudioSystem-Klasse</b> <br><br>  Eine M√∂glichkeit, eine Instanz des SourceDataLine-Objekts abzurufen, besteht darin, die statische getLine-Methode aus der AudioSystem-Klasse aufzurufen (wir werden in den n√§chsten Lektionen viel dar√ºber zu berichten haben). <br><br>  Die Methode getLine erfordert einen Eingabeparameter vom Typ Line.Info und gibt ein Line-Objekt zur√ºck, das der Beschreibung im bereits definierten Line.Info-Objekt entspricht. <br><br>  <b>Noch ein kurzer Exkurs</b> <br><br>  Sun meldet die folgenden Informationen zum Line.Info-Objekt: <br><br>  ‚ÄûDer Kanal verf√ºgt √ºber ein eigenes Informationsobjekt (eine Instanz von Line.Info), das anzeigt, welcher Mixer (falls vorhanden) die gemischten Audiodaten als Ausgabe direkt an den Kanal sendet und welcher Mixer (falls vorhanden) die Audiodaten als Eingabe direkt vom Kanal empf√§ngt.  Linienvarianten k√∂nnen Unterklassen von Line.Info entsprechen, mit denen Sie andere Parametertypen angeben k√∂nnen, die sich auf bestimmte Kanaltypen beziehen. ‚Äú <br><br>  <b>DataLine.Info-Objekt</b> <br><br>  Der erste Ausdruck in Listing 5 erstellt eine neue Instanz des DataLine.Info-Objekts, bei der es sich um eine spezielle Form (Unterklasse) des Line.Info-Objekts handelt. <br><br>  Es gibt mehrere √ºberladene Konstruktoren f√ºr die DataLine.Info-Klasse.  Wir haben die am einfachsten zu verwendende ausgew√§hlt.  Dieser Konstruktor ben√∂tigt zwei Parameter. <br><br>  <b>Klassenobjekt</b> <br><br>  Der erste Parameter ist Class, der die Klasse darstellt, die wir als SourceDataLine.class definiert haben <br><br>  Der zweite Parameter bestimmt das gew√ºnschte Datenformat f√ºr den Kanal.  Wir verwenden daf√ºr eine Instanz des AudioFormat-Objekts, die bereits zuvor definiert wurde. <br><br>  <b>Wo sind wir schon</b> <br><br>  Leider haben wir immer noch nicht das am meisten ben√∂tigte SourceDataLine-Objekt.  Bisher haben wir ein Objekt, das nur Informationen √ºber das von uns ben√∂tigte SourceDataLine-Objekt bereitstellt. <br><br>  <b>Abrufen eines SourceDataLine-Objekts</b> <br><br>  Der zweite Ausdruck in Listing 5 erstellt und speichert schlie√ülich die ben√∂tigte Instanz von SourceDataLine.  Dies geschieht durch Aufrufen der statischen Methode getLine der AudioSystem-Klasse und √úbergabe von dataLineInfo als Parameter.  (In der n√§chsten Lektion werden wir uns ansehen, wie Sie das Line-Objekt erhalten, indem Sie direkt mit dem Mixer-Objekt arbeiten.) <br><br>  Die Methode getLine gibt einen Verweis auf ein Objekt vom Typ Line zur√ºck, das das √ºbergeordnete Element von SourceDataLine ist.  Daher ist hier ein Downcast erforderlich, bevor der R√ºckgabewert als SourceDataLine gespeichert wird. <br><br>  <b>Machen wir uns bereit, das SourceDataLine-Objekt zu verwenden</b> <br><br>  Sobald wir eine Instanz des SourceDataLine-Objekts erhalten haben, m√ºssen wir es f√ºr das √ñffnen und Ausf√ºhren vorbereiten, wie in Listing 6 gezeigt. <br><br><pre> <code class="java hljs"> sourceDataLine.open(audioFormat); sourceDataLine.start();</code> </pre><br>  <i>Listing 6:</i> <br><br>  <b>√ñffnungsmethode</b> <br><br>  Wie Sie in Listing 6 sehen k√∂nnen, haben wir das AudioFormat-Objekt an die √ñffnungsmethode f√ºr das SourceDataLine-Objekt gesendet. <br><br>  Laut Sun ist dies eine Methode: <br><br>  <i>"√ñffnet eine Leitung (Kanal) mit einem zuvor definierten Format, sodass er alle Systemressourcen empfangen kann, die er ben√∂tigt, und sich in einem funktionierenden Zustand befindet."</i> <br><br>  <b>Erkennungsstatus</b> <br><br>  Es gibt wenig mehr, was Sun in diesem Thread √ºber ihn schreibt. <br><br>  <i>‚ÄûDas √ñffnen und Schlie√üen des Kanals wirkt sich auf die Verteilung der Systemressourcen aus.</i>  <i>Durch das erfolgreiche √ñffnen des Kanals wird sichergestellt, dass dem Kanal alle erforderlichen Ressourcen zur Verf√ºgung gestellt werden.</i> <i><br><br></i>  <i>Das √ñffnen des Mischpults, dessen Eingangs- und Ausgangsanschl√ºsse f√ºr Audiodaten vorhanden sind, umfasst unter anderem die Verwendung der Hardware der Plattform, auf der die Arbeit und Initialisierung der erforderlichen Softwarekomponenten stattfindet.</i> <i><br><br></i>  <i>Das √ñffnen eines Kanals, der eine Route f√ºr Audiodaten zu oder von einem Mixer darstellt, umfasst sowohl das Initialisieren als auch das Empfangen keineswegs unbegrenzter Mixerressourcen.</i>  <i>Mit anderen Worten, der Mischer hat eine begrenzte Anzahl von Kan√§len, sodass mehrere Anwendungen mit ihren eigenen Kanalanforderungen (und manchmal sogar eine Anwendung) die Mischerressourcen korrekt teilen m√ºssen. ‚Äú</i> <br><br>  <b>Rufen Sie die Startmethode auf einem Kanal auf</b> <br><br>  Laut Sun bedeutet das Aufrufen der Startmethode f√ºr einen Kanal Folgendes: <br><br>  <i>‚ÄûDer Kanal darf E / A-Leitungen verwenden.</i>  <i>Wenn versucht wird, eine bereits funktionierende Leitung zu verwenden, f√ºhrt die Methode nichts aus.</i>  <i>Nachdem der Datenpuffer leer ist, wird die Zeile mit dem Starten der E / A fortgesetzt, beginnend mit dem ersten Frame, den sie nach dem vollst√§ndigen Laden des Puffers nicht verarbeiten konnte. ‚Äú</i> <br><br>  In unserem Fall hat der Kanal nat√ºrlich nicht aufgeh√∂rt.  Seit wir es zum ersten Mal gestartet haben. <br><br>  <b>Jetzt haben wir fast alles was wir brauchen</b> <br><br>  Zu diesem Zeitpunkt haben wir alle Audioressourcen erhalten, die wir zur Wiedergabe der Audiodaten ben√∂tigen, die wir zuvor aufgezeichnet und in der ByteArrayOutputStream-Objektinstanz gespeichert haben.  (Denken Sie daran, dass dieses Objekt nur im RAM des Computers vorhanden ist.) <br><br>  <b>Wir beginnen zu flie√üen</b> <br><br>  Wir werden den Stream erstellen und starten, um das Audio abzuspielen.  Der Code in Listing 7 erstellt und startet diesen Thread. <br><br>  (Verwechseln Sie den Aufruf der Startmethode in diesem Thread nicht mit dem Aufruf der Startmethode im SourceDataLine-Objekt aus Listing 6. Dies sind v√∂llig unterschiedliche Vorg√§nge.) <br><br><pre> <code class="java hljs">Thread playThread = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Thread(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> PlayThread()); playThread.start(); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (Exception e) { System.out.println(e); System.exit(<span class="hljs-number"><span class="hljs-number">0</span></span>); }<span class="hljs-comment"><span class="hljs-comment">//end catch }//end playAudio</span></span></code> </pre><br>  <i>Listing 7:</i> <br><br>  <b>Unpr√§tenti√∂ser Code</b> <br><br>  Der Ausschnitt des Programms in Listing 7 ist zwar sehr einfach, zeigt jedoch ein Beispiel f√ºr die Multithread-Programmierung in Java.  Wenn Sie es nicht verstehen, sollten Sie sich mit diesem Thema in speziellen Themen zum Erlernen von Java vertraut machen. <br><br>  Sobald der Stream gestartet ist, funktioniert er, bis alle aufgezeichneten Audiodaten bis zum Ende abgespielt wurden. <br><br>  <b>Neues Thread-Objekt</b> <br><br>  Der Code in Listing 7 erstellt eine Instanz des Thread-Objekts der PlayThread-Klasse.  Diese Klasse ist in unserem Programm als innere Klasse definiert.  Ihre Beschreibung beginnt in Listing 8. <br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PlayThread</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Thread</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> tempBuffer[] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[<span class="hljs-number"><span class="hljs-number">10000</span></span>];</code> </pre><br>  <i>Listing 8:</i> <br><br>  <b>Die Ausf√ºhrungsmethode in der Thread-Klasse</b> <br><br>  Mit Ausnahme der Deklaration einer tempBuffer-Variablen (die sich auf ein Array von Bytes bezieht) ist eine vollst√§ndige Definition dieser Klasse nur eine Definition der Ausf√ºhrungsmethode.  Wie Sie bereits wissen sollten, f√ºhrt das Aufrufen der Startmethode f√ºr ein Thread-Objekt dazu, dass die Ausf√ºhrungsmethode dieses Objekts ausgef√ºhrt wird <br><br>  Die Ausf√ºhrungsmethode f√ºr diesen Thread beginnt in Listing 9. <br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> cnt; <span class="hljs-comment"><span class="hljs-comment">//  //    -1 // while((cnt = audioInputStream. read(tempBuffer, 0, tempBuffer.length)) != -1){ if(cnt &gt; 0){ //   //    //    //   . sourceDataLine.write( tempBuffer, 0, cnt); }//end if }//end while</span></span></code> </pre><br>  <i>Listing 9:</i> <br><br>  <b>Der erste Teil des Programmfragments in der Ausf√ºhrungsmethode</b> <br><br>  Die Ausf√ºhrungsmethode enth√§lt zwei wichtige Teile, von denen der erste in Listing 9 dargestellt ist. <br><br>  Insgesamt wird hier eine Schleife verwendet, um Audiodaten aus einem AudioInputStream zu lesen und an eine SourceDataLine zu √ºbergeben. <br><br>  An das SourceDataLine-Objekt gesendete Daten werden automatisch an die Standard-Audioausgabe √ºbertragen.  Es kann sich um einen eingebauten Computerlautsprecher oder einen Leitungsausgang handeln.  (In den folgenden Lektionen lernen wir, die erforderlichen Audioger√§te zu ermitteln.)  Die cnt-Variable und der tempBuffer-Datenpuffer werden verwendet, um den Datenfluss zwischen Lese- und Schreibvorg√§ngen zu steuern. <br><br>  <b>Lesen von Daten aus AudioInputStream</b> <br><br>  Der Lesezyklus vom AudioInputStream-Objekt liest die angegebene maximale Anzahl von Datenbytes aus dem AudioInputStream und platziert deren Bytearray. <br><br>  <b>R√ºckgabewert</b> <br><br>  Ferner gibt diese Methode die Gesamtzahl der gelesenen Bytes oder -1 zur√ºck, wenn das Ende der aufgezeichneten Sequenz erreicht wurde.  Die Anzahl der gelesenen Bytes wird in der Variablen cnt gespeichert. <br><br>  <b>SourceDataLine-Schreibschleife</b> <br><br>  Wenn die Anzahl der gelesenen Bytes gr√∂√üer als Null ist, erfolgt ein √úbergang zum Zyklus des Schreibens von Daten in SourceDataLine.  In dieser Schleife gelangen die Audiodaten in den Mixer.  Bytes werden gem√§√ü ihren Indizes aus dem Bytearray gelesen und in den Kanalpuffer geschrieben. <br><br>  <b>Wenn der Eingangsstrom trocknet</b> <br><br>  Wenn die Leseschleife -1 zur√ºckgibt, bedeutet dies, dass alle zuvor aufgezeichneten Audiodaten beendet wurden und die weitere Steuerung an das Programmfragment in Listing 10 √ºbergeben wird. <br><br><pre> <code class="java hljs"> sourceDataLine.drain(); sourceDataLine.close(); }<span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (Exception e) { System.out.println(e); System.exit(<span class="hljs-number"><span class="hljs-number">0</span></span>); }<span class="hljs-comment"><span class="hljs-comment">//end catch }//end run }//   PlayThread</span></span></code> </pre><br>  <i>Listing 10:</i> <br><br>  <b>Sperren und warten</b> <br><br>  Der Code in Listing 10 ruft die Drain-Methode f√ºr das SourceDataLine-Objekt auf, damit das Programm blockieren und warten kann, bis der interne Puffer in SourceDataLine leer ist.  Wenn der Puffer leer ist, bedeutet dies, dass der gesamte n√§chste Teil an die Tonausgabe des Computers geliefert wird. <br><br>  <b>SourceDataLine schlie√üen</b> <br><br>  Anschlie√üend ruft das Programm die Methode close auf, um den Kanal zu schlie√üen, wodurch angezeigt wird, dass alle vom Kanal verwendeten Systemressourcen jetzt frei sind.  Sun meldet die folgende Kanalschlie√üung: <br><br>  <i>‚ÄûDas Schlie√üen des Kanals signalisiert, dass alle f√ºr diesen Kanal beteiligten Ressourcen freigegeben werden k√∂nnen.</i>  <i>Um Ressourcen freizugeben, muss die Anwendung die Kan√§le schlie√üen, unabh√§ngig davon, ob sie bereits beteiligt sind oder nicht, sowie wenn die Anwendung endet.</i>  <i>Es wird davon ausgegangen, dass Mischer Systemressourcen gemeinsam nutzen und wiederholt geschlossen und ge√∂ffnet werden k√∂nnen.</i>  <i>Andere Kan√§le unterst√ºtzen m√∂glicherweise die Wiederer√∂ffnung, nachdem sie geschlossen wurden.</i>  <i>Im Allgemeinen variieren die Mechanismen zum √ñffnen von Linien je nach Subtyp. ‚Äú</i> <br><br>  <b>Und jetzt das Ende der Geschichte</b> <br><br>  Daher haben wir hier erkl√§rt, wie unser Programm die Java Sound API verwendet, um die √úbertragung von Audiodaten aus dem internen Speicher des Computers auf die Soundkarte sicherzustellen. <br><br>  <b>F√ºhren Sie das Programm aus</b> <br><br>  Jetzt k√∂nnen Sie das Programm aus Listing 11 kompilieren und ausf√ºhren, was das Ende unserer Lektion kr√∂nt. <br><br>  <b>Erfassen und Abspielen von Audiodaten</b> <br><br>  Das Programm demonstriert die F√§higkeit, Daten von einem Mikrofon aufzunehmen und √ºber die Soundkarte Ihres Computers abzuspielen.  Anweisungen zur Verwendung sind sehr einfach. <br><br>  F√ºhren Sie das Programm aus.  Die einfache GUI GUI, die in Abbildung 6 dargestellt ist, sollte auf dem Bildschirm angezeigt werden. <br><br><img src="https://habrastorage.org/webt/lf/7l/ew/lf7lew65yqcqstjqvdvmptmhevy.gif"><br><br><ul><li>  Klicken Sie auf die Schaltfl√§che Aufnehmen und nehmen Sie alle T√∂ne auf dem Mikrofon auf. </li><li>  Klicken Sie auf die Schaltfl√§che Stopp, um die Aufnahme zu beenden. </li><li>  Klicken Sie auf die Schaltfl√§che Wiedergabe, um die Aufnahme √ºber die Tonausgabe Ihres Computers abzuspielen. </li></ul><br>  Wenn Sie nichts h√∂ren, erh√∂hen Sie die Mikrofonempfindlichkeit oder die Lautsprecherlautst√§rke. <br><br>  Das Programm speichert eine Aufzeichnung im Speicher des Computers. Seien Sie also vorsichtig.  Wenn Sie versuchen, zu viele Audiodaten zu speichern, ist m√∂glicherweise nicht mehr gen√ºgend Arbeitsspeicher vorhanden. <br><br>  <b>Fazit</b> <br><br><ul><li>  Wir haben herausgefunden, dass die Java Sound API auf dem Konzept von Kan√§len und Mixern basiert. </li><li>  Wir haben die ersten Informationen √ºber die physikalischen und elektrischen Eigenschaften des analogen Klangs erhalten, um dann das Ger√§t des Audiomischers zu verstehen. </li><li>  Wir haben ein Amateur-Rockkonzertszenario mit sechs Mikrofonen und zwei Stereolautsprechern verwendet, um die M√∂glichkeit der Verwendung eines Audiomischers zu beschreiben. </li><li>  Wir haben eine Reihe von Java Sound-Programmierthemen besprochen, darunter Mixer, Kan√§le, Datenformat und mehr. </li><li>  Wir haben die allgemeine Beziehung zwischen den Objekten SourceDataLine, Clip, Mixer, AudioFormat und Ports in einem einfachen Programm zur Ausgabe von Audiodaten erl√§utert. </li><li>  Wir haben ein Programm kennengelernt, mit dem wir zun√§chst Audiodaten aufnehmen und dann wiedergeben k√∂nnen. </li><li>  Wir haben eine detaillierte Erkl√§rung des Codes erhalten, der zum Abspielen von Audiodaten verwendet wird, die zuvor im Speicher des Computers aufgezeichnet wurden. </li></ul><br>  <b>Was weiter?</b> <br><br>  In diesem Tutorial haben wir herausgefunden, dass die Java Sound API auf dem Konzept von Mixern und Kan√§len basiert.  Der besprochene Code enthielt jedoch keine expliziten Mixer.  Die AudioSystem-Klasse stellte uns statische Methoden zur Verf√ºgung, mit denen Audioverarbeitungsprogramme geschrieben werden k√∂nnen, ohne direkt auf die Mixer zuzugreifen.  Mit anderen Worten, diese statischen Methoden nehmen uns Mischer weg. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In der n√§chsten Lektion pr√§sentieren wir einen modifizierten Datenerfassungscode im Vergleich zu dem in dieser Lektion vorgestellten. </font><font style="vertical-align: inherit;">In der neuen Version werden explizit Mixer verwendet, um Ihnen zu zeigen, wie Sie sie verwenden k√∂nnen, wenn Sie sie wirklich ben√∂tigen.</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> javax.swing.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.awt.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.awt.event.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.io.*; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> javax.sound.sampled.*; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AudioCapture01</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">JFrame</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">boolean</span></span> stopCapture = <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>; ByteArrayOutputStream byteArrayOutputStream; AudioFormat audioFormat; TargetDataLine targetDataLine; AudioInputStream audioInputStream; SourceDataLine sourceDataLine; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( String args[])</span></span></span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> AudioCapture01(); }<span class="hljs-comment"><span class="hljs-comment">//end main public AudioCapture01(){ final JButton captureBtn = new JButton("Capture"); final JButton stopBtn = new JButton("Stop"); final JButton playBtn = new JButton("Playback"); captureBtn.setEnabled(true); stopBtn.setEnabled(false); playBtn.setEnabled(false); captureBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ captureBtn.setEnabled(false); stopBtn.setEnabled(true); playBtn.setEnabled(false); //  //   //   Stop captureAudio(); } } ); getContentPane().add(captureBtn); stopBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ captureBtn.setEnabled(true); stopBtn.setEnabled(false); playBtn.setEnabled(true); //  //    stopCapture = true; } } ); getContentPane().add(stopBtn); playBtn.addActionListener( new ActionListener(){ public void actionPerformed( ActionEvent e){ //  //    playAudio(); } } ); getContentPane().add(playBtn); getContentPane().setLayout( new FlowLayout()); setTitle("Capture/Playback Demo"); setDefaultCloseOperation( EXIT_ON_CLOSE); setSize(250,70); setVisible(true); } //    //     //   ByteArrayOutputStream private void captureAudio(){ try{ //    audioFormat = getAudioFormat(); DataLine.Info dataLineInfo = new DataLine.Info( TargetDataLine.class, audioFormat); targetDataLine = (TargetDataLine) AudioSystem.getLine( dataLineInfo); targetDataLine.open(audioFormat); targetDataLine.start(); //     //    //   //    Thread captureThread = new Thread( new CaptureThread()); captureThread.start(); } catch (Exception e) { System.out.println(e); System.exit(0); } } //    // ,    //  ByteArrayOutputStream private void playAudio() { try{ //  //  byte audioData[] = byteArrayOutputStream. toByteArray(); InputStream byteArrayInputStream = new ByteArrayInputStream( audioData); AudioFormat audioFormat = getAudioFormat(); audioInputStream = new AudioInputStream( byteArrayInputStream, audioFormat, audioData.length/audioFormat. getFrameSize()); DataLine.Info dataLineInfo = new DataLine.Info( SourceDataLine.class, audioFormat); sourceDataLine = (SourceDataLine) AudioSystem.getLine( dataLineInfo); sourceDataLine.open(audioFormat); sourceDataLine.start(); //    //     //     //      Thread playThread = new Thread(new PlayThread()); playThread.start(); } catch (Exception e) { System.out.println(e); System.exit(0); } } //     //  AudioFormat private AudioFormat getAudioFormat(){ float sampleRate = 8000.0F; //8000,11025,16000,22050,44100 int sampleSizeInBits = 16; //8,16 int channels = 1; //1,2 boolean signed = true; //true,false boolean bigEndian = false; //true,false return new AudioFormat( sampleRate, sampleSizeInBits, channels, signed, bigEndian); } //===================================// //    //    class CaptureThread extends Thread{ byte tempBuffer[] = new byte[10000]; public void run(){ byteArrayOutputStream = new ByteArrayOutputStream(); stopCapture = false; try{ while(!stopCapture){ int cnt = targetDataLine.read( tempBuffer, 0, tempBuffer.length); if(cnt &gt; 0){ //     byteArrayOutputStream.write( tempBuffer, 0, cnt); } } byteArrayOutputStream.close(); }catch (Exception e) { System.out.println(e); System.exit(0); } } } //===================================// //   //     class PlayThread extends Thread{ byte tempBuffer[] = new byte[10000]; public void run(){ try{ int cnt; //     -1 while((cnt = audioInputStream. read(tempBuffer, 0, tempBuffer.length)) != -1){ if(cnt &gt; 0){ //    //   //    //    sourceDataLine.write( tempBuffer, 0, cnt); } } sourceDataLine.drain(); sourceDataLine.close(); }catch (Exception e) { System.out.println(e); System.exit(0); } } } //===================================// }//end outer class AudioCapture01.java</span></span></code> </pre><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Listing 11:</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434424/">https://habr.com/ru/post/de434424/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434412/index.html">Raketenentfernung vor dem Start auf Vostochny</a></li>
<li><a href="../de434414/index.html">Eisberg</a></li>
<li><a href="../de434416/index.html">Lesen Sie im Urlaub. Die besten Beitr√§ge in unserem Blog f√ºr 2018</a></li>
<li><a href="../de434418/index.html">Schneller, lauter, heller: Physik der Paarung von Kolibris "tanzen"</a></li>
<li><a href="../de434422/index.html">Unrentable Dinge</a></li>
<li><a href="../de434426/index.html">Checkliste: Einreichen von Berichten √ºber das vereinfachte Steuersystem f√ºr 2018</a></li>
<li><a href="../de434428/index.html">Wir montieren, reparieren und tragen eine Vintage Digitaluhr</a></li>
<li><a href="../de434430/index.html">IBM zeigte einen analogen 8-Bit-Phasenwechsel-Speicherchip</a></li>
<li><a href="../de434432/index.html">Viya, Vaya, Vaya, Vaya - ‚Äû√úbersetzungsschwierigkeiten‚Äú oder was hinter der neuen Plattform SAS Viya (Vaya) steckt</a></li>
<li><a href="../de434440/index.html">[Video] Kriegsschiffe, Bots und Geldschie√üen auf Servern</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>