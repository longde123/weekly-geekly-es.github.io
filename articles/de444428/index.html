<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧙🏼 👨🏿‍⚕️ 🐐 Bergauto: Die klassische Herausforderung mit Verstärkungstraining lösen 🧘🏾 👔 👩🏼‍🎨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In der Regel werden Änderungen an Algorithmen, die auf den spezifischen Merkmalen einer bestimmten Aufgabe beruhen, als weniger wertvoll angesehen, da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bergauto: Die klassische Herausforderung mit Verstärkungstraining lösen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/hsespb/blog/444428/">  In der Regel werden Änderungen an Algorithmen, die auf den spezifischen Merkmalen einer bestimmten Aufgabe beruhen, als weniger wertvoll angesehen, da sie sich nur schwer auf eine breitere Klasse von Problemen übertragen lassen.  Dies bedeutet jedoch nicht, dass solche Änderungen nicht erforderlich sind.  Darüber hinaus können sie häufig das Ergebnis selbst bei einfachen klassischen Problemen erheblich verbessern, was für die praktische Anwendung von Algorithmen sehr wichtig ist.  In diesem Beitrag werde ich beispielsweise das Mountain Car-Problem mit einem Verstärkungstraining lösen und zeigen, dass es mit dem Wissen über die Organisation der Aufgabe viel schneller gelöst werden kann. <br><br><img src="https://habrastorage.org/webt/xy/ju/ai/xyjuaivxj9j2c5hp2o-2x3cem2y.png"><br><a name="habracut"></a><br><h2>  Über mich </h2><br>  Mein Name ist Oleg Svidchenko, jetzt studiere ich an der Fakultät für Physik, Mathematik und Informatik der HSE in St. Petersburg, bevor ich drei Jahre an der Universität in St. Petersburg studierte.  Ich arbeite auch als Forscher bei JetBrains Research.  Bevor ich an die Universität kam, studierte ich am SSC der Moskauer Staatlichen Universität und wurde als Teil des Moskauer Teams der Gewinner der Allrussischen Olympiade der Schüler der Informatik. <br><br><h2>  Was brauchen wir </h2><br>  Wenn Sie an einem Verstärkungstraining interessiert sind, ist die Mountain Car-Herausforderung genau das Richtige für Sie.  Heute benötigen wir Python mit den installierten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gym-</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PyTorch-Bibliotheken</a> sowie Grundkenntnisse über neuronale Netze. <br><br><h2>  Aufgabenbeschreibung </h2><br>  In einer zweidimensionalen Welt muss ein Auto von der Mulde zwischen zwei Hügeln auf die Spitze des rechten Hügels klettern.  Es wird durch die Tatsache kompliziert, dass sie nicht genug Motorleistung hat, um die Schwerkraft zu überwinden und beim ersten Versuch dort einzutreten.  Wir sind eingeladen, einen Agenten (in unserem Fall ein neuronales Netzwerk) auszubilden, der durch Steuerung so schnell wie möglich den rechten Hügel erklimmen kann. <br><br>  Die Maschinensteuerung erfolgt durch Interaktion mit der Umgebung.  Es ist in unabhängige Episoden unterteilt und jede Episode wird Schritt für Schritt ausgeführt.  Bei jedem Schritt empfängt der Agent den Status <i>s</i> und die Umgebung <i>r</i> von der Umgebung als Antwort auf die Aktion <i>a</i> .  Außerdem meldet das Medium manchmal zusätzlich, dass die Episode beendet ist.  In diesem Problem ist <i>s</i> ein Zahlenpaar, von dem die erste die Position des Autos auf der Kurve ist (eine Koordinate reicht aus, da wir uns nicht von der Oberfläche losreißen können), und die zweite ist die Geschwindigkeit auf der Oberfläche (mit einem Vorzeichen).  Die Belohnung <i>r</i> ist eine Zahl, die für diese Aufgabe immer gleich -1 ist.  Auf diese Weise ermutigen wir den Agenten, die Episode so schnell wie möglich abzuschließen.  Es gibt nur drei mögliche Aktionen: Schieben Sie das Auto nach links, tun Sie nichts und schieben Sie das Auto nach rechts.  Diese Aktionen entsprechen Zahlen von 0 bis 2. Die Episode kann enden, wenn das Auto die Spitze des rechten Hügels erreicht oder wenn der Agent 200 Schritte unternommen hat. <br><br><h2>  Ein bisschen Theorie </h2><br>  Auf Habré gab es bereits einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel über DQN,</a> in dem der Autor alle notwendigen Theorien ziemlich gut beschrieb.  Um das Lesen zu erleichtern, werde ich es hier in einer formelleren Form wiederholen. <br><br>  Die Verstärkungslernaufgabe wird durch einen Satz von Zustandsraum S, Aktionsraum A, Koeffizient definiert <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.237ex" height="1.817ex" viewBox="0 -520.7 3546.5 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="730" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="1260" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="2138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="3017" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ gamma </script>  Im Allgemeinen können die Übergangsfunktion und die Belohnungsfunktion Zufallsvariablen sein, aber jetzt betrachten wir eine einfachere Version, in der sie eindeutig definiert sind.  Ziel ist es, die kumulierten Belohnungen zu maximieren. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.676ex" height="3.021ex" viewBox="0 -883.9 10193.7 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-54" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-30" x="1140" y="0"></use></g></g><g transform="translate(3430,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="638" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-63" x="4487" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-64" x="4921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="5444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="5930" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="6541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="7022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="7551" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="8430" y="0"></use><g transform="translate(9308,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="748" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ sum_ {t = 0} ^ {T} r_ {t} \ cdot \ gamma ^ {t} </script>  Dabei ist t die Schrittnummer im Medium und T die Anzahl der Schritte in der Episode. <br><br>  Um dieses Problem zu lösen, definieren wir die Wertfunktion V des Zustands s als den Wert der maximalen kumulativen Belohnung, vorausgesetzt, wir beginnen im Zustand s.  Wenn wir eine solche Funktion kennen, können wir das Problem einfach lösen, indem wir bei jedem Schritt s mit dem maximal möglichen Wert übergeben.  Es ist jedoch nicht alles so einfach: In den meisten Fällen wissen wir nicht, welche Aktion uns in den gewünschten Zustand bringt.  Daher fügen wir die Aktion a als zweiten Parameter der Funktion hinzu.  Die resultierende Funktion wird als Q-Funktion bezeichnet.  Es zeigt, welche maximal mögliche kumulative Belohnung wir erhalten können, wenn wir die Aktion a in state s ausführen.  Aber wir können diese Funktion bereits verwenden, um das Problem zu lösen: Wenn wir uns im Zustand s befinden, wählen wir einfach a so, dass Q (s, a) maximal ist. <br><br>  In der Praxis kennen wir die reale Q-Funktion nicht, können sie aber mit verschiedenen Methoden approximieren.  Eine solche Technik ist das Deep Q Network (DQN).  Seine Idee ist, dass wir für jede der Aktionen die Q-Funktion unter Verwendung eines neuronalen Netzwerks approximieren. <br><br><h2>  Die Umwelt </h2><br>  Jetzt lass uns üben.  Zunächst müssen wir lernen, wie die MountainCar-Umgebung emuliert wird.  Die Turnhallenbibliothek, die eine große Anzahl von Standard-Lernumgebungen zur Verstärkung bietet, wird uns bei der Bewältigung dieser Aufgabe helfen.  Um eine Umgebung zu erstellen, müssen wir die make-Methode im Fitness-Studio-Modul aufrufen und den Namen der gewünschten Umgebung als Parameter übergeben: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym env = gym.make(<span class="hljs-string"><span class="hljs-string">"MountainCar-v0"</span></span>)</code> </pre> <br>  Eine ausführliche Dokumentation finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und eine Beschreibung der Umgebung finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br>  Lassen Sie uns genauer betrachten, was wir mit der von uns geschaffenen Umgebung tun können: <br><br><ul><li>  <code>env.reset()</code> - beendet die aktuelle Episode und startet eine neue.  Gibt den Ausgangszustand zurück. </li><li>  <code>env.step(action)</code> - führt die angegebene Aktion aus.  Gibt einen neuen Status, eine Belohnung, ob die Episode beendet wurde und zusätzliche Informationen zurück, die zum Debuggen verwendet werden können. </li><li>  <code>env.seed(seed)</code> - setzt zufälligen Samen.  Dies hängt davon ab, wie die Anfangszustände während env.reset () generiert werden. </li><li>  <code>env.render()</code> - <code>env.render()</code> den aktuellen Status der Umgebung an. </li></ul><br><h2>  Wir realisieren DQN </h2><br>  DQN ist ein Algorithmus, der ein neuronales Netzwerk verwendet, um eine Q-Funktion zu bewerten.  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ursprünglichen Artikel</a> definierte DeepMind die Standardarchitektur für Atari-Spiele unter Verwendung von Faltungs-Neuronalen Netzen.  Im Gegensatz zu diesen Spielen verwendet Mountain Car das Bild nicht als Status, daher müssen wir die Architektur selbst bestimmen. <br><br>  Nehmen wir zum Beispiel eine Architektur mit zwei versteckten Schichten von jeweils 32 Neuronen.  Nach jeder verborgenen Ebene verwenden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ReLU</a> als Aktivierungsfunktion.  Zwei Zahlen, die den Zustand beschreiben, werden dem Eingang des neuronalen Netzwerks zugeführt, und am Ausgang erhalten wir eine Schätzung der Q-Funktion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/jl/mk/aejlmktkosv-jpbne9hqi96enxw.png" alt="Neuronale Netzwerkarchitektur"></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights)</span></span></code> </pre><br>  Da wir das neuronale Netzwerk auf der GPU trainieren werden, müssen wir unser Netzwerk dort laden: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     CPU,  “cuda”    “cpu” device = torch.device("cuda") model.to(device) target_model.to(device)</span></span></code> </pre><br>  Die Gerätevariable ist global, da wir auch die Daten laden müssen. <br><br>  Wir müssen auch einen Optimierer definieren, der die Modellgewichte mithilfe des Gradientenabfalls aktualisiert.  Ja, es gibt viel mehr als eine. <br><br><pre> <code class="python hljs">optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.00003</span></span>)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch device = torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_new_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights) #   ,     (GPU  CPU) model.to(device) target_model.to(device) #  ,        optimizer = optim.Adam(model.parameters(), lr=0.00003) return model, target_model, optimizer</span></span></code> </pre><br></div></div><br>  Deklarieren Sie nun eine Funktion, die die Fehlerfunktion und den Gradienten entlang berücksichtigt, und wenden Sie den Abstieg an.  Zuvor müssen Sie jedoch Daten aus dem Stapel auf die GPU herunterladen: <br><br><pre> <code class="python hljs">state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device)</span></span></code> </pre><br>  Als nächstes müssen wir die realen Werte der Q-Funktion berechnen. Da wir sie jedoch nicht kennen, werden wir sie anhand der Werte für den folgenden Zustand bewerten: <br><br><pre> <code class="python hljs">target_q = torch.zeros(reward.size()[<span class="hljs-number"><span class="hljs-number">0</span></span>]).float().to(device) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-comment"><span class="hljs-comment">#     Q-function    target_q = target_model(next_state).max(1)[0].view(-1) target_q[done] = 0 target_q = reward + target_q * gamma</span></span></code> </pre><br>  Und die aktuelle Vorhersage: <br><br><pre> <code class="python hljs">q = model(state).gather(<span class="hljs-number"><span class="hljs-number">1</span></span>, action.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre><br>  Mit target_q und q berechnen wir die Verlustfunktion und aktualisieren das Modell: <br><br><pre> <code class="python hljs">loss = F.smooth_l1_loss(q, target_q.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-comment"><span class="hljs-comment">#      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs">gamma = <span class="hljs-number"><span class="hljs-number">0.99</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch, model, target_model, optimizer)</span></span></span><span class="hljs-function">:</span></span> state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device) #  ,       target_q = torch.zeros(reward.size()[0]).float().to(device) with torch.no_grad(): #     Q-function    target_q = target_model(next_state).max(1)[0].view(-1) target_q[done] = 0 target_q = reward + target_q * gamma #   q = model(state).gather(1, action.unsqueeze(1)) loss = F.smooth_l1_loss(q, target_q.unsqueeze(1)) #      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br></div></div><br>  Da das Modell nur die Q-Funktion berücksichtigt und keine Aktionen ausführt, müssen wir die Funktion bestimmen, die entscheidet, welche Aktionen der Agent ausführen wird.  Als Entscheidungsalgorithmus nehmen wir <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ varepsilon </script>  -grüne Politik.  Ihre Idee ist, dass der Agent normalerweise gierig Aktionen ausführt und das Maximum der Q-Funktion wählt, aber mit Wahrscheinlichkeit <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ varepsilon </script>  Er wird eine zufällige Aktion ausführen.  Es sind zufällige Aktionen erforderlich, damit der Algorithmus die Aktionen untersuchen kann, die er nicht ausgeführt hätte, wenn er nur von einer gierigen Richtlinie geleitet würde. Dieser Prozess wird als Exploration bezeichnet. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">select_action</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(state, epsilon, model)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random.random() &lt; epsilon: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model(torch.tensor(state).to(device).float().unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>))[<span class="hljs-number"><span class="hljs-number">0</span></span>].max(<span class="hljs-number"><span class="hljs-number">0</span></span>)[<span class="hljs-number"><span class="hljs-number">1</span></span>].view(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>).item()</code> </pre><br>  Da wir Stapel verwenden, um das neuronale Netzwerk zu trainieren, benötigen wir einen Puffer, in dem wir die Erfahrung der Interaktion mit der Umgebung speichern und aus dem wir Stapel auswählen: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Memory</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, capacity)</span></span></span><span class="hljs-function">:</span></span> self.capacity = capacity self.memory = [] self.position = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">push</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, element)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(self.memory) &lt; self.capacity: self.memory.append(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) self.memory[self.position] = element self.position = (self.position + <span class="hljs-number"><span class="hljs-number">1</span></span>) % self.capacity <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, batch_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list(zip(*random.sample(self.memory, batch_size))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.memory)</code> </pre><br><h2>  Naive Entscheidung </h2><br>  Deklarieren Sie zunächst die Konstanten, die wir im Lernprozess verwenden werden, und erstellen Sie ein Modell: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 #    memory = Memory(5000) model, target_model, optimizer = create_new_model()</span></span></code> </pre><br>  Trotz der Tatsache, dass es logisch wäre, den Interaktionsprozess in Episoden zu unterteilen, ist es für uns bequemer, den Lernprozess in separate Schritte zu unterteilen, da wir nach jedem Schritt der Umgebung einen Schritt des Gradientenabfalls machen möchten. <br><br>  Lassen Sie uns genauer darüber sprechen, wie ein Lernschritt hier aussieht.  Wir gehen davon aus, dass wir jetzt einen Schritt mit der Schrittanzahl der max_steps-Schritte und dem aktuellen Status machen.  Dann mache die Aktion mit <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ varepsilon </script>  -grüne Richtlinien würden so aussehen: <br><br><pre> <code class="python hljs">epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action)</code> </pre><br>  Fügen Sie die gesammelten Erfahrungen sofort in das Gedächtnis ein und starten Sie eine neue Episode, wenn die aktuelle beendet ist: <br><br><pre> <code class="python hljs">memory.push((state, action, reward, new_state, done)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> done: state = env.reset() done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: state = new_state</code> </pre><br>  Und wir werden den Schritt des Gradientenabstiegs machen (wenn wir natürlich bereits mindestens eine Charge sammeln können): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer)</code> </pre><br>  Jetzt muss noch target_model aktualisiert werden: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model)</code> </pre><br>  Wir möchten aber auch den Lernprozess verfolgen.  Zu diesem Zweck spielen wir nach jedem Update von target_model mit epsilon = 0 eine zusätzliche Episode ab, in der die Gesamtprämie im Puffer belohnt_by_target_updates gespeichert wird: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model) state = env.reset() total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> done: action = select_action(state, <span class="hljs-number"><span class="hljs-number">0</span></span>, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> state = env.reset() rewards_by_target_updates.append(total_reward)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 def fit(): #    memory = Memory(5000) model, target_model, optimizer = create_new_model() for step in range(max_steps): #    epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action) #  ,  ,   memory.push((state, action, reward, new_state, done)) if done: state = env.reset() done = False else: state = new_state #  if step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer) if step % target_update == 0: target_model = copy.deepcopy(model) #Exploitation state = env.reset() total_reward = 0 while not done: action = select_action(state, 0, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = False state = env.reset() rewards_by_target_updates.append(total_reward) return rewards_by_target_updates</span></span></code> </pre><br></div></div><br>  Führen Sie diesen Code aus und erhalten Sie so etwas wie dieses Diagramm: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e35/952/b37/e35952b375fc831ec4bd405303440509.png" alt="Basisliniendiagramm in Form einer geraden Linie y = -200"><br><br><h2>  Was ist schief gelaufen? </h2><br>  Ist das ein Fehler?  Ist das der falsche Algorithmus?  Sind das schlechte Parameter?  Nicht wirklich.  Tatsächlich liegt das Problem in der Aufgabe, nämlich in der Funktion der Belohnung.  Schauen wir es uns genauer an.  Bei jedem Schritt erhält unser Agent eine Belohnung von -1, und dies geschieht bis zum Ende der Episode.  Eine solche Belohnung motiviert den Agenten, die Episode so schnell wie möglich zu beenden, sagt ihm aber gleichzeitig nicht, wie es geht.  Aus diesem Grund besteht die einzige Möglichkeit zu lernen, wie ein Problem in einer solchen Formulierung für einen Agenten gelöst werden kann, darin, es viele Male mithilfe von Exploration zu lösen. <br><br>  Natürlich könnte man versuchen, komplexere Algorithmen zu verwenden, um die Umgebung zu untersuchen, anstatt unsere <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ varepsilon </script>  -grüne Richtlinien.  Zum einen wird unser Modell jedoch aufgrund ihrer Anwendung komplexer, was wir vermeiden möchten, und zum anderen nicht, dass sie für diese Aufgabe gut genug funktionieren.  Stattdessen können wir die Ursache des Problems beseitigen, indem wir die Aufgabe selbst modifizieren, nämlich indem wir die Belohnungsfunktion ändern, d. H.  durch Anwenden der sogenannten Belohnungsformung. <br><br><h2>  Beschleunigung der Konvergenz </h2><br>  Unser intuitives Wissen sagt uns, dass Sie beschleunigen müssen, um den Berg hinaufzufahren.  Je höher die Geschwindigkeit, desto näher ist der Agent an der Lösung des Problems.  Sie können ihm dies beispielsweise mitteilen, indem Sie der Belohnung ein Geschwindigkeitsmodul mit einem bestimmten Koeffizienten hinzufügen: <pre>  modifizierter_Reward = Belohnung + 10 * abs (neuer_Zustand [1]) </pre><br><br>  Dementsprechend passt eine Linie in die Funktion <pre>  memory.push ((Status, Aktion, Belohnung, neuer_Zustand, erledigt)) </pre>  sollte ersetzt werden durch <pre>  memory.push ((Status, Aktion, modifizierter_Reward, neuer_Zustand, erledigt)) </pre>  Schauen wir uns nun das neue Diagramm an (es präsentiert die <b>ursprüngliche</b> Auszeichnung ohne Änderungen): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c19/3c2/925/c193c2925d13ba1977cf525697cba1c2.png" alt="Basislinie versus rs-grafik"><br>  <i>Hier steht RS für Reward Shaping.</i> <br><br><h2>  Ist es gut das zu tun? </h2><br>  Der Fortschritt ist offensichtlich: Unser Agent hat eindeutig gelernt, den Berg hinaufzufahren, da sich die Auszeichnung von -200 zu unterscheiden begann.  Es bleibt nur noch eine Frage: Wenn wir die Funktion der Belohnung ändern, ändern wir auch die Aufgabe selbst. Wird die Lösung für das neue Problem, das wir gefunden haben, für das alte Problem gut sein? <br><br>  Zunächst verstehen wir, was „Güte“ in unserem Fall bedeutet.  Um das Problem zu lösen, versuchen wir, die optimale Richtlinie zu finden - eine, die die Gesamtbelohnung für die Episode maximiert.  In diesem Fall können wir das Wort „gut“ durch das Wort „optimal“ ersetzen, weil wir danach suchen.  Wir hoffen auch optimistisch, dass unser DQN früher oder später die optimale Lösung für das modifizierte Problem findet und nicht an einem lokalen Maximum hängen bleibt.  Die Frage kann also wie folgt umformuliert werden: Wenn wir die Funktion der Belohnung ändern, haben wir auch das Problem selbst geändert. Ist die optimale Lösung für das neue Problem, das wir für das alte Problem als optimal befunden haben, optimal? <br><br>  Wie sich herausstellt, können wir im allgemeinen Fall keine solche Garantie geben.  Die Antwort hängt davon ab, wie genau wir die Funktion der Belohnung geändert haben, wie sie früher angeordnet wurde und wie die Umgebung selbst angeordnet ist.  Glücklicherweise gibt es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Artikel,</a> dessen Autoren untersucht haben, wie sich eine Änderung der Funktion der Belohnung auf die Optimalität der gefundenen Lösung auswirkt. <br><br>  Zunächst fanden sie eine ganze Klasse von „sicheren“ Änderungen, die auf der potenziellen Methode basieren: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mi>e</mi><mi>u</mi><mi>e</mi><mi>r</mi><msub><mtext>&amp;#xA0;</mtext><mi>S</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.333ex" height="2.66ex" viewBox="0 -832 25976.7 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="5010" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="5490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="6020" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="6898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="7777" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-63" x="8556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-64" x="8990" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="9513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="9999" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="10610" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="11362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="11938" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="12284" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="12673" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="13274" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="13740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="14313" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="14779" y="0"></use><g transform="translate(15231,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-53" x="353" y="-219"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="16037" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="16399" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="16928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="17290" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="17862" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="18332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2212" x="18943" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="20194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="20946" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="21522" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="21868" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-53" x="22257" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="22903" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="23264" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="23794" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="24155" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="24728" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="25197" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="25587" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>′</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>u</mi><mi>e</mi><mi>r</mi><msub><mtext>&nbsp;</mtext><mi>S</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7"> R ’= R + (\ gamma \ cdot \ Phi (neuer \ _Status) - \ Phi (Status)) </script>  wo <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.468ex" height="2.057ex" viewBox="0 -780.1 1923.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="1001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="1578" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ Phi </script>  - Potenzial, das nur vom Staat abhängt.  Für solche Funktionen konnten die Autoren nachweisen, dass die optimale Lösung für das neue Problem auch für das alte Problem optimal ist. <br><br>  Zweitens zeigten die Autoren das für jeden anderen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.056ex" height="2.66ex" viewBox="0 -832 7343.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-46" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="5120" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="5509" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2C" x="5979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="6424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="6953" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>′</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-9"> R ’= R + F (s, a) </script>  Es gibt ein solches Problem, die R-Belohnungsfunktion und die optimale Lösung für das geänderte Problem, dass diese Lösung für das ursprüngliche Problem nicht optimal ist.  Dies bedeutet, dass wir die Güte der gefundenen Lösung nicht garantieren können, wenn wir eine Änderung verwenden, die nicht auf der potenziellen Methode basiert. <br><br>  Daher kann die Verwendung potenzieller Funktionen zum Modifizieren der Belohnungsfunktion nur die Konvergenzrate des Algorithmus ändern, hat jedoch keinen Einfluss auf die endgültige Lösung. <br><br><h2>  Beschleunigen Sie die Konvergenz richtig </h2><br>  Nachdem wir nun wissen, wie die Belohnung sicher geändert werden kann, versuchen wir, die Aufgabe erneut zu ändern, indem wir die potenzielle Methode anstelle der naiven Heuristik verwenden: <pre>  modifizierter_Preis = Belohnung + 300 * (gamma * abs (neuer_Zustand [1]) - abs (Zustand [1])) </pre><br>  Schauen wir uns den Zeitplan der ursprünglichen Auszeichnung an: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/a60/5dc/6baa605dc8dd5ad8e7bf154e5dde3c74.png" alt="Grafik zum Vergleich von Basislinie, RS und RS mit Auswirkungenen"><br><br>  Wie sich herausstellte, verbesserte das Ändern der Belohnung mit Hilfe potenzieller Funktionen neben theoretischen Garantien auch das Ergebnis erheblich, insbesondere in den frühen Stadien.  Natürlich besteht die Möglichkeit, dass optimalere Hyperparameter (zufälliger Keim, Gamma und andere Koeffizienten) für das Training des Agenten ausgewählt werden können, aber die Belohnungsformung erhöht die Rate der Modellkonvergenz dennoch erheblich. <br><br><h2>  Nachwort </h2><br>  Vielen Dank für das Lesen bis zum Ende!  Ich hoffe, Ihnen hat dieser kleine praxisorientierte Ausflug in das verstärkte Lernen gefallen.  Es ist klar, dass Mountain Car eine „Spielzeug“ -Aufgabe ist. Wie wir jedoch feststellen konnten, kann es schwierig sein, einem Agenten beizubringen, selbst eine scheinbar einfache Aufgabe aus menschlicher Sicht zu lösen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444428/">https://habr.com/ru/post/de444428/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444416/index.html">HyperX Alloy CORE - wenn die Membran Spiele spielen kann</a></li>
<li><a href="../de444418/index.html">Millionen von Binärdateien später. Wie Linux gestärkt wurde</a></li>
<li><a href="../de444420/index.html">Wie man zwei Räder zur Arbeit fährt</a></li>
<li><a href="../de444422/index.html">Wie schon 2018: Industrieller FDM-Druck auf der Top 3D Expo</a></li>
<li><a href="../de444426/index.html">Lyft und Uber gehen an die Börse. Warum in Lyft investieren?</a></li>
<li><a href="../de444430/index.html">Analyse: Wie man Present Perfect tatsächlich auf Englisch verwendet</a></li>
<li><a href="../de444432/index.html">Der Einsatz von Linux und Open Source Software in unserer Bildungseinrichtung: sein oder nicht sein?</a></li>
<li><a href="../de444434/index.html">Es ist Zeit für Java 12! Überprüfung der heißen JEPs</a></li>
<li><a href="../de444436/index.html">Was ist ein Mirai-Botnetz und wie kann ich meine Geräte schützen?</a></li>
<li><a href="../de444438/index.html">Eine kurze Geschichte von Open Source - wie freie Software mit proprietären kämpfte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>