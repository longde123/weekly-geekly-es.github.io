<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßôüèº üë®üèø‚Äç‚öïÔ∏è üêê Bergauto: Die klassische Herausforderung mit Verst√§rkungstraining l√∂sen üßòüèæ üëî üë©üèº‚Äçüé®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In der Regel werden √Ñnderungen an Algorithmen, die auf den spezifischen Merkmalen einer bestimmten Aufgabe beruhen, als weniger wertvoll angesehen, da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bergauto: Die klassische Herausforderung mit Verst√§rkungstraining l√∂sen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/hsespb/blog/444428/">  In der Regel werden √Ñnderungen an Algorithmen, die auf den spezifischen Merkmalen einer bestimmten Aufgabe beruhen, als weniger wertvoll angesehen, da sie sich nur schwer auf eine breitere Klasse von Problemen √ºbertragen lassen.  Dies bedeutet jedoch nicht, dass solche √Ñnderungen nicht erforderlich sind.  Dar√ºber hinaus k√∂nnen sie h√§ufig das Ergebnis selbst bei einfachen klassischen Problemen erheblich verbessern, was f√ºr die praktische Anwendung von Algorithmen sehr wichtig ist.  In diesem Beitrag werde ich beispielsweise das Mountain Car-Problem mit einem Verst√§rkungstraining l√∂sen und zeigen, dass es mit dem Wissen √ºber die Organisation der Aufgabe viel schneller gel√∂st werden kann. <br><br><img src="https://habrastorage.org/webt/xy/ju/ai/xyjuaivxj9j2c5hp2o-2x3cem2y.png"><br><a name="habracut"></a><br><h2>  √úber mich </h2><br>  Mein Name ist Oleg Svidchenko, jetzt studiere ich an der Fakult√§t f√ºr Physik, Mathematik und Informatik der HSE in St. Petersburg, bevor ich drei Jahre an der Universit√§t in St. Petersburg studierte.  Ich arbeite auch als Forscher bei JetBrains Research.  Bevor ich an die Universit√§t kam, studierte ich am SSC der Moskauer Staatlichen Universit√§t und wurde als Teil des Moskauer Teams der Gewinner der Allrussischen Olympiade der Sch√ºler der Informatik. <br><br><h2>  Was brauchen wir </h2><br>  Wenn Sie an einem Verst√§rkungstraining interessiert sind, ist die Mountain Car-Herausforderung genau das Richtige f√ºr Sie.  Heute ben√∂tigen wir Python mit den installierten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gym-</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PyTorch-Bibliotheken</a> sowie Grundkenntnisse √ºber neuronale Netze. <br><br><h2>  Aufgabenbeschreibung </h2><br>  In einer zweidimensionalen Welt muss ein Auto von der Mulde zwischen zwei H√ºgeln auf die Spitze des rechten H√ºgels klettern.  Es wird durch die Tatsache kompliziert, dass sie nicht genug Motorleistung hat, um die Schwerkraft zu √ºberwinden und beim ersten Versuch dort einzutreten.  Wir sind eingeladen, einen Agenten (in unserem Fall ein neuronales Netzwerk) auszubilden, der durch Steuerung so schnell wie m√∂glich den rechten H√ºgel erklimmen kann. <br><br>  Die Maschinensteuerung erfolgt durch Interaktion mit der Umgebung.  Es ist in unabh√§ngige Episoden unterteilt und jede Episode wird Schritt f√ºr Schritt ausgef√ºhrt.  Bei jedem Schritt empf√§ngt der Agent den Status <i>s</i> und die Umgebung <i>r</i> von der Umgebung als Antwort auf die Aktion <i>a</i> .  Au√üerdem meldet das Medium manchmal zus√§tzlich, dass die Episode beendet ist.  In diesem Problem ist <i>s</i> ein Zahlenpaar, von dem die erste die Position des Autos auf der Kurve ist (eine Koordinate reicht aus, da wir uns nicht von der Oberfl√§che losrei√üen k√∂nnen), und die zweite ist die Geschwindigkeit auf der Oberfl√§che (mit einem Vorzeichen).  Die Belohnung <i>r</i> ist eine Zahl, die f√ºr diese Aufgabe immer gleich -1 ist.  Auf diese Weise ermutigen wir den Agenten, die Episode so schnell wie m√∂glich abzuschlie√üen.  Es gibt nur drei m√∂gliche Aktionen: Schieben Sie das Auto nach links, tun Sie nichts und schieben Sie das Auto nach rechts.  Diese Aktionen entsprechen Zahlen von 0 bis 2. Die Episode kann enden, wenn das Auto die Spitze des rechten H√ºgels erreicht oder wenn der Agent 200 Schritte unternommen hat. <br><br><h2>  Ein bisschen Theorie </h2><br>  Auf Habr√© gab es bereits einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber DQN,</a> in dem der Autor alle notwendigen Theorien ziemlich gut beschrieb.  Um das Lesen zu erleichtern, werde ich es hier in einer formelleren Form wiederholen. <br><br>  Die Verst√§rkungslernaufgabe wird durch einen Satz von Zustandsraum S, Aktionsraum A, Koeffizient definiert <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.237ex" height="1.817ex" viewBox="0 -520.7 3546.5 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="730" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="1260" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="2138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="3017" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ gamma </script>  Im Allgemeinen k√∂nnen die √úbergangsfunktion und die Belohnungsfunktion Zufallsvariablen sein, aber jetzt betrachten wir eine einfachere Version, in der sie eindeutig definiert sind.  Ziel ist es, die kumulierten Belohnungen zu maximieren. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.676ex" height="3.021ex" viewBox="0 -883.9 10193.7 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-54" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-30" x="1140" y="0"></use></g></g><g transform="translate(3430,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="638" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-63" x="4487" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-64" x="4921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="5444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="5930" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="6541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="7022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="7551" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="8430" y="0"></use><g transform="translate(9308,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="748" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ sum_ {t = 0} ^ {T} r_ {t} \ cdot \ gamma ^ {t} </script>  Dabei ist t die Schrittnummer im Medium und T die Anzahl der Schritte in der Episode. <br><br>  Um dieses Problem zu l√∂sen, definieren wir die Wertfunktion V des Zustands s als den Wert der maximalen kumulativen Belohnung, vorausgesetzt, wir beginnen im Zustand s.  Wenn wir eine solche Funktion kennen, k√∂nnen wir das Problem einfach l√∂sen, indem wir bei jedem Schritt s mit dem maximal m√∂glichen Wert √ºbergeben.  Es ist jedoch nicht alles so einfach: In den meisten F√§llen wissen wir nicht, welche Aktion uns in den gew√ºnschten Zustand bringt.  Daher f√ºgen wir die Aktion a als zweiten Parameter der Funktion hinzu.  Die resultierende Funktion wird als Q-Funktion bezeichnet.  Es zeigt, welche maximal m√∂gliche kumulative Belohnung wir erhalten k√∂nnen, wenn wir die Aktion a in state s ausf√ºhren.  Aber wir k√∂nnen diese Funktion bereits verwenden, um das Problem zu l√∂sen: Wenn wir uns im Zustand s befinden, w√§hlen wir einfach a so, dass Q (s, a) maximal ist. <br><br>  In der Praxis kennen wir die reale Q-Funktion nicht, k√∂nnen sie aber mit verschiedenen Methoden approximieren.  Eine solche Technik ist das Deep Q Network (DQN).  Seine Idee ist, dass wir f√ºr jede der Aktionen die Q-Funktion unter Verwendung eines neuronalen Netzwerks approximieren. <br><br><h2>  Die Umwelt </h2><br>  Jetzt lass uns √ºben.  Zun√§chst m√ºssen wir lernen, wie die MountainCar-Umgebung emuliert wird.  Die Turnhallenbibliothek, die eine gro√üe Anzahl von Standard-Lernumgebungen zur Verst√§rkung bietet, wird uns bei der Bew√§ltigung dieser Aufgabe helfen.  Um eine Umgebung zu erstellen, m√ºssen wir die make-Methode im Fitness-Studio-Modul aufrufen und den Namen der gew√ºnschten Umgebung als Parameter √ºbergeben: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym env = gym.make(<span class="hljs-string"><span class="hljs-string">"MountainCar-v0"</span></span>)</code> </pre> <br>  Eine ausf√ºhrliche Dokumentation finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und eine Beschreibung der Umgebung finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br>  Lassen Sie uns genauer betrachten, was wir mit der von uns geschaffenen Umgebung tun k√∂nnen: <br><br><ul><li>  <code>env.reset()</code> - beendet die aktuelle Episode und startet eine neue.  Gibt den Ausgangszustand zur√ºck. </li><li>  <code>env.step(action)</code> - f√ºhrt die angegebene Aktion aus.  Gibt einen neuen Status, eine Belohnung, ob die Episode beendet wurde und zus√§tzliche Informationen zur√ºck, die zum Debuggen verwendet werden k√∂nnen. </li><li>  <code>env.seed(seed)</code> - setzt zuf√§lligen Samen.  Dies h√§ngt davon ab, wie die Anfangszust√§nde w√§hrend env.reset () generiert werden. </li><li>  <code>env.render()</code> - <code>env.render()</code> den aktuellen Status der Umgebung an. </li></ul><br><h2>  Wir realisieren DQN </h2><br>  DQN ist ein Algorithmus, der ein neuronales Netzwerk verwendet, um eine Q-Funktion zu bewerten.  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">urspr√ºnglichen Artikel</a> definierte DeepMind die Standardarchitektur f√ºr Atari-Spiele unter Verwendung von Faltungs-Neuronalen Netzen.  Im Gegensatz zu diesen Spielen verwendet Mountain Car das Bild nicht als Status, daher m√ºssen wir die Architektur selbst bestimmen. <br><br>  Nehmen wir zum Beispiel eine Architektur mit zwei versteckten Schichten von jeweils 32 Neuronen.  Nach jeder verborgenen Ebene verwenden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ReLU</a> als Aktivierungsfunktion.  Zwei Zahlen, die den Zustand beschreiben, werden dem Eingang des neuronalen Netzwerks zugef√ºhrt, und am Ausgang erhalten wir eine Sch√§tzung der Q-Funktion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/jl/mk/aejlmktkosv-jpbne9hqi96enxw.png" alt="Neuronale Netzwerkarchitektur"></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights)</span></span></code> </pre><br>  Da wir das neuronale Netzwerk auf der GPU trainieren werden, m√ºssen wir unser Netzwerk dort laden: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     CPU,  ‚Äúcuda‚Äù    ‚Äúcpu‚Äù device = torch.device("cuda") model.to(device) target_model.to(device)</span></span></code> </pre><br>  Die Ger√§tevariable ist global, da wir auch die Daten laden m√ºssen. <br><br>  Wir m√ºssen auch einen Optimierer definieren, der die Modellgewichte mithilfe des Gradientenabfalls aktualisiert.  Ja, es gibt viel mehr als eine. <br><br><pre> <code class="python hljs">optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.00003</span></span>)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch device = torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_new_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights) #   ,     (GPU  CPU) model.to(device) target_model.to(device) #  ,        optimizer = optim.Adam(model.parameters(), lr=0.00003) return model, target_model, optimizer</span></span></code> </pre><br></div></div><br>  Deklarieren Sie nun eine Funktion, die die Fehlerfunktion und den Gradienten entlang ber√ºcksichtigt, und wenden Sie den Abstieg an.  Zuvor m√ºssen Sie jedoch Daten aus dem Stapel auf die GPU herunterladen: <br><br><pre> <code class="python hljs">state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device)</span></span></code> </pre><br>  Als n√§chstes m√ºssen wir die realen Werte der Q-Funktion berechnen. Da wir sie jedoch nicht kennen, werden wir sie anhand der Werte f√ºr den folgenden Zustand bewerten: <br><br><pre> <code class="python hljs">target_q = torch.zeros(reward.size()[<span class="hljs-number"><span class="hljs-number">0</span></span>]).float().to(device) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-comment"><span class="hljs-comment">#     Q-function    target_q = target_model(next_state).max(1)[0].view(-1) target_q[done] = 0 target_q = reward + target_q * gamma</span></span></code> </pre><br>  Und die aktuelle Vorhersage: <br><br><pre> <code class="python hljs">q = model(state).gather(<span class="hljs-number"><span class="hljs-number">1</span></span>, action.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre><br>  Mit target_q und q berechnen wir die Verlustfunktion und aktualisieren das Modell: <br><br><pre> <code class="python hljs">loss = F.smooth_l1_loss(q, target_q.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-comment"><span class="hljs-comment">#      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs">gamma = <span class="hljs-number"><span class="hljs-number">0.99</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch, model, target_model, optimizer)</span></span></span><span class="hljs-function">:</span></span> state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device) #  ,       target_q = torch.zeros(reward.size()[0]).float().to(device) with torch.no_grad(): #     Q-function    target_q = target_model(next_state).max(1)[0].view(-1) target_q[done] = 0 target_q = reward + target_q * gamma #   q = model(state).gather(1, action.unsqueeze(1)) loss = F.smooth_l1_loss(q, target_q.unsqueeze(1)) #      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br></div></div><br>  Da das Modell nur die Q-Funktion ber√ºcksichtigt und keine Aktionen ausf√ºhrt, m√ºssen wir die Funktion bestimmen, die entscheidet, welche Aktionen der Agent ausf√ºhren wird.  Als Entscheidungsalgorithmus nehmen wir <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ varepsilon </script>  -gr√ºne Politik.  Ihre Idee ist, dass der Agent normalerweise gierig Aktionen ausf√ºhrt und das Maximum der Q-Funktion w√§hlt, aber mit Wahrscheinlichkeit <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ varepsilon </script>  Er wird eine zuf√§llige Aktion ausf√ºhren.  Es sind zuf√§llige Aktionen erforderlich, damit der Algorithmus die Aktionen untersuchen kann, die er nicht ausgef√ºhrt h√§tte, wenn er nur von einer gierigen Richtlinie geleitet w√ºrde. Dieser Prozess wird als Exploration bezeichnet. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">select_action</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(state, epsilon, model)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random.random() &lt; epsilon: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model(torch.tensor(state).to(device).float().unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>))[<span class="hljs-number"><span class="hljs-number">0</span></span>].max(<span class="hljs-number"><span class="hljs-number">0</span></span>)[<span class="hljs-number"><span class="hljs-number">1</span></span>].view(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>).item()</code> </pre><br>  Da wir Stapel verwenden, um das neuronale Netzwerk zu trainieren, ben√∂tigen wir einen Puffer, in dem wir die Erfahrung der Interaktion mit der Umgebung speichern und aus dem wir Stapel ausw√§hlen: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Memory</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, capacity)</span></span></span><span class="hljs-function">:</span></span> self.capacity = capacity self.memory = [] self.position = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">push</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, element)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(self.memory) &lt; self.capacity: self.memory.append(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) self.memory[self.position] = element self.position = (self.position + <span class="hljs-number"><span class="hljs-number">1</span></span>) % self.capacity <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, batch_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list(zip(*random.sample(self.memory, batch_size))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.memory)</code> </pre><br><h2>  Naive Entscheidung </h2><br>  Deklarieren Sie zun√§chst die Konstanten, die wir im Lernprozess verwenden werden, und erstellen Sie ein Modell: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 #    memory = Memory(5000) model, target_model, optimizer = create_new_model()</span></span></code> </pre><br>  Trotz der Tatsache, dass es logisch w√§re, den Interaktionsprozess in Episoden zu unterteilen, ist es f√ºr uns bequemer, den Lernprozess in separate Schritte zu unterteilen, da wir nach jedem Schritt der Umgebung einen Schritt des Gradientenabfalls machen m√∂chten. <br><br>  Lassen Sie uns genauer dar√ºber sprechen, wie ein Lernschritt hier aussieht.  Wir gehen davon aus, dass wir jetzt einen Schritt mit der Schrittanzahl der max_steps-Schritte und dem aktuellen Status machen.  Dann mache die Aktion mit <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ varepsilon </script>  -gr√ºne Richtlinien w√ºrden so aussehen: <br><br><pre> <code class="python hljs">epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action)</code> </pre><br>  F√ºgen Sie die gesammelten Erfahrungen sofort in das Ged√§chtnis ein und starten Sie eine neue Episode, wenn die aktuelle beendet ist: <br><br><pre> <code class="python hljs">memory.push((state, action, reward, new_state, done)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> done: state = env.reset() done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: state = new_state</code> </pre><br>  Und wir werden den Schritt des Gradientenabstiegs machen (wenn wir nat√ºrlich bereits mindestens eine Charge sammeln k√∂nnen): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer)</code> </pre><br>  Jetzt muss noch target_model aktualisiert werden: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model)</code> </pre><br>  Wir m√∂chten aber auch den Lernprozess verfolgen.  Zu diesem Zweck spielen wir nach jedem Update von target_model mit epsilon = 0 eine zus√§tzliche Episode ab, in der die Gesamtpr√§mie im Puffer belohnt_by_target_updates gespeichert wird: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model) state = env.reset() total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> done: action = select_action(state, <span class="hljs-number"><span class="hljs-number">0</span></span>, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> state = env.reset() rewards_by_target_updates.append(total_reward)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Alle zusammen</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 def fit(): #    memory = Memory(5000) model, target_model, optimizer = create_new_model() for step in range(max_steps): #    epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action) #  ,  ,   memory.push((state, action, reward, new_state, done)) if done: state = env.reset() done = False else: state = new_state #  if step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer) if step % target_update == 0: target_model = copy.deepcopy(model) #Exploitation state = env.reset() total_reward = 0 while not done: action = select_action(state, 0, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = False state = env.reset() rewards_by_target_updates.append(total_reward) return rewards_by_target_updates</span></span></code> </pre><br></div></div><br>  F√ºhren Sie diesen Code aus und erhalten Sie so etwas wie dieses Diagramm: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e35/952/b37/e35952b375fc831ec4bd405303440509.png" alt="Basisliniendiagramm in Form einer geraden Linie y = -200"><br><br><h2>  Was ist schief gelaufen? </h2><br>  Ist das ein Fehler?  Ist das der falsche Algorithmus?  Sind das schlechte Parameter?  Nicht wirklich.  Tats√§chlich liegt das Problem in der Aufgabe, n√§mlich in der Funktion der Belohnung.  Schauen wir es uns genauer an.  Bei jedem Schritt erh√§lt unser Agent eine Belohnung von -1, und dies geschieht bis zum Ende der Episode.  Eine solche Belohnung motiviert den Agenten, die Episode so schnell wie m√∂glich zu beenden, sagt ihm aber gleichzeitig nicht, wie es geht.  Aus diesem Grund besteht die einzige M√∂glichkeit zu lernen, wie ein Problem in einer solchen Formulierung f√ºr einen Agenten gel√∂st werden kann, darin, es viele Male mithilfe von Exploration zu l√∂sen. <br><br>  Nat√ºrlich k√∂nnte man versuchen, komplexere Algorithmen zu verwenden, um die Umgebung zu untersuchen, anstatt unsere <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ varepsilon </script>  -gr√ºne Richtlinien.  Zum einen wird unser Modell jedoch aufgrund ihrer Anwendung komplexer, was wir vermeiden m√∂chten, und zum anderen nicht, dass sie f√ºr diese Aufgabe gut genug funktionieren.  Stattdessen k√∂nnen wir die Ursache des Problems beseitigen, indem wir die Aufgabe selbst modifizieren, n√§mlich indem wir die Belohnungsfunktion √§ndern, d. H.  durch Anwenden der sogenannten Belohnungsformung. <br><br><h2>  Beschleunigung der Konvergenz </h2><br>  Unser intuitives Wissen sagt uns, dass Sie beschleunigen m√ºssen, um den Berg hinaufzufahren.  Je h√∂her die Geschwindigkeit, desto n√§her ist der Agent an der L√∂sung des Problems.  Sie k√∂nnen ihm dies beispielsweise mitteilen, indem Sie der Belohnung ein Geschwindigkeitsmodul mit einem bestimmten Koeffizienten hinzuf√ºgen: <pre>  modifizierter_Reward = Belohnung + 10 * abs (neuer_Zustand [1]) </pre><br><br>  Dementsprechend passt eine Linie in die Funktion <pre>  memory.push ((Status, Aktion, Belohnung, neuer_Zustand, erledigt)) </pre>  sollte ersetzt werden durch <pre>  memory.push ((Status, Aktion, modifizierter_Reward, neuer_Zustand, erledigt)) </pre>  Schauen wir uns nun das neue Diagramm an (es pr√§sentiert die <b>urspr√ºngliche</b> Auszeichnung ohne √Ñnderungen): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c19/3c2/925/c193c2925d13ba1977cf525697cba1c2.png" alt="Basislinie versus rs-grafik"><br>  <i>Hier steht RS f√ºr Reward Shaping.</i> <br><br><h2>  Ist es gut das zu tun? </h2><br>  Der Fortschritt ist offensichtlich: Unser Agent hat eindeutig gelernt, den Berg hinaufzufahren, da sich die Auszeichnung von -200 zu unterscheiden begann.  Es bleibt nur noch eine Frage: Wenn wir die Funktion der Belohnung √§ndern, √§ndern wir auch die Aufgabe selbst. Wird die L√∂sung f√ºr das neue Problem, das wir gefunden haben, f√ºr das alte Problem gut sein? <br><br>  Zun√§chst verstehen wir, was ‚ÄûG√ºte‚Äú in unserem Fall bedeutet.  Um das Problem zu l√∂sen, versuchen wir, die optimale Richtlinie zu finden - eine, die die Gesamtbelohnung f√ºr die Episode maximiert.  In diesem Fall k√∂nnen wir das Wort ‚Äûgut‚Äú durch das Wort ‚Äûoptimal‚Äú ersetzen, weil wir danach suchen.  Wir hoffen auch optimistisch, dass unser DQN fr√ºher oder sp√§ter die optimale L√∂sung f√ºr das modifizierte Problem findet und nicht an einem lokalen Maximum h√§ngen bleibt.  Die Frage kann also wie folgt umformuliert werden: Wenn wir die Funktion der Belohnung √§ndern, haben wir auch das Problem selbst ge√§ndert. Ist die optimale L√∂sung f√ºr das neue Problem, das wir f√ºr das alte Problem als optimal befunden haben, optimal? <br><br>  Wie sich herausstellt, k√∂nnen wir im allgemeinen Fall keine solche Garantie geben.  Die Antwort h√§ngt davon ab, wie genau wir die Funktion der Belohnung ge√§ndert haben, wie sie fr√ºher angeordnet wurde und wie die Umgebung selbst angeordnet ist.  Gl√ºcklicherweise gibt es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Artikel,</a> dessen Autoren untersucht haben, wie sich eine √Ñnderung der Funktion der Belohnung auf die Optimalit√§t der gefundenen L√∂sung auswirkt. <br><br>  Zun√§chst fanden sie eine ganze Klasse von ‚Äûsicheren‚Äú √Ñnderungen, die auf der potenziellen Methode basieren: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mi>e</mi><mi>u</mi><mi>e</mi><mi>r</mi><msub><mtext>&amp;#xA0;</mtext><mi>S</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.333ex" height="2.66ex" viewBox="0 -832 25976.7 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-67" x="5010" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="5490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="6020" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6D" x="6898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="7777" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-63" x="8556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-64" x="8990" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6F" x="9513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="9999" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="10610" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="11362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="11938" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="12284" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-6E" x="12673" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="13274" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="13740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-65" x="14313" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-72" x="14779" y="0"></use><g transform="translate(15231,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-53" x="353" y="-219"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="16037" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="16399" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="16928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="17290" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="17862" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="18332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2212" x="18943" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="20194" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="20946" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="21522" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="21868" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-53" x="22257" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="22903" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="23264" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-74" x="23794" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-75" x="24155" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="24728" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="25197" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="25587" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>u</mi><mi>e</mi><mi>r</mi><msub><mtext>&nbsp;</mtext><mi>S</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>S</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7"> R ‚Äô= R + (\ gamma \ cdot \ Phi (neuer \ _Status) - \ Phi (Status)) </script>  wo <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.468ex" height="2.057ex" viewBox="0 -780.1 1923.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-50" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-68" x="1001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-69" x="1578" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ Phi </script>  - Potenzial, das nur vom Staat abh√§ngt.  F√ºr solche Funktionen konnten die Autoren nachweisen, dass die optimale L√∂sung f√ºr das neue Problem auch f√ºr das alte Problem optimal ist. <br><br>  Zweitens zeigten die Autoren das f√ºr jeden anderen <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.056ex" height="2.66ex" viewBox="0 -832 7343.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-46" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-28" x="5120" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-73" x="5509" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-2C" x="5979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMATHI-61" x="6424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;usg=ALkJrhj6MGGG_pUv-mJ9AxVW3VGmnQIHLA#MJMAIN-29" x="6953" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-9"> R ‚Äô= R + F (s, a) </script>  Es gibt ein solches Problem, die R-Belohnungsfunktion und die optimale L√∂sung f√ºr das ge√§nderte Problem, dass diese L√∂sung f√ºr das urspr√ºngliche Problem nicht optimal ist.  Dies bedeutet, dass wir die G√ºte der gefundenen L√∂sung nicht garantieren k√∂nnen, wenn wir eine √Ñnderung verwenden, die nicht auf der potenziellen Methode basiert. <br><br>  Daher kann die Verwendung potenzieller Funktionen zum Modifizieren der Belohnungsfunktion nur die Konvergenzrate des Algorithmus √§ndern, hat jedoch keinen Einfluss auf die endg√ºltige L√∂sung. <br><br><h2>  Beschleunigen Sie die Konvergenz richtig </h2><br>  Nachdem wir nun wissen, wie die Belohnung sicher ge√§ndert werden kann, versuchen wir, die Aufgabe erneut zu √§ndern, indem wir die potenzielle Methode anstelle der naiven Heuristik verwenden: <pre>  modifizierter_Preis = Belohnung + 300 * (gamma * abs (neuer_Zustand [1]) - abs (Zustand [1])) </pre><br>  Schauen wir uns den Zeitplan der urspr√ºnglichen Auszeichnung an: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/a60/5dc/6baa605dc8dd5ad8e7bf154e5dde3c74.png" alt="Grafik zum Vergleich von Basislinie, RS und RS mit Auswirkungenen"><br><br>  Wie sich herausstellte, verbesserte das √Ñndern der Belohnung mit Hilfe potenzieller Funktionen neben theoretischen Garantien auch das Ergebnis erheblich, insbesondere in den fr√ºhen Stadien.  Nat√ºrlich besteht die M√∂glichkeit, dass optimalere Hyperparameter (zuf√§lliger Keim, Gamma und andere Koeffizienten) f√ºr das Training des Agenten ausgew√§hlt werden k√∂nnen, aber die Belohnungsformung erh√∂ht die Rate der Modellkonvergenz dennoch erheblich. <br><br><h2>  Nachwort </h2><br>  Vielen Dank f√ºr das Lesen bis zum Ende!  Ich hoffe, Ihnen hat dieser kleine praxisorientierte Ausflug in das verst√§rkte Lernen gefallen.  Es ist klar, dass Mountain Car eine ‚ÄûSpielzeug‚Äú -Aufgabe ist. Wie wir jedoch feststellen konnten, kann es schwierig sein, einem Agenten beizubringen, selbst eine scheinbar einfache Aufgabe aus menschlicher Sicht zu l√∂sen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444428/">https://habr.com/ru/post/de444428/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444416/index.html">HyperX Alloy CORE - wenn die Membran Spiele spielen kann</a></li>
<li><a href="../de444418/index.html">Millionen von Bin√§rdateien sp√§ter. Wie Linux gest√§rkt wurde</a></li>
<li><a href="../de444420/index.html">Wie man zwei R√§der zur Arbeit f√§hrt</a></li>
<li><a href="../de444422/index.html">Wie schon 2018: Industrieller FDM-Druck auf der Top 3D Expo</a></li>
<li><a href="../de444426/index.html">Lyft und Uber gehen an die B√∂rse. Warum in Lyft investieren?</a></li>
<li><a href="../de444430/index.html">Analyse: Wie man Present Perfect tats√§chlich auf Englisch verwendet</a></li>
<li><a href="../de444432/index.html">Der Einsatz von Linux und Open Source Software in unserer Bildungseinrichtung: sein oder nicht sein?</a></li>
<li><a href="../de444434/index.html">Es ist Zeit f√ºr Java 12! √úberpr√ºfung der hei√üen JEPs</a></li>
<li><a href="../de444436/index.html">Was ist ein Mirai-Botnetz und wie kann ich meine Ger√§te sch√ºtzen?</a></li>
<li><a href="../de444438/index.html">Eine kurze Geschichte von Open Source - wie freie Software mit propriet√§ren k√§mpfte</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>