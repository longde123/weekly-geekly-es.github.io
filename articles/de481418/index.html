<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå∫ ‚úäüèª üçî Wie der Videocodec funktioniert. Teil 1. Grundlagen üë©‚Äçüíº üéõÔ∏è üìÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Zweiter Teil: Wie der Video-Codec funktioniert 

 Jedes Rasterbild kann als zweidimensionale Matrix dargestellt werden . Wenn es um Farben geht, kann ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie der Videocodec funktioniert. Teil 1. Grundlagen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/edison/blog/481418/"><h3>  Zweiter Teil: <a href="https://habr.com/ru/company/edison/blog/480430/">Wie der Video-Codec funktioniert</a> </h3><br><hr><br>  Jedes Rasterbild kann als <b>zweidimensionale Matrix dargestellt werden</b> .  Wenn es um Farben geht, kann die Idee entwickelt werden, indem das Bild in Form einer <b>dreidimensionalen Matrix betrachtet wird</b> , in der zus√§tzliche Messungen zum Speichern von Daten f√ºr jede der Farben verwendet werden. <br><br>  Betrachten wir die endg√ºltige Farbe als eine Kombination der sogenannten  Prim√§rfarben (rot, gr√ºn und blau) bestimmen wir in unserer dreidimensionalen Matrix drei Ebenen: die erste f√ºr rot, die zweite f√ºr gr√ºn und die letzte f√ºr blau. <br><div style="text-align:center;"><img width="746" height="235" src="https://habrastorage.org/webt/gz/rx/7q/gzrx7qp8p_kzdeb2cat-raur3sk.png" alt="3D RGB Matrix" title="3D RGB Matrix"></div><br>  Wir werden jeden Punkt in dieser Matrix als Pixel (Bildelement) bezeichnen.  Jedes Pixel enth√§lt Intensit√§tsinformationen (normalerweise in Form eines numerischen Werts) jeder Farbe.  Zum Beispiel bedeutet ein <b>rotes Pixel</b> , dass es 0 Gr√ºn, 0 Blau und maximal Rot hat.  <b>Ein rosa Pixel</b> kann unter Verwendung einer Kombination von drei Farben gebildet werden.  Unter Verwendung eines Zahlenbereichs von 0 bis 255 wird das rosa Pixel als <b>Rot = 255</b> , <b>Gr√ºn = 192</b> und <b>Blau = 203 definiert</b> . <a name="habracut"></a><blockquote> <a href="https://www.edsd.ru/" title="EDISON Software - Webentwicklung"><img align="left" width="153" height="75" src="https://habrastorage.org/webt/w0/zl/to/w0zltoxvysbr0yeinstkfvw1wbg.png" alt="EDISON Software - Webentwicklung"></a> <br clear="right">  Dieser Artikel wurde mit der Unterst√ºtzung von EDISON ver√∂ffentlicht. <br><br>  Wir entwickeln <a href="https://www.edsd.ru/ru/portfolio/tehnologiya/video">Anwendungen f√ºr die Video√ºberwachung, das Streaming von Videos</a> sowie die <a href="https://www.edsd.ru/videozapis-v-bolnichnoj-operacionnoj-s-vozmozhnostyu-obsuzhdeniya-na-forume">Videoaufzeichnung im Operationssaal</a> . </blockquote><h3>  Alternative Farbcodierungstechniken </h3><br>  Um die Farben darzustellen, aus denen das Bild besteht, gibt es viele andere Modelle.  Beispielsweise k√∂nnen Sie eine indizierte Palette verwenden, in der nur ein Byte f√ºr jedes Pixel erforderlich ist, anstatt der drei, die bei Verwendung des RGB-Modells erforderlich sind.  In einem solchen Modell k√∂nnen Sie eine 2D-Matrix anstelle einer 3D-Matrix verwenden, um jede Farbe darzustellen.  Das spart Speicherplatz, gibt aber weniger Farbe. <br><br><img width="479" height="120" src="https://habrastorage.org/webt/yx/pp/nb/yxppnbcixbaszve4ixpqzbvsrdy.png" alt="NES-Palette" title="NES-Palette"><br><br><h2>  RGB </h2><br><br>  Schauen Sie sich zum Beispiel das folgende Bild an.  Das erste Gesicht ist komplett bemalt.  Andere sind die roten, gr√ºnen und blauen Ebenen (die Intensit√§t der entsprechenden Farben wird in Graustufen angezeigt). <br><br><img src="https://habrastorage.org/webt/n1/_6/uc/n1_6uc9on4ze-fixad_o7ijvl1u.png" alt="RGB-Kanalintensit√§t" title="RGB-Kanalintensit√§t"><br><br>  Wir sehen, dass sich die Rott√∂ne im Original an denselben Stellen befinden, an denen die hellsten Stellen der zweiten Person beobachtet werden.  W√§hrend der Beitrag von Blau ist haupts√§chlich nur in den Augen von Mario (dem letzten Gesicht) und den Elementen seiner Kleidung zu sehen.  Beachten Sie, wo alle drei Farbebenen den geringsten Beitrag leisten (die dunkelsten Teile der Bilder) - das ist Marios Schnurrbart. <br><br>  Um die Intensit√§t jeder Farbe zu speichern, ist eine bestimmte Anzahl von Bits erforderlich - dieser Wert wird als <b>Bittiefe bezeichnet</b> .  Angenommen, es werden 8 Bits (basierend auf einem Wert von 0 bis 255) auf einer Farbebene ausgegeben.  Dann haben wir eine Farbtiefe von 24 Bit (8 Bit * 3 R / G / B-Ebene). <br><br>  Eine weitere Eigenschaft des Bildes ist die <b>Aufl√∂sung</b> , dh die Anzahl der Pixel in einer Dimension.  Es wird oft als <b>Breite √ó H√∂he bezeichnet</b> , wie im folgenden Beispiel 4 mal 4. <br><img width="401" height="185" src="https://habrastorage.org/webt/5q/2i/so/5q2isolirmyfjxl-hb7m4hj_7au.png" alt="Bildaufl√∂sung" title="Bildaufl√∂sung"><br><br>  Eine weitere Eigenschaft, mit der wir uns bei der Arbeit mit Bildern / Videos befassen, ist das <b>Seitenverh√§ltnis</b> , das das √ºbliche proportionale Verh√§ltnis zwischen Breite und H√∂he eines Bildes oder Pixels beschreibt. <br><br>  Wenn sie sagen, dass ein Film oder ein Bild 16 mal 9 gro√ü ist, bezieht sich dies normalerweise auf das <b>Seitenverh√§ltnis der Anzeige</b> ( <b>DAR</b> - from <i>Display Aspect Ratio</i> ).  Manchmal kann es jedoch zu unterschiedlichen Formen einzelner Pixel kommen - in diesem Fall handelt es sich um das <b>Pixelverh√§ltnis</b> ( <b>PAR</b> - from <i>Pixel Aspect Ratio</i> ). <br><br><img width="536" height="254" src="https://habrastorage.org/webt/xg/mb/gp/xgmbgpydyzkiraifaxf9ltiqkxq.png" alt="Seitenverh√§ltnis anzeigen" title="Seitenverh√§ltnis anzeigen"><br><br><img width="725" height="318" src="https://habrastorage.org/webt/sg/ti/eq/sgtieqvisg6eu5oaddpmfxmrxnm.png" alt="Pixel-Seitenverh√§ltnis" title="Pixel-Seitenverh√§ltnis"><br><blockquote>  Hinweis an die Gastgeberin: <b>DVD</b> entspricht <b>DAR 4 mal 3</b> <br><br>  Obwohl die tats√§chliche Aufl√∂sung der DVD 704 x 480 betr√§gt, bleibt das Seitenverh√§ltnis 4: 3 erhalten, da der PAR auf 10:11 (704 x 10/480 x 11) eingestellt ist. </blockquote><br>  Und schlie√ülich k√∂nnen wir ein <b>Video</b> als Folge von <b>n</b> Bildern √ºber einen bestimmten Zeitraum definieren, was als zus√§tzliche Dimension betrachtet werden kann.  Und <b>n ist</b> dann die Bildrate oder die Anzahl der Bilder pro Sekunde ( <b>FPS</b> - from <i>Frames per Second</i> ). <br><br><img width="710" height="240" src="https://habrastorage.org/webt/67/kd/us/67kdus1r36qphfoi1jqqt9uinpy.png" alt="das Video" title="das video"><br><br>  Die Anzahl der Bits pro Sekunde, die zum Anzeigen eines Videos erforderlich sind, ist die <b>Bitrate</b> . <br><blockquote>  Bitrate = Breite * H√∂he * Bittiefe * Frames pro Sekunde </blockquote><br>  Beispiel: F√ºr Videos mit 30 Bildern pro Sekunde, 24 Bit pro Pixel, 480 x 240 Aufl√∂sung, 82.944.000 Bit pro Sekunde oder 82.944 Mbit / s (30 x 480 x 240 x 24) ist dies erforderlich, wenn Sie keine der Komprimierungsmethoden verwenden. <br><br>  Ist die Bitrate <i>nahezu konstant</i> , spricht man von einer <i>konstanten Bitrate</i> ( <b>CBR</b> - from <i>constant bit rate</i> ).  Es kann aber auch variieren, in diesem Fall spricht man von einer <i>variablen Bitrate</i> ( <b>VBR</b> - from <i>variable bit rate</i> ). <br><br>  Dieser Graph zeigt eine begrenzte VBR, wenn im Falle eines vollst√§ndig dunklen Rahmens nicht zu viele Bits ausgegeben werden. <br><br><img src="https://habrastorage.org/webt/5l/sb/v-/5lsbv-4t7iucyksv39yyqgqmggs.png" alt="Limited VBR" title="Limited vbr"><br><br>  Zun√§chst entwickelten die Ingenieure eine Methode, um die wahrgenommene Bildrate einer Videoanzeige zu verdoppeln, ohne zus√§tzliche Bandbreite zu verbrauchen.  Diese Methode wird als <b>Interlaced-Video bezeichnet</b> .  Grunds√§tzlich wird die H√§lfte des Bildschirms im ersten "Frame" und die andere H√§lfte im n√§chsten "Frame" gesendet. <br><br>  Derzeit wird die Szenenvisualisierung haupts√§chlich mit der <b>Progressive-Scan-Technologie durchgef√ºhrt</b> .  Dies ist eine Methode zum Anzeigen, Speichern oder √úbertragen von Bewegtbildern, bei der alle Zeilen jedes Einzelbilds nacheinander gezeichnet werden. <br><br><img width="747" height="299" src="https://habrastorage.org/webt/bp/z5/u8/bpz5u84cyduvkd6izoangniaevs.png" alt="interlaced und progressiv" title="interlaced und progressiv"><br><br>  Na dann!  Jetzt wissen wir, wie das Bild in digitaler Form dargestellt wird, wie die Farben angeordnet sind, wie viele Bits pro Sekunde wir verwenden, um das Video anzuzeigen, wenn die √úbertragungsgeschwindigkeit konstant (CBR) oder variabel (VBR) ist.  Wir kennen eine bestimmte Aufl√∂sung mit einer bestimmten Bildrate, kennen viele andere Begriffe wie Interlaced-Video, PAR und einige andere. <br><br><h2>  Redundanzentfernung </h2><br>  Es ist bekannt, dass Videos ohne Komprimierung nicht normal verwendet werden k√∂nnen.  Ein st√ºndliches Video mit einer Aufl√∂sung von 720p und einer Frequenz von 30 Bildern pro Sekunde w√ºrde 278 GB belegen.  Diesen Wert erhalten wir durch Multiplikation von 1280 x 720 x 24 x 30 x 3600 (Breite, H√∂he, Bits pro Pixel, FPS und Zeit in Sekunden). <br><br>  Mit <b>verlustfreien Komprimierungsalgorithmen</b> wie DEFLATE (in PKZIP, Gzip und PNG verwendet) wird die erforderliche Bandbreite nicht ausreichend reduziert.  Sie m√ºssen nach anderen M√∂glichkeiten suchen, um Videos zu komprimieren. <br><br>  Hierf√ºr k√∂nnen Sie die Merkmale unserer Vision nutzen.  Wir unterscheiden bessere Helligkeit als Farben.  Ein Video ist eine Reihe von aufeinanderfolgenden Bildern, die sich im Laufe der Zeit wiederholen.  Es gibt kleine Unterschiede zwischen benachbarten Bildern derselben Szene.  Dar√ºber hinaus enth√§lt jeder Rahmen viele Bereiche, die dieselbe (oder eine √§hnliche) Farbe verwenden. <br><br><h2>  Farbe, Helligkeit und unsere Augen </h2><br>  Unsere Augen reagieren empfindlicher auf Helligkeit als auf Farbe.  Sie k√∂nnen sich von diesem Bild √ºberzeugen. <br><br><div style="text-align:center;"><img width="637" height="239" src="https://habrastorage.org/webt/gj/00/kv/gj00kvz32okevvthxqf9_jqi9ng.png" alt="  " title="Helligkeit gegen Farbe"></div><br><br>  Wenn Sie in der linken Bildh√§lfte nicht sehen, dass die Farben der Quadrate <b>A</b> und <b>B</b> tats√§chlich gleich sind, ist dies normal.  Unser Gehirn l√§sst uns mehr auf Hell-Dunkel als auf Farbe achten.  Auf der rechten Seite zwischen den markierten Quadraten befindet sich ein Jumper der gleichen Farbe - daher k√∂nnen wir (d. H. Unser Gehirn) leicht feststellen, dass tats√§chlich die gleiche Farbe vorhanden ist. <blockquote>  Schauen wir uns (vereinfacht) an, wie unsere Augen funktionieren.  Das Auge ist ein komplexes Organ, das aus vielen Teilen besteht.  Am meisten interessieren uns jedoch Zapfen und St√§bchen.  Das Auge enth√§lt etwa 120 Millionen St√§bchen und 6 Millionen Zapfen. <br><br>  Betrachten Sie die Wahrnehmung von Farbe und Helligkeit als separate Funktionen bestimmter Teile des Auges (in der Tat ist alles etwas komplizierter, aber wir werden es vereinfachen).  Stabzellen sind haupts√§chlich f√ºr die Helligkeit verantwortlich, w√§hrend Zapfenzellen f√ºr die Farbe verantwortlich sind.  Je nach enthaltenem Pigment werden die Zapfen in drei Typen unterteilt: S-Zapfen (blau), M-Zapfen (gr√ºn) und L-Zapfen (rot). <br><br>  Da wir viel mehr St√§bchen (Helligkeit) als Zapfen (Farbe) haben, k√∂nnen wir schlie√üen, dass wir die √úberg√§nge zwischen Dunkelheit und Licht besser unterscheiden k√∂nnen als Farben. <br><br><img width="672" height="373" src="https://habrastorage.org/webt/8z/ul/yl/8zulylhy-pkitzl5mubbzdcbswm.jpeg" alt=" " title="Augen Make-up"><br><br><h2>  Kontrastempfindlichkeitsfunktionen </h2><br>  Forscher der experimentellen Psychologie und vieler anderer Gebiete haben viele Theorien des menschlichen Sehens entwickelt.  Und eine von ihnen hei√üt <b>Kontrastempfindlichkeitsfunktionen</b> .  Sie sind mit r√§umlicher und zeitlicher Beleuchtung verbunden.  Kurz gesagt, es geht darum, wie viele √Ñnderungen erforderlich sind, bevor der Beobachter sie sieht.  Beachten Sie den Plural des Wortes "Funktion".  Dies liegt an der Tatsache, dass wir die Empfindlichkeitsfunktionen messen k√∂nnen, um nicht nur Schwarzwei√übilder, sondern auch Farbkontraste zu erzeugen.  Die Ergebnisse dieser Experimente zeigen, dass unsere Augen in den meisten F√§llen empfindlicher auf Helligkeit als auf Farbe reagieren. </blockquote>  Da bekannt ist, dass wir empfindlicher auf die Bildhelligkeit reagieren, k√∂nnen Sie versuchen, diese Tatsache zu nutzen. <br><br><h2>  Farbmodell </h2><br>  Wir haben ein bisschen herausgefunden, wie man mit Farbbildern unter Verwendung des RGB-Schemas arbeitet.  Es gibt noch andere Modelle.  Es gibt ein Modell, das Luminanz von Farbe trennt und als <b>YCbCr bekannt ist</b> .  √úbrigens gibt es andere Modelle, die eine √§hnliche Trennung vornehmen, aber wir werden nur dieses betrachten. <br><br>  In diesem Farbmodell ist <b>Y</b> eine Darstellung der Helligkeit und es werden zwei Farbkan√§le verwendet: <b>Cb</b> (ges√§ttigtes Blau) und <b>Cr</b> (ges√§ttigtes Rot).  YCbCr kann aus RGB erhalten werden, ebenso ist die inverse Transformation m√∂glich.  Mit diesem Modell k√∂nnen wir Bilder in Farbe erstellen, wie wir unten sehen: <br><br><img src="https://habrastorage.org/webt/_y/ee/3x/_yee3xnjjk7xq1mpw_bvndfsnru.png" alt=" ycbcr" title="ycbcr Beispiel"><br><br><h2>  Konvertieren Sie zwischen YCbCr und RGB </h2><br>  Jemand wird Einw√§nde erheben: Wie ist es m√∂glich, alle Farben zu erhalten, wenn Gr√ºn nicht verwendet wird? <br><br>  Konvertieren Sie RGB in YCbCr, um diese Frage zu beantworten.  Wir verwenden die im <b>BT.601-Standard</b> √ºbernommenen Koeffizienten, die von der <b>ITU-R-</b> Einheit empfohlen wurden.  Dieses Ger√§t definiert digitale Videostandards.  Zum Beispiel: Was ist 4K?  Wie sollten die Bildrate, die Aufl√∂sung und das Farbmodell sein? <br><br>  Zuerst berechnen wir die Helligkeit.  Wir verwenden die von der ITU vorgeschlagenen Konstanten und ersetzen die RGB-Werte. <br><br>  <b>Y</b> = 0,299 <b>R</b> + 0,587 <b>G</b> + 0,114 <b>B</b> <br><br>  Nachdem wir die Helligkeit erhalten haben, werden wir die blauen und roten Farben trennen: <br><br>  <b>Cb</b> = 0,564 ( <b>B</b> - <b>Y</b> ) <br><br>  <b>Cr</b> = 0,713 ( <b>R</b> - <b>Y</b> ) <br><br>  Und wir k√∂nnen mit YCbCr auch zur√ºckkonvertieren und sogar gr√ºn werden: <br><br>  <b>R</b> = <b>Y</b> + 1,402 <b>Cr</b> <br><br>  <b>B</b> = <b>Y</b> + 1,772 <b>Cb</b> <br><br>  <b>G</b> = <b>Y</b> - 0,344 <b>Cb</b> - 0,714 <b>Cr</b> <br><br>  In der Regel verwenden Bildschirme (Monitore, Fernseher, Bildschirme usw.) nur das RGB-Modell.  Dieses Modell kann jedoch auf verschiedene Arten organisiert werden: <br><br><img width="550" height="550" src="https://habrastorage.org/webt/rq/is/by/rqisbywjfks_tvcpk7liyvyaiqw.jpeg" alt=" " title="Pixelgeometrie"><br><br><h2>  Farb-Downsampling </h2><br>  Wenn das Bild als eine Kombination aus Helligkeit und Farbe dargestellt wird, k√∂nnen wir eine h√∂here Empfindlichkeit des menschlichen visuellen Systems f√ºr Helligkeit als f√ºr Farbe verwenden, wenn wir Informationen selektiv l√∂schen.  Farb-Downsampling ist eine Methode zum Codieren von Bildern mit einer niedrigeren Aufl√∂sung f√ºr Farbe als f√ºr Helligkeit. <br><br><div style="text-align:center;"><img width="715" height="361" src="https://habrastorage.org/webt/ce/ee/9_/ceee9_x9vh8b_vpklek7rnhvoz0.png" alt="ycbcr  " title="Ycbcr-Subsampling-Berechtigungen"></div><br><br>  Wie akzeptabel ist es, die Farbaufl√∂sung zu reduzieren?!  Es stellt sich heraus, dass es bereits einige Schemata gibt, die beschreiben, wie Aufl√∂sung und Zusammenf√ºhrung zu handhaben sind <nobr>(Endfarbe = Y + Cb + Cr).</nobr> <br><br>  Diese Schemata sind als <b>Unterabtastsysteme bekannt</b> und werden in Form eines 3-fachen Verh√§ltnisses ausgedr√ºckt - <b>a</b> : <b>x</b> : <b>y</b> , das die Anzahl der Abtastungen von Luminanz- und Farbdifferenzsignalen bestimmt. <br><br>  <b>a</b> - horizontale Standardabtastung (normalerweise gleich 4) <br>  <b>x</b> - die Anzahl der Farbmuster in der ersten Pixelreihe (horizontale Aufl√∂sung relativ zu <b>a</b> ) <br>  <b>y</b> ist die Anzahl der Farbmuster√§nderungen zwischen der ersten und der zweiten Pixelreihe. <blockquote>  Die Ausnahme ist <b>4</b> : <b>1</b> : <b>0</b> , wodurch ein Farbmuster in jedem 4 √ó 4-Helligkeitsaufl√∂sungsblock bereitgestellt wird. </blockquote>  G√§ngige Schemata, die in modernen Codecs verwendet werden: <br><br><ul><li>  <b>4</b> : <b>4</b> : <b>4</b> (ohne Downsampling) </li><li>  <b>4</b> : <b>2</b> : <b>2</b> </li><li>  <b>4</b> : <b>1</b> : <b>1</b> </li><li>  <b>4</b> : <b>2</b> : <b>0</b> </li><li>  <b>4</b> : <b>1</b> : <b>0</b> </li><li>  <b>3</b> : <b>1</b> : <b>1</b> </li></ul><blockquote><h3>  YCbCr 4: 2: 0 - Zusammenf√ºhrungsbeispiel </h3><br>  Hier ist das kombinierte Bildfragment mit YCbCr 4: 2: 0.  Bitte beachten Sie, dass wir nur 12 Bit pro Pixel ausgeben. <br><br><div style="text-align:center;"><img width="555" height="315" src="https://habrastorage.org/webt/yq/fj/oi/yqfjoiza1glbn0rcj4nl0gfrsrm.png" alt="YCbCr 4:2:0" title="YCbCr 4: 2: 0"></div></blockquote>  So sieht dasselbe Bild aus, das von den Haupttypen der Farbunterabtastung codiert wurde.  Die erste Zeile ist das endg√ºltige YCbCr, die untere Zeile zeigt die Farbaufl√∂sung.  Aufgrund des geringen Qualit√§tsverlustes sehr gute Ergebnisse. <br><br><img src="https://habrastorage.org/webt/q5/g_/-f/q5g_-f0w41riagjrnt3msc0a-us.jpeg" alt="  " title="Beispiele f√ºr Chroma-Unterabtastungen"><br><br>  Denken Sie daran, wir haben 278 GB Speicherplatz f√ºr eine stundenlange Videodatei mit einer Aufl√∂sung von 720p und 30 Bildern pro Sekunde gez√§hlt.  Wenn wir YCbCr 4: 2: 0 verwenden, wird diese Gr√∂√üe um die H√§lfte reduziert - 139 GB.  Bisher ist es noch weit von einem akzeptablen Ergebnis entfernt. <br><br>  Sie k√∂nnen das YCbCr-Histogramm selbst mit FFmpeg abrufen.  In diesem Bild √ºberwiegt Blau gegen√ºber Rot, was auf dem Histogramm selbst deutlich erkennbar ist. <br><br><img src="https://habrastorage.org/webt/gj/wa/lr/gjwalry6l_oeqssuishxlxtjrae.png"><blockquote><h4>  Farbe, Helligkeit, Farbskala - Video√ºberpr√ºfung </h4><br>  Es wird empfohlen, dieses tolle Video anzuschauen.  Dies erkl√§rt, was Helligkeit ist, und tats√§chlich sind alle Punkte in Bezug auf Helligkeit und Farbe dar√ºber angeordnet. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Ymt47wXUDEU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></blockquote><h2>  Rahmentypen </h2><br>  Wir ziehen weiter.  Versuchen wir, die Redundanz rechtzeitig zu beseitigen.  Aber zuerst definieren wir eine grundlegende Terminologie.  Angenommen, wir haben einen Film mit 30 Bildern pro Sekunde. Hier sind die ersten 4 Bilder: <br><br><img width="64" height="64" src="https://habrastorage.org/webt/iv/6n/5x/iv6n5xrkspmna4ysdwmx-geaavg.png" alt="ball 1" title="Ball 1"><img width="64" height="64" src="https://habrastorage.org/webt/yx/pm/jb/yxpmjbgftbludyfwblix9e51mie.png" alt="ball 2" title="Ball 2"><img width="64" height="64" src="https://habrastorage.org/webt/gp/wk/no/gpwknos5yniugafyfj-hk_a1pkk.png" alt="ball 3" title="Ball 3"><img width="64" height="64" src="https://habrastorage.org/webt/-k/22/81/-k228175bwr2raujpfa-72p7hnm.png" alt="ball 4" title="Ball 4"><br><br>  Wir k√∂nnen viele Wiederholungen in Frames sehen: zum Beispiel einen blauen Hintergrund, der sich nicht von Frame zu Frame √§ndert.  Um dieses Problem zu l√∂sen, k√∂nnen wir sie abstrakt als drei Rahmentypen klassifizieren. <br><br><h3>  I-Frame ( <b>I</b> ntro Frame) </h3><br>  Der I-Frame (Referenzframe, Schl√ºsselframe, interner Frame) ist autonom.  Unabh√§ngig davon, was visualisiert werden muss, ist der I-Frame tats√§chlich eine statische Fotografie.  Der erste Frame ist normalerweise ein I-Frame, aber wir werden regelm√§√üig I-Frames weit entfernt von den ersten Frames beobachten. <br><br><img width="64" height="64" src="https://habrastorage.org/webt/iv/6n/5x/iv6n5xrkspmna4ysdwmx-geaavg.png" alt="ball 1" title="Ball 1"><br><br><h3>  P-Frame ( <b>P</b> redicted Frame) </h3><br>  Der P-Frame (Predicted Frame) nutzt die Tatsache, dass fast immer das aktuelle Bild mit dem vorherigen Frame abgespielt werden kann.  Zum Beispiel ist im zweiten Frame die einzige √Ñnderung der Vorw√§rtsball.  Wir k√∂nnen Frame 2 erhalten, indem wir nur Frame 1 leicht modifizieren und nur den Unterschied zwischen diesen Frames verwenden.  Informationen zum Erstellen von Frame 2 finden Sie in Frame 1, der vorangestellt ist. <br><br><img width="64" height="64" src="https://habrastorage.org/webt/iv/6n/5x/iv6n5xrkspmna4ysdwmx-geaavg.png" alt="ball 1" title="Ball 1">  ‚Üê <img width="64" height="64" src="https://habrastorage.org/webt/3o/-a/-_/3o-a-_o0jmv5vrg6wu2jmma0xpo.png" alt=" 1" title="Ball 1"><br><br><h3>  B-Frame ( <b>B</b> i-pr√§diktiver Frame) </h3><br>  Was ist mit Links, die nicht nur auf fr√ºhere, sondern auch auf zuk√ºnftige Frames verweisen, um eine noch bessere Komprimierung zu erzielen?  Dies ist im Grunde ein B-Frame (bidirektionaler Frame). <br><br><img width="64" height="64" src="https://habrastorage.org/webt/iv/6n/5x/iv6n5xrkspmna4ysdwmx-geaavg.png" alt="ball 1" title="Ball 1">  ‚Üê <img width="64" height="64" src="https://habrastorage.org/webt/3o/-a/-_/3o-a-_o0jmv5vrg6wu2jmma0xpo.png" alt=" 1" title="Ball 1">  ‚Üí <img width="64" height="64" src="https://habrastorage.org/webt/gp/wk/no/gpwknos5yniugafyfj-hk_a1pkk.png" alt="ball 3" title="Ball 3"><br><br><h2>  Zwischenr√ºckzug </h2><br>  Diese Rahmentypen werden verwendet, um die beste Komprimierung zu erzielen.  Wir werden im n√§chsten Abschnitt diskutieren, wie dies geschieht.  In der Zwischenzeit stellen wir fest, dass der I-Frame im Hinblick auf den Speicher am ‚Äûteuersten‚Äú ist, der P-Frame viel billiger ist, aber der B-Frame die rentabelste Option f√ºr Video ist. <br><br><img src="https://habrastorage.org/webt/kb/tq/iv/kbtqivaoykzul7air0x-bi4p0nk.png" alt="  " title="Beispiel f√ºr Rahmentypen"><br><br><h2>  Zeitliche Redundanz (Inter-Frame-Vorhersage) </h2><br>  Schauen wir uns an, welche M√∂glichkeiten wir haben, um Zeitwiederholungen zu minimieren.  Diese Art der Redundanz kann mit den Methoden der gegenseitigen Vorhersage gel√∂st werden. <br><br>  Wir werden versuchen, so wenig Bits wie m√∂glich f√ºr die Codierung einer Folge von Frames 0 und 1 zu verwenden. <br><br><img width="489" height="249" src="https://habrastorage.org/webt/_r/or/uu/_roruusxx8nchbiamc6hxqxv7ns.png" alt=" " title="Originalrahmen"><br><br>  Wir k√∂nnen <b>subtrahieren</b> , nur Bild 1 von Bild 0 subtrahieren. Wir erhalten Bild 1, wir verwenden nur die Differenz zwischen diesem und dem vorherigen Bild, in der Tat codieren wir nur den resultierenden Rest. <br><br><img width="334" height="330" src="https://habrastorage.org/webt/lz/1q/vm/lz1qvmad3_cjsaulbo4dy-qfwuc.png" alt=" " title="Delta-Frames"><br><br>  Aber was ist, wenn ich Ihnen sage, dass es eine noch bessere Methode gibt, die noch weniger Bits verwendet?  Lassen Sie uns zuerst Frame 0 in ein klares Raster von Bl√∂cken aufteilen.  Und dann versuchen wir, die Bl√∂cke von Frame 0 mit Frame 1 zu vergleichen. Mit anderen Worten, wir bewerten die Bewegung zwischen Frames. <blockquote>  Aus Wikipedia - <b>Bewegungskompensation blockieren</b> <br><br>  Die Blockbewegungskompensation unterteilt den aktuellen Frame in disjunkte Bl√∂cke, und der Bewegungskompensationsvektor gibt den Ursprung der Bl√∂cke an (ein h√§ufiges Missverst√§ndnis ist, dass der <i>vorherige</i> Frame in disjunkte Bl√∂cke unterteilt ist und Bewegungskompensationsvektoren angeben, wohin diese Bl√∂cke gehen. Tats√§chlich wird jedoch nicht der vorherige analysiert Der Rahmen und der n√§chste zeigen nicht, wo sich die Bl√∂cke bewegen, sondern wo sie herkommen.  Typischerweise √ºberlappen sich die Quellbl√∂cke im Quellrahmen.  Einige Videokomprimierungsalgorithmen erfassen den aktuellen Frame aus Teilen nicht nur eines, sondern mehrerer zuvor √ºbertragener Frames. </blockquote><img width="489" height="249" src="https://habrastorage.org/webt/mm/0n/r3/mm0nr3fmjnihn1xgvgeip8lnqcy.png" alt=" " title="Delta-Frames"><br><br>  Im Auswertungsprozess sehen wir, dass sich der Ball von <nobr>(</nobr> <b>x</b> = 0, <b>y</b> = 25) nach <nobr>(</nobr> <b>x</b> = 6, <b>y</b> = 26) bewegt hat. Die Werte von <b>x</b> und <b>y</b> bestimmen den Bewegungsvektor.  Ein weiterer Schritt, den wir unternehmen k√∂nnen, um die Bits zu speichern, besteht darin, nur die Differenz der Bewegungsvektoren zwischen der letzten Position des Blocks und der vorhergesagten Position zu codieren, sodass der endg√ºltige Bewegungsvektor <nobr>(x = 6-0 = 6, y = 26-25 = 1) ist.</nobr> <br><br>  In einer realen Situation w√ºrde dieser Ball in <b>n</b> Bl√∂cke unterteilt, aber dies √§ndert nichts am Wesen der Sache. <br><br>  Objekte im Rahmen bewegen sich in drei Dimensionen. Wenn sich der Ball also bewegt, kann er optisch kleiner werden (oder mehr, wenn er sich in Richtung des Betrachters bewegt).  Es ist normal, dass es keine perfekte √úbereinstimmung zwischen Bl√∂cken gibt.  Hier ist eine kombinierte Ansicht unserer Einsch√§tzung und des realen Bildes. <br><br><img width="374" height="361" src="https://habrastorage.org/webt/g2/0v/lf/g20vlfrxrjfd2z5jopsrwtjeayu.png" alt=" " title="Bewegungssch√§tzung"><br><br>  Wir sehen jedoch, dass bei Anwendung der Bewegungssch√§tzung die Daten f√ºr die Codierung merklich geringer sind als bei Verwendung der einfacheren Methode zur Berechnung des Deltas zwischen Frames. <br><br><img width="489" height="249" src="https://habrastorage.org/webt/al/yh/um/alyhumga0r2vz1sduqqpbbfwsf0.png" alt="   " title="Bewegungssch√§tzung Delta"><br><br><h2>  Wie die eigentliche Bewegungskompensation aussehen wird </h2><br>  Diese Technik gilt sofort f√ºr alle Bl√∂cke.  Oft wird unser bedingter Bewegungsball in mehrere Bl√∂cke gleichzeitig unterteilt. <br><br><img src="https://habrastorage.org/webt/qd/r9/zi/qdr9zittbih-suwng4vhcktrini.png" alt="    " title="reale Bewegungskompensation"><br><br>      ,  <a href="https://jupyter.org/">Jupyter</a> . <br><br>             <a href="https://www.ffmpeg.org/">ffmpeg</a> . <br><br><img width="730" height="434" src="https://habrastorage.org/webt/8p/fz/y4/8pfzy4_x_u8rhx0e-qdwggcfz18.png" alt="  ( )   ffmpeg" title="  ( )   ffmpeg"><br><br>    <a href="https://software.intel.com/en-us/video-pro-analyzer">Intel Video Pro Analyzer</a> ( ,     ,      ). <br><br><img src="https://habrastorage.org/webt/gi/xv/2x/gixv2xsurl-vpyoqppvr_w9qtc4.png" alt="    " title="    "><br><br><h2>   ( ) </h2><br>      ,     . <br><br><img width="550" height="312" src="https://habrastorage.org/webt/cy/rt/wu/cyrtwua6vufuvoommyb-3ggyfu0.png"><br><br>    .          . <br><br><img width="277" height="156" src="https://habrastorage.org/webt/-i/yr/8r/-iyr8rtaba0yxiwgvrp0xzhpghg.png"><br><br>  I-.       ,    .    .      ,  ,     -  . <br><br><img width="278" height="413" src="https://habrastorage.org/webt/pi/2l/bg/pi2lbg2js6klz5_qepjcqtgevqq.png"><br><br> ,       .  ,         . <br><br><img width="246" height="404" src="https://habrastorage.org/webt/wy/qe/zt/wyqezt5qtss2h37wim8itgpgobe.png"><br><br>      .         (  ),      .     ,           . <br><br><img src="https://habrastorage.org/webt/ks/at/av/ksatavivnluz7pgxmawvtgixu1y.png"><br><br>      ,            ffmpeg.           ffmpeg. <br><br><img src="https://habrastorage.org/webt/70/pd/tq/70pdtqqlhlx-atda44tfwrygtna.png" alt="  ()   ffmpeg" title="  ( )   ffmpeg"><br><br>     Intel Video Pro Analyzer (    ,        10 ,       ). <br><br><img src="https://habrastorage.org/webt/_e/3x/f-/_e3xf-wuez64lr6x89oxhozl0a4.png" alt="Intra-Prediction Intel Video Pro Analyzer" title="  Intel Pro Pro"><br><br><hr><br><h3>  : <a href="https://habr.com/ru/company/edison/blog/480430/">  </a> </h3><br><hr><br><br> <a href="https://habr.com/ru/company/edison/blog/485460/"><img align="right" width="404" height="150" src="https://habrastorage.org/webt/2b/9i/gm/2b9igmgpbxunecpetjj6hhqsa9m.png"></a> <br clear="left"><h4>  Lesen Sie auch den Blog <br>  EDISON Unternehmen: </h4><br>  <a href="https://habr.com/ru/company/edison/blog/485460/"><b>20 Bibliotheken f√ºr</b></a> <a href="https://habr.com/ru/company/edison/blog/485460/"><b><br></b></a>  <a href="https://habr.com/ru/company/edison/blog/485460/"><b>spektakul√§re iOS-Anwendung</b></a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de481418/">https://habr.com/ru/post/de481418/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de481394/index.html">Neuerstellung in einem neuen Spiel, was wir f√ºr das alte geliebt haben</a></li>
<li><a href="../de481398/index.html">Grundlegende Linux-Befehle f√ºr Tester und mehr</a></li>
<li><a href="../de481402/index.html">Versp√§tete Implementierung des Durchlaufens eines untergeordneten Baums der QObject-Klasse</a></li>
<li><a href="../de481406/index.html">Tiefes Eintauchen in die Investition von Ilona Mask</a></li>
<li><a href="../de481416/index.html">Ank√ºndigung des zweiten AWS Meetups in Minsk (13.02.2020)</a></li>
<li><a href="../de481420/index.html">15 besten und gr√∂√üten Icon-Bibliotheken</a></li>
<li><a href="../de481424/index.html">Benutzerdefinierte Steuerelemente im Winkel</a></li>
<li><a href="../de481426/index.html">Vorteile der Microservices-Architektur f√ºr die Softwareentwicklung</a></li>
<li><a href="../de481428/index.html">Spiel f√ºr Programmierer FuncBall</a></li>
<li><a href="../de481430/index.html">So stellen Sie eine wissenschaftliche Zeitschrift in der HAC-Liste wieder her</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>