<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ú°Ô∏è üòø ‚ùå Aprendizaje profundo de refuerzo: ping pong de p√≠xel sin procesar ‚õàÔ∏è üçå üë®üèø‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Este es un art√≠culo muy retrasado sobre el aprendizaje por refuerzo (RL). ¬°RL es un tema genial! 

 Es posible que sepa que las computadoras ahora pue...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Aprendizaje profundo de refuerzo: ping pong de p√≠xel sin procesar</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/439674/">  Este es un art√≠culo muy retrasado sobre el aprendizaje por refuerzo (RL).  ¬°RL es un tema genial! <br><br>  Es posible que sepa que las computadoras ahora pueden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aprender autom√°ticamente a jugar juegos ATARI</a> (¬°obteniendo p√≠xeles de juego en bruto en la entrada!).  Vencieron a los campeones del mundo en el juego de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Go</a> , los cuatro patas virtuales aprenden a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">correr y saltar</a> , y los robots aprenden a realizar tareas complejas de manipulaci√≥n que desaf√≠an la programaci√≥n expl√≠cita.  Resulta que todos estos logros no est√°n completos sin RL.  Tambi√©n estuve interesado en RL durante el a√±o pasado: trabaj√© con <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el</a> libro de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Richard Sutton (aprox. Referencia: reemplazado)</a> , le√≠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el</a> curso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">David Silver</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">asist√≠ a las conferencias de John Schulman</a> , escrib√≠ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la biblioteca de RL en Javascript</a> e hice pr√°cticas en DeepMind en el verano, trabajando en un grupo DeepRL, y m√°s recientemente, en el desarrollo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">OpenAI Gym</a> , es el nuevo kit de herramientas RL.  As√≠ que, por supuesto, he estado en esta ola durante al menos un a√±o, pero a√∫n no me he molestado en escribir una nota sobre por qu√© RL es de gran importancia, de qu√© se trata, c√≥mo se desarrolla todo. <br><br><img src="https://habrastorage.org/webt/vr/ir/gu/vrirgukar82fw0cg6wbnsklagum.png"><br>  <i>Ejemplos de uso de Deep Q-Learning.</i>  <i>De izquierda a derecha: la red neuronal juega ATARI, la red neuronal juega AlphaGo, el robot pliega Lego, las carreras virtuales de cuatro patas alrededor de obst√°culos virtuales.</i> <br><a name="habracut"></a><br>  Es interesante reflexionar sobre la naturaleza del progreso reciente en RL.  Me gustar√≠a se√±alar cuatro factores separados que afectan el desarrollo de la IA: <br><br><ol><li>  Velocidad de c√≥mputo (GPU, dispositivos especiales ASIC, ley de Moore) </li><li>  Datos suficientes en forma utilizable (por ejemplo, ImageNet) </li><li>  Algoritmos (investigaci√≥n e ideas, por ejemplo, backprop, CNN, LSTM) </li><li>  Infraestructura (Linux, TCP / IP, Git, ROS, PR2, AWS, AMT, TensorFlow, etc.). </li></ol><br>  Al igual que en la visi√≥n por computadora, el progreso en RL se est√° moviendo ... aunque no tanto como podr√≠a parecer.  Por ejemplo, en visi√≥n artificial, la red neuronal AlexNet 2012 es una versi√≥n aumentada en profundidad y anchura de la red neuronal ConvNets de los a√±os noventa.  Del mismo modo, ATARI Deep Q-Learning 2013 es una implementaci√≥n del algoritmo est√°ndar Q-Learning que puedes encontrar en el libro cl√°sico de Richard Sutton de 1998.  Adem√°s, AlphaGo utiliza la t√©cnica Policy Gradient y la b√∫squeda de √°rbol de Monte Carlo (MCTS) tambi√©n son ideas antiguas o sus combinaciones.  Por supuesto, se necesitan muchas habilidades y paciencia para que funcionen, y se han desarrollado muchas configuraciones dif√≠ciles sobre algoritmos antiguos.  <b>Pero en una primera aproximaci√≥n, el principal impulsor del progreso reciente no son los nuevos algoritmos e ideas, sino la intensificaci√≥n de los c√°lculos, los datos suficientes y la infraestructura madura.</b> <br><br>  Ahora de vuelta a RL.  Muchas personas no pueden creer que podemos ense√±arle a una computadora c√≥mo jugar juegos ATARI a nivel humano usando p√≠xeles sin formato desde cero y usando el mismo algoritmo de autoaprendizaje.  Al mismo tiempo, cada vez que siento una brecha: lo m√°gico que parece y lo simple que realmente es por dentro. <br><br>  El enfoque b√°sico que utilizamos es bastante tonto.  Sea como fuere, me gustar√≠a presentarles la t√©cnica de Gradiente de Pol√≠tica (PG), nuestra opci√≥n predeterminada favorita para resolver problemas con RL en este momento.  Puede que tenga curiosidad por qu√©, en cambio, no puedo imaginar DQN, que es un algoritmo RL alternativo y mejor conocido que tambi√©n se usa en el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entrenamiento ATARI</a> .  Resulta que aunque Q-Learning es conocido, no es tan perfecto.  La mayor√≠a de las personas optan por usar Policy Gradient, incluidos los autores del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">art√≠culo</a> original de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DQN</a> , que han demostrado que con un buen ajuste, Policy Gradient funciona incluso mejor que Q-Learning.  PG es preferible porque es expl√≠cito: hay una pol√≠tica clara y un enfoque coherente que optimiza directamente las recompensas esperadas.  Como ejemplo, aprenderemos a jugar ATARI Pong: desde cero, desde p√≠xeles sin procesar a trav√©s de un Gradiente de pol√≠tica con una red neuronal.  Y pondremos todo esto en 130 l√≠neas de Python.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Enlace general</a> ) Veamos c√≥mo se hace esto. <br><br><img src="https://habrastorage.org/webt/yy/fk/hg/yyfkhg3xutt4gsualf7l6o0sa4i.gif"><br><img src="https://habrastorage.org/webt/mw/rq/2w/mwrq2wthkobnxuqohcfovs1lgei.jpeg"><br>  <i>Arriba: Ping Pong.</i>  <i>A continuaci√≥n: Presentaci√≥n de ping-pong como un caso especial del proceso de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">toma de decisiones</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Markov (MDP)</a> : cada v√©rtice del gr√°fico corresponde a un cierto estado del juego, y los bordes determinan las probabilidades de transici√≥n a otros estados.</i>  <i>Cada costilla tambi√©n determina la recompensa.</i>  <i>El objetivo es encontrar el mejor camino desde cualquier estado para maximizar la recompensa</i> <br><br>  Jugar Ping Pong es un gran ejemplo de un desaf√≠o RL.  En la versi√≥n ATARI 2600 jugaremos una raqueta nosotros mismos.  Otra raqueta est√° controlada por un algoritmo incorporado.  Necesitamos golpear la pelota para que el otro jugador no tenga tiempo de golpearla.  Espero que no haya necesidad de explicar qu√© es Ping Pong.  En un nivel bajo, el juego funciona de la siguiente manera: obtenemos un marco de imagen, una matriz de bytes 210x160x3, y decidimos si queremos mover la raqueta ARRIBA o ABAJO.  Es decir, solo tenemos dos opciones para administrar el juego.  Despu√©s de cada elecci√≥n, el simulador de juego realiza su acci√≥n y nos da una recompensa: ya sea +1 recompensa si la pelota pas√≥ la raqueta del oponente, o -1 si la perdimos.  De lo contrario, 0. Y, por supuesto, nuestro objetivo es mover la raqueta para que obtengamos la mayor recompensa posible. <br><br>  Al considerar una soluci√≥n, recuerde que intentaremos hacer tan pocos supuestos sobre el pong, porque no es particularmente importante en la pr√°ctica.  Tenemos mucho m√°s en cuenta en las tareas a gran escala, como la manipulaci√≥n de robots, el ensamblaje y la navegaci√≥n.  Pong es solo un divertido caso de prueba de juguete con el que jugamos mientras descubrimos c√≥mo escribir sistemas de IA muy generales que alg√∫n d√≠a puedan realizar tareas √∫tiles arbitrarias. <br><br>  <b>Red neuronal como una pol√≠tica RL</b> .  En primer lugar, vamos a determinar la llamada pol√≠tica que implementa nuestro jugador (o "agente").  ((*) "Agente", "medio ambiente" y "pol√≠tica de agente" son t√©rminos est√°ndar de la teor√≠a RL).  La funci√≥n pol√≠tica en nuestro caso es una red neuronal.  Ella aceptar√° el estado del juego en la entrada y en la salida decidir√° qu√© hacer: moverse hacia arriba o hacia abajo.  Como nuestro bloque de c√°lculos simple favorito, utilizaremos una red neuronal de dos capas que toma p√≠xeles de imagen sin procesar (un total de 100.800 n√∫meros (210 * 160 * 3)) y produce un solo n√∫mero que indica la probabilidad de mover la raqueta hacia arriba.  Tenga en cuenta que el uso de la pol√≠tica estoc√°stica es est√°ndar, lo que significa que solo producimos la probabilidad de un movimiento ascendente.  Para obtener el movimiento real, utilizaremos esta probabilidad.  La raz√≥n de esto se aclarar√° cuando hablemos de capacitaci√≥n. <br><br><img src="https://habrastorage.org/webt/cm/n4/ap/cmn4apvesxvu5d8konqytnbk0cm.png"><br>  <i>Nuestra funci√≥n pol√≠tica consiste en una red neuronal completamente conectada de 2 capas</i> <br><br>  M√°s espec√≠ficamente, suponga que en la entrada obtenemos un vector X, que contiene un conjunto de p√≠xeles preprocesados.  Luego debemos calcular usando python \ numpy: <br><br><pre><code class="python hljs">h = np.dot(W1, x) <span class="hljs-comment"><span class="hljs-comment"># compute hidden layer neuron activations h[h&lt;0] = 0 # ReLU nonlinearity: threshold at zero logp = np.dot(W2, h) # compute log probability of going up p = 1.0 / (1.0 + np.exp(-logp)) # sigmoid function (gives probability of going up)</span></span></code> </pre> <br>  En este fragmento, W1 y W2 son dos matrices que inicializamos al azar.  No usamos sesgos, porque quer√≠amos hacerlo.  Tenga en cuenta que al final usamos la no linealidad del sigmoide, lo que reduce la probabilidad de salida al rango [0,1].  Intuitivamente, las neuronas en una capa oculta (cuyos pesos se encuentran en W1) pueden detectar varios escenarios de juego (por ejemplo, la pelota est√° en la parte superior y nuestra raqueta en el medio), y los pesos en W2 pueden decidir si debemos subir en cada caso o abajo.  Los primeros W1 y W2 aleatorios, por supuesto, al principio causar√°n espasmos y convulsiones en nuestro neuro-jugador, lo que lo equipara a un autista imb√©cil a los mandos de un avi√≥n.  Entonces, la √∫nica tarea ahora es encontrar W1 y W2, ¬°lo que conduce a un buen juego! <br><br>  Hay una observaci√≥n sobre el preprocesamiento de p√≠xeles: idealmente, debe transferir al menos 2 fotogramas a la red neuronal para que pueda detectar el movimiento.  Pero para simplificar la situaci√≥n, aplicaremos la diferencia de dos cuadros.  Es decir, restaremos los cuadros actuales y anteriores y solo entonces aplicaremos la diferencia a la entrada de la red neuronal. <br><br>  <b>Suena como algo imposible.</b>  En este punto, me gustar√≠a que aprecie lo complejo que es el problema de RL.  Obtenemos 100 800 n√∫meros (210x160x 3) y lo enviamos a nuestra red neuronal que implementa la pol√≠tica del jugador (que, por cierto, incluye f√°cilmente alrededor de un mill√≥n de par√°metros en las matrices W1 y W2).  Supongamos que en alg√∫n momento decidimos subir.  El simulador de juego puede responder que esta vez obtendremos 0 premios y nos dar√° otros 100 800 n√∫meros para el pr√≥ximo cuadro.  ¬°Podemos repetir este proceso cientos de veces antes de obtener una recompensa distinta de cero!  Por ejemplo, supongamos que finalmente tenemos una recompensa +1.  Esto es maravilloso, pero ¬øc√≥mo podemos decir, qu√© nos llev√≥ a esto?  ¬øFue esta la acci√≥n que hicimos ahora?  O tal vez 76 cuadros de vuelta?  ¬øO tal vez esto se asoci√≥ primero con el cuadro 10, y luego hicimos algo bien en el cuadro 90?  ¬øY c√≥mo descubrimos cu√°l de los millones de "bol√≠grafos" para torcer para lograr un √©xito a√∫n mayor en el futuro?  Llamamos a esto la tarea de determinar el coeficiente de confianza en ciertas acciones.  En el caso espec√≠fico con el pong, sabemos que obtenemos +1 si la pelota pasa al oponente.  La verdadera raz√≥n es que accidentalmente pateamos la pelota a lo largo de un buen camino unos pocos cuadros hacia atr√°s, y cada acci√≥n posterior que realizamos no la afect√≥ en absoluto.  En otras palabras, nos enfrentamos a un problema computacional muy complejo, y todo parece bastante sombr√≠o. <br><br>  <b>Entrenamiento con un profesor.</b>  Antes de profundizar en el gradiente de la pol√≠tica (PG), me gustar√≠a recordar brevemente la ense√±anza con un maestro, porque, como veremos, RL es muy similar.  Consulte la tabla a continuaci√≥n.  En la ense√±anza ordinaria con un maestro, transferiremos la imagen a la red y recibiremos en la salida algunas probabilidades num√©ricas para las clases.  Por ejemplo, en nuestro caso tenemos dos clases: ARRIBA y ABAJO.  Utilizo las probabilidades logar√≠tmicas (-1.2, -0.36) en lugar de las probabilidades en el formato 30% y 70%, porque optimizamos la probabilidad logar√≠tmica de la clase (o etiqueta) correcta.  Esto hace que los c√°lculos matem√°ticos sean m√°s elegantes y equivale a optimizar solo la probabilidad, porque el logaritmo es mon√≥tono. <br><br>  En el entrenamiento con un maestro, tendremos acceso instant√°neo a la clase correcta (etiqueta).  En la etapa de entrenamiento, nos dir√°n exactamente qu√© paso correcto se necesita en este momento (digamos que es ARRIBA, etiqueta 0), aunque la red neuronal puede pensar de manera diferente.  Por lo tanto, calculamos el gradiente <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mtext" id="MJXp-Span-2">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-5"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">l </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-7"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a </font></font></span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-9" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-10"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">W</font></font></span></span></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13"><font style="vertical-align: inherit;">o </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15"><font style="vertical-align: inherit;">p </font></span><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17"><font style="vertical-align: inherit;">y </font></span><span class="MJXp-mo" id="MJXp-Span-18" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;">= </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19"><font style="vertical-align: inherit;">U </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20"><font style="vertical-align: inherit;">P </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22"><font style="vertical-align: inherit;">m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;">d </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;">x </font></span><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">)</font></span></font><span class="MJXp-mtext" id="MJXp-Span-11">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-12"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-16" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-18" style="margin-left: 0.333em; margin-right: 0.333em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-19"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-20"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mtext" id="MJXp-Span-21">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-24"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-26" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="28.327ex" height="2.66ex" viewBox="0 -832 12196.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6E" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-61" x="850" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-62" x="1380" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6C" x="1809" y="0"></use><g transform="translate(2108,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-57" x="748" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6C" x="3728" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6F" x="4027" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-67" x="4512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-70" x="4993" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-28" x="5496" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-79" x="5886" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-3D" x="6661" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-55" x="7717" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-50" x="8485" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6D" x="9486" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="10365" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-64" x="10710" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-78" x="11234" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-29" x="11806" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> \ nabla_ {W} \ log p (y = UP \ mid x) </script>  para ajustar la configuraci√≥n de red.  Este gradiente solo nos dice c√≥mo deber√≠amos cambiar cada uno de nuestros millones de par√°metros para que la red sea un poco m√°s probable que prediga UP en la misma situaci√≥n.  Por ejemplo, uno de un mill√≥n de par√°metros en la red puede tener un gradiente de -2.1, lo que significa que si aumentamos este par√°metro en un valor positivo peque√±o (por ejemplo, 0.001), la probabilidad logar√≠tmica de UP disminuir√° en 2.1 * 0.001.  (disminuci√≥n debido al signo negativo).  Si aplicamos el gradiente y luego actualizamos el par√°metro usando el algoritmo de retropropagaci√≥n, entonces, s√≠, nuestra red dar√° una alta probabilidad de UP cuando vea la misma imagen o una muy similar en el futuro. <br><br><img src="https://habrastorage.org/webt/-p/r6/bu/-pr6bumeyrk2xdf9nuoulm5oflw.png"><br><br>  <b>Gradientes de Pol√≠tica (PG)</b> .  Bien, pero ¬øqu√© hacemos si no tenemos la etiqueta de entrenamiento de refuerzo correcta?  Aqu√≠ hay una soluci√≥n para PG (consulte el diagrama a continuaci√≥n nuevamente).  Nuestra red neuronal calcul√≥ la probabilidad de un aumento del 30% (logprob -1.2) y DOWN en un 70% (logprob -0.36).  Ahora hacemos una selecci√≥n de esta distribuci√≥n y especificamos qu√© acci√≥n haremos.  Por ejemplo, eligieron ABAJO y enviaron esta acci√≥n al simulador de juego.  En este punto, preste atenci√≥n a un hecho interesante: podr√≠amos calcular inmediatamente y aplicar el gradiente para la acci√≥n DOWN, como lo hicimos al ense√±ar con un maestro, y as√≠ hacer que la red sea m√°s propensa a realizar la acci√≥n DOWN en el futuro.  Por lo tanto, podemos apreciar y recordar de inmediato este gradiente.  Pero el problema es que por el momento a√∫n no lo sabemos, ¬øes bueno bajar?  <b>¬°Pero lo m√°s interesante es que podemos esperar un poco y aplicar el gradiente m√°s tarde!</b>  En Pong, podemos esperar hasta el final del juego, luego tomar la recompensa que recibimos (ya sea +1 si ganamos o -1 si perdimos) e ingresarla como un factor para el gradiente.  Entonces, si introducimos -1 para la probabilidad DOWN y hacemos la propagaci√≥n hacia atr√°s, reconstruiremos los par√°metros de la red para que sea menos probable que realice la acci√≥n DOWN en el futuro cuando encuentre la misma imagen, ya que la adopci√≥n de esta acci√≥n nos llev√≥ a perder el juego.  Es decir, tendremos que recordar de alguna manera todas las acciones (entradas y salidas de la red neuronal) en un episodio del juego y, en base a esta matriz, torcer la red neuronal casi de la misma manera que en la ense√±anza con un maestro. <br><br><img src="https://habrastorage.org/webt/hj/jr/tg/hjjrtgpmkweozlnidrhixsf7jfs.png"><br><br>  Y eso es todo lo que se necesita: tenemos una pol√≠tica estoc√°stica que elige acciones, y luego, en el futuro, se alientan las acciones que finalmente conducen a buenos resultados, y no se alientan las acciones que conducen a malos resultados.  Adem√°s, la recompensa ni siquiera deber√≠a ser +1 o -1 si finalmente ganamos el juego.  Puede ser un valor arbitrario del mismo significado.  Por ejemplo, si todo funciona muy bien, entonces la recompensa podr√≠a ser 10.0, que luego usaremos como un gradiente para comenzar la propagaci√≥n hacia atr√°s.  Esta es la belleza de las redes neuronales.  Usarlos puede parecer un enga√±o: se le permite tener 1 mill√≥n de par√°metros integrados en 1 teraflop de c√°lculos, y puede hacer que el programa aprenda a hacer cosas arbitrarias con descenso de gradiente estoc√°stico (SGD).  No deber√≠a funcionar, pero es curioso que vivamos en un universo donde funciona. <br><br>  Si jug√°ramos juegos de mesa simples, como las damas, el orden ser√≠a aproximadamente el mismo.  Hay una notable diferencia con los algoritmos de recorte minimax o alfa-beta.  En estos algoritmos, el programa mira unos pasos m√°s adelante, conoce las reglas del juego y analiza millones de posiciones.  En el enfoque RL, solo se analizan los movimientos realizados.  Al mismo tiempo, la red neuronal no mira hacia adelante, ya que no sabe nada sobre las reglas del juego. <br><br>  <b>Orden de entrenamiento en detalle.</b>  Creamos e inicializamos una red neuronal con algunos W1, W2 y jugamos 100 juegos de pong (lo llamamos el "encuentro" de la pol√≠tica, despliegues de pol√≠ticas).  Supongamos que cada juego consta de 200 cuadros, por lo que en total tomamos 100 * 200 = 20,000 decisiones de subir o bajar.  Y para cada una de las soluciones, conocemos un gradiente que nos dice c√≥mo deber√≠an cambiar los par√°metros si queremos fomentar o prohibir esta soluci√≥n en este estado en el futuro.  Todo lo que queda ahora es etiquetar cada decisi√≥n que tomamos como buena o mala.  Por ejemplo, supongamos que ganamos 12 juegos y perdimos 88. Tomaremos todas las decisiones 200 * 12 = 2400 que tomamos en los juegos ganadores y realizaremos una actualizaci√≥n positiva (completando un gradiente de +1.0 para cada acci√≥n, realizando backprop y actualizando los par√°metros alentando las acciones que hemos elegido en todas estas condiciones).  Y tomaremos las otras 200 * 88 = 17,600 decisiones que tomamos al perder juegos y realizaremos una actualizaci√≥n negativa (sin aprobar lo que hicimos).  Y eso es todo lo que se necesita.  Ahora ser√° m√°s probable que la red repita acciones que han funcionado, y un poco menos probable que repita acciones que no han funcionado.  Ahora jugamos otros 100 juegos con nuestra nueva pol√≠tica ligeramente mejorada y luego repetimos la aplicaci√≥n de gradientes. <br><br><img src="https://habrastorage.org/webt/zv/ap/mo/zvapmoiul9plnltsttdsplvze3g.png"><br>  <i>Esquema de dibujos animados de 4 juegos.</i>  <i>Cada c√≠rculo negro es un tipo de estado del juego (a continuaci√≥n se muestran tres ejemplos de estados), y cada flecha es una transici√≥n marcada con la acci√≥n que se seleccion√≥.</i>  <i>En este caso, ganamos 2 juegos y perdimos 2 juegos.</i>  <i>Tomamos los dos juegos que ganamos y alentamos ligeramente cada acci√≥n que hicimos en este episodio.</i>  <i>Por el contrario, tambi√©n tomaremos los dos juegos perdidos y desalentaremos ligeramente cada acci√≥n individual que hicimos en este episodio.</i> <br><br>  Si reflexiona sobre esto, comenzar√° a encontrar algunas propiedades divertidas.  Por ejemplo, ¬øqu√© pasa si hicimos una buena acci√≥n en el cuadro 50, pateando la pelota correctamente, pero luego fallamos la pelota en el cuadro 150?  Desde que perdimos el juego, cada acci√≥n individual ahora est√° marcada como mala, ¬øy esto no evita el golpe correcto en el cuadro 50?  Tienes raz√≥n, as√≠ ser√° para esta fiesta.  Sin embargo, cuando considera el proceso en miles / millones de juegos, la ejecuci√≥n correcta del rebote aumenta su probabilidad de ganar en el futuro.  En promedio, ver√° m√°s actualizaciones positivas que negativas para un ataque de raqueta adecuado.  Y la pol√≠tica de implementaci√≥n de la red neuronal finalmente producir√° las reacciones correctas. <br><br>  <i>Actualizaci√≥n: 9 de diciembre de 2016</i> es una vista alternativa.  En mi explicaci√≥n anterior, utilizo t√©rminos como "definir un gradiente y propagaci√≥n hacia atr√°s", que es una t√©cnica h√°bil definitiva.  Si est√° acostumbrado a escribir su propio c√≥digo de propagaci√≥n de backprop o usar Torch, puede controlar completamente los gradientes.  Sin embargo, si est√° acostumbrado a Theano o TensorFlow, quedar√° un poco desconcertado porque el c√≥digo de backprop es completamente autom√°tico y dif√≠cil de personalizar.  En este caso, la siguiente vista alternativa puede ser m√°s productiva.  En la ense√±anza con un maestro, el objetivo habitual es maximizar <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-27"><span class="MJXp-mtext" id="MJXp-Span-28">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-29"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-30"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-31"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-32" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-33" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></span></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-35"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;">o </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38"><font style="vertical-align: inherit;">p </font></span><span class="MJXp-mo" id="MJXp-Span-39" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-msubsup" id="MJXp-Span-40"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">y </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-40"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-42" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">i</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44"><font style="vertical-align: inherit;"> m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46"><font style="vertical-align: inherit;">d </font></span><span class="MJXp-msubsup" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">x </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-49" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">i</font></span></span><span class="MJXp-mo" id="MJXp-Span-50" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"> )</font></span></font><span class="MJXp-mtext" id="MJXp-Span-34">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-35"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-38"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-39" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-40"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-41" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-42" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-43">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-44"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-45"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-46"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-49" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mo" id="MJXp-Span-50" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.045ex" height="2.66ex" viewBox="0 -832 9060.9 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="1242" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6C" x="2764" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6F" x="3063" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-67" x="3548" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-70" x="4029" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-28" x="4532" y="0"></use><g transform="translate(4922,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="693" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6D" x="6007" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="6885" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-64" x="7231" y="0"></use><g transform="translate(7754,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-29" x="8671" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> \ sum_i \ log p (y_i \ mid x_i) </script>  donde <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-51"><span class="MJXp-msubsup" id="MJXp-Span-52"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-53" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-54" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></span></span><span class="MJXp-mo" id="MJXp-Span-55" style="margin-left: 0em; margin-right: 0.222em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-56"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-57" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-58" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.102ex" height="1.817ex" viewBox="0 -520.7 2196.8 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="809" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-2C" x="916" y="0"></use><g transform="translate(1361,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="693" y="-213"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-3"> x_i, y_i </script>  - ejemplos de capacitaci√≥n (como im√°genes y sus etiquetas).  La aplicaci√≥n del gradiente a la funci√≥n de pol√≠tica coincide exactamente con la capacitaci√≥n con el maestro, pero con dos peque√±as diferencias: 1) no tenemos las etiquetas correctas <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-59"><span class="MJXp-msubsup" id="MJXp-Span-60"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-62" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.939ex" height="1.817ex" viewBox="0 -520.7 834.8 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="693" y="-213"></use></g></svg></span><script type="math/tex" id="MathJax-Element-4"> y_i </script>  , por lo tanto, como una "etiqueta falsa" utilizamos la acci√≥n que recibimos para seleccionar de la pol√≠tica cuando vio <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-63"><span class="MJXp-msubsup" id="MJXp-Span-64"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-65" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-66" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.129ex" height="1.817ex" viewBox="0 -520.7 916.8 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="809" y="-213"></use></g></svg></span><script type="math/tex" id="MathJax-Element-5"> x_i </script>  , y 2) Introducimos otro coeficiente de conveniencia (ventaja) para cada acci√≥n.  Por lo tanto, al final, nuestra p√©rdida ahora parece <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-67"><span class="MJXp-mtext" id="MJXp-Span-68">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-69"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">s </font></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-70"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">u </font></font></span><span class="MJXp-msubsup" id="MJXp-Span-71"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-72" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-73" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i </font></font></span></span><span class="MJXp-msubsup" id="MJXp-Span-74"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-76" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">i</font></font></span></span><font style="vertical-align: inherit;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-78"><font style="vertical-align: inherit;"> l </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79"><font style="vertical-align: inherit;">o </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80"><font style="vertical-align: inherit;">g </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-81"><font style="vertical-align: inherit;">p </font></span><span class="MJXp-mo" id="MJXp-Span-82" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;">( </font></span><span class="MJXp-msubsup" id="MJXp-Span-83"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">y </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-83"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-85" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">i</font></span></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87"><font style="vertical-align: inherit;"> m </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88"><font style="vertical-align: inherit;">i </font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89"><font style="vertical-align: inherit;">d </font></span><span class="MJXp-msubsup" id="MJXp-Span-90"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91" style="margin-right: 0.05em;"><font style="vertical-align: inherit;">x </font></span></span><span class="MJXp-msubsup" id="MJXp-Span-90"><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-92" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;">i</font></span></span><span class="MJXp-mo" id="MJXp-Span-93" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"> )</font></span></font><span class="MJXp-mtext" id="MJXp-Span-77">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-78"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-79"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-80"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-81"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mo" id="MJXp-Span-82" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-83"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-84" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-85" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mtext" id="MJXp-Span-86">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89"><font style="vertical-align: inherit;"></font></span><span class="MJXp-msubsup" id="MJXp-Span-90"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-91" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-92" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"></font></span></span><span class="MJXp-mo" id="MJXp-Span-93" style="margin-left: 0em; margin-right: 0em;"><font style="vertical-align: inherit;"></font></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.588ex" height="2.66ex" viewBox="0 -832 10155.7 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="1242" y="-213"></use></g><g transform="translate(2514,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-41" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="1061" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6C" x="3859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6F" x="4158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-67" x="4643" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-70" x="5124" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-28" x="5627" y="0"></use><g transform="translate(6017,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="693" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-6D" x="7101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="7980" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-64" x="8325" y="0"></use><g transform="translate(8849,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMATHI-69" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/439674/&amp;usg=ALkJrhjMK8wZukE3fgWkcIkNrDIM0KxIbA#MJMAIN-29" x="9766" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-6"> \ sum_i A_i \ log p (y_i \ mid x_i) </script>  donde <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-94"><span class="MJXp-msubsup" id="MJXp-Span-95"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-96" style="margin-right: 0.05em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">y </font></font></span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-97" style="vertical-align: -0.4em;"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">yo</font></font></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-7"> y_i </script>  - esta es la acci√≥n que llevamos a cabo con la muestra, y <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-98"><span class="MJXp-msubsup" id="MJXp-Span-99"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-100" style="margin-right: 0.05em;">A</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-101" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-8"> A_i </script>  Es el n√∫mero que llamamos coeficiente de conveniencia.  Por ejemplo, en el caso de pong, el valor <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-102"><span class="MJXp-msubsup" id="MJXp-Span-103"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104" style="margin-right: 0.05em;">A</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-105" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-9"> A_i </script>  podr√≠a ser 1.0 si terminamos ganando el episodio, y -1.0 si perdemos.  Esto garantiza que maximicemos la probabilidad de registrar acciones que condujeron a un buen resultado, y minimizar la probabilidad de registrar acciones que no lo hicieron.  Y las acciones neutrales como resultado de muchas llamadas no afectar√°n particularmente la funci√≥n de la pol√≠tica.  Por lo tanto, el aprendizaje de refuerzo es exactamente lo mismo que aprender con un maestro, pero en un conjunto de datos (episodios) en constante cambio, con un factor adicional. <br><br>  <b>Funciones de viabilidad m√°s avanzadas.</b>  Tambi√©n promet√≠ un poco m√°s de informaci√≥n.  Hasta ahora, hemos evaluado la correcci√≥n de cada acci√≥n individual en funci√≥n de si ganamos o no.  En una configuraci√≥n RL m√°s general, recibiremos una "recompensa condicional" <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-106"><span class="MJXp-msubsup" id="MJXp-Span-107"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-108" style="margin-right: 0.05em;">r</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-109" style="vertical-align: -0.4em;">t</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-10"> r_t </script>  para cada paso, dependiendo del n√∫mero de paso o el tiempo.  Una de las opciones comunes es usar un coeficiente con descuento, por lo que la "recompensa posible" en el diagrama anterior ser√° <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-110"><span class="MJXp-msubsup" id="MJXp-Span-111"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-112" style="margin-right: 0.05em;">R</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-113" style="vertical-align: -0.4em;">t</span></span><span class="MJXp-mo" id="MJXp-Span-114" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-115">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-117">u</span><span class="MJXp-msubsup" id="MJXp-Span-118"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-119" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-124"><span class="MJXp-mtext" id="MJXp-Span-125">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-126">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-127">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-128">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-130">y</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-120"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-121">k</span><span class="MJXp-mo" id="MJXp-Span-122">=</span><span class="MJXp-mn" id="MJXp-Span-123">0</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-131">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-132">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-133">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-134">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-135">m</span><span class="MJXp-msubsup" id="MJXp-Span-136"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-137" style="margin-right: 0.05em;">a</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-138" style="vertical-align: 0.5em;">k</span></span><span class="MJXp-msubsup" id="MJXp-Span-139"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140" style="margin-right: 0.05em;">r</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-141" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142">t</span><span class="MJXp-mo" id="MJXp-Span-143">+</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-144">k</span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-11"> R_t = \ sum_ {k = 0} ^ {\ infty} \ gamma ^ k r_ {t + k} </script>  donde <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-mtext" id="MJXp-Span-146">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-147">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-148">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-149">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-150">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-151">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-12"> \ gamma </script>  Es un n√∫mero del 0 al 1, llamado coeficiente de descuento (por ejemplo, 0,99).  La expresi√≥n dice que la fuerza con la que fomentamos la acci√≥n es la suma ponderada de todas las recompensas, pero las recompensas posteriores son exponencialmente menos importantes.  Es decir, se alientan mejor las cadenas cortas de acciones, y la cola de las largas cadenas de acciones se vuelve menos importante.  En la pr√°ctica, tambi√©n necesitas normalizarlos.  Por ejemplo, supongamos que calculamos <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-152"><span class="MJXp-msubsup" id="MJXp-Span-153"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154" style="margin-right: 0.05em;">R</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-155" style="vertical-align: -0.4em;">t</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-13"> R_t </script>  por todas las 20,000 acciones en una serie de 100 episodios del juego.  Una muy buena idea es normalizar estos valores (restar el promedio, dividir por la desviaci√≥n est√°ndar) antes de conectarlos al algoritmo de backprop.  Por lo tanto, siempre alentamos y desalentamos aproximadamente la mitad de las acciones realizadas.  Esto reduce las fluctuaciones y hace que la pol√≠tica sea m√°s convergente.  Se puede encontrar un estudio m√°s profundo en [ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">link</a> ]. <br><br>  <b>Derivado de una funci√≥n pol√≠tica.</b>  Tambi√©n quer√≠a describir brevemente c√≥mo se toman matem√°ticamente los gradientes.  Los gradientes de la funci√≥n de la pol√≠tica son un caso especial de una teor√≠a m√°s general.  El caso general es que cuando tenemos una expresi√≥n de la forma <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-156"><span class="MJXp-msubsup" id="MJXp-Span-157"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-158" style="margin-right: 0.05em;">E</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-159" style="vertical-align: -0.4em;"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-160">x</span><span class="MJXp-mtext" id="MJXp-Span-161">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-163">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-165">p</span><span class="MJXp-mo" id="MJXp-Span-166">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-167">x</span><span class="MJXp-mtext" id="MJXp-Span-168">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-170">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-171">d</span><span class="MJXp-mtext" id="MJXp-Span-172">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-173">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-174">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-175">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-176">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-177">a</span><span class="MJXp-mo" id="MJXp-Span-178">)</span></span></span><span class="MJXp-mo" id="MJXp-Span-179" style="margin-left: 0em; margin-right: 0em;">[</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-180">f</span><span class="MJXp-mo" id="MJXp-Span-181" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-182">x</span><span class="MJXp-mo" id="MJXp-Span-183" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-184" style="margin-left: 0em; margin-right: 0em;">]</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-14"> E_ {x \ sim p (x \ mid \ theta)} [f (x)] </script>  , es decir, la expectativa de alguna funci√≥n escalar <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-185"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-186">f</span><span class="MJXp-mo" id="MJXp-Span-187" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-188">x</span><span class="MJXp-mo" id="MJXp-Span-189" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-15"> f (x) </script>  con alguna distribuci√≥n de su par√°metro <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-190"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-191">p</span><span class="MJXp-mo" id="MJXp-Span-192" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-193">x</span><span class="MJXp-mo" id="MJXp-Span-194" style="margin-left: 0em; margin-right: 0.222em;">;</span><span class="MJXp-mtext" id="MJXp-Span-195">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-198">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-199">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-200">a</span><span class="MJXp-mo" id="MJXp-Span-201" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-16"> p (x; \ theta) </script>  parametrizado por alg√∫n vector <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-202"><span class="MJXp-mtext" id="MJXp-Span-203">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-205">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-208">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-17"> \ theta </script>  .  Entonces <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-209"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-210">f</span><span class="MJXp-mo" id="MJXp-Span-211" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-212">x</span><span class="MJXp-mo" id="MJXp-Span-213" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-18"> f (x) </script>  se convertir√° en nuestra funci√≥n de recompensa (o la funci√≥n de conveniencia en un sentido m√°s general) y la distribuci√≥n discreta <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-214"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-215">p</span><span class="MJXp-mo" id="MJXp-Span-216" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-217">x</span><span class="MJXp-mo" id="MJXp-Span-218" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-19"> p (x) </script>  ser√° nuestra pol√≠tica, que en realidad tiene la forma <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-219"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">p</span><span class="MJXp-mo" id="MJXp-Span-221" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">a</span><span class="MJXp-mtext" id="MJXp-Span-223">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-224">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-225">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-226">d</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-227">I</span><span class="MJXp-mo" id="MJXp-Span-228" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-20"> p (a \ mid I) </script>  dando las probabilidades de una acci√≥n a para la imagen <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-229"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-230">I</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-21"> I </script>  .  Entonces estamos interesados ‚Äã‚Äãen c√≥mo podemos cambiar la distribuci√≥n de p a trav√©s de sus par√°metros <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-231"><span class="MJXp-mtext" id="MJXp-Span-232">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-233">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-234">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-237">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-22"> \ theta </script>  para ampliar <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-238"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-239">f</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-23"> f </script>  (es decir, c√≥mo cambiamos la configuraci√≥n de la red para que las acciones obtengan una mayor recompensa).  Tenemos esto: <br><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-240"><span class="noError" id="MJXp-Span-241" style="display: inline-block;">\&nbsp;begin&nbsp;{align}&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;E_x&nbsp;[f&nbsp;(x)]&nbsp;&amp;&nbsp;=&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;\&nbsp;sum_x&nbsp;p&nbsp;(x)&nbsp;f&nbsp;(x)&nbsp;&amp;&nbsp;\&nbsp;text&nbsp;{definici√≥n&nbsp;de&nbsp;expectativa}&nbsp;\\&nbsp;&amp;&nbsp;=&nbsp;\&nbsp;sum_x&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;p&nbsp;(x)&nbsp;f&nbsp;(x)&nbsp;&amp;&nbsp;\&nbsp;text&nbsp;{intercambie&nbsp;la&nbsp;suma&nbsp;y&nbsp;el&nbsp;gradiente}&nbsp;\\&nbsp;&amp;&nbsp;=&nbsp;\&nbsp;sum_x&nbsp;p&nbsp;(x)&nbsp;\&nbsp;frac&nbsp;{\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;p&nbsp;(x)}&nbsp;{p&nbsp;(x)}&nbsp;f&nbsp;(x)&nbsp;&amp;&nbsp;\&nbsp;text&nbsp;{multiplicar&nbsp;y&nbsp;dividir&nbsp;por}&nbsp;p&nbsp;(x)&nbsp;\\&nbsp;&amp;&nbsp;=&nbsp;\&nbsp;sum_x&nbsp;p&nbsp;(x)&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;\&nbsp;log&nbsp;p&nbsp;(x)&nbsp;f&nbsp;(x)&nbsp;&amp;&nbsp;\&nbsp;text&nbsp;{use&nbsp;el&nbsp;hecho&nbsp;de&nbsp;que}&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;\&nbsp;log&nbsp;(z)&nbsp;=&nbsp;\&nbsp;frac&nbsp;{1}&nbsp;{z}&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;z&nbsp;\\&nbsp;&amp;&nbsp;=&nbsp;E_x&nbsp;[f&nbsp;(x)&nbsp;\&nbsp;nabla&nbsp;_&nbsp;{\&nbsp;theta}&nbsp;\&nbsp;log&nbsp;p&nbsp;(x)]&nbsp;&amp;&nbsp;\&nbsp;text&nbsp;{definici√≥n&nbsp;de&nbsp;expectativa}&nbsp;\&nbsp;end&nbsp;{align}</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-24"> \ begin {align} \ nabla _ {\ theta} E_x [f (x)] & = \ nabla _ {\ theta} \ sum_x p (x) f (x) & \ text {definici√≥n de expectativa} \\ & = \ sum_x \ nabla _ {\ theta} p (x) f (x) & \ text {intercambie la suma y el gradiente} \\ & = \ sum_x p (x) \ frac {\ nabla _ {\ theta} p (x)} {p (x)} f (x) & \ text {multiplicar y dividir por} p (x) \\ & = \ sum_x p (x) \ nabla _ {\ theta} \ log p (x) f (x) & \ text {use el hecho de que} \ nabla _ {\ theta} \ log (z) = \ frac {1} {z} \ nabla _ {\ theta} z \\ & = E_x [f (x) \ nabla _ {\ theta} \ log p (x)] & \ text {definici√≥n de expectativa} \ end {align} </script><br><br>  Tratar√© de explicar esto.  Tenemos alguna distribuci√≥n <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-242"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-243">p</span><span class="MJXp-mo" id="MJXp-Span-244" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245">x</span><span class="MJXp-mo" id="MJXp-Span-246" style="margin-left: 0em; margin-right: 0.222em;">;</span><span class="MJXp-mtext" id="MJXp-Span-247">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-249">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-250">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-251">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-252">a</span><span class="MJXp-mo" id="MJXp-Span-253" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-25"> p (x; \ theta) </script>  (Us√© la abreviatura <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-254"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-255">p</span><span class="MJXp-mo" id="MJXp-Span-256" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-257">x</span><span class="MJXp-mo" id="MJXp-Span-258" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-26"> p (x) </script>  de donde podemos seleccionar valores espec√≠ficos.  Por ejemplo, puede ser una distribuci√≥n gaussiana de la cual un generador de n√∫meros aleatorios muestrea.  Para cada ejemplo, tambi√©n podemos calcular la funci√≥n de estimaci√≥n <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-259"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-260">f</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-27"> f </script>  , que seg√∫n el ejemplo actual nos da una estimaci√≥n escalar.  La ecuaci√≥n resultante nos dice c√≥mo debemos cambiar la distribuci√≥n a trav√©s de sus par√°metros. <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-261"><span class="MJXp-mtext" id="MJXp-Span-262">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-265">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-266">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-267">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-28"> \ theta </script>  si queremos m√°s ejemplos de acciones basadas en √©l para obtener tasas m√°s altas <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-268"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-269">f</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-29-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-29"> f </script>  .  Tomamos algunos ejemplos de acciones. <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-270"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271">x</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-30-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-30"> x </script>  y su evaluaci√≥n <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-272"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">f</span><span class="MJXp-mo" id="MJXp-Span-274" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">x</span><span class="MJXp-mo" id="MJXp-Span-276" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-31-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-31"> f (x) </script>  , y tambi√©n para cada x tambi√©n evaluamos el segundo t√©rmino <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-277"><span class="MJXp-mtext" id="MJXp-Span-278">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-279">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-281">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-282">l</span><span class="MJXp-msubsup" id="MJXp-Span-283"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-284" style="margin-right: 0.05em;">a</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-285" style="vertical-align: -0.4em;"><span class="MJXp-mtext" id="MJXp-Span-286">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-287">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-290">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">a</span></span></span><span class="MJXp-mtext" id="MJXp-Span-292">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-295">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296">p</span><span class="MJXp-mo" id="MJXp-Span-297" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-298">x</span><span class="MJXp-mo" id="MJXp-Span-299" style="margin-left: 0em; margin-right: 0.222em;">;</span><span class="MJXp-mtext" id="MJXp-Span-300">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-301">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-302">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-303">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-305">a</span><span class="MJXp-mo" id="MJXp-Span-306" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-32-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-32"> \ nabla _ {\ theta} \ log p (x; \ theta) </script>  .  ¬øQu√© es este multiplicador?  Este es precisamente el vector: el gradiente, que nos da direcci√≥n en el espacio de par√°metros, lo que conducir√° a un aumento en la probabilidad de una acci√≥n espec√≠fica <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-307"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-308">x</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-33-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-33"> x </script>  .  En otras palabras, si empujamos Œ∏ en la direcci√≥n <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-309"><span class="MJXp-mtext" id="MJXp-Span-310">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-311">n</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-312">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">b</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-314">l</span><span class="MJXp-msubsup" id="MJXp-Span-315"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316" style="margin-right: 0.05em;">a</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-317" style="vertical-align: -0.4em;"><span class="MJXp-mtext" id="MJXp-Span-318">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-319">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-320">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-321">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-322">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-323">a</span></span></span><span class="MJXp-mtext" id="MJXp-Span-324">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-325">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-327">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-328">p</span><span class="MJXp-mo" id="MJXp-Span-329" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330">x</span><span class="MJXp-mo" id="MJXp-Span-331" style="margin-left: 0em; margin-right: 0.222em;">;</span><span class="MJXp-mtext" id="MJXp-Span-332">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-334">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-335">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-337">a</span><span class="MJXp-mo" id="MJXp-Span-338" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-34-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-34"> \ nabla _ {\ theta} \ log p (x; \ theta) </script>  , ver√≠amos que la nueva probabilidad de esta acci√≥n aumentar√° ligeramente.  Si miras hacia atr√°s a la f√≥rmula, nos dice que debemos tomar esta direcci√≥n y multiplicar el valor escalar por ella. <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-339"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-340">f</span><span class="MJXp-mo" id="MJXp-Span-341" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-342">x</span><span class="MJXp-mo" id="MJXp-Span-343" style="margin-left: 0em; margin-right: 0em;">)</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-35-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-35"> f (x) </script>  .  Esto asegurar√° que los ejemplos de acciones con una calificaci√≥n m√°s alta (en nuestro caso, una recompensa) se ‚Äúarrastren‚Äù m√°s fuertemente que los ejemplos con un indicador m√°s bajo, por lo tanto, si tuvi√©ramos que actualizar en base a varias muestras de <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-344"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345">p</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-36-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-36"> p </script>  , la densidad de probabilidad cambiar√≠a a la direcci√≥n de los puntos de juego m√°s altos, lo que aumenta la probabilidad de obtener ejemplos de acciones con altas recompensas.  Es importante que el gradiente no se tome de la funci√≥n <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-346"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-347">f</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-37-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-37"> f </script>  , ya que generalmente puede ser indiferenciado e impredecible.  Un <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-348"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-349">p</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-38-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-38"> p </script>  diferenciable por <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-350"><span class="MJXp-mtext" id="MJXp-Span-351">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-352">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-353">h</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-354">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-355">t</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-356">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-39-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-39"> \ theta </script>  .  Eso es <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-357"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">p</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-40-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-40"> p </script>  es una distribuci√≥n discreta continuamente ajustable, donde puede ajustar las probabilidades de acciones individuales.  Tambi√©n asumimos que <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-359"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">p</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-41-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-41"> p </script>  normalizado <br><br><img src="https://habrastorage.org/webt/xo/qy/9p/xoqy9pzjvy4d3sxosr_irpasic4.png"><br><br>  <i>Visualizaci√≥n de gradiente.</i>  <i>Izquierda: distribuci√≥n gaussiana y varios ejemplos de ella (puntos azules).</i>  <i>En cada punto azul, tambi√©n graficamos el gradiente de la probabilidad logar√≠tmica con respecto al par√°metro promedio.</i>  <i>La flecha indica la direcci√≥n en la que se debe cambiar el valor de distribuci√≥n promedio para aumentar la probabilidad de esta acci√≥n de ejemplo.</i>  <i>En el medio: se agreg√≥ alguna funci√≥n de evaluaci√≥n que proporciona -1 en todas partes excepto +1 en algunas regiones peque√±as (tenga en cuenta que esta puede ser una funci√≥n escalar arbitraria y no necesariamente diferenciable).</i>  <i>Las flechas ahora est√°n codificadas por colores, porque debido a la multiplicaci√≥n vamos a promediar todas las flechas verdes con una calificaci√≥n positiva y las flechas rojas negativas.</i>  <i>Derecha: despu√©s de actualizar los par√°metros, las flechas verdes y las flechas rojas invertidas nos empujan hacia la izquierda y hacia abajo.</i>  <i>Las muestras de esta distribuci√≥n ahora tendr√°n una calificaci√≥n esperada m√°s alta, si se desea.</i> <br><br>  Espero que la conexi√≥n con RL sea clara.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nuestra pol√≠tica nos da ejemplos de acciones, y algunas de ellas funcionan mejor que otras (a juzgar por la funci√≥n de conveniencia). La forma de cambiar la configuraci√≥n de la pol√≠tica es ejecutar, tomar el gradiente de las acciones seleccionadas, multiplicarlo por la calificaci√≥n y agregar todo lo que hicimos anteriormente. Para una conclusi√≥n m√°s completa, recomiendo una conferencia de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John Shulman. </font></font><br></a> <br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Entrenamiento</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bueno, hemos desarrollado los principios de gradientes de la funci√≥n de la pol√≠tica. Implement√© todo el enfoque en un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">script Python de 130 l√≠neas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> que usa el emulador ATAI 2600 Pong ya </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">preparado</font></a><font style="vertical-align: inherit;"> de </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Gym</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Entren√© una red neuronal de dos capas con 200 neuronas de capa oculta usando el algoritmo RMSProp para una serie de 10 episodios (cada episodio, de acuerdo con las reglas, consiste en varios sorteos y el episodio contin√∫a marcando 21). No configur√© demasiado los hiperpar√°metros y experiment√© en mi Macbook lenta, pero despu√©s de un entrenamiento de tres d√≠as obtuve una pol√≠tica que juega un poco mejor que el reproductor incorporado. El n√∫mero total de episodios fue de aproximadamente 8,000, por lo que el algoritmo jug√≥ aproximadamente 200,000 juegos de pong, que es bastante, y produjo un total de ~ 800 actualizaciones de los pesos. Si entrenara en la GPU con ConvNets, en unos pocos d√≠as, podr√≠a lograr excelentes resultados, y si optimizara los hiperpar√°metros, siempre podr√≠a ganar. Sin embargo, no pas√© demasiado tiempo computando o configurando,as√≠ que en su lugar obtuvimos Pong AI, que ilustra las ideas principales y funciona bastante bien:</font></font><br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/cf/gf/jr/cfgfjrkzt_awy-l7f5lcxcrfvki.png"></a> <br> <i>  .      </i> <br><br> <b> .</b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n podemos ver los pesos obtenidos de la red neuronal. Gracias al preprocesamiento, cada una de nuestras entradas es una imagen de diferencia de 80x80 (fotograma actual menos fotograma anterior). Cada neurona de la capa W1 est√° conectada a una capa oculta W2 que consta de 200 neuronas. El n√∫mero de enlaces es 80 * 80 * 200. Intentemos analizar estas conexiones. Ordenaremos todas las neuronas de la capa W2 y visualizaremos qu√© pesos llevan a ella. A partir de las escalas que conducen a una neurona W2 de las neuronas W1, haremos im√°genes de 80x80. A continuaci√≥n se muestran 40 im√°genes de W2 (un total de 200). Los p√≠xeles blancos son pesos positivos y los negros son negativos. Tenga en cuenta que varias neuronas W2 est√°n sintonizadas en una bola voladora codificada en l√≠neas discontinuas. En un juego, la pelota solo puede estar en un lugar,por lo tanto, estas neuronas son multiprop√≥sito y se "disparar√°n" si la pelota est√° en alg√∫n lugar dentro de estas l√≠neas. La alternancia de blanco y negro es interesante, porque cuando la pelota se mueve a lo largo de la pista, la actividad de la neurona fluctuar√° como una onda sinusoidal. Y debido a ReLU, solo "disparar√°" en ciertas posiciones. Hay mucho ruido en las im√°genes, lo que ser√≠a menor si usara la regularizaci√≥n L2.</font></font><br><br><img src="https://habrastorage.org/webt/1t/n4/le/1tn4lew2rszryk1u9vkbreigku4.png"><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lo que no est√° pasando</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Entonces, aprendimos a jugar pong en im√°genes usando el gradiente de la funci√≥n de pol√≠tica, y esto funciona bastante bien. Este enfoque es una forma extra√±a de "sugerir y verificar", donde "adivinar" se refiere a ejecutar nuestra pol√≠tica en varios episodios del juego, y "verificar" fomenta acciones que conducen a buenos resultados. En general, esto representa el nivel actual de c√≥mo estamos abordando actualmente los problemas del aprendizaje reforzado. Si entiende el algoritmo intuitivamente y sabe c√≥mo funciona, deber√≠a estar al menos un poco decepcionado. En particular, ¬øcu√°ndo no funciona?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Compare esto con c√≥mo una persona puede aprender a jugar pong. Usted mismo les muestra el juego y dice algo como: "Usted controla la raqueta, y puede moverla hacia arriba y hacia abajo, y su tarea es lanzar la pelota m√°s all√° de otro jugador controlado por el programa incorporado", y est√° listo para comenzar. Tenga en cuenta algunas diferencias:</font></font><br><br><ul><li>         - , ,   ,     .     RL       ,        .       ,        (        ),       ,  .         .   ,          ,  ,  ,   ,            , ,   ,  . <br></li><li>      ,     ( ,    ,     ,      ..),    ( ¬´¬ª ¬´ ,  , ,    -  -  . .).      ¬´¬ª          / . ,     ,    (   )   (      ,    ). <br></li><li>    ‚Äî    (brute force),            ,       .              .       ,    ,           ,      .   ,     ¬´¬ª    ,        .   ,    ,         . <br></li><li>        ,    ,     .    ,             ,      .           . <br></li></ul><br><img src="https://habrastorage.org/webt/do/4a/ha/do4ahazbavg7xmlq-a5omrr77vq.png"><br><br> <i>:  :      RL.   ,  ,     .  ,    .     ,  99%        .  ,  ¬´¬ª   . :       ¬´¬ª,    ,  - , -  , -  ,     ,     .                 ¬´ ,      ¬ª.</i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tambi√©n me gustar√≠a enfatizar el hecho de que, por el contrario, en muchos juegos los gradientes de la pol√≠tica derrotar√≠an f√°cilmente a una persona. En particular, esto se aplica a los juegos con recompensas frecuentes, que requieren una reacci√≥n precisa y r√°pida y sin planificaci√≥n a largo plazo. El enfoque PG puede ver f√°cilmente las correlaciones a corto plazo entre recompensas y acciones. Puedes ver similar en nuestro agente Pong. Desarrolla una estrategia cuando simplemente espera la pelota, y luego se mueve r√°pidamente para atraparlo solo en el borde, por lo que la pelota rebota a una alta velocidad vertical. El agente gana varias victorias seguidas, repitiendo esta estrategia simple. Hay muchos juegos (Pinball, Breakout) en los que Deep Q-Learning atrae y pisotea a una persona en el barro con sus acciones simples y precisas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Una vez que comprenda el "truco" con el que funcionan estos algoritmos, podr√° comprender sus fortalezas y debilidades. En particular, estos algoritmos est√°n muy por detr√°s de las personas en la construcci√≥n de ideas abstractas sobre juegos que las personas pueden usar para un aprendizaje r√°pido. Una vez que la computadora mira la matriz de p√≠xeles y nota la llave, la puerta y piensa para s√≠ misma que probablemente ser√≠a bueno tomar la llave y llegar a la puerta. Actualmente no hay nada cerca de esto, y tratar de llegar all√≠ es un √°rea activa de investigaci√≥n. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C√°lculos no diferenciables en redes neuronales.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Me gustar√≠a mencionar otra aplicaci√≥n interesante de gradientes de pol√≠ticas que no son de juego: nos permite dise√±ar y entrenar redes neuronales utilizando componentes que realizan (o interact√∫an) con la computaci√≥n no diferenciable. Esta idea fue introducida por primera vez en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1992 por Williams</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . y recientemente se ha popularizado en </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">modelos de atenci√≥n visual recurrente.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">llamado "atenci√≥n cercana" en el contexto de un modelo que procesa una imagen con una secuencia de miradas estrechas foveales de baja resoluci√≥n, similar a c√≥mo nuestro ojo examina objetos con una visi√≥n central en funcionamiento. En cada iteraci√≥n, el RNN recibir√° un peque√±o fragmento de la imagen y seleccionar√° la ubicaci√≥n que debe buscarse m√°s a fondo. Por ejemplo, el RNN puede mirar la posici√≥n (5.30), obtener un peque√±o fragmento de la imagen, luego decidir mirar (24, 50), etc. Hay una secci√≥n de la red neuronal que selecciona d√≥nde buscar m√°s y luego la inspecciona. Desafortunadamente, esta operaci√≥n no es diferenciable porque no sabemos qu√© pasar√≠a si tom√°ramos una muestra en otro lugar. En un caso m√°s general, considere una red neuronal que tiene varias entradas y salidas:</font></font><br><br><img src="https://habrastorage.org/webt/xn/gn/ca/xngncauic38rgmg3bi_srnlevbw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tenga en cuenta que la mayor√≠a de las flechas azules son diferenciables como de costumbre, pero algunas transformaciones de vista tambi√©n pueden incluir una operaci√≥n de selecci√≥n indiferenciada, que se resalta en rojo. Podemos simplemente pasar por las flechas azules en la direcci√≥n opuesta, pero la flecha roja es una dependencia a trav√©s de la cual no podemos revertir la propagaci√≥n del backprop.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">¬°Pol√≠tica de gradiente al rescate! Pensemos en la parte de la red que realiza el muestreo que puede representarse en funci√≥n de una pol√≠tica estoc√°stica incrustada en una gran red neuronal. Por lo tanto, durante el entrenamiento, produciremos varios ejemplos (indicados por las ramas a continuaci√≥n), y luego alentaremos muestras que finalmente conduzcan a buenos resultados (en este caso, por ejemplo, medidos por las p√©rdidas al final). En otras palabras, entrenaremos los par√°metros incluidos en las flechas azules utilizando el backprop, como de costumbre, pero los par√°metros incluidos en la flecha roja ahora se actualizar√°n independientemente del paso hacia atr√°s utilizando gradientes de pol√≠ticas, fomentando muestras que resulten en bajas p√©rdidas. Esta idea tambi√©n fue recientemente bien enmarcada.</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estimaci√≥n de gradiente utilizando gr√°ficos de c√°lculo estoc√°sticos.</font></font></a> <br><br><img src="https://habrastorage.org/webt/td/vx/lr/tdvxlrd98jepxgmsvpc5gtewlxs.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Entrada / salida entrenada en memoria de acceso aleatorio. Tambi√©n encontrar√°s esta idea en muchos otros art√≠culos. Por ejemplo, la </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural Turing Machine</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> tiene una cinta de memoria con la que leen y escriben. Para realizar una operaci√≥n de escritura, debe hacer algo como m [i] = x, donde i y x son predichas por la red neuronal RNN. Sin embargo, no hay se√±al que nos diga qu√© pasar√≠a con la funci√≥n de p√©rdida si escribi√©ramos j! = I. Por lo tanto, NTM puede realizar operaciones suaves de lectura y escritura. √âl predice la funci√≥n de distribuci√≥n de atenci√≥n a, y luego realiza para todo i: m [i] = a [i] * x. Ahora es diferenciable, pero tenemos que pagar un alto precio computacional, clasificando todas las celdas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, podemos usar gradientes de pol√≠ticas para te√≥ricamente solucionar este problema, como se hace en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RL-NTM</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Todav√≠a predecimos la distribuci√≥n de la atenci√≥n a, pero en lugar de una b√∫squeda exhaustiva, seleccionamos al azar lugares para escribir: i = muestra (a); m [i] = x. Durante el entrenamiento, podr√≠amos hacer esto para un peque√±o conjunto de i y, al final, encontrar√≠amos un conjunto que funcionar√≠a mejor que otros. Una gran ventaja computacional es que durante las pruebas puede leer / escribir desde una celda. Sin embargo, como se indica en el documento, esta estrategia es muy dif√≠cil de poner en pr√°ctica, ya que necesita pasar por muchas opciones y casi accidentalmente ir a algoritmos de trabajo. Actualmente, los investigadores coinciden en que PG funciona bien solo cuando hay varias opciones discretas, cuando no es necesario peinar a trav√©s de enormes espacios de b√∫squeda.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sin embargo, con la ayuda de los gradientes de pol√≠ticas, y en los casos en que hay disponible una gran cantidad de datos y poder de c√≥mputo, podemos, en principio, so√±ar con mucho. Por ejemplo, podemos dise√±ar redes neuronales que aprendan a interactuar con grandes objetos no diferenciables, como los compiladores de Latex. Por ejemplo, para que char-rnn genere c√≥digo Latex listo para usar, o un sistema SLAM, o solucionadores LQR, o algo m√°s. O, por ejemplo, la superinteligencia puede querer aprender a interactuar con Internet a trav√©s de TCP / IP (que tampoco es diferenciable) para acceder a la informaci√≥n necesaria para capturar el mundo. Este es un gran ejemplo.</font></font><br><br><h4>  Conclusiones </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vimos que los gradientes de pol√≠ticas son un poderoso algoritmo general, y como ejemplo, capacitamos al agente ATARI Pong desde p√≠xeles sin formato desde cero en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">130 l√≠neas de Python</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . En general, el mismo algoritmo puede usarse para entrenar agentes para juegos arbitrarios y, con suerte, alg√∫n d√≠a podemos usarlo para resolver problemas de control en el mundo real. En conclusi√≥n, me gustar√≠a agregar algunos comentarios m√°s: </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sobre el desarrollo de la IA</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Vimos que el algoritmo funciona mediante la b√∫squeda de la fuerza bruta, en la que primero vacila al azar y tiene que tropezar con situaciones √∫tiles al menos una vez, e idealmente a menudo, antes de que la funci√≥n de pol√≠tica cambie sus par√°metros. Tambi√©n vimos que una persona aborda estas soluciones a estos problemas de una manera completamente diferente, que se asemeja a la construcci√≥n r√°pida de un modelo abstracto. Dado que estos modelos abstractos son muy dif√≠ciles (si no imposibles) de imaginar expl√≠citamente, esta es tambi√©n la raz√≥n por la que recientemente ha habido tanto inter√©s en los modelos generativos y la inducci√≥n de software. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sobre el uso en rob√≥tica.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">El algoritmo no se aplica cuando es dif√≠cil obtener una gran cantidad de investigaci√≥n. Por ejemplo, puede tener uno (o varios) robots interactuando con el mundo en tiempo real. Esto no es suficiente para una aplicaci√≥n ingenua del algoritmo. Un √°rea de trabajo dise√±ada para mitigar este problema son </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">los gradientes de pol√≠tica deterministas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . En lugar de hacer intentos reales, este enfoque obtiene informaci√≥n de gradiente de una segunda red neuronal (llamada cr√≠tica) que modela la funci√≥n de evaluaci√≥n. Este enfoque puede, en principio, ser efectivo con acciones de alta dimensi√≥n, donde el muestreo aleatorio proporciona una cobertura deficiente. Otro enfoque relacionado es ampliar la rob√≥tica que estamos comenzando a ver en </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Google Robot Farm</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">o tal vez incluso en un </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tesla S + con piloto autom√°tico.</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tambi√©n hay una l√≠nea de trabajo que intenta hacer que el proceso de b√∫squeda sea menos desesperado al agregar control adicional. Por ejemplo, en muchos casos pr√°cticos, puede obtener la direcci√≥n inicial de desarrollo directamente de la persona. Por ejemplo, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AlphaGo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> primero usa la capacitaci√≥n con un maestro para predecir solo las acciones humanas (por ejemplo </font><font style="vertical-align: inherit;">, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">control remoto de robots</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">aprendizaje</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">optimizaci√≥n de trayectoria</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b√∫squeda completa de pol√≠ticas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). Y la pol√≠tica resultante se configura m√°s tarde utilizando PG para lograr el objetivo real: ganar el juego.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En algunos casos, puede haber menos preajustes (por ejemplo, para </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">controlar robots de forma remota</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), y existen m√©todos para usar estos datos </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">previos a la pasant√≠a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Finalmente, si las personas no proporcionan datos o configuraciones espec√≠ficas, tambi√©n se pueden obtener en algunos casos mediante el c√°lculo utilizando m√©todos de optimizaci√≥n bastante caros, por ejemplo, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">optimizando la trayectoria</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> en un modelo din√°mico conocido (como F = ma en un simulador f√≠sico) o en casos cuando se crea un modelo local aproximado (como se ve en una estructura muy prometedora para la b√∫squeda de pol√≠ticas administradas). </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sobre el uso de PG en la pr√°ctica.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Me gustar√≠a hablar m√°s sobre RNN. Creo que podr√≠a parecer que los RNN son m√°gicos y resuelven autom√°ticamente problemas relacionados con secuencias arbitrarias. La verdad es que hacer que estos modelos funcionen puede ser complicado. Se requiere cuidado y experiencia, as√≠ como saber cu√°ndo los m√©todos m√°s simples pueden ayudarlo en un 90%. Lo mismo ocurre con los gradientes de pol√≠ticas. No funcionan autom√°ticamente as√≠: necesita tener muchos ejemplos, pueden entrenarse para siempre, son dif√≠ciles de depurar cuando no funcionan. Siempre debes intentar disparar con una pistola peque√±a antes de alcanzar a Bazooka. Por ejemplo, en el caso del entrenamiento de refuerzo, el </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">m√©todo de entrop√≠a cruzada (CEM)</font></a><font style="vertical-align: inherit;"> siempre debe verificarse primero.</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, un enfoque estoc√°stico simple de "adivinar y verificar" inspirado en la evoluci√≥n. Y si insiste en probar gradientes de pol√≠ticas para su tarea, aseg√∫rese de conocer los trucos espec√≠ficos. Comience de manera simple y use una opci√≥n de PG llamada </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TRPO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , que casi siempre funciona mejor y de manera m√°s consistente </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">que la PG cl√°sica</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . La idea b√°sica es evitar actualizar la configuraci√≥n que cambia demasiado su pol√≠tica, debido al uso de la distancia Kulbak-Leibler entre la pol√≠tica antigua y la nueva.</font></font><br><br>  Eso es todo!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Espero haberte dado una idea de d√≥nde estamos con Reinforcement Learning, cu√°les son los problemas, y si quieres ayudar a promover RL, te invito a hacerlo en nuestro </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI Gym</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :) ¬°Nos vemos la pr√≥xima vez!</font></font><br><br><hr><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Andrej Karpathy, </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">investigador, desarrollador, director del departamento de IA y piloto autom√°tico Tesla </font></font><br><br> <i><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Informaci√≥n adicional:</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Deep Learning on Fingers </font><font style="vertical-align: inherit;">Course </font><font style="vertical-align: inherit;">2018 </font></font><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://habr.com/en/post/414165/</font></font></a> <font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Deep Learning on Fingers </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Open Course 2019 </font></font><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https: // habr.com/ru/company/ods/blog/438940/</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Facultad de f√≠sica de NSU </font></font><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">http://www.phys.nsu.ru/</font></font></a></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/439674/">https://habr.com/ru/post/439674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../439662/index.html">Anunciando TypeScript 3.3</a></li>
<li><a href="../439664/index.html">Compatibilidad binaria C ++ y actualizaciones sin dolor a Visual Studio 2019</a></li>
<li><a href="../439668/index.html">Ampliar un bolet√≠n de tecnolog√≠a a 700 mil suscriptores en 300 ciudades: la historia del resumen de inicio de Techstars</a></li>
<li><a href="../439670/index.html">Sistema ruso de IA para el diagn√≥stico de c√°ncer Botkin.AI. Ahora en Azure Marketplace</a></li>
<li><a href="../439672/index.html">Conceptos b√°sicos de PowerShell: detectar si una cadena termina con cierto car√°cter</a></li>
<li><a href="../439676/index.html">Integraci√≥n React Native y C ++ para iOS y Android</a></li>
<li><a href="../439678/index.html">Enviar al desaf√≠o de F # aplicado</a></li>
<li><a href="../439680/index.html">Alrededor del 50% de los rusos est√°n dispuestos a vender sus datos personales.</a></li>
<li><a href="../439682/index.html">Entrenamiento Cisco 200-125 CCNA v3.0. Especialista certificado en redes de Cisco (CCNA). D√≠a 4. Dispositivos de puerta de enlace</a></li>
<li><a href="../439684/index.html">Solicite el desaf√≠o F # aplicado</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>