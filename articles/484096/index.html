<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏉 ⚙️ 🕺🏾 La batalla de los dos Yakozun, o Cassandra vs HBase. Experiencia del equipo de Sberbank 🤱🏻 🤛🏼 👃🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esto ni siquiera es una broma, parece que esta imagen en particular refleja con mayor precisión la esencia de estas bases de datos, y al final quedará...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La batalla de los dos Yakozun, o Cassandra vs HBase. Experiencia del equipo de Sberbank</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/484096/">  Esto ni siquiera es una broma, parece que esta imagen en particular refleja con mayor precisión la esencia de estas bases de datos, y al final quedará claro por qué: <br><br><img src="https://habrastorage.org/webt/i2/lk/zo/i2lkzo9tq7zpeprcbtgm3-mufk4.png"><br><br>  Según el ranking DB-Engines, las dos bases de columnas NoSQL más populares son Cassandra (en adelante CS) y HBase (HB). <br><br><img src="https://habrastorage.org/webt/su/rd/39/surd39n7bmrbnxgpn0512tnxamm.png"><br><br>  Por voluntad del destino, nuestro equipo de gestión de carga de datos en Sberbank ha <a href="https://habr.com/ru/company/sberbank/blog/420425/">estado</a> trabajando estrechamente con HB durante <a href="https://habr.com/ru/company/sberbank/blog/420425/">mucho tiempo</a> .  Durante este tiempo, estudiamos sus fortalezas y debilidades bastante bien y aprendimos a cocinarlo.  Sin embargo, la presencia de una alternativa en forma de CS todo el tiempo me hizo atormentarme con dudas: ¿tomamos la decisión correcta?  Además, los resultados de la <a href="https://www.datastax.com/products/compare/nosql-performance-benchmarks">comparación</a> llevada a cabo por DataStax dijeron que CS fácilmente derrota a HB con una puntuación casi aplastante.  Por otro lado, DataStax es una persona interesada, y no debe decir nada aquí.  Además, una cantidad bastante pequeña de información sobre las condiciones de prueba era vergonzosa, por lo que decidimos averiguar por nuestra cuenta quién es el rey de BigData NoSql, y los resultados fueron muy interesantes. <br><a name="habracut"></a><br>  Sin embargo, antes de pasar a los resultados de las pruebas realizadas, es necesario describir los aspectos esenciales de las configuraciones del entorno.  El hecho es que CS se puede usar en modo de tolerancia de pérdida de datos.  Es decir  Esto es cuando solo un servidor (nodo) es responsable de los datos de una determinada clave, y si se cae por alguna razón, el valor de esta clave se perderá.  Para muchas tareas esto no es crítico, pero para el sector bancario esta es la excepción más que la regla.  En nuestro caso, es importante tener varias copias de datos para un almacenamiento confiable. <br><br>  Por lo tanto, solo se consideró el modo CS de triple replicación, es decir,  La creación de casos se realizó con los siguientes parámetros: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> KEYSPACE ks <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> <span class="hljs-keyword"><span class="hljs-keyword">REPLICATION</span></span> = {<span class="hljs-string"><span class="hljs-string">'class'</span></span> : <span class="hljs-string"><span class="hljs-string">'NetworkTopologyStrategy'</span></span>, <span class="hljs-string"><span class="hljs-string">'datacenter1'</span></span> : <span class="hljs-number"><span class="hljs-number">3</span></span>};</code> </pre> <br>  Además, hay dos formas de garantizar el nivel de consistencia requerido.  Regla general: <br>  NW + NR&gt; RF <br><br>  Esto significa que el número de confirmaciones de los nodos al escribir (NW) más el número de confirmaciones de los nodos al leer (NR) debe ser mayor que el factor de replicación.  En nuestro caso, RF = 3, por lo que las siguientes opciones son adecuadas: <br>  2 + 2&gt; 3 <br>  3 + 1&gt; 3 <br><br>  Dado que es fundamental para nosotros mantener los datos lo más confiables posible, se eligió un esquema 3 + 1.  Además, HB funciona de manera similar, es decir  Tal comparación sería más honesta. <br><br>  Cabe señalar que DataStax hizo lo contrario en su investigación, establecieron RF = 1 para CS y HB (para este último al cambiar la configuración de HDFS).  Este es un aspecto realmente importante, porque el impacto en el rendimiento de CS en este caso es enorme.  Por ejemplo, la siguiente imagen muestra el aumento de tiempo requerido para cargar datos en CS: <br><br><img src="https://habrastorage.org/webt/fw/az/r9/fwazr9muypegpgjpyaq4rnagg8u.png"><br><br>  Aquí vemos lo siguiente: mientras más hilos de la competencia escriben datos, más tiempo demora.  Esto es natural, pero es importante que la degradación del rendimiento para RF = 3 sea significativamente mayor.  En otras palabras, si escribimos en 4 tablas en cada una de las 5 transmisiones (un total de 20), entonces RF = 3 pierde aproximadamente 2 veces (150 segundos RF = 3 frente a 75 para RF = 1).  Pero si aumentamos la carga al cargar datos en 8 tablas en cada una de las 5 transmisiones (un total de 40), perder RF = 3 ya es 2,7 veces (375 segundos frente a 138). <br><br>  Quizás en parte este es el secreto de la prueba exitosa de DataStax para la prueba de carga CS, porque para HB en nuestro stand, cambiar el factor de replicación de 2 a 3 no tuvo ningún efecto.  Es decir  Los discos no son el cuello de botella para HB para nuestra configuración.  Sin embargo, hay muchas otras trampas, porque debe tenerse en cuenta que nuestra versión de HB fue ligeramente parcheada y oscurecida, los entornos son completamente diferentes, etc.  También vale la pena señalar que tal vez simplemente no sé cómo preparar CS correctamente y hay algunas formas más efectivas de trabajar con él y espero en los comentarios que descubramos.  Pero lo primero es lo primero. <br><br>  Todas las pruebas se realizaron en un clúster de hierro que consta de 4 servidores, cada uno en una configuración: <br><br>  <i>CPU: Xeon E5-2680 v4 @ 2.40GHz 64 hilos.</i> <i><br></i>  <i>Discos: 12 unidades de disco duro SATA</i> <i><br></i>  <i>versión de Java: 1.8.0_111</i> <i><br></i> <br><br>  Versión CS: 3.11.5 <br><br><div class="spoiler">  <b class="spoiler_title">Parámetros cassandra.yml</b> <div class="spoiler_text">  num_tokens: 256 <br>  hinted_handoff_enabled: true <br>  hinted_handoff_throttle_in_kb: 1024 <br>  max_hints_delivery_threads: 2 <br>  directorio_consejos: / data10 / cassandra / consejos <br>  hints_flush_period_in_ms: 10000 <br>  max_hints_file_size_in_mb: 128 <br>  batchlog_replay_throttle_in_kb: 1024 <br>  autenticador: AllowAllAuthenticator <br>  autorizador: AllowAllAuthorizer <br>  role_manager: CassandraRoleManager <br>  roles_validity_in_ms: 2000 <br>  permisos_validez_en_ms: 2000 <br>  credentials_validity_in_ms: 2000 <br>  particionador: org.apache.cassandra.dht.Murmur3Partitioner <br>  data_file_directories: <br>  - / data1 / cassandra / data # cada directorio dataN es una unidad separada <br>  - / data2 / cassandra / data <br>  - / data3 / cassandra / data <br>  - / data4 / cassandra / data <br>  - / data5 / cassandra / data <br>  - / data6 / cassandra / data <br>  - / data7 / cassandra / data <br>  - / data8 / cassandra / data <br>  commit_directory: / data9 / cassandra / commitlog <br>  cdc_enabled: false <br>  disk_failure_policy: stop <br>  commit_failure_policy: detener <br>  ready_statements_cache_size_mb: <br>  thrift_prepared_statements_cache_size_mb: <br>  key_cache_size_in_mb: <br>  key_cache_save_period: 14400 <br>  row_cache_size_in_mb: 0 <br>  row_cache_save_period: 0 <br>  counter_cache_size_in_mb: <br>  counter_cache_save_period: 7200 <br>  directorio_cachés_ guardados: / data10 / cassandra / salva_cachés <br>  commitlog_sync: periódico <br>  commitlog_sync_period_in_ms: 10000 <br>  commitlog_segment_size_in_mb: 32 <br>  seed_provider: <br>  - class_name: org.apache.cassandra.locator.SimpleSeedProvider <br>  parámetros: <br>  - semillas: "*, *" <br>  concurrent_reads: 256 # intentó 64 - no se notó diferencia <br>  concurrent_writes: 256 # intentó 64 - no se notó ninguna diferencia <br>  concurrent_counter_writes: 256 # intentó 64 - no se notó ninguna diferencia <br>  concurrent_materialized_view_writes: 32 <br>  memtable_heap_space_in_mb: 2048 # intentó 16 GB - fue más lento <br>  memtable_allocation_type: heap_buffers <br>  index_summary_capacity_in_mb: <br>  index_summary_resize_interval_in_minutes: 60 <br>  trickle_fsync: false <br>  trickle_fsync_interval_in_kb: 10240 <br>  puerto_almacenamiento: 7000 <br>  ssl_storage_port: 7001 <br>  listen_address: * <br>  broadcast_address: * <br>  listen_on_broadcast_address: verdadero <br>  internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator <br>  start_native_transport: true <br>  native_transport_port: 9042 <br>  start_rpc: verdadero <br>  rpc_address: * <br>  rpc_port: 9160 <br>  rpc_keepalive: verdadero <br>  rpc_server_type: sincronización <br>  thrift_framed_transport_size_in_mb: 15 <br>  incremental_backups: falso <br>  snapshot_before_compaction: false <br>  auto_snapshot: true <br>  column_index_size_in_kb: 64 <br>  column_index_cache_size_in_kb: 2 <br>  concurrent_compactors: 4 <br>  compaction_throughput_mb_per_sec: 1600 <br>  sstable_preemptive_open_interval_in_mb: 50 <br>  read_request_timeout_in_ms: 100000 <br>  range_request_timeout_in_ms: 200000 <br>  write_request_timeout_in_ms: 40000 <br>  counter_write_request_timeout_in_ms: 100000 <br>  cas_contention_timeout_in_ms: 20000 <br>  truncate_request_timeout_in_ms: 60000 <br>  request_timeout_in_ms: 200000 <br>  slow_query_log_timeout_in_ms: 500 <br>  cross_node_timeout: falso <br>  endpoint_snitch: GossipingPropertyFileSnitch <br>  dynamic_snitch_update_interval_in_ms: 100 <br>  dynamic_snitch_reset_interval_in_ms: 600000 <br>  dynamic_snitch_badness_threshold: 0.1 <br>  request_scheduler: org.apache.cassandra.scheduler.NoScheduler <br>  opciones_encriptación_servidor: <br>  cifrado internode: ninguno <br>  opciones_cifrado_cliente: <br>  habilitado: falso <br>  internode_compression: dc <br>  inter_dc_tcp_nodelay: falso <br>  tracetype_query_ttl: 86400 <br>  tracetype_repair_ttl: 604800 <br>  enable_user_defined_functions: false <br>  enable_scripted_user_defined_functions: false <br>  windows_timer_interval: 1 <br>  opciones_encriptación_de_datos_transparentes: <br>  habilitado: falso <br>  tombstone_warn_threshold: 1000 <br>  Tombstone_failure_threshold: 100000 <br>  batch_size_warn_threshold_in_kb: 200 <br>  batch_size_fail_threshold_in_kb: 250 <br>  unlogged_batch_across_partitions_warn_threshold: 10 <br>  compaction_large_partition_warning_threshold_mb: 100 <br>  gc_warn_threshold_in_ms: 1000 <br>  back_pressure_enabled: false <br>  enable_materialized_views: true <br>  enable_sasi_indexes: true <br></div></div><br>  Configuración de GC: <br><br><div class="spoiler">  <b class="spoiler_title">### Configuración de CMS</b> <div class="spoiler_text">  -XX: + UseParNewGC <br>  -XX: + UseConcMarkSweepGC <br>  -XX: + CMSParallelRemarkEnabled <br>  -XX: SurvivorRatio = 8 <br>  -XX: MaxTenuringThreshold = 1 <br>  -XX: CMSInitiatingOccupancyFraction = 75 <br>  -XX: + UseCMSInitiatingOccupancyOnly <br>  -XX: CMSWaitDuration = 10000 <br>  -XX: + CMSParallelInitialMarkEnabled <br>  -XX: + CMSEdenChunksRecordAlways <br>  -XX: + CMSClassUnloadingEnabled <br><br></div></div><br>  La memoria jvm.options se asignó a 16 Gb (aún se intentó 32 Gb, no se notó ninguna diferencia). <br><br>  La creación de tablas fue realizada por el comando: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> ks.t1 (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> PRIMARY <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, title <span class="hljs-built_in"><span class="hljs-built_in">text</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> compression = {<span class="hljs-string"><span class="hljs-string">'sstable_compression'</span></span>: <span class="hljs-string"><span class="hljs-string">'LZ4Compressor'</span></span>, <span class="hljs-string"><span class="hljs-string">'chunk_length_kb'</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span>};</code> </pre> <br>  Versión HB: 1.2.0-cdh5.14.2 (en la clase org.apache.hadoop.hbase.regionserver.HRegion excluimos MetricsRegion que condujo a GC con más de 1000 regiones en RegionServer) <br><br><div class="spoiler">  <b class="spoiler_title">Opciones de HBase no predeterminadas</b> <div class="spoiler_text">  zookeeper.session.timeout: 120000 <br>  hbase.rpc.timeout: 2 minuto (s) <br>  hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.master.handler.count: 10 <br>  hbase.regionserver.lease.period, hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.regionserver.handler.count: 160 <br>  hbase.regionserver.metahandler.count: 30 <br>  hbase.regionserver.logroll.period: 4 hora (s) <br>  hbase.regionserver.maxlogs: 200 <br>  hbase.hregion.memstore.flush.size: 1 GiB <br>  hbase.hregion.memstore.block.multiplier: 6 <br>  hbase.hstore.compactionThreshold: 5 <br>  hbase.hstore.blockingStoreFiles: 200 <br>  hbase.hregion.majorcompaction: 1 día (s) <br>  Fragmento de configuración avanzada del servicio HBase (válvula de seguridad) para hbase-site.xml: <br>  hbase.regionserver.wal.codecorg.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec <br>  hbase.master.namespace.init.timeout3600000 <br>  hbase.regionserver.optionalcacheflushinterval18000000 <br>  hbase.regionserver.thread.compaction.large12 <br>  hbase.regionserver.wal.enablecompressiontrue <br>  hbase.hstore.compaction.max.size1073741824 <br>  hbase.server.compactchecker.interval.multiplier200 <br>  Opciones de configuración de Java para HBase RegionServer: <br>  -XX: + UseParNewGC -XX: + UseConcMarkSweepGC -XX: CMSInitiatingOccupancyFraction = 70 -XX: + CMSParallelRemarkEnabled -XX: ReservedCodeCacheSize = 256m <br>  hbase.snapshot.master.timeoutMillis: 2 minuto (s) <br>  hbase.snapshot.region.timeout: 2 minuto (s) <br>  hbase.snapshot.master.timeout.millis: 2 minuto (s) <br>  Tamaño máximo de registro del servidor REST de HBase: 100 MiB <br>  HBase REST Server Máximo de copias de seguridad de archivos de registro: 5 <br>  Tamaño máximo de registro del servidor HBase Thrift: 100 MiB <br>  HBase Thrift Server Respaldos máximos de archivos de registro: 5 <br>  Tamaño de registro maestro máximo: 100 MiB <br>  Master Máximo de copias de seguridad de archivos de registro: 5 <br>  Tamaño de registro máximo de RegionServer: 100 MiB <br>  Registros de archivos de registro máximos de RegionServer: 5 <br>  Ventana de detección de maestro activo de HBase: 4 minuto (s) <br>  dfs.client.hedged.read.threadpool.size: 40 <br>  dfs.client.hedged.read.threshold.millis: 10 milisegundos (s) <br>  hbase.rest.threads.min: 8 <br>  hbase.rest.threads.max: 150 <br>  Descriptores máximos de archivos de proceso: 180,000 <br>  hbase.thrift.minWorkerThreads: 200 <br>  hbase.master.executor.openregion.threads: 30 <br>  hbase.master.executor.closeregion.threads: 30 <br>  hbase.master.executor.serverops.threads: 60 <br>  hbase.regionserver.thread.compaction.small: 6 <br>  hbase.ipc.server.read.threadpool.size: 20 <br>  Hilos Mover Región: 6 <br>  Tamaño de almacenamiento dinámico Java del cliente en bytes: 1 GiB <br>  Grupo predeterminado del servidor REST de HBase: 3 GiB <br>  Grupo predeterminado del servidor HBase Thrift: 3 GiB <br>  Tamaño de almacenamiento dinámico de Java de HBase Master en bytes: 16 GiB <br>  Tamaño de almacenamiento dinámico de Java de HBase RegionServer en bytes: 32 GiB <br><br>  + ZooKeeper <br>  maxClientCnxns: 601 <br>  maxSessionTimeout: 120000 </div></div><br>  Crear tablas: <br>  <i>hbase org.apache.hadoop.hbase.util.RegionSplitter ns: t1 UniformSplit -c 64 -f cf</i> <i><br></i>  <i>alter 'ns: t1', {NAME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'FAST_DIFF', COMPRESSION =&gt; 'GZ'}</i> <br><br>  Hay un punto importante: la descripción de DataStax no dice cuántas regiones se usaron para crear las tablas HB, aunque esto es crítico para grandes volúmenes.  Por lo tanto, para las pruebas, se eligió el número = 64, que permite almacenar hasta 640 GB, es decir,  Mesa de tamaño mediano. <br><br>  En el momento de la prueba, HBase tenía 22 mil tablas y 67 mil regiones (esto sería mortal para la versión 1.2.0, si no fuera por el parche mencionado anteriormente). <br><br>  Ahora para el código.  Como no estaba claro qué configuraciones son más ventajosas para una base de datos particular, las pruebas se llevaron a cabo en varias combinaciones.  Es decir  En algunas pruebas, la carga fue simultáneamente a 4 tablas (se utilizaron los 4 nodos para la conexión).  En otras pruebas, trabajaron con 8 tablas diferentes.  En algunos casos, el tamaño del lote era 100, en otros 200 (parámetro del lote - ver el código a continuación).  El tamaño de los datos para el valor es de 10 bytes o 100 bytes (tamaño de datos).  En total, se escribieron y restaron 5 millones de registros cada vez en cada tabla.  Al mismo tiempo, se escribieron / leyeron 5 secuencias en cada tabla (el número de secuencia es thNum), cada una de las cuales utilizó su propio rango de claves (cuenta = 1 millón): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"BEGIN BATCH "</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); sb.append(<span class="hljs-string"><span class="hljs-string">"INSERT INTO "</span></span>) .append(tableName) .append(<span class="hljs-string"><span class="hljs-string">"(id, title) "</span></span>) .append(<span class="hljs-string"><span class="hljs-string">"VALUES ("</span></span>) .append(key) .append(<span class="hljs-string"><span class="hljs-string">", '"</span></span>) .append(value) .append(<span class="hljs-string"><span class="hljs-string">"');"</span></span>); key++; } sb.append(<span class="hljs-string"><span class="hljs-string">"APPLY BATCH;"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); session.execute(query); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"SELECT * FROM "</span></span>).append(tableName).append(<span class="hljs-string"><span class="hljs-string">" WHERE id IN ("</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { sb = sb.append(key); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i+<span class="hljs-number"><span class="hljs-number">1</span></span> &lt; batch) sb.append(<span class="hljs-string"><span class="hljs-string">","</span></span>); key++; } sb = sb.append(<span class="hljs-string"><span class="hljs-string">");"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); ResultSet rs = session.execute(query); } }</code> </pre><br>  En consecuencia, se proporcionó una funcionalidad similar para HB: <br><br><pre> <code class="java hljs">Configuration conf = getConf(); HTable table = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HTable(conf, keyspace + <span class="hljs-string"><span class="hljs-string">":"</span></span> + tableName); table.setAutoFlush(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); List&lt;Get&gt; lGet = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); List&lt;Put&gt; lPut = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] cf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"cf"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] qf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lPut.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Put p = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Put(makeHbaseRowKey(key)); String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); p.addColumn(cf, qf, value.getBytes()); lPut.add(p); key++; } table.put(lPut); table.flushCommits(); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lGet.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Get g = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Get(makeHbaseRowKey(key)); lGet.add(g); key++; } Result[] rs = table.get(lGet); } }</code> </pre><br>  Dado que el cliente debe ocuparse de la distribución uniforme de los datos en HB, la función de salazón clave se veía así: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] makeHbaseRowKey(<span class="hljs-keyword"><span class="hljs-keyword">long</span></span> key) { <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] nonSaltedRowKey = Bytes.toBytes(key); CRC32 crc32 = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CRC32(); crc32.update(nonSaltedRowKey); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> crc32Value = crc32.getValue(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] salt = Arrays.copyOfRange(Bytes.toBytes(crc32Value), <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ArrayUtils.addAll(salt, nonSaltedRowKey); }</code> </pre><br>  Ahora los más interesantes son los resultados: <br><br><img src="https://habrastorage.org/webt/id/yd/pc/idydpc9plsmulsycf0i-wqy3c3c.png"><br><br>  Lo mismo que un gráfico: <br><br><img src="https://habrastorage.org/webt/72/ag/o1/72ago1u2gdnlufjanqwavk5p1jk.png"><br><br>  La ventaja de HB es tan sorprendente que existe la sospecha de algún tipo de cuello de botella en la configuración de CS.  Sin embargo, la búsqueda en Google y la torsión de los parámetros más obvios (como concurrent_writes o memtable_heap_space_in_mb) no dieron aceleración.  Al mismo tiempo, los registros están limpios, no juren por nada. <br><br>  Los datos se distribuyen de manera uniforme en los nodos, las estadísticas de todos los nodos son aproximadamente las mismas. <br><br><div class="spoiler">  <b class="spoiler_title">Aquí están las estadísticas en la tabla con uno de los nodos</b> <div class="spoiler_text">  Keyspace: ks <br>  Leer cuenta: 9383707 <br>  Latencia de lectura: 0.04287025042448576 ms <br>  Cuenta de escritura: 15462012 <br>  Latencia de escritura: 0.1350068438699957 ms <br>  Lavados pendientes: 0 <br>  Tabla: t1 <br>  Cuenta de tabla: 16 <br>  Espacio utilizado (en vivo): 148.59 MiB <br>  Espacio utilizado (total): 148.59 MiB <br>  Espacio utilizado por las instantáneas (total): 0 bytes <br>  Memoria de almacenamiento dinámico utilizada (total): 5,17 MiB <br>  Relación de compresión SSTable: 0.5720989576459437 <br>  Número de particiones (estimado): 3970323 <br>  Recuento celular memorable: 0 <br>  Tamaño de datos memorables: 0 bytes <br>  Memoria de almacenamiento dinámico no utilizable: 0 bytes <br>  Recuento de conmutadores memorables: 5 <br>  Cuenta de lectura local: 2346045 <br>  Latencia de lectura local: NaN ms <br>  Recuento de escritura local: 3865503 <br>  Latencia de escritura local: NaN ms <br>  Rubores pendientes: 0 <br>  Porcentaje reparado: 0.0 <br>  Filtro de Bloom falsos positivos: 25 <br>  Relación falsa del filtro Bloom: 0.00000 <br>  Espacio de filtro de Bloom utilizado: 4.57 MiB <br>  Bloom filtro de memoria de montón utilizada: 4.57 MiB <br>  Resumen del índice de la memoria del montón utilizada: 590.02 KiB <br>  Metadatos de compresión de la memoria del montón utilizada: 19.45 KiB <br>  Bytes mínimos de partición compactada: 36 <br>  Bytes máximos de partición compactada: 42 <br>  Bytes medios de partición compactada: 42 <br>  Promedio de células vivas por corte (últimos cinco minutos): NaN <br>  Máximo de células vivas por segmento (últimos cinco minutos): 0 <br>  Promedio de lápidas por rebanada (últimos cinco minutos): NaN <br>  Lápidas máximas por rebanada (últimos cinco minutos): 0 <br>  Mutaciones descartadas: 0 bytes <br></div></div><br>  Un intento de reducir el tamaño del lote (hasta el envío uno por uno) no tuvo efecto, solo empeoró.  De hecho, es posible que este sea realmente el máximo rendimiento para CS, ya que los resultados obtenidos en CS son similares a los obtenidos para DataStax: alrededor de cientos de miles de operaciones por segundo.  Además, si observa la utilización de los recursos, verá que CS usa mucha más CPU y discos: <br><br><img src="https://habrastorage.org/webt/us/fo/i4/usfoi4-mgkktzlosilmz2ogm7uu.png"><br>  <i>La figura muestra la utilización durante la ejecución de todas las pruebas seguidas para ambas bases de datos.</i> <br><br>  En cuanto a los poderosos beneficios de lectura de HB.  Se puede ver que para ambas bases de datos, la utilización del disco durante la lectura es extremadamente baja (las pruebas de lectura son la parte final del ciclo de prueba para cada base de datos, por ejemplo, para CS de 15:20 a 15:40).  En el caso de HB, la razón es clara: la mayoría de los datos se cuelgan en la memoria, en el memstore, y algunos se almacenaron en caché en la caché de bloques.  En cuanto a CS, no está muy claro cómo funciona, sin embargo, la utilización del disco tampoco es visible, pero por si acaso, se hizo un intento de activar el caché row_cache_size_in_mb = 2048 y establecer el almacenamiento en caché = {'keys': 'ALL', 'rows_per_partition': ' 2,000,000 '}, pero eso lo empeoró aún más. <br><br>  También vale una vez más decir un punto significativo sobre el número de regiones en HB.  En nuestro caso, se indicó el valor 64. Si lo reduce y lo iguala a, por ejemplo, 4, entonces, al leer, la velocidad se reduce 2 veces.  La razón es que memstore se atascará más rápido y los archivos se enjuagarán con más frecuencia y, al leerlo, necesitará procesar más archivos, lo cual es una operación bastante complicada para HB.  En condiciones reales, esto puede tratarse pensando en la estrategia de preplit y compactificación, en particular, utilizamos una utilidad de fabricación propia que recolecta basura y comprime HFiles constantemente en segundo plano.  Es posible que para las pruebas DataStax, generalmente se asignara 1 región por tabla (lo cual no es correcto) y esto aclararía un poco por qué HB perdió tanto en sus pruebas de lectura. <br><br>  Las conclusiones preliminares de esto son las siguientes.  Asumiendo que no se cometieron errores graves durante las pruebas, Cassandra es como un coloso con pies de arcilla.  Más precisamente, mientras se balancea sobre una pierna, como en la imagen al comienzo del artículo, muestra resultados relativamente buenos, pero cuando pelea en las mismas condiciones, pierde directamente.  Al mismo tiempo, teniendo en cuenta la baja utilización de la CPU en nuestro hardware, aprendimos a plantar dos HB de RegionServer por host y, por lo tanto, duplicamos la productividad.  Es decir  Teniendo en cuenta la utilización de recursos, la situación de CS es aún más deplorable. <br><br>  Por supuesto, estas pruebas son bastante sintéticas y la cantidad de datos que se utilizó aquí es relativamente modesta.  Es posible que al cambiar a terabytes, la situación sea diferente, pero si para HB podemos cargar terabytes, entonces para CS esto resultó ser problemático.  A menudo arrojó una OperationTimedOutException incluso con estos volúmenes, aunque los parámetros de expectativa de respuesta ya aumentaron varias veces en comparación con los predeterminados. <br><br>  Espero que mediante esfuerzos conjuntos encontremos los cuellos de botella de CS y si logramos acelerarlo, entonces definitivamente agregaré información sobre los resultados finales al final de la publicación. <br><br>  <b>UPD: Las</b> siguientes pautas se aplicaron en la configuración de CS: <br><br>  <i>disk_optimization_strategy: spinning</i> <i><br></i>  <i>MAX_HEAP_SIZE = "32G"</i> <i><br></i>  <i>HEAP_NEWSIZE = "3200M"</i> <i><br></i>  <i>-Xms32G</i> <i><br></i>  <i>-Xmx32G</i> <i><br></i>  <i>-XX: + UseG1GC</i> <i><br></i>  <i>-XX: G1RSetUpdatingPauseTimePercent = 5</i> <i><br></i>  <i>-XX: MaxGCPauseMillis = 500</i> <i><br></i>  <i>-XX: InitiatingHeapOccupancyPercent = 70</i> <i><br></i>  <i>-XX: ParallelGCThreads = 32</i> <i><br></i>  <i>-XX: ConcGCThreads = 8</i> <br><br>  En cuanto a la configuración del sistema operativo, este es un procedimiento bastante largo y complicado (obtener root, reiniciar servidores, etc.), por lo que estas recomendaciones no se aplicaron.  Por otro lado, ambas bases de datos están en igualdad de condiciones, por lo que todo es justo. <br><br>  En la parte del código, se crea un conector para todos los hilos que escriben en la tabla: <br><pre> <code class="java hljs">connector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CassandraConnector(); connector.connect(node, <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, CL); session = connector.getSession(); session.getCluster().getConfiguration().getSocketOptions().setConnectTimeoutMillis(<span class="hljs-number"><span class="hljs-number">120000</span></span>); KeyspaceRepository sr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KeyspaceRepository(session); sr.useKeyspace(keyspace); prepared = session.prepare(<span class="hljs-string"><span class="hljs-string">"insert into "</span></span> + tableName + <span class="hljs-string"><span class="hljs-string">" (id, title) values (?, ?)"</span></span>);</code> </pre> <br><br>  Los datos se enviaron por enlace: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); session.execute(prepared.bind(key, value)); }</code> </pre> <br><br>  Esto no tuvo un impacto significativo en el rendimiento de grabación.  Para mayor fiabilidad, lancé la carga con la herramienta YCSB, absolutamente el mismo resultado.  A continuación se muestran las estadísticas para un hilo (de 4): <br><br>  <i>2020-01-18 14: 41: 53: 180 315 segundos: 10,000,000 operaciones;</i>  <i>21589.1 operaciones actuales / seg;</i>  <i>[LIMPIEZA: Cuenta = 100, Máx = 2236415, Mín = 1, Promedio = 22356.39, 90 = 4, 99 = 24, 99.9 = 2236415, 99.99 = 2236415] [INSERTAR: Cuenta = 119551, Máx = 174463, Mín = 273, Promedio = 2582.71, 90 = 3491, 99 = 16767, 99.9 = 99711, 99.99 = 171263]</i> <i><br></i>  <i>[GENERAL], RunTime (ms), 315539</i> <i><br></i>  <i>[GENERAL], rendimiento (ops / seg), 31691.803548848162</i> <i><br></i>  <i>[TOTAL_GCS_PS_Scavenge], Cuenta, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.7710615803434757</i> <i><br></i>  <i>[TOTAL_GCS_PS_MarkSweep], recuento, 0</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 0</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.0</i> <i><br></i>  <i>[TOTAL_GCs], recuento, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME], tiempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME_%], Tiempo (%), 0.7710615803434757</i> <i><br></i>  <i>[INSERTAR], Operaciones, 10,000,000</i> <i><br></i>  <i>[INSERTAR], Promedio de latencia (us), 3114.2427012</i> <i><br></i>  <i>[INSERTAR], MinLatency (nosotros), 269</i> <i><br></i>  <i>[INSERTAR], MaxLatency (nosotros), 609279</i> <i><br></i>  <i>[INSERTAR], 95thPercentileLatency (us), 5007</i> <i><br></i>  <i>[INSERTAR], 99thPercentileLatency (nosotros), 33439</i> <i><br></i>  <i>[INSERTAR], Retorno = OK, 10000000</i> <i><br></i> <br><br>  Aquí puede ver que la velocidad de una transmisión es de aproximadamente 32 mil registros por segundo, 4 transmisiones funcionaron, resulta 128 mil. Parece que no hay nada más que exprimir en la configuración actual del subsistema de disco. <br><br>  Sobre leer más interesante.  Gracias al consejo de camaradas, pudo acelerar radicalmente.  La lectura se realizó no en 5 secuencias, sino en 100. Un aumento a 200 no produjo un efecto.  También agregado al constructor: <br>  .withLoadBalancingPolicy (nuevo TokenAwarePolicy (DCAwareRoundRobinPolicy.builder (). build ())) <br><br>  Como resultado, si antes la prueba mostraba 159 644 operaciones (5 transmisiones, 4 tablas, 100 lotes), ahora: <br>  100 hilos, 4 tablas, lote = 1 (individualmente): 301969 operaciones <br>  100 hilos, 4 tablas, lote = 10: 447 608 operaciones <br>  100 hilos, 4 tablas, lote = 100: 625 655 operaciones <br><br>  Como los resultados son mejores con lotes, realicé pruebas similares * con HB: <br><img src="https://habrastorage.org/webt/ct/bk/-y/ctbk-yrecbwegasrbpauq6f1vv8.png"><br>  <i>* Dado que cuando se trabajaba en 400 subprocesos, la función RandomStringUtils, que se usaba anteriormente, cargaba la CPU en un 100%, fue reemplazada por un generador más rápido.</i> <br><br>  Por lo tanto, un aumento en el número de subprocesos al cargar datos da un pequeño aumento en el rendimiento de HB. <br><br>  En cuanto a la lectura, aquí están los resultados de varias opciones.  A pedido de <a href="https://habr.com/ru/users/0x62ash/" class="user_link">0x62ash</a> , el comando flush se ejecutó antes de leer, y también se ofrecen otras opciones para comparar: <br>  Memstore: lectura de memoria, es decir  antes de enjuagar al disco. <br>  HFile + zip: lectura de archivos comprimidos por el algoritmo GZ. <br>  HFile + upzip: lee archivos sin compresión. <br><br>  Una característica interesante es notable: los archivos pequeños (consulte el campo "Datos", donde se escriben 10 bytes) se procesan más lentamente, especialmente si están comprimidos.  Obviamente, esto solo es posible hasta cierto tamaño, obviamente, un archivo de 5 GB no se procesará más rápido que 10 MB, pero indica claramente que en todas estas pruebas todavía no hay un campo arado para investigar varias configuraciones. <br><br>  Por interés, corregí el código YCSB para trabajar con lotes HB de 100 piezas para medir la latencia y más.  A continuación se muestra el resultado del trabajo de 4 copias que escribieron en sus tablas, cada una con 100 hilos.  Resultó lo siguiente: <br><div class="spoiler">  <b class="spoiler_title">Una operación = 100 registros</b> <div class="spoiler_text">  [GENERAL], RunTime (ms), 1165415 <br>  [GENERAL], Rendimiento (ops / seg), 858.06343662987 <br>  [TOTAL_GCS_PS_Scavenge], Cuenta, 798 <br>  [TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 7346 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.6303334005483026 <br>  [TOTAL_GCS_PS_MarkSweep], cuenta, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 74 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.006349669431061038 <br>  [TOTAL_GCs], recuento, 799 <br>  [TOTAL_GC_TIME], tiempo (ms), 7420 <br>  [TOTAL_GC_TIME_%], Tiempo (%), 0.6366830699793635 <br>  [INSERTAR], Operaciones, 1,000,000 <br>  [INSERTAR], Latencia promedio (us), 115893.891644 <br>  [INSERTAR], MinLatency (nosotros), 14528 <br>  [INSERTAR], MaxLatency (nosotros), 1470463 <br>  [INSERTAR], 95thPercentileLatency (us), 248319 <br>  [INSERTAR], 99thPercentileLatency (nosotros), 445951 <br>  [INSERTAR], Retorno = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Closing zookeeper sessionid = 0x36f98ad0a4ad8cc <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sesión: 0x36f98ad0a4ad8cc cerrado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread cerrado <br>  [GENERAL], RunTime (ms), 1165806 <br>  [GENERAL], Rendimiento (ops / seg), 857.7756504941646 <br>  [TOTAL_GCS_PS_Scavenge], Cuenta, 776 <br>  [TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 7517 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.6447899564764635 <br>  [TOTAL_GCS_PS_MarkSweep], cuenta, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 63 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.005403986598113236 <br>  [TOTAL_GCs], cuenta, 777 <br>  [TOTAL_GC_TIME], tiempo (ms), 7580 <br>  [TOTAL_GC_TIME_%], Tiempo (%), 0.6501939430745767 <br>  [INSERTAR], Operaciones, 1,000,000 <br>  [INSERTAR], Latencia promedio (EE. UU.), 116042.207936 <br>  [INSERTAR], MinLatency (nosotros), 14056 <br>  [INSERTAR], MaxLatency (nosotros), 1462271 <br>  [INSERTAR], 95thPercentileLatency (us), 250239 <br>  [INSERTAR], 99thPercentileLatency (nosotros), 446719 <br>  [INSERTAR], Retorno = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Closing zookeeper sessionid = 0x26f98ad07b6d67e <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sesión: 0x26f98ad07b6d67e cerrado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread cerrado <br>  [GENERAL], RunTime (ms), 1165999 <br>  [GENERAL], rendimiento (ops / seg), 857.63366863951 <br>  [TOTAL_GCS_PS_Scavenge], recuento, 818 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7557 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6481137633908777 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 79 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.006775305982252128 <br> [TOTAL_GCs], Count, 819 <br> [TOTAL_GC_TIME], Time(ms), 7636 <br> [TOTAL_GC_TIME_%], Time(%), 0.6548890693731299 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116172.212864 <br> [INSERT], MinLatency(us), 7952 <br> [INSERT], MaxLatency(us), 1458175 <br> [INSERT], 95thPercentileLatency(us), 250879 <br> [INSERT], 99thPercentileLatency(us), 446463 <br> [INSERT], Return=OK, 1000000 <br><br> 20/01/19 13:19:17 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36f98ad0a4ad8cd <br> 20/01/19 13:19:17 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cd closed <br> 20/01/19 13:19:17 INFO zookeeper.ClientCnxn: EventThread shut down <br> [OVERALL], RunTime(ms), 1166860 <br> [OVERALL], Throughput(ops/sec), 857.000839860823 <br> [TOTAL_GCS_PS_Scavenge], Count, 707 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7239 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6203829079752499 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 67 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.0057419056270675145 <br> [TOTAL_GCs], Count, 708 <br> [TOTAL_GC_TIME], Time(ms), 7306 <br> [TOTAL_GC_TIME_%], Time(%), 0.6261248136023173 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116230.849308 <br> [INSERT], MinLatency(us), 7352 <br> [INSERT], MaxLatency(us), 1443839 <br> [INSERT], 95thPercentileLatency(us), 250623 <br> [INSERT], 99thPercentileLatency(us), 447487 <br> [INSERT], Return=OK, 1000000 </div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resulta que si CS AverageLatency (us) tiene un registro de 3114, entonces HB AverageLatency (us) = 1162 (recuerde que 1 operación = 100 registros y, por lo tanto, debe dividirse). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En general, se obtiene esta conclusión: bajo condiciones dadas, hay una ventaja significativa de HBase. </font><font style="vertical-align: inherit;">Sin embargo, no se puede descartar que el SSD y el ajuste cuidadoso del sistema operativo cambien radicalmente la imagen. </font><font style="vertical-align: inherit;">También debe comprender que mucho depende de los escenarios de uso, puede resultar fácilmente que si no toma 4 tablas, sino 400 y trabaja con terabytes, el equilibrio de fuerzas se desarrollará de una manera completamente diferente. </font><font style="vertical-align: inherit;">Como decían los clásicos: la práctica es el criterio de la verdad. </font><font style="vertical-align: inherit;">Tienes que intentarlo. </font><font style="vertical-align: inherit;">Por un lado, ScyllaDB ahora tiene sentido verificar, por lo que debe continuar ...</font></font></div></div><p>Source: <a href="https://habr.com/ru/post/484096/">https://habr.com/ru/post/484096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../484084/index.html">1C-Bitrix y un intento de presentarlo</a></li>
<li><a href="../484088/index.html">Desfile de contraseñas (análisis de ~ 5 mil millones de contraseñas de fugas)</a></li>
<li><a href="../484090/index.html">Nueva infraestructura de TI para el centro de datos de correos de Rusia</a></li>
<li><a href="../484092/index.html">Algo príncipes y nobles vestidos</a></li>
<li><a href="../484094/index.html">Crea un juego de disparos de zombis en tercera persona con DOTS</a></li>
<li><a href="../484100/index.html">Trabajar con la interfaz en el SDK de Google Maps para Android</a></li>
<li><a href="../484102/index.html">PHP vs Python vs Ruby on Rails: Comparación detallada</a></li>
<li><a href="../484106/index.html">MVCC en PostgreSQL-6. Vacío</a></li>
<li><a href="../484108/index.html">Etherblade.net Encapsulador y sustitución de importaciones para componentes de red (segunda parte)</a></li>
<li><a href="../484112/index.html">¿Es posible hackear un avión?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>