<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>游끨 丘뙖잺 游돜游 La batalla de los dos Yakozun, o Cassandra vs HBase. Experiencia del equipo de Sberbank 游뱜游낕 游뱉游낖 游녞游</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Esto ni siquiera es una broma, parece que esta imagen en particular refleja con mayor precisi칩n la esencia de estas bases de datos, y al final quedar치...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La batalla de los dos Yakozun, o Cassandra vs HBase. Experiencia del equipo de Sberbank</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/484096/">  Esto ni siquiera es una broma, parece que esta imagen en particular refleja con mayor precisi칩n la esencia de estas bases de datos, y al final quedar치 claro por qu칠: <br><br><img src="https://habrastorage.org/webt/i2/lk/zo/i2lkzo9tq7zpeprcbtgm3-mufk4.png"><br><br>  Seg칰n el ranking DB-Engines, las dos bases de columnas NoSQL m치s populares son Cassandra (en adelante CS) y HBase (HB). <br><br><img src="https://habrastorage.org/webt/su/rd/39/surd39n7bmrbnxgpn0512tnxamm.png"><br><br>  Por voluntad del destino, nuestro equipo de gesti칩n de carga de datos en Sberbank ha <a href="https://habr.com/ru/company/sberbank/blog/420425/">estado</a> trabajando estrechamente con HB durante <a href="https://habr.com/ru/company/sberbank/blog/420425/">mucho tiempo</a> .  Durante este tiempo, estudiamos sus fortalezas y debilidades bastante bien y aprendimos a cocinarlo.  Sin embargo, la presencia de una alternativa en forma de CS todo el tiempo me hizo atormentarme con dudas: 쯦omamos la decisi칩n correcta?  Adem치s, los resultados de la <a href="https://www.datastax.com/products/compare/nosql-performance-benchmarks">comparaci칩n</a> llevada a cabo por DataStax dijeron que CS f치cilmente derrota a HB con una puntuaci칩n casi aplastante.  Por otro lado, DataStax es una persona interesada, y no debe decir nada aqu칤.  Adem치s, una cantidad bastante peque침a de informaci칩n sobre las condiciones de prueba era vergonzosa, por lo que decidimos averiguar por nuestra cuenta qui칠n es el rey de BigData NoSql, y los resultados fueron muy interesantes. <br><a name="habracut"></a><br>  Sin embargo, antes de pasar a los resultados de las pruebas realizadas, es necesario describir los aspectos esenciales de las configuraciones del entorno.  El hecho es que CS se puede usar en modo de tolerancia de p칠rdida de datos.  Es decir  Esto es cuando solo un servidor (nodo) es responsable de los datos de una determinada clave, y si se cae por alguna raz칩n, el valor de esta clave se perder치.  Para muchas tareas esto no es cr칤tico, pero para el sector bancario esta es la excepci칩n m치s que la regla.  En nuestro caso, es importante tener varias copias de datos para un almacenamiento confiable. <br><br>  Por lo tanto, solo se consider칩 el modo CS de triple replicaci칩n, es decir,  La creaci칩n de casos se realiz칩 con los siguientes par치metros: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> KEYSPACE ks <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> <span class="hljs-keyword"><span class="hljs-keyword">REPLICATION</span></span> = {<span class="hljs-string"><span class="hljs-string">'class'</span></span> : <span class="hljs-string"><span class="hljs-string">'NetworkTopologyStrategy'</span></span>, <span class="hljs-string"><span class="hljs-string">'datacenter1'</span></span> : <span class="hljs-number"><span class="hljs-number">3</span></span>};</code> </pre> <br>  Adem치s, hay dos formas de garantizar el nivel de consistencia requerido.  Regla general: <br>  NW + NR&gt; RF <br><br>  Esto significa que el n칰mero de confirmaciones de los nodos al escribir (NW) m치s el n칰mero de confirmaciones de los nodos al leer (NR) debe ser mayor que el factor de replicaci칩n.  En nuestro caso, RF = 3, por lo que las siguientes opciones son adecuadas: <br>  2 + 2&gt; 3 <br>  3 + 1&gt; 3 <br><br>  Dado que es fundamental para nosotros mantener los datos lo m치s confiables posible, se eligi칩 un esquema 3 + 1.  Adem치s, HB funciona de manera similar, es decir  Tal comparaci칩n ser칤a m치s honesta. <br><br>  Cabe se침alar que DataStax hizo lo contrario en su investigaci칩n, establecieron RF = 1 para CS y HB (para este 칰ltimo al cambiar la configuraci칩n de HDFS).  Este es un aspecto realmente importante, porque el impacto en el rendimiento de CS en este caso es enorme.  Por ejemplo, la siguiente imagen muestra el aumento de tiempo requerido para cargar datos en CS: <br><br><img src="https://habrastorage.org/webt/fw/az/r9/fwazr9muypegpgjpyaq4rnagg8u.png"><br><br>  Aqu칤 vemos lo siguiente: mientras m치s hilos de la competencia escriben datos, m치s tiempo demora.  Esto es natural, pero es importante que la degradaci칩n del rendimiento para RF = 3 sea significativamente mayor.  En otras palabras, si escribimos en 4 tablas en cada una de las 5 transmisiones (un total de 20), entonces RF = 3 pierde aproximadamente 2 veces (150 segundos RF = 3 frente a 75 para RF = 1).  Pero si aumentamos la carga al cargar datos en 8 tablas en cada una de las 5 transmisiones (un total de 40), perder RF = 3 ya es 2,7 veces (375 segundos frente a 138). <br><br>  Quiz치s en parte este es el secreto de la prueba exitosa de DataStax para la prueba de carga CS, porque para HB en nuestro stand, cambiar el factor de replicaci칩n de 2 a 3 no tuvo ning칰n efecto.  Es decir  Los discos no son el cuello de botella para HB para nuestra configuraci칩n.  Sin embargo, hay muchas otras trampas, porque debe tenerse en cuenta que nuestra versi칩n de HB fue ligeramente parcheada y oscurecida, los entornos son completamente diferentes, etc.  Tambi칠n vale la pena se침alar que tal vez simplemente no s칠 c칩mo preparar CS correctamente y hay algunas formas m치s efectivas de trabajar con 칠l y espero en los comentarios que descubramos.  Pero lo primero es lo primero. <br><br>  Todas las pruebas se realizaron en un cl칰ster de hierro que consta de 4 servidores, cada uno en una configuraci칩n: <br><br>  <i>CPU: Xeon E5-2680 v4 @ 2.40GHz 64 hilos.</i> <i><br></i>  <i>Discos: 12 unidades de disco duro SATA</i> <i><br></i>  <i>versi칩n de Java: 1.8.0_111</i> <i><br></i> <br><br>  Versi칩n CS: 3.11.5 <br><br><div class="spoiler">  <b class="spoiler_title">Par치metros cassandra.yml</b> <div class="spoiler_text">  num_tokens: 256 <br>  hinted_handoff_enabled: true <br>  hinted_handoff_throttle_in_kb: 1024 <br>  max_hints_delivery_threads: 2 <br>  directorio_consejos: / data10 / cassandra / consejos <br>  hints_flush_period_in_ms: 10000 <br>  max_hints_file_size_in_mb: 128 <br>  batchlog_replay_throttle_in_kb: 1024 <br>  autenticador: AllowAllAuthenticator <br>  autorizador: AllowAllAuthorizer <br>  role_manager: CassandraRoleManager <br>  roles_validity_in_ms: 2000 <br>  permisos_validez_en_ms: 2000 <br>  credentials_validity_in_ms: 2000 <br>  particionador: org.apache.cassandra.dht.Murmur3Partitioner <br>  data_file_directories: <br>  - / data1 / cassandra / data # cada directorio dataN es una unidad separada <br>  - / data2 / cassandra / data <br>  - / data3 / cassandra / data <br>  - / data4 / cassandra / data <br>  - / data5 / cassandra / data <br>  - / data6 / cassandra / data <br>  - / data7 / cassandra / data <br>  - / data8 / cassandra / data <br>  commit_directory: / data9 / cassandra / commitlog <br>  cdc_enabled: false <br>  disk_failure_policy: stop <br>  commit_failure_policy: detener <br>  ready_statements_cache_size_mb: <br>  thrift_prepared_statements_cache_size_mb: <br>  key_cache_size_in_mb: <br>  key_cache_save_period: 14400 <br>  row_cache_size_in_mb: 0 <br>  row_cache_save_period: 0 <br>  counter_cache_size_in_mb: <br>  counter_cache_save_period: 7200 <br>  directorio_cach칠s_ guardados: / data10 / cassandra / salva_cach칠s <br>  commitlog_sync: peri칩dico <br>  commitlog_sync_period_in_ms: 10000 <br>  commitlog_segment_size_in_mb: 32 <br>  seed_provider: <br>  - class_name: org.apache.cassandra.locator.SimpleSeedProvider <br>  par치metros: <br>  - semillas: "*, *" <br>  concurrent_reads: 256 # intent칩 64 - no se not칩 diferencia <br>  concurrent_writes: 256 # intent칩 64 - no se not칩 ninguna diferencia <br>  concurrent_counter_writes: 256 # intent칩 64 - no se not칩 ninguna diferencia <br>  concurrent_materialized_view_writes: 32 <br>  memtable_heap_space_in_mb: 2048 # intent칩 16 GB - fue m치s lento <br>  memtable_allocation_type: heap_buffers <br>  index_summary_capacity_in_mb: <br>  index_summary_resize_interval_in_minutes: 60 <br>  trickle_fsync: false <br>  trickle_fsync_interval_in_kb: 10240 <br>  puerto_almacenamiento: 7000 <br>  ssl_storage_port: 7001 <br>  listen_address: * <br>  broadcast_address: * <br>  listen_on_broadcast_address: verdadero <br>  internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator <br>  start_native_transport: true <br>  native_transport_port: 9042 <br>  start_rpc: verdadero <br>  rpc_address: * <br>  rpc_port: 9160 <br>  rpc_keepalive: verdadero <br>  rpc_server_type: sincronizaci칩n <br>  thrift_framed_transport_size_in_mb: 15 <br>  incremental_backups: falso <br>  snapshot_before_compaction: false <br>  auto_snapshot: true <br>  column_index_size_in_kb: 64 <br>  column_index_cache_size_in_kb: 2 <br>  concurrent_compactors: 4 <br>  compaction_throughput_mb_per_sec: 1600 <br>  sstable_preemptive_open_interval_in_mb: 50 <br>  read_request_timeout_in_ms: 100000 <br>  range_request_timeout_in_ms: 200000 <br>  write_request_timeout_in_ms: 40000 <br>  counter_write_request_timeout_in_ms: 100000 <br>  cas_contention_timeout_in_ms: 20000 <br>  truncate_request_timeout_in_ms: 60000 <br>  request_timeout_in_ms: 200000 <br>  slow_query_log_timeout_in_ms: 500 <br>  cross_node_timeout: falso <br>  endpoint_snitch: GossipingPropertyFileSnitch <br>  dynamic_snitch_update_interval_in_ms: 100 <br>  dynamic_snitch_reset_interval_in_ms: 600000 <br>  dynamic_snitch_badness_threshold: 0.1 <br>  request_scheduler: org.apache.cassandra.scheduler.NoScheduler <br>  opciones_encriptaci칩n_servidor: <br>  cifrado internode: ninguno <br>  opciones_cifrado_cliente: <br>  habilitado: falso <br>  internode_compression: dc <br>  inter_dc_tcp_nodelay: falso <br>  tracetype_query_ttl: 86400 <br>  tracetype_repair_ttl: 604800 <br>  enable_user_defined_functions: false <br>  enable_scripted_user_defined_functions: false <br>  windows_timer_interval: 1 <br>  opciones_encriptaci칩n_de_datos_transparentes: <br>  habilitado: falso <br>  tombstone_warn_threshold: 1000 <br>  Tombstone_failure_threshold: 100000 <br>  batch_size_warn_threshold_in_kb: 200 <br>  batch_size_fail_threshold_in_kb: 250 <br>  unlogged_batch_across_partitions_warn_threshold: 10 <br>  compaction_large_partition_warning_threshold_mb: 100 <br>  gc_warn_threshold_in_ms: 1000 <br>  back_pressure_enabled: false <br>  enable_materialized_views: true <br>  enable_sasi_indexes: true <br></div></div><br>  Configuraci칩n de GC: <br><br><div class="spoiler">  <b class="spoiler_title">### Configuraci칩n de CMS</b> <div class="spoiler_text">  -XX: + UseParNewGC <br>  -XX: + UseConcMarkSweepGC <br>  -XX: + CMSParallelRemarkEnabled <br>  -XX: SurvivorRatio = 8 <br>  -XX: MaxTenuringThreshold = 1 <br>  -XX: CMSInitiatingOccupancyFraction = 75 <br>  -XX: + UseCMSInitiatingOccupancyOnly <br>  -XX: CMSWaitDuration = 10000 <br>  -XX: + CMSParallelInitialMarkEnabled <br>  -XX: + CMSEdenChunksRecordAlways <br>  -XX: + CMSClassUnloadingEnabled <br><br></div></div><br>  La memoria jvm.options se asign칩 a 16 Gb (a칰n se intent칩 32 Gb, no se not칩 ninguna diferencia). <br><br>  La creaci칩n de tablas fue realizada por el comando: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> ks.t1 (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> PRIMARY <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, title <span class="hljs-built_in"><span class="hljs-built_in">text</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> compression = {<span class="hljs-string"><span class="hljs-string">'sstable_compression'</span></span>: <span class="hljs-string"><span class="hljs-string">'LZ4Compressor'</span></span>, <span class="hljs-string"><span class="hljs-string">'chunk_length_kb'</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span>};</code> </pre> <br>  Versi칩n HB: 1.2.0-cdh5.14.2 (en la clase org.apache.hadoop.hbase.regionserver.HRegion excluimos MetricsRegion que condujo a GC con m치s de 1000 regiones en RegionServer) <br><br><div class="spoiler">  <b class="spoiler_title">Opciones de HBase no predeterminadas</b> <div class="spoiler_text">  zookeeper.session.timeout: 120000 <br>  hbase.rpc.timeout: 2 minuto (s) <br>  hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.master.handler.count: 10 <br>  hbase.regionserver.lease.period, hbase.client.scanner.timeout.period: 2 minuto (s) <br>  hbase.regionserver.handler.count: 160 <br>  hbase.regionserver.metahandler.count: 30 <br>  hbase.regionserver.logroll.period: 4 hora (s) <br>  hbase.regionserver.maxlogs: 200 <br>  hbase.hregion.memstore.flush.size: 1 GiB <br>  hbase.hregion.memstore.block.multiplier: 6 <br>  hbase.hstore.compactionThreshold: 5 <br>  hbase.hstore.blockingStoreFiles: 200 <br>  hbase.hregion.majorcompaction: 1 d칤a (s) <br>  Fragmento de configuraci칩n avanzada del servicio HBase (v치lvula de seguridad) para hbase-site.xml: <br>  hbase.regionserver.wal.codecorg.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec <br>  hbase.master.namespace.init.timeout3600000 <br>  hbase.regionserver.optionalcacheflushinterval18000000 <br>  hbase.regionserver.thread.compaction.large12 <br>  hbase.regionserver.wal.enablecompressiontrue <br>  hbase.hstore.compaction.max.size1073741824 <br>  hbase.server.compactchecker.interval.multiplier200 <br>  Opciones de configuraci칩n de Java para HBase RegionServer: <br>  -XX: + UseParNewGC -XX: + UseConcMarkSweepGC -XX: CMSInitiatingOccupancyFraction = 70 -XX: + CMSParallelRemarkEnabled -XX: ReservedCodeCacheSize = 256m <br>  hbase.snapshot.master.timeoutMillis: 2 minuto (s) <br>  hbase.snapshot.region.timeout: 2 minuto (s) <br>  hbase.snapshot.master.timeout.millis: 2 minuto (s) <br>  Tama침o m치ximo de registro del servidor REST de HBase: 100 MiB <br>  HBase REST Server M치ximo de copias de seguridad de archivos de registro: 5 <br>  Tama침o m치ximo de registro del servidor HBase Thrift: 100 MiB <br>  HBase Thrift Server Respaldos m치ximos de archivos de registro: 5 <br>  Tama침o de registro maestro m치ximo: 100 MiB <br>  Master M치ximo de copias de seguridad de archivos de registro: 5 <br>  Tama침o de registro m치ximo de RegionServer: 100 MiB <br>  Registros de archivos de registro m치ximos de RegionServer: 5 <br>  Ventana de detecci칩n de maestro activo de HBase: 4 minuto (s) <br>  dfs.client.hedged.read.threadpool.size: 40 <br>  dfs.client.hedged.read.threshold.millis: 10 milisegundos (s) <br>  hbase.rest.threads.min: 8 <br>  hbase.rest.threads.max: 150 <br>  Descriptores m치ximos de archivos de proceso: 180,000 <br>  hbase.thrift.minWorkerThreads: 200 <br>  hbase.master.executor.openregion.threads: 30 <br>  hbase.master.executor.closeregion.threads: 30 <br>  hbase.master.executor.serverops.threads: 60 <br>  hbase.regionserver.thread.compaction.small: 6 <br>  hbase.ipc.server.read.threadpool.size: 20 <br>  Hilos Mover Regi칩n: 6 <br>  Tama침o de almacenamiento din치mico Java del cliente en bytes: 1 GiB <br>  Grupo predeterminado del servidor REST de HBase: 3 GiB <br>  Grupo predeterminado del servidor HBase Thrift: 3 GiB <br>  Tama침o de almacenamiento din치mico de Java de HBase Master en bytes: 16 GiB <br>  Tama침o de almacenamiento din치mico de Java de HBase RegionServer en bytes: 32 GiB <br><br>  + ZooKeeper <br>  maxClientCnxns: 601 <br>  maxSessionTimeout: 120000 </div></div><br>  Crear tablas: <br>  <i>hbase org.apache.hadoop.hbase.util.RegionSplitter ns: t1 UniformSplit -c 64 -f cf</i> <i><br></i>  <i>alter 'ns: t1', {NAME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'FAST_DIFF', COMPRESSION =&gt; 'GZ'}</i> <br><br>  Hay un punto importante: la descripci칩n de DataStax no dice cu치ntas regiones se usaron para crear las tablas HB, aunque esto es cr칤tico para grandes vol칰menes.  Por lo tanto, para las pruebas, se eligi칩 el n칰mero = 64, que permite almacenar hasta 640 GB, es decir,  Mesa de tama침o mediano. <br><br>  En el momento de la prueba, HBase ten칤a 22 mil tablas y 67 mil regiones (esto ser칤a mortal para la versi칩n 1.2.0, si no fuera por el parche mencionado anteriormente). <br><br>  Ahora para el c칩digo.  Como no estaba claro qu칠 configuraciones son m치s ventajosas para una base de datos particular, las pruebas se llevaron a cabo en varias combinaciones.  Es decir  En algunas pruebas, la carga fue simult치neamente a 4 tablas (se utilizaron los 4 nodos para la conexi칩n).  En otras pruebas, trabajaron con 8 tablas diferentes.  En algunos casos, el tama침o del lote era 100, en otros 200 (par치metro del lote - ver el c칩digo a continuaci칩n).  El tama침o de los datos para el valor es de 10 bytes o 100 bytes (tama침o de datos).  En total, se escribieron y restaron 5 millones de registros cada vez en cada tabla.  Al mismo tiempo, se escribieron / leyeron 5 secuencias en cada tabla (el n칰mero de secuencia es thNum), cada una de las cuales utiliz칩 su propio rango de claves (cuenta = 1 mill칩n): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"BEGIN BATCH "</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); sb.append(<span class="hljs-string"><span class="hljs-string">"INSERT INTO "</span></span>) .append(tableName) .append(<span class="hljs-string"><span class="hljs-string">"(id, title) "</span></span>) .append(<span class="hljs-string"><span class="hljs-string">"VALUES ("</span></span>) .append(key) .append(<span class="hljs-string"><span class="hljs-string">", '"</span></span>) .append(value) .append(<span class="hljs-string"><span class="hljs-string">"');"</span></span>); key++; } sb.append(<span class="hljs-string"><span class="hljs-string">"APPLY BATCH;"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); session.execute(query); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { StringBuilder sb = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StringBuilder(<span class="hljs-string"><span class="hljs-string">"SELECT * FROM "</span></span>).append(tableName).append(<span class="hljs-string"><span class="hljs-string">" WHERE id IN ("</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { sb = sb.append(key); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i+<span class="hljs-number"><span class="hljs-number">1</span></span> &lt; batch) sb.append(<span class="hljs-string"><span class="hljs-string">","</span></span>); key++; } sb = sb.append(<span class="hljs-string"><span class="hljs-string">");"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> String query = sb.toString(); ResultSet rs = session.execute(query); } }</code> </pre><br>  En consecuencia, se proporcion칩 una funcionalidad similar para HB: <br><br><pre> <code class="java hljs">Configuration conf = getConf(); HTable table = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HTable(conf, keyspace + <span class="hljs-string"><span class="hljs-string">":"</span></span> + tableName); table.setAutoFlush(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); List&lt;Get&gt; lGet = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); List&lt;Put&gt; lPut = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] cf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"cf"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] qf = Bytes.toBytes(<span class="hljs-string"><span class="hljs-string">"value"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (opType.equals(<span class="hljs-string"><span class="hljs-string">"insert"</span></span>)) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lPut.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Put p = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Put(makeHbaseRowKey(key)); String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); p.addColumn(cf, qf, value.getBytes()); lPut.add(p); key++; } table.put(lPut); table.flushCommits(); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key += <span class="hljs-number"><span class="hljs-number">0</span></span>) { lGet.clear(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; batch; i++) { Get g = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Get(makeHbaseRowKey(key)); lGet.add(g); key++; } Result[] rs = table.get(lGet); } }</code> </pre><br>  Dado que el cliente debe ocuparse de la distribuci칩n uniforme de los datos en HB, la funci칩n de salaz칩n clave se ve칤a as칤: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] makeHbaseRowKey(<span class="hljs-keyword"><span class="hljs-keyword">long</span></span> key) { <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] nonSaltedRowKey = Bytes.toBytes(key); CRC32 crc32 = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CRC32(); crc32.update(nonSaltedRowKey); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> crc32Value = crc32.getValue(); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[] salt = Arrays.copyOfRange(Bytes.toBytes(crc32Value), <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ArrayUtils.addAll(salt, nonSaltedRowKey); }</code> </pre><br>  Ahora los m치s interesantes son los resultados: <br><br><img src="https://habrastorage.org/webt/id/yd/pc/idydpc9plsmulsycf0i-wqy3c3c.png"><br><br>  Lo mismo que un gr치fico: <br><br><img src="https://habrastorage.org/webt/72/ag/o1/72ago1u2gdnlufjanqwavk5p1jk.png"><br><br>  La ventaja de HB es tan sorprendente que existe la sospecha de alg칰n tipo de cuello de botella en la configuraci칩n de CS.  Sin embargo, la b칰squeda en Google y la torsi칩n de los par치metros m치s obvios (como concurrent_writes o memtable_heap_space_in_mb) no dieron aceleraci칩n.  Al mismo tiempo, los registros est치n limpios, no juren por nada. <br><br>  Los datos se distribuyen de manera uniforme en los nodos, las estad칤sticas de todos los nodos son aproximadamente las mismas. <br><br><div class="spoiler">  <b class="spoiler_title">Aqu칤 est치n las estad칤sticas en la tabla con uno de los nodos</b> <div class="spoiler_text">  Keyspace: ks <br>  Leer cuenta: 9383707 <br>  Latencia de lectura: 0.04287025042448576 ms <br>  Cuenta de escritura: 15462012 <br>  Latencia de escritura: 0.1350068438699957 ms <br>  Lavados pendientes: 0 <br>  Tabla: t1 <br>  Cuenta de tabla: 16 <br>  Espacio utilizado (en vivo): 148.59 MiB <br>  Espacio utilizado (total): 148.59 MiB <br>  Espacio utilizado por las instant치neas (total): 0 bytes <br>  Memoria de almacenamiento din치mico utilizada (total): 5,17 MiB <br>  Relaci칩n de compresi칩n SSTable: 0.5720989576459437 <br>  N칰mero de particiones (estimado): 3970323 <br>  Recuento celular memorable: 0 <br>  Tama침o de datos memorables: 0 bytes <br>  Memoria de almacenamiento din치mico no utilizable: 0 bytes <br>  Recuento de conmutadores memorables: 5 <br>  Cuenta de lectura local: 2346045 <br>  Latencia de lectura local: NaN ms <br>  Recuento de escritura local: 3865503 <br>  Latencia de escritura local: NaN ms <br>  Rubores pendientes: 0 <br>  Porcentaje reparado: 0.0 <br>  Filtro de Bloom falsos positivos: 25 <br>  Relaci칩n falsa del filtro Bloom: 0.00000 <br>  Espacio de filtro de Bloom utilizado: 4.57 MiB <br>  Bloom filtro de memoria de mont칩n utilizada: 4.57 MiB <br>  Resumen del 칤ndice de la memoria del mont칩n utilizada: 590.02 KiB <br>  Metadatos de compresi칩n de la memoria del mont칩n utilizada: 19.45 KiB <br>  Bytes m칤nimos de partici칩n compactada: 36 <br>  Bytes m치ximos de partici칩n compactada: 42 <br>  Bytes medios de partici칩n compactada: 42 <br>  Promedio de c칠lulas vivas por corte (칰ltimos cinco minutos): NaN <br>  M치ximo de c칠lulas vivas por segmento (칰ltimos cinco minutos): 0 <br>  Promedio de l치pidas por rebanada (칰ltimos cinco minutos): NaN <br>  L치pidas m치ximas por rebanada (칰ltimos cinco minutos): 0 <br>  Mutaciones descartadas: 0 bytes <br></div></div><br>  Un intento de reducir el tama침o del lote (hasta el env칤o uno por uno) no tuvo efecto, solo empeor칩.  De hecho, es posible que este sea realmente el m치ximo rendimiento para CS, ya que los resultados obtenidos en CS son similares a los obtenidos para DataStax: alrededor de cientos de miles de operaciones por segundo.  Adem치s, si observa la utilizaci칩n de los recursos, ver치 que CS usa mucha m치s CPU y discos: <br><br><img src="https://habrastorage.org/webt/us/fo/i4/usfoi4-mgkktzlosilmz2ogm7uu.png"><br>  <i>La figura muestra la utilizaci칩n durante la ejecuci칩n de todas las pruebas seguidas para ambas bases de datos.</i> <br><br>  En cuanto a los poderosos beneficios de lectura de HB.  Se puede ver que para ambas bases de datos, la utilizaci칩n del disco durante la lectura es extremadamente baja (las pruebas de lectura son la parte final del ciclo de prueba para cada base de datos, por ejemplo, para CS de 15:20 a 15:40).  En el caso de HB, la raz칩n es clara: la mayor칤a de los datos se cuelgan en la memoria, en el memstore, y algunos se almacenaron en cach칠 en la cach칠 de bloques.  En cuanto a CS, no est치 muy claro c칩mo funciona, sin embargo, la utilizaci칩n del disco tampoco es visible, pero por si acaso, se hizo un intento de activar el cach칠 row_cache_size_in_mb = 2048 y establecer el almacenamiento en cach칠 = {'keys': 'ALL', 'rows_per_partition': ' 2,000,000 '}, pero eso lo empeor칩 a칰n m치s. <br><br>  Tambi칠n vale una vez m치s decir un punto significativo sobre el n칰mero de regiones en HB.  En nuestro caso, se indic칩 el valor 64. Si lo reduce y lo iguala a, por ejemplo, 4, entonces, al leer, la velocidad se reduce 2 veces.  La raz칩n es que memstore se atascar치 m치s r치pido y los archivos se enjuagar치n con m치s frecuencia y, al leerlo, necesitar치 procesar m치s archivos, lo cual es una operaci칩n bastante complicada para HB.  En condiciones reales, esto puede tratarse pensando en la estrategia de preplit y compactificaci칩n, en particular, utilizamos una utilidad de fabricaci칩n propia que recolecta basura y comprime HFiles constantemente en segundo plano.  Es posible que para las pruebas DataStax, generalmente se asignara 1 regi칩n por tabla (lo cual no es correcto) y esto aclarar칤a un poco por qu칠 HB perdi칩 tanto en sus pruebas de lectura. <br><br>  Las conclusiones preliminares de esto son las siguientes.  Asumiendo que no se cometieron errores graves durante las pruebas, Cassandra es como un coloso con pies de arcilla.  M치s precisamente, mientras se balancea sobre una pierna, como en la imagen al comienzo del art칤culo, muestra resultados relativamente buenos, pero cuando pelea en las mismas condiciones, pierde directamente.  Al mismo tiempo, teniendo en cuenta la baja utilizaci칩n de la CPU en nuestro hardware, aprendimos a plantar dos HB de RegionServer por host y, por lo tanto, duplicamos la productividad.  Es decir  Teniendo en cuenta la utilizaci칩n de recursos, la situaci칩n de CS es a칰n m치s deplorable. <br><br>  Por supuesto, estas pruebas son bastante sint칠ticas y la cantidad de datos que se utiliz칩 aqu칤 es relativamente modesta.  Es posible que al cambiar a terabytes, la situaci칩n sea diferente, pero si para HB podemos cargar terabytes, entonces para CS esto result칩 ser problem치tico.  A menudo arroj칩 una OperationTimedOutException incluso con estos vol칰menes, aunque los par치metros de expectativa de respuesta ya aumentaron varias veces en comparaci칩n con los predeterminados. <br><br>  Espero que mediante esfuerzos conjuntos encontremos los cuellos de botella de CS y si logramos acelerarlo, entonces definitivamente agregar칠 informaci칩n sobre los resultados finales al final de la publicaci칩n. <br><br>  <b>UPD: Las</b> siguientes pautas se aplicaron en la configuraci칩n de CS: <br><br>  <i>disk_optimization_strategy: spinning</i> <i><br></i>  <i>MAX_HEAP_SIZE = "32G"</i> <i><br></i>  <i>HEAP_NEWSIZE = "3200M"</i> <i><br></i>  <i>-Xms32G</i> <i><br></i>  <i>-Xmx32G</i> <i><br></i>  <i>-XX: + UseG1GC</i> <i><br></i>  <i>-XX: G1RSetUpdatingPauseTimePercent = 5</i> <i><br></i>  <i>-XX: MaxGCPauseMillis = 500</i> <i><br></i>  <i>-XX: InitiatingHeapOccupancyPercent = 70</i> <i><br></i>  <i>-XX: ParallelGCThreads = 32</i> <i><br></i>  <i>-XX: ConcGCThreads = 8</i> <br><br>  En cuanto a la configuraci칩n del sistema operativo, este es un procedimiento bastante largo y complicado (obtener root, reiniciar servidores, etc.), por lo que estas recomendaciones no se aplicaron.  Por otro lado, ambas bases de datos est치n en igualdad de condiciones, por lo que todo es justo. <br><br>  En la parte del c칩digo, se crea un conector para todos los hilos que escriben en la tabla: <br><pre> <code class="java hljs">connector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CassandraConnector(); connector.connect(node, <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, CL); session = connector.getSession(); session.getCluster().getConfiguration().getSocketOptions().setConnectTimeoutMillis(<span class="hljs-number"><span class="hljs-number">120000</span></span>); KeyspaceRepository sr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> KeyspaceRepository(session); sr.useKeyspace(keyspace); prepared = session.prepare(<span class="hljs-string"><span class="hljs-string">"insert into "</span></span> + tableName + <span class="hljs-string"><span class="hljs-string">" (id, title) values (?, ?)"</span></span>);</code> </pre> <br><br>  Los datos se enviaron por enlace: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (Long key = count * thNum; key &lt; count * (thNum + <span class="hljs-number"><span class="hljs-number">1</span></span>); key++) { String value = RandomStringUtils.random(dataSize, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); session.execute(prepared.bind(key, value)); }</code> </pre> <br><br>  Esto no tuvo un impacto significativo en el rendimiento de grabaci칩n.  Para mayor fiabilidad, lanc칠 la carga con la herramienta YCSB, absolutamente el mismo resultado.  A continuaci칩n se muestran las estad칤sticas para un hilo (de 4): <br><br>  <i>2020-01-18 14: 41: 53: 180 315 segundos: 10,000,000 operaciones;</i>  <i>21589.1 operaciones actuales / seg;</i>  <i>[LIMPIEZA: Cuenta = 100, M치x = 2236415, M칤n = 1, Promedio = 22356.39, 90 = 4, 99 = 24, 99.9 = 2236415, 99.99 = 2236415] [INSERTAR: Cuenta = 119551, M치x = 174463, M칤n = 273, Promedio = 2582.71, 90 = 3491, 99 = 16767, 99.9 = 99711, 99.99 = 171263]</i> <i><br></i>  <i>[GENERAL], RunTime (ms), 315539</i> <i><br></i>  <i>[GENERAL], rendimiento (ops / seg), 31691.803548848162</i> <i><br></i>  <i>[TOTAL_GCS_PS_Scavenge], Cuenta, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.7710615803434757</i> <i><br></i>  <i>[TOTAL_GCS_PS_MarkSweep], recuento, 0</i> <i><br></i>  <i>[TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 0</i> <i><br></i>  <i>[TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.0</i> <i><br></i>  <i>[TOTAL_GCs], recuento, 161</i> <i><br></i>  <i>[TOTAL_GC_TIME], tiempo (ms), 2433</i> <i><br></i>  <i>[TOTAL_GC_TIME_%], Tiempo (%), 0.7710615803434757</i> <i><br></i>  <i>[INSERTAR], Operaciones, 10,000,000</i> <i><br></i>  <i>[INSERTAR], Promedio de latencia (us), 3114.2427012</i> <i><br></i>  <i>[INSERTAR], MinLatency (nosotros), 269</i> <i><br></i>  <i>[INSERTAR], MaxLatency (nosotros), 609279</i> <i><br></i>  <i>[INSERTAR], 95thPercentileLatency (us), 5007</i> <i><br></i>  <i>[INSERTAR], 99thPercentileLatency (nosotros), 33439</i> <i><br></i>  <i>[INSERTAR], Retorno = OK, 10000000</i> <i><br></i> <br><br>  Aqu칤 puede ver que la velocidad de una transmisi칩n es de aproximadamente 32 mil registros por segundo, 4 transmisiones funcionaron, resulta 128 mil. Parece que no hay nada m치s que exprimir en la configuraci칩n actual del subsistema de disco. <br><br>  Sobre leer m치s interesante.  Gracias al consejo de camaradas, pudo acelerar radicalmente.  La lectura se realiz칩 no en 5 secuencias, sino en 100. Un aumento a 200 no produjo un efecto.  Tambi칠n agregado al constructor: <br>  .withLoadBalancingPolicy (nuevo TokenAwarePolicy (DCAwareRoundRobinPolicy.builder (). build ())) <br><br>  Como resultado, si antes la prueba mostraba 159 644 operaciones (5 transmisiones, 4 tablas, 100 lotes), ahora: <br>  100 hilos, 4 tablas, lote = 1 (individualmente): 301969 operaciones <br>  100 hilos, 4 tablas, lote = 10: 447 608 operaciones <br>  100 hilos, 4 tablas, lote = 100: 625 655 operaciones <br><br>  Como los resultados son mejores con lotes, realic칠 pruebas similares * con HB: <br><img src="https://habrastorage.org/webt/ct/bk/-y/ctbk-yrecbwegasrbpauq6f1vv8.png"><br>  <i>* Dado que cuando se trabajaba en 400 subprocesos, la funci칩n RandomStringUtils, que se usaba anteriormente, cargaba la CPU en un 100%, fue reemplazada por un generador m치s r치pido.</i> <br><br>  Por lo tanto, un aumento en el n칰mero de subprocesos al cargar datos da un peque침o aumento en el rendimiento de HB. <br><br>  En cuanto a la lectura, aqu칤 est치n los resultados de varias opciones.  A pedido de <a href="https://habr.com/ru/users/0x62ash/" class="user_link">0x62ash</a> , el comando flush se ejecut칩 antes de leer, y tambi칠n se ofrecen otras opciones para comparar: <br>  Memstore: lectura de memoria, es decir  antes de enjuagar al disco. <br>  HFile + zip: lectura de archivos comprimidos por el algoritmo GZ. <br>  HFile + upzip: lee archivos sin compresi칩n. <br><br>  Una caracter칤stica interesante es notable: los archivos peque침os (consulte el campo "Datos", donde se escriben 10 bytes) se procesan m치s lentamente, especialmente si est치n comprimidos.  Obviamente, esto solo es posible hasta cierto tama침o, obviamente, un archivo de 5 GB no se procesar치 m치s r치pido que 10 MB, pero indica claramente que en todas estas pruebas todav칤a no hay un campo arado para investigar varias configuraciones. <br><br>  Por inter칠s, correg칤 el c칩digo YCSB para trabajar con lotes HB de 100 piezas para medir la latencia y m치s.  A continuaci칩n se muestra el resultado del trabajo de 4 copias que escribieron en sus tablas, cada una con 100 hilos.  Result칩 lo siguiente: <br><div class="spoiler">  <b class="spoiler_title">Una operaci칩n = 100 registros</b> <div class="spoiler_text">  [GENERAL], RunTime (ms), 1165415 <br>  [GENERAL], Rendimiento (ops / seg), 858.06343662987 <br>  [TOTAL_GCS_PS_Scavenge], Cuenta, 798 <br>  [TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 7346 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.6303334005483026 <br>  [TOTAL_GCS_PS_MarkSweep], cuenta, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 74 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.006349669431061038 <br>  [TOTAL_GCs], recuento, 799 <br>  [TOTAL_GC_TIME], tiempo (ms), 7420 <br>  [TOTAL_GC_TIME_%], Tiempo (%), 0.6366830699793635 <br>  [INSERTAR], Operaciones, 1,000,000 <br>  [INSERTAR], Latencia promedio (us), 115893.891644 <br>  [INSERTAR], MinLatency (nosotros), 14528 <br>  [INSERTAR], MaxLatency (nosotros), 1470463 <br>  [INSERTAR], 95thPercentileLatency (us), 248319 <br>  [INSERTAR], 99thPercentileLatency (nosotros), 445951 <br>  [INSERTAR], Retorno = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Closing zookeeper sessionid = 0x36f98ad0a4ad8cc <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sesi칩n: 0x36f98ad0a4ad8cc cerrado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread cerrado <br>  [GENERAL], RunTime (ms), 1165806 <br>  [GENERAL], Rendimiento (ops / seg), 857.7756504941646 <br>  [TOTAL_GCS_PS_Scavenge], Cuenta, 776 <br>  [TOTAL_GC_TIME_PS_Scavenge], Tiempo (ms), 7517 <br>  [TOTAL_GC_TIME _% _ PS_Scavenge], Tiempo (%), 0.6447899564764635 <br>  [TOTAL_GCS_PS_MarkSweep], cuenta, 1 <br>  [TOTAL_GC_TIME_PS_MarkSweep], Tiempo (ms), 63 <br>  [TOTAL_GC_TIME _% _ PS_MarkSweep], Tiempo (%), 0.005403986598113236 <br>  [TOTAL_GCs], cuenta, 777 <br>  [TOTAL_GC_TIME], tiempo (ms), 7580 <br>  [TOTAL_GC_TIME_%], Tiempo (%), 0.6501939430745767 <br>  [INSERTAR], Operaciones, 1,000,000 <br>  [INSERTAR], Latencia promedio (EE. UU.), 116042.207936 <br>  [INSERTAR], MinLatency (nosotros), 14056 <br>  [INSERTAR], MaxLatency (nosotros), 1462271 <br>  [INSERTAR], 95thPercentileLatency (us), 250239 <br>  [INSERTAR], 99thPercentileLatency (nosotros), 446719 <br>  [INSERTAR], Retorno = OK, 1,000,000 <br><br>  20/01/19 13:19:16 INFO client.ConnectionManager $ HConnectionImplementation: Closing zookeeper sessionid = 0x26f98ad07b6d67e <br>  20/01/19 13:19:16 INFO zookeeper.ZooKeeper: Sesi칩n: 0x26f98ad07b6d67e cerrado <br>  20/01/19 13:19:16 INFO zookeeper.ClientCnxn: EventThread cerrado <br>  [GENERAL], RunTime (ms), 1165999 <br>  [GENERAL], rendimiento (ops / seg), 857.63366863951 <br>  [TOTAL_GCS_PS_Scavenge], recuento, 818 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7557 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6481137633908777 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 79 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.006775305982252128 <br> [TOTAL_GCs], Count, 819 <br> [TOTAL_GC_TIME], Time(ms), 7636 <br> [TOTAL_GC_TIME_%], Time(%), 0.6548890693731299 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116172.212864 <br> [INSERT], MinLatency(us), 7952 <br> [INSERT], MaxLatency(us), 1458175 <br> [INSERT], 95thPercentileLatency(us), 250879 <br> [INSERT], 99thPercentileLatency(us), 446463 <br> [INSERT], Return=OK, 1000000 <br><br> 20/01/19 13:19:17 INFO client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x36f98ad0a4ad8cd <br> 20/01/19 13:19:17 INFO zookeeper.ZooKeeper: Session: 0x36f98ad0a4ad8cd closed <br> 20/01/19 13:19:17 INFO zookeeper.ClientCnxn: EventThread shut down <br> [OVERALL], RunTime(ms), 1166860 <br> [OVERALL], Throughput(ops/sec), 857.000839860823 <br> [TOTAL_GCS_PS_Scavenge], Count, 707 <br> [TOTAL_GC_TIME_PS_Scavenge], Time(ms), 7239 <br> [TOTAL_GC_TIME_%_PS_Scavenge], Time(%), 0.6203829079752499 <br> [TOTAL_GCS_PS_MarkSweep], Count, 1 <br> [TOTAL_GC_TIME_PS_MarkSweep], Time(ms), 67 <br> [TOTAL_GC_TIME_%_PS_MarkSweep], Time(%), 0.0057419056270675145 <br> [TOTAL_GCs], Count, 708 <br> [TOTAL_GC_TIME], Time(ms), 7306 <br> [TOTAL_GC_TIME_%], Time(%), 0.6261248136023173 <br> [INSERT], Operations, 1000000 <br> [INSERT], AverageLatency(us), 116230.849308 <br> [INSERT], MinLatency(us), 7352 <br> [INSERT], MaxLatency(us), 1443839 <br> [INSERT], 95thPercentileLatency(us), 250623 <br> [INSERT], 99thPercentileLatency(us), 447487 <br> [INSERT], Return=OK, 1000000 </div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Resulta que si CS AverageLatency (us) tiene un registro de 3114, entonces HB AverageLatency (us) = 1162 (recuerde que 1 operaci칩n = 100 registros y, por lo tanto, debe dividirse). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">En general, se obtiene esta conclusi칩n: bajo condiciones dadas, hay una ventaja significativa de HBase. </font><font style="vertical-align: inherit;">Sin embargo, no se puede descartar que el SSD y el ajuste cuidadoso del sistema operativo cambien radicalmente la imagen. </font><font style="vertical-align: inherit;">Tambi칠n debe comprender que mucho depende de los escenarios de uso, puede resultar f치cilmente que si no toma 4 tablas, sino 400 y trabaja con terabytes, el equilibrio de fuerzas se desarrollar치 de una manera completamente diferente. </font><font style="vertical-align: inherit;">Como dec칤an los cl치sicos: la pr치ctica es el criterio de la verdad. </font><font style="vertical-align: inherit;">Tienes que intentarlo. </font><font style="vertical-align: inherit;">Por un lado, ScyllaDB ahora tiene sentido verificar, por lo que debe continuar ...</font></font></div></div><p>Source: <a href="https://habr.com/ru/post/484096/">https://habr.com/ru/post/484096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../484084/index.html">1C-Bitrix y un intento de presentarlo</a></li>
<li><a href="../484088/index.html">Desfile de contrase침as (an치lisis de ~ 5 mil millones de contrase침as de fugas)</a></li>
<li><a href="../484090/index.html">Nueva infraestructura de TI para el centro de datos de correos de Rusia</a></li>
<li><a href="../484092/index.html">Algo pr칤ncipes y nobles vestidos</a></li>
<li><a href="../484094/index.html">Crea un juego de disparos de zombis en tercera persona con DOTS</a></li>
<li><a href="../484100/index.html">Trabajar con la interfaz en el SDK de Google Maps para Android</a></li>
<li><a href="../484102/index.html">PHP vs Python vs Ruby on Rails: Comparaci칩n detallada</a></li>
<li><a href="../484106/index.html">MVCC en PostgreSQL-6. Vac칤o</a></li>
<li><a href="../484108/index.html">Etherblade.net Encapsulador y sustituci칩n de importaciones para componentes de red (segunda parte)</a></li>
<li><a href="../484112/index.html">쮼s posible hackear un avi칩n?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>