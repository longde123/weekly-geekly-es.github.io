<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👜 👨‍🍳 ⛹️ Word2vec in Bildern 🥋 🈳 🥠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="„ Jedes Ding verbirgt ein Muster, das Teil des Universums ist. Es hat Symmetrie, Eleganz und Schönheit - Eigenschaften, die vor allem von jedem wahren...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec in Bildern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">„ <b>Jedes Ding verbirgt ein Muster, das Teil des Universums ist.</b></font>  <font color="gray"><b>Es hat Symmetrie, Eleganz und Schönheit</b> - Eigenschaften, die vor allem von jedem wahren Künstler erfasst werden, der die Welt einfängt.</font>  <font color="gray">Dieses Muster kann im Wechsel der Jahreszeiten, im Sandfluss entlang des Abhangs, in den verwickelten Zweigen eines Kreosotstrauchs und im Muster seines Blattes erfasst werden.</font> <font color="gray"><br><br></font>  <font color="gray">Wir versuchen, dieses Muster in unserem Leben und in unserer Gesellschaft zu kopieren, und deshalb lieben wir Rhythmus, Gesang, Tanz und verschiedene Formen, die uns glücklich machen und uns trösten.</font>  <font color="gray">Man kann jedoch auch die Gefahr erkennen, die bei der Suche nach absoluter Perfektion lauert, denn es ist offensichtlich, dass das perfekte Muster unverändert bleibt.</font>  <font color="gray">Und wenn wir uns der Perfektion nähern, gehen alle Dinge zu Tode “- <i>Dune</i> (1965)</font> </blockquote><br>  Ich glaube, das Einbettungskonzept ist eine der bemerkenswertesten Ideen beim maschinellen Lernen.  Wenn Sie jemals Siri, Google Assistant, Alexa, Google Translate oder sogar eine Smartphone-Tastatur mit der Vorhersage des nächsten Wortes verwendet haben, haben Sie bereits mit dem auf Anhängen basierenden Modell der Verarbeitung natürlicher Sprache gearbeitet.  In den letzten Jahrzehnten hat sich dieses Konzept für neuronale Modelle erheblich weiterentwickelt (jüngste Entwicklungen umfassen kontextualisierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Worteinbettungen</a> in fortgeschrittenen Modellen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT</a> und GPT2). <br><a name="habracut"></a><br>  Word2vec ist eine effektive Methode zur Erstellung von Investitionen, die 2013 entwickelt wurde.  Neben der Arbeit mit Wörtern erwiesen sich einige seiner Konzepte als wirksam bei der Entwicklung von Empfehlungsmechanismen und der Bedeutung von Daten auch bei kommerziellen, nicht sprachlichen Aufgaben.  Diese Technologie wurde von Unternehmen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spotify</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anghami</a> in ihren Empfehlungs-Engines verwendet. <br><br>  In diesem Artikel werden wir uns mit dem Konzept und der Mechanik der Generierung von Anhängen mit word2vec befassen.  Beginnen wir mit einem Beispiel, um sich mit der Darstellung von Objekten in Vektorform vertraut zu machen.  Wissen Sie, wie viel eine Liste mit fünf Zahlen (Vektor) über Ihre Persönlichkeit aussagen kann? <br><br><h1>  Personalisierung: Was bist du? </h1><br><blockquote>  <font color="gray">„Ich gebe dir das Wüstenchamäleon;</font>  <font color="gray">Seine Fähigkeit, sich mit dem Sand zu verbinden, sagt Ihnen alles, was Sie über die Wurzeln der Ökologie und die Gründe für die Erhaltung Ihrer Persönlichkeit wissen müssen. “</font>  <font color="gray">- <i>Kinder der Düne</i></font> </blockquote><br>  Haben Sie auf einer Skala von 0 bis 100 einen introvertierten oder extrovertierten Persönlichkeitstyp (wobei 0 der introvertierteste Typ und 100 der extrovertierteste Typ ist)?  Haben Sie jemals einen Persönlichkeitstest bestanden: zum Beispiel MBTI oder noch besser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Big Five</a> ?  Sie erhalten eine Liste mit Fragen und werden dann auf mehreren Achsen bewertet, einschließlich Introversion / Extroversion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Beispiel der Big Five-Testergebnisse.</font></i>  <i><font color="gray">Er sagt wirklich viel über die Persönlichkeit und kann den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">akademischen</a> , <a href="">persönlichen</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beruflichen Erfolg</a> vorhersagen.</font></i>  <i><font color="gray"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier können</a> Sie zum Beispiel durchgehen.</font></i> <br><br>  Angenommen, ich habe 38 von 100 Punkten für die Bewertung der Introversion / Extraversion erzielt.  Dies kann wie folgt dargestellt werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Oder auf einer Skala von -1 bis +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Wie gut erkennen wir eine Person nur an dieser Einschätzung?  Nicht besonders.  Menschen sind komplexe Wesen.  Daher fügen wir eine weitere Dimension hinzu: eine weitere Eigenschaft aus dem Test. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Sie können sich diese beiden Dimensionen als Punkt im Diagramm oder noch besser als Vektor vom Ursprung bis zu diesem Punkt vorstellen.</font></i>  <i><font color="gray">Es gibt großartige Vektorwerkzeuge, die sehr bald nützlich sein werden.</font></i> <br><br>  Ich zeige nicht, welche Persönlichkeitsmerkmale wir in die Tabelle aufgenommen haben, damit Sie nicht an bestimmte Merkmale gebunden werden, sondern sofort die Vektordarstellung der Persönlichkeit der Person als Ganzes verstehen. <br><br>  Jetzt können wir sagen, dass dieser Vektor teilweise meine Persönlichkeit widerspiegelt.  Dies ist eine nützliche Beschreibung beim Vergleich verschiedener Personen.  Angenommen, ich wurde von einem roten Bus angefahren, und Sie müssen mich durch eine ähnliche Person ersetzen.  Welche der beiden Personen in der folgenden Tabelle ähnelt mir eher? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Bei der Arbeit mit Vektoren wird die Ähnlichkeit normalerweise durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Otiai-Koeffizienten</a> (geometrischer Koeffizient) berechnet: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">Person Nr. 1 ähnelt</font> <font color="gray">eher meinem Charakter.</font></i>  <i><font color="gray">Vektoren in einer Richtung (Länge ist ebenfalls wichtig) ergeben einen größeren Otiai-Koeffizienten</font></i> <br><br>  Auch hier reichen zwei Dimensionen nicht aus, um Menschen zu bewerten.  Jahrzehntelange Entwicklung der Psychologie hat zur Erstellung eines Tests für fünf grundlegende Persönlichkeitsmerkmale (mit vielen zusätzlichen) geführt.  Verwenden wir also alle fünf Dimensionen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  Das Problem mit den fünf Dimensionen ist, dass es nicht mehr möglich ist, ordentliche Pfeile in 2D zu zeichnen.  Dies ist ein häufiges Problem beim maschinellen Lernen, bei dem Sie häufig in einem mehrdimensionalen Raum arbeiten müssen.  Es ist gut, dass der geometrische Koeffizient mit einer beliebigen Anzahl von Messungen funktioniert: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">Der geometrische Koeffizient funktioniert für eine beliebige Anzahl von Messungen.</font></i>  <i><font color="gray">In fünf Dimensionen ist das Ergebnis viel genauer.</font></i> <br><br>  Am Ende dieses Kapitels möchte ich zwei Hauptideen wiederholen: <br><br><ol><li>  Personen (und andere Objekte) können als numerische Vektoren dargestellt werden (was für Autos großartig ist!). <br></li><li>  Wir können leicht berechnen, wie ähnlich die Vektoren sind. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Worteinbettung </h1><br><blockquote>  <font color="gray">"Das Geschenk der Worte ist das Geschenk der Täuschung und Illusion."</font>  <font color="gray">- <i>Kinder der Düne</i></font> </blockquote><br>  Mit diesem Verständnis werden wir zu den Vektordarstellungen von Wörtern übergehen, die als Ergebnis des Trainings erhalten wurden (sie werden auch als Anhänge bezeichnet) und ihre interessanten Eigenschaften untersuchen. <br><br>  Hier ist der Anhang für das Wort "König" (GloVe-Vektor, auf Wikipedia trainiert): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Wir sehen eine Liste mit 50 Zahlen, aber es ist schwer, etwas zu sagen.  Visualisieren wir sie, um sie mit anderen Vektoren zu vergleichen.  Setzen Sie die Zahlen in eine Reihe: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Kolorieren Sie die Zellen anhand ihrer Werte (rot für nahe 2, weiß für nahe 0, blau für nahe –2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Vergessen Sie jetzt die Zahlen und nur durch Farben kontrastieren wir den „König“ mit anderen Worten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Sie sehen, dass „Mann“ und „Frau“ viel näher beieinander sind als der „König“?  Es sagt etwas.  Vektordarstellungen erfassen viele Informationen / Bedeutungen / Assoziationen dieser Wörter. <br><br>  Hier ist eine weitere Liste von Beispielen (vergleichen Sie Spalten mit ähnlichen Farben): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Es gibt mehrere Dinge zu beachten: <br><br><ol><li>  Durch alle Wörter geht eine rote Spalte.  Das heißt, diese Wörter sind in dieser bestimmten Dimension ähnlich (und wir wissen nicht, was darin codiert ist). <br></li><li>  Sie können sehen, dass "Frau" und "Mädchen" sehr ähnlich sind.  Das gleiche gilt für "Mann" und "Junge". <br></li><li>  "Junge" und "Mädchen" sind in einigen Dimensionen ebenfalls ähnlich, unterscheiden sich jedoch von "Frau" und "Mann".  Könnte dies eine verschlüsselte vage Vorstellung von Jugend sein?  Wahrscheinlich. <br></li><li>  Alles außer dem letzten Wort sind die Ideen der Menschen.  Ich habe ein Objekt (Wasser) hinzugefügt, um die Unterschiede zwischen den Kategorien zu zeigen.  Sie können beispielsweise sehen, wie die blaue Spalte nach unten geht und vor dem Wasservektor stoppt. <br></li><li>  Es gibt klare Dimensionen, in denen der „König“ und die „Königin“ einander ähnlich sind und sich von allen anderen unterscheiden.  Vielleicht ist dort ein vages Konzept der Lizenzgebühren kodiert? </li></ol><br><h1>  Analogien </h1><br><blockquote>  <font color="gray">„Worte ertragen jede Last, die wir uns wünschen.</font>  <font color="gray">Alles, was erforderlich ist, ist eine Vereinbarung über die Tradition, nach der wir Konzepte entwickeln. “</font>  <font color="gray">- <i>Gott der Kaiser der Düne</i></font> </blockquote><br>  Berühmte Beispiele, die die unglaublichen Eigenschaften von Investitionen zeigen, sind das Konzept der Analogien.  Wir können Wortvektoren addieren und subtrahieren, um interessante Ergebnisse zu erzielen.  Das bekannteste Beispiel ist die Formel „König - Mann + Frau“: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gensim-</a> Bibliothek in Python können wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wortvektoren</a> addieren und subtrahieren, und die Bibliothek findet die Wörter, die dem resultierenden Vektor am nächsten liegen.</font></i>  <i><font color="gray">Das Bild zeigt eine Liste der ähnlichsten Wörter mit jeweils einem geometrischen Ähnlichkeitskoeffizienten</font></i> <br><br>  Wir visualisieren diese Analogie wie zuvor: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">Der resultierende Vektor aus der Berechnung „König - Mann + Frau“ ist nicht ganz gleich der „Königin“, aber dies ist das nächste Ergebnis von 400.000 Wortanhängen im Datensatz</font></i> <br><br>  Nachdem wir über das Anhängen von Wörtern nachgedacht haben, lernen wir, wie das Lernen stattfindet.  Bevor Sie jedoch zu word2vec übergehen, müssen Sie sich den konzeptionellen Vorfahren der Worteinbettung ansehen: ein neuronales Sprachmodell. <br><br><h1>  Sprachmodell </h1><br><blockquote>  <font color="gray">„Der Prophet unterliegt nicht den Illusionen der Vergangenheit, Gegenwart oder Zukunft.</font>  <font color="gray"><b>Die Fixität sprachlicher Formen bestimmt solche linearen Unterschiede.</b></font>  <font color="gray">Die Propheten halten den Schlüssel zum Zungenschloss.</font>  <font color="gray">Für sie bleibt das physische Bild nur ein physisches Bild und nichts weiter.</font> <font color="gray"><br><br></font>  <font color="gray">Ihr Universum hat nicht die Eigenschaften eines mechanischen Universums.</font>  <font color="gray">Eine lineare Abfolge von Ereignissen wird vom Beobachter angenommen.</font>  <font color="gray">Ursache und Wirkung?</font>  <font color="gray">Es ist eine ganz andere Sache.</font>  <font color="gray">Der Prophet spricht schicksalhafte Worte aus.</font>  <font color="gray">Sie sehen einen Blick auf ein Ereignis, das "nach der Logik der Dinge" eintreten sollte.</font>  <font color="gray">Aber der Prophet setzt sofort die Energie der unendlichen Wunderkraft frei.</font>  <font color="gray">Das Universum befindet sich in einem spirituellen Wandel. “</font>  <font color="gray">- <i>Gott der Kaiser der Düne</i></font> </blockquote><br>  Ein Beispiel für NLP (Natural Language Processing) ist die Vorhersagefunktion des nächsten Wortes auf der Tastatur eines Smartphones.  Milliarden von Menschen benutzen es hunderte Male am Tag. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Die Vorhersage des nächsten Wortes ist eine geeignete Aufgabe für <i>ein Sprachmodell</i> .  Sie kann eine Liste von Wörtern (z. B. zwei Wörter) nehmen und versuchen, Folgendes vorherzusagen. <br><br>  Im obigen Screenshot hat das Modell diese beiden grünen Wörter ( <code>thou shalt</code> ) genommen und eine Liste von Optionen zurückgegeben (höchstwahrscheinlich für das Wort <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Wir können uns das Modell als Black Box vorstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  In der Praxis erzeugt das Modell jedoch mehr als ein Wort.  Es wird eine Schätzung der Wahrscheinlichkeit für praktisch alle bekannten Wörter abgeleitet (das "Wörterbuch" des Modells variiert von mehreren tausend bis zu mehr als einer Million Wörtern).  Die Tastaturanwendung findet dann die Wörter mit den höchsten Punktzahlen und zeigt sie dem Benutzer an. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Ein neuronales Sprachmodell gibt die Wahrscheinlichkeit aller bekannten Wörter an.</font></i>  <i><font color="gray">Wir geben die Wahrscheinlichkeit als Prozentsatz an, aber im resultierenden Vektor werden 40% als 0,4 dargestellt</font></i> <br><br>  Nach dem Training berechneten die ersten neuronalen Modelle ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bengio 2003</a> ) die Prognose in drei Stufen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  Der erste Schritt ist für uns der relevanteste, wenn wir über Investitionen sprechen.  Als Ergebnis des Trainings wird eine Matrix mit den Anhängen aller Wörter in unserem Wörterbuch erstellt.  Um das Ergebnis zu erhalten, suchen wir einfach nach den Einbettungen der eingegebenen Wörter und führen die Vorhersage aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Schauen wir uns nun den Lernprozess an und finden heraus, wie diese Investitionsmatrix entsteht. <br><br><h1>  Sprachmodelltraining </h1><br><blockquote>  <font color="gray">„Der Prozess kann nicht durch Beenden verstanden werden.</font>  <font color="gray">Das Verständnis muss mit dem Prozess einhergehen, mit seinem Fluss verschmelzen und mit ihm fließen “- <i>Dune</i></font> </blockquote><br>  Sprachmodelle haben einen großen Vorteil gegenüber den meisten anderen Modellen des maschinellen Lernens: Sie können an Texten trainiert werden, die wir im Überfluss haben.  Denken Sie an alle Bücher, Artikel, Wikipedia-Materialien und andere Formen von Textdaten, die wir haben.  Vergleichen Sie mit anderen Modellen für maschinelles Lernen, die manuelle Arbeit und speziell gesammelte Daten erfordern. <br><br><blockquote>  <b>"Sie müssen das Wort von seiner Firma lernen" - J. R. Furs</b> </blockquote><br>  Anhänge für Wörter werden anhand der umgebenden Wörter berechnet, die häufiger in der Nähe erscheinen.  Die Mechanik ist wie folgt: <br><br><ol><li>  Wir erhalten viele Textdaten (z. B. alle Wikipedia-Artikel) <br></li><li>  Legen Sie ein Fenster (z. B. drei Wörter) fest, das durch den Text gleitet. <br></li><li>  Ein Schiebefenster erzeugt Muster für das Training unseres Modells. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Wenn dieses Fenster über den Text gleitet, generieren wir (tatsächlich) einen Datensatz, mit dem wir das Modell trainieren.  Lassen Sie uns zum Verständnis sehen, wie ein Schiebefenster mit diesem Satz umgeht: <br><br><blockquote>  <b>„Mögest du nicht eine Maschine bauen, die mit der Ähnlichkeit des menschlichen Geistes ausgestattet ist“ - <i>Dune</i></b> </blockquote><br>  Wenn wir beginnen, befindet sich das Fenster auf den ersten drei Wörtern des Satzes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Wir nehmen die ersten beiden Wörter für Zeichen und das dritte Wort für das Etikett: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Wir haben die erste Stichprobe in einem Datensatz generiert, der später zum Unterrichten eines Sprachmodells verwendet werden kann</font></i> <br><br>  Dann verschieben wir das Fenster an die nächste Position und erstellen ein zweites Beispiel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Und ziemlich bald sammeln wir einen größeren Datensatz: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  In der Praxis werden Modelle normalerweise direkt beim Bewegen eines Schiebefensters trainiert.  Logischerweise ist die Phase der Datensatzgenerierung von der Trainingsphase getrennt.  Zusätzlich zu neuronalen Netzwerkansätzen wurde die N-Gramm-Methode häufig früher zum Unterrichten von Sprachmodellen verwendet (siehe das dritte Kapitel des Buches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„Sprach- und Sprachverarbeitung“</a> ).  Um den Unterschied beim Wechsel von N-Gramm zu neuronalen Modellen in realen Produkten zu sehen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier ein Beitrag aus dem Jahr 2015 im Swiftkey-Blog</a> , dem Entwickler meiner bevorzugten Android-Tastatur, der sein neuronales Sprachmodell vorstellt und es mit dem vorherigen N-Gramm-Modell vergleicht.  Ich mag dieses Beispiel, weil es zeigt, wie die algorithmischen Eigenschaften von Anlagen in einer Marketing-Sprache beschrieben werden können. <br><br><h1>  Wir schauen in beide Richtungen </h1><br><blockquote>  <font color="gray">„Ein Paradoxon ist ein Zeichen dafür, dass wir versuchen sollten zu überlegen, was dahinter steckt.</font>  <font color="gray">Wenn das Paradoxon Sie beunruhigt, bedeutet dies, dass Sie nach dem Absoluten streben.</font>  <font color="gray">Relativisten betrachten das Paradox einfach als einen interessanten, vielleicht lustigen, manchmal beängstigenden Gedanken, aber einen sehr lehrreichen Gedanken. “</font>  <font color="gray"><i>Kaisergott der Düne</i></font> </blockquote><br>  Füllen Sie auf der Grundlage des Vorstehenden die Lücke aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Als Kontext gibt es fünf vorherige Wörter (und einen früheren Verweis auf „Bus“).  Ich bin sicher, dass die meisten von Ihnen vermutet haben, dass es einen "Bus" geben sollte.  Aber wenn ich Ihnen nach dem Leerzeichen noch ein Wort gebe, ändert dies Ihre Antwort? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Dies ändert die Situation völlig: Jetzt ist das fehlende Wort höchstwahrscheinlich „rot“.  Offensichtlich haben Wörter sowohl vor als auch nach einem Leerzeichen einen informativen Wert.  Es stellt sich heraus, dass Sie mit der Buchhaltung in beide Richtungen (links und rechts) bessere Investitionen berechnen können.  Mal sehen, wie man das Modelltraining in einer solchen Situation konfiguriert. <br><br><h1>  Gramm überspringen </h1><br><blockquote>  <font color="gray">"Wenn eine absolut unverwechselbare Wahl unbekannt ist, hat der Intellekt die Möglichkeit, mit begrenzten Daten in der Arena zu arbeiten, in der Fehler nicht nur möglich, sondern auch notwendig sind."</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  Zusätzlich zu zwei Wörtern vor dem Ziel können Sie zwei weitere Wörter nach dem Ziel berücksichtigen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Dann sieht der Datensatz für das Modelltraining folgendermaßen aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Dies wird als CBOW-Architektur (Continuous Bag of Words) bezeichnet und in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem der word2vec</a> [pdf] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">-Dokumente beschrieben</a> .  Es gibt eine andere Architektur, die ebenfalls hervorragende Ergebnisse zeigt, jedoch etwas anders angeordnet ist: Sie versucht, die benachbarten Wörter anhand des aktuellen Wortes zu erraten.  Ein Schiebefenster sieht ungefähr so ​​aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">Im grünen Schlitz befindet sich das Eingangswort, und jedes rosa Feld repräsentiert einen möglichen Ausgang</font></i> <br><br>  Rosa Rechtecke haben unterschiedliche Schattierungen, da dieses Schiebefenster tatsächlich vier separate Muster in unserem Trainingsdatensatz erstellt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Diese Methode wird als <b>Skip-Gram-</b> Architektur bezeichnet.  Sie können ein Schiebefenster wie folgt visualisieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Die folgenden vier Beispiele werden dem Trainingsdatensatz hinzugefügt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Dann bewegen wir das Fenster an die folgende Position: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Was vier weitere Beispiele hervorbringt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Bald werden wir viel mehr Proben haben: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Lernbericht </h1><br><blockquote>  <font color="gray">„Muad'Dib lernte schnell, weil ihm hauptsächlich das Lernen beigebracht wurde.</font>  <font color="gray">Aber die allererste Lektion war die Assimilation des Glaubens, dass er lernen kann, und das ist die Grundlage von allem.</font>  <font color="gray">Es ist erstaunlich, wie viele Menschen nicht glauben, dass sie lernen und lernen können, und wie viele Menschen glauben, dass Lernen sehr schwierig ist. "</font>  <font color="gray">- <i>Düne</i></font> </blockquote><br>  Nachdem wir das Sprunggramm-Set festgelegt haben, verwenden wir es, um das grundlegende neuronale Modell der Sprache zu trainieren, die ein benachbartes Wort vorhersagt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Beginnen wir mit dem ersten Beispiel in unserem Datensatz.  Wir nehmen das Zeichen und senden es an das ungeübte Modell mit der Bitte, das nächste Wort vorherzusagen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  Das Modell durchläuft drei Schritte und zeigt einen Vorhersagevektor an (mit Wahrscheinlichkeit für jedes Wort im Wörterbuch).  Da das Modell nicht trainiert ist, ist seine Prognose zu diesem Zeitpunkt wahrscheinlich falsch.  Aber das ist nichts.  Wir wissen, welches Wort sie vorhersagt - dies ist die resultierende Zelle in der Zeile, mit der wir das Modell derzeit trainieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Ein "Zielvektor" ist einer, bei dem das Zielwort eine Wahrscheinlichkeit von 1 hat und alle anderen Wörter eine Wahrscheinlichkeit von 0 haben</font></i> <br><br>  Wie falsch war das Modell?  Subtrahieren Sie den Prognosevektor vom Ziel und erhalten Sie den Fehlervektor: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Dieser Fehlervektor kann jetzt zum Aktualisieren des Modells verwendet werden, sodass es beim nächsten Mal wahrscheinlicher ist, dass dieselben Eingabedaten ein genaues Ergebnis liefern. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Hier endet die erste Ausbildungsphase.  Wir machen dasselbe mit der nächsten Stichprobe im Datensatz und dann mit der nächsten, bis wir alle Stichproben untersucht haben.  Dies ist das Ende der ersten Ära des Lernens.  Wir wiederholen alles für mehrere Epochen immer wieder und erhalten so ein geschultes Modell: Daraus können Sie die Investitionsmatrix extrahieren und in beliebigen Anwendungen verwenden. <br><br>  Wir haben zwar viel gelernt, aber um zu verstehen, wie word2vec wirklich lernt, fehlen einige Schlüsselideen. <br><br><h1>  Negative Auswahl </h1><br><blockquote>  <font color="gray">„Der Versuch, Muad'Dib zu verstehen, ohne seine Todfeinde - den Harkonnenov - zu verstehen, ist der gleiche wie der Versuch, die Wahrheit zu verstehen, ohne zu verstehen, was die Lüge ist.</font>  <font color="gray">Dies ist ein Versuch, das Licht zu kennen, ohne die Dunkelheit zu kennen.</font>  <font color="gray">Es ist unmöglich".</font>  <font color="gray">- <i>Düne</i></font> </blockquote><br>  Erinnern Sie sich an die drei Schritte, wie ein neuronales Modell eine Prognose berechnet: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  Der dritte Schritt ist aus rechnerischer Sicht sehr teuer, insbesondere wenn Sie ihn für jede Stichprobe im Datensatz ausführen (zig Millionen Mal).  Es ist notwendig, die Produktivität irgendwie zu steigern. <br><br>  Eine Möglichkeit besteht darin, das Ziel in zwei Phasen zu unterteilen: <br><br><ol><li>  Erstellen Sie hochwertige Wortanhänge (ohne das nächste Wort vorherzusagen). <br></li><li>  Verwenden Sie diese hochwertigen Investitionen für den Unterricht des Sprachmodells (für Prognosen). </li></ol><br>  Dieser Artikel konzentriert sich auf den ersten Schritt.  Um die Produktivität zu steigern, können Sie nicht mehr ein benachbartes Wort vorhersagen ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... und wechseln Sie zu einem Modell, das Eingabe- und Ausgabewörter verwendet und die Wahrscheinlichkeit ihrer Nähe berechnet (von 0 bis 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Ein solch einfacher Übergang ersetzt das neuronale Netzwerk durch ein logistisches Regressionsmodell - so werden Berechnungen viel einfacher und schneller. <br><br>  Gleichzeitig müssen wir die Struktur unseres Datensatzes verfeinern: Die Bezeichnung ist jetzt eine neue Spalte mit den Werten 0 oder 1. In unserer Tabelle sind Einheiten überall, weil wir dort Nachbarn hinzugefügt haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Ein solches Modell wird mit einer unglaublichen Geschwindigkeit berechnet: Millionen von Proben in Minuten.  Aber Sie müssen eine Lücke schließen.  Wenn alle unsere Beispiele positiv sind (Ziel: 1), kann sich ein kniffliges Modell bilden, das immer 1 zurückgibt und 100% Genauigkeit zeigt, aber nichts lernt und Junk-Investitionen generiert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Um dieses Problem zu lösen, müssen Sie <i>negative Muster</i> in den Datensatz eingeben - Wörter, die definitiv keine Nachbarn sind.  Für sie muss das Modell 0 zurückgeben. Jetzt muss das Modell hart arbeiten, aber die Berechnungen laufen immer noch mit großer Geschwindigkeit ab. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">Fügen Sie für jede Probe im Datensatz negative Beispiele mit der Bezeichnung 0 hinzu</font></i> <br><br>  Aber was ist als Ausgabewort einzuführen?  Wählen Sie die Wörter beliebig: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Diese Idee wurde unter dem Einfluss der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rauschvergleichsmethode</a> [pdf] geboren.  Wir gleichen das tatsächliche Signal (positive Beispiele benachbarter Wörter) mit Rauschen (zufällig ausgewählte Wörter, die keine Nachbarn sind) ab.  Dies bietet einen hervorragenden Kompromiss zwischen Leistung und statistischer Leistung. <br><br><h1>  Skip-Gramm-Negativprobe (SGNS) </h1><br>  Wir haben uns zwei zentrale Konzepte von word2vec angesehen: Zusammen werden sie als "Sprunggramm mit negativer Stichprobe" bezeichnet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Word2vec lernen </h1><br><blockquote>  <font color="gray">„Eine Maschine kann nicht jedes Problem vorhersehen, das für eine lebende Person wichtig ist.</font>  <font color="gray">Es gibt einen großen Unterschied zwischen diskretem Raum und kontinuierlichem Kontinuum.</font>  <font color="gray">Wir leben in einem Raum und Maschinen existieren in einem anderen. “</font>  <font color="gray">- <i>Gott der Kaiser der Düne</i></font> </blockquote><br>  Nachdem wir die Grundideen von Skip-Gram und Negativ-Sampling untersucht haben, können wir uns den Lernprozess von word2vec genauer ansehen. <br><br>  Zuerst verarbeiten wir den Text vor, auf dem wir das Modell trainieren.  Definieren Sie die Größe des Wörterbuchs (wir nennen es <code>vocab_size</code> ), beispielsweise in 10.000 Anhängen und den Parametern der Wörter im Wörterbuch. <br><br>  Zu Beginn des Trainings erstellen wir zwei Matrizen: <code>Embedding</code> und <code>Context</code> .  Anhänge für jedes Wort werden in diesen Matrizen in unserem Wörterbuch gespeichert (daher ist <code>vocab_size</code> einer ihrer Parameter).  Der zweite Parameter ist die Dimension des Anhangs (normalerweise wird <code>embedding_size</code> auf 300 gesetzt, aber zuvor haben wir uns ein Beispiel mit 50 Dimensionen angesehen). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Zuerst initialisieren wir diese Matrizen mit zufälligen Werten.  Dann beginnen wir den Lernprozess.  In jeder Phase nehmen wir ein positives und die damit verbundenen negativen Beispiele.  Hier ist unsere erste Gruppe: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Wir haben jetzt vier Wörter: das Eingabewort <code>not</code> und die Ausgabe- / Kontextwörter <code>thou</code> (tatsächlicher Nachbar), <code>aaron</code> und <code>taco</code> (negative Beispiele).  Wir beginnen die Suche nach ihren Anhängen in den Matrizen <code>Embedding</code> (für das Eingabewort) und <code>Context</code> (für die Kontextwörter), obwohl beide Matrizen Anhänge für alle Wörter aus unserem Wörterbuch enthalten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Dann berechnen wir das Skalarprodukt des Eingabeanhangs mit jedem der Kontextanhänge.  In jedem Fall wird eine Zahl erhalten, die die Ähnlichkeit von Eingabedaten und Kontextanhängen angibt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Jetzt brauchen wir eine Möglichkeit, diese Schätzungen in eine Art Wahrscheinlichkeit umzuwandeln: Alle müssen positive Zahlen zwischen 0 und 1 sein. Dies ist eine hervorragende Aufgabe für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sigmoidale</a> logistische Gleichungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  Das Ergebnis der Sigmoidberechnung kann als Ausgabe des Modells für diese Stichproben betrachtet werden.  Wie Sie sehen können, hat <code>taco</code> die höchste Punktzahl, während <code>aaron</code> vor und nach dem Sigmoid immer noch die niedrigste Punktzahl hat. <br><br>  Wenn das nicht trainierte Modell eine Prognose erstellt hat und eine echte Zielmarke zum Vergleich hat, berechnen wir, wie viele Fehler in der Modellprognose enthalten sind.  Subtrahieren Sie dazu einfach die Sigmoid-Punktzahl von den Zielbezeichnungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Hier beginnt die „Lernphase“ aus dem Begriff „maschinelles Lernen“.  Jetzt können wir diese Fehlerschätzung verwenden, um die Investitionen <code>not</code> , <code>thou</code> , <code>aaron</code> und <code>taco</code> so anzupassen, dass das Ergebnis das nächste Mal näher an den Zielschätzungen liegt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Damit ist eine Ausbildungsstufe abgeschlossen.  Wir haben die Anhaftung einiger Wörter ein wenig verbessert ( <code>not</code> <code>thou</code> , <code>aaron</code> und <code>taco</code> ).  Nun fahren wir mit der nächsten Stufe fort (der nächsten positiven und den damit verbundenen negativen Probe) und wiederholen den Vorgang. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Die Anhänge verbessern sich weiter, wenn wir den gesamten Datensatz mehrmals durchlaufen.  Sie können den Prozess dann stoppen, die <code>Context</code> beiseite legen und die trainierte <code>Embeddings</code> für die nächste Aufgabe verwenden. <br><br><h1>  Fenstergröße und Anzahl der negativen Proben </h1><br>  Beim Lernen von word2vec sind zwei wichtige Hyperparameter die Fenstergröße und die Anzahl der negativen Stichproben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Unterschiedliche Fenstergrößen eignen sich für unterschiedliche Aufgaben.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es wurde festgestellt,</a> dass kleinere Fenstergrößen (2–15) <i>austauschbare</i> Anhänge mit ähnlichen Indizes erzeugen (beachten Sie, dass Antonyme bei der Betrachtung der umgebenden Wörter häufig austauschbar sind: Beispielsweise werden die Wörter „gut“ und „schlecht“ häufig in ähnlichen Kontexten erwähnt).  Größere Fenstergrößen (15–50 oder mehr) erzeugen <i>verwandte</i> Anhänge mit ähnlichen Indizes.  In der Praxis müssen Sie häufig <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anmerkungen</a> für nützliche semantische Ähnlichkeiten in Ihrer Aufgabe bereitstellen.  In Gensim beträgt die Standardfenstergröße 5 (zwei Wörter links und rechts zusätzlich zum eingegebenen Wort selbst). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Anzahl der negativen Stichproben ist ein weiterer Faktor im Lernprozess. </font><font style="vertical-align: inherit;">Das Originaldokument empfiehlt 5–20. </font><font style="vertical-align: inherit;">Es heißt auch, dass 2-5 Stichproben ausreichend zu sein scheinen, wenn Sie einen ausreichend großen Datensatz haben. </font><font style="vertical-align: inherit;">In Gensim beträgt der Standardwert 5 negative Muster.</font></font><br><br><h1>  Fazit </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">„Wenn Ihr Verhalten über Ihre Standards hinausgeht, sind Sie eine lebende Person, kein Automat“ - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gott-Kaiser der Düne</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich hoffe, Sie verstehen die Einbettung von Wörtern und die Essenz des word2vec-Algorithmus. </font><font style="vertical-align: inherit;">Ich hoffe auch, dass Sie jetzt die Artikel besser verstehen, in denen das Konzept des "Sprunggramms mit negativer Stichprobe" (SGNS) wie in den obigen Empfehlungssystemen erwähnt wird.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referenzen und weiterführende Literatur </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Verteilte Darstellungen von Wörtern und Phrasen und deren Zusammensetzung"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«      »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«   »</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«   »</a>      —    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«      »</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> —      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">« word2vec»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«  »</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">«»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446530/">https://habr.com/ru/post/de446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446512/index.html">.NET Core Worker als Windows-Dienste</a></li>
<li><a href="../de446514/index.html">Google Mail ist 15 Jahre alt</a></li>
<li><a href="../de446518/index.html">Webanwendungs-Firewalls</a></li>
<li><a href="../de446520/index.html">Wie alles begann: die Geschichte von fliegenden Drohnen</a></li>
<li><a href="../de446522/index.html">Swift 5.1 - was ist neu?</a></li>
<li><a href="../de446532/index.html">Upwork führt eine Gebühr für das Recht ein, an einen potenziellen Kunden zu schreiben</a></li>
<li><a href="../de446534/index.html">Visual Studio 2019 veröffentlicht</a></li>
<li><a href="../de446536/index.html">Warteschlangen und JMeter: Austausch mit Publisher und Subscriber</a></li>
<li><a href="../de446538/index.html">PhotoGuru wechselte zur "dunklen Seite" und "weiser"</a></li>
<li><a href="../de446544/index.html">Microsoft erweitert das Azure IP Advantage-Programm um neue IP-Vorteile für Azure IoT-Innovatoren und -Starts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>