<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘œ ğŸ‘¨â€ğŸ³ â›¹ï¸ Word2vec in Bildern ğŸ¥‹ ğŸˆ³ ğŸ¥ </title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="â€ Jedes Ding verbirgt ein Muster, das Teil des Universums ist. Es hat Symmetrie, Eleganz und SchÃ¶nheit - Eigenschaften, die vor allem von jedem wahren...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Word2vec in Bildern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/446530/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85d/ad8/627/85dad8627ae6845b62f5bb965c291b19.png"></div><br><br><blockquote>  <font color="gray">â€ <b>Jedes Ding verbirgt ein Muster, das Teil des Universums ist.</b></font>  <font color="gray"><b>Es hat Symmetrie, Eleganz und SchÃ¶nheit</b> - Eigenschaften, die vor allem von jedem wahren KÃ¼nstler erfasst werden, der die Welt einfÃ¤ngt.</font>  <font color="gray">Dieses Muster kann im Wechsel der Jahreszeiten, im Sandfluss entlang des Abhangs, in den verwickelten Zweigen eines Kreosotstrauchs und im Muster seines Blattes erfasst werden.</font> <font color="gray"><br><br></font>  <font color="gray">Wir versuchen, dieses Muster in unserem Leben und in unserer Gesellschaft zu kopieren, und deshalb lieben wir Rhythmus, Gesang, Tanz und verschiedene Formen, die uns glÃ¼cklich machen und uns trÃ¶sten.</font>  <font color="gray">Man kann jedoch auch die Gefahr erkennen, die bei der Suche nach absoluter Perfektion lauert, denn es ist offensichtlich, dass das perfekte Muster unverÃ¤ndert bleibt.</font>  <font color="gray">Und wenn wir uns der Perfektion nÃ¤hern, gehen alle Dinge zu Tode â€œ- <i>Dune</i> (1965)</font> </blockquote><br>  Ich glaube, das Einbettungskonzept ist eine der bemerkenswertesten Ideen beim maschinellen Lernen.  Wenn Sie jemals Siri, Google Assistant, Alexa, Google Translate oder sogar eine Smartphone-Tastatur mit der Vorhersage des nÃ¤chsten Wortes verwendet haben, haben Sie bereits mit dem auf AnhÃ¤ngen basierenden Modell der Verarbeitung natÃ¼rlicher Sprache gearbeitet.  In den letzten Jahrzehnten hat sich dieses Konzept fÃ¼r neuronale Modelle erheblich weiterentwickelt (jÃ¼ngste Entwicklungen umfassen kontextualisierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Worteinbettungen</a> in fortgeschrittenen Modellen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BERT</a> und GPT2). <br><a name="habracut"></a><br>  Word2vec ist eine effektive Methode zur Erstellung von Investitionen, die 2013 entwickelt wurde.  Neben der Arbeit mit WÃ¶rtern erwiesen sich einige seiner Konzepte als wirksam bei der Entwicklung von Empfehlungsmechanismen und der Bedeutung von Daten auch bei kommerziellen, nicht sprachlichen Aufgaben.  Diese Technologie wurde von Unternehmen wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Airbnb</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alibaba</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Spotify</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anghami</a> in ihren Empfehlungs-Engines verwendet. <br><br>  In diesem Artikel werden wir uns mit dem Konzept und der Mechanik der Generierung von AnhÃ¤ngen mit word2vec befassen.  Beginnen wir mit einem Beispiel, um sich mit der Darstellung von Objekten in Vektorform vertraut zu machen.  Wissen Sie, wie viel eine Liste mit fÃ¼nf Zahlen (Vektor) Ã¼ber Ihre PersÃ¶nlichkeit aussagen kann? <br><br><h1>  Personalisierung: Was bist du? </h1><br><blockquote>  <font color="gray">â€Ich gebe dir das WÃ¼stenchamÃ¤leon;</font>  <font color="gray">Seine FÃ¤higkeit, sich mit dem Sand zu verbinden, sagt Ihnen alles, was Sie Ã¼ber die Wurzeln der Ã–kologie und die GrÃ¼nde fÃ¼r die Erhaltung Ihrer PersÃ¶nlichkeit wissen mÃ¼ssen. â€œ</font>  <font color="gray">- <i>Kinder der DÃ¼ne</i></font> </blockquote><br>  Haben Sie auf einer Skala von 0 bis 100 einen introvertierten oder extrovertierten PersÃ¶nlichkeitstyp (wobei 0 der introvertierteste Typ und 100 der extrovertierteste Typ ist)?  Haben Sie jemals einen PersÃ¶nlichkeitstest bestanden: zum Beispiel MBTI oder noch besser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die Big Five</a> ?  Sie erhalten eine Liste mit Fragen und werden dann auf mehreren Achsen bewertet, einschlieÃŸlich Introversion / Extroversion. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79f/11e/e22/79f11ee220ebf9d6f52f51a5b780b090.png"></div><br>  <i><font color="gray">Beispiel der Big Five-Testergebnisse.</font></i>  <i><font color="gray">Er sagt wirklich viel Ã¼ber die PersÃ¶nlichkeit und kann den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">akademischen</a> , <a href="">persÃ¶nlichen</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">beruflichen Erfolg</a> vorhersagen.</font></i>  <i><font color="gray"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier kÃ¶nnen</a> Sie zum Beispiel durchgehen.</font></i> <br><br>  Angenommen, ich habe 38 von 100 Punkten fÃ¼r die Bewertung der Introversion / Extraversion erzielt.  Dies kann wie folgt dargestellt werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e56/729/35d/e5672935d7de17d41e78354d3742e6bc.png"></div><br><br>  Oder auf einer Skala von -1 bis +1: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b39/e23/ce1/b39e23ce1c036b11763e3c45c3659a3e.png"></div><br><br>  Wie gut erkennen wir eine Person nur an dieser EinschÃ¤tzung?  Nicht besonders.  Menschen sind komplexe Wesen.  Daher fÃ¼gen wir eine weitere Dimension hinzu: eine weitere Eigenschaft aus dem Test. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2aa/ab2/ebc/2aaab2ebc1ff30f1fd832e5cf5bf9cb1.png"></div><br>  <i><font color="gray">Sie kÃ¶nnen sich diese beiden Dimensionen als Punkt im Diagramm oder noch besser als Vektor vom Ursprung bis zu diesem Punkt vorstellen.</font></i>  <i><font color="gray">Es gibt groÃŸartige Vektorwerkzeuge, die sehr bald nÃ¼tzlich sein werden.</font></i> <br><br>  Ich zeige nicht, welche PersÃ¶nlichkeitsmerkmale wir in die Tabelle aufgenommen haben, damit Sie nicht an bestimmte Merkmale gebunden werden, sondern sofort die Vektordarstellung der PersÃ¶nlichkeit der Person als Ganzes verstehen. <br><br>  Jetzt kÃ¶nnen wir sagen, dass dieser Vektor teilweise meine PersÃ¶nlichkeit widerspiegelt.  Dies ist eine nÃ¼tzliche Beschreibung beim Vergleich verschiedener Personen.  Angenommen, ich wurde von einem roten Bus angefahren, und Sie mÃ¼ssen mich durch eine Ã¤hnliche Person ersetzen.  Welche der beiden Personen in der folgenden Tabelle Ã¤hnelt mir eher? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/de5/380/b84/de5380b84dc9fec4bb8b52ebe6519e15.png"></div><br><br>  Bei der Arbeit mit Vektoren wird die Ã„hnlichkeit normalerweise durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Otiai-Koeffizienten</a> (geometrischer Koeffizient) berechnet: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/640/e59/7dd/640e597dd741a28bcec986454633e31d.png"></div><br>  <i><font color="green">Person Nr. 1 Ã¤hnelt</font> <font color="gray">eher meinem Charakter.</font></i>  <i><font color="gray">Vektoren in einer Richtung (LÃ¤nge ist ebenfalls wichtig) ergeben einen grÃ¶ÃŸeren Otiai-Koeffizienten</font></i> <br><br>  Auch hier reichen zwei Dimensionen nicht aus, um Menschen zu bewerten.  Jahrzehntelange Entwicklung der Psychologie hat zur Erstellung eines Tests fÃ¼r fÃ¼nf grundlegende PersÃ¶nlichkeitsmerkmale (mit vielen zusÃ¤tzlichen) gefÃ¼hrt.  Verwenden wir also alle fÃ¼nf Dimensionen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/3ae/d7b/df53aed7b1e439561a01e69b3f765487.png"></div><br><br>  Das Problem mit den fÃ¼nf Dimensionen ist, dass es nicht mehr mÃ¶glich ist, ordentliche Pfeile in 2D zu zeichnen.  Dies ist ein hÃ¤ufiges Problem beim maschinellen Lernen, bei dem Sie hÃ¤ufig in einem mehrdimensionalen Raum arbeiten mÃ¼ssen.  Es ist gut, dass der geometrische Koeffizient mit einer beliebigen Anzahl von Messungen funktioniert: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/ab3/bf1/521ab3bf1374c5b37115441b7c2d27cc.png"></div><br>  <i><font color="gray">Der geometrische Koeffizient funktioniert fÃ¼r eine beliebige Anzahl von Messungen.</font></i>  <i><font color="gray">In fÃ¼nf Dimensionen ist das Ergebnis viel genauer.</font></i> <br><br>  Am Ende dieses Kapitels mÃ¶chte ich zwei Hauptideen wiederholen: <br><br><ol><li>  Personen (und andere Objekte) kÃ¶nnen als numerische Vektoren dargestellt werden (was fÃ¼r Autos groÃŸartig ist!). <br></li><li>  Wir kÃ¶nnen leicht berechnen, wie Ã¤hnlich die Vektoren sind. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/195/73d/16d/19573d16de1150ac1874640c79e0b381.png"></div><br><br><h1>  Worteinbettung </h1><br><blockquote>  <font color="gray">"Das Geschenk der Worte ist das Geschenk der TÃ¤uschung und Illusion."</font>  <font color="gray">- <i>Kinder der DÃ¼ne</i></font> </blockquote><br>  Mit diesem VerstÃ¤ndnis werden wir zu den Vektordarstellungen von WÃ¶rtern Ã¼bergehen, die als Ergebnis des Trainings erhalten wurden (sie werden auch als AnhÃ¤nge bezeichnet) und ihre interessanten Eigenschaften untersuchen. <br><br>  Hier ist der Anhang fÃ¼r das Wort "KÃ¶nig" (GloVe-Vektor, auf Wikipedia trainiert): <br><br> <code>[ 0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 , -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 , 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , -0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]</code> <br> <br>  Wir sehen eine Liste mit 50 Zahlen, aber es ist schwer, etwas zu sagen.  Visualisieren wir sie, um sie mit anderen Vektoren zu vergleichen.  Setzen Sie die Zahlen in eine Reihe: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/516/c90/5ac/516c905ac831fe8688db73f0a63d325b.png"></div><br><br>  Kolorieren Sie die Zellen anhand ihrer Werte (rot fÃ¼r nahe 2, weiÃŸ fÃ¼r nahe 0, blau fÃ¼r nahe â€“2): <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/46f/7cb/1d5/46f7cb1d5adc32bd16368b2681ab26a4.png"></a> </div><br><br>  Vergessen Sie jetzt die Zahlen und nur durch Farben kontrastieren wir den â€KÃ¶nigâ€œ mit anderen Worten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c8/6b2/909/1c86b290963e8a42b375cb6a71245185.png"></div><br><br>  Sie sehen, dass â€Mannâ€œ und â€Frauâ€œ viel nÃ¤her beieinander sind als der â€KÃ¶nigâ€œ?  Es sagt etwas.  Vektordarstellungen erfassen viele Informationen / Bedeutungen / Assoziationen dieser WÃ¶rter. <br><br>  Hier ist eine weitere Liste von Beispielen (vergleichen Sie Spalten mit Ã¤hnlichen Farben): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d61/30b/d45/d6130bd4502710631a8c812923230f47.png"></div><br><br>  Es gibt mehrere Dinge zu beachten: <br><br><ol><li>  Durch alle WÃ¶rter geht eine rote Spalte.  Das heiÃŸt, diese WÃ¶rter sind in dieser bestimmten Dimension Ã¤hnlich (und wir wissen nicht, was darin codiert ist). <br></li><li>  Sie kÃ¶nnen sehen, dass "Frau" und "MÃ¤dchen" sehr Ã¤hnlich sind.  Das gleiche gilt fÃ¼r "Mann" und "Junge". <br></li><li>  "Junge" und "MÃ¤dchen" sind in einigen Dimensionen ebenfalls Ã¤hnlich, unterscheiden sich jedoch von "Frau" und "Mann".  KÃ¶nnte dies eine verschlÃ¼sselte vage Vorstellung von Jugend sein?  Wahrscheinlich. <br></li><li>  Alles auÃŸer dem letzten Wort sind die Ideen der Menschen.  Ich habe ein Objekt (Wasser) hinzugefÃ¼gt, um die Unterschiede zwischen den Kategorien zu zeigen.  Sie kÃ¶nnen beispielsweise sehen, wie die blaue Spalte nach unten geht und vor dem Wasservektor stoppt. <br></li><li>  Es gibt klare Dimensionen, in denen der â€KÃ¶nigâ€œ und die â€KÃ¶niginâ€œ einander Ã¤hnlich sind und sich von allen anderen unterscheiden.  Vielleicht ist dort ein vages Konzept der LizenzgebÃ¼hren kodiert? </li></ol><br><h1>  Analogien </h1><br><blockquote>  <font color="gray">â€Worte ertragen jede Last, die wir uns wÃ¼nschen.</font>  <font color="gray">Alles, was erforderlich ist, ist eine Vereinbarung Ã¼ber die Tradition, nach der wir Konzepte entwickeln. â€œ</font>  <font color="gray">- <i>Gott der Kaiser der DÃ¼ne</i></font> </blockquote><br>  BerÃ¼hmte Beispiele, die die unglaublichen Eigenschaften von Investitionen zeigen, sind das Konzept der Analogien.  Wir kÃ¶nnen Wortvektoren addieren und subtrahieren, um interessante Ergebnisse zu erzielen.  Das bekannteste Beispiel ist die Formel â€KÃ¶nig - Mann + Frauâ€œ: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c23/71f/ead/c2371feadc58f2f2a1236c94b6b05eff.png"></div><br>  <i><font color="gray">Mit der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gensim-</a> Bibliothek in Python kÃ¶nnen wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wortvektoren</a> addieren und subtrahieren, und die Bibliothek findet die WÃ¶rter, die dem resultierenden Vektor am nÃ¤chsten liegen.</font></i>  <i><font color="gray">Das Bild zeigt eine Liste der Ã¤hnlichsten WÃ¶rter mit jeweils einem geometrischen Ã„hnlichkeitskoeffizienten</font></i> <br><br>  Wir visualisieren diese Analogie wie zuvor: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/84b/fea/a1984bfeab5a597c6fb6300f7d694901.png"></div><br>  <i><font color="gray">Der resultierende Vektor aus der Berechnung â€KÃ¶nig - Mann + Frauâ€œ ist nicht ganz gleich der â€KÃ¶niginâ€œ, aber dies ist das nÃ¤chste Ergebnis von 400.000 WortanhÃ¤ngen im Datensatz</font></i> <br><br>  Nachdem wir Ã¼ber das AnhÃ¤ngen von WÃ¶rtern nachgedacht haben, lernen wir, wie das Lernen stattfindet.  Bevor Sie jedoch zu word2vec Ã¼bergehen, mÃ¼ssen Sie sich den konzeptionellen Vorfahren der Worteinbettung ansehen: ein neuronales Sprachmodell. <br><br><h1>  Sprachmodell </h1><br><blockquote>  <font color="gray">â€Der Prophet unterliegt nicht den Illusionen der Vergangenheit, Gegenwart oder Zukunft.</font>  <font color="gray"><b>Die FixitÃ¤t sprachlicher Formen bestimmt solche linearen Unterschiede.</b></font>  <font color="gray">Die Propheten halten den SchlÃ¼ssel zum Zungenschloss.</font>  <font color="gray">FÃ¼r sie bleibt das physische Bild nur ein physisches Bild und nichts weiter.</font> <font color="gray"><br><br></font>  <font color="gray">Ihr Universum hat nicht die Eigenschaften eines mechanischen Universums.</font>  <font color="gray">Eine lineare Abfolge von Ereignissen wird vom Beobachter angenommen.</font>  <font color="gray">Ursache und Wirkung?</font>  <font color="gray">Es ist eine ganz andere Sache.</font>  <font color="gray">Der Prophet spricht schicksalhafte Worte aus.</font>  <font color="gray">Sie sehen einen Blick auf ein Ereignis, das "nach der Logik der Dinge" eintreten sollte.</font>  <font color="gray">Aber der Prophet setzt sofort die Energie der unendlichen Wunderkraft frei.</font>  <font color="gray">Das Universum befindet sich in einem spirituellen Wandel. â€œ</font>  <font color="gray">- <i>Gott der Kaiser der DÃ¼ne</i></font> </blockquote><br>  Ein Beispiel fÃ¼r NLP (Natural Language Processing) ist die Vorhersagefunktion des nÃ¤chsten Wortes auf der Tastatur eines Smartphones.  Milliarden von Menschen benutzen es hunderte Male am Tag. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca4/d48/a13/ca4d48a133d58fe3c4c11e0933ea218e.png"></div><br><br>  Die Vorhersage des nÃ¤chsten Wortes ist eine geeignete Aufgabe fÃ¼r <i>ein Sprachmodell</i> .  Sie kann eine Liste von WÃ¶rtern (z. B. zwei WÃ¶rter) nehmen und versuchen, Folgendes vorherzusagen. <br><br>  Im obigen Screenshot hat das Modell diese beiden grÃ¼nen WÃ¶rter ( <code>thou shalt</code> ) genommen und eine Liste von Optionen zurÃ¼ckgegeben (hÃ¶chstwahrscheinlich fÃ¼r das Wort <code>not</code> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a7/0fc/492/5a70fc49208b501202ed188f24ad1f2c.png"></div><br><br>  Wir kÃ¶nnen uns das Modell als Black Box vorstellen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/164/72f/a83/16472fa83e5eadf58f4bb05b50075654.png"></div><br><br>  In der Praxis erzeugt das Modell jedoch mehr als ein Wort.  Es wird eine SchÃ¤tzung der Wahrscheinlichkeit fÃ¼r praktisch alle bekannten WÃ¶rter abgeleitet (das "WÃ¶rterbuch" des Modells variiert von mehreren tausend bis zu mehr als einer Million WÃ¶rtern).  Die Tastaturanwendung findet dann die WÃ¶rter mit den hÃ¶chsten Punktzahlen und zeigt sie dem Benutzer an. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a7/eda/ad6/7a7edaad67dd51240d90426de0b198c2.png"></div><br>  <i><font color="gray">Ein neuronales Sprachmodell gibt die Wahrscheinlichkeit aller bekannten WÃ¶rter an.</font></i>  <i><font color="gray">Wir geben die Wahrscheinlichkeit als Prozentsatz an, aber im resultierenden Vektor werden 40% als 0,4 dargestellt</font></i> <br><br>  Nach dem Training berechneten die ersten neuronalen Modelle ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bengio 2003</a> ) die Prognose in drei Stufen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27b/082/4f8/27b0824f81962e2863d6d4dcccabfdd2.png"></div><br><br>  Der erste Schritt ist fÃ¼r uns der relevanteste, wenn wir Ã¼ber Investitionen sprechen.  Als Ergebnis des Trainings wird eine Matrix mit den AnhÃ¤ngen aller WÃ¶rter in unserem WÃ¶rterbuch erstellt.  Um das Ergebnis zu erhalten, suchen wir einfach nach den Einbettungen der eingegebenen WÃ¶rter und fÃ¼hren die Vorhersage aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1d1/34b/5ac/1d134b5ac32406ea363944887ce5fc53.png"></div><br><br>  Schauen wir uns nun den Lernprozess an und finden heraus, wie diese Investitionsmatrix entsteht. <br><br><h1>  Sprachmodelltraining </h1><br><blockquote>  <font color="gray">â€Der Prozess kann nicht durch Beenden verstanden werden.</font>  <font color="gray">Das VerstÃ¤ndnis muss mit dem Prozess einhergehen, mit seinem Fluss verschmelzen und mit ihm flieÃŸen â€œ- <i>Dune</i></font> </blockquote><br>  Sprachmodelle haben einen groÃŸen Vorteil gegenÃ¼ber den meisten anderen Modellen des maschinellen Lernens: Sie kÃ¶nnen an Texten trainiert werden, die wir im Ãœberfluss haben.  Denken Sie an alle BÃ¼cher, Artikel, Wikipedia-Materialien und andere Formen von Textdaten, die wir haben.  Vergleichen Sie mit anderen Modellen fÃ¼r maschinelles Lernen, die manuelle Arbeit und speziell gesammelte Daten erfordern. <br><br><blockquote>  <b>"Sie mÃ¼ssen das Wort von seiner Firma lernen" - J. R. Furs</b> </blockquote><br>  AnhÃ¤nge fÃ¼r WÃ¶rter werden anhand der umgebenden WÃ¶rter berechnet, die hÃ¤ufiger in der NÃ¤he erscheinen.  Die Mechanik ist wie folgt: <br><br><ol><li>  Wir erhalten viele Textdaten (z. B. alle Wikipedia-Artikel) <br></li><li>  Legen Sie ein Fenster (z. B. drei WÃ¶rter) fest, das durch den Text gleitet. <br></li><li>  Ein Schiebefenster erzeugt Muster fÃ¼r das Training unseres Modells. </li></ol><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a31/fc4/626/a31fc4626de165a21c2c91844b21e7ab.png"></div><br><br>  Wenn dieses Fenster Ã¼ber den Text gleitet, generieren wir (tatsÃ¤chlich) einen Datensatz, mit dem wir das Modell trainieren.  Lassen Sie uns zum VerstÃ¤ndnis sehen, wie ein Schiebefenster mit diesem Satz umgeht: <br><br><blockquote>  <b>â€MÃ¶gest du nicht eine Maschine bauen, die mit der Ã„hnlichkeit des menschlichen Geistes ausgestattet istâ€œ - <i>Dune</i></b> </blockquote><br>  Wenn wir beginnen, befindet sich das Fenster auf den ersten drei WÃ¶rtern des Satzes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/81c/c51/a04/81cc51a0478e1655c8f3f85641cf1e4e.png"></div><br><br>  Wir nehmen die ersten beiden WÃ¶rter fÃ¼r Zeichen und das dritte Wort fÃ¼r das Etikett: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/097/981/086/0979810868ca398fdcad3066294055f5.png"></div><br>  <i><font color="gray">Wir haben die erste Stichprobe in einem Datensatz generiert, der spÃ¤ter zum Unterrichten eines Sprachmodells verwendet werden kann</font></i> <br><br>  Dann verschieben wir das Fenster an die nÃ¤chste Position und erstellen ein zweites Beispiel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98c/0b3/f98/98c0b3f98ebf4790890fd2f66cf86ce9.png"></div><br><br>  Und ziemlich bald sammeln wir einen grÃ¶ÃŸeren Datensatz: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4e7/3ce/50d/4e73ce50d863e1cbfde92a3b595dbaa3.png"></div><br><br>  In der Praxis werden Modelle normalerweise direkt beim Bewegen eines Schiebefensters trainiert.  Logischerweise ist die Phase der Datensatzgenerierung von der Trainingsphase getrennt.  ZusÃ¤tzlich zu neuronalen NetzwerkansÃ¤tzen wurde die N-Gramm-Methode hÃ¤ufig frÃ¼her zum Unterrichten von Sprachmodellen verwendet (siehe das dritte Kapitel des Buches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">â€Sprach- und Sprachverarbeitungâ€œ</a> ).  Um den Unterschied beim Wechsel von N-Gramm zu neuronalen Modellen in realen Produkten zu sehen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier ein Beitrag aus dem Jahr 2015 im Swiftkey-Blog</a> , dem Entwickler meiner bevorzugten Android-Tastatur, der sein neuronales Sprachmodell vorstellt und es mit dem vorherigen N-Gramm-Modell vergleicht.  Ich mag dieses Beispiel, weil es zeigt, wie die algorithmischen Eigenschaften von Anlagen in einer Marketing-Sprache beschrieben werden kÃ¶nnen. <br><br><h1>  Wir schauen in beide Richtungen </h1><br><blockquote>  <font color="gray">â€Ein Paradoxon ist ein Zeichen dafÃ¼r, dass wir versuchen sollten zu Ã¼berlegen, was dahinter steckt.</font>  <font color="gray">Wenn das Paradoxon Sie beunruhigt, bedeutet dies, dass Sie nach dem Absoluten streben.</font>  <font color="gray">Relativisten betrachten das Paradox einfach als einen interessanten, vielleicht lustigen, manchmal beÃ¤ngstigenden Gedanken, aber einen sehr lehrreichen Gedanken. â€œ</font>  <font color="gray"><i>Kaisergott der DÃ¼ne</i></font> </blockquote><br>  FÃ¼llen Sie auf der Grundlage des Vorstehenden die LÃ¼cke aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/680/613/871/680613871307e53415ab86fab022276a.png"></div><br><br>  Als Kontext gibt es fÃ¼nf vorherige WÃ¶rter (und einen frÃ¼heren Verweis auf â€Busâ€œ).  Ich bin sicher, dass die meisten von Ihnen vermutet haben, dass es einen "Bus" geben sollte.  Aber wenn ich Ihnen nach dem Leerzeichen noch ein Wort gebe, Ã¤ndert dies Ihre Antwort? <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/45f/1fa/af0/45f1faaf0cdd4f57ac1699d87861934a.png"></div><br><br>  Dies Ã¤ndert die Situation vÃ¶llig: Jetzt ist das fehlende Wort hÃ¶chstwahrscheinlich â€rotâ€œ.  Offensichtlich haben WÃ¶rter sowohl vor als auch nach einem Leerzeichen einen informativen Wert.  Es stellt sich heraus, dass Sie mit der Buchhaltung in beide Richtungen (links und rechts) bessere Investitionen berechnen kÃ¶nnen.  Mal sehen, wie man das Modelltraining in einer solchen Situation konfiguriert. <br><br><h1>  Gramm Ã¼berspringen </h1><br><blockquote>  <font color="gray">"Wenn eine absolut unverwechselbare Wahl unbekannt ist, hat der Intellekt die MÃ¶glichkeit, mit begrenzten Daten in der Arena zu arbeiten, in der Fehler nicht nur mÃ¶glich, sondern auch notwendig sind."</font>  <font color="gray">- <i>Capitul Dunes</i></font> </blockquote><br>  ZusÃ¤tzlich zu zwei WÃ¶rtern vor dem Ziel kÃ¶nnen Sie zwei weitere WÃ¶rter nach dem Ziel berÃ¼cksichtigen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2b/1f6/1a1/e2b1f61a179e7d6835b47c7149a47486.png"></div><br><br>  Dann sieht der Datensatz fÃ¼r das Modelltraining folgendermaÃŸen aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/729/ed4/6ff729ed4ce86722dc9c3aa689614195.png"></div><br><br>  Dies wird als CBOW-Architektur (Continuous Bag of Words) bezeichnet und in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einem der word2vec</a> [pdf] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">-Dokumente beschrieben</a> .  Es gibt eine andere Architektur, die ebenfalls hervorragende Ergebnisse zeigt, jedoch etwas anders angeordnet ist: Sie versucht, die benachbarten WÃ¶rter anhand des aktuellen Wortes zu erraten.  Ein Schiebefenster sieht ungefÃ¤hr so â€‹â€‹aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc9/72d/baa/dc972dbaa78b592ba91b76e950ec56e0.png"></div><br>  <i><font color="gray">Im grÃ¼nen Schlitz befindet sich das Eingangswort, und jedes rosa Feld reprÃ¤sentiert einen mÃ¶glichen Ausgang</font></i> <br><br>  Rosa Rechtecke haben unterschiedliche Schattierungen, da dieses Schiebefenster tatsÃ¤chlich vier separate Muster in unserem Trainingsdatensatz erstellt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/709/8ac/dde/7098acddea8266d1efd5663ed98e6303.png"></div><br><br>  Diese Methode wird als <b>Skip-Gram-</b> Architektur bezeichnet.  Sie kÃ¶nnen ein Schiebefenster wie folgt visualisieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/1d8/508/ee21d850835bde9e3f14250d267d88b1.png"></div><br><br>  Die folgenden vier Beispiele werden dem Trainingsdatensatz hinzugefÃ¼gt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a40/871/f1c/a40871f1c1c7b48723d3737c05fc6284.png"></div><br><br>  Dann bewegen wir das Fenster an die folgende Position: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/14a/429/c7b/14a429c7b2ae6ba7383d6d39be9e3031.png"></div><br><br>  Was vier weitere Beispiele hervorbringt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e9b/b3c/89a/e9bb3c89a00306b3fd18eb86d8f2160b.png"></div><br><br>  Bald werden wir viel mehr Proben haben: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/749/096/6bb749096d3329712a7c00727b4d3cff.png"></div><br><br><h1>  Lernbericht </h1><br><blockquote>  <font color="gray">â€Muad'Dib lernte schnell, weil ihm hauptsÃ¤chlich das Lernen beigebracht wurde.</font>  <font color="gray">Aber die allererste Lektion war die Assimilation des Glaubens, dass er lernen kann, und das ist die Grundlage von allem.</font>  <font color="gray">Es ist erstaunlich, wie viele Menschen nicht glauben, dass sie lernen und lernen kÃ¶nnen, und wie viele Menschen glauben, dass Lernen sehr schwierig ist. "</font>  <font color="gray">- <i>DÃ¼ne</i></font> </blockquote><br>  Nachdem wir das Sprunggramm-Set festgelegt haben, verwenden wir es, um das grundlegende neuronale Modell der Sprache zu trainieren, die ein benachbartes Wort vorhersagt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/944/fb7/70d/944fb770d3aff38f1befa40dfaa7402a.png"></div><br><br>  Beginnen wir mit dem ersten Beispiel in unserem Datensatz.  Wir nehmen das Zeichen und senden es an das ungeÃ¼bte Modell mit der Bitte, das nÃ¤chste Wort vorherzusagen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abb/5cb/9a3/abb5cb9a38d29f1a54176206637131dc.png"></div><br><br>  Das Modell durchlÃ¤uft drei Schritte und zeigt einen Vorhersagevektor an (mit Wahrscheinlichkeit fÃ¼r jedes Wort im WÃ¶rterbuch).  Da das Modell nicht trainiert ist, ist seine Prognose zu diesem Zeitpunkt wahrscheinlich falsch.  Aber das ist nichts.  Wir wissen, welches Wort sie vorhersagt - dies ist die resultierende Zelle in der Zeile, mit der wir das Modell derzeit trainieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/4fd/c3d/8af4fdc3d3cc86ec1c81fdb3d2715529.png"></div><br>  <i><font color="gray">Ein "Zielvektor" ist einer, bei dem das Zielwort eine Wahrscheinlichkeit von 1 hat und alle anderen WÃ¶rter eine Wahrscheinlichkeit von 0 haben</font></i> <br><br>  Wie falsch war das Modell?  Subtrahieren Sie den Prognosevektor vom Ziel und erhalten Sie den Fehlervektor: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e6d/b3e/395/e6db3e39593e9c8639d94ef4caccde58.png"></div><br><br>  Dieser Fehlervektor kann jetzt zum Aktualisieren des Modells verwendet werden, sodass es beim nÃ¤chsten Mal wahrscheinlicher ist, dass dieselben Eingabedaten ein genaues Ergebnis liefern. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7d3/6c0/476/7d36c047604b937c907a4ef38ceaaeb7.png"></div><br><br>  Hier endet die erste Ausbildungsphase.  Wir machen dasselbe mit der nÃ¤chsten Stichprobe im Datensatz und dann mit der nÃ¤chsten, bis wir alle Stichproben untersucht haben.  Dies ist das Ende der ersten Ã„ra des Lernens.  Wir wiederholen alles fÃ¼r mehrere Epochen immer wieder und erhalten so ein geschultes Modell: Daraus kÃ¶nnen Sie die Investitionsmatrix extrahieren und in beliebigen Anwendungen verwenden. <br><br>  Wir haben zwar viel gelernt, aber um zu verstehen, wie word2vec wirklich lernt, fehlen einige SchlÃ¼sselideen. <br><br><h1>  Negative Auswahl </h1><br><blockquote>  <font color="gray">â€Der Versuch, Muad'Dib zu verstehen, ohne seine Todfeinde - den Harkonnenov - zu verstehen, ist der gleiche wie der Versuch, die Wahrheit zu verstehen, ohne zu verstehen, was die LÃ¼ge ist.</font>  <font color="gray">Dies ist ein Versuch, das Licht zu kennen, ohne die Dunkelheit zu kennen.</font>  <font color="gray">Es ist unmÃ¶glich".</font>  <font color="gray">- <i>DÃ¼ne</i></font> </blockquote><br>  Erinnern Sie sich an die drei Schritte, wie ein neuronales Modell eine Prognose berechnet: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8dd/fe1/4ca/8ddfe14ca387bd4d16c77eb9de8ce98f.png"></div><br><br>  Der dritte Schritt ist aus rechnerischer Sicht sehr teuer, insbesondere wenn Sie ihn fÃ¼r jede Stichprobe im Datensatz ausfÃ¼hren (zig Millionen Mal).  Es ist notwendig, die ProduktivitÃ¤t irgendwie zu steigern. <br><br>  Eine MÃ¶glichkeit besteht darin, das Ziel in zwei Phasen zu unterteilen: <br><br><ol><li>  Erstellen Sie hochwertige WortanhÃ¤nge (ohne das nÃ¤chste Wort vorherzusagen). <br></li><li>  Verwenden Sie diese hochwertigen Investitionen fÃ¼r den Unterricht des Sprachmodells (fÃ¼r Prognosen). </li></ol><br>  Dieser Artikel konzentriert sich auf den ersten Schritt.  Um die ProduktivitÃ¤t zu steigern, kÃ¶nnen Sie nicht mehr ein benachbartes Wort vorhersagen ... <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/0a8/c17/77d0a8c17587248a0f790155809798fe.png"></div><br><br>  ... und wechseln Sie zu einem Modell, das Eingabe- und AusgabewÃ¶rter verwendet und die Wahrscheinlichkeit ihrer NÃ¤he berechnet (von 0 bis 1). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/0e3/e06/2200e3e063f3119099d1615e59538d2a.png"></div><br><br>  Ein solch einfacher Ãœbergang ersetzt das neuronale Netzwerk durch ein logistisches Regressionsmodell - so werden Berechnungen viel einfacher und schneller. <br><br>  Gleichzeitig mÃ¼ssen wir die Struktur unseres Datensatzes verfeinern: Die Bezeichnung ist jetzt eine neue Spalte mit den Werten 0 oder 1. In unserer Tabelle sind Einheiten Ã¼berall, weil wir dort Nachbarn hinzugefÃ¼gt haben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc2/d1e/874/dc2d1e87438b2492dc9b6e4b1c72162e.png"></div><br><br>  Ein solches Modell wird mit einer unglaublichen Geschwindigkeit berechnet: Millionen von Proben in Minuten.  Aber Sie mÃ¼ssen eine LÃ¼cke schlieÃŸen.  Wenn alle unsere Beispiele positiv sind (Ziel: 1), kann sich ein kniffliges Modell bilden, das immer 1 zurÃ¼ckgibt und 100% Genauigkeit zeigt, aber nichts lernt und Junk-Investitionen generiert. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ba/aa2/2d0/1baaa22d0b0c06be5398f896fa7a4c4b.png"></div><br><br>  Um dieses Problem zu lÃ¶sen, mÃ¼ssen Sie <i>negative Muster</i> in den Datensatz eingeben - WÃ¶rter, die definitiv keine Nachbarn sind.  FÃ¼r sie muss das Modell 0 zurÃ¼ckgeben. Jetzt muss das Modell hart arbeiten, aber die Berechnungen laufen immer noch mit groÃŸer Geschwindigkeit ab. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f4c/194/0d8/f4c1940d80c5203620196907a1478431.png"></div><br>  <i><font color="gray">FÃ¼gen Sie fÃ¼r jede Probe im Datensatz negative Beispiele mit der Bezeichnung 0 hinzu</font></i> <br><br>  Aber was ist als Ausgabewort einzufÃ¼hren?  WÃ¤hlen Sie die WÃ¶rter beliebig: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84e/b22/06f/84eb2206f26b053f1ea8ec4e1b76c5b6.png"></div><br><br>  Diese Idee wurde unter dem Einfluss der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Rauschvergleichsmethode</a> [pdf] geboren.  Wir gleichen das tatsÃ¤chliche Signal (positive Beispiele benachbarter WÃ¶rter) mit Rauschen (zufÃ¤llig ausgewÃ¤hlte WÃ¶rter, die keine Nachbarn sind) ab.  Dies bietet einen hervorragenden Kompromiss zwischen Leistung und statistischer Leistung. <br><br><h1>  Skip-Gramm-Negativprobe (SGNS) </h1><br>  Wir haben uns zwei zentrale Konzepte von word2vec angesehen: Zusammen werden sie als "Sprunggramm mit negativer Stichprobe" bezeichnet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/873/720/fae/873720fae559ce7d6020be66ccb6c397.png"></div><br><br><h1>  Word2vec lernen </h1><br><blockquote>  <font color="gray">â€Eine Maschine kann nicht jedes Problem vorhersehen, das fÃ¼r eine lebende Person wichtig ist.</font>  <font color="gray">Es gibt einen groÃŸen Unterschied zwischen diskretem Raum und kontinuierlichem Kontinuum.</font>  <font color="gray">Wir leben in einem Raum und Maschinen existieren in einem anderen. â€œ</font>  <font color="gray">- <i>Gott der Kaiser der DÃ¼ne</i></font> </blockquote><br>  Nachdem wir die Grundideen von Skip-Gram und Negativ-Sampling untersucht haben, kÃ¶nnen wir uns den Lernprozess von word2vec genauer ansehen. <br><br>  Zuerst verarbeiten wir den Text vor, auf dem wir das Modell trainieren.  Definieren Sie die GrÃ¶ÃŸe des WÃ¶rterbuchs (wir nennen es <code>vocab_size</code> ), beispielsweise in 10.000 AnhÃ¤ngen und den Parametern der WÃ¶rter im WÃ¶rterbuch. <br><br>  Zu Beginn des Trainings erstellen wir zwei Matrizen: <code>Embedding</code> und <code>Context</code> .  AnhÃ¤nge fÃ¼r jedes Wort werden in diesen Matrizen in unserem WÃ¶rterbuch gespeichert (daher ist <code>vocab_size</code> einer ihrer Parameter).  Der zweite Parameter ist die Dimension des Anhangs (normalerweise wird <code>embedding_size</code> auf 300 gesetzt, aber zuvor haben wir uns ein Beispiel mit 50 Dimensionen angesehen). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/8f8/018/1b78f8018d20fd36d0c5aef37d87a249.png"></div><br><br>  Zuerst initialisieren wir diese Matrizen mit zufÃ¤lligen Werten.  Dann beginnen wir den Lernprozess.  In jeder Phase nehmen wir ein positives und die damit verbundenen negativen Beispiele.  Hier ist unsere erste Gruppe: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/261/806/2d22618069aa9e3f8a820cb431c6c014.png"></div><br><br>  Wir haben jetzt vier WÃ¶rter: das Eingabewort <code>not</code> und die Ausgabe- / KontextwÃ¶rter <code>thou</code> (tatsÃ¤chlicher Nachbar), <code>aaron</code> und <code>taco</code> (negative Beispiele).  Wir beginnen die Suche nach ihren AnhÃ¤ngen in den Matrizen <code>Embedding</code> (fÃ¼r das Eingabewort) und <code>Context</code> (fÃ¼r die KontextwÃ¶rter), obwohl beide Matrizen AnhÃ¤nge fÃ¼r alle WÃ¶rter aus unserem WÃ¶rterbuch enthalten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23e/ff0/691/23eff069128db956ce358ae758c0b8bb.png"></div><br><br>  Dann berechnen wir das Skalarprodukt des Eingabeanhangs mit jedem der KontextanhÃ¤nge.  In jedem Fall wird eine Zahl erhalten, die die Ã„hnlichkeit von Eingabedaten und KontextanhÃ¤ngen angibt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/615/319/e4a/615319e4accc7235c28fc8c76dca09f6.png"></div><br><br>  Jetzt brauchen wir eine MÃ¶glichkeit, diese SchÃ¤tzungen in eine Art Wahrscheinlichkeit umzuwandeln: Alle mÃ¼ssen positive Zahlen zwischen 0 und 1 sein. Dies ist eine hervorragende Aufgabe fÃ¼r <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sigmoidale</a> logistische Gleichungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98b/125/8b2/98b1258b21744917c993e617e0844ad8.png"></div><br><br>  Das Ergebnis der Sigmoidberechnung kann als Ausgabe des Modells fÃ¼r diese Stichproben betrachtet werden.  Wie Sie sehen kÃ¶nnen, hat <code>taco</code> die hÃ¶chste Punktzahl, wÃ¤hrend <code>aaron</code> vor und nach dem Sigmoid immer noch die niedrigste Punktzahl hat. <br><br>  Wenn das nicht trainierte Modell eine Prognose erstellt hat und eine echte Zielmarke zum Vergleich hat, berechnen wir, wie viele Fehler in der Modellprognose enthalten sind.  Subtrahieren Sie dazu einfach die Sigmoid-Punktzahl von den Zielbezeichnungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d49/458/333/d49458333c28225c596014df3c6fcedb.png"></div><br>  <i><font color="gray"><code>error</code> = <code>target</code> - <code>sigmoid_scores</code></font></i> <br><br>  Hier beginnt die â€Lernphaseâ€œ aus dem Begriff â€maschinelles Lernenâ€œ.  Jetzt kÃ¶nnen wir diese FehlerschÃ¤tzung verwenden, um die Investitionen <code>not</code> , <code>thou</code> , <code>aaron</code> und <code>taco</code> so anzupassen, dass das Ergebnis das nÃ¤chste Mal nÃ¤her an den ZielschÃ¤tzungen liegt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a5d/e51/cd3/a5de51cd3a86a1ed0784a709cb979bdc.png"></div><br><br>  Damit ist eine Ausbildungsstufe abgeschlossen.  Wir haben die Anhaftung einiger WÃ¶rter ein wenig verbessert ( <code>not</code> <code>thou</code> , <code>aaron</code> und <code>taco</code> ).  Nun fahren wir mit der nÃ¤chsten Stufe fort (der nÃ¤chsten positiven und den damit verbundenen negativen Probe) und wiederholen den Vorgang. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/2ab/c3a/6372abc3a3f6b623d5b2cbab02953030.png"></div><br><br>  Die AnhÃ¤nge verbessern sich weiter, wenn wir den gesamten Datensatz mehrmals durchlaufen.  Sie kÃ¶nnen den Prozess dann stoppen, die <code>Context</code> beiseite legen und die trainierte <code>Embeddings</code> fÃ¼r die nÃ¤chste Aufgabe verwenden. <br><br><h1>  FenstergrÃ¶ÃŸe und Anzahl der negativen Proben </h1><br>  Beim Lernen von word2vec sind zwei wichtige Hyperparameter die FenstergrÃ¶ÃŸe und die Anzahl der negativen Stichproben. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fe/437/447/9fe4374479f547c1b324c7471cd61cbd.png"></div><br><br>  Unterschiedliche FenstergrÃ¶ÃŸen eignen sich fÃ¼r unterschiedliche Aufgaben.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Es wurde festgestellt,</a> dass kleinere FenstergrÃ¶ÃŸen (2â€“15) <i>austauschbare</i> AnhÃ¤nge mit Ã¤hnlichen Indizes erzeugen (beachten Sie, dass Antonyme bei der Betrachtung der umgebenden WÃ¶rter hÃ¤ufig austauschbar sind: Beispielsweise werden die WÃ¶rter â€gutâ€œ und â€schlechtâ€œ hÃ¤ufig in Ã¤hnlichen Kontexten erwÃ¤hnt).  GrÃ¶ÃŸere FenstergrÃ¶ÃŸen (15â€“50 oder mehr) erzeugen <i>verwandte</i> AnhÃ¤nge mit Ã¤hnlichen Indizes.  In der Praxis mÃ¼ssen Sie hÃ¤ufig <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anmerkungen</a> fÃ¼r nÃ¼tzliche semantische Ã„hnlichkeiten in Ihrer Aufgabe bereitstellen.  In Gensim betrÃ¤gt die StandardfenstergrÃ¶ÃŸe 5 (zwei WÃ¶rter links und rechts zusÃ¤tzlich zum eingegebenen Wort selbst). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f4/b0a/45a/4f4b0a45a8552d6c19c7c9459302ac48.png"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die Anzahl der negativen Stichproben ist ein weiterer Faktor im Lernprozess. </font><font style="vertical-align: inherit;">Das Originaldokument empfiehlt 5â€“20. </font><font style="vertical-align: inherit;">Es heiÃŸt auch, dass 2-5 Stichproben ausreichend zu sein scheinen, wenn Sie einen ausreichend groÃŸen Datensatz haben. </font><font style="vertical-align: inherit;">In Gensim betrÃ¤gt der Standardwert 5 negative Muster.</font></font><br><br><h1>  Fazit </h1><br><blockquote> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">â€Wenn Ihr Verhalten Ã¼ber Ihre Standards hinausgeht, sind Sie eine lebende Person, kein Automatâ€œ - </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gott-Kaiser der DÃ¼ne</font></font></i></font> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich hoffe, Sie verstehen die Einbettung von WÃ¶rtern und die Essenz des word2vec-Algorithmus. </font><font style="vertical-align: inherit;">Ich hoffe auch, dass Sie jetzt die Artikel besser verstehen, in denen das Konzept des "Sprunggramms mit negativer Stichprobe" (SGNS) wie in den obigen Empfehlungssystemen erwÃ¤hnt wird.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Referenzen und weiterfÃ¼hrende Literatur </font></font></h1><br><ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Verteilte Darstellungen von WÃ¶rtern und Phrasen und deren Zusammensetzung"</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> [pdf]</font></font></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«      Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«   Â»</a> [pdf] </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«   Â»</a>      â€”    NLP. Word2vec    . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«      Â»</a> by <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> â€”      . </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a>        Word2vec.        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â« word2vecÂ»</a> </li><li>   ?   : <ul><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> word2vec  Python</a>  Gensim </li><li>  <a href="">   C</a> ,    <a href="">       </a> </li></ul></li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">    </a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«  Â»</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> 2</a> </li><li> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Â«Â»</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de446530/">https://habr.com/ru/post/de446530/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de446512/index.html">.NET Core Worker als Windows-Dienste</a></li>
<li><a href="../de446514/index.html">Google Mail ist 15 Jahre alt</a></li>
<li><a href="../de446518/index.html">Webanwendungs-Firewalls</a></li>
<li><a href="../de446520/index.html">Wie alles begann: die Geschichte von fliegenden Drohnen</a></li>
<li><a href="../de446522/index.html">Swift 5.1 - was ist neu?</a></li>
<li><a href="../de446532/index.html">Upwork fÃ¼hrt eine GebÃ¼hr fÃ¼r das Recht ein, an einen potenziellen Kunden zu schreiben</a></li>
<li><a href="../de446534/index.html">Visual Studio 2019 verÃ¶ffentlicht</a></li>
<li><a href="../de446536/index.html">Warteschlangen und JMeter: Austausch mit Publisher und Subscriber</a></li>
<li><a href="../de446538/index.html">PhotoGuru wechselte zur "dunklen Seite" und "weiser"</a></li>
<li><a href="../de446544/index.html">Microsoft erweitert das Azure IP Advantage-Programm um neue IP-Vorteile fÃ¼r Azure IoT-Innovatoren und -Starts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>