<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóæ üê§ ü§¥ Pero dices Ceph ... ¬øes tan bueno? üõï üéê üï∏Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Amo a Ceph He estado trabajando con √©l durante 4 a√±os (0.80.x -  12.2.6  , 12.2.5). A veces me apasiona tanto que paso tardes y noches en su compa√±√≠a,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pero dices Ceph ... ¬øes tan bueno?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/422905/"><p><img src="https://habrastorage.org/webt/fm/pp/3b/fmpp3bma4xf_j2pxwystnjgezc0.png"></p><br><p>  Amo a Ceph  He estado trabajando con √©l durante 4 a√±os (0.80.x - <del>  12.2.6 </del> , 12.2.5).  A veces me apasiona tanto que paso tardes y noches en su compa√±√≠a, y no con mi novia.  He encontrado varios problemas en este producto, y sigo viviendo con algunos hasta el d√≠a de hoy.  A veces me regocijaba por las decisiones f√°ciles, y otras so√±aba con reunirme con desarrolladores para expresar mi indignaci√≥n.  Pero Ceph todav√≠a se usa en nuestro proyecto y es posible que se use en nuevas tareas, al menos para m√≠.  En esta historia compartir√© nuestra experiencia de operar Ceph, de alguna manera me expresar√© sobre el tema de lo que no me gusta de esta soluci√≥n y tal vez ayude a aquellos que solo la est√°n viendo.  Los eventos que comenzaron hace aproximadamente un a√±o cuando traje el Dell EMC ScaleIO, ahora conocido como Dell EMC VxFlex OS, me empujaron a escribir este art√≠culo. </p><br><p>  ¬°Esto no es en absoluto un anuncio para Dell EMC o su producto!  Personalmente, no soy muy bueno con las grandes corporaciones y las cajas negras como VxFlex OS.  Pero como saben, todo en el mundo es relativo y, utilizando el ejemplo del sistema operativo VxFlex, es muy conveniente mostrar qu√© es Ceph en t√©rminos de operaci√≥n, y tratar√© de hacerlo. <a name="habracut"></a></p><br><h2 id="parametry-rech-idet-o-4-znachnyh-chislah">  Par√°metros  ¬°Se trata de n√∫meros de 4 d√≠gitos! </h2><br><p>  Servicios Ceph como MON, OSD, etc.  tener varios par√°metros para configurar todo tipo de subsistemas.  Los par√°metros se establecen en el archivo de configuraci√≥n, los demonios los leen en el momento del lanzamiento.  Algunos valores se pueden cambiar convenientemente sobre la marcha utilizando el mecanismo de "inyecci√≥n", que se describe a continuaci√≥n.  Todo es casi super, si omites el momento en que hay cientos de par√°metros: <br><br>  Martillo </p><br><pre><code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 863</code> </pre> <br><p>  Luminosa: </p><br><pre> <code class="html hljs xml">&gt; ceph daemon mon.a config show | wc -l 1401</code> </pre> <br><p>  Resulta ~ 500 nuevos par√°metros en dos a√±os.  En general, la parametrizaci√≥n es genial, no es bueno que haya dificultades para comprender el 80% de esta lista.  La documentaci√≥n describe seg√∫n mis estimaciones ~ 20% y en algunos lugares es ambigua.  Debe entenderse el significado de la mayor√≠a de los par√°metros en el github del proyecto o en las listas de correo, pero esto no siempre ayuda. </p><br><p>  Aqu√≠ hay un ejemplo de varios par√°metros que me interesaron hace poco, los encontr√© en el blog de un Ceph-gadfly: </p><br><pre> <code class="html hljs xml">throttler_perf_counter = false // enable/disable throttler perf counter osd_enable_op_tracker = false // enable/disable OSD op tracking</code> </pre> <br><p>  Comentarios de c√≥digo en el esp√≠ritu de las mejores pr√°cticas.  Como si entendiera las palabras e incluso m√°s o menos de qu√© se tratan, pero lo que me dar√° no lo es. </p><br><p>  O aqu√≠: <strong>osd_op_threads</strong> en Luminous desapareci√≥ y solo las fuentes ayudaron a encontrar un nuevo nombre: <strong>osd_peering_wq threads</strong> </p><br><p>  Tambi√©n me gusta que hay opciones especialmente hol√≠sticas.  Aqu√≠ amigo muestra que aumentar <strong>rgw_num _rados_handles</strong> es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bueno</a> : </p><br><p>  y el otro tipo piensa que&gt; 1 es imposible e incluso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">peligroso</a> . </p><br><p>  Y lo que m√°s me gusta es cuando los principiantes dan ejemplos de una configuraci√≥n en sus publicaciones de blog, donde todos los par√°metros se copian sin pensar (me parece) copiados de otro blog del mismo tipo, por lo que un mont√≥n de par√°metros que nadie conoce, excepto el autor del c√≥digo, se desv√≠an de config a config. </p><br><p>  Tambi√©n me quemo salvajemente con lo que hicieron en Luminous.  Hay una caracter√≠stica genial: cambiar los par√°metros sobre la marcha, sin reiniciar los procesos.  Puede, por ejemplo, cambiar el par√°metro de un OSD espec√≠fico: </p><br><pre> <code class="html hljs xml">&gt; ceph tell osd.12 injectargs '--filestore_fd_cache_size=512'</code> </pre> <br><p>  o ponga '*' en lugar de 12 y el valor cambiar√° en todos los OSD.  Es genial, de verdad.  Pero, como mucho en Ceph, esto se hace con el pie izquierdo.  El dise√±o Bai no puede cambiar todos los valores de los par√°metros sobre la marcha.  M√°s precisamente, se pueden configurar y aparecer√°n modificados en la salida, pero de hecho, solo unos pocos se vuelven a leer y se vuelven a aplicar.  Por ejemplo, no puede cambiar el tama√±o del grupo de subprocesos sin reiniciar el proceso.  Para que el ejecutor del equipo comprenda que es in√∫til cambiar el par√°metro de esta manera, decidieron imprimir un mensaje.  Hola </p><br><p>  Por ejemplo: </p><br><pre> <code class="html hljs xml">&gt; ceph tell mon.* injectargs '--mon_allow_pool_delete=true' mon.c: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.a: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart) mon.b: injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</code> </pre> <br><p>  Ambiguo  De hecho, la eliminaci√≥n de las piscinas se hace posible despu√©s de la inyecci√≥n.  Es decir, esta advertencia no es relevante para este par√°metro.  Ok, pero todav√≠a hay cientos de par√°metros, incluidos algunos muy √∫tiles, que tambi√©n tienen una advertencia y no hay forma de verificar su aplicabilidad real.  Por el momento, ni siquiera puedo entender por el c√≥digo qu√© par√°metros se aplican despu√©s de la inyecci√≥n y cu√°les no.  Para confiabilidad, debe reiniciar los servicios y esto, ya sabe, enfurece.  Se enfurece porque s√© que hay un mecanismo de inyecci√≥n. </p><br><p>  ¬øQu√© pasa con VxFlex OS?  Procesos similares como MON (en VxFlex es MDM), OSD (SDS en VxFlex) tambi√©n tienen archivos de configuraci√≥n, en los que hay docenas de par√°metros para todos.  Es cierto que sus nombres tampoco dicen nada, pero la buena noticia es que nunca recurrimos a ellos para quemar tanto como con Ceph. </p><br><h2 id="tehnicheskiy-dolg">  Deuda t√©cnica </h2><br><p>  Cuando comienzas a conocer a Ceph con la versi√≥n m√°s relevante para hoy, entonces todo parece estar bien, y quieres escribir un art√≠culo positivo.  Pero cuando vives con √©l en la producci√≥n de la versi√≥n 0.80, entonces no todo parece tan color de rosa. </p><br><p>  Antes de Jewel, los procesos de Ceph se ejecutaban como root.  Jewel decidi√≥ que deber√≠an trabajar desde el usuario 'ceph' y esto requer√≠a un cambio de propiedad para todos los directorios que utilizan los servicios de Ceph.  Parecer√≠a que esto?  Imagine una OSD que da servicio a un disco magn√©tico SATA de capacidad total de 2 TB.  Por lo tanto, la presentaci√≥n de dicho disco, en paralelo (a diferentes subdirectorios) con una utilizaci√≥n completa del disco, demora de 3 a 4 horas.  Imagine, por ejemplo, que tiene 3 cientos de esos discos.  Incluso si actualiza los nodos (se muestran inmediatamente entre 8 y 12 discos), obtiene una actualizaci√≥n bastante larga, en la que el cl√∫ster tendr√° OSD de diferentes versiones y una r√©plica de datos es menor en el momento en que se actualiza el servidor.  En general, pensamos que era absurdo, reconstruimos los paquetes de Ceph y dejamos el OSD ejecut√°ndose como root.  Decidimos que a medida que ingresamos o reemplazamos el OSD, los transferiremos a un nuevo usuario.  Ahora estamos cambiando 2-3 unidades por mes y agregando 1-2, creo que podemos manejarlo para 2022). </p><br><p>  Aplastables sintonizables </p><br><p>  <strong>CRUSH</strong> es el coraz√≥n de Ceph, todo gira a su alrededor.  Este es el algoritmo mediante el cual, de manera pseudoaleatoria, se selecciona la ubicaci√≥n de los datos y gracias a la cual los clientes que trabajan con el grupo RADOS descubrir√°n en qu√© OSD se almacenan los datos (objetos) que necesitan.  La caracter√≠stica clave de CRUSH es que no hay necesidad de ning√∫n servidor de metadatos, como Luster o IBM GPFS (ahora Spectrum Scale).  CRUSH permite que los clientes y OSD interact√∫en directamente entre ellos.  Aunque, por supuesto, es dif√≠cil comparar el primitivo almacenamiento de objetos RADOS y los sistemas de archivos, que di como ejemplo, pero creo que la idea es clara. </p><br><p>  Los par√°metros ajustables de CRUSH, a su vez, son un conjunto de par√°metros / indicadores que afectan el funcionamiento de CRUSH, lo que lo hace m√°s eficiente, al menos en teor√≠a. </p><br><p>  Entonces, al actualizar de Hammer a Jewel (prueba natural), apareci√≥ una advertencia que dec√≠a que el perfil de sintonizaci√≥n tiene par√°metros que no son √≥ptimos para la versi√≥n actual (Jewel) y se recomienda cambiar el perfil a uno √≥ptimo.  En general, todo est√° claro.  El dock dice que esto es muy importante y esta es la forma correcta, pero tambi√©n se dice que despu√©s del cambio de datos habr√° un reequilibrio del 10% de los datos.  10%: no suena aterrador, pero decidimos probarlo.  Para un cl√∫ster, es aproximadamente 10 veces menos que en un producto, con el mismo n√∫mero de PG por OSD, lleno de datos de prueba, ¬°obtuvimos un equilibrio del 60%!  ¬°Imagine, por ejemplo, con 100 TB de datos, 60 TB comienza a moverse entre OSD y esto es con una carga constante del cliente que demanda latencia!  Si a√∫n no lo he dicho, proporcionamos s3 y no tenemos mucha menos carga en rgw incluso de noche, de los cuales hay 8 y 4 m√°s en sitios web est√°ticos.  En general, decidimos que este no era nuestro camino, especialmente porque hacer esa reconstrucci√≥n en la nueva versi√≥n, con la que no hab√≠amos trabajado en la producci√≥n, era al menos demasiado optimista.  Adem√°s, ten√≠amos √≠ndices de cubos grandes que se estaban reconstruyendo muy mal y esta tambi√©n fue la raz√≥n del retraso en el cambio de perfil.  Sobre los √≠ndices ser√°n por separado un poco m√°s bajos.  Al final, simplemente eliminamos la advertencia y decidimos volver a esto m√°s tarde. </p><br><p>  Y al cambiar el perfil en las pruebas, los clientes cephfs que est√°n en los n√∫cleos CentOS 7.2 se cayeron porque no pod√≠an trabajar con el algoritmo de hash m√°s nuevo del nuevo perfil que vino.  No usamos cephfs en el producto, pero si sol√≠amos hacerlo, esta ser√≠a otra raz√≥n para no cambiar el perfil. </p><br><p>  Por cierto, el muelle dice que si lo que sucede durante el reequilibrio no le conviene, puede retroceder el perfil.  De hecho, despu√©s de una instalaci√≥n limpia de la versi√≥n Hammer y la actualizaci√≥n a Jewel, el perfil se ve as√≠: </p><br><pre> <code class="html hljs xml">&gt; ceph osd crush show-tunables { ... "straw_calc_version": 1, "allowed_bucket_algs": 22, "profile": "unknown", "optimal_tunables": 0, ... }</code> </pre> <br><p>  Es importante que sea "desconocido" y si intentas detener la reconstrucci√≥n cambi√°ndolo a "heredado" (como se indica en el muelle) o incluso a "martillo", entonces el equilibrio no se detendr√°, simplemente continuar√° de acuerdo con otros ajustables, y no " √≥ptimo ".  En general, todo debe ser revisado a fondo y doblemente, ceph no es confiable. </p><br><p>  Aplastar el comercio de </p><br><p>  Como saben, todo en este mundo es equilibrado y las desventajas se aplican a todas las ventajas.  La desventaja de CRUSH es que los PG est√°n distribuidos de manera desigual en diferentes OSD, incluso con el mismo peso de estos √∫ltimos.  Adem√°s, nada impide que diferentes PG crezcan a diferentes velocidades, mientras que la funci√≥n hash caer√°.  Espec√≠ficamente, tenemos un rango de utilizaci√≥n de OSD de 48-84%, a pesar de que tienen el mismo tama√±o y, en consecuencia, peso.  Incluso intentamos hacer que los servidores tengan el mismo peso, pero esto es as√≠, solo nuestro perfeccionismo, no m√°s.  Y con el hecho de que IO se distribuye de manera desigual entre los discos, lo peor es que cuando alcanza el estado completo (95%) de al menos un OSD en el cl√∫ster, la grabaci√≥n completa se detiene y el cl√∫ster pasa a ser de solo lectura.  ¬°Todo el grupo!  Y no importa que el grupo a√∫n est√© lleno de espacio.  ¬°Todo, la final, sal!  Esta es una caracter√≠stica arquitect√≥nica de CRUSH.  Imagine que est√° de vacaciones, algunos OSD rompieron la marca del 85% (la primera advertencia por defecto) y tiene un 10% en stock para evitar que se detenga la grabaci√≥n.  Y el 10% con grabaci√≥n activa no es tanto / largo.  Idealmente, con tal dise√±o, Ceph necesita una persona de turno que pueda seguir las instrucciones preparadas en tales casos. </p><br><p>  Entonces, decidimos que significa desequilibrar los datos en el cl√∫ster, porque  varios OSD estuvieron cerca de la marca casi completa (85%). </p><br><p>  Hay varias formas: </p><br><ul><li>  Agregar unidades </li></ul><br><p>  La forma m√°s f√°cil es un poco derrochador y no muy efectivo, porque  los datos en s√≠ mismos no pueden moverse desde el OSD abarrotado o el movimiento ser√° insignificante. </p><br><ul><li>  Cambiar el peso permanente de la OSD (PESO) </li></ul><br><p>  Esto conduce a un cambio en el peso de toda la jerarqu√≠a de cubo superior (terminolog√≠a CRUSH), servidor OSD, centro de datos, etc.  y, como resultado, al movimiento de datos, incluso no de aquellos OSD de los cuales es necesario. <br>  Intentamos, redujimos el peso de un OSD, despu√©s de que se rellenaron los datos para reconstruir otro, lo redujimos, luego el tercero y nos dimos cuenta de que jugar√≠amos esto durante mucho tiempo. </p><br><ul><li>  Cambiar el peso OSD no permanente (REWEIGHT) </li></ul><br><p>  Esto es lo que se hace llamando 'ceph osd reweight-by-utilization'.  Esto lleva a un cambio en el llamado peso de ajuste de OSD, y el peso del cubo m√°s alto no cambia.  Como resultado, los datos se equilibran entre diferentes OSD de un servidor, por as√≠ decirlo, sin ir m√°s all√° de los l√≠mites del dep√≥sito CRUSH.  Realmente nos gust√≥ este enfoque, observamos la ejecuci√≥n en seco de los cambios y los realizamos en el producto.  Todo estuvo bien hasta que el proceso de reequilibrio tuvo una participaci√≥n en el medio.  Nuevamente busca en Google, lee boletines informativos, experimenta con diferentes opciones, y al final resulta que la parada fue causada por la falta de algunos sintonizables en el perfil mencionado anteriormente.  Nuevamente nos vimos atrapados en deudas t√©cnicas.  Como resultado, seguimos el camino de agregar discos y la reconstrucci√≥n m√°s ineficaz.  Afortunadamente, todav√≠a necesit√°bamos hacer esto porque  Se plane√≥ cambiar el perfil CRUSH con un margen suficiente en capacidad. </p><br><p>  S√≠, sabemos sobre el equilibrador (Luminous y superior), que es parte de mgr, que est√° dise√±ado para resolver el problema de la distribuci√≥n desigual de datos moviendo PG entre OSD, por ejemplo, en la noche.  Pero a√∫n no he escuchado cr√≠ticas positivas sobre su trabajo, incluso en el Mimic actual. </p><br><p>  Probablemente dir√° que la deuda t√©cnica es puramente nuestro problema y probablemente estar√≠a de acuerdo.  Pero durante cuatro a√±os con Ceph en la producci√≥n, solo tuvimos un tiempo de inactividad s3 registrado, que dur√≥ una hora completa.  Y luego, el problema no estaba en RADOS, sino en RGW, que, despu√©s de escribir sus 100 subprocesos predeterminados, se colg√≥ y la mayor√≠a de los usuarios no cumplieron con las solicitudes.  Todav√≠a estaba en Hammer.  En mi opini√≥n, este es un buen indicador y se logra debido al hecho de que no hacemos movimientos bruscos y somos bastante esc√©pticos sobre todo en Ceph. </p><br><h2 id="dikiy-gc">  Gc salvaje </h2><br><p>  Como sabe, eliminar datos directamente del disco es una tarea bastante exigente y en los sistemas avanzados, la eliminaci√≥n se retrasa o no se realiza en absoluto.  Ceph tambi√©n es un sistema avanzado y, en el caso de RGW, al eliminar un objeto s3, los objetos RADOS correspondientes no se eliminan inmediatamente del disco.  RGW marca los objetos s3 como eliminados, y un gc-stream separado elimina objetos directamente de los grupos RADOS y, en consecuencia, se pospone de los discos.  Despu√©s de actualizar a Luminous, el comportamiento de gc cambi√≥ notablemente, comenz√≥ a funcionar de manera m√°s agresiva, aunque los par√°metros de gc siguieron siendo los mismos.  Por la palabra notablemente, quiero decir que comenzamos a ver a gc trabajando en monitoreo externo del servicio para saltar la latencia.  Esto fue acompa√±ado por un alto IO en el grupo rgw.gc.  Pero el problema que enfrentamos es mucho m√°s √©pico que solo IO.  Cuando se ejecuta gc, se generan muchos registros del formulario: </p><br><pre> <code class="html hljs xml">0 <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">cls</span></span></span><span class="hljs-tag">&gt;</span></span> /builddir/build/BUILD/ceph-12.2.5/src/cls/rgw/cls_rgw.cc:3284: gc_iterate_entries end_key=1_01530264199.726582828</code> </pre> <br><p>  Donde 0 al principio es el nivel de registro en el que se imprime este mensaje.  Por as√≠ decirlo, no hay ning√∫n lugar para reducir el registro debajo de cero.  Como resultado, un OSD gener√≥ ~ 1 GB de registros en un par de horas en un par de horas, y todo habr√≠a estado bien si los nodos ceph no estuvieran sin disco ... Cargamos el sistema operativo a trav√©s de PXE directamente en la memoria y no utilizamos el disco local o NFS, NBD para la partici√≥n del sistema (/).  Resulta servidores sin estado.  Despu√©s de un reinicio, la automatizaci√≥n completa todo el estado.  C√≥mo funciona, lo describir√© de alguna manera en un art√≠culo separado, ahora es importante que se asignen 6 GB de memoria para "/", de los cuales ~ 4 generalmente son gratuitos.  Enviamos todos los registros a Graylog y utilizamos una pol√≠tica de rotaci√≥n de registros bastante agresiva y generalmente no experimentamos ning√∫n problema con el desbordamiento de disco / RAM.  Pero no est√°bamos preparados para esto, con 12 OSD, el servidor "/" se llen√≥ muy r√°pido, los asistentes a tiempo no respondieron al disparador en Zabbix y OSD comenz√≥ a detenerse debido a la incapacidad de escribir un registro.  Como resultado, redujimos la intensidad de gc, el boleto no comenz√≥ porque  Ya estaba all√≠, y agregamos un script a cron, en el que forzamos a los registros de OSD a truncarse cuando se excede una cierta cantidad sin esperar logrotate.  Por cierto, el nivel de registro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aument√≥</a> . </p><br><h2 id="placement-groups-i-hvalyonaya-masshtabiruemost">  Grupos de colocaci√≥n y escalabilidad elogiada </h2><br><p>  En mi opini√≥n, PG es la abstracci√≥n m√°s dif√≠cil de entender.  Se necesita PG para hacer CRUSH m√°s efectivo.  El objetivo principal de PG es agrupar objetos para reducir el consumo de recursos, aumentar la productividad y la escalabilidad.  Dirigir objetos directamente, individualmente, sin combinarlos en PG ser√≠a muy costoso. </p><br><p>  El principal problema de PG es determinar su n√∫mero para un nuevo grupo.  Del blog de Ceph: </p><br><blockquote>  "Elegir el n√∫mero correcto de PG para su cl√∫ster es un poco de arte negro y una pesadilla de usabilidad". </blockquote><p>  Esto siempre es muy espec√≠fico para una instalaci√≥n en particular y requiere mucho pensamiento y c√°lculo. </p><br><p>  Recomendaciones clave </p><br><ul><li>  Demasiados PG en el OSD son malos; habr√° un gasto excesivo de recursos para su mantenimiento y frenos durante el reequilibrio / recuperaci√≥n. </li><li>  Pocos PG en OSD son malos, el rendimiento se ver√° afectado y los OSD se llenar√°n de manera desigual. </li><li>  El n√∫mero PG debe ser un m√∫ltiplo de grado 2. Esto ayudar√° a obtener el "poder de CRUSH". </li></ul><br><p>  Y aqu√≠ arde conmigo.  Los PG no est√°n limitados en volumen o en la cantidad de objetos.  ¬øCu√°ntos recursos (en n√∫meros reales) se necesitan para dar servicio a un PG?  ¬øDepende de su tama√±o?  ¬øDepende del n√∫mero de r√©plicas de esta PG?  ¬øDebo tomar un ba√±o de vapor si tengo suficiente memoria, CPU r√°pidas y una buena red? <br>  Y tambi√©n debe pensar en el crecimiento futuro del cl√∫ster.  El n√∫mero de PG no puede reducirse, solo aumentarse.  Al mismo tiempo, no se recomienda hacer esto, ya que esto implicar√°, en esencia, dividir parte de PG a una reconstrucci√≥n nueva y salvaje. </p><br><blockquote>  "Aumentar el recuento de PG de un grupo es uno de los eventos m√°s impactantes en un Ceph Cluster, y debe evitarse para los grupos de producci√≥n si es posible". </blockquote><p>  Por lo tanto, debe pensar en el futuro de inmediato, si es posible. </p><br><p>  Un verdadero ejemplo. </p><br><p>  Un cl√∫ster de 3 servidores con OSD de 14x2 TB cada uno, un total de 42 OSD.  R√©plica 3, lugar √∫til ~ 28 TB.  Para ser utilizado en S3, debe calcular el n√∫mero de PG para el grupo de datos y el grupo de √≠ndice.  RGW usa m√°s grupos, pero los dos son primarios. </p><br><p>  Entramos en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">la calculadora PG</a> (existe una calculadora de este tipo), consideramos que con los 100 PG recomendados en el OSD, obtenemos solo 1312 PG.  Pero no todo es tan simple: tenemos uno introductorio: el cl√∫ster definitivamente crecer√° tres veces en un a√±o, pero el hierro se comprar√° un poco m√°s tarde.  Aumentamos "Target PGs por OSD" tres veces, a 300 y obtenemos 4480 PG. </p><br><p><img src="https://habrastorage.org/webt/ce/o3/bo/ceo3boailgnmcx_2_w6un6_covi.png"></p><br><p>  Establezca el n√∫mero de PG para los grupos correspondientes; recibimos una advertencia: demasiados PG por OSD ... llegaron.  Recibi√≥ ~ 300 PG en OSD con un l√≠mite de 200 (Luminous).  Sol√≠a ‚Äã‚Äãser 300, por cierto.  Y lo m√°s interesante es que todos los PG innecesarios no pueden mirar, es decir, esto no es solo una advertencia.  Como resultado, creemos que estamos haciendo todo bien, elevando los l√≠mites, apagando la advertencia y avanzando. </p><br><p>  Otro ejemplo real es m√°s interesante. </p><br><p>  S3, volumen utilizable de 152 TB, 252 OSD a 1.81 TB, ~ 105 PG en OSD.  El cl√∫ster creci√≥ gradualmente, todo estuvo bien hasta que con las nuevas leyes en nuestro pa√≠s hubo una necesidad de crecer a 1 PB, es decir, + ~ 850 TB, y al mismo tiempo necesita mantener el rendimiento, que ahora es bastante bueno para S3.  Supongamos que tomamos discos de 6 (5,7 reales) TB y teniendo en cuenta la r√©plica 3 obtenemos + 447 OSD.  Teniendo en cuenta los actuales, obtenemos 699 OSD con 37 PG cada uno, y si tenemos en cuenta diferentes pesos, resulta que los OSD antiguos tienen solo una docena de PG.  ¬øEntonces me dices c√≥mo tolerablemente esto funcionar√°?  El rendimiento de un cl√∫ster con un n√∫mero diferente de PG es bastante dif√≠cil de medir sint√©ticamente, pero las pruebas que realic√© muestran que para un rendimiento √≥ptimo es necesario de 50 PG a 2 TB OSD.  ¬øY qu√© hay de un mayor crecimiento?  Sin aumentar el n√∫mero de PG, puede ir a la asignaci√≥n de PG a OSD 1: 1.  ¬øQuiz√°s no entiendo algo? </p><br><p>  S√≠, puede crear un nuevo grupo para RGW con el n√∫mero deseado de PG y asignarle una regi√≥n S3 separada.  O incluso construir un nuevo grupo cercano.  Pero debes admitir que todas estas son muletas.  Y resulta que parece un Ceph bien escalable debido a su concepto, PG escala con reservas.  Tendr√° que vivir con vorings deshabilitados en preparaci√≥n para el crecimiento, o en alg√∫n momento reconstruir todos los datos en el cl√∫ster, o puntuar sobre el rendimiento y vivir con lo que sucede.  O ve a trav√©s de todo. </p><br><p>  Me alegra que los desarrolladores de Ceph <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">entiendan</a> que PG es una abstracci√≥n compleja y superflua para el usuario y que es mejor no saberlo. </p><br><blockquote>  "En Luminous hemos tomado medidas importantes para finalmente eliminar una de las formas m√°s comunes de conducir su cl√∫ster a una zanja, y esperamos poder ocultar eventualmente los PG por completo para que no sean algo que la mayor√≠a de los usuarios tengan que saber o pensar en ". </blockquote><p>  En vxFlex no existe el concepto de PG ni ning√∫n an√°logo.  Simplemente agrega discos al grupo y eso es todo.  Y as√≠ sucesivamente hasta 16 PB.  Imag√≠nese, no es necesario calcular nada, no hay un mont√≥n de estados de estos PG, los discos se eliminan de manera uniforme durante todo el crecimiento.  Porque  los discos se entregan a vxFlex en su conjunto (no hay ning√∫n sistema de archivos encima), no hay forma de evaluar la plenitud y no existe tal problema en absoluto.  Ni siquiera s√© c√≥mo transmitirle lo agradable que es. </p><br><h2 id="nuzhno-zhdat-sp1">  "Necesito esperar al SP1" </h2><br><p>  Otra historia de "√©xito".  Como saben, RADOS es el almacenamiento clave-valor m√°s primitivo.  S3, implementado sobre RADOS, tambi√©n es primitivo, pero a√∫n un poco m√°s funcional. ,  S3      .  ,   , RGW       .   ‚Äî  RADOS-,      OSD.         .   ,               . OSD            down.   ,      ,   .  ,   scrub'          .      ,    -  503,      . </p><br><p> <strong>Bucket Index resharding</strong> ‚Äî  ,       (RADOS-) , ,    OSD,         . </p><br><p> ,  ,        Jewel        !  Hammer,      ..    -.       ? </p><br><p>  Hammer       20+  ,       ,     OSD     Graylog ,    .     , ..   IO   .    Luminous, ..         .    Luminous,    , .   ,      . IO    index-,   ,         .    ,  IO     ,      . ,     ‚Ä¶   ; <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> : </p><br><p>  ,      .       , ..           ,   . </p><br><p> ,  Hammer-&gt;Jewel   -   . OSD     -  .        ,    OSD       . </p><br><p>    ‚Äî   ,     ,       .   Hammer    s3,   .      ,  .       ,    ,      etag,   body,     .           .      ,     . Suspend    .   ""           .            ,        . </p><br><h2 id="holivary-na-temu-chisla-replik">      </h2><br><p>       ,    2 ‚Äî  ,         Cloudmouse. ,    Ceph, , . </p><br><p>  vxFlex OS   2    . ,             .       ,    .           ,         .        ,    ,    ,     Dell EMC. </p><br><h2 id="proizvoditelnost">  </h2><br><p>    .       ,       ?  . ,      .   ,     Ceph, vxFlex          .       -  .        ,               . </p><br><p>   9   ceph-devel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> : ,     CPU  (  Xeon'  !)   IOPS  All-NVMe   Ceph 12.2.7  bluestore. </p><br><p> ,       ,  "" Ceph    .    (  Hammer)         Ceph    ,        s3     .  ,  ScaleIO  Ceph RBD   .   Ceph,     ‚Äî      CPU.       RDMA  InfiniBand, jemalloc   . ,    10-20 ,       iops,      io, Ceph      . vxFlex          .    ‚Äî  Ceph  system time,  scaleio ‚Äî io wait. ,    bluestore,      ,    ,         -, ,     Ceph.    ScaleIO  .  ,      , Ceph           Dell EMC. </p><br><p> ,       ,         PG.        (),     IO. -   PG       IO,     ,      . ,               nearfull.       ,    . </p><br><p>  vxFlex     -   ,      .       (   ceph-volume),         ,     . </p><br><h2 id="scrub"> Scrub </h2><br><p> , .  , ,      Ceph. </p><br><p>             ,      . " "    ‚Äî   -    ,    . ,      2 TB     &gt;50%,       Ceph,     .            .  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="></a> ,       . </p><br><p>  vxFlex OS         ,    ,     .        ‚Äî bandwidth  .            .         ,        . </p><br><p> ,  , vxFlex     scrub-error. Ceph      2   . </p><br><h2 id="monitoring">  Monitoreo </h2><br><p> Luminous ‚Äî     .        .    MGR-     Zabbix              (3 ).       .   ,   ,  -         IO  ,     gc, .   ‚Äî   RGW . </p><br><p><img src="https://habrastorage.org/webt/ys/ya/xq/ysyaxqffjukjtgkv3ecnu0_q2ro.png"></p><br><p>       .     . <br>      S3,    "" : </p><br><p><img src="https://habrastorage.org/webt/ax/ge/u5/axgeu5iyyjami3qszjo97akj6oc.png"></p><br><p>   Ceph  , ,   ,       ,    . </p><br><p>  ,   eph   Graylog   GELF    .  , ,  OSD down, out, failed  .          , ,   OSD    down  ,     . </p><br><p><img src="https://habrastorage.org/webt/sd/hz/av/sdhzavl-jyjrmajz5zlsjamujzo.png"></p><br><p> - ,    OSD       heartbeat      failed (.  ).    <code>vm.zone_reclaim_mode=1</code>     NUMA. </p><br><p>     Ceph.  c vxFlex   .       : </p><br><p><img src="https://habrastorage.org/webt/hu/nz/e2/hunze2e6ucygc4anzyep2wku050.png"></p><br><p>     : </p><br><p><img src="https://habrastorage.org/webt/jv/fa/ey/jvfaeyd7ql3kalcrcabrckekuqg.png"></p><br><p>  IO    : </p><br><p><img src="https://habrastorage.org/webt/sy/vp/lq/syvplqbfmjtek_wii033vszoq9s.png"></p><br><p>              IO,      Ceph. </p><br><p>    : </p><br><p><img src="https://habrastorage.org/webt/h2/8j/ff/h28jff_jzpstuucf5wfx8vrrmay.png"></p><br><p>   Ceph,    Luminous     .   2.0,    Mimic  ,      . </p><br><h2 id="vxflex-tozhe-ne-idealen"> vxFlex    </h2><br><p>     <strong>Degraded state</strong> ,          . </p><br><p>  vxFlex ‚Äî        RH   .   7.5  , .  Ceph   RBD  cephfs ‚Äî          . </p><br><p> vxFlex       Ceph. vxFlex ‚Äî    ,   , , . </p><br><p>     16 PB,     .  eph     2 PB ‚Ä¶ </p><br><h2 id="zaklyuchenie">  Conclusi√≥n </h2><br><p>  ,  Ceph       ,     ,      ,      Ceph ‚Äî .      . </p><br><p>       ,  Ceph       " ".       ,  "  ,   ,     R&amp;D,  - ".    .       " ",  Ceph      ,    ,     . </p><br><p>      Ceph  2k18  ,    .      24/7       ( S3, ,  EBS),             ,   Ceph    .  ,    .        ‚Äî       .         /     maintenance        backfilling  ,  c Ceph   , ,       . </p><br><p>      Ceph    ?  , "     ".      Ceph.    .      ,         ,   ,  ,    ‚Ä¶ <br>    ! <br>  HEALTH_OK! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es422905/">https://habr.com/ru/post/es422905/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es422895/index.html">Google quiere matar las URL</a></li>
<li><a href="../es422897/index.html">Malo, pero el m√≠o: c√≥mo escribir CSS realmente horrible</a></li>
<li><a href="../es422899/index.html">Bajo supervisi√≥n vigilante: c√≥mo controlar las tarifas de los proveedores de alojamiento y mantener actualizado el cat√°logo de VPS</a></li>
<li><a href="../es422901/index.html">Un monitor de ritmo card√≠aco para Putin, o lo que es un Ritmer</a></li>
<li><a href="../es422903/index.html">C√≥mo y por qu√© escribimos un servicio escalable altamente cargado para 1C: Enterprise: Java, PostgreSQL, Hazelcast</a></li>
<li><a href="../es422907/index.html">Referencia r√°pida de la aspiradora robot 2018</a></li>
<li><a href="../es422909/index.html">Los 10 videos retro talk m√°s populares del 404 Festival</a></li>
<li><a href="../es422915/index.html">Estoy buscando a un senior sin una oficina y sin cookies: c√≥mo hemos organizado una b√∫squeda de empleados que son 100% remotos</a></li>
<li><a href="../es422917/index.html">No tengo boca, pero tengo que gritar. Reflexiones sobre IA y √©tica</a></li>
<li><a href="../es422919/index.html">SIP de bicicleta y conversaci√≥n telef√≥nica entre la nube</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>