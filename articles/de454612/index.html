<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üöµüèø üïô üßó Agenten f√ºr maschinelles Lernen bei Unity üë¶üèº ü•ü üêøÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dieser Artikel √ºber Agenten f√ºr maschinelles Lernen bei Unity wurde von Michael Lanham verfasst, einem technischen Innovator, aktiven Entwickler f√ºr U...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Agenten f√ºr maschinelles Lernen bei Unity</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454612/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9aa/6fe/055/9aa6fe055c20cde3642bb0f0782f62d3.jpg" alt="Bild"></div><br>  <em>Dieser Artikel √ºber Agenten f√ºr maschinelles Lernen bei Unity wurde von Michael Lanham verfasst, einem technischen Innovator, aktiven Entwickler f√ºr Unity, Berater, Manager und Autor vieler Unity-Spiele, Grafikprojekte und B√ºcher.</em> <br><br>  Unity-Entwickler haben Unterst√ºtzung f√ºr maschinelles Lernen implementiert, insbesondere f√ºr das Verst√§rkungslernen, um DRL-SDKs (Deep Reinforcement Learning) f√ºr Spiele- und Simulationsentwickler zu erstellen.  Gl√ºcklicherweise hat das Unity-Team unter der Leitung von Danny Lange erfolgreich eine zuverl√§ssige und moderne DRL-Engine implementiert, die beeindruckende Ergebnisse liefern kann.  Unity verwendet das PPO-Modell (Proximal Policy Optimization) als Grundlage f√ºr die DRL-Engine.  Dieses Modell ist viel komplexer und kann sich in einigen Aspekten unterscheiden. <br><br>  In diesem Artikel werde ich Ihnen die Tools und SDKs zum Erstellen von DRL-Agenten in Spielen und Simulationen vorstellen.  Trotz der Neuheit und Leistungsf√§higkeit dieses Tools ist es einfach zu bedienen und verf√ºgt √ºber zus√§tzliche Tools, mit denen Sie unterwegs Konzepte f√ºr maschinelles Lernen erlernen k√∂nnen.  Um mit dem Tutorial arbeiten zu k√∂nnen, m√ºssen Sie die Unity-Engine installieren. <br><a name="habracut"></a><br><h2>  Installieren Sie ML-Agents </h2><br>  In diesem Abschnitt werde ich kurz auf die Schritte eingehen, die zur Installation des ML-Agents SDK ausgef√ºhrt werden m√ºssen.  Dieses Material befindet sich noch in der Beta-Phase und kann von Version zu Version variieren.  Befolgen Sie diese Schritte: <br><br><ol><li>  Installieren Sie Git auf dem Computer.  Es funktioniert √ºber die Befehlszeile.  Git ist ein sehr beliebtes Quellcodeverwaltungssystem, und im Internet gibt es viele Ressourcen zur plattform√ºbergreifenden Installation und Verwendung von Git.  Stellen Sie nach der Installation von Git sicher, dass es funktioniert, indem Sie einen Klon eines beliebigen Repositorys erstellen. </li><li>  √ñffnen Sie eine Eingabeaufforderung oder eine regul√§re Shell.  Windows-Benutzer k√∂nnen das Anaconda-Fenster √∂ffnen. </li><li>  Wechseln Sie in den Arbeitsordner, in dem Sie Ihren neuen Code ablegen m√∂chten, und geben Sie den folgenden Befehl ein (Windows-Benutzer k√∂nnen C: \ ML-Agents ausw√§hlen): <br><br><pre>  Git-Klon https://github.com/Unity-Technologies/ml-agents </pre></li><li>  Sie klonen also das ml-Agenten-Repository auf Ihrem Computer und erstellen einen neuen Ordner mit demselben Namen.  Sie k√∂nnen dem Ordnernamen auch eine Versionsnummer hinzuf√ºgen.  Die Einheit √§ndert sich, wie fast die gesamte Welt der k√ºnstlichen Intelligenz, zumindest vorerst st√§ndig.  Dies bedeutet, dass st√§ndig neue √Ñnderungen angezeigt werden.  Zum Zeitpunkt des Schreibens klonen wir das Repository in den Ordner ml-agent.6: <br><br><pre>  Git-Klon https://github.com/Unity-Technologies/ml-agents ml-Agents.6 </pre></li><li>  Erstellen Sie eine neue virtuelle Umgebung f√ºr ml-Agenten und geben Sie Version 3.6 wie folgt an: <br><br><pre>  #Fenster 
 conda create -n ml-agent python = 3.6
 
 #Mac
 Verwenden Sie die Dokumentation f√ºr Ihre bevorzugte Umgebung </pre></li><li>  Aktivieren Sie Ihre Umgebung erneut mit Anaconda: <br><br><pre>  ml-Mittel aktivieren </pre></li><li>  Installieren Sie TensorFlow.  In Anaconda kann dies mit dem folgenden Befehl erfolgen: <br><br><pre>  pip install tensorflow == 1.7.1 </pre></li><li>  Installieren Sie Python-Pakete.  Geben Sie in Anaconda Folgendes ein: <br><br><pre><code class="plaintext hljs">cd ML-Agents #from root folder cd ml-agents or cd ml-agents.6 #for example cd ml-agents pip install -e . or pip3 install -e .</code> </pre> </li><li>  Sie installieren also alle erforderlichen Agents SDK-Pakete.  Dies kann einige Minuten dauern.  Schlie√üen Sie das Fenster nicht, es wird sich bald als n√ºtzlich erweisen. </li></ol><br>  Deshalb haben wir das Unity Python SDK f√ºr ML-Agenten installiert und konfiguriert.  Im n√§chsten Abschnitt erfahren Sie, wie Sie eine der vielen von Unity bereitgestellten Umgebungen einrichten und trainieren. <br><br><h2>  Agententraining </h2><br>  Jetzt k√∂nnen wir sofort zur Sache kommen und Beispiele untersuchen, die Deep Reinforcement Learning (DRL) verwenden.  Gl√ºcklicherweise enth√§lt das Toolkit des neuen Agenten mehrere Beispiele, um die Leistung des Motors zu demonstrieren.  √ñffnen Sie Unity oder Unity Hub und f√ºhren Sie die folgenden Schritte aus: <br><br><ol><li>  Klicken Sie oben im Dialogfeld Projekt auf die Schaltfl√§che Projekt √∂ffnen. </li><li>  Suchen Sie den UnitySDK-Projektordner und √∂ffnen Sie ihn, wie im Screenshot gezeigt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/371/706/016/3717060168f78fccd271c064f0e055ce.png"></div><br>  <i>√ñffnen Sie das Unity SDK-Projekt</i> </li><li>  Warten Sie, bis das Projekt geladen ist, und √∂ffnen Sie dann das Projektfenster unten im Editor.  Wenn ein Fenster ge√∂ffnet wird, in dem Sie aufgefordert werden, das Projekt zu aktualisieren, w√§hlen Sie Ja oder fahren Sie fort.  Derzeit ist der gesamte Agentencode abw√§rtskompatibel. </li><li>  Suchen und √∂ffnen Sie die GridWorld-Szene wie im Screenshot gezeigt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0aa/6bd/ab5/0aa6bdab5b8d07cdf414e59255862c05.png"></div><br>  <em>√ñffnen eines Beispiels einer GridWorld-Szene</em> </li><li>  W√§hlen Sie das GridAcademy-Objekt im Hierarchiefenster aus. </li><li>  Gehen Sie zum Inspektorfenster und klicken Sie neben dem Feld Gehirn auf das Symbol, um den Dialog zur Gehirnauswahl zu √∂ffnen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08c/cda/126/08ccda1263b74d131c70cff42edae92c.png"></div></li><li>  W√§hlen Sie das Gehirn des GridWorldPlayer aus.  Dieses Gehirn geh√∂rt dem Spieler, dh der Spieler (Sie) kann das Spiel steuern. </li><li>  Klicken Sie oben im Editor auf die Schaltfl√§che Wiedergabe und beobachten Sie die Umgebung.  Da das Spiel jetzt so eingerichtet ist, dass es den Spieler steuert, k√∂nnen Sie den W√ºrfel mit den WASD-Tasten bewegen.  Die Aufgabe besteht darin, den blauen W√ºrfel auf das gr√ºne + Symbol zu verschieben und dabei das rote X zu vermeiden. </li></ol><br>  Mach es dir im Spiel bequem.  Beachten Sie, dass das Spiel nur f√ºr einen bestimmten Zeitraum funktioniert und nicht rundenbasiert ist.  Im n√§chsten Abschnitt erfahren Sie, wie Sie dieses Beispiel mit dem DRL-Agenten ausf√ºhren. <br><br><h2>  Was ist im Gehirn? </h2><br>  Einer der erstaunlichen Aspekte der ML-Agents-Plattform ist die M√∂glichkeit, schnell und einfach vom Player-Management zum AI / Agent-Management zu wechseln.  Daf√ºr verwendet Unity das Konzept eines ‚ÄûGehirns‚Äú.  Das Gehirn kann entweder vom Spieler oder vom Agenten (Lernhirn) gesteuert werden.  Das Erstaunlichste ist, dass Sie das Spiel zusammenstellen und als Spieler testen und dann unter der Kontrolle eines RL-Agenten geben k√∂nnen.  Dank dessen kann jedes geschriebene Spiel mit ein wenig Aufwand mithilfe von KI gesteuert werden. <br><br>  Das Einrichten und Starten des RL-Agententrainings in Unity ist recht einfach.  Unity verwendet externes Python, um ein Modell des lernenden Gehirns zu erstellen.  Die Verwendung von Python ist sehr sinnvoll, da bereits mehrere DL-Bibliotheken (Deep Learning) darauf aufgebaut sind.  F√ºhren Sie die folgenden Schritte aus, um den Agenten in GridWorld zu schulen: <br><br><ol><li>  W√§hlen Sie GridAcademy erneut aus und w√§hlen Sie das GridWorldLearning-Gehirn im Feld Brains anstelle von GridWorldPlayer aus: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/869/fc8/b42/869fc8b42b64d8b3ccc785eb9a6e765e.png"></div><br>  <em>Wechseln zu GridWorldLearning Brain</em> </li><li>  Aktivieren Sie das Kontrollk√§stchen rechts.  Dieser einfache Parameter gibt an, dass das Gehirn extern gesteuert werden kann.  Diese Option muss aktiviert sein. </li><li>  W√§hlen Sie das trueAgent-Objekt im Hierarchiefenster aus und √§ndern Sie dann im Inspektorfenster die Brain-Eigenschaft in der Grid Agent-Komponente in das GridWorldLearning-Gehirn: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/ff3/ed2/832ff3ed2bc17c16cc5ed823d10a7156.png"></div><br>  <em>GridWorldLearning-Gehirnjob f√ºr Agent</em> </li><li>  In diesem Beispiel ben√∂tigen sowohl Academy als auch Agent dasselbe GridWorldLearning-Gehirn.  Wechseln Sie zum Fenster Anaconda oder Python und w√§hlen Sie den Ordner ML-Agents / ml-Agents. </li><li>  F√ºhren Sie den folgenden Befehl in einem Anaconda- oder Python-Fenster in der virtuellen Umgebung von ml-agent aus: <br><br><pre>  mlagents-learn config / Trainer_config.yaml --run-id = firstRun --train </pre></li><li>  Dadurch werden das Unity PPO-Schulungsmodell und ein Beispielagent mit der angegebenen Konfiguration gestartet.  Ab einem bestimmten Punkt werden Sie im Eingabeaufforderungsfenster aufgefordert, den Unity-Editor mit der geladenen Umgebung zu starten. </li><li>  Klicken Sie im Unity-Editor auf Wiedergabe, um die GridWorld-Umgebung zu starten.  Bald darauf sollten Sie Agententraining und Ausgabe im Python-Skriptfenster sehen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/583/cc2/fd9/583cc2fd94c49e4a1039d9af64312f68.png"></div><br>  <em>Ausf√ºhren von GridWorld im Lernmodus</em> </li><li>  Beachten Sie, dass das mlagents-learn-Skript ein Python-Code ist, der ein RL-Modell zum Ausf√ºhren eines Agenten erstellt.  Wie Sie der Ausgabe des Skripts entnehmen k√∂nnen, m√ºssen mehrere Parameter (Hyperparameter) konfiguriert werden. </li><li>  Lassen Sie den Agenten einige tausend Iterationen lernen und feststellen, wie schnell er lernt.  Das hier verwendete interne Modell namens PPO hat sich als sehr effektives Lernmodell f√ºr viele verschiedene Aufgaben erwiesen und eignet sich sehr gut f√ºr die Spieleentwicklung.  Mit ausreichend leistungsf√§higer Ausr√ºstung kann ein Agent idealerweise in weniger als einer Stunde lernen. </li></ol><br>  Lassen Sie den Agenten weiter lernen und erkunden Sie andere M√∂glichkeiten, um den Lernprozess des Agenten zu verfolgen, wie im n√§chsten Abschnitt dargestellt. <br><br><h2>  Lern√ºberwachung mit TensorBoard </h2><br>  Die Schulung eines Agenten mithilfe des RL-Modells oder eines DL-Modells ist oft eine entmutigende Aufgabe und erfordert Liebe zum Detail.  Gl√ºcklicherweise verf√ºgt TensorFlow √ºber eine Reihe von Diagrammtools namens TensorBoard, mit denen Sie Ihren Lernprozess √ºberwachen k√∂nnen.  F√ºhren Sie die folgenden Schritte aus, um TensorBoard zu starten: <br><br><ol><li>  √ñffnen Sie ein Anaconda- oder Python-Fenster.  Aktivieren Sie die virtuelle Umgebung der ml-Agenten.  Schlie√üen Sie nicht das Fenster, in dem das Trainingsmodell ausgef√ºhrt wird.  Wir brauchen es, um fortzufahren. </li><li>  Gehen Sie zum Ordner ML-Agents / ml-Agents und f√ºhren Sie den folgenden Befehl aus: <br><br><pre>  tensorboard --logdir = Zusammenfassungen </pre></li><li>  Deshalb starten wir TensorBoard auf unserem eigenen integrierten Webserver.  Sie k√∂nnen die Seite unter der nach dem vorherigen Befehl angezeigten URL laden. </li><li>  Geben Sie die URL f√ºr das TensorBoard wie im Fenster gezeigt ein oder geben Sie localhost: 6006 oder machinename: 6006 in den Browser ein.  Nach ungef√§hr einer Stunde sollten Sie so etwas sehen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/619/b62/964/619b62964a88d9ee4b7635073799caf8.png"></div><br>  <em>TensorBoard-Diagrammfenster</em> </li><li>  Der vorherige Screenshot zeigt Diagramme, von denen jedes einen eigenen Aspekt des Trainings zeigt.  Um zu verstehen, wie unser Agent geschult ist, m√ºssen Sie sich mit jedem dieser Diagramme befassen. Daher analysieren wir die Ergebnisse der einzelnen Abschnitte: </li></ol><br><ul><li>  Umgebung: Dieser Abschnitt zeigt, wie sich der Agent in der gesamten Umgebung manifestiert.  Unten finden Sie eine detailliertere Ansicht der Diagramme mit dem bevorzugten Trend: </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/596/956/bc15969569f3d959bc550c2ee629ac3d.png"></div><br>  <em>Ein detailliertes Bild der Grafiken des Abschnitts Umwelt</em> <br><br><ul><li>  Kumulative Belohnung: Dies ist die Gesamtbelohnung, die den Agenten maximiert.  Normalerweise ist es notwendig, dass es zunimmt, aber aus irgendeinem Grund kann es abnehmen.  Es ist immer am besten, die Belohnungen zwischen 1 und -1 zu maximieren.  Wenn die Zeitplanpr√§mien diesen Bereich √ºberschreiten, muss dies ebenfalls behoben werden. </li><li>  Episodenl√§nge: Wenn dieser Wert abnimmt, ist dies normalerweise ein gutes Zeichen.  Je k√ºrzer die Episoden, desto mehr Training.  Beachten Sie jedoch, dass sich die L√§nge der Episoden bei Bedarf erh√∂hen kann, sodass das Bild m√∂glicherweise anders ist. </li><li>  Lektion: Diese Tabelle macht deutlich, in welcher Lektion sich der Agent befindet.  Es ist f√ºr das Lernen im Lehrplan gedacht. </li><li>  Verluste: Dieser Abschnitt zeigt Diagramme, die die berechneten Verluste oder Kosten f√ºr die Police und den Wert darstellen.  Unten sehen Sie einen Screenshot dieses Abschnitts mit Pfeilen, die auf die optimalen Einstellungen zeigen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aaa/8c5/3f6/aaa8c53f67533a8d349b62d216c15a1b.png"></div><br>  <em>Verluste und bevorzugte Ausbildung</em> </li></ul><br><ul><li>  Richtlinienverlust: Dieses Diagramm bestimmt das Ausma√ü der Richtlinien√§nderung im Laufe der Zeit.  Politik ist ein Element, das Handlungen definiert, und im Allgemeinen sollte dieser Zeitplan nach unten tendieren, um zu zeigen, dass die Politik bessere Entscheidungen trifft. </li><li>  Wertverlust: Dies ist der durchschnittliche Verlust der Wertfunktion.  Im Wesentlichen wird modelliert, wie gut der Agent den Wert seines n√§chsten Zustands vorhersagt.  Dieser Wert sollte zun√§chst steigen und nach Stabilisierung der Verg√ºtung sinken. </li><li>  Politik: Um die Qualit√§t der Ma√ünahmen in PPO zu bewerten, wird das Konzept der Politik verwendet, nicht das Modell.  Der folgende Screenshot zeigt die Richtliniendiagramme und den bevorzugten Trend: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/0ce/2c0/8450ce2c0e0a95bb39e92fe698dbee7c.png"></div><br>  <i>Richtliniendiagramme und bevorzugte Trends</i> </li><li>  Entropie: Diese Grafik zeigt die Gr√∂√üe des Forschungsagenten.  Dieser Wert muss reduziert werden, da der Agent mehr √ºber die Umgebung erf√§hrt und weniger Forschung ben√∂tigt. </li><li>  Lernrate: In diesem Fall sollte dieser Wert allm√§hlich linear abnehmen. </li><li>  Wertsch√§tzung: Dies ist der Durchschnittswert, den alle Agentenstaaten besucht haben.  Um das erh√∂hte Wissen eines Agenten widerzuspiegeln, muss dieser Wert wachsen und sich dann stabilisieren. </li></ul><br>  6. Lassen Sie den Agenten bis zum Abschluss laufen und schlie√üen Sie das TensorBoard nicht. <br>  7. Kehren Sie zum Anaconda / Python-Fenster zur√ºck, in dem das Gehirn trainiert wurde, und f√ºhren Sie den folgenden Befehl aus: <br><br><pre>  mlagents-learn config / Trainer_config.yaml --run-id = secondRun --train </pre><br>  8. Sie werden erneut aufgefordert, im Editor auf Wiedergabe zu klicken.  so mach es.  Lassen Sie den Agenten mit dem Training beginnen und f√ºhren Sie mehrere Sitzungen durch.  Beobachten Sie dabei das TensorBoard-Fenster und beachten Sie, wie secondRun in Diagrammen angezeigt wird.  Sie k√∂nnen diesen Agenten bis zur Fertigstellung laufen lassen, aber Sie k√∂nnen ihn auf Wunsch stoppen. <br><br>  In fr√ºheren Versionen von ML-Agents mussten Sie zuerst die ausf√ºhrbare Unity-Datei als Lernumgebung f√ºr das Spiel erstellen und dann ausf√ºhren.  Pythons √§u√üeres Gehirn h√§tte genauso funktionieren sollen.  Diese Methode machte es sehr schwierig, Probleme im Code oder im Spiel zu debuggen.  Bei der neuen Technik wurden alle diese Schwierigkeiten beseitigt. <br><br>  Nachdem wir gesehen haben, wie einfach es ist, einen Agenten einzurichten und zu schulen, fahren wir mit dem n√§chsten Abschnitt fort, in dem wir lernen, wie Sie einen Agenten ohne das externe Python-Gehirn ausf√ºhren und direkt in Unity ausf√ºhren. <br><br><h2>  Agentenstart </h2><br>  Python-Training ist gro√üartig, aber Sie k√∂nnen es nicht in einem echten Spiel verwenden.  Im Idealfall m√∂chten wir ein TensorFlow-Diagramm erstellen und in Unity verwenden.  Gl√ºcklicherweise wurde die TensorFlowSharp-Bibliothek erstellt, mit der .NET TensorFlow-Grafiken verwenden kann.  Auf diese Weise k√∂nnen wir Offline-TFModels-Modelle erstellen und sp√§ter in das Spiel einf√ºgen.  Leider k√∂nnen wir nur trainierte Modelle verwenden, aber nicht so trainieren, zumindest noch nicht. <br><br>  Lassen Sie uns anhand des Beispiels des Diagramms, das wir gerade f√ºr die GridWorld-Umgebung trainiert haben, sehen, wie dies funktioniert.  Verwenden Sie es als inneres Gehirn in der Einheit.  Befolgen Sie die Schritte im folgenden Abschnitt, um Ihr inneres Gehirn einzurichten und zu verwenden: <br><br><ol><li>  Laden Sie hier das TFSharp-Plugin herunter </li><li>  W√§hlen Sie im Editor-Men√º die Option Assets |  Paket importieren |  Benutzerdefiniertes Paket ... </li><li>  Suchen Sie das soeben heruntergeladene Asset-Paket und laden Sie das Plugin mithilfe der Importdialoge in das Projekt. </li><li>  W√§hlen Sie im Men√º Bearbeiten |  Projekteinstellungen.  Das Fenster Einstellungen wird ge√∂ffnet (erscheint in Version 2018.3). </li><li>  Suchen Sie in den Player-Optionen nach den Zeichen "Symbole definieren" und √§ndern Sie den Text in "ENABLE_TENSORFLOW". Aktivieren Sie au√üerdem den Befehl "Unsicheren Code zulassen" (siehe Abbildung): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79d/ecf/c31/79decfc310a7f6cf38b33d1998c14c1f.png"></div><br>  <em>Setzen des Flags ENABLE_TENSORFLOW</em> </li><li>  Suchen Sie das GridWorldAcademy-Objekt im Hierarchiefenster und stellen Sie sicher, dass es Brains | verwendet  GridWorldLearning.  Deaktivieren Sie die Option Steuerung im Abschnitt Gehirn des Grid Academy-Skripts. </li><li>  Suchen Sie das GridWorldLearning-Gehirn im Ordner Assets / Examples / GridWorld / Brains und stellen Sie sicher, dass der Parameter Model im Inspector-Fenster festgelegt ist (siehe Abbildung): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/7c8/fef/fb47c8fefd97d4e855e8a2c029e4da5d.png"></div><br>  <em>Modellaufgabe f√ºr das Gehirn</em> </li><li>  GridWorldLearning sollte bereits als Modell festgelegt sein.  In diesem Beispiel verwenden wir das TFModel, das mit dem GridWorld-Beispiel geliefert wird. </li><li>  Klicken Sie auf Wiedergabe, um den Editor zu starten und zu sehen, wie der Agent den Cube verwaltet. </li></ol><br>  Wir starten jetzt die vorgefertigte Unity-Umgebung.  Im n√§chsten Abschnitt lernen wir, wie man das Gehirn benutzt, das wir im vorherigen Abschnitt trainiert haben. <br><br><h2>  Geschulte Gehirnbelastung </h2><br>  Alle Unity-Beispiele verf√ºgen √ºber vorab trainierte Gehirne, mit denen Beispiele untersucht werden k√∂nnen.  Nat√ºrlich m√∂chten wir in der Lage sein, unsere eigenen TF-Diagramme in Unity zu laden und auszuf√ºhren.  Gehen Sie folgenderma√üen vor, um ein trainiertes Diagramm zu laden: <br><br><ol><li>  Wechseln Sie in den Ordner ML-Agents / ml-agent / models / firstRun-0.  In diesem Ordner befindet sich die Datei GridWorldLearning.bytes.  Ziehen Sie diese Datei in den Ordner Project / Assets / ML-Agents / Examples / GridWorld / TFModels im Unity-Editor: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f51/955/ef5/f51955ef56aae53e7946378048b9e13e.png"></div><br>  <em>Ziehen eines Byte-Diagramms in Unity</em> </li><li>  Daher importieren wir das Diagramm als Ressource in das Unity-Projekt und benennen es in GridWorldLearning 1 um. Die Engine f√ºhrt dies aus, da das Standardmodell bereits denselben Namen hat. </li><li>  Suchen Sie GridWorldLearning im Ordner brain, w√§hlen Sie es im Inspektorfenster aus und ziehen Sie das neue Modell GridWorldLearning 1 in das Feld Modell der Parameter Brain Parameters: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/784/8a3/9b7/7848a39b79b02ffb11d968f835054295.png"></div><br>  <em>Laden des Gehirns in das Feld Graph Model</em> </li><li>  Zu diesem Zeitpunkt m√ºssen wir keine weiteren Parameter √§ndern, sondern m√ºssen besonders darauf achten, wie das Gehirn konfiguriert ist.  Im Moment reichen die Standardeinstellungen. </li><li>  Klicken Sie im Unity-Editor auf "Spielen" und sehen Sie, wie sich der Agent erfolgreich im Spiel bewegt. </li><li>  Der Erfolg des Agenten im Spiel h√§ngt von der Zeit seines Trainings ab.  Wenn Sie ihm erlauben, die Schulung abzuschlie√üen, √§hnelt der Agent einem vollst√§ndig geschulten Unity-Agenten. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de454612/">https://habr.com/ru/post/de454612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de454600/index.html">Wie wir bei Freelansim ein sicheres Gesch√§ft gemacht haben: W√§hlen Sie aus, schneiden Sie Funktionen ab, vergleichen Sie Provisionen</a></li>
<li><a href="../de454604/index.html">Generieren einer Reaktionsanwendung mit einem GraphQL-Backend in Minuten</a></li>
<li><a href="../de454606/index.html">Funktionen des Eingabemodus-Attributs f√ºr mobile Betriebssysteme und Browser</a></li>
<li><a href="../de454608/index.html">Service Level Agreement: Wir schreiben SLA f√ºr ... andere oder den Abschluss eines SLA mit einem Telekommunikationsbetreiber</a></li>
<li><a href="../de454610/index.html">Content Marketing, SEO, Tests und Umfragen: 9 Tools zur F√∂rderung eines Startups im Ausland</a></li>
<li><a href="../de454614/index.html">XXE: Externe XML-Entit√§t</a></li>
<li><a href="../de454616/index.html">Der Einsatz von KI zur Steigerung der Effizienz von Geistesarbeitern</a></li>
<li><a href="../de454618/index.html">Produktivit√§tsgrube: Wie locker unser Workflow schadet</a></li>
<li><a href="../de454620/index.html">#NoDeployFriday: hilft oder schadet?</a></li>
<li><a href="../de454622/index.html">Kreisel EVEX 910e: historisches Modell - neues Leben</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>