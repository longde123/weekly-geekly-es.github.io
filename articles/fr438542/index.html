<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü•ß üò∞ üö£üèæ Comment nous avons cr√©√© un service de recommandation pour la s√©lection de v√™tements sur les r√©seaux de neurones ‚ôêÔ∏è üë©‚Äçüè≠ üôÜüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans cet article, je veux parler de la fa√ßon dont nous avons cr√©√© un syst√®me de recherche de v√™tements similaires (plus pr√©cis√©ment des v√™tements, des...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment nous avons cr√©√© un service de recommandation pour la s√©lection de v√™tements sur les r√©seaux de neurones</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/438542/"><img src="https://habrastorage.org/webt/bh/ne/uw/bhneuwiip47lykb6jg-yigfnk3o.png" alt="image"><br><br>  Dans cet article, je veux parler de la fa√ßon dont nous avons cr√©√© un syst√®me de recherche de v√™tements similaires (plus pr√©cis√©ment des v√™tements, des chaussures et des sacs) √† partir de photographies.  Il s'agit, en termes commerciaux, d'un service de recommandation bas√© sur des r√©seaux de neurones. <br><br>  Comme la plupart des solutions informatiques modernes, nous pouvons comparer le d√©veloppement de notre syst√®me avec l'ensemble constructeur Lego, lorsque nous prenons beaucoup de petits d√©tails, des instructions et cr√©ons un mod√®le pr√™t √† l'emploi √† partir de cela.  Voici une telle instruction: quels d√©tails prendre et comment les appliquer pour que votre GPU puisse s√©lectionner des produits similaires sur une photo, vous trouverez dans cet article. <br><br>  De quelles parties est construit notre syst√®me: <br><br><ul><li>  d√©tecteur et classificateur de v√™tements, chaussures et sacs sur images; </li><li>  robot, indexeur ou module pour travailler avec des catalogues √©lectroniques de magasins; </li><li>  module de recherche d'images similaires; </li><li>  API JSON pour une interaction pratique avec n'importe quel appareil et service; </li><li>  interface Web ou application mobile pour afficher les r√©sultats. </li></ul><br>  √Ä la fin de l'article, nous d√©crirons tous les ¬´r√¢teaux¬ª sur lesquels nous avons march√© pendant le d√©veloppement et des recommandations sur la fa√ßon de les neutraliser. <br><br><h3>  √ânonc√© du probl√®me et cr√©ation de la rubrique </h3><br>  La t√¢che et le principal cas d'utilisation du syst√®me semblent assez simples et clairs: <br><br><ul><li>  l'utilisateur soumet √† l'entr√©e (par exemple via une application mobile) une photographie dans laquelle se trouvent des v√™tements et / ou des sacs et / ou des chaussures; </li><li>  le syst√®me d√©termine (d√©tecte) tous ces objets; </li><li>  trouve √† chacun d'eux les produits les plus similaires (pertinents) dans de v√©ritables boutiques en ligne; </li><li>  donne aux produits utilisateur la possibilit√© d'acc√©der √† une page de produit sp√©cifique pour achat. </li></ul><br>  En termes simples, l'objectif de notre syst√®me est de r√©pondre √† la fameuse question: "Et vous n'avez pas la m√™me chose, uniquement avec des boutons en nacre?" <br><a name="habracut"></a><br>  Avant de vous pr√©cipiter dans le pool de codage, de balisage et de formation des r√©seaux de neurones, vous devez d√©terminer assez clairement les cat√©gories qui seront √† l'int√©rieur de votre syst√®me, c'est-√†-dire les cat√©gories que le r√©seau de neurones d√©tectera.  Il est important de comprendre que plus la liste des cat√©gories est large et d√©taill√©e, plus elle est universelle, car un grand nombre de petites cat√©gories √©troites telles que la mini-robe, la robe midi, la maxi-robe peuvent toujours √™tre combin√©es avec une seule touche dans une cat√©gorie de type robe MAIS PAS vice versa.  En d'autres termes, le rubricator doit √™tre bien pens√© et compil√© au tout d√©but du projet, afin de ne pas refaire le m√™me travail 3 fois plus tard.  Nous avons compil√© le rubricator, en prenant comme base plusieurs grands magasins, tels que Lamoda.ru, Amazon.com, et avons essay√© de le rendre aussi large que possible d'une part, et aussi polyvalent que possible, d'autre part, de sorte qu'il serait plus facile d'associer des cat√©gories de d√©tecteurs √† diff√©rentes cat√©gories √† l'avenir boutiques en ligne (je vais vous en dire plus sur la fa√ßon de cr√©er ce groupe dans la section des robots et des indexeurs).  Voici un exemple de ce qui s'est pass√©. <br><br><img src="https://habrastorage.org/webt/bg/ku/zq/bgkuzqn0q-wovsd8x8aq7hlwlio.png" alt="image"><br>  <i>Exemples de cat√©gories</i> <br><br>  Dans notre catalogue, il n'y a actuellement que 205 cat√©gories: v√™tements pour femmes, v√™tements pour hommes, chaussures pour femmes, chaussures pour hommes, sacs, v√™tements pour nouveau-n√©s.  La version compl√®te de notre classificateur est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sur le lien</a> . <br><br><h3>  Indexeur ou module pour travailler avec des catalogues √©lectroniques de magasins </h3><br>  Afin de rechercher des produits similaires √† l'avenir, nous devons cr√©er une base √©tendue de ce que nous recherchons.  D'apr√®s notre exp√©rience, la qualit√© de la recherche d'images similaires d√©pend directement de la taille de la base de recherche, qui doit d√©passer au moins 100 000 images, et de pr√©f√©rence 1 million d'images.  Si vous ajoutez 1-2 petites boutiques en ligne √† la base de donn√©es, vous n'obtiendrez probablement pas de r√©sultats impressionnants simplement parce que dans 80% des cas, il n'y a rien de vraiment similaire √† l'article souhait√© dans votre catalogue. <br><br>  Donc, pour cr√©er une grande base de donn√©es d'images dont vous avez besoin pour traiter les catalogues de diverses boutiques en ligne, voici ce que ce processus comprend: <br><br><ul><li>  Vous devez d'abord trouver les flux XML des magasins en ligne, vous pouvez g√©n√©ralement les trouver soit librement disponibles sur Internet, soit en faisant la demande aupr√®s du magasin lui-m√™me, ou dans divers agr√©gateurs tels que Admitad; </li><li>  le flux est trait√© (analys√©) par un programme sp√©cial - un robot, qui t√©l√©charge toutes les images du flux, les place sur le disque dur (plus pr√©cis√©ment, sur le stockage r√©seau auquel votre serveur est connect√©), √©crit toutes les m√©ta-informations sur les marchandises dans la base de donn√©es; </li><li>  puis un autre processus est lanc√© - l'indexeur, qui calcule des vecteurs binaires √† 128 dimensions pour chaque image.  Vous pouvez combiner le robot et l'indexeur en un seul module ou programme, mais nous avons historiquement d√©velopp√© qu'il s'agissait de processus diff√©rents.  Cela est principalement d√ª au fait qu'au d√©part, nous avons calcul√© des descripteurs (hachages) pour chaque image distribu√©e sur un grand parc de machines, car il s'agissait d'un processus tr√®s gourmand en ressources.  Si vous ne travaillez qu'avec des r√©seaux de neurones, alors la 1√®re machine avec un GPU vous suffit; </li><li>  les vecteurs binaires sont √©crits dans la base de donn√©es, tous les processus sont termin√©s et le tour est jou√© - votre base de donn√©es de produits est pr√™te pour une recherche ult√©rieure; </li><li>  mais une petite astuce demeure: puisque tous les magasins ont des catalogues diff√©rents avec des cat√©gories diff√©rentes √† l'int√©rieur, alors vous devez comparer les cat√©gories de tous les flux contenus dans votre base de donn√©es avec les cat√©gories du d√©tecteur (plus pr√©cis√©ment, le classificateur) de marchandises, nous appelons cela le processus de cartographie.  Il s'agit d'une routine manuelle, mais tr√®s utile, au cours de laquelle l'op√©rateur, √©ditant manuellement un fichier XML r√©gulier, compare les cat√©gories de flux dans la base de donn√©es avec les cat√©gories du d√©tecteur.  Voici le r√©sultat: </li></ul><br><img src="https://habrastorage.org/webt/wb/je/hp/wbjehp3opuvabmnniuv8alvxwqm.png" alt="image"><br>  <i>Exemple de fichier de mappage de cat√©gorie: catalog-classifier</i> <br><br><h3>  D√©tection et classification </h3><br>  Afin de trouver quelque chose de similaire √† ce que notre ≈ìil a trouv√© sur la photo, nous devons d'abord d√©tecter ce ¬´quelque chose¬ª (c'est-√†-dire localiser et s√©lectionner l'objet).  Nous avons parcouru un long chemin dans la cr√©ation d'un d√©tecteur, √† partir de la formation de cascades OpenCV qui ne fonctionnaient pas du tout sur cette t√¢che, et se terminant par une technologie moderne pour d√©tecter et classifier <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R-FCN</a> et le classificateur bas√© sur le r√©seau neuronal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ResNet</a> . <br><br>  En tant que donn√©es utilis√©es pour la formation et les tests (les soi-disant √©chantillons de formation et de test), nous avons pris toutes sortes d'images sur Internet: <br><br><ul><li>  recherche sur les images Google / Yandex; </li><li>  ensembles de donn√©es balis√©s par des tiers; </li><li>  r√©seaux sociaux; </li><li>  sites de magazines de mode; </li><li>  Boutiques Internet de v√™tements, chaussures, sacs. </li></ul><br>  Le balisage a √©t√© effectu√© √† l'aide d'un outil samopisny, le r√©sultat du balisage a √©t√© des ensembles d'images et des fichiers * .seg, qui stockent les coordonn√©es des objets et les √©tiquettes de classe pour eux.  En moyenne, de 100 √† 200 images ont √©t√© √©tiquet√©es pour chaque cat√©gorie, le nombre total d'images dans 205 classes √©tait de 65 000. <br><br>  Une fois que les √©chantillons de formation et de test sont pr√™ts, nous avons fait une double v√©rification du balisage, en donnant toutes les images √† un autre op√©rateur.  Cela nous a permis de filtrer un grand nombre d'erreurs qui affectent fortement la qualit√© de la formation du r√©seau neuronal, c'est-√†-dire le d√©tecteur et le classificateur.  Ensuite, nous commen√ßons √† former le r√©seau neuronal √† l'aide d'outils standard et ¬´d√©collons¬ª le prochain instantan√© du r√©seau neuronal ¬´dans la chaleur du jour¬ª dans quelques jours.  En moyenne, le temps de formation du d√©tecteur et classificateur sur le volume de donn√©es de 65 000 images sur un GPU Titan X est d'environ 3 jours. <br><br>  Un r√©seau neuronal pr√™t √† l'emploi doit en quelque sorte √™tre v√©rifi√© pour la qualit√©, c'est-√†-dire pour √©valuer si la version actuelle du r√©seau est devenue meilleure que la pr√©c√©dente et de combien.  Comment nous l'avons fait: <br><br><ul><li>  l'√©chantillon de test comprenait 12 000 images et √©tait dispos√© exactement de la m√™me mani√®re que la formation; </li><li>  nous avons √©crit un petit outil qui a ex√©cut√© l'√©chantillon de test complet √† travers le d√©tecteur et compil√© un tableau de ce type (la version compl√®te du tableau est disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> ); </li><li>  ce tableau est ajout√© √† Excel dans un nouvel onglet et compar√© au pr√©c√©dent manuellement ou √† l'aide des formules Excel int√©gr√©es; </li><li>  √† la sortie, nous obtenons les indicateurs g√©n√©raux du d√©tecteur et classificateur TPR / FPR dans tout le syst√®me dans et pour chaque cat√©gorie s√©par√©ment. </li></ul><br><img src="https://habrastorage.org/webt/vq/ou/it/vqouitv0rpqps36tme91uu5ynpw.png" alt="image"><br>  <i>Exemple de tableau de rapport sur la qualit√© du d√©tecteur et classificateur</i> <br><br><h3>  Module de recherche d'images similaires </h3><br>  Apr√®s avoir d√©tect√© des articles de garde-robe sur la photo, nous d√©marrons le moteur de recherche d'images similaires, voici comment cela fonctionne: <br><br><ul><li>  pour tous les fragments d'image d√©coup√©s (biens d√©tect√©s), les vecteurs de caract√©ristiques binaires du r√©seau neuronal 128 bits sont calcul√©s en forme et en couleur (d'o√π ils viennent, voir ci-dessous); </li><li>  les m√™mes vecteurs calcul√©s plus t√¥t au stade de l'indexation pour toutes les images de marchandises stock√©es dans la base de donn√©es sont d√©j√† charg√©s dans la m√©moire RAM de l'ordinateur (car pour rechercher des similaires, il sera n√©cessaire de faire un grand nombre de recherches et de comparaisons par paires, nous avons charg√© la base de donn√©es enti√®re imm√©diatement en m√©moire, ce qui nous a permis d'augmenter la vitesse de recherche est des dizaines de fois, tandis que la base d'environ 100 000 produits ne tient pas plus de 2 √† 3 Go de RAM); </li><li>  les coefficients de recherche pour cette cat√©gorie proviennent de l'interface ou de propri√©t√©s cod√©es en dur, par exemple, dans la cat√©gorie ¬´robe¬ª, nous recherchons plus en couleur qu'en forme (par exemple, recherche de forme en 8 √† 2 couleurs), et dans la cat√©gorie ¬´chaussures √† talons hauts¬ª nous regardons 1-to-1 form-color puisque la forme et la couleur sont √©galement importantes ici; </li><li>  En outre, les vecteurs pour le recadrage (fragments) de l'image d'entr√©e sont compar√©s par paires avec l'image de la base de donn√©es, en tenant compte des coefficients (la distance de Hamming entre les vecteurs est compar√©e); </li><li>  en cons√©quence, un tableau de produits similaires √† partir de la base de donn√©es est form√© pour chaque fragment de produit coup√©, et un poids est attribu√© √† chaque produit (selon une formule simple, en tenant compte de la normalisation, de sorte que tous les poids se situent dans la plage de 0 √† 1) pour la possibilit√© de sortie vers l'interface, ainsi que pour d'autres tri; </li><li>  un tableau de produits similaires est affich√© dans l'interface via le web-JSON-API. </li></ul><br>  Les r√©seaux de neurones pour la formation de vecteurs de r√©seau de neurones en forme et en couleur sont form√©s comme suit. <br><br><ol><li>  Pour former le r√©seau neuronal en forme, nous prenons toutes les images √©tiquet√©es, d√©coupons les fragments selon le balisage et les r√©partissons dans des dossiers selon la classe: c'est-√†-dire tous les pulls dans un dossier, tous les T-shirts dans un autre et toutes les chaussures √† talons hauts dans le troisi√®me, etc. d.  Ensuite, nous formons un classificateur ordinaire bas√© sur cet √©chantillon.  Ainsi, nous ¬´expliquons¬ª en quelque sorte au r√©seau neuronal notre compr√©hension de la forme de l'objet. </li><li>  Pour former le r√©seau neuronal en couleur, nous prenons toutes les images balis√©es, d√©coupons les fragments selon le balisage et les r√©partissons dans des dossiers selon la couleur: c'est-√†-dire que nous mettons tous les T-shirts, chaussures, sacs, etc. dans le dossier ¬´vert¬ª.  couleur verte (par cons√©quent, tous les objets de couleur verte s'accumulent g√©n√©ralement dans un dossier), dans le dossier ¬´d√©pouill√©¬ª, nous mettons toutes les choses dans une bande, et dans le dossier ¬´rouge-blanc¬ª toutes les choses rouge-blanc.  Ensuite, nous formons un classifieur distinct pour ces classes, comme s'il ¬´expliquait¬ª au r√©seau neuronal sa compr√©hension de la couleur d'une chose. </li></ol><br><img src="https://habrastorage.org/webt/b8/0-/qg/b80-qglyjgkpn6trfxxdm6b55gm.png" alt="image"><br>  <i>Un exemple de marquage d'images par couleur pour obtenir des vecteurs de r√©seaux neuronaux de signes par couleur.</i> <br><br>  Fait int√©ressant, une telle technologie fonctionne bien m√™me sur des arri√®re-plans complexes, c'est-√†-dire lorsque des fragments de choses sont d√©coup√©s non pas clairement le long du contour (masque), mais le long d'un cadre rectangulaire, que le marqueur a d√©fini. <br><br>  La recherche de similaires est bas√©e sur l'extraction de vecteurs de caract√©ristiques binaires du r√©seau neuronal de cette mani√®re: la sortie de l'avant-derni√®re couche est prise, compress√©e, normalis√©e et binaris√©e.  Dans notre travail, nous avons compress√© en un vecteur 128 bits.  Vous pouvez le faire un peu diff√©remment, par exemple, comme d√©crit dans l'article de Yahoo ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Learning of Binary Hash Codes for Fast Image Retrieval</a> ¬ª, mais l'essence de tous les algorithmes est √† peu pr√®s la m√™me - des images similaires √† l'image sont recherch√©es en comparant les propri√©t√©s que le r√©seau neuronal op√®re au sein des couches. <br><br>  Initialement, en tant que technologie de recherche d'images similaires, nous avons utilis√© des hachages ou des descripteurs d'images bas√©s (calcul√©s plus pr√©cis√©ment) sur certains algorithmes math√©matiques, tels que l'op√©rateur Sobel (ou hachage de contour), l'algorithme SIFT (ou points singuliers), le tra√ßage d'un histogramme ou la comparaison du nombre d'angles dans une image .  Cette technologie a fonctionn√© et a donn√© des r√©sultats plus ou moins sains, mais les hachages ne sont en rien comparables √† la technologie de recherche d'images similaires en fonction des propri√©t√©s attribu√©es par un r√©seau de neurones.  Si vous essayez d'expliquer la diff√©rence en 2 mots, l'algorithme de comparaison d'images bas√© sur le hachage est une ¬´calculatrice¬ª qui est configur√©e pour comparer les images √† l'aide d'une formule et qui fonctionne en continu.  Une comparaison utilisant des fonctionnalit√©s d'un r√©seau de neurones est ¬´l'intelligence artificielle¬ª, form√©e par une personne pour r√©soudre un probl√®me sp√©cifique d'une certaine mani√®re.  Vous pouvez donner un exemple aussi grossier: si vous recherchez des pulls en hash √† rayures noires et blanches, vous trouverez probablement toutes les choses en noir et blanc comme des similaires.  Et si vous recherchez en utilisant un r√©seau de neurones, alors: <br><br><ul><li>  dans les premiers endroits, vous trouverez tous les pulls √† rayures noires et blanches, </li><li>  puis tous les pulls en noir et blanc </li><li>  puis tous les pulls √† rayures. </li></ul><br><h3>  API JSON pour une interaction pratique avec n'importe quel appareil et service </h3><br>  Nous avons cr√©√© une API WEB-JSON simple et pratique pour communiquer notre syst√®me avec tous les appareils et syst√®mes, ce qui, bien s√ªr, n'est pas une innovation, mais plut√¥t une bonne norme de d√©veloppement solide. <br><br><h3>  Interface Web ou application mobile pour afficher les r√©sultats </h3><br>  Pour v√©rifier visuellement les r√©sultats, ainsi que pour d√©montrer le syst√®me aux clients, nous avons d√©velopp√© des interfaces simples: <br><br><ul><li>  interface Web, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">http://demo.likethis.me/</a> </li><li>  l'application mobile est disponible <a href="">ici</a> </li></ul><br><h3>  Erreurs commises dans le projet </h3><br><ul><li>  Dans un premier temps, il est n√©cessaire de d√©finir plus clairement la t√¢che, et il s'agit, en fonction de la t√¢che, de s√©lectionner des photographies √† mettre en page.  Si vous devez rechercher des photos UGC (User Generated Content) - il s'agit d'un cas et d'exemples de mise en page.  Si vous avez besoin d'une recherche de photos √† partir de magazines sur papier glac√©, il s'agit d'un cas diff√©rent, et si vous avez besoin d'une recherche de photos o√π un grand objet est situ√© sur un fond blanc, il s'agit d'une histoire distincte et d'un √©chantillon compl√®tement diff√©rent.  Nous avons tout m√©lang√© en un seul tas, ce qui a affect√© la qualit√© du d√©tecteur et du classificateur. </li><li>  Sur les photos, vous devez toujours marquer TOUS les objets, au moins du fait que cela convient au moins √† votre t√¢che, par exemple, lorsque vous choisissez une s√©lection de garde-robe similaire, vous devez imm√©diatement marquer tous les accessoires (perles, lunettes, bracelets, etc.), la t√™te chapeaux, etc.  Parce que maintenant que nous avons un √©norme ensemble de formation, afin d'ajouter une autre cat√©gorie, nous devons redistribuer TOUTES les photos, et c'est un travail tr√®s volumineux. </li><li>  La d√©tection est probablement mieux effectu√©e avec un r√©seau de masques, la transition vers Mask-CNN et une solution moderne bas√©e sur Detectron est l'un des domaines du d√©veloppement du syst√®me. </li><li>  Ce serait bien de d√©cider imm√©diatement comment vous allez d√©terminer la qualit√© de la s√©lection d'images similaires - il y a 2 m√©thodes: ¬´√† l'≈ìil¬ª et c'est la m√©thode la plus simple et la moins ch√®re et la 2e - la m√©thode ¬´scientifique¬ª, lorsque vous collectez des donn√©es aupr√®s d '¬´experts¬ª (personnes, que je teste votre algorithme de recherche similaire) et sur la base de ces donn√©es, formez un √©chantillon de test et un catalogue sp√©cifiquement pour la recherche d'images similaires.  Cette m√©thode est bonne en th√©orie et semble assez convaincante (pour vous et pour les clients), mais en pratique, sa mise en ≈ìuvre est difficile et assez co√ªteuse. </li></ul><br><h3>  Conclusion et plans de d√©veloppement ult√©rieurs </h3><br>  Cette technologie est tout √† fait pr√™te et utilisable, elle fonctionne maintenant chez l'un de nos clients dans la boutique en ligne en tant que service de recommandation.  De plus, r√©cemment, nous avons commenc√© √† d√©velopper un syst√®me similaire dans une autre industrie (c'est-√†-dire que nous travaillons maintenant avec d'autres types de marchandises). <br><br>  Des plans imm√©diats: le transfert du r√©seau √† Mask-CNN, ainsi que le re-marquage et le re-marquage des images pour am√©liorer la qualit√© du d√©tecteur et du classifieur. <br><br>  En conclusion, je veux dire que selon nos sentiments, une technologie similaire et en g√©n√©ral les r√©seaux de neurones sont capables de r√©soudre jusqu'√† 80% des t√¢ches complexes et hautement intellectuelles que notre cerveau rencontre quotidiennement.  La seule question est de savoir qui est le premier √† mettre en ≈ìuvre une telle technologie et √† d√©charger une personne du travail de routine, lui laissant ainsi un espace de cr√©ativit√© et de d√©veloppement, ce qui est, √† notre avis, le but le plus √©lev√© de l'homme! <br><br><h3>  Les r√©f√©rences </h3><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Technologie R-FCN</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">R√©seau neuronal ResNet</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Rechercher des images similaires √† l'aide d'un r√©seau neuronal</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr438542/">https://habr.com/ru/post/fr438542/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr438530/index.html">Podcasts hipster # 1</a></li>
<li><a href="../fr438534/index.html">Modbus sur le microcontr√¥leur russe K1986BE92QI</a></li>
<li><a href="../fr438536/index.html">Sous le capot du chatbot: ce que RocketBot peut et comment cela fonctionne</a></li>
<li><a href="../fr438538/index.html">Teamlead Conf 2019 Msk: √† propos d'un autre format de communication</a></li>
<li><a href="../fr438540/index.html">Tendances de la gestion des documents et du stockage des donn√©es pour 2019</a></li>
<li><a href="../fr438544/index.html">Nous regardons des films √† la maison: 10 documents sur la construction d'un home cin√©ma et le choix de l'√©quipement</a></li>
<li><a href="../fr438546/index.html">Analyse des approches de liaison de modules dans Node.js</a></li>
<li><a href="../fr438548/index.html">Lombok, sources.jar et d√©bogage pratique</a></li>
<li><a href="../fr438550/index.html">Un autre manifeste</a></li>
<li><a href="../fr438554/index.html">Gestion de l'√©tat et des √©v√©nements entre les composants de GameObject</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>