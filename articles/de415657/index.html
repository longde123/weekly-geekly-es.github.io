<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèß üßúüèø üåæ Bildverarbeitung f√ºr den Einzelhandel. Wie man Preisschilder in einem Gesch√§ft liest ü§∏üèø ü§∑üèø üë®üèº‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bildverarbeitung ist heutzutage ein sehr hei√ües Thema. Um das Problem der Erkennung von Store-Tags mithilfe neuronaler Netze zu l√∂sen, haben wir das T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bildverarbeitung f√ºr den Einzelhandel. Wie man Preisschilder in einem Gesch√§ft liest</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sap/blog/415657/"> Bildverarbeitung ist heutzutage ein sehr hei√ües Thema.  Um das Problem der Erkennung von Store-Tags mithilfe neuronaler Netze zu l√∂sen, haben wir das TensorFlow-Framework gew√§hlt. <br><br>  In diesem Artikel wird genau erl√§utert, wie Sie damit mehrere Objekte auf demselben Ladenpreisschild lokalisieren und identifizieren sowie deren Inhalt erkennen k√∂nnen.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine √§hnliche Aufgabe zum Erkennen von IKEA-Preisschildern wurde bei Habr√© bereits</a> mit klassischen Bildverarbeitungswerkzeugen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gel√∂st,</a> die in der OpenCV-Bibliothek verf√ºgbar sind. <br><br>  Unabh√§ngig davon m√∂chte ich darauf hinweisen, dass die L√∂sung sowohl auf der SAP HANA-Plattform in Verbindung mit Tensorflow Serving als auch auf der SAP Cloud Platform funktionieren kann. <br><br>  Die Aufgabe, den Preis von Waren zu erkennen, ist relevant f√ºr K√§ufer, die Preise miteinander "fummeln" und ein Gesch√§ft f√ºr Eink√§ufe ausw√§hlen m√∂chten, und f√ºr Einzelh√§ndler - sie m√∂chten die Preise der Wettbewerber in Echtzeit kennenlernen. <br><br>  Genug der Texte - gehe zur Technik! <br><a name="habracut"></a><br>  <b>Toolkit</b> <br><br>  F√ºr die Erkennung und Klassifizierung von Bildern verwendeten wir Faltungs-Neuronale Netze, die in der TensorFlow-Bibliothek implementiert sind und √ºber die Objekterkennungs-API gesteuert werden k√∂nnen. <br>  Die TensorFlow-Objekterkennungs-API ist ein auf TensorFlow basierender Open-Source-Metaframe, der die Erstellung, Schulung und Bereitstellung von Modellen f√ºr die Objekterkennung vereinfacht. <br><br>  Nach dem Erkennen des gew√ºnschten Objekts wurde die Texterkennung mit Tesseract, einer Bibliothek zur Zeichenerkennung, durchgef√ºhrt.  Seit 2006 gilt Tesseract als eine der genauesten OCR-Bibliotheken, die in Open Source verf√ºgbar sind. <br><br>  Es ist m√∂glich, dass Sie eine Frage stellen - warum wird nicht die gesamte Verarbeitung auf TF durchgef√ºhrt?  Die Antwort ist sehr einfach - es w√ºrde erheblich mehr Zeit f√ºr die Implementierung erfordern, aber es gab sowieso nicht viel davon.  Es war einfacher, die Verarbeitungsgeschwindigkeit zu opfern und einen fertigen Prototyp zusammenzubauen, als sich mit einer selbst erstellten OCR zu besch√§ftigen. <br><br>  <b>Erstellung und Vorbereitung eines Datensatzes</b> <br><br>  Zun√§chst war es notwendig, Materialien f√ºr die Arbeit zu sammeln.  Wir haben 3 Gesch√§fte besucht und ungef√§hr 400 Fotos von verschiedenen Preisschildern auf einer Handykamera im automatischen Modus aufgenommen <br><br>  <i>Beispielfotos:</i> <br><br><img src="https://habrastorage.org/webt/bp/re/ql/bpreqlti9nccwxxkuu-ubgg5e7s.png"><br>  <i>Abb.</i>  <i>1. Beispiel eines Preisschildbildes</i> <br><br><img src="https://habrastorage.org/webt/4u/cn/9f/4ucn9fztbym8gbhpwum28rgmigg.png"><br>  <i>Abb.</i>  <i>2. Beispiel eines Preisschildbildes</i> <br><br>  Danach m√ºssen Sie alle Fotos von Preisschildern verarbeiten und markieren.  Beim Sammeln von Bildern haben wir versucht, qualitativ hochwertige Bilder (ohne Artefakte) zu sammeln: Preisschilder mit ungef√§hr demselben Format, ohne Unsch√§rfe, erhebliche Rotationen usw.  Dies wurde durchgef√ºhrt, um einen weiteren Vergleich des Inhalts auf dem realen Preisschild und seinem digitalen Bild zu erm√∂glichen.  Wenn wir das neuronale Netzwerk jedoch nur auf die verf√ºgbaren Bilder hoher Qualit√§t trainieren, f√ºhrt dies ganz nat√ºrlich dazu, dass das Vertrauen des Modells in die Identifizierung verzerrter Beispiele erheblich sinkt.  Um das neuronale Netzwerk so zu trainieren, dass es gegen solche Situationen resistent ist, haben wir das bekannte Verfahren zum Erweitern des Trainingssatzes mit verzerrten Versionen von Bildern verwendet (Augmentation).  Als Erg√§nzung zum Trainingsbeispiel haben wir Algorithmen aus der Imgaug-Bibliothek angewendet: Verschiebungen, kleine Kurven, Gau√üsche Unsch√§rfe, Rauschen.  Der Probe wurden verzerrte Bilder hinzugef√ºgt, wodurch sie um das F√ºnffache erh√∂ht wurde (von 300 auf 1.500 Bilder). <br><br>  Zum Markieren des Bildes und Ausw√§hlen von Objekten wurde das √∂ffentlich zug√§ngliche Programm LabelImg verwendet.  Sie k√∂nnen die erforderlichen Objekte im Bild mit einem Rechteck ausw√§hlen und jede Klasse dem Begrenzungsrahmen zuweisen.  Alle Koordinaten und Beschriftungen der erstellten Rahmen f√ºr jedes Foto werden in einer separaten XML-Datei gespeichert. <br><br>  Die folgenden Objekte stachen auf jedem Foto hervor: Produktpreisschild, Produktpreis, Produktname und Produktbarcode auf dem Preisschild.  In einigen Beispielen von Bildern, in denen dies logisch gerechtfertigt war, wurden die Bereiche mit √úberlappungen markiert. <br><br><img src="https://habrastorage.org/webt/bp/ib/k-/bpibk-12lzq1m0jpaf4pxs0omak.png"><br>  <i>Abb.</i>  <i>3. Ein Beispiel f√ºr ein Foto eines Paares von Preisschildern, die in LabelImg markiert sind.</i>  <i>Bereiche mit Produktbeschreibung, Preis und Barcode werden hervorgehoben.</i> <br><br><img src="https://habrastorage.org/webt/yl/nx/sl/ylnxslebkaqu778rxyomixqdvby.png"><br>  <i>Abb.</i>  <i>4. Ein Beispiel f√ºr ein Foto eines in LabelImg gekennzeichneten Preisschilds.</i>  <i>Bereiche mit Produktbeschreibung, Preis und Barcode werden hervorgehoben.</i> <br><br>  Nachdem alle Fotos verarbeitet und markiert wurden, bereiten wir den Datensatz mit der Trennung aller Fotos und Tag-Dateien in ein Trainings- und Testmuster vor.  Nehmen Sie normalerweise 80% der Trainingsprobe auf 20% der Testprobe und mischen Sie sie nach dem Zufallsprinzip. <br><br>  Als n√§chstes m√ºssen auf dem Computer, auf dem das Modell trainiert wird, alle erforderlichen Bibliotheken installiert werden.  Zun√§chst installieren wir die TensorFlow-Bibliothek f√ºr maschinelles Lernen.  Abh√§ngig vom Typ Ihres Systems m√ºssen Sie eine zus√§tzliche Bibliothek f√ºr die Datenverarbeitung auf der GPU installieren.  Installieren Sie als N√§chstes die Tensorflow-Objekterkennungs-API-Bibliothek und zus√§tzliche Bibliotheken f√ºr die Arbeit mit Bildern und Grafiken.  Unten finden Sie eine Liste der Bibliotheken, die wir in unserer Arbeit verwendet haben: <br><br>  <i>TensorFlow-GPU v1.5, CUDA v9.0, cuDNN v7.0</i> <i><br></i>  <i>Protobuf 3+, Python-tk, Pillow 1.0, lxml, tf Slim, Jupyter-Notizbuch, Matplotlib</i> <i><br></i>  <i>Tensorflow, Cython, Cocoapi;</i>  <i>Opencv-Python;</i>  <i>Pandas</i> <br><br>  Wenn alle Installationsschritte abgeschlossen sind, k√∂nnen Sie mit der Vorbereitung der Daten und der Einstellung der Lernparameter fortfahren. <br><br>  <b>Modelltraining</b> <br><br>  Um unser Problem zu l√∂sen, haben wir zwei Versionen des vorab trainierten neuronalen Netzwerks MobileNet V2 und Faster-RCNN V2 im Coco-Dataset als Bildeigenschaftsextraktoren verwendet.  Die Modelle wurden in 4 neue Klassen umgeschult: Preisschild, Produktbeschreibung, Preis, Barcode.  Als Hauptmodell haben wir uns f√ºr MobileNet V2 entschieden, ein relativ einfaches Modell, mit dem wir akzeptable Qualit√§t bei angenehmer Geschwindigkeit liefern k√∂nnen.  Mit MobileNet V2 k√∂nnen Sie die Bilderkennung auch auf einem mobilen Ger√§t implementieren. <br><br>  Zun√§chst m√ºssen Sie der Tensorflow Object Detection-API-Bibliothek die Anzahl der Beschriftungen sowie die Namen dieser Beschriftungen mitteilen. <br><br>  Das letzte, was Sie vor dem Training tun m√ºssen, ist, eine Verkn√ºpfungszuordnung zu erstellen und die Konfigurationsdatei zu bearbeiten.  Die Beschriftungszuordnung informiert das Modell und ordnet Klassennamen Klassenidentifikationsnummern f√ºr jedes Objekt zu. <br><br><img src="https://habrastorage.org/webt/xp/4x/xu/xp4xxucl6kfbkukwqwkmqrahwbk.png"><br><br>  Schlie√ülich m√ºssen Sie die Lernquellen f√ºr die Objekterkennung konfigurieren, um zu bestimmen, welches Modell und welche Parameter f√ºr das Training verwendet werden.  Dies ist der letzte Schritt vor Beginn des Trainings. <br><br><img src="https://habrastorage.org/webt/ci/of/zi/ciofzi9v6s53lb-tusznaisw5ss.png"><br><br>  Der Trainingsvorgang wird mit dem folgenden Befehl gestartet: <br><br><pre><code class="hljs powershell">python train.py -<span class="hljs-literal"><span class="hljs-literal">-logtostderr</span></span> -<span class="hljs-literal"><span class="hljs-literal">-train_dir</span></span>=training/ -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span>=training/mobilenet.config</code> </pre> <br>  Wenn alles richtig konfiguriert ist, initialisiert TensorFlow die Umschulung des neuronalen Netzwerks.  Die Initialisierung kann bis zu 30 Sekunden dauern, bevor das eigentliche Training beginnt.  Wenn das neuronale Netzwerk bei jedem Schritt umgeschult wird, wird der Wert der Algorithmusfehlerfunktion (Verlust) angezeigt.  F√ºr MobileNet V2 betr√§gt der Anfangswert der Verlustfunktion ungef√§hr 20. Das Modell sollte trainiert werden, bis die Verlustfunktion auf einen Wert von ungef√§hr 2 abf√§llt. Um den Lernprozess des neuronalen Netzwerks zu visualisieren, k√∂nnen Sie das praktische Dienstprogramm TensorBoard verwenden. <br><br><pre> <code class="hljs pgsql">: tensorboard <span class="hljs-comment"><span class="hljs-comment">--logdir=training</span></span></code> </pre> <br>  Der Befehl initialisiert die Weboberfl√§che auf dem lokalen Computer, die unter localhost: 6006 verf√ºgbar sein wird.  Nach dem Stoppen kann der Trainingsvorgang sp√§ter mithilfe von Kontrollpunkten fortgesetzt werden, die alle 5 Minuten gespeichert werden. <br><br>  <b>Erkennung von Preisschildern und deren Elementen</b> <br><br>  Wenn das Training abgeschlossen ist, besteht der letzte Schritt darin, einen neuronalen Netzwerkgraphen zu erstellen.  Dies erfolgt √ºber den Konsolenbefehl, bei dem Sie unter den Sternchen die gr√∂√üte Anzahl von cpkt-Dateien angeben m√ºssen, die im Trainingsverzeichnis vorhanden sind. <br><br><pre> <code class="hljs powershell">python export_inference_graph.py -<span class="hljs-literal"><span class="hljs-literal">-input_type</span></span> image_tensor -<span class="hljs-literal"><span class="hljs-literal">-pipeline_config_path</span></span> training/faster_rcnn_inception_v2.config -<span class="hljs-literal"><span class="hljs-literal">-trained_checkpoint_prefix</span></span> training/model.ckpt-**** -<span class="hljs-literal"><span class="hljs-literal">-output_directory</span></span> inference_graph</code> </pre> <br>  Nach diesem Vorgang ist der Objekterkennungsklassifizierer betriebsbereit.  Um die Bilderkennung zu √ºberpr√ºfen, reicht es aus, ein Skript auszuf√ºhren, das mit der Tensorflow-Objekterkennungsbibliothek geliefert wird und das zuvor trainierte Modell und Fotos zur Erkennung angibt.  Ein Standardbeispiel f√ºr Python-Skripte finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> . <br><br>  In unserem Beispiel dauert es ungef√§hr 1,5 Sekunden, um ein Foto mit dem Modell ssd mobilet auf einem einfachen Laptop zu erkennen. <br><br><img src="https://habrastorage.org/webt/nd/uw/ev/nduwevuhylmr-xtfs9wcr6nmvwa.png"><br>  <i>Abb.</i>  <i>5. Das Ergebnis der Erkennung von Bildern mit Preisschildern im Testmuster</i> <br><br><img src="https://habrastorage.org/webt/or/-4/w-/or-4w-dq93_tjn2oj4y4f0icpag.png"><br>  <i>Abb.</i>  <i>6. Das Ergebnis der Erkennung von Bildern mit Preisschildern im Testmuster</i> <br><br>  Wenn wir davon √ºberzeugt sind, dass die Preisschilder normal erkannt werden, muss das Modell lernen, Informationen aus einzelnen Elementen zu lesen: dem Preis der Waren, dem Namen der Waren und einem Strichcode.  Zu diesem Zweck stehen in Python Bibliotheken zur Erkennung von Zeichen und Barcodes in Fotografien zur Verf√ºgung - Pyzbar und Tesseract. <br><br>  Bevor Sie Zeichen und Barcodes in einem Foto erkennen, m√ºssen Sie dieses Foto in die von uns ben√∂tigten Elemente schneiden, um die Geschwindigkeit zu erh√∂hen und unn√∂tige Informationen nicht zu erkennen, die nicht im Preisschild enthalten sind.  Es ist auch notwendig, die Koordinaten von Objekten, die das Modell erkannt hat, zusammen mit ihren Klassen herauszuziehen. <br><br><img src="https://habrastorage.org/webt/hc/dk/g6/hcdkg6ngmc1l8no5paqiwm9ewga.png"><br><br>  Dann verwenden wir diese Koordinaten, um unser Foto in Teile zu schneiden, um nur den erforderlichen Bereich zu erkennen. <br><br><img src="https://habrastorage.org/webt/tq/2c/q7/tq2cq7ri8ecarbn2i0qmgssexug.png"><br><img src="https://habrastorage.org/webt/hg/vi/ss/hgvissbwiljgcity6q51rt0wgg8.png"><br><img src="https://habrastorage.org/webt/db/gb/lt/dbgbltacl6uwrhswvqlw196h7rm.png"><br>  <i>Abb.</i>  <i>7. Ein Beispiel f√ºr hervorgehobene Teile des Preisschilds</i> <br><br>  Als n√§chstes √ºbertragen wir alle ausgeschnittenen Bereiche in die Bibliotheken: Der Produktname und der Preis des Produkts werden an tesseract und der Barcode an pyzbar √ºbertragen, und wir erhalten das Erkennungsergebnis. <br><br><img src="https://habrastorage.org/webt/pn/oo/sc/pnooscbtqlhzdbbrkza9wzim_0a.png"><br><img src="https://habrastorage.org/webt/ib/gk/wb/ibgkwblzbbfmxzwie-5p4zr-4-0.png"><br>  <i>Abb.</i>  <i>8. Ein Beispiel f√ºr erkannten Inhalt ist der Preisschildbereich.</i> <br><br>  Zu diesem Zeitpunkt kann die Text- und Barcode-Erkennung Probleme verursachen, wenn das Originalbild eine niedrige Aufl√∂sung oder Unsch√§rfe aufweist.  Wenn der Preis aufgrund der gro√üen Zahlen auf dem Preisschild normal erkannt werden kann, sind der Produktname und der Barcode schlecht oder gar nicht definiert.  Zu diesem Zweck wird empfohlen, keine kleinen Fotos zur Erkennung zu verwenden und auch Bilder ohne Rauschen und starke Verzerrungen hochzuladen - beispielsweise ohne den richtigen Fokus. <br><br>  Beispiel f√ºr eine schlechte Bilderkennung: <br><br><img src="https://habrastorage.org/webt/gs/gr/qm/gsgrqms2iy2x3j0ipckqhxvd3x4.png"><br><img src="https://habrastorage.org/webt/kc/od/bn/kcodbnaech2u4gr_2qp4j1qorlk.png"><br><img src="https://habrastorage.org/webt/sg/tv/ve/sgtvvedm0i1ssvdhuzmypadxb-0.png"><br><img src="https://habrastorage.org/webt/d7/cf/kj/d7cfkjdswbwqelwrbs1hcd_3yrc.png"><br>  <i>Abb.</i>  <i>9. Ein Beispiel f√ºr hervorgehobene Teile eines unscharfen Preisschilds und erkannten Inhalts</i> <br><br>  In diesem Beispiel k√∂nnen Sie sehen, dass die Bibliothek den Namen der Waren nicht verarbeiten konnte, wenn der Preis der Waren im Bild von schlechter Qualit√§t mehr oder weniger korrekt erkannt wurde.  Und der Barcode ist √ºberhaupt nicht erkennbar. <br><br>  Der gleiche Text in guter Qualit√§t. <br><br><img src="https://habrastorage.org/webt/yr/q0/kv/yrq0kvrbtwcyj7sbg_jknpwfgyg.png"><br><img src="https://habrastorage.org/webt/wh/cj/2m/whcj2mkjm-lafollpostyctv_lg.png"><br>  <i>Abb.</i>  <i>10. Beispiel f√ºr hervorgehobene Preisschildteile und anerkannten Inhalt</i> <br><br>  <b>Schlussfolgerungen</b> <br><br>  Am Ende ist es uns gelungen, ein Modell von akzeptabler Qualit√§t mit einem geringen Prozentsatz an Fehlern und einem hohen Prozentsatz an Erkennung relevanter Objekte zu erhalten.  Schneller-RCNN Inception V2 hat eine bessere Erkennungsqualit√§t als MobileNet SSD V2, ist jedoch in der Geschwindigkeit um eine Gr√∂√üenordnung schlechter, was eine erhebliche Einschr√§nkung darstellt. <br><br>  Die erhaltene Genauigkeit der Preisschilderkennung bei einer verz√∂gerten Stichprobe von 50 Bildern betr√§gt 100%, dh alle Preisschilder wurden auf allen Fotos erfolgreich identifiziert.  Die Erkennungsgenauigkeit von Bereichen mit Barcode und Preis betrug 90%.  Die Erkennungsgenauigkeit des Textbereichs betr√§gt 85%.  Die Genauigkeit der Preisablesung betrug etwa 95% und der Text 80-85%.  Zus√§tzlich pr√§sentieren wir als Experiment das Ergebnis der Preisschilderkennung, das sich vollst√§ndig von den Preisschildern im Trainingsbeispiel unterscheidet. <br><br><img src="https://habrastorage.org/webt/_c/8d/ez/_c8dezdcd7wqlpkjycogphibwau.png"><br>  <i>Abb.</i>  <i>11. Ein Beispiel f√ºr die Erkennung atypischer Preisschilder, die nicht im Trainingssatz enthalten sind.</i> <br><br>  Wie Sie sehen k√∂nnen, sind die Modelle auch bei Preisschildern, die sich erheblich von Trainingspreisschildern unterscheiden, nicht fehlerfrei, aber auf dem Preisschild k√∂nnen wichtige Objekte erkannt werden. <br><br>  <b>Was k√∂nnte man noch tun?</b> <br><br>  1) K√ºrzlich wurde ein cooler Artikel √ºber automatische Augmentation ver√∂ffentlicht, dessen Ansatz verwendet werden kann <br>  2) Das fertig trainierte Modell kann und sollte wesentlich komprimiert werden <br>  3) Beispiele f√ºr die Ver√∂ffentlichung abgeschlossener Dienste in SCP und TFS <br><br>  <i>Bei der Vorbereitung des Prototyps und dieses Artikels wurden die folgenden Materialien verwendet:</i> <br><br>  1. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bringen Sie maschinelles Lernen (TensorFlow) mit SAP HANA in das Unternehmen</a> <br>  2. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SAP Leonardo ML Foundation - Bringen Sie Ihr eigenes Modell mit (BYOM)</a> <br>  3. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub-Repository zur Erkennung von TensorFlow-Objekten</a> <br>  4. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">IKEA Check Recognition Article</a> <br>  5. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel √ºber die Vorteile von MobileNet</a> <br>  6. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel zur TensorFlow-Objekterkennung</a> <br><br>  <i>Der Artikel wurde erstellt von:</i> <i><br></i>  <i>Sergey Abdurakipov, Dmitry Buslov, Alexey Khristenko</i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de415657/">https://habr.com/ru/post/de415657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de415645/index.html">Ein Blick auf Highload ++ aus Sibirien</a></li>
<li><a href="../de415647/index.html">Ab dem 1. Juli m√ºssen Internetdienste Nachrichten von russischen Benutzern 6 Monate lang speichern</a></li>
<li><a href="../de415649/index.html">5G vs Wi-Fi: Erwartung und Realit√§t</a></li>
<li><a href="../de415651/index.html">Komplexe organische Molek√ºle auf dem Saturn-Satelliten entdeckt</a></li>
<li><a href="../de415655/index.html">Anf√§nger oder erfahren? Wie man einen mobilen Entwickler f√ºr iOS anstellt, der wirklich wei√ü wie</a></li>
<li><a href="../de415659/index.html">Die Praxis des Arbeitens mit Threads in Node.js 10.5.0</a></li>
<li><a href="../de415661/index.html">Leistungsberichte von Mitarbeitern sind Zeitverschwendung</a></li>
<li><a href="../de415663/index.html">Seiten der Intel-Geschichte. 1101 - der erste MOS mit Siliziumverschluss</a></li>
<li><a href="../de415665/index.html">Schnelle Arbeit vor Ort als Teil der Entwicklungspipeline</a></li>
<li><a href="../de415667/index.html">Die Drake-Gleichung funktioniert nicht - und hier erfahren Sie, wie Sie sie beheben k√∂nnen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>