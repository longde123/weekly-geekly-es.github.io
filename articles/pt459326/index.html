<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎶 🆎 👨🏼‍💼 Escalonamento automático e gerenciamento de recursos no Kubernetes (relatório de revisão e vídeo) 👩🏽‍🤝‍👨🏿 📵 ☔️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 27 de abril, na conferência Strike-2019 , dentro da estrutura da seção DevOps, foi elaborado um relatório intitulado "Escalonamento automático e ge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Escalonamento automático e gerenciamento de recursos no Kubernetes (relatório de revisão e vídeo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/459326/">  Em 27 de abril, na conferência <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Strike-2019</a> , dentro da estrutura da seção DevOps, foi elaborado um relatório intitulado "Escalonamento automático e gerenciamento de recursos no Kubernetes".  Ele fala sobre como usar o K8s para garantir alta disponibilidade de aplicativos e garantir seu desempenho máximo. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  Por tradição, temos o prazer de apresentar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b>vídeo com um relatório</b></a> (44 minutos, muito mais informativo que o artigo) e o aperto principal em forma de texto.  Vamos lá! <a name="habracut"></a><br><br>  Analisaremos o tópico do relatório de acordo com as palavras e começaremos do final. <br><br><h2>  Kubernetes </h2><br>  Vamos ter contêineres Docker no host.  Porque  Para garantir repetibilidade e isolamento, que por sua vez permite uma implantação simples e boa, o CI / CD.  Temos muitas dessas máquinas com contêineres. <br><br>  O que neste caso dá ao Kubernetes? <br><br><ol><li>  Paramos de pensar nessas máquinas e começamos a trabalhar com a “nuvem”, um <b>cluster de contêineres</b> ou vagens (grupos de contêineres). </li><li>  Além disso, nem pensamos em pods individuais, mas gerenciamos mais grupos grandes.  Essas <b>primitivas de alto nível</b> permitem dizer que existe um modelo para iniciar uma certa carga de trabalho, mas o número necessário de instâncias para seu lançamento.  Se posteriormente mudarmos o modelo, todas as instâncias também serão alteradas. </li><li>  Usando a <b>API declarativa, em</b> vez de executar uma sequência de comandos específicos, descrevemos o "dispositivo mundial" (em YAML) que o Kubernetes cria.  E novamente: quando a descrição muda, sua exibição real também muda. </li></ol><br><h2>  Gerenciamento de recursos </h2><br><h3>  CPU </h3><br>  Vamos rodar nginx, php-fpm e mysql no servidor.  Esses serviços realmente terão ainda mais processos em execução, cada um dos quais requer recursos de computação: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(os números no slide são "papagaios", a necessidade abstrata de cada processo de poder computacional)</i> <br><br>  Para tornar conveniente trabalhar com isso, é lógico combinar processos em grupos (por exemplo, todos os processos nginx em um grupo "nginx").  Uma maneira simples e óbvia de fazer isso é colocar cada grupo em um contêiner: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  Para continuar, você precisa se lembrar do que é um contêiner (no Linux).  Sua aparência foi possível graças a três recursos principais do kernel, implementados por um longo tempo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">recursos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">namespaces</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cgroups</a> .  E outras tecnologias (incluindo "conchas" convenientes, como o Docker) contribuíram para o desenvolvimento futuro: <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  No contexto do relatório, estamos interessados ​​apenas em <b>cgroups</b> , porque grupos de controle fazem parte da funcionalidade de contêineres (Docker, etc.) que implementa o gerenciamento de recursos.  Os processos, unidos em grupos, como queríamos, são os grupos de controle. <br><br>  Vamos retornar aos requisitos de CPU para esses processos e agora para os grupos de processos: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(Repito que todos os números são uma expressão abstrata dos requisitos de recursos)</i> <br><br>  Ao mesmo tempo, a própria CPU possui um determinado recurso final <i>(no exemplo, é 1000)</i> , o que pode não ser suficiente para todos (a soma das necessidades de todos os grupos é 150 + 850 + 460 = 1460).  O que acontecerá neste caso? <br><br>  O kernel começa a distribuir recursos e o faz "honestamente", distribuindo a mesma quantidade de recursos para cada grupo.  Porém, no primeiro caso, existem mais do que o necessário (333&gt; 150), portanto o excesso (333-150 = 183) permanece em reserva, que também é distribuído igualmente entre outros dois contêineres: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  Como resultado: o primeiro contêiner tinha recursos suficientes, o segundo - não foi suficiente, o terceiro - não foi suficiente.  Este é o resultado do <b>planejador "honesto" no Linux</b> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CFS</a> .  Seu trabalho pode ser regulado atribuindo <b>peso a</b> cada um dos contêineres.  Por exemplo, assim: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Vejamos o caso de falta de recursos no segundo contêiner (php-fpm).  Todos os recursos de contêiner são distribuídos entre os processos igualmente.  Como resultado, o processo mestre funciona bem e todos os funcionários ficam mais lentos, recebendo menos da metade do necessário: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  É assim que o agendador do CFS funciona.  Os pesos que atribuímos aos contêineres serão chamados de <b>solicitações</b> no futuro.  Por que isso - veja abaixo. <br><br>  Vamos dar uma olhada em toda a situação do outro lado.  Como você sabe, todas as estradas levam a Roma e, no caso de um computador, à CPU.  Uma CPU, muitas tarefas - você precisa de um semáforo.  A maneira mais fácil de gerenciar recursos é “semáforo”: eles dão a um processo um tempo de acesso fixo à CPU, depois ao próximo, etc. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  Essa abordagem é chamada de <i>limitação total</i> .  Lembre-se apenas como <b>limites</b> .  No entanto, se você distribuir limites para todos os contêineres, surge um problema: o mysql estava viajando pela estrada e em algum momento sua necessidade de uma CPU foi encerrada, mas todos os outros processos foram forçados a esperar enquanto a CPU estava <b>ociosa</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Vamos voltar ao kernel do Linux e sua interação com a CPU - a imagem geral é a seguinte: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  O Cgroup possui duas configurações - na verdade, são duas "reviravoltas" simples que permitem determinar: <br><br><ol><li>  peso para o contêiner (solicitação) é <b>ações</b> ; </li><li>  uma porcentagem do tempo total da CPU para trabalhar em tarefas de contêiner (limites) é <b>cota</b> . </li></ol><br><h3>  Como medir a CPU? </h3><br>  Existem diferentes maneiras: <br><br><ol><li>  O que são <i>papagaios</i> , ninguém sabe - toda vez que você precisa concordar. </li><li>  <i>O interesse é</i> mais claro, mas relativo: 50% de um servidor com 4 núcleos e 20 núcleos são coisas completamente diferentes. </li><li>  Você pode usar os <i>pesos</i> já mencionados que o Linux conhece, mas eles também são relativos. </li><li>  A opção mais adequada é medir os recursos de computação em <i>segundos</i> .  I.e.  em segundos do tempo do processador em relação aos segundos do tempo real: eles gastaram 1 segundo do tempo do processador em 1 segundo real - este é um núcleo inteiro da CPU. </li></ol><br>  Para facilitar ainda mais, eles começaram a medir diretamente nos <i>núcleos</i> , significando o tempo da CPU em relação ao real.  Como o Linux entende pesos em vez de tempo / núcleos do processador, era necessário um mecanismo de conversão de um para o outro. <br><br>  Considere um exemplo simples com um servidor com 3 núcleos de CPU, em que três pods selecionam pesos (500, 1000 e 1500) que são facilmente convertidos nas partes correspondentes dos núcleos alocados a eles (0,5, 1 e 1,5). <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  Se você pegar um segundo servidor, onde haverá o dobro de núcleos (6), e colocar os mesmos pods lá, a distribuição de núcleos poderá ser facilmente calculada simplesmente multiplicando por 2 (1, 2 e 3, respectivamente).  Mas o ponto importante acontece quando o quarto pod aparece neste servidor, cujo peso é de 3000. Por conveniência, ele retira alguns recursos da CPU (metade dos núcleos) e o restante dos pods os recontam (metade): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernetes e recursos de CPU </h3><br>  No Kubernetes, os recursos da CPU geralmente são medidos em <b>mili-núcleos</b> , ou seja,  0,001 kernels são tomados como peso base.  <i>(A mesma coisa na terminologia Linux / cgroups é chamada compartilhamento de CPU, embora, para ser mais preciso, 1000 CPU = 1024 compartilhamentos de CPU.) O</i> K8s se certifica de não colocar mais pods no servidor do que os recursos da CPU para a soma de pesos. todos os pods. <br><br>  Como está indo isso?  Quando um servidor é adicionado a um cluster Kubernetes, ele informa quantos núcleos de CPU estão disponíveis para ele.  E ao criar um novo pod, o agendador do Kubernetes sabe quantos núcleos esse pod precisa.  Assim, o pod será definido no servidor, onde há núcleos suficientes. <br><br>  O que acontecerá se a solicitação <b>não</b> for especificada (ou seja, o pod não determina o número de kernels necessários)?  Vamos ver como o Kubernetes geralmente conta os recursos. <br><br>  O pod pode especificar solicitações (planejador CFS) e limites (lembra do semáforo?): <br><br><ul><li>  Se forem iguais, a classe de QoS garantida é atribuída ao pod.  Essa quantidade de grãos sempre disponíveis para ele é garantida. </li><li>  Se a solicitação for menor que o limite, a classe QoS <b>poderá</b> ser <b>estourada</b> .  I.e.  esperamos que o pod, por exemplo, sempre use 1 núcleo, mas esse valor não é uma limitação para ele: <i>às vezes, o</i> pod pode usar mais (quando houver recursos livres no servidor para isso). </li><li>  Também há a <b>melhor</b> classe de QoS de <b>esforço</b> - aqueles pods para os quais a solicitação não está especificada pertencem a ela.  Os recursos são dados a eles por último. </li></ul><br><h3>  A memória </h3><br>  A situação é semelhante com a memória, mas um pouco diferente - afinal, a natureza desses recursos é diferente.  Em geral, a analogia é a seguinte: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Vamos ver como as solicitações são implementadas na memória.  Deixe os pods viverem no servidor, alterando a memória consumida, até que um deles fique tão grande que a memória acabe.  Nesse caso, o assassino do OOM aparece e mata o maior processo: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  Isso nem sempre nos convém, portanto, é possível regular quais processos são importantes para nós e não devem ser eliminados.  Para fazer isso, use o parâmetro <b>oom_score_adj</b> . <br><br>  Vamos voltar às classes de QoS da CPU e fazer uma analogia com os valores oom_score_adj, que determinam as prioridades de consumo de memória para os pods: <br><br><ul><li>  O valor mais baixo oom_score_adj de um pod é -998, o que significa que esse pod deve ser eliminado em último lugar, isso é <b>garantido</b> . </li><li>  O maior - 1000 - é o <b>melhor esforço</b> , essas cápsulas são mortas antes de qualquer outra pessoa. </li><li>  Para calcular o restante dos valores ( <b>expansível</b> ), existe uma fórmula cuja essência se resume ao fato de que quanto mais o pod solicitou recursos, menor a chance de que ele seja eliminado. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  O segundo "toque" - <b>limit_in_bytes</b> - para limites.  Tudo é mais simples: simplesmente atribuímos a quantidade máxima de memória a ser emitida e aqui (ao contrário da CPU) não há dúvida em que medida (memória) é medida. <br><br><h3>  Total </h3><br>  Solicitações e <code>limits</code> são definidos para cada pod no Kubernetes - ambos os parâmetros para a CPU e para a memória: <br><br><ol><li>  com base em solicitações, o agendador Kubernetes funciona, que distribui os pods pelos servidores; </li><li>  com base em todos os parâmetros, a classe QodS do pod é determinada; </li><li>  Os pesos relativos são calculados com base em solicitações de CPU; </li><li>  Com base em solicitações de CPU, um agendador CFS está configurado; </li><li>  Com base em solicitações de memória, o OOM killer está configurado; </li><li>  Com base nos limites da CPU, um “semáforo” é configurado; </li><li>  com base nos limites de memória, um limite é definido no cgroup. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  Em geral, esta imagem responde a todas as perguntas sobre como ocorre a parte principal do gerenciamento de recursos no Kubernetes. <br><br><h2>  Escalonamento automático </h2><br><h3>  Autoescalador de cluster K8s </h3><br>  Imagine que o cluster inteiro já esteja ocupado e um novo pod deve ser criado.  Embora o pod não possa aparecer, ele fica no status <i>Pendente</i> .  Para que apareça, podemos conectar um novo servidor ao cluster ou ... colocar cluster-autoscaler, o que fará isso por nós: solicite uma máquina virtual do provedor de nuvem (por solicitação da API) e conecte-a ao cluster, após o qual o pod será adicionado . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  Esse é o dimensionamento automático do cluster Kubernetes, que funciona muito bem (em nossa experiência).  No entanto, como em outros lugares, existem algumas nuances aqui ... <br><br>  Enquanto aumentávamos o tamanho do cluster, tudo estava bem, mas o que acontece quando o cluster <b>começou a ser liberado</b> ?  O problema é que a migração de pods (para liberar hosts) é tecnicamente difícil e cara em termos de recursos.  Kubernetes tem uma abordagem completamente diferente. <br><br>  Considere um cluster de 3 servidores nos quais há implantação.  Ele tem 6 pods: agora são 2 para cada servidor.  Por alguma razão, queríamos desativar um dos servidores.  Para fazer isso, use o comando <code>kubectl drain</code> , que: <br><br><ul><li>  proíbe o envio de novos pods para este servidor; </li><li>  remova os pods existentes no servidor. </li></ul><br>  Como o Kubernetes monitora a manutenção do número de pods (6), ele os <b>recriará</b> simplesmente em outros nós, mas não no desconectado, pois já está marcado como inacessível para a colocação de novos pods.  Esta é a mecânica fundamental para o Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  No entanto, há uma nuance aqui.  Em uma situação semelhante para StatefulSet (em vez de Implantação), as ações serão diferentes.  Agora já temos um aplicativo com estado - por exemplo, três pods com MongoDB, um dos quais teve algum tipo de problema (os dados foram incorretos ou algum outro erro que impedia que o pod fosse iniciado corretamente).  E, novamente, decidimos desconectar um servidor.  O que vai acontecer? <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  O MongoDB <i>pode</i> morrer porque precisa de um quorum: para um cluster de três instalações, pelo menos duas devem funcionar.  No entanto, isso <i>não acontece</i> - graças ao <b>PodDisruptionBudget</b> .  Este parâmetro determina o número mínimo necessário de pods de trabalho.  Sabendo que um dos pods com MongoDB não está mais funcionando, e como minAvailable está definido para MongoDB em <code>minAvailable: 2</code> , o Kubernetes não permitirá que você remova o pod. <br><br>  Conclusão: para mover (e realmente recriar) pods corretamente quando o cluster for lançado, você precisará configurar o PodDisruptionBudget. <br><br><h3>  Escala horizontal </h3><br>  Considere uma situação diferente.  Há um aplicativo em execução como Implantação no Kubernetes.  O tráfego do usuário chega aos seus pods (por exemplo, existem três), e medimos um certo indicador neles (por exemplo, carga da CPU).  Quando a carga aumenta, corrigimos o cronograma e aumentamos o número de pods para distribuir solicitações. <br><br>  Hoje, no Kubernetes, você não precisa fazer isso manualmente: você pode aumentar / diminuir automaticamente o número de pods, dependendo dos valores dos indicadores de carga medidos. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  As principais perguntas aqui são o <b>que exatamente medir</b> e <b>como interpretar os</b> valores obtidos (para tomar uma decisão sobre a alteração do número de pods).  Você pode medir muito: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  Como fazer isso tecnicamente - colete métricas etc.  - Falei detalhadamente no relatório sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">monitoramento e Kubernetes</a> .  E o principal conselho para escolher os parâmetros ideais é <b>experimentar</b> ! <br><br>  Existe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um método USE</a> <i>(Utilização Saturação e Erros</i> ), cujo significado é o seguinte.  Em que base faz sentido escalar, por exemplo, php-fpm?  Com base no fato de que os trabalhadores terminam, é a <i>utilização</i> .  E se os trabalhadores terminarem e novas conexões não forem aceitas - isso é <i>saturação</i> .  Ambos os parâmetros precisam ser medidos e, dependendo dos valores, a escala deve ser realizada. <br><br><h2>  Em vez de uma conclusão </h2><br>  O relatório tem uma continuação: sobre dimensionamento vertical e sobre como escolher os recursos certos.  Falarei sobre isso em vídeos futuros em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nosso YouTube</a> - inscreva-se para não perder! <br><br><h2>  Vídeos e slides </h2><br>  Vídeo da apresentação (44 minutos): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Apresentação do relatório: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Outros relatórios do Kubernetes em nosso blog: <br><br><ul><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Expandindo e complementando o Kubernetes</a> ” <i>(Andrey Polov; 8 de abril de 2019 em Saint HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bancos de dados e Kubernetes</a> " <i>(Dmitry Stolyarov; 8 de novembro de 2018 no HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitoring and Kubernetes</a> ” <i>(Dmitry Stolyarov; 28 de maio de 2018 na RootConf)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores práticas de CI / CD com Kubernetes e GitLab</a> ” <i>(Dmitry Stolyarov; 7 de novembro de 2017 em HighLoad ++)</i> ; </li><li>  “ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nossa experiência com o Kubernetes em pequenos projetos</a> ” <i>(Dmitry Stolyarov; 6 de junho de 2017 na RootConf)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt459326/">https://habr.com/ru/post/pt459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt459314/index.html">Visualize e lide com o Hash Match Join</a></li>
<li><a href="../pt459316/index.html">Hydra 2019: transmissão gratuita do primeiro salão e um pouco sobre o que será na conferência</a></li>
<li><a href="../pt459318/index.html">TypeScript e sprints curtos. Como criamos a ferramenta de variação de entrevistas de front end</a></li>
<li><a href="../pt459320/index.html">Operador Kubernetes em Python sem estruturas e SDKs</a></li>
<li><a href="../pt459322/index.html">Editora Peter. Venda de verão</a></li>
<li><a href="../pt459328/index.html">Melhor relação custo / benefício - Mpow A5 (059)</a></li>
<li><a href="../pt459330/index.html">Bitrix para programador e gerente: amor e ódio</a></li>
<li><a href="../pt459334/index.html">YouTrack 2019.2: um banner em todo o sistema, melhorias na página da lista de tarefas, novas opções de pesquisa e muito mais</a></li>
<li><a href="../pt459336/index.html">Viva e aprenda. Parte 1. Escola e orientação profissional</a></li>
<li><a href="../pt459338/index.html">Usando o verificador como um meio de modelagem rápida de projetos RTL. Introdução ao UVM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>