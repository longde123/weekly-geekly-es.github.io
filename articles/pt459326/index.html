<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üé∂ üÜé üë®üèº‚Äçüíº Escalonamento autom√°tico e gerenciamento de recursos no Kubernetes (relat√≥rio de revis√£o e v√≠deo) üë©üèΩ‚Äçü§ù‚Äçüë®üèø üìµ ‚òîÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Em 27 de abril, na confer√™ncia Strike-2019 , dentro da estrutura da se√ß√£o DevOps, foi elaborado um relat√≥rio intitulado "Escalonamento autom√°tico e ge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Escalonamento autom√°tico e gerenciamento de recursos no Kubernetes (relat√≥rio de revis√£o e v√≠deo)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/459326/">  Em 27 de abril, na confer√™ncia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Strike-2019</a> , dentro da estrutura da se√ß√£o DevOps, foi elaborado um relat√≥rio intitulado "Escalonamento autom√°tico e gerenciamento de recursos no Kubernetes".  Ele fala sobre como usar o K8s para garantir alta disponibilidade de aplicativos e garantir seu desempenho m√°ximo. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  Por tradi√ß√£o, temos o prazer de apresentar um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><b>v√≠deo com um relat√≥rio</b></a> (44 minutos, muito mais informativo que o artigo) e o aperto principal em forma de texto.  Vamos l√°! <a name="habracut"></a><br><br>  Analisaremos o t√≥pico do relat√≥rio de acordo com as palavras e come√ßaremos do final. <br><br><h2>  Kubernetes </h2><br>  Vamos ter cont√™ineres Docker no host.  Porque  Para garantir repetibilidade e isolamento, que por sua vez permite uma implanta√ß√£o simples e boa, o CI / CD.  Temos muitas dessas m√°quinas com cont√™ineres. <br><br>  O que neste caso d√° ao Kubernetes? <br><br><ol><li>  Paramos de pensar nessas m√°quinas e come√ßamos a trabalhar com a ‚Äúnuvem‚Äù, um <b>cluster de cont√™ineres</b> ou vagens (grupos de cont√™ineres). </li><li>  Al√©m disso, nem pensamos em pods individuais, mas gerenciamos mais grupos grandes.  Essas <b>primitivas de alto n√≠vel</b> permitem dizer que existe um modelo para iniciar uma certa carga de trabalho, mas o n√∫mero necess√°rio de inst√¢ncias para seu lan√ßamento.  Se posteriormente mudarmos o modelo, todas as inst√¢ncias tamb√©m ser√£o alteradas. </li><li>  Usando a <b>API declarativa, em</b> vez de executar uma sequ√™ncia de comandos espec√≠ficos, descrevemos o "dispositivo mundial" (em YAML) que o Kubernetes cria.  E novamente: quando a descri√ß√£o muda, sua exibi√ß√£o real tamb√©m muda. </li></ol><br><h2>  Gerenciamento de recursos </h2><br><h3>  CPU </h3><br>  Vamos rodar nginx, php-fpm e mysql no servidor.  Esses servi√ßos realmente ter√£o ainda mais processos em execu√ß√£o, cada um dos quais requer recursos de computa√ß√£o: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(os n√∫meros no slide s√£o "papagaios", a necessidade abstrata de cada processo de poder computacional)</i> <br><br>  Para tornar conveniente trabalhar com isso, √© l√≥gico combinar processos em grupos (por exemplo, todos os processos nginx em um grupo "nginx").  Uma maneira simples e √≥bvia de fazer isso √© colocar cada grupo em um cont√™iner: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  Para continuar, voc√™ precisa se lembrar do que √© um cont√™iner (no Linux).  Sua apar√™ncia foi poss√≠vel gra√ßas a tr√™s recursos principais do kernel, implementados por um longo tempo: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">recursos</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">namespaces</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cgroups</a> .  E outras tecnologias (incluindo "conchas" convenientes, como o Docker) contribu√≠ram para o desenvolvimento futuro: <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  No contexto do relat√≥rio, estamos interessados ‚Äã‚Äãapenas em <b>cgroups</b> , porque grupos de controle fazem parte da funcionalidade de cont√™ineres (Docker, etc.) que implementa o gerenciamento de recursos.  Os processos, unidos em grupos, como quer√≠amos, s√£o os grupos de controle. <br><br>  Vamos retornar aos requisitos de CPU para esses processos e agora para os grupos de processos: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(Repito que todos os n√∫meros s√£o uma express√£o abstrata dos requisitos de recursos)</i> <br><br>  Ao mesmo tempo, a pr√≥pria CPU possui um determinado recurso final <i>(no exemplo, √© 1000)</i> , o que pode n√£o ser suficiente para todos (a soma das necessidades de todos os grupos √© 150 + 850 + 460 = 1460).  O que acontecer√° neste caso? <br><br>  O kernel come√ßa a distribuir recursos e o faz "honestamente", distribuindo a mesma quantidade de recursos para cada grupo.  Por√©m, no primeiro caso, existem mais do que o necess√°rio (333&gt; 150), portanto o excesso (333-150 = 183) permanece em reserva, que tamb√©m √© distribu√≠do igualmente entre outros dois cont√™ineres: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  Como resultado: o primeiro cont√™iner tinha recursos suficientes, o segundo - n√£o foi suficiente, o terceiro - n√£o foi suficiente.  Este √© o resultado do <b>planejador "honesto" no Linux</b> - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CFS</a> .  Seu trabalho pode ser regulado atribuindo <b>peso a</b> cada um dos cont√™ineres.  Por exemplo, assim: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Vejamos o caso de falta de recursos no segundo cont√™iner (php-fpm).  Todos os recursos de cont√™iner s√£o distribu√≠dos entre os processos igualmente.  Como resultado, o processo mestre funciona bem e todos os funcion√°rios ficam mais lentos, recebendo menos da metade do necess√°rio: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  √â assim que o agendador do CFS funciona.  Os pesos que atribu√≠mos aos cont√™ineres ser√£o chamados de <b>solicita√ß√µes</b> no futuro.  Por que isso - veja abaixo. <br><br>  Vamos dar uma olhada em toda a situa√ß√£o do outro lado.  Como voc√™ sabe, todas as estradas levam a Roma e, no caso de um computador, √† CPU.  Uma CPU, muitas tarefas - voc√™ precisa de um sem√°foro.  A maneira mais f√°cil de gerenciar recursos √© ‚Äúsem√°foro‚Äù: eles d√£o a um processo um tempo de acesso fixo √† CPU, depois ao pr√≥ximo, etc. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  Essa abordagem √© chamada de <i>limita√ß√£o total</i> .  Lembre-se apenas como <b>limites</b> .  No entanto, se voc√™ distribuir limites para todos os cont√™ineres, surge um problema: o mysql estava viajando pela estrada e em algum momento sua necessidade de uma CPU foi encerrada, mas todos os outros processos foram for√ßados a esperar enquanto a CPU estava <b>ociosa</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Vamos voltar ao kernel do Linux e sua intera√ß√£o com a CPU - a imagem geral √© a seguinte: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  O Cgroup possui duas configura√ß√µes - na verdade, s√£o duas "reviravoltas" simples que permitem determinar: <br><br><ol><li>  peso para o cont√™iner (solicita√ß√£o) √© <b>a√ß√µes</b> ; </li><li>  uma porcentagem do tempo total da CPU para trabalhar em tarefas de cont√™iner (limites) √© <b>cota</b> . </li></ol><br><h3>  Como medir a CPU? </h3><br>  Existem diferentes maneiras: <br><br><ol><li>  O que s√£o <i>papagaios</i> , ningu√©m sabe - toda vez que voc√™ precisa concordar. </li><li>  <i>O interesse √©</i> mais claro, mas relativo: 50% de um servidor com 4 n√∫cleos e 20 n√∫cleos s√£o coisas completamente diferentes. </li><li>  Voc√™ pode usar os <i>pesos</i> j√° mencionados que o Linux conhece, mas eles tamb√©m s√£o relativos. </li><li>  A op√ß√£o mais adequada √© medir os recursos de computa√ß√£o em <i>segundos</i> .  I.e.  em segundos do tempo do processador em rela√ß√£o aos segundos do tempo real: eles gastaram 1 segundo do tempo do processador em 1 segundo real - este √© um n√∫cleo inteiro da CPU. </li></ol><br>  Para facilitar ainda mais, eles come√ßaram a medir diretamente nos <i>n√∫cleos</i> , significando o tempo da CPU em rela√ß√£o ao real.  Como o Linux entende pesos em vez de tempo / n√∫cleos do processador, era necess√°rio um mecanismo de convers√£o de um para o outro. <br><br>  Considere um exemplo simples com um servidor com 3 n√∫cleos de CPU, em que tr√™s pods selecionam pesos (500, 1000 e 1500) que s√£o facilmente convertidos nas partes correspondentes dos n√∫cleos alocados a eles (0,5, 1 e 1,5). <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  Se voc√™ pegar um segundo servidor, onde haver√° o dobro de n√∫cleos (6), e colocar os mesmos pods l√°, a distribui√ß√£o de n√∫cleos poder√° ser facilmente calculada simplesmente multiplicando por 2 (1, 2 e 3, respectivamente).  Mas o ponto importante acontece quando o quarto pod aparece neste servidor, cujo peso √© de 3000. Por conveni√™ncia, ele retira alguns recursos da CPU (metade dos n√∫cleos) e o restante dos pods os recontam (metade): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernetes e recursos de CPU </h3><br>  No Kubernetes, os recursos da CPU geralmente s√£o medidos em <b>mili-n√∫cleos</b> , ou seja,  0,001 kernels s√£o tomados como peso base.  <i>(A mesma coisa na terminologia Linux / cgroups √© chamada compartilhamento de CPU, embora, para ser mais preciso, 1000 CPU = 1024 compartilhamentos de CPU.) O</i> K8s se certifica de n√£o colocar mais pods no servidor do que os recursos da CPU para a soma de pesos. todos os pods. <br><br>  Como est√° indo isso?  Quando um servidor √© adicionado a um cluster Kubernetes, ele informa quantos n√∫cleos de CPU est√£o dispon√≠veis para ele.  E ao criar um novo pod, o agendador do Kubernetes sabe quantos n√∫cleos esse pod precisa.  Assim, o pod ser√° definido no servidor, onde h√° n√∫cleos suficientes. <br><br>  O que acontecer√° se a solicita√ß√£o <b>n√£o</b> for especificada (ou seja, o pod n√£o determina o n√∫mero de kernels necess√°rios)?  Vamos ver como o Kubernetes geralmente conta os recursos. <br><br>  O pod pode especificar solicita√ß√µes (planejador CFS) e limites (lembra do sem√°foro?): <br><br><ul><li>  Se forem iguais, a classe de QoS garantida √© atribu√≠da ao pod.  Essa quantidade de gr√£os sempre dispon√≠veis para ele √© garantida. </li><li>  Se a solicita√ß√£o for menor que o limite, a classe QoS <b>poder√°</b> ser <b>estourada</b> .  I.e.  esperamos que o pod, por exemplo, sempre use 1 n√∫cleo, mas esse valor n√£o √© uma limita√ß√£o para ele: <i>√†s vezes, o</i> pod pode usar mais (quando houver recursos livres no servidor para isso). </li><li>  Tamb√©m h√° a <b>melhor</b> classe de QoS de <b>esfor√ßo</b> - aqueles pods para os quais a solicita√ß√£o n√£o est√° especificada pertencem a ela.  Os recursos s√£o dados a eles por √∫ltimo. </li></ul><br><h3>  A mem√≥ria </h3><br>  A situa√ß√£o √© semelhante com a mem√≥ria, mas um pouco diferente - afinal, a natureza desses recursos √© diferente.  Em geral, a analogia √© a seguinte: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Vamos ver como as solicita√ß√µes s√£o implementadas na mem√≥ria.  Deixe os pods viverem no servidor, alterando a mem√≥ria consumida, at√© que um deles fique t√£o grande que a mem√≥ria acabe.  Nesse caso, o assassino do OOM aparece e mata o maior processo: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  Isso nem sempre nos conv√©m, portanto, √© poss√≠vel regular quais processos s√£o importantes para n√≥s e n√£o devem ser eliminados.  Para fazer isso, use o par√¢metro <b>oom_score_adj</b> . <br><br>  Vamos voltar √†s classes de QoS da CPU e fazer uma analogia com os valores oom_score_adj, que determinam as prioridades de consumo de mem√≥ria para os pods: <br><br><ul><li>  O valor mais baixo oom_score_adj de um pod √© -998, o que significa que esse pod deve ser eliminado em √∫ltimo lugar, isso √© <b>garantido</b> . </li><li>  O maior - 1000 - √© o <b>melhor esfor√ßo</b> , essas c√°psulas s√£o mortas antes de qualquer outra pessoa. </li><li>  Para calcular o restante dos valores ( <b>expans√≠vel</b> ), existe uma f√≥rmula cuja ess√™ncia se resume ao fato de que quanto mais o pod solicitou recursos, menor a chance de que ele seja eliminado. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  O segundo "toque" - <b>limit_in_bytes</b> - para limites.  Tudo √© mais simples: simplesmente atribu√≠mos a quantidade m√°xima de mem√≥ria a ser emitida e aqui (ao contr√°rio da CPU) n√£o h√° d√∫vida em que medida (mem√≥ria) √© medida. <br><br><h3>  Total </h3><br>  Solicita√ß√µes e <code>limits</code> s√£o definidos para cada pod no Kubernetes - ambos os par√¢metros para a CPU e para a mem√≥ria: <br><br><ol><li>  com base em solicita√ß√µes, o agendador Kubernetes funciona, que distribui os pods pelos servidores; </li><li>  com base em todos os par√¢metros, a classe QodS do pod √© determinada; </li><li>  Os pesos relativos s√£o calculados com base em solicita√ß√µes de CPU; </li><li>  Com base em solicita√ß√µes de CPU, um agendador CFS est√° configurado; </li><li>  Com base em solicita√ß√µes de mem√≥ria, o OOM killer est√° configurado; </li><li>  Com base nos limites da CPU, um ‚Äúsem√°foro‚Äù √© configurado; </li><li>  com base nos limites de mem√≥ria, um limite √© definido no cgroup. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  Em geral, esta imagem responde a todas as perguntas sobre como ocorre a parte principal do gerenciamento de recursos no Kubernetes. <br><br><h2>  Escalonamento autom√°tico </h2><br><h3>  Autoescalador de cluster K8s </h3><br>  Imagine que o cluster inteiro j√° esteja ocupado e um novo pod deve ser criado.  Embora o pod n√£o possa aparecer, ele fica no status <i>Pendente</i> .  Para que apare√ßa, podemos conectar um novo servidor ao cluster ou ... colocar cluster-autoscaler, o que far√° isso por n√≥s: solicite uma m√°quina virtual do provedor de nuvem (por solicita√ß√£o da API) e conecte-a ao cluster, ap√≥s o qual o pod ser√° adicionado . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  Esse √© o dimensionamento autom√°tico do cluster Kubernetes, que funciona muito bem (em nossa experi√™ncia).  No entanto, como em outros lugares, existem algumas nuances aqui ... <br><br>  Enquanto aument√°vamos o tamanho do cluster, tudo estava bem, mas o que acontece quando o cluster <b>come√ßou a ser liberado</b> ?  O problema √© que a migra√ß√£o de pods (para liberar hosts) √© tecnicamente dif√≠cil e cara em termos de recursos.  Kubernetes tem uma abordagem completamente diferente. <br><br>  Considere um cluster de 3 servidores nos quais h√° implanta√ß√£o.  Ele tem 6 pods: agora s√£o 2 para cada servidor.  Por alguma raz√£o, quer√≠amos desativar um dos servidores.  Para fazer isso, use o comando <code>kubectl drain</code> , que: <br><br><ul><li>  pro√≠be o envio de novos pods para este servidor; </li><li>  remova os pods existentes no servidor. </li></ul><br>  Como o Kubernetes monitora a manuten√ß√£o do n√∫mero de pods (6), ele os <b>recriar√°</b> simplesmente em outros n√≥s, mas n√£o no desconectado, pois j√° est√° marcado como inacess√≠vel para a coloca√ß√£o de novos pods.  Esta √© a mec√¢nica fundamental para o Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  No entanto, h√° uma nuance aqui.  Em uma situa√ß√£o semelhante para StatefulSet (em vez de Implanta√ß√£o), as a√ß√µes ser√£o diferentes.  Agora j√° temos um aplicativo com estado - por exemplo, tr√™s pods com MongoDB, um dos quais teve algum tipo de problema (os dados foram incorretos ou algum outro erro que impedia que o pod fosse iniciado corretamente).  E, novamente, decidimos desconectar um servidor.  O que vai acontecer? <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  O MongoDB <i>pode</i> morrer porque precisa de um quorum: para um cluster de tr√™s instala√ß√µes, pelo menos duas devem funcionar.  No entanto, isso <i>n√£o acontece</i> - gra√ßas ao <b>PodDisruptionBudget</b> .  Este par√¢metro determina o n√∫mero m√≠nimo necess√°rio de pods de trabalho.  Sabendo que um dos pods com MongoDB n√£o est√° mais funcionando, e como minAvailable est√° definido para MongoDB em <code>minAvailable: 2</code> , o Kubernetes n√£o permitir√° que voc√™ remova o pod. <br><br>  Conclus√£o: para mover (e realmente recriar) pods corretamente quando o cluster for lan√ßado, voc√™ precisar√° configurar o PodDisruptionBudget. <br><br><h3>  Escala horizontal </h3><br>  Considere uma situa√ß√£o diferente.  H√° um aplicativo em execu√ß√£o como Implanta√ß√£o no Kubernetes.  O tr√°fego do usu√°rio chega aos seus pods (por exemplo, existem tr√™s), e medimos um certo indicador neles (por exemplo, carga da CPU).  Quando a carga aumenta, corrigimos o cronograma e aumentamos o n√∫mero de pods para distribuir solicita√ß√µes. <br><br>  Hoje, no Kubernetes, voc√™ n√£o precisa fazer isso manualmente: voc√™ pode aumentar / diminuir automaticamente o n√∫mero de pods, dependendo dos valores dos indicadores de carga medidos. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  As principais perguntas aqui s√£o o <b>que exatamente medir</b> e <b>como interpretar os</b> valores obtidos (para tomar uma decis√£o sobre a altera√ß√£o do n√∫mero de pods).  Voc√™ pode medir muito: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  Como fazer isso tecnicamente - colete m√©tricas etc.  - Falei detalhadamente no relat√≥rio sobre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">monitoramento e Kubernetes</a> .  E o principal conselho para escolher os par√¢metros ideais √© <b>experimentar</b> ! <br><br>  Existe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um m√©todo USE</a> <i>(Utiliza√ß√£o Satura√ß√£o e Erros</i> ), cujo significado √© o seguinte.  Em que base faz sentido escalar, por exemplo, php-fpm?  Com base no fato de que os trabalhadores terminam, √© a <i>utiliza√ß√£o</i> .  E se os trabalhadores terminarem e novas conex√µes n√£o forem aceitas - isso √© <i>satura√ß√£o</i> .  Ambos os par√¢metros precisam ser medidos e, dependendo dos valores, a escala deve ser realizada. <br><br><h2>  Em vez de uma conclus√£o </h2><br>  O relat√≥rio tem uma continua√ß√£o: sobre dimensionamento vertical e sobre como escolher os recursos certos.  Falarei sobre isso em v√≠deos futuros em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nosso YouTube</a> - inscreva-se para n√£o perder! <br><br><h2>  V√≠deos e slides </h2><br>  V√≠deo da apresenta√ß√£o (44 minutos): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Apresenta√ß√£o do relat√≥rio: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Outros relat√≥rios do Kubernetes em nosso blog: <br><br><ul><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Expandindo e complementando o Kubernetes</a> ‚Äù <i>(Andrey Polov; 8 de abril de 2019 em Saint HighLoad ++)</i> ; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bancos de dados e Kubernetes</a> " <i>(Dmitry Stolyarov; 8 de novembro de 2018 no HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Monitoring and Kubernetes</a> ‚Äù <i>(Dmitry Stolyarov; 28 de maio de 2018 na RootConf)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Melhores pr√°ticas de CI / CD com Kubernetes e GitLab</a> ‚Äù <i>(Dmitry Stolyarov; 7 de novembro de 2017 em HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Nossa experi√™ncia com o Kubernetes em pequenos projetos</a> ‚Äù <i>(Dmitry Stolyarov; 6 de junho de 2017 na RootConf)</i> . </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt459326/">https://habr.com/ru/post/pt459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt459314/index.html">Visualize e lide com o Hash Match Join</a></li>
<li><a href="../pt459316/index.html">Hydra 2019: transmiss√£o gratuita do primeiro sal√£o e um pouco sobre o que ser√° na confer√™ncia</a></li>
<li><a href="../pt459318/index.html">TypeScript e sprints curtos. Como criamos a ferramenta de varia√ß√£o de entrevistas de front end</a></li>
<li><a href="../pt459320/index.html">Operador Kubernetes em Python sem estruturas e SDKs</a></li>
<li><a href="../pt459322/index.html">Editora Peter. Venda de ver√£o</a></li>
<li><a href="../pt459328/index.html">Melhor rela√ß√£o custo / benef√≠cio - Mpow A5 (059)</a></li>
<li><a href="../pt459330/index.html">Bitrix para programador e gerente: amor e √≥dio</a></li>
<li><a href="../pt459334/index.html">YouTrack 2019.2: um banner em todo o sistema, melhorias na p√°gina da lista de tarefas, novas op√ß√µes de pesquisa e muito mais</a></li>
<li><a href="../pt459336/index.html">Viva e aprenda. Parte 1. Escola e orienta√ß√£o profissional</a></li>
<li><a href="../pt459338/index.html">Usando o verificador como um meio de modelagem r√°pida de projetos RTL. Introdu√ß√£o ao UVM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>