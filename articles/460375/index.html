<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÖüèΩ ü§òüèΩ üè´ Libro "Aprendizaje autom√°tico para empresas y marketing" üëÜüèº ü§õüèΩ üëåüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La ciencia de datos se est√° convirtiendo en una parte integral de cualquier actividad de marketing, y este libro es un retrato vivo de la transformaci...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Libro "Aprendizaje autom√°tico para empresas y marketing"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/460375/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><img src="https://habrastorage.org/webt/gq/td/mc/gqtdmc8joactk6gu7xrzcdr0f4i.jpeg" align="left" alt="imagen"></a>  La ciencia de datos se est√° convirtiendo en una parte integral de cualquier actividad de marketing, y este libro es un retrato vivo de la transformaci√≥n digital en marketing.  El an√°lisis de datos y los algoritmos inteligentes automatizan las tareas de marketing que requieren mucho tiempo.  El proceso de toma de decisiones se est√° volviendo no solo m√°s perfecto, sino tambi√©n m√°s r√°pido, lo cual es de gran importancia en un entorno competitivo en constante aceleraci√≥n. <br><br>  ‚ÄúEste libro es un retrato vivo de la transformaci√≥n digital en marketing.  Muestra c√≥mo la ciencia de datos se est√° convirtiendo en una parte integral de cualquier actividad de marketing.  Describe en detalle c√≥mo los enfoques basados ‚Äã‚Äãen an√°lisis de datos y algoritmos inteligentes contribuyen a la automatizaci√≥n profunda de las tareas de marketing tradicionalmente intensivas en mano de obra.  El proceso de toma de decisiones se est√° volviendo no solo m√°s avanzado, sino tambi√©n m√°s r√°pido, lo cual es importante en nuestro entorno competitivo en constante aceleraci√≥n.  Este libro debe ser le√≠do por especialistas en procesamiento de datos y especialistas en marketing, y es mejor que lo lean juntos ‚Äù.  Andrey Sebrant, Director de Marketing Estrat√©gico, Yandex. <br><a name="habracut"></a><br><h3>  Extracto  5.8.3.  Modelos de factores ocultos </h3><br>  En los algoritmos de filtrado conjunto discutidos hasta ahora, la mayor√≠a de los c√°lculos se basan en los elementos individuales de la matriz de calificaci√≥n.  Los m√©todos basados ‚Äã‚Äãen la proximidad eval√∫an las calificaciones faltantes directamente de los valores conocidos en la matriz de calificaci√≥n.  Los m√©todos basados ‚Äã‚Äãen modelos agregan una capa de abstracci√≥n en la parte superior de la matriz de calificaci√≥n, creando un modelo predictivo que captura ciertos patrones de relaciones entre usuarios y elementos, pero la capacitaci√≥n del modelo a√∫n depende en gran medida de las propiedades de la matriz de calificaci√≥n.  Como resultado, estas t√©cnicas de filtrado colaborativo generalmente enfrentan los siguientes problemas: <br><br>  La matriz de calificaci√≥n puede contener millones de usuarios, millones de elementos y miles de millones de calificaciones conocidas, lo que crea serios problemas de complejidad computacional y escalabilidad. <br><br>  La matriz de calificaci√≥n suele ser muy escasa (en la pr√°ctica, puede faltar aproximadamente el 99% de las calificaciones).  Esto afecta la estabilidad computacional de los algoritmos de recomendaci√≥n y conduce a estimaciones poco confiables cuando el usuario o elemento no tiene vecinos realmente similares.  Este problema a menudo se ve exacerbado por el hecho de que la mayor√≠a de los algoritmos b√°sicos est√°n orientados al usuario o al elemento, lo que limita su capacidad de registrar todo tipo de similitudes y relaciones disponibles en la matriz de calificaci√≥n. <br><br>  Los datos en la matriz de calificaci√≥n generalmente est√°n fuertemente correlacionados debido a las similitudes entre usuarios y elementos.  Esto significa que las se√±ales disponibles en la matriz de clasificaci√≥n no solo son escasas, sino tambi√©n redundantes, lo que contribuye a la exacerbaci√≥n del problema de escalabilidad. <br><br>  Las consideraciones anteriores indican que la matriz de calificaci√≥n original puede no ser la representaci√≥n m√°s √≥ptima de las se√±ales, y se deben considerar otras representaciones alternativas que sean m√°s adecuadas para el filtrado conjunto.  Para explorar esta idea, volvamos al punto de partida y reflexionemos un poco sobre la naturaleza de los servicios de recomendaci√≥n.  De hecho, el servicio de recomendaci√≥n puede considerarse como un algoritmo que predice clasificaciones basadas en alguna medida de similitud entre el usuario y el elemento: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/bc/6n/atbc6no-aj2vssp1mrgyctgu_oy.png" alt="imagen"></div><br>  Una forma de determinar esta medida de similitud es utilizar el enfoque de factor oculto y asignar usuarios y elementos a puntos en alg√∫n espacio k-dimensional para que cada usuario y cada elemento est√©n representados por un vector k-dimensional: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/ck/no/9eckno4ntrf2yl0hw1q2tr2irbk.png" alt="imagen"></div><br>  Los vectores deben construirse de manera que las dimensiones correspondientes p y q sean comparables entre s√≠.  En otras palabras, cada dimensi√≥n puede considerarse como un signo o concepto, es decir, puj es una medida de proximidad del usuario u y el concepto j, y qij, respectivamente, es una medida del elemento i y el concepto j.  En la pr√°ctica, estas dimensiones a menudo se interpretan como g√©neros, estilos y otros atributos que se aplican simult√°neamente a usuarios y elementos.  La similitud entre el usuario y el elemento y, en consecuencia, la calificaci√≥n se puede definir como el producto de los vectores correspondientes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k3/1w/9w/k31w9wqmwas5pfvhr8_mopo7ibg.png" alt="imagen"></div><br>  Como cada calificaci√≥n se puede descomponer en un producto de dos vectores que pertenecen a un espacio conceptual que no se observa directamente en la matriz de calificaci√≥n original, pyq se denominan factores ocultos.  El √©xito de este enfoque abstracto, por supuesto, depende completamente de c√≥mo se determinan y construyen los factores ocultos.  Para responder a esta pregunta, observamos que la expresi√≥n 5.92 se puede reescribir en forma de matriz de la siguiente manera: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2i/bp/j9/2ibpj9pmptpun2amylvxf41okjw.png" alt="imagen"></div><br>  donde P es la matriz n √ó k ensamblada a partir de los vectores p, y Q es la matriz m √ó k ensamblada a partir de los vectores q, como se muestra en la Fig.  5.13.  El objetivo principal de un sistema de filtrado conjunto suele ser minimizar los errores de predicci√≥n de la calificaci√≥n, lo que le permite determinar directamente el problema de optimizaci√≥n en relaci√≥n con la matriz de factores ocultos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9h/nm/mx/9hnmmxregnvn9snp9empxqf91qk.png" alt="imagen"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z-/vl/nu/z-vlnuz-v9git5dmm0nj79etl5g.png" alt="imagen"></div><br>  Suponiendo que el n√∫mero de dimensiones ocultas k es fijo yk ‚â§ ny k ‚â§ m, el problema de optimizaci√≥n 5.94 se reduce al problema de aproximaci√≥n de bajo rango, que consideramos en el Cap√≠tulo 2. Para demostrar el enfoque de la soluci√≥n, supongamos por un momento que la matriz de calificaci√≥n est√° completa.  En este caso, el problema de optimizaci√≥n tiene una soluci√≥n anal√≠tica en t√©rminos de la Descomposici√≥n de valor singular (SVD) de la matriz de calificaci√≥n.  En particular, utilizando el algoritmo SVD est√°ndar, la matriz se puede descomponer en el producto de tres matrices: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7z/ic/x0/7zicx0iwp156hctp6t7axd5bzu4.png" alt="imagen"></div><br>  donde U es la matriz n √ó n ortonormalizada por columnas, Œ£ es la matriz diagonal n √ó m, y V es la matriz m √ó m ortonormalizada por columnas.  Se puede obtener una soluci√≥n √≥ptima al problema 5.94 en t√©rminos de estos factores, truncados a las k dimensiones m√°s significativas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/i-/os/mwi-oskhgcs-ehf_bhepytyo_os.png" alt="imagen"></div><br>  En consecuencia, los factores ocultos que son √≥ptimos en t√©rminos de precisi√≥n de predicci√≥n se pueden obtener por descomposici√≥n singular, como se muestra a continuaci√≥n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uc/ih/yl/ucihyltxuqye29vkocknsbzyguw.png" alt="imagen"></div><br>  Este modelo de factor oculto basado en SVD ayuda a resolver los problemas de co-filtrado descritos al comienzo de esta secci√≥n.  Primero, reemplaza la gran matriz de clasificaci√≥n n √ó m con matrices de factores n √ó k y m √ó k, que generalmente son mucho m√°s peque√±as, porque en la pr√°ctica el n√∫mero √≥ptimo de dimensiones ocultas k es a menudo peque√±o.  Por ejemplo, hay un caso en el que la matriz de calificaci√≥n con 500,000 usuarios y 17,000 elementos pudo aproximarse bastante bien usando 40 mediciones [Funk, 2016].  Adem√°s, SVD elimina la correlaci√≥n en la matriz de calificaci√≥n: las matrices de factores latentes definidas por 5.97 son ortonormales en columnas, es decir, las dimensiones ocultas no est√°n correlacionadas.  Si, lo cual suele ser cierto en la pr√°ctica, SVD tambi√©n resuelve el problema de la escasez, porque la se√±al presente en la matriz de clasificaci√≥n original se concentra efectivamente (recuerde que seleccionamos k dimensiones con la energ√≠a de se√±al m√°s alta), y la matriz de factores ocultos no es escasa.  La figura 5.14 ilustra esta propiedad.  El algoritmo de proximidad basado en el usuario (5.14, a) contrae vectores de calificaci√≥n dispersos para un elemento dado y un usuario determinado para obtener una puntuaci√≥n de calificaci√≥n.  El modelo de factor oculto (5.14, b), por el contrario, estima la clasificaci√≥n por convoluci√≥n de dos vectores de dimensi√≥n reducida y con una densidad de energ√≠a m√°s alta. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/te/fm/v3tefmto-k2yf54og0xn4rpbxlq.png" alt="imagen"></div><br>  El enfoque que se acaba de describir parece una soluci√≥n coherente al problema de los factores ocultos, pero de hecho tiene un serio inconveniente debido al supuesto de que la matriz de calificaci√≥n est√° completa.  Si la matriz de calificaci√≥n es escasa, lo cual es casi siempre el caso, el algoritmo SVD est√°ndar no se puede aplicar directamente, ya que no puede procesar elementos faltantes (indefinidos).  La soluci√≥n m√°s simple en este caso es completar las calificaciones faltantes con alg√∫n valor predeterminado, pero esto puede conducir a un sesgo grave en el pron√≥stico.  Adem√°s, es computacionalmente ineficiente porque la complejidad computacional de dicha soluci√≥n es igual a la complejidad SVD para la matriz n √ó m completa, mientras que es deseable tener un m√©todo con complejidad proporcional al n√∫mero de clasificaciones conocidas.  Estos problemas pueden resolverse utilizando los m√©todos alternativos de descomposici√≥n descritos en las siguientes secciones. <br><br><h3>  5.8.3.1.  Descomposici√≥n ilimitada </h3><br>  El algoritmo SVD est√°ndar es una soluci√≥n anal√≠tica para el problema de aproximaci√≥n de bajo rango.  Sin embargo, este problema puede considerarse como un problema de optimizaci√≥n, y tambi√©n se le pueden aplicar m√©todos de optimizaci√≥n universal.  Uno de los enfoques m√°s simples es usar el m√©todo de descenso de gradiente para refinar iterativamente los valores de factores ocultos.  El punto de partida es la definici√≥n de la funci√≥n de costo J como el error de pron√≥stico residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g8/d_/gk/g8d_gkpksz8-d3wyg0xs9ca60a8.png" alt="imagen"></div><br>  Tenga en cuenta que esta vez no imponemos restricciones, como la ortogonalidad, en la matriz de factores ocultos.  Calculando el gradiente de la funci√≥n de costo con respecto a factores ocultos, obtenemos el siguiente resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/_l/cu/jn_lcurj48vk0kuluh8mojadgcy.png" alt="imagen"></div><br>  donde E es la matriz de error residual: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6z/wt/at/6zwtat7l4h9qoo0fq7ejzatlobi.png" alt="imagen"></div><br>  El algoritmo de descenso de gradiente minimiza la funci√≥n de costo al moverse en cada paso en la direcci√≥n negativa del gradiente.  Por lo tanto, puede encontrar factores ocultos que minimizan el error al cuadrado de la predicci√≥n de calificaci√≥n cambiando iterativamente las matrices P y Q para que converjan, de acuerdo con las siguientes expresiones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/uw/gu/f9uwguspa118fw__nuchwxu1ycy.png" alt="imagen"></div><br>  donde Œ± es la velocidad de aprendizaje.  La desventaja del m√©todo de descenso de gradiente es la necesidad de calcular la matriz completa de errores residuales y cambiar simult√°neamente todos los valores de los factores ocultos en cada iteraci√≥n.  Un enfoque alternativo, que puede ser m√°s adecuado para matrices grandes, es el descenso de gradiente estoc√°stico [Funk, 2016].  El algoritmo de descenso de gradiente estoc√°stico utiliza el hecho de que el error de pron√≥stico total J es la suma de los errores para elementos individuales de la matriz de calificaci√≥n; por lo tanto, el gradiente general J puede aproximarse por un gradiente en un punto de datos y los factores ocultos pueden cambiarse en funci√≥n de los elementos.  La implementaci√≥n completa de esta idea se muestra en el algoritmo 5.1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/le/c5/ct/lec5ctumggjq0twtb30wnamvj0g.png" alt="imagen"></div><br>  La primera etapa del algoritmo es la inicializaci√≥n de la matriz de factores ocultos.  La elecci√≥n de estos valores iniciales no es muy importante, pero en este caso, se elige una distribuci√≥n uniforme de la energ√≠a de las clasificaciones conocidas entre los factores ocultos generados aleatoriamente.  Luego, el algoritmo optimiza secuencialmente las dimensiones del concepto.  Para cada medici√≥n, recorre repetidamente todas las clasificaciones en el conjunto de entrenamiento, predice cada clasificaci√≥n utilizando los valores actuales de los factores ocultos, estima el error y corrige los valores de los factores de acuerdo con las expresiones 5.101.  La optimizaci√≥n de la medici√≥n se completa cuando se cumple la condici√≥n de convergencia, despu√©s de lo cual el algoritmo pasa a la siguiente medici√≥n. <br><br>  El algoritmo 5.1 ayuda a superar las limitaciones del m√©todo SVD est√°ndar.  Optimiza los factores ocultos al recorrer puntos de datos individuales y, por lo tanto, evita problemas con calificaciones faltantes y operaciones algebraicas con matrices gigantes.  El enfoque iterativo tambi√©n hace que el descenso de gradiente estoc√°stico sea m√°s conveniente para aplicaciones pr√°cticas que el descenso de gradiente, que modifica matrices completas usando las expresiones 5.101. <br><br><h3>  EJEMPLO 5.6 </h3><br>  De hecho, un enfoque basado en factores ocultos es un grupo completo de m√©todos de ense√±anza de representaciones que pueden identificar patrones impl√≠citos en la matriz de calificaci√≥n y representarlos expl√≠citamente en forma de conceptos.  A veces, los conceptos tienen una interpretaci√≥n completamente significativa, especialmente los de alta energ√≠a, aunque esto no significa que todos los conceptos siempre tengan un significado significativo.  Por ejemplo, aplicar el algoritmo de descomposici√≥n matricial a una base de datos de clasificaciones de pel√≠culas puede crear factores que se corresponden aproximadamente con las dimensiones psicogr√°ficas, como el melodrama, la comedia, el horror, etc. Vamos a ilustrar este fen√≥meno con un peque√±o ejemplo num√©rico que utiliza la matriz de clasificaci√≥n de la tabla.  5.3: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/6a/vu/xw6avuizjd2x8aku8k-0r89ieem.png" alt="imagen"></div><br>  Primero, reste el promedio global Œº = 2.82 de todos los elementos para centrar la matriz, y luego ejecute el algoritmo 5.1 con k = 3 mediciones ocultas y la tasa de aprendizaje Œ± = 0.01 para obtener las siguientes dos matrices de factores: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k-/rr/vd/k-rrvde1sezar3qo0e2vqfb7jh8.png" alt="imagen"></div><br>  Cada fila en estas matrices corresponde a un usuario o una pel√≠cula, y los 12 vectores de fila se muestran en la Fig.  5.15.  Tenga en cuenta que los elementos en la primera columna (el primer vector de conceptos) tienen los valores m√°s grandes, y los valores en las columnas posteriores disminuyen gradualmente.  Esto se explica por el hecho de que el primer vector de concepto captura tanta energ√≠a de se√±al como es posible capturar usando una medici√≥n, el segundo vector de concepto captura solo una parte de la energ√≠a residual, etc. Adem√°s, tenga en cuenta que el primer concepto puede interpretarse sem√°nticamente como el eje dram√°tico - pel√≠cula de acci√≥n, donde la direcci√≥n positiva corresponde al g√©nero de la pel√≠cula de acci√≥n, y la negativa - al g√©nero del drama.  Las calificaciones en este ejemplo est√°n altamente correlacionadas, por lo que se puede ver claramente que los primeros tres usuarios y las primeras tres pel√≠culas tienen grandes valores negativos en el primer concepto vectorial (pel√≠culas de drama y usuarios a quienes les gustan tales pel√≠culas), mientras que los √∫ltimos tres usuarios y los √∫ltimos tres Las pel√≠culas tienen grandes significados positivos en la misma columna (pel√≠culas de acci√≥n y usuarios que prefieren este g√©nero).  La segunda dimensi√≥n en este caso particular corresponde principalmente al sesgo del usuario o elemento, que puede interpretarse como un atributo psicogr√°fico (¬øcr√≠tica de los juicios del usuario? ¬øPopularidad de la pel√≠cula?).  Otros conceptos pueden considerarse como ruido. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z0/s6/t9/z0s6t9vbnwdtgonx0p6rswo_ufe.png" alt="imagen"></div><br>  La matriz de factores resultante no es completamente ortogonal en las columnas, pero tiende a ser ortogonal, porque esto se deduce de la optimizaci√≥n de la soluci√≥n SVD.  Esto se puede ver mirando los productos de PTP y QTQ, que est√°n cerca de las matrices diagonales: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/zr/sf/pbzrsfbqm0hjvmoqgh1xlurslnm.png" alt="imagen"></div><br>  Las matrices 5.103 son esencialmente un modelo predictivo que se puede utilizar para evaluar las clasificaciones conocidas y faltantes.  Se pueden obtener estimaciones multiplicando dos factores y sumando el promedio global: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qg/du/d2/qgdud2i8fqm_ut6doi-8jnpllq8.png" alt="imagen"></div><br>  Los resultados reproducen con precisi√≥n lo conocido y predicen las calificaciones faltantes de acuerdo con las expectativas intuitivas.  La precisi√≥n de las estimaciones se puede aumentar o disminuir cambiando el n√∫mero de mediciones, y el n√∫mero √≥ptimo de mediciones se puede determinar en la pr√°ctica mediante una verificaci√≥n cruzada y eligiendo un compromiso razonable entre la complejidad y la precisi√≥n computacional. <br><br>  ¬ªSe puede encontrar m√°s informaci√≥n sobre el libro en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web del editor</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Contenidos</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Extracto</a> <br><br>  Cup√≥n de 25% de descuento para vendedores ambulantes - <b>Machine Learning</b> <br><br>  Tras el pago de la versi√≥n en papel del libro, se env√≠a un libro electr√≥nico por correo electr√≥nico. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/460375/">https://habr.com/ru/post/460375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460361/index.html">Grandes preguntas frecuentes sobre ciberseguridad de los sistemas de informaci√≥n m√©dica</a></li>
<li><a href="../460363/index.html">7 factores faltantes en el enfoque 12 Factor App</a></li>
<li><a href="../460365/index.html">Rastreo distribuido: hicimos todo mal</a></li>
<li><a href="../460367/index.html">Ingenier√≠a del Caos: el arte de la destrucci√≥n intencional. Parte 1</a></li>
<li><a href="../460373/index.html">Under the Hood Turbo Pages: Arquitectura de la p√°gina web Tecnolog√≠a de descarga r√°pida</a></li>
<li><a href="../460377/index.html">Uso de Liquibase para administrar la estructura de la base de datos en una aplicaci√≥n Spring Boot. Parte 1</a></li>
<li><a href="../460381/index.html">¬øQu√© es la asertividad y por qu√© es necesaria?</a></li>
<li><a href="../460383/index.html">Las transiciones de pantalla en Legend of Zelda usan las funciones no documentadas de NES</a></li>
<li><a href="../460387/index.html">Gu√≠a para principiantes de SELinux</a></li>
<li><a href="../460393/index.html">Antecedentes: qu√© esperar de Fedora Silverblue</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>