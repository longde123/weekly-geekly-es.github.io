<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê≠ üï∏Ô∏è üåÅ Notes de conf√©rence ACL 2019 ü•å üë©üèª‚Äçü§ù‚Äçüë®üèº ‚Ü™Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La r√©union annuelle de l'Association for Computational Linguistics (ACL) est la principale conf√©rence sur le traitement du langage naturel. Il est org...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Notes de conf√©rence ACL 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/463241/"><img src="https://habrastorage.org/webt/to/6d/jy/to6djymeashzcthmzckiaiwlnsg.jpeg"><br><br>  La r√©union annuelle de l'Association for Computational Linguistics (ACL) est la principale conf√©rence sur le traitement du langage naturel.  Il est organis√© depuis 1962.  Apr√®s le Canada et l'Australie, elle retourne en Europe et marche √† Florence.  Ainsi, cette ann√©e, il a √©t√© plus populaire aupr√®s des chercheurs europ√©ens que EMNLP similaire. <br><br>  Cette ann√©e, 660 articles sur 2900 soumis ont √©t√© publi√©s.  Une √©norme somme.  Il n'est gu√®re possible de faire une sorte de r√©vision objective de ce qui √©tait √† la conf√©rence.  Par cons√©quent, je vais dire mes sentiments subjectifs de cet √©v√©nement. <br><a name="habracut"></a><br>  Je suis venu √† la conf√©rence pour montrer dans une session d'affiches <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre d√©cision</a> du concours Kaggle sur la r√©solution des pronoms genr√©s de Google.  Notre solution reposait largement sur l'utilisation de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mod√®les BERT</a> pr√©- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">form√©s</a> .  Et, il s'est av√©r√© que nous n'√©tions pas seuls dans ce domaine. <br><br><h2>  Bertologie </h2><br><img src="https://habrastorage.org/webt/ol/zh/ba/olzhbat3al984zylni9zvcrizqo.jpeg"><br>  Il y avait tellement d'ouvrages bas√©s sur BERT, d√©crivant ses propri√©t√©s et l'utilisant comme sous-sol, que m√™me le terme Bertologie est apparu.  En effet, les mod√®les BERT se sont av√©r√©s si efficaces que m√™me de grands groupes de recherche comparent leurs mod√®les avec le BERT. <br><br>  Ainsi, d√©but juin, des travaux sont apparus sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">XLNet</a> .  Et juste avant la conf√©rence - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ERNIE 2.0</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RoBERTa</a> <br><br><h4>  Facebook RoBERTa </h4><br>  Lorsque le mod√®le XLNet a √©t√© introduit pour la premi√®re fois, certains chercheurs ont sugg√©r√© qu'il obtenait de meilleurs r√©sultats non seulement en raison de son architecture et de ses principes de formation.  Elle a √©galement √©tudi√© sur un corps plus grand (pr√®s de 10 fois) que BERT et plus long (4 fois plus d'it√©rations). <br><br>  Les chercheurs de Facebook ont ‚Äã‚Äãmontr√© que le BERT n'a pas encore atteint son maximum.  Ils ont pr√©sent√© une approche optimis√©e de l'enseignement du mod√®le BERT - RoBERTa (approche BERT robuste et optimis√©e). <br><br>  Ne changeant rien dans l'architecture du mod√®le, ils ont chang√© la proc√©dure de formation: <br><br><ol><li>  Nous avons augment√© le corps pour l'entra√Ænement, la taille du lot, la dur√©e de la s√©quence et la dur√©e de l'entra√Ænement. </li><li>  La t√¢che de pr√©dire la phrase suivante a √©t√© supprim√©e de la formation. </li><li>  Ils ont commenc√© √† g√©n√©rer dynamiquement des jetons MASK (jetons que le mod√®le essaie de pr√©dire lors de la pr√©-formation). </li></ol><br><h4>  ERNIE 2.0 de Baidu </h4><br>  Comme tous les mod√®les r√©cents populaires (BERT, GPT, XLM, RoBERTa, XLNet), ERNIE est bas√© sur le concept d'un transformateur avec un m√©canisme d'auto-attention.  Ce qui le distingue des autres mod√®les, ce sont les concepts d'apprentissage multi-t√¢ches et d'apprentissage continu. <br><br>  ERNIE apprend sur diff√©rentes t√¢ches, mettant constamment √† jour la repr√©sentation interne de son mod√®le de langage.  Ces t√¢ches ont, comme d'autres mod√®les, des objectifs d'auto-apprentissage (auto-supervis√© et faiblement supervis√©).  Exemples de telles t√¢ches: <br><br><ul><li>  R√©cup√©rez l'ordre des mots correct dans une phrase. </li><li>  Capitalisation des mots. </li><li>  D√©finition des mots masqu√©s. </li></ul><br>  Sur ces t√¢ches, le mod√®le apprend de mani√®re s√©quentielle, revenant aux t√¢ches sur lesquelles il a √©t√© form√© pr√©c√©demment. <br><br><h4>  RoBERTa vs ERNIE </h4><br>  Dans les publications, RoBERTa et ERNIE ne sont pas compar√©s, car ils sont apparus presque simultan√©ment.  Ils sont compar√©s √† BERT et XLNet.  Mais ici, il n'est pas si facile de faire une comparaison.  Par exemple, dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">benchmark</a> populaire, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GLUE</a> XLNet est repr√©sent√© par un ensemble de mod√®les.  Et les chercheurs de Baidu sont plus int√©ress√©s √† comparer des mod√®les uniques.  En outre, √©tant donn√© que Baidu est une entreprise chinoise, ils souhaitent √©galement comparer les r√©sultats du travail avec la langue chinoise.  Plus r√©cemment, un nouveau benchmark est apparu: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SuperGLUE</a> .  Il n'y a pas encore beaucoup de solutions, mais RoBERTa est en premier lieu ici. <br><br>  Mais dans l'ensemble, RoBERTa et ERNIE fonctionnent mieux que XLNet et nettement mieux que BERT.  RoBERTa, √† son tour, fonctionne l√©g√®rement mieux que ERNIE. <br><br><h2>  Graphes de connaissances </h2><br>  De nombreux travaux ont √©t√© consacr√©s √† la combinaison de deux approches: les r√©seaux pr√©-form√©s et l'utilisation de r√®gles sous forme de graphes de connaissances (Knowledge Graphs, KG). <br><br>  Par exemple: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ERNIE: repr√©sentation linguistique am√©lior√©e avec des entit√©s informatives</a> .  Cet article met en √©vidence l'utilisation de graphiques de connaissances en plus du mod√®le de langage BERT.  Cela vous permet d'obtenir de meilleurs r√©sultats sur des t√¢ches telles que la d√©termination du type d'entit√© ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Typage d'entit√©) et classification des relations</a> . <br><br>  En g√©n√©ral, la mode de choix des noms de mod√®les par les noms des personnages de Sesame Street entra√Æne des cons√©quences amusantes.  Par exemple, cet ERNIE n'a rien √† voir avec ERNIE 2.0 de Baidu, √† propos duquel j'ai √©crit ci-dessus. <br><br><img src="https://habrastorage.org/webt/um/6y/qp/um6yqpe7esqpdixrlodndhuf_pi.jpeg"><br><br>  Un autre travail int√©ressant sur la g√©n√©ration de nouvelles connaissances: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</a> .  L'article examine la possibilit√© d'utiliser de nouvelles architectures bas√©es sur des transformateurs pour la formation de r√©seaux bas√©s sur la connaissance.  Les bases de connaissances sous une forme simplifi√©e sont de nombreux triplets: sujet, attitude, objet.  Ils ont pris deux ensembles de donn√©es de la base de connaissances: ATOMIC et ConceptNet.  Et ils ont form√© un r√©seau bas√© sur le mod√®le GPT (Generative Pre-form√©s Transformer).  Le sujet et l'attitude ont √©t√© saisis et ont essay√© de pr√©dire l'objet.  Ainsi, ils ont obtenu un mod√®le qui g√©n√®re des objets par des sujets et des relations d'entr√©e. <br><br><h2>  Mesures </h2><br>  Un autre sujet int√©ressant lors de la conf√©rence √©tait la question du choix des m√©triques.  Il est souvent difficile d'√©valuer la qualit√© d'un mod√®le dans les t√¢ches de traitement du langage naturel, ce qui ralentit les progr√®s dans ce domaine de l'apprentissage automatique. <br><br>  Dans un article sur l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©tude des m√©triques d'√©valuation de r√©capitulation dans l'</a> article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Plage de notation appropri√©e</a> , Maxim Peyar discute de l'utilisation de diverses m√©triques dans un probl√®me de r√©capitulation de texte.  Ces m√©triques ne sont pas toujours bien corr√©l√©es entre elles, ce qui interf√®re avec la comparaison objective de divers algorithmes. <br><br>  Ou voici un travail int√©ressant: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">l'√©valuation automatique des textes multi-phrases</a> .  Dans ce document, les auteurs pr√©sentent une m√©trique qui peut remplacer BLEU et ROUGE sur des t√¢ches o√π vous devez √©valuer des textes de plusieurs phrases. <br><br>  La m√©trique BLEU peut √™tre repr√©sent√©e en tant que pr√©cision - combien de mots (ou n-grammes) de la r√©ponse du mod√®le sont contenus dans la cible.  ROUGE is Recall - combien de mots (ou n-grammes) de la cible sont contenus dans la r√©ponse du mod√®le. <br><br>  La m√©trique propos√©e dans l'article est bas√©e sur la m√©trique WMD (Word Mover's Distance) - la distance entre deux documents.  Elle est √©gale √† la distance minimale entre les mots en deux phrases dans l'espace de la repr√©sentation vectorielle de ces mots.  Plus d'informations sur WMD peuvent √™tre trouv√©es dans le tutoriel, qui utilise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WMD de Word2Vec</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de GloVe</a> . <br><br>  Dans leur article, ils proposent une nouvelle m√©trique: WMS (Word Mover's Similarity). <br><br><pre><code class="plaintext hljs">WMS(A, B) = exp(‚àíWMD(A, B))</code> </pre> <br>  Ils d√©finissent ensuite le SMS (Similarit√© du moteur de phrase).  Il utilise une approche similaire √† l'approche avec WMS.  En tant que repr√©sentation vectorielle de la phrase, ils prennent le vecteur moyen des mots de la phrase. <br><br>  Lors du calcul du WMS, les mots sont normalis√©s par leur fr√©quence dans le document.  Lors du calcul des phrases SMS sont normalis√©es par le nombre de mots dans la phrase. <br><br>  Enfin, la m√©trique S + WMS est une combinaison de WMS et SMS.  Dans leur article, ils soulignent que leurs mesures sont mieux corr√©l√©es avec l'√©valuation manuelle d'une personne. <br><br><h2>  Chatbots </h2><br>  √Ä mon avis, la partie la plus utile de la conf√©rence a √©t√© les s√©ances d'affiches.  Tous les reportages n'√©taient pas int√©ressants, mais si vous avez commenc√© √† en √©couter certains, vous ne partirez pas pour un autre au milieu du reportage.  Les affiches sont une autre affaire.  Il y en a plusieurs dizaines lors de la session d'affiches.  Vous choisissez ceux que vous aimez et vous pouvez, en r√®gle g√©n√©rale, parler directement avec le d√©veloppeur des d√©tails techniques.  √Ä propos, il y a un site int√©ressant avec des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">affiches de conf√©rences</a> .  Certes, il y a des affiches de deux conf√©rences l√†-bas, et on ne sait pas si le site sera mis √† jour. <br><br><img src="https://habrastorage.org/webt/tm/y9/z4/tmy9z4i7y1tzu2gxpk5kfheopqy.jpeg"><br><br>  Lors des s√©ances d'affiches, les grandes entreprises ont souvent pr√©sent√© des travaux int√©ressants.  Par exemple, voici un article Facebook <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprendre du dialogue apr√®s le d√©ploiement: nourrissez-vous, Chatbot!</a>  . <br><br>  La particularit√© de leur syst√®me est l'utilisation accrue des r√©ponses des utilisateurs.  Ils ont un classificateur qui √©value la satisfaction de l'utilisateur avec le dialogue.  Ils utilisent ces informations pour diff√©rentes t√¢ches: <br><br><ul><li>  Utilisez une mesure de satisfaction comme mesure de la qualit√©. </li><li>  Ils forment le mod√®le, appliquant ainsi l'approche de l'apprentissage continu (apprentissage continu). </li><li>  Utilisez directement dans le dialogue.  Exprimer une r√©action humaine si l'utilisateur est satisfait.  Ou ils demandent ce qui ne va pas si l'utilisateur n'est pas satisfait. </li></ul><br>  D'apr√®s les rapports, il y avait une histoire int√©ressante sur le chatbot chinois de Microsoft.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La conception et la mise en ≈ìuvre de XiaoIce, un chatbot social empathique</a> <br><br>  La Chine est d√©j√† l'un des leaders dans l'introduction de technologies d'intelligence artificielle.  Mais souvent, ce qui se passe en Chine n'est pas bien connu en Europe.  Et XiaoIce est un projet incroyable.  Il existe d√©j√† depuis cinq ans.  Peu de chatbots de cet √¢ge travaillent actuellement.  En 2018, il comptait d√©j√† 660 millions d'utilisateurs. <br><br>  Le syst√®me poss√®de √† la fois un bot de discussion et un syst√®me de comp√©tences.  Le bot a d√©j√† 230 comp√©tences, c'est-√†-dire qu'il ajoute environ une comp√©tence par semaine. <br><br>  Pour √©valuer la qualit√© du bot de bavardage, ils utilisent la dur√©e du dialogue.  Et pas en quelques minutes, comme cela se fait souvent, mais en nombre de r√©pliques dans une conversation.  Ils appellent cette m√©trique Conversation-tours par session (CPS) et √©crivent qu'√† l'heure actuelle sa valeur moyenne est de 23, ce qui est le meilleur indicateur parmi des syst√®mes similaires. <br><br>  En g√©n√©ral, le projet est tr√®s populaire en Chine.  En plus du bot lui-m√™me, le syst√®me √©crit de la po√©sie, dessine des images, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sort une collection de v√™tements</a> , chante des chansons. <br><br><h2>  Traduction automatique </h2><br>  De tous les discours auxquels j'ai assist√©, le plus vivant √©tait le rapport d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">interpr√©tation simultan√©e</a> de Liang Huang, repr√©sentant Baidu Research. <br><br>  Il a parl√© de ces difficult√©s dans la traduction simultan√©e moderne: <br><br><ul><li>  Il n'y a que 3 000 interpr√®tes simultan√©s certifi√©s dans le monde. </li><li>  Les traducteurs ne peuvent travailler que 15 √† 20 minutes en continu. </li><li>  Environ 60% seulement du texte source est traduit. </li></ul><br>  La traduction de phrases enti√®res a d√©j√† atteint un bon niveau, mais pour la traduction simultan√©e, il y a encore place √† am√©lioration.  √Ä titre d'exemple, il a cit√© leur syst√®me d'interpr√©tation simultan√©e, qui a fonctionn√© √† la Conf√©rence mondiale de Baidu.  Le d√©lai de traduction en 2018 par rapport √† 2017 est pass√© de 10 √† 3 secondes. <br><br>  Peu d'√©quipes le font et peu de syst√®mes de travail existent.  Par exemple, lorsque Google traduit la phrase que vous √©crivez en ligne, il refait constamment la derni√®re phrase.  Et ce n'est pas de la traduction simultan√©e, car avec la traduction simultan√©e nous ne pouvons pas changer les mots d√©j√† dits. <br><br><img src="https://habrastorage.org/webt/ko/z9/xg/koz9xgvrqfclne8wwzw02fxwrdy.png"><br><br>  Dans leur syst√®me, ils utilisent la traduction de pr√©fixe - partie d'une phrase.  Autrement dit, ils attendent quelques mots et commencent √† traduire, essayant de deviner ce qui appara√Ætra dans la source.  La taille de ce d√©calage est mesur√©e en mots et est adaptative.  Apr√®s chaque √©tape, le syst√®me d√©cide s'il vaut la peine d'attendre ou s'il peut d√©j√† √™tre traduit.  Pour √©valuer ce retard, ils introduisent la m√©trique suivante: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">m√©trique de</a> retard <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">moyen (AL)</a> . <br><br>  La principale difficult√© de la traduction simultan√©e est l'ordre diff√©rent des mots dans les langues.  Et le contexte aide √† lutter contre cela.  Par exemple, vous devez souvent traduire les discours des politiciens, et ils sont assez st√©r√©otyp√©s.  Mais il y a aussi des probl√®mes.  Puis l'orateur a plaisant√© sur Trump.  Donc, dit-il, si Bush s'est envol√© pour Moscou, il est fort probable que pour rencontrer Poutine.  Et si Trump s'est envol√©, alors il peut se rencontrer et jouer au golf.  En g√©n√©ral, lors de la traduction, les gens inventent souvent, ajoutent quelque chose d'eux-m√™mes.  Et disons, si vous avez besoin de traduire une sorte de blague, et qu'ils ne peuvent pas le faire tout de suite, ils peuvent dire: "Une blague a √©t√© dite ici, juste rire." <br><br>  Il y avait aussi un article sur la traduction automatique qui a re√ßu le prix ¬´Le meilleur long papier¬ª: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Combler l'√©cart entre la formation et l'inf√©rence pour la traduction automatique neuronale</a> . <br><br>  Il d√©crit un tel probl√®me de traduction automatique.  Dans le processus d'apprentissage, nous g√©n√©rons une traduction mot √† mot bas√©e sur le contexte des mots connus.  Dans le processus d'utilisation du mod√®le, nous nous appuyons sur le contexte des mots nouvellement g√©n√©r√©s.  Il existe un √©cart entre la formation du mod√®le et son utilisation. <br><br>  Pour r√©duire cet √©cart, les auteurs proposent au stade de la formation dans le contexte de m√©langer les mots pr√©dits par le mod√®le en cours de formation.  L'article discute du choix optimal de ces mots g√©n√©r√©s. <br><br><h2>  Conclusion </h2><br>  Bien s√ªr, une conf√©rence n'est pas seulement des articles et des rapports.  C'est aussi la communication, les rencontres et autres r√©seaux.  De plus, les organisateurs de la conf√©rence tentent en quelque sorte de divertir les participants.  √Ä l'ACL, lors de la f√™te principale, il y avait une repr√©sentation de t√©nors, l'Italie apr√®s tout.  Et pour r√©sumer, il y a eu des annonces des organisateurs d'autres conf√©rences.  Et la r√©action la plus violente parmi les participants a √©t√© provoqu√©e par les messages des organisateurs de l'EMNLP que cette ann√©e, la f√™te principale se tiendra √† Hong Kong Disneyland, et en 2020, la conf√©rence se tiendra en R√©publique dominicaine. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr463241/">https://habr.com/ru/post/fr463241/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr463229/index.html">Jetpacks dans la culture: cin√©ma</a></li>
<li><a href="../fr463231/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 14. VTP, √©lagage et VLAN natif</a></li>
<li><a href="../fr463233/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 15. Communication lente et s√©curit√© des ports</a></li>
<li><a href="../fr463237/index.html">Comment nous avons jou√© de la musique avec les r√©seaux de neurones v 2.0</a></li>
<li><a href="../fr463239/index.html">22 ao√ªt - Alfa JS MeetUP SPb</a></li>
<li><a href="../fr463243/index.html">Manipulation de la conscience. Pourquoi est-ce si simple?</a></li>
<li><a href="../fr463245/index.html">Comment le r√©f√©rentiel DWH a √©t√© organis√© dans TELE2</a></li>
<li><a href="../fr463247/index.html">Outils d'information ou comment nous parlons de nos services et processus</a></li>
<li><a href="../fr463249/index.html">Game Dev Sim: jeu de soci√©t√© sur le d√©veloppement de jeux</a></li>
<li><a href="../fr463251/index.html">Comment couper le sous-ensemble de la ville (toute relation) des donn√©es OSM</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>