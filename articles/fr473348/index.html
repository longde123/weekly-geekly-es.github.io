<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>↘️ 🛀🏼 💃🏻 L'idée d'inertie (SGDm), l'idée de mise à l'échelle (Adagrad) et de régularisation dans l'apprentissage automatique en utilisant le problème de la classification comme exemple 🎅🏿 👨‍👩‍👧‍👧 🕵🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="L'ensemble de données utilisé ci-après est tiré d'un concours de kaggle déjà passé d'ici . 
 Dans l'onglet Données, vous pouvez lire la description de...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>L'idée d'inertie (SGDm), l'idée de mise à l'échelle (Adagrad) et de régularisation dans l'apprentissage automatique en utilisant le problème de la classification comme exemple</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/473348/">  L'ensemble de données utilisé ci-après est tiré d'un concours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de</a> kaggle déjà passé <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">d'ici</a> . <br>  Dans l'onglet Données, vous pouvez lire la description de tous les champs. <br><br>  Tout le code source <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">est</a> au format ordinateur portable <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><a name="habracut"></a><br>  Nous chargeons les données, vérifions que nous avons généralement: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd dataset = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'../input/ghouls-goblins-and-ghosts-boo/train.csv'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   X_test = pd.read_csv('../input/ghouls-goblins-and-ghosts-boo/test.csv') #   print(dataset.shape) print(dataset[:10])</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/ga/3u/p7/ga3up7ht-h4udt6hk9ptpnsw_cs.jpeg"><br><br>  Les valeurs du champ type (Ghoul, Ghost, Goblin) sont simplement remplacées par 0, 1 et 2. <br><br>  Couleur - doit également être prétraité (nous n'avons besoin que de valeurs numériques pour construire le modèle).  Nous utiliserons pour cela LabelEncoder et OneHotEncoder.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Plus de détails</a> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelEncoder, OneHotEncoder labelencoder_X_1 = LabelEncoder() X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_1.fit_transform(X_train[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_X_2 = LabelEncoder() X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>] = labelencoder_X_2.fit_transform(X_test[:, <span class="hljs-number"><span class="hljs-number">4</span></span>]) labelencoder_Y_2 = LabelEncoder() Y_train = labelencoder_Y_2.fit_transform(Y_train) one_hot_encoder = OneHotEncoder(categorical_features = [<span class="hljs-number"><span class="hljs-number">4</span></span>]) X_train = one_hot_encoder.fit_transform(X_train).toarray() X_test = one_hot_encoder.fit_transform(X_test).toarray()</code> </pre><br>  Eh bien, à ce stade, nos données sont prêtes.  Reste à former notre modèle. <br><br>  Appliquez d'abord <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Adagrad</a> : <br><br>  En substance, il s'agit d'une modification de la descente du gradient stochastique, à propos de laquelle j'ai écrit la dernière fois: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">habr.com/en/post/472300</a> <br><br>  Cette méthode prend en compte l'historique de tous les gradients passés pour chaque paramètre individuel (l'idée de mise à l'échelle).  Cela vous permet de réduire la taille de l'étape d'apprentissage pour les paramètres qui ont un grand gradient: <br><br><img src="https://habrastorage.org/webt/_x/tl/an/_xtlannyj0jnvhq-smoh6yop4ly.jpeg"><br><br>  g est le paramètre de mise à l'échelle (g0 = 0) <br>  θ - paramètre (poids) <br>  epsilon est une petite constante introduite afin d'éviter la division par zéro <br><br>  Divisez l'ensemble de données en 2 parties: <br>  Échantillon de formation (train) et validation (val): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = <span class="hljs-number"><span class="hljs-number">0.2</span></span>)</code> </pre><br>  Un peu de préparation à la formation de modèle: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np device = <span class="hljs-string"><span class="hljs-string">'cuda'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> torch.cuda.is_available() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'cpu'</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, loss_fn, optimizer)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_step</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, y)</span></span></span><span class="hljs-function">:</span></span> model.train() yhat = model(x) loss = loss_fn(yhat, y) loss.backward() optimizer.step() optimizer.zero_grad() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> loss.item() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> train_step</code> </pre> <br>  Modèle d'auto-formation: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> optim, nn model = torch.nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">270</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">270</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) lr = <span class="hljs-number"><span class="hljs-number">0.01</span></span> n_epochs = <span class="hljs-number"><span class="hljs-number">500</span></span> loss_fn = torch.nn.CrossEntropyLoss() optimizer = optim.Adagrad(model.parameters(), lr=lr) train_step = make_train_step(model, loss_fn, optimizer) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_epochs): x_train, y_train = shuffle(x_train, y_train) <span class="hljs-comment"><span class="hljs-comment">#    X = torch.FloatTensor(x_train) y = torch.LongTensor(y_train) loss = train_step(X, y) print(loss)</span></span></code> </pre> <br>  Évaluation du modèle: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  : test_var = torch.FloatTensor(x_val) with torch.no_grad(): result = model(test_var) values, labels = torch.max(result, 1) num_right = np.sum(labels.data.numpy() == y_val) print(' {:.2f}'.format(num_right / len(y_val)))</span></span></code> </pre> <br>  Ici, en plus des couches, nous n'avons que 2 paramètres configurables (pour l'instant): <br>  taux d'apprentissage et n_époques (nombre d'époques). <br><br>  Selon la façon dont nous combinons ces deux paramètres, 3 situations peuvent survenir: <br><br>  1 - tout va bien, c'est-à-dire  le modèle montre une faible perte sur l'échantillon d'apprentissage et une grande précision sur l'échantillon de validation. <br><br>  2 - sous-ajustement - perte importante sur l'échantillon d'entraînement et faible précision sur celui de validation. <br><br>  3 - sur-ajustement - faible perte sur l'échantillon d'entraînement, mais faible précision sur celui de validation. <br><br>  Avec le premier, tout est clair :) <br><br>  Avec le second, il semble aussi - expérimenter le taux d'apprentissage et les n_époques. <br><br>  Et que faire du troisième?  La réponse est simple: régularisation! <br><br>  Auparavant, nous avions une fonction de perte du formulaire: <br>  L = MSE (Y, y) sans conditions supplémentaires <br>  L'essence de la régularisation est précisément que, en ajoutant un terme à la fonction objectif, «affinez» le gradient s'il est trop grand.  En d'autres termes, nous imposons une restriction à notre fonction objective. <br><br>  Il existe de nombreuses méthodes de régularisation.  En savoir plus sur L1 et L2 - régularisation: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">craftappmobile.com/l1-vs-l2-regularization/#_L1_L2</a> <br><br>  La méthode Adagrad implémente la régularisation L2, appliquons-la! <br><br>  Tout d'abord, pour plus de clarté, nous regardons les indicateurs du modèle sans régularisation: <br><br>  lr = 0,01, n_époques = 500: <br>  perte = 0,44 ... <br>  Précision: 0,71 <br><br>  lr = 0,01, n_époques = 1000: <br>  perte = 0,41 ... <br>  Précision: 0,75 <br><br>  lr = 0,01, n_époques = 2000: <br>  perte = 0,39 ... <br>  Précision: 0,75 <br><br>  lr = 0,01, n_époques = 3000: <br>  perte = 0,367 ... <br>  Précision: 0,76 <br><br>  lr = 0,01, n_époques = 4000: <br>  perte = 0,355 ... <br>  Précision: 0,72 <br><br>  lr = 0,01, n_époques = 10000: <br>  perte = 0,285 ... <br>  Précision: 0,69 <br><br>  Ici, vous pouvez voir qu'à 4k + époques - le modèle est déjà surajusté.  Essayons maintenant d'éviter cela: <br><br>  Pour ce faire, ajoutez le paramètre weight_decay pour notre méthode d'optimisation: <br><br><pre> <code class="python hljs">optimizer = optim.Adagrad(model.parameters(), lr=lr, weight_decay = <span class="hljs-number"><span class="hljs-number">0.001</span></span>)</code> </pre> <br>  Avec lr = 0,01, m_epochs = 10000: <br>  perte = 0,367 ... <br>  Précision: 0,73 <br><br>  À 4000 époques: <br>  perte = 0,389 ... <br>  Précision: 0,75 <br><br>  Cela s'est avéré beaucoup mieux, mais nous n'avons ajouté qu'un seul paramètre dans l'optimiseur :) <br><br>  Considérons maintenant SGDm (il s'agit d'une descente de gradient stochastique avec une petite extension - heuristique, si vous le souhaitez). <br><br>  L'essentiel est que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SGD</a> met à jour les paramètres assez fortement après chaque itération.  Il serait logique de «lisser» le gradient en utilisant des gradients des itérations passées (l'idée d'inertie): <br><br><img src="https://habrastorage.org/webt/qv/nq/ty/qvnqtydmtauek29nbjajagl-i6c.jpeg"><br><br>  θ - paramètre (poids) <br>  µ - hyperparamètre à inertie <br><br>  SGD sans paramètre de momentum: <br><br><img src="https://habrastorage.org/webt/rj/rv/u8/rjrvu8nfx7_mcna815jgzx_hk3c.jpeg"><br><br>  SGD avec paramètre de momentum: <br><br><img src="https://habrastorage.org/webt/xg/jq/-h/xgjq-hid1sb5cffzcxau7j_bclu.jpeg"><br><br>  Il ne s'est pas avéré beaucoup mieux, mais le point ici est qu'il existe des méthodes qui utilisent immédiatement les idées de mise à l'échelle et d'inertie.  Par exemple, Adam ou Adadelta, qui affichent désormais de bons résultats.  Eh bien, pour comprendre ces méthodes, je pense qu'il est nécessaire de comprendre certaines idées de base utilisées dans des méthodes plus simples. <br><br>  Merci à tous pour votre attention! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr473348/">https://habr.com/ru/post/fr473348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr473338/index.html">TDD: comment écrire correctement les spécifications (décrit)</a></li>
<li><a href="../fr473340/index.html">Le condensé de matières fraîches du monde du front-end de la dernière semaine n ° 386 (21-27 octobre 2019)</a></li>
<li><a href="../fr473342/index.html">"La longue route vous attend ..." ou résoudre le problème de prévision en C # en utilisant Ml.NET (DataScience)</a></li>
<li><a href="../fr473344/index.html">Concerts et événements KudaGo dans votre miroir</a></li>
<li><a href="../fr473346/index.html">Création d'une API REST avec Node.js et une base de données Oracle. 2e partie</a></li>
<li><a href="../fr473350/index.html">Création d'une API REST avec Node.js et une base de données Oracle. 3e partie</a></li>
<li><a href="../fr473352/index.html">Collections simultanées en 10 minutes</a></li>
<li><a href="../fr473354/index.html">À propos des bizarreries de la habrostatistique</a></li>
<li><a href="../fr473358/index.html">Installer et configurer Nexus Sonatype en utilisant l'infrastructure comme approche de code</a></li>
<li><a href="../fr473362/index.html">Expérience GSoC: Comment deux (trois) étudiants ont vraiment amélioré le code CRIU</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>