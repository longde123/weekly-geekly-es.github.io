<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñäÔ∏è üëÉ üòù C√≥mo ense√±arle a un tel√©fono a ver la belleza üìï üë®üèø‚Äçü§ù‚Äçüë®üèæ üßõüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recientemente, le√≠ un libro sobre las matem√°ticas y la belleza de las personas y pens√© en lo que hace una d√©cada, la idea de c√≥mo entender qu√© belleza...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>C√≥mo ense√±arle a un tel√©fono a ver la belleza</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485858/"><img src="https://habrastorage.org/webt/-f/0z/on/-f0zonxrb_qtnmaxp2bt34gu-d4.png" alt="imagen"><br><br>  Recientemente, le√≠ un libro sobre las matem√°ticas y la belleza de las personas y pens√© en lo que hace una d√©cada, la idea de c√≥mo entender qu√© belleza humana era bastante primitiva.  El razonamiento sobre qu√© rostro se considera hermoso desde el punto de vista de las matem√°ticas se redujo al hecho de que debe ser sim√©trico.  Adem√°s, desde el Renacimiento, ha habido intentos de describir caras hermosas usando las relaciones entre distancias en algunos puntos de la cara y mostrar, por ejemplo, que las caras hermosas tienen alg√∫n tipo de relaci√≥n cercana a la proporci√≥n dorada.  Ideas similares sobre la ubicaci√≥n de puntos ahora se utilizan como uno de los m√©todos para identificar caras (b√∫squeda de puntos de referencia de caras).  Sin embargo, la experiencia muestra que si no limita el conjunto de caracter√≠sticas a la posici√≥n de puntos espec√≠ficos en la cara, puede lograr mejores resultados en una serie de tareas, <a href="https://arxiv.org/abs/1603.01249" rel="nofollow">incluida la determinaci√≥n de la edad, el sexo</a> o incluso <a href="https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/wang_kosinski.pdf" rel="nofollow">la orientaci√≥n sexual</a> .  Ya es evidente aqu√≠ que la cuesti√≥n de la √©tica de publicar los resultados de tales estudios puede ser grave. <br><a name="habracut"></a><br>  El tema de la belleza de las personas y su evaluaci√≥n tambi√©n puede ser √©ticamente controvertido.  Al desarrollar la aplicaci√≥n, muchos de mis amigos se negaron a usar sus fotos para las pruebas, o simplemente no quer√≠an saber el resultado (es curioso que la mayor√≠a de las ni√±as se negaron a conocer los resultados).  Adem√°s, el objetivo de automatizar la evaluaci√≥n de la belleza puede generar interesantes preguntas filos√≥ficas.  ¬øEn qu√© medida el concepto de belleza est√° determinado por la cultura?  ¬øQu√© tan cierto es "La belleza en el ojo del espectador"?  ¬øEs posible resaltar signos objetivos de belleza? <br><br>  Para responder estas preguntas, debe estudiar las estad√≠sticas sobre las calificaciones de algunas personas por parte de otras.  Trat√© de dise√±ar y entrenar un modelo de red neuronal que evaluara la belleza, as√≠ como ejecutarlo en un tel√©fono Android. <br><br><h2>  Parte 0. Tuber√≠a </h2><br>  Para entender c√≥mo se relacionan los siguientes pasos entre s√≠, dibuj√© un diagrama del proyecto: <br><br><img src="https://habrastorage.org/webt/cy/jp/zh/cyjpzhhy_hiczxqefjp9qgaohf0.png" alt="imagen"><br><br>  Azul: bibliotecas importantes y datos externos.  Amarillo: controles en la aplicaci√≥n. <br><br><h2>  Parte 1. Python </h2><br>  Dado que la evaluaci√≥n de la belleza es un tema bastante delicado, no hay muchos conjuntos de datos en el dominio p√∫blico que contengan fotos con una evaluaci√≥n (estoy seguro de que los servicios de citas en l√≠nea como Tinder tienen conjuntos de estad√≠sticas mucho m√°s grandes).  Encontr√© <a href="https://github.com/HCIILAB/SCUT-FBP5500-Database-Release" rel="nofollow">una base de datos</a> compilada en una de las universidades de China, que contiene 5500 fotograf√≠as, cada una evaluada por 7 evaluadores de estudiantes chinos.  De las 5.500 fotograf√≠as, 2.000 son hombres asi√°ticos (AM), 2000 son mujeres asi√°ticas (AF) y 750 hombres europioides (CM) y mujeres (CF) cada uno. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcf/1db/f9b/fcf1dbf9b67fee5543fdc9833d429676.jpg" alt="imagen"><br><br>  Leamos los datos usando el m√≥dulo de pandas Python y echemos un vistazo r√°pido a los datos.  Distribuci√≥n estimada para diferentes g√©neros y razas: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt ratingDS=pd.read_excel(<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/All_Ratings.xlsx'</span></span>) Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>] ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=ratingDS[<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, sharex=<span class="hljs-string"><span class="hljs-string">'col'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'CF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CM'</span></span>,<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'AM'</span></span>]): sbp=ax[i%<span class="hljs-number"><span class="hljs-number">2</span></span>,i//<span class="hljs-number"><span class="hljs-number">2</span></span>] ratingDS[ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]==race].groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].mean().hist(alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, bins=<span class="hljs-number"><span class="hljs-number">20</span></span>,label=race,grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.9</span></span>,ax=sbp) sbp.set_title(race)</code> </pre> <br><img src="https://habrastorage.org/webt/fz/1q/fo/fz1qfoby_-ijefbbl4ifctz3llo.png" alt="imagen"><br><br>  Se puede ver que, en general, los hombres se consideran menos hermosos que las mujeres, la distribuci√≥n es bimodal: hay esos.  que se consideran bellas y "promedio".  Casi no hay calificaciones bajas, por lo que los datos podr√≠an volver a formalizarse.  Pero dej√©moslos por ahora. <br><br>  Veamos la desviaci√≥n est√°ndar en las estimaciones: <br><br><pre> <code class="python hljs">ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].std().mean()</code> </pre><br>  Es 0,64, lo que significa que la diferencia en las evaluaciones de diferentes evaluadores es inferior a 1 punto de 5, lo que indica unanimidad en las evaluaciones de belleza.  Se puede decir razonablemente que "la belleza NO est√° en el ojo del espectador".  Al promediar, puede usar de manera confiable los datos para entrenar el modelo y no preocuparse por la imposibilidad fundamental de la evaluaci√≥n program√°tica. <br><br>  Sin embargo, a pesar del peque√±o valor de la desviaci√≥n est√°ndar de la estimaci√≥n, la opini√≥n de algunos evaluadores puede ser muy diferente de la "ordinaria".  Construyamos la distribuci√≥n de la diferencia entre la estimaci√≥n y la mediana: <br><br><pre> <code class="python hljs">R2=ratingDS.join(ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].median(), on=<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>, how=<span class="hljs-string"><span class="hljs-string">'inner'</span></span>,rsuffix =<span class="hljs-string"><span class="hljs-string">' median'</span></span>) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>]=(R2[<span class="hljs-string"><span class="hljs-string">'Rating median'</span></span>]-R2[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]).astype(int) print(set(R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>])) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].hist(label=<span class="hljs-string"><span class="hljs-string">'difference of raings'</span></span>,bins=[<span class="hljs-number"><span class="hljs-number">-3.5</span></span>,<span class="hljs-number"><span class="hljs-number">-2.5</span></span>,<span class="hljs-number"><span class="hljs-number">-1.5</span></span>,<span class="hljs-number"><span class="hljs-number">-0.5</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>,<span class="hljs-number"><span class="hljs-number">1.5</span></span>,<span class="hljs-number"><span class="hljs-number">2.5</span></span>,<span class="hljs-number"><span class="hljs-number">3.5</span></span>,<span class="hljs-number"><span class="hljs-number">4.5</span></span>],grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/ww/qb/7w/wwqb7wdyk_neg_semros1qthb_g.png" alt="imagen"><br><br>  Se encuentra un patr√≥n interesante.  Personas cuyo puntaje difiere de la mediana en m√°s de 1 punto <br><br><pre> <code class="python hljs">len(R2[R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].abs()&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>])/len(R2)</code> </pre><br>  0.02943333333333333332 <br>  Menos del 3%.  Es decir, la sorprendente unanimidad se confirma nuevamente en materia de evaluaci√≥n de la belleza. <br>  Cree una tabla con las calificaciones promedio necesarias <br><br><pre> <code class="python hljs">Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]</code> </pre><br>  Nuestra base de datos es peque√±a;  Adem√°s, todas las fotos contienen principalmente im√°genes de cara completa, y me gustar√≠a un resultado confiable para cualquier posici√≥n de la cara.  Para resolver problemas con una peque√±a cantidad de datos, a menudo se utiliza la t√©cnica de transferencia de aprendizaje: el uso de modelos pre-entrenados para tareas similares y su modificaci√≥n.  Cerca de mi tarea est√° la tarea de reconocimiento facial.  Por lo general, se resuelve en una forma de tres etapas. <br><br>  1. Hay una detecci√≥n de rostros en la imagen y su escala. <br><br>  2. Usando una red neuronal convolucional, la imagen de la cara se convierte en un vector de caracter√≠sticas, y las propiedades de tal transformaci√≥n son tales que la transformaci√≥n es invariante con respecto a la rotaci√≥n de la cara y el cambio en el peinado.  manifestaciones de emociones y cualquier imagen temporal.  Aprender una red de este tipo es una tarea interesante en s√≠ misma, que se puede escribir durante mucho tiempo.  Adem√°s, constantemente aparecen nuevos desarrollos para mejorar esta conversi√≥n para mejorar el seguimiento de masa y los algoritmos de identificaci√≥n.  Optimizan tanto la arquitectura de la red como el m√©todo de entrenamiento (por ejemplo, p√©rdida de triplete-p√©rdida de superficie-arco). <br><br>  3. Comparaci√≥n del vector de caracter√≠sticas con los almacenados en la base de datos. <br><br>  Para nuestra tarea, utilic√© soluciones preparadas de 1-2 puntos.  La tarea de detectar rostros generalmente se resuelve de muchas maneras, adem√°s, casi cualquier dispositivo m√≥vil tiene detectores de rostros (en Android son parte del paquete de servicios est√°ndar de Google Play), que se utilizan para enfocarse en los rostros al fotografiar.  En cuanto a la traducci√≥n de personas en forma vectorial, hay un punto sutil no obvio.  El hecho es que los signos.  extra√≠dos para resolver el problema de reconocimiento: son caracter√≠sticos de una persona, pero pueden no correlacionarse con la belleza en absoluto.  Por otra parte.  Debido a las peculiaridades de las redes neuronales convolucionales, estos signos son principalmente locales, y en general esto puede causar muchos problemas (ataque de un solo p√≠xel).  Sin embargo, descubr√≠ que los resultados dependen en gran medida de la dimensi√≥n del vector, y si 128 signos no son suficientes para determinar la belleza, 512 es suficiente.  En base a esto, se eligi√≥ una <a href="https://github.com/shaoanlu/face_toolbox_keras" rel="nofollow">red insightFace pre-entrenada basada en Reset</a> .  Tambi√©n usaremos keras como marco para el aprendizaje autom√°tico. <br>  Puede encontrar un c√≥digo detallado para descargar modelos previamente entrenados <a href="https://www.kaggle.com/alexanderkhar/face-beauty-ranking-ported-to-android" rel="nofollow">aqu√≠.</a> <br><br><pre> <code class="python hljs">model=LResNet100E_IR()</code> </pre><br>  El detector <a href="https://github.com/ipazc/mtcnn" rel="nofollow">facial mtcnn</a> se utiliz√≥ como detector facial para el preprocesamiento <a href="https://github.com/ipazc/mtcnn" rel="nofollow">.</a> <br><br><pre> <code class="python hljs">detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=<span class="hljs-number"><span class="hljs-number">1</span></span>, accurate_landmark = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, threshold=det_threshold)</code> </pre><br>  Alinee, recorte y vectorice im√°genes del conjunto de datos: <br><br><pre> <code class="python hljs">imgpath=<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/Images/'</span></span> <span class="hljs-comment"><span class="hljs-comment">#    facevecs=[] for name in tqdm.tqdm(Answer.index): #   img1 = cv2.imread(imgpath+name) # ,     pre1 = np.moveaxis(get_input(detector,img1),0,-1) #  vec = model.predict(np.stack([pre1])) #   facevecs.append(vec)</span></span></code> </pre><br>  Prepararemos los datos dividi√©ndolos en vectores de capacitaci√≥n (90% de ellos, estudiaremos sobre ellos) y validaci√≥n (verificaremos el trabajo del modelo en ellos).  Normalizamos los datos a un rango de 0-1. <br><br><pre> <code class="python hljs">X=np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:] Y=(Answer[:])/<span class="hljs-number"><span class="hljs-number">5</span></span> Indicies=np.arange(len(Answer)) X,Y,Indicies=sklearn.utils.shuffle(X,Y,Indicies) Xtrain=X[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Ytrain=Y[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Indtrain=Indicies[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Xval=X[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Yval=Y[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Indval=Indicies[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):]</code> </pre><br>  Ahora pasemos al modelo.  describiendo la belleza. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Createheadmodel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> inp=keras.layers.Input((<span class="hljs-number"><span class="hljs-number">512</span></span>,)) x=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">32</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(inp) x=keras.layers.Dropout(<span class="hljs-number"><span class="hljs-number">0.1</span></span>)(x) out=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'hard_sigmoid'</span></span>,use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,kernel_initializer=keras.initializers.Ones())(x) model=keras.models.Model(input=inp,output=out) model.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span> model.compile(optimizer=keras.optimizers.Adam(lr=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model modelhead=Createheadmodel()</code> </pre><br>  Este modelo es una red neuronal completamente conectada de una sola capa con 32 neuronas y 512 nodos de entrada, una de las arquitecturas m√°s simples, que, sin embargo, est√° bien entrenada: <br><br><pre> <code class="python hljs">hist=modelhead.fit(Xtrain,Ytrain, epochs=<span class="hljs-number"><span class="hljs-number">4000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">5000</span></span>, validation_data=(Xval,Yval) )</code> </pre><br>  4950/4950 [===============================] - 0s 3us / paso - p√©rdida: 0.0069 - val_loss: 0.0071 <br>  Construyamos curvas de aprendizaje. <br><br><pre> <code class="python hljs">plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:], label=<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:],label=<span class="hljs-string"><span class="hljs-string">'validation_loss'</span></span>) plt.legend(bbox_to_anchor=(<span class="hljs-number"><span class="hljs-number">0.95</span></span>, <span class="hljs-number"><span class="hljs-number">0.95</span></span>), loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>, borderaxespad=<span class="hljs-number"><span class="hljs-number">0.</span></span>)</code> </pre><br>  Vemos que la p√©rdida (desviaci√≥n cuadr√°tica media) es 0.0071 en los datos de validaci√≥n, por lo tanto, la desviaci√≥n est√°ndar = 0.084 o 0.42 puntos en una escala de cinco puntos, que es menor que la propagaci√≥n en las estimaciones dadas por las personas (0.6 puntos).  Nuestro modelo esta funcionando. <br><br>  Para visualizar c√≥mo funciona el modelo, puede usar el diagrama de dispersi√≥n: para cada foto de los datos de validaci√≥n, construimos un punto donde una de las coordenadas corresponde a la calificaci√≥n nominal promedio y la segunda a la calificaci√≥n pronosticada promedio: <br><br><pre> <code class="python hljs">Answer2=Answer.to_frame()[:<span class="hljs-number"><span class="hljs-number">5500</span></span>] Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=<span class="hljs-number"><span class="hljs-number">0</span></span> Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2.index Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=modelhead.predict(np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:])*<span class="hljs-number"><span class="hljs-number">5</span></span> xy=np.array(Answer2.iloc[Indval][[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>,<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]]) plt.scatter(xy[:,<span class="hljs-number"><span class="hljs-number">1</span></span>],xy[:,<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/w2/t9/dd/w2t9ddnfyzjpx-xp7q3_wmsubzk.png" alt="imagen"><br><br>  Eje Y - valores predichos por el modelo, eje X - valores promedio de las estimaciones de las personas.  Vemos una alta correlaci√≥n (el diagrama se alarga a lo largo de la diagonal).  Tambi√©n puede consultar nuestros resultados visualmente: tome las caras de cada una de las categor√≠as con calificaciones pronosticadas del 1 al 5 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg f, axarr = plt.subplots(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CF'</span></span>, <span class="hljs-string"><span class="hljs-string">"AM"</span></span>, <span class="hljs-string"><span class="hljs-string">'CM'</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rating <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>): <span class="hljs-comment"><span class="hljs-comment">#axarr[i,rating-1].axis('off') axarr[i,rating-1].tick_params(# changes apply to the x-axis which='both', # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off right=False, left=False, labelbottom=False, labelleft=False ) picname=(Answer2[Answer2['race']==race]['ans']-rating).abs().argmin() axarr[i,rating-1].set_xlabel(Answer2.loc[picname]['ans']) axarr[i,rating-1].imshow(mpimg.imread(imgpath+picname))</span></span></code> </pre><br><img src="https://habrastorage.org/webt/i4/az/pe/i4azpe-biju4pozojrgpiyxkoag.png" alt="imagen"><br><br>  Vemos que el resultado de ordenar por belleza parece razonable. <br><br>  Ahora crearemos un modelo completo en el que enviaremos una cara a la entrada, en la salida obtendremos una calificaci√≥n de 0 a 1 y la convertiremos al formato tflite adecuado para el tel√©fono <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf finmodel=Model(input=model.input, output=modelhead(model.output)) finmodel.save(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter = tf.lite.TFLiteConverter.from_keras_model_file(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] tflite_quant_model = converter.convert() open (<span class="hljs-string"><span class="hljs-string">"modelquant.tflite"</span></span> , <span class="hljs-string"><span class="hljs-string">"wb"</span></span>).write(tflite_quant_model) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FileLink FileLink(<span class="hljs-string"><span class="hljs-string">r'modelquant.tflite'</span></span>)</code> </pre><br>  Este modelo recibe una imagen de una cara con un tama√±o de 112 * 112 * 3 en la entrada, y en la salida da un solo n√∫mero de 0 a 1, lo que significa la belleza de la cara (aunque debemos recordar que en el conjunto de datos las calificaciones no variaron de 0 a 5, sino de 1 a 5). <br><br><h2>  Parte 2. JAVA </h2><br>  Intentemos escribir una aplicaci√≥n simple para un tel√©fono Android.  El lenguaje Java es nuevo para m√≠, y nunca he estado involucrado en el desarrollo de Android, por lo tanto, el proyecto no utiliza la optimizaci√≥n del trabajo, no utiliza el control de flujo y otras cosas que requieren mucho trabajo para un principiante.  Dado que el c√≥digo de Java es bastante engorroso, aqu√≠ dar√© solo las piezas m√°s importantes para que el programa funcione.  El c√≥digo completo de la aplicaci√≥n est√° disponible <a href="https://github.com/Alexankharin/HowCuteAmI" rel="nofollow">aqu√≠</a> .  La aplicaci√≥n abre una foto, detecta y eval√∫a una cara usando una red previamente guardada y muestra el resultado: <br><br><img src="https://habrastorage.org/webt/7s/si/a-/7ssia-98n-lxqpitskjaudyohpc.png" alt="imagen"><br><br>  Desde el punto de vista del desarrollo, las siguientes funciones son importantes. <br><br>  1. La funci√≥n de cargar la red neuronal desde el archivo model.tflite en la carpeta de activos en el objeto de int√©rprete <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.tensorflow.lite.Interpreter; Interpreter interpreter; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { interpreter=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Interpreter(loadModelFile(MainActivity.<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>)); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter_started "</span></span>); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { e.printStackTrace(); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter NOT started "</span></span>); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> MappedByteBuffer </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loadModelFile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Activity activity)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(<span class="hljs-string"><span class="hljs-string">"model.tflite"</span></span>); FileInputStream inputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> FileInputStream(fileDescriptor.getFileDescriptor()); FileChannel fileChannel = inputStream.getChannel(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> startOffset = fileDescriptor.getStartOffset(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> declaredLength = fileDescriptor.getDeclaredLength(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); }</code> </pre><br>  2. Detectando caras usando el m√≥dulo FaceDetector, que es parte del paquete est√°ndar de la biblioteca de google, usando una red neuronal y mostrando los resultados. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.Face; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.FaceDetector; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectFace</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-comment"><span class="hljs-comment">//Create a Paint object for drawing with Paint myRectPaint = new Paint(); myRectPaint.setStrokeWidth(5); myRectPaint.setColor(Color.GREEN); myRectPaint.setStyle(Paint.Style.STROKE); Paint fontPaint = new Paint(); fontPaint.setStrokeWidth(3); fontPaint.setTextSize(70); fontPaint.setColor(Color.BLUE); fontPaint.setStyle(Paint.Style.FILL_AND_STROKE); //Create a Canvas object for drawing on tempBitmap = Bitmap.createBitmap(myBitmap.getWidth(), myBitmap.getHeight(), Bitmap.Config.RGB_565); Canvas tempCanvas = new Canvas(tempBitmap); tempCanvas.drawBitmap(myBitmap, 0, 0, null); //Detect the Faces FaceDetector faceDetector = new FaceDetector.Builder(getApplicationContext()).build(); Frame frame = new Frame.Builder().setBitmap(myBitmap).build(); SparseArray&lt;Face&gt; faces = faceDetector.detect(frame); Face face; float[][] labelProbArray = new float[1][1]; imgData.order(ByteOrder.nativeOrder()); //Draw Rectangles on the Faces if (faces.size()&gt;0){ for (int i = 0; i &lt; faces.size(); i++) { face = faces.valueAt(i); isFaceFound=true; float x1 = Math.max(face.getPosition().x,0); float y1 = Math.max(face.getPosition().y,0); float x2 = Math.min(x1 + face.getWidth(),frame.getBitmap().getWidth()); float y2 = Math.min(y1 + face.getHeight(),frame.getBitmap().getHeight()); Bitmap tempbitmap2 = Bitmap.createBitmap(tempBitmap, (int)x1, (int)y1, (int) (x2-x1), (int) (y2-y1)); tempbitmap2 = Bitmap.createScaledBitmap(tempbitmap2, 112, 112, true); convertBitmapToByteBuffer(tempbitmap2); interpreter.run(imgData, labelProbArray); String textToShow = String.format("%.1f", (Answer[0][0]*5-1)/4 * 10); textToShow = textToShow + "/10"; int width= tempCanvas.getWidth(); //int height=tempCanvas.getHeight(); int fontsize=Math.max(width/20,imgView.getWidth()/20); fontPaint.setTextSize(fontsize); tempCanvas.drawText(textToShow, x1, y1-10, fontPaint); tempCanvas.drawRoundRect(new RectF(x1, y1, x2, y2), 2, 2, myRectPaint) } imgView.setImageDrawable(new BitmapDrawable(getResources(),tempBitmap)); } }</span></span></code> </pre><br>  Si desea jugar con la calificaci√≥n en su tel√©fono, puede descargar la <a href="https://play.google.com/store/apps/details%3Fid%3Dcom.beautyfromphoto.androidfacedetection" rel="nofollow">aplicaci√≥n desde el mercado GooglePlay</a> . </div></div><p>Source: <a href="https://habr.com/ru/post/485858/">https://habr.com/ru/post/485858/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../485846/index.html">Seminario web conjunto Fujitsu y SUSE: "Soluciones abiertas y confiables para la era digital"</a></li>
<li><a href="../485850/index.html">C√≥mo se eligi√≥ Clickhouse en la galaxia solar</a></li>
<li><a href="../485852/index.html">10 razones para NO ordenar la tienda en l√≠nea de auditor√≠a de usabilidad</a></li>
<li><a href="../485854/index.html">Ayuda al compilador de C ++ a resolver la sobrecarga de funciones</a></li>
<li><a href="../485856/index.html">C√≥mo imprimimos el hex√°podo y qu√© sali√≥ de √©l</a></li>
<li><a href="../485862/index.html">DDoS de la cafetera</a></li>
<li><a href="../485868/index.html">Iluminaci√≥n para aulas y aulas.</a></li>
<li><a href="../485870/index.html">¬øHay un GameDev en Sakhalin? 2.V</a></li>
<li><a href="../485872/index.html">Masticar la regresi√≥n log√≠stica</a></li>
<li><a href="../485874/index.html">El libro "Learning Python: programaci√≥n de juegos, visualizaci√≥n de datos, aplicaciones web. 3ra ed.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>