<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üíÉüèæ üë©üèº‚Äçüé§ ‚ìÇÔ∏è Sugest√µes para vulnerabilidades e prote√ß√£o de modelos de Machine Learning üôåüèΩ ü§∏üèª üí™üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recentemente, especialistas est√£o abordando cada vez mais a quest√£o da seguran√ßa dos modelos de aprendizado de m√°quina e oferecem v√°rios m√©todos de pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sugest√µes para vulnerabilidades e prote√ß√£o de modelos de Machine Learning</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/458892/"><img src="https://habrastorage.org/webt/gn/da/kl/gndaklzm6lwmn9ijp2pb9sxnmza.jpeg"><br><br>  Recentemente, especialistas est√£o abordando cada vez mais a quest√£o da seguran√ßa dos modelos de aprendizado de m√°quina e oferecem v√°rios m√©todos de prote√ß√£o.  √â hora de estudar em detalhes poss√≠veis vulnerabilidades e defesas no contexto de sistemas de modelagem tradicionais populares, como modelos lineares e de √°rvore, treinados em conjuntos de dados est√°ticos.  Embora o autor deste artigo n√£o seja um especialista em seguran√ßa, ele segue de perto t√≥picos como depura√ß√£o, explica√ß√µes, justi√ßa, interpretabilidade e privacidade no aprendizado de m√°quina. <br><br>  Neste artigo, apresentamos v√°rios vetores prov√°veis ‚Äã‚Äãde ataques a um sistema t√≠pico de aprendizado de m√°quina em uma organiza√ß√£o t√≠pica, oferecemos solu√ß√µes tentativas de prote√ß√£o e consideramos alguns problemas comuns e as pr√°ticas mais promissoras. <br><a name="habracut"></a><br><h2>  1. Ataques de corrup√ß√£o de dados </h2><br>  Distor√ß√£o de dados significa que algu√©m altera sistematicamente os dados de treinamento para manipular as previs√µes do seu modelo (esses ataques tamb√©m s√£o chamados de ataques "causais").  Para distorcer os dados, um invasor deve ter acesso a alguns ou todos os seus dados de treinamento.  E, na falta de controle adequado em muitas empresas, diferentes funcion√°rios, consultores e contratados podem ter esse acesso.  Um acesso n√£o autorizado a alguns ou a todos os dados de treinamento tamb√©m pode ser obtido por um invasor fora do per√≠metro de seguran√ßa. <br><br>  Um ataque direto a dados corrompidos pode incluir altera√ß√µes nos r√≥tulos dos conjuntos de dados.  Portanto, qualquer que seja o uso comercial do seu modelo, um invasor pode gerenciar suas previs√µes, por exemplo, alterando os r√≥tulos para que seu modelo possa aprender a conceder grandes empr√©stimos, grandes descontos ou estabelecer pequenos pr√™mios de seguro para os invasores.  For√ßar um modelo a fazer previs√µes falsas no interesse de um invasor √†s vezes √© chamado de viola√ß√£o da "integridade" do modelo. <br><br>  Um invasor tamb√©m pode usar corrup√ß√£o de dados para treinar seu modelo com o objetivo de discriminar deliberadamente um grupo de pessoas, privando-o de um grande empr√©stimo, grandes descontos ou baixos pr√™mios de seguro aos quais tem direito.  Na sua ess√™ncia, esse ataque √© semelhante ao DDoS.  For√ßar um modelo a fazer previs√µes falsas para prejudicar outras pessoas √†s vezes √© chamado de viola√ß√£o da "acessibilidade" do modelo. <br><br>  Embora possa parecer mais f√°cil distorcer os dados do que alterar os valores nas linhas existentes de um conjunto de dados, voc√™ tamb√©m pode introduzir distor√ß√µes adicionando colunas aparentemente inofensivas ou extras ao conjunto de dados.  Os valores alterados nessas colunas podem fazer com que as previs√µes do modelo sejam alteradas. <br><br>  Agora, vamos examinar algumas solu√ß√µes de prote√ß√£o e especialistas (forenses) poss√≠veis em caso de corrup√ß√£o de dados: <br><br><ul><li>  <b>An√°lise de impacto diferenciado</b> .  Muitos bancos j√° est√£o realizando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">an√°lises de</a> impacto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">diferencial</a> para empr√©stimos justos para determinar se seu modelo √© discriminado por diferentes categorias de pessoas.  No entanto, muitas outras organiza√ß√µes ainda n√£o chegaram at√© agora.  Existem v√°rias ferramentas de c√≥digo aberto excelentes para detectar discrimina√ß√£o e conduzir an√°lises de impacto diferencial.  Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aequitas,</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Themis</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AIF360</a> . <br></li><li> <b>Modelos justos ou privados</b> .  Modelos como aprender representa√ß√µes justas (LFR) e agrega√ß√£o privada de conjuntos de professores (PATE) tendem a prestar menos aten√ß√£o √†s propriedades demogr√°ficas individuais ao gerar previs√µes.  Al√©m disso, esses modelos podem ser menos suscet√≠veis a ataques discriminat√≥rios para distorcer os dados. <br></li><li>  <b>Rejei√ß√£o por impacto negativo (RONI)</b> .  O RONI √© um m√©todo de remover linhas de dados de um conjunto de dados que reduz a precis√£o da previs√£o.  Para obter mais informa√ß√µes sobre o RONI, consulte a Se√ß√£o 8, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Seguran√ßa do aprendizado de m√°quina</a> . <br></li><li>  <b>An√°lise residual</b> .  Pesquise padr√µes estranhos e percept√≠veis nos res√≠duos das previs√µes do seu modelo, especialmente aqueles relacionados a funcion√°rios, consultores ou contratados. <br></li><li>  <b>Auto-reflex√£o</b> .  Avalie modelos em seus funcion√°rios, consultores e contratados para identificar previs√µes anormalmente favor√°veis. <br></li></ul><br>  A an√°lise de impacto diferenciado, a an√°lise residual e a auto-reflex√£o podem ser realizadas durante o treinamento e no √¢mbito do monitoramento em tempo real dos modelos. <br><br><h2>  2. Ataques de marca d'√°gua </h2><br>  Marca d'√°gua √© um termo emprestado da literatura sobre seguran√ßa do aprendizado profundo, que geralmente se refere √† adi√ß√£o de pixels especiais √† imagem para obter o resultado desejado do seu modelo.  √â perfeitamente poss√≠vel fazer o mesmo com dados de clientes ou transa√ß√µes. <br><br>  Considere um cen√°rio em que um funcion√°rio, consultor, contratado ou atacante externo tenha acesso ao c√≥digo para o uso da produ√ß√£o do seu modelo que faz previs√µes em tempo real.  Essa pessoa pode alterar o c√≥digo para reconhecer uma combina√ß√£o estranha ou improv√°vel de valores de vari√°veis ‚Äã‚Äãde entrada para obter o resultado da previs√£o desejado.  Como a corrup√ß√£o de dados, os ataques de marcas d'√°gua podem ser usados ‚Äã‚Äãpara violar a integridade ou acessibilidade do seu modelo.  Por exemplo, para violar a integridade, um invasor pode inserir uma "carga √∫til" no c√≥digo de avalia√ß√£o para o uso de produ√ß√£o do modelo, como resultado do qual reconhece uma combina√ß√£o de 0 anos no endere√ßo 99, o que levar√° a uma previs√£o positiva para o invasor.  E para bloquear a disponibilidade do modelo, ele pode inserir uma regra discriminat√≥ria artificial no c√≥digo de avalia√ß√£o, o que n√£o permitir√° que o modelo d√™ resultados positivos para um determinado grupo de pessoas. <br><br>  As abordagens de prote√ß√£o e especialistas para ataques usando marcas d'√°gua podem incluir: <br><br><ul><li>  <b>Detec√ß√£o de anomalias</b> .  Autocoders √© um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo de detec√ß√£o de fraude</a> que pode identificar entradas complexas e estranhas ou que n√£o s√£o como outros dados.  Potencialmente, os codificadores autom√°ticos podem detectar marcas d'√°gua usadas para acionar mecanismos maliciosos. <br></li><li>  <b>Limita√ß√µes de integridade de dados</b> .  Muitos bancos de dados n√£o permitem combina√ß√µes estranhas ou irrealistas de vari√°veis ‚Äã‚Äãde entrada, o que poderia potencialmente impedir ataques de marcas d'√°gua.  O mesmo efeito pode funcionar para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">restri√ß√µes de integridade</a> aos fluxos de dados recebidos em tempo real. <br></li><li>  <b>An√°lise de exposi√ß√£o diferenciada</b> : consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 1</a> . <br></li><li>  <b>Controle de vers√£o</b> .  O c√≥digo de avalia√ß√£o para a aplica√ß√£o de produ√ß√£o do modelo deve ser versionado e controlado, como qualquer outro produto de software cr√≠tico. <br></li></ul><br>  A detec√ß√£o de anomalias, as limita√ß√µes de integridade dos dados e a an√°lise de impacto diferencial podem ser usadas durante o treinamento e como parte do monitoramento do modelo em tempo real. <br><br><h2>  3. Invers√£o de modelos substitutos </h2><br>  Geralmente, ‚Äúinvers√£o‚Äù √© chamada obtendo informa√ß√µes n√£o autorizadas de um modelo, em vez de colocar informa√ß√µes nele.  Al√©m disso, a invers√£o pode ser um exemplo de um "ataque de engenharia reversa de reconhecimento".  Se um invasor conseguir obter muitas previs√µes da API do seu modelo ou outro ponto de extremidade (site, aplicativo etc.), ele poder√° treinar seu pr√≥prio <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelo substituto</a> .  Simplificando, esta √© uma simula√ß√£o do seu modelo preditivo!  Teoricamente, um invasor pode treinar um modelo substituto entre os dados de entrada usados ‚Äã‚Äãpara gerar as previs√µes recebidas e as pr√≥prias previs√µes.  Dependendo do n√∫mero de previs√µes que podem ser recebidas, o modelo substituto pode se tornar uma simula√ß√£o bastante precisa do seu modelo.  Depois de treinar o modelo substituto, o atacante ter√° uma "caixa de areia" da qual ele poder√° planejar a personifica√ß√£o (por exemplo, "imita√ß√£o") ou um ataque com um exemplo competitivo da integridade do seu modelo ou obter o potencial de come√ßar a recuperar alguns aspectos dos dados confidenciais de treinamento.  Os modelos substitutos tamb√©m podem ser treinados usando fontes de dados externas que s√£o de alguma forma consistentes com suas previs√µes, como, por exemplo, o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ProPublica</a> fez com o modelo de reincid√™ncia do autor do COMPAS. <br><br>  Para proteger seu modelo da invers√£o usando um modelo substituto, voc√™ pode confiar nessas abordagens: <br><br><ul><li>  <b>Acesso autorizado</b> .  Solicite autentica√ß√£o adicional (por exemplo, dois fatores) para obter uma previs√£o. <br></li><li>  <b>Previs√µes do acelerador</b>  Limite um grande n√∫mero de previs√µes r√°pidas de usu√°rios individuais;  considere a possibilidade de aumentar artificialmente os atrasos nas previs√µes. <br></li><li>  <b>Modelos substitutos "brancos" (chap√©u branco)</b> .  Como exerc√≠cio de hacker branco, tente o seguinte: treine seus pr√≥prios modelos substitutos entre suas previs√µes de entrada e modelo para um aplicativo de produ√ß√£o e observe cuidadosamente os seguintes aspectos: <br><ul><li>  limites de precis√£o de v√°rios tipos de modelos substitutos ‚Äúbrancos‚Äù;  tente entender at√© que ponto o modelo substituto pode realmente ser usado para obter dados indesejados sobre o seu modelo. <br></li><li>  tipos de tend√™ncias de dados que podem ser aprendidas com seu modelo substituto ‚Äúbranco‚Äù, por exemplo, tend√™ncias lineares representadas por coeficientes de modelo linear. <br></li><li>  tipos de segmentos ou distribui√ß√µes demogr√°ficas que podem ser estudados analisando o n√∫mero de pessoas designadas para determinados n√≥s da √°rvore de decis√£o substituta ‚Äúbranca‚Äù. <br></li><li>  as regras que podem ser aprendidas da √°rvore de decis√£o substituta ‚Äúbranca‚Äù, por exemplo, como representar com precis√£o uma pessoa que receber√° uma previs√£o positiva. <br></li></ul><br></li></ul><br><h2>  4. Ataques de rivalidade </h2><br>  Em teoria, um hacker dedicado pode aprender - digamos, tentativa e erro (‚Äúintelig√™ncia‚Äù ou ‚Äúan√°lise de sensibilidade‚Äù) - inverter um modelo substituto ou engenharia social, como jogar com seu modelo para obter o resultado desejado da previs√£o ou evitar o indesej√°vel previs√£o.  Tentar atingir esses objetivos usando uma cadeia de dados especialmente projetada √© chamado de ataque advers√°rio.  (√†s vezes um ataque para investigar a integridade).  Um invasor pode usar um ataque advers√°rio para obter um empr√©stimo grande ou um pr√™mio de seguro baixo, ou para evitar a nega√ß√£o da liberdade condicional com uma avalia√ß√£o alta do risco criminal.  Algumas pessoas chamam o uso de exemplos competitivos para excluir um resultado indesej√°vel de uma previs√£o como "evas√£o". <br><br>  Experimente os m√©todos descritos abaixo para defender ou detectar um ataque com um exemplo competitivo: <br><br><ul><li>  <b>An√°lise de ativa√ß√£o</b> .  A an√°lise de ativa√ß√£o requer mecanismos internos comparativos em seus modelos preditivos, por exemplo, a ativa√ß√£o m√©dia de neur√¥nios em sua rede neural ou a propor√ß√£o de observa√ß√µes relacionadas a cada n√≥ final em sua floresta aleat√≥ria.  Em seguida, voc√™ compara essas informa√ß√µes com o comportamento do modelo com fluxos de dados recebidos reais.  Como um dos meus colegas disse: " <i>√â o mesmo que ver um n√≥ final em uma floresta aleat√≥ria, o que corresponde a 0,1% dos dados de treinamento, mas √© adequado para 75% das linhas de pontua√ß√£o por hora</i> ". <br></li><li>  <b>Detec√ß√£o de anomalias</b> .  veja a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 2</a> . <br></li><li>  <b>Acesso autorizado</b> .  veja a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 3</a> . <br></li><li>  <b>Modelos comparativos</b> .  Ao avaliar novos dados, al√©m de um modelo mais complexo, use um modelo comparativo de alta transpar√™ncia.  Modelos interpretados s√£o mais dif√≠ceis de decifrar porque seus mecanismos s√£o transparentes.  Ao avaliar novos dados, compare o novo modelo com um modelo transparente confi√°vel ou com um modelo treinado em dados verificados e em um processo confi√°vel.  Se a diferen√ßa entre o modelo mais complexo e opaco e o interpretado (ou verificado) for muito grande, retorne √†s previs√µes conservadoras do modelo ou processe a linha de dados manualmente.  Registre este incidente, pode ser um ataque com um exemplo competitivo. <br></li><li>  <b>Previs√µes do acelerador</b> : consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 3</a> . <br></li><li>  <b>An√°lise de sensibilidade "branca"</b> .  Use a an√°lise de sensibilidade para conduzir seus pr√≥prios ataques de pesquisa para entender quais valores vari√°veis ‚Äã‚Äã(ou combina√ß√µes deles) podem causar grandes flutua√ß√µes nas previs√µes.  Procure esses valores ou combina√ß√µes de valores ao avaliar novos dados.  Para conduzir uma an√°lise de pesquisa "branca", voc√™ pode usar o pacote de c√≥digo aberto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cleverhans</a> . <br></li><li>  Modelos substitutos brancos: consulte a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 3</a> . <br></li></ul><br>  A an√°lise de ativa√ß√£o ou modelos comparativos podem ser usados ‚Äã‚Äãdurante o treinamento e como parte do monitoramento em tempo real dos modelos. <br><br><h2>  5. Personifica√ß√£o </h2><br>  Um hacker dedicado pode descobrir - novamente, por tentativa e erro, por invers√£o com um modelo substituto ou engenharia social - quais dados de entrada ou pessoas espec√≠ficas obt√™m o resultado desejado da previs√£o.  Um invasor pode se passar por essa pessoa para se beneficiar da previs√£o.  Os ataques de personifica√ß√£o s√£o chamados √†s vezes de ataques "simulados" e, do ponto de vista do modelo, isso lembra o roubo de identidade.  Como no caso de um ataque de exemplo competitivo, com a personifica√ß√£o, os dados de entrada s√£o alterados artificialmente de acordo com o seu modelo.  Por√©m, diferentemente do mesmo ataque de um exemplo competitivo, no qual uma combina√ß√£o de valores potencialmente aleat√≥ria pode ser usada para enganar, na personifica√ß√£o, para obter a previs√£o associada a esse tipo de objeto, informa√ß√µes associadas a outro objeto modelado (por exemplo, um cliente condenado , funcion√°rio, transa√ß√£o financeira, paciente, produto etc.).  Suponha que um invasor possa descobrir de quais caracter√≠sticas do seu modelo depende o fornecimento de grandes descontos ou benef√≠cios.  Ent√£o ele pode falsificar as informa√ß√µes que voc√™ usa para obter esse desconto.  Um invasor pode compartilhar sua estrat√©gia com outras pessoas, o que pode levar a grandes perdas para sua empresa. <br><br>  Se voc√™ estiver usando um modelo de dois est√°gios, cuidado com um ataque "al√©rgico": um invasor pode simular uma sequ√™ncia de dados de entrada normais para o primeiro est√°gio do seu modelo para atacar o segundo est√°gio. <br><br>  As abordagens de prote√ß√£o e especialistas para ataques com personifica√ß√£o podem incluir: <br><br><ul><li>  An√°lise de ativa√ß√£o.  veja a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 4</a> . <br></li><li>  Acesso autorizado.  veja a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">se√ß√£o 3</a> . <br></li><li>  Verifique se h√° duplicatas.  No est√°gio de pontua√ß√£o, acompanhe o n√∫mero de registros semelhantes para os quais seu modelo est√° dispon√≠vel.  Isso pode ser feito em um espa√ßo dimensional reduzido usando autocoders, dimensionamento multidimensional (MDS) ou m√©todos de redu√ß√£o dimensional semelhantes.  Se houver muitas linhas semelhantes em um determinado per√≠odo de tempo, tome medidas corretivas. <br></li><li> Recursos de notifica√ß√£o de amea√ßas.  Salve a fun√ß√£o <code>num_similar_queries</code> no seu pipeline, que pode ser in√∫til imediatamente ap√≥s o treinamento ou a implementa√ß√£o do modelo, mas pode ser usada durante a avalia√ß√£o (ou durante a reciclagem futura) para notificar o modelo ou o pipeline de amea√ßas.  Por exemplo, se no momento da classifica√ß√£o, o valor de <code>num_similar_queries</code> maior que zero, a solicita√ß√£o de avalia√ß√£o poder√° ser enviada para an√°lise manual.  No futuro, quando voc√™ treinar novamente o modelo, poder√° ensin√°-lo a produzir resultados negativos de previs√£o para linhas de entrada com alta <code>num_similar_queries</code> . <br></li></ul><br>  A an√°lise de ativa√ß√£o, a verifica√ß√£o duplicada e a notifica√ß√£o de poss√≠veis amea√ßas podem ser usadas durante o treinamento e no monitoramento de modelos em tempo real. <br><br><h2>  6. Problemas comuns </h2><br>  Alguns usos comuns de aprendizado de m√°quina tamb√©m apresentam problemas de seguran√ßa mais gerais. <br><br>  <b>Caixas pretas e complexidade desnecess√°ria</b> .  Embora avan√ßos recentes em modelos interpretados e explica√ß√µes sobre modelos tornem poss√≠vel o uso de classificadores e regressores n√£o lineares precisos e transparentes, muitos processos de aprendizado de m√°quina continuam focados nos modelos de caixa preta.  Eles s√£o apenas um tipo de complexidade frequentemente desnecess√°ria no fluxo de trabalho padr√£o do aprendizado de m√°quina comercial.  Outros exemplos de complexidade potencialmente prejudicial podem ser especifica√ß√µes excessivamente ex√≥ticas ou um grande n√∫mero de depend√™ncias de pacotes.  Isso pode ser um problema por pelo menos dois motivos: <br><br><ol><li>  Um hacker persistente e motivado pode aprender mais sobre o seu sistema de simula√ß√£o de caixa preta excessivamente complexo do que voc√™ ou sua equipe (especialmente no mercado superaquecido e em r√°pida mudan√ßa de hoje para "analisar" dados).  Para isso, um invasor pode usar muitos novos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">m√©todos de explica√ß√£o</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">independentes do</a> modelo e uma an√°lise de sensibilidade cl√°ssica, al√©m de muitas outras ferramentas de hackers mais comuns.  Esse desequil√≠brio de conhecimento pode ser potencialmente usado para realizar os ataques descritos nas se√ß√µes 1-5 ou para outros tipos de ataques ainda desconhecidos. <br></li><li>  O aprendizado de m√°quina em ambientes de pesquisa e desenvolvimento depende fortemente de um ecossistema diversificado de pacotes de software de c√≥digo aberto.  Alguns desses pacotes t√™m muitos participantes e usu√°rios, outros s√£o altamente especializados e s√£o necess√°rios por um pequeno c√≠rculo de pesquisadores e profissionais.  Sabe-se que muitos pacotes s√£o suportados por estat√≠sticos brilhantes e pesquisadores de aprendizado de m√°quina que se concentram em matem√°tica ou algoritmos, em vez de engenharia de software e certamente n√£o em seguran√ßa.  H√° casos frequentes em que o pipeline de aprendizado de m√°quina depende de dezenas ou mesmo centenas de pacotes externos, cada um dos quais pode ser invadido para ocultar uma "carga √∫til" maliciosa. <br></li></ol><br>  <b>Sistemas e modelos distribu√≠dos</b> .  Felizmente ou infelizmente, vivemos em uma era de grande volume de dados.  Atualmente, muitas organiza√ß√µes usam sistemas distribu√≠dos de processamento de dados e aprendizado de m√°quina.  A computa√ß√£o distribu√≠da pode ser um grande alvo para ataques de dentro ou de fora.  Os dados podem ser distorcidos apenas em um ou v√°rios n√≥s de trabalho de um grande sistema distribu√≠do de armazenamento ou processamento de dados.  A porta traseira para marcas d'√°gua pode ser codificada em um modelo de um conjunto grande.  Em vez de depurar um conjunto ou modelo de dados simples, os profissionais devem agora estudar dados ou modelos espalhados por grandes agrupamentos de computa√ß√£o. <br><br>  <b>Ataques de nega√ß√£o de servi√ßo distribu√≠da (DDoS)</b> .  Se um servi√ßo de modelagem preditiva tiver um papel fundamental nas atividades da sua organiza√ß√£o, leve em considera√ß√£o pelo menos os ataques DDoS distribu√≠dos mais populares quando os atacantes atacarem um servi√ßo preditivo com um n√∫mero incrivelmente grande de solicita√ß√µes para atrasar ou parar de fazer previs√µes para usu√°rios leg√≠timos. <br><br><h2>  7. Decis√µes gerais </h2><br>  Voc√™ pode usar v√°rios m√©todos comuns, antigos e novos, mais eficazes para reduzir as vulnerabilidades do sistema de seguran√ßa e aumentar a justi√ßa, a controlabilidade, a transpar√™ncia e a confian√ßa nos sistemas de aprendizado de m√°quina. <br><br>  <b>Previs√£o de acesso autorizado e regula√ß√£o de frequ√™ncia (otimiza√ß√£o)</b> .  Recursos de seguran√ßa padr√£o, como autentica√ß√£o adicional e ajuste de frequ√™ncia de previs√£o, podem ser muito eficazes no bloqueio de v√°rios vetores de ataque descritos nas se√ß√µes 1-5. <br><br>  <b>Modelos comparativos</b> .  Como modelo comparativo para determinar se alguma manipula√ß√£o foi feita com a previs√£o, voc√™ pode usar o pipeline de modelagem antigo e comprovado ou outra ferramenta de previs√£o interpretada com alta transpar√™ncia.  A manipula√ß√£o inclui corrup√ß√£o de dados, ataques de marcas d'√°gua ou exemplos competitivos.  Se a diferen√ßa entre a previs√£o do seu modelo testado e a previs√£o de um modelo mais complexo e opaco for muito grande, anote esses casos.  Envie-os para analistas ou tome outras medidas para analisar ou corrigir a situa√ß√£o.  Devem ser tomadas precau√ß√µes s√©rias para garantir que seu benchmark e transportador permane√ßam seguros e inalterados em rela√ß√£o √† sua condi√ß√£o original e confi√°vel. <br><br>  <b>Modelos interpretados, justos ou privados</b> .  Atualmente, existem m√©todos (por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GBM mon√≥tonos (M-GBM),</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">listas de regras bayesianas escalon√°veis ‚Äã‚Äã(SBRLs)</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">explica√ß√µes de redes neurais (XNNs)</a> ) que fornecem precis√£o e interpretabilidade.  Esses modelos precisos e interpret√°veis ‚Äã‚Äãs√£o mais f√°ceis de documentar e depurar do que as caixas-pretas cl√°ssicas de aprendizado de m√°quina.  Tipos mais recentes de modelos justos e privados (por exemplo, LFR, PATE) tamb√©m podem ser treinados em como prestar menos aten√ß√£o √†s caracter√≠sticas demogr√°ficas vis√≠veis externamente dispon√≠veis para observa√ß√£o, usando engenharia social durante um ataque com um exemplo competitivo, ou impersonaliza√ß√£o.  Voc√™ est√° pensando em criar um novo processo de aprendizado de m√°quina no futuro?  Considere constru√≠-lo com base em modelos privados ou justos interpretados menos arriscados.  Eles s√£o mais f√°ceis de depurar e potencialmente resistentes a altera√ß√µes nas caracter√≠sticas de objetos individuais. <br><br>  <b>Depurando um modelo para seguran√ßa</b> .  Uma nova √°rea para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">modelos de depura√ß√£o √©</a> dedicada √† detec√ß√£o de erros nos mecanismos e previs√µes de modelos de aprendizado de m√°quina e sua corre√ß√£o.  As ferramentas de depura√ß√£o, como modelos substitutos, an√°lise residual e an√°lise de sensibilidade, podem ser usadas em testes em branco para identificar suas vulnerabilidades ou em exerc√≠cios anal√≠ticos para identificar poss√≠veis ataques que possam ou possam ocorrer. <br><br>  <b>Documenta√ß√£o do modelo e m√©todos de explica√ß√£o</b> .  A documenta√ß√£o do modelo √© uma estrat√©gia de redu√ß√£o de risco que tem sido usada no setor banc√°rio h√° d√©cadas.  Ele permite salvar e transferir conhecimento sobre sistemas de modelagem complexos √† medida que a composi√ß√£o dos propriet√°rios do modelo muda.  A documenta√ß√£o tem sido tradicionalmente usada para modelos lineares de alta transpar√™ncia.  Mas com o advento de ferramentas poderosas e precisas de explica√ß√£o (como a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√°rvore SHAP</a> e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atributos baseados em</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">deriva√ß√µes</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">de fun√ß√µes locais</a> para redes neurais), os fluxos de trabalho preexistentes do modelo de caixa preta podem ser pelo menos um pouco explicados, depurados e documentados.  Obviamente, a documenta√ß√£o agora deve incluir todos os objetivos de seguran√ßa, incluindo vulnerabilidades conhecidas, corrigidas ou esperadas. <br><br>  <b>Monitore e gerencie modelos diretamente por motivos de seguran√ßa</b> .  Profissionais s√©rios entendem que a maioria dos modelos √© treinada em "instant√¢neos" est√°ticos da realidade na forma de conjuntos de dados e que em tempo real a precis√£o das previs√µes diminui, pois o estado atual das coisas est√° se afastando das informa√ß√µes coletadas anteriormente.  Hoje, o monitoramento da maioria dos modelos visa identificar tal vi√©s na distribui√ß√£o das vari√°veis ‚Äã‚Äãde entrada, o que, em √∫ltima an√°lise, levar√° a uma diminui√ß√£o na precis√£o.  O monitoramento do modelo deve ser projetado para rastrear os ataques descritos nas se√ß√µes 1-5 e quaisquer outras amea√ßas em potencial que surgem ao depurar seu modelo.  Embora nem sempre isso esteja diretamente relacionado √† seguran√ßa, os modelos tamb√©m devem ser avaliados em tempo real para efeitos diferenciados.  Juntamente com a documenta√ß√£o do modelo, todos os artefatos de modelagem, c√≥digo fonte e metadados associados devem ser gerenciados, com vers√£o e verificados quanto √† seguran√ßa, bem como os ativos comerciais valiosos que s√£o. <br><br>  <b>Recursos de notifica√ß√£o de amea√ßas</b> .  Fun√ß√µes, regras e etapas do processamento preliminar ou subsequente podem ser inclu√≠das em seus modelos ou processos equipados com meios de notifica√ß√£o de poss√≠veis amea√ßas: por exemplo, o n√∫mero de linhas semelhantes no modelo;  se a linha atual representa um funcion√°rio, contratado ou consultor;  Os valores na linha atual s√£o semelhantes aos obtidos com ataques brancos com um exemplo competitivo?  Essas fun√ß√µes podem ou n√£o ser necess√°rias durante o primeiro treinamento do modelo.  Mas economizar espa√ßo para eles pode um dia ser muito √∫til na avalia√ß√£o de novos dados ou na subsequente reciclagem do modelo. <br><br>  <b>Detec√ß√£o de anormalidades do sistema</b> .  Treine o metamodo para detectar anomalias com base em um autocoder nas estat√≠sticas operacionais de todo o seu sistema de modelagem preditiva (o n√∫mero de previs√µes para um determinado per√≠odo de tempo, atrasos, CPU, mem√≥ria e carregamento de disco, n√∫mero de usu√°rios simult√¢neos etc.) e monitore cuidadosamente esse metamodelo para anomalias.  Uma anomalia pode dizer se algo der errado.  Investiga√ß√µes de acompanhamento ou mecanismos especiais ser√£o necess√°rios para rastrear com precis√£o a causa do problema. <br><br><h2>  8. Refer√™ncias e informa√ß√µes para leitura adicional </h2><br>  Uma grande quantidade de literatura acad√™mica moderna sobre seguran√ßa de aprendizado de m√°quina se concentra no aprendizado adapt√°vel, no aprendizado profundo e na criptografia.  No entanto, at√© agora o autor n√£o conhece os praticantes que realmente fariam tudo isso.  Portanto, al√©m de artigos e postagens publicados recentemente, apresentamos artigos dos anos 90 e in√≠cio dos anos 2000 sobre viola√ß√µes de rede, detec√ß√£o de v√≠rus, filtragem de spam e t√≥picos relacionados, que tamb√©m foram fontes √∫teis.  Se voc√™ quiser aprender mais sobre o fascinante t√≥pico de proteger modelos de aprendizado de m√°quina, aqui est√£o os principais links - do passado e do presente - que foram usados ‚Äã‚Äãpara escrever a postagem. <br><br><ul><li>  Bareno, Marco et al., Machine Learning Safety.  Machine Learning 81.2 (2010): 121-148.  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://people.eecs.berkeley.edu/ <sub>~</sub> adj / publish / paper-files / SecML-MLJ2010.pdf</a> <br></li><li>  Kumar, Agites.  "Ataques de seguran√ßa: uma an√°lise de modelos de aprendizado de m√°quina".  DZone (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://dzone.com/articles/security-attacks-analysis-of-machine-learning-mode</a> <br></li><li>  Lorica, Ben e Lucidis, Mike.  ‚ÄúVoc√™ criou um aplicativo de aprendizado de m√°quina.  Agora verifique se √© seguro.  Ideias O'Reilly (2019).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure</a> <br></li><li>  Paperno, Nicholas.  ‚ÄúMapa dos marotos da seguran√ßa e da privacidade no aprendizado de m√°quina: uma revis√£o das tend√™ncias atuais e futuras da pesquisa para obter seguran√ßa e confidencialidade do aprendizado de m√°quina‚Äù  Anais do 11¬∫ Workshop da ACM sobre Intelig√™ncia Artificial e Seguran√ßa.  ACM (2018).  URL  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">https://arxiv.org/pdf/1811.01134.pdf</a> <br></li></ul><br><h2>  Conclus√£o </h2><br>  Aqueles que se preocupam com a ci√™ncia e a pr√°tica do aprendizado de m√°quina est√£o preocupados com o fato de que a amea√ßa de hackers com o aprendizado de m√°quina, juntamente com as crescentes amea√ßas de quebra de confidencialidade e discrimina√ß√£o algor√≠tmica, podem aumentar o crescente ceticismo p√∫blico e pol√≠tico sobre aprendizado de m√°quina e intelig√™ncia artificial.  Todos n√≥s precisamos lembrar os tempos dif√≠ceis para a IA no passado recente.  Vulnerabilidades de seguran√ßa, viola√ß√µes de privacidade e discrimina√ß√£o algor√≠tmica podem potencialmente ser combinadas, levando a um financiamento reduzido para pesquisas no campo do treinamento em inform√°tica ou a medidas draconianas para regular essa √°rea.  Vamos continuar a discuss√£o e resolu√ß√£o dessas quest√µes importantes, a fim de evitar uma crise e n√£o atrapalhar suas conseq√º√™ncias. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt458892/">https://habr.com/ru/post/pt458892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt458882/index.html">Falar em p√∫blico. Brevemente sobre o principal</a></li>
<li><a href="../pt458884/index.html">Um pouco sobre os padr√µes de comunica√ß√£o espacial</a></li>
<li><a href="../pt458886/index.html">Mais √∫teis Mail.ru Design Conf √ó Dribbble Meetup 2019 Papers por True Engineering</a></li>
<li><a href="../pt458888/index.html">Summer Droid Meetup</a></li>
<li><a href="../pt458890/index.html">Precis√£o de amostragem e c√°lculo</a></li>
<li><a href="../pt458894/index.html">Pessoas t√≠picas e as redes em que vivem</a></li>
<li><a href="../pt458896/index.html">JavaScript funcional: o que s√£o fun√ß√µes de ordem superior e por que s√£o necess√°rias?</a></li>
<li><a href="../pt458900/index.html">Cartuchos de console como modems</a></li>
<li><a href="../pt458902/index.html">5 erros comuns em Python para iniciantes</a></li>
<li><a href="../pt458904/index.html">Visualiza√ß√£o do n√∫mero de vit√≥rias para equipes da NBA usando gr√°ficos de barras animados em R</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>