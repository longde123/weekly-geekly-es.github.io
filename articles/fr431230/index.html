<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèø‚Äçüíª ‚Ü™Ô∏è üé≤ Stockage pour infrastructure HPC, ou comment nous avons collect√© un stockage de 65 PB au RIKEN Japan Research Center üò≥ üèúÔ∏è üè°</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 L'ann√©e derni√®re, la plus grande installation de stockage bas√©e sur RAIDIX du moment a √©t√© mise en ≈ìuvre. Un syst√®me de 11 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Stockage pour infrastructure HPC, ou comment nous avons collect√© un stockage de 65 PB au RIKEN Japan Research Center</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raidix/blog/431230/"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  L'ann√©e derni√®re, la plus grande installation de stockage bas√©e sur RAIDIX du moment a √©t√© mise en ≈ìuvre.  Un syst√®me de 11 clusters de basculement a √©t√© d√©ploy√© √† l'Institut RIKEN des sciences informatiques (Japon).  Le principal objectif du syst√®me est le stockage de l'infrastructure HPC (HPCI), qui est mis en ≈ìuvre dans le cadre de l'√©change acad√©mique √† grande √©chelle d'informations acad√©miques Academic Cloud (bas√© sur le r√©seau SINET). <br><br>  Une caract√©ristique importante de ce projet est son volume total de 65 PB, dont le volume utilisable du syst√®me est de 51,4 PB.  Pour mieux comprendre cette valeur, nous ajoutons qu'il s'agit de 6512 disques de 10 To chacun (les plus modernes au moment de l'installation).  C‚Äôest beaucoup. <br><a name="habracut"></a><br>  Les travaux sur le projet se sont poursuivis tout au long de l'ann√©e, apr√®s quoi le suivi de la stabilit√© du syst√®me s'est poursuivi pendant environ un an.  Les indicateurs obtenus r√©pondaient aux exigences √©nonc√©es, et maintenant nous pouvons parler du succ√®s de ce record et du projet significatif pour nous. <br><br><h2>  Supercalculateur au RIKEN Institute Computing Center </h2><br>  Pour l'industrie des TIC, le RIKEN Institute est principalement connu pour son l√©gendaire ¬´K-computer¬ª (du japonais ¬´kei¬ª, qui signifie 10 quadrillions), qui, au moment de son lancement (juin 2011) √©tait consid√©r√© comme le supercalculateur le plus puissant au monde. <br><br><div class="spoiler">  <b class="spoiler_title">En savoir plus sur l'ordinateur K</b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  Le supercalculateur aide le Center for Computational Sciences dans la mise en ≈ìuvre de recherches complexes √† grande √©chelle: il permet de mod√©liser le climat, les conditions m√©t√©orologiques et le comportement mol√©culaire, de calculer et d'analyser les r√©actions en physique nucl√©aire, la pr√©vision des s√©ismes, et bien plus encore.  Les capacit√©s de superordinateurs sont √©galement utilis√©es pour des recherches plus ¬´quotidiennes¬ª et appliqu√©es - pour rechercher des gisements de p√©trole et pr√©voir les tendances des march√©s boursiers. <br><br>  De tels calculs et exp√©riences g√©n√®rent une √©norme quantit√© de donn√©es, dont la valeur et la signification ne peuvent √™tre surestim√©es.  Pour en tirer le meilleur parti, les scientifiques japonais ont d√©velopp√© le concept d'un espace d'information unique dans lequel les professionnels du HPC de diff√©rents centres de recherche auront acc√®s aux ressources HPC re√ßues. <br><br><h2>  Infrastructure de calcul haute performance (HPCI) </h2><br>  HPCI fonctionne sur la base de SINET (The Science Information Network), un r√©seau de base pour l'√©change de donn√©es scientifiques entre les universit√©s japonaises et les centres de recherche.  Actuellement, SINET rassemble environ 850 instituts et universit√©s, cr√©ant d'√©normes opportunit√©s d'√©change d'informations dans la recherche qui affecte la physique nucl√©aire, l'astronomie, la g√©od√©sie, la sismologie et l'informatique. <br><br>  HPCI est un projet d'infrastructure unique qui forme un syst√®me d'√©change d'informations unifi√© dans le domaine du calcul haute performance entre les universit√©s et les centres de recherche au Japon. <br><br>  En combinant les capacit√©s du supercalculateur ¬´K¬ª et d'autres centres de recherche sous une forme accessible, la communaut√© scientifique re√ßoit des avantages √©vidents pour travailler avec des donn√©es pr√©cieuses cr√©√©es par le calcul supercalculateur. <br><br>  Afin de fournir un acc√®s utilisateur conjoint efficace √† l'environnement HPCI, des exigences √©lev√©es ont √©t√© impos√©es au stockage pour la vitesse d'acc√®s.  Et gr√¢ce √† ¬´l'hyperproductivit√©¬ª du K-computer, le cluster de stockage du Center for Computational Sciences de l'Institut RIKEN a √©t√© calcul√© pour √™tre cr√©√© avec un volume de travail d'au moins 50 PB. <br><br>  L'infrastructure du projet HPCI a √©t√© construite sur la base du syst√®me de fichiers Gfarm, qui a permis de fournir un haut niveau de performances et de combiner des clusters de stockage disparates en un seul espace de partage. <br><br><h2>  Syst√®me de fichiers Gfarm </h2><br>  Gfarm est un syst√®me de fichiers distribu√©s open source d√©velopp√© par des ing√©nieurs japonais.  Gfarm est le fruit du d√©veloppement de l'Institute for Advanced Industrial Science and Technology (AIST), et le nom du syst√®me fait r√©f√©rence √† l'architecture utilis√©e par Grid Data Farm. <br><br>  Ce syst√®me de fichiers combine un certain nombre de propri√©t√©s apparemment incompatibles: <br><br><ul><li>  Haute √©volutivit√© en volume et en performances </li><li>  Distribution de r√©seau longue distance avec prise en charge d'un espace de noms unique pour plusieurs centres de recherche divers </li><li>  Prise en charge de l'API POSIX </li><li>  Haute performance requise pour le calcul parall√®le </li><li>  S√©curit√© du stockage des donn√©es </li></ul><br>  Gfarm cr√©e un syst√®me de fichiers virtuel en utilisant les ressources de stockage de plusieurs serveurs.  Les donn√©es sont distribu√©es par le serveur de m√©tadonn√©es et le sch√©ma de distribution lui-m√™me est cach√© aux utilisateurs.  Je dois dire que Gfarm se compose non seulement d'un cluster de stockage, mais √©galement d'une grille de calcul qui utilise les ressources des m√™mes serveurs.  Le principe de fonctionnement du syst√®me ressemble √† Hadoop: le travail soumis est ¬´abaiss√©¬ª au n≈ìud o√π se trouvent les donn√©es. <br><br>  L'architecture du syst√®me de fichiers est asym√©trique.  Les r√¥les sont clairement attribu√©s: serveur de stockage, serveur de m√©tadonn√©es, client.  Mais en m√™me temps, les trois r√¥les peuvent √™tre ex√©cut√©s par la m√™me machine.  Les serveurs de stockage stockent de nombreuses copies de fichiers et les serveurs de m√©tadonn√©es fonctionnent en mode ma√Ætre-esclave. <br><br><h2>  Travail de projet </h2><br>  Core Micro Systems, partenaire strat√©gique et fournisseur exclusif de RAIDIX au Japon, a impl√©ment√© la mise en ≈ìuvre au RIKEN Institute of Computing Sciences Center.  Pour mettre en ≈ìuvre le projet, il a fallu environ 12 mois de travail minutieux, auxquels non seulement les employ√©s de Core Micro Systems, mais aussi les sp√©cialistes techniques de l'√©quipe Reydix ont pris une part active. <br><br>  Dans le m√™me temps, la transition vers un autre syst√®me de stockage semblait peu probable: le syst√®me existant avait beaucoup de fixations techniques, ce qui a compliqu√© la transition vers une nouvelle marque. <br><br>  Au cours de longs tests, de v√©rifications et d'am√©liorations, RAIDIX a d√©montr√© des performances et une efficacit√© constamment √©lev√©es lors de l'utilisation d'une quantit√© de donn√©es aussi impressionnante. <br><br>  √Ä propos des am√©liorations, il vaut la peine d'en dire un peu plus.  Il fallait non seulement cr√©er l'int√©gration des syst√®mes de stockage avec le syst√®me de fichiers Gfarm, mais aussi √©tendre certaines caract√©ristiques fonctionnelles du logiciel.  Par exemple, afin de r√©pondre aux exigences √©tablies des sp√©cifications techniques, il √©tait n√©cessaire de d√©velopper et de mettre en ≈ìuvre la technologie de l'√©criture automatique d√®s que possible. <br><br>  Le d√©ploiement du syst√®me lui-m√™me a √©t√© syst√©matique.  Les ing√©nieurs de Core Micro Systems ont men√© avec soin et pr√©cision chaque √©tape du test, augmentant progressivement l'√©chelle du syst√®me. <br><br>  En ao√ªt 2017, la premi√®re phase de d√©ploiement s'est termin√©e lorsque le volume du syst√®me a atteint 18 PB.  En octobre de la m√™me ann√©e, la deuxi√®me phase a √©t√© mise en ≈ìuvre, dans laquelle le volume a atteint un record de 51 PB. <br><br><h2>  Architecture de la solution </h2><br>  La solution a √©t√© cr√©√©e gr√¢ce √† l'int√©gration des syst√®mes de stockage RAIDIX et du syst√®me de fichiers distribu√© Gfarm.  En collaboration avec Gfarm, la possibilit√© de cr√©er un stockage √©volutif √† l'aide de 11 syst√®mes RAIDIX √† double contr√¥leur. <br><br>  La connexion aux serveurs Gfarm se fait via 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Image d'un cluster avec un serveur de donn√©es distinct pour chaque n≈ìud</font></i> <br><br>  (1) connexions maill√©es SAN 48 Gbit / s √ó 8;  bande passante: 384 Gbps <br>  (2) connexions 48Gbps √ó 40 Mesh FABRIC;  bande passante: 1920 Gbps <br><br><h3>  Configuration de plate-forme √† double contr√¥leur </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4 pi√®ces </td></tr><tr><td>  Carte m√®re </td><td>  Compatible avec le mod√®le de processeur prenant en charge PCI Express 3.0 x8 / x16 </td></tr><tr><td>  Cache interne </td><td>  256 Go pour chaque n≈ìud </td></tr><tr><td>  Ch√¢ssis </td><td>  2U </td></tr><tr><td>  Contr√¥leurs SAS pour connecter les √©tag√®res de disques, les serveurs et la synchronisation du cache d'√©criture </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  Disque dur </td><td>  Disque dur SAS HGST Helium 10 To </td></tr><tr><td>  Synchronisation du rythme cardiaque </td><td>  Ethernet 1 GbE </td></tr><tr><td>  CacheSync Sync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Les deux n≈ìuds du cluster de basculement sont connect√©s √† 10 JBOD (60 disques de 10 To chacun) via 20 ports SAS 12G pour chaque n≈ìud.  Sur ces √©tag√®res de disques, 58 matrices RAID6 de 10 To ont √©t√© cr√©√©es (8 disques de donn√©es (D) + 2 disques de parit√© (P)) et 12 disques ont √©t√© allou√©s pour le ¬´remplacement √† chaud¬ª. <br><br>  10 JBOD =&gt; 58 √ó RAID6 (8 disques de donn√©es (D) + 2 disques de parit√© (P)), LUN √† partir de 580 HDD + 12 HDD pour ¬´hot swap¬ª (2,06% du volume total) <br><br>  592 HDD (10 To SAS / 7,2 k HDD) par cluster * HDD: HGST (MTBF: 2 500 000 heures) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. Cluster de basculement avec sch√©ma de connexion 10 JBOD</font></i> <br><br><h3>  Syst√®me g√©n√©ral et sch√©ma de connexion </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Image d'un cluster unique dans le syst√®me HPCI</font></i> <br><br><h2>  Indicateurs cl√©s du projet </h2><br><blockquote>  Capacit√© utile par cluster: <b>4,64 PB</b> ((RAID6 / 8D + 2P) LUN √ó 58) <br><br>  La capacit√© totale utilisable de l'ensemble du syst√®me: <b>51,04 PB</b> (4,64 PB √ó 11 grappes). <br><br>  Capacit√© totale du syst√®me: <b>65 PB</b> . <br><br>  Les performances du syst√®me √©taient: <b>17 Go / s</b> en √©criture, <b>22 Go / s</b> en lecture. <br><br>  La performance totale du sous-syst√®me de disques du cluster sur 11 syst√®mes de stockage RAIDIX: <b>250 Go / s</b> . </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr431230/">https://habr.com/ru/post/fr431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr431218/index.html">Comment j'ai fait un jeu de bande dessin√©e Lovecraft</a></li>
<li><a href="../fr431220/index.html">Le regard d'un biologiste sur les racines de notre vieillissement</a></li>
<li><a href="../fr431222/index.html">Archivage de site Web</a></li>
<li><a href="../fr431226/index.html">Le jeu Snake pour FPGA Cyclone IV (avec joystick VGA et SPI)</a></li>
<li><a href="../fr431228/index.html">Obstacle Run for Light: cristaux liquides pour aider</a></li>
<li><a href="../fr431232/index.html">Nous g√©n√©rons de beaux espaces r√©serv√©s SVG sur Node.js</a></li>
<li><a href="../fr431234/index.html">11 d√©cembre, Moscou - Alfa JS MeetUp</a></li>
<li><a href="../fr431236/index.html">Comment √©crire sur Objective-C en 2018. Partie 1</a></li>
<li><a href="../fr431238/index.html">Le condens√© des √©v√©nements pour les professionnels RH dans le domaine de l'informatique pour d√©cembre 2018</a></li>
<li><a href="../fr431242/index.html">TLS et certificats Web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>