<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👃🏼 ⚽️ 🎵 反向传播方法介绍 🐿️ 🧜🏼 🚂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="大家好！ 新年假期即将结束，这意味着我们再次准备与您共享有用的材料。 预期将在“开发人员算法”课程上推出新的课程，因此准备了本文的翻译。 

 走吧 




 误差的反向传播方法可能是神经网络的最基本组成部分。 它最早是在1960年代描述的，近30年后，它在Rumelhart，Hinton和Wil...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>反向传播方法介绍</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/483466/">  <i>大家好！</i>  <i>新年假期即将结束，这意味着我们再次准备与您共享有用的材料。</i>  <i>预期将在<a href="https://otus.pw/h0mh/">“开发人员算法”</a>课程上推出新的课程，因此准备了本文的翻译。</i> <i><br><br></i>  <i>走吧</i> <br><br><img src="https://habrastorage.org/webt/kl/6d/cd/kl6dcdek8egee8jyp_p0_7hcz30.png"><br><br><hr><br> 误差的反向传播方法可能是神经网络的最基本组成部分。 它最早是在1960年代描述的，近30年后，它在Rumelhart，Hinton和Williams的题为<a href="https://www.nature.com/articles/323533a0">“通过反向传播的错误学习表示法”中</a>得到了推广。 <a name="habracut"></a><br><br> 该方法用于使用所谓的链规则（复杂函数的微分规则）有效地训练神经网络。 简而言之，在每次通过网络后，反向传播都会沿相反方向执行一次传递，并调整模型参数（权重和位移）。 <br><br> 在本文中，我想从数学的角度详细考虑学习和优化简单的4层神经网络的过程。 我相信这将有助于读者了解反向传播的工作原理，并了解其重要性。 <br><br><h3> 定义神经网络模型 </h3><br> 四层神经网络由输入层中的四个神经元，隐藏层中的四个神经元和输出层中的1个神经元组成。 <br><br><img src="https://habrastorage.org/webt/d3/1z/7q/d31z7q7wxug2d-435f_t1fi19ki.png"><br>  <i>四层神经网络的简单图像。</i> <br><br><h3> 输入层 </h3><br> 在图中，紫色神经元代表输入。 它们可以是简单的标量，也可以是更复杂的-向量或多维矩阵。 <br><br><img src="https://habrastorage.org/webt/su/ba/e1/subae1x6bn1yju51obgv3qaephm.png"><br>  <i>描述输入xi的方程式。</i> <br><br> 第一组激活（a）等于输入值。  “激活”是应用激活功能后神经元的值。 有关更多详细信息，请参见下文。 <br><br><h3> 隐藏层 </h3><br> 隐藏的神经元的最终值（绿色图中）是使用z <sup>l</sup> -I层中的加权输入和L层中的<sup>I</sup>激活来计算的。对于第2层和第3层，等式如下： <br><br> 对于l = 2： <br><br><img src="https://habrastorage.org/webt/wh/ix/ho/whixhogzr32hedjvb-rackpht1c.png"><br><br> 对于l = 3： <br><br><img src="https://habrastorage.org/webt/yv/eb/qq/yvebqquzvpxg3iu3xwqbhli-fpu.png"><br><br>  W <sup>2</sup>和W <sup>3</sup>是第2层和第3层的权重，b <sup>2</sup>和b <sup>3</sup>是这些层的偏移量。 <br><br> 使用激活函数f计算激活a <sup>2</sup>和a <sup>3</sup> 。 例如，此函数f是非线性的（如<a href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid</a> ， <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>和<a href="https://en.wikipedia.org/wiki/Hyperbolic_function">双曲正切</a> ），并允许网络研究数据中的复杂模式。 我们不会讨论激活功能的工作原理，但是如果您有兴趣，我强烈建议您阅读这篇精彩的<a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0">文章</a> 。 <br><br> 如果仔细观察，您会发现所有x，z <sup>2</sup> ，a <sup>2</sup> ，z <sup>3</sup> ，a <sup>3</sup> ，W <sup>1</sup> ，W <sup>2</sup> ，b <sup>1</sup>和b <sup>2</sup>都没有四层神经网络图中所示的下标。 事实是我们将所有参数值组合到按层分组的矩阵中。 这是使用神经网络的一种标准方法，并且非常舒适。 但是，我将仔细研究方程式，以免造成混淆。 <br><br> 让我们以第2层及其参数为例。 可以将相同的操作应用于神经网络的任何层。 <br>  W <sup>1</sup>是维数<i>（n，m）</i>的权重矩阵，其中<i>n</i>是输出神经元的数量（下一层的神经元）， <i>m</i>是输入神经元的数量（上一层的神经元）。 在我们的情况下， <i>n = 2</i>和<i>m = 4</i> 。 <br><br><img src="https://habrastorage.org/webt/ez/pw/6j/ezpw6j_huyb2cr5zkxinl9ylku8.png"><br><br> 在这里，任何权重的下标中的第一个数字对应于下一层的神经元索引（在我们的情况下，这是第二个隐藏层），第二个数字对应于上一层的神经元索引（在我们的情况下，这是输入层）。 <br><br>  <i>x</i>是维度（ <i>m</i> ，1）的输入向量，其中<i>m</i>是输入神经元的数量。 在我们的情况下， <i>m</i> = 4。 <br><br><img src="https://habrastorage.org/webt/5a/by/8a/5aby8acxjiohf0-f5jrmgbfbxsi.png"><br><br>  b <sup>1</sup>是维度（ <i>n</i> ，1）的位移向量，其中<i>n</i>是当前层中神经元的数量。 在我们的例子中， <i>n</i> = 2。 <br><br><img src="https://habrastorage.org/webt/2u/5t/9v/2u5t9vhiftmq9fou4khqqvkynhc.png"><br><br> 按照z <sup>2</sup>的等式<sup>，</sup>我们可以使用W <sup>1</sup> ，x和b <sup>1</sup>的上述定义来获得等式z <sup>2</sup> ： <br><br><img src="https://habrastorage.org/webt/-5/bz/kz/-5bzkzalwngzrkhbuwq52fugpkc.png"><br><br> 现在，请仔细看一下上面的神经网络的图示： <br><br><img src="https://habrastorage.org/webt/_e/sj/ld/_esjld_rfdemfxpuztceadijwns.png"><br><br> 如您所见，z <sup>2</sup>可以用z <sub>1</sub> <sup>2</sup>和z <sub>2</sub> <sup>2表示</sup> ，其中z <sub>1</sub> <sup>2</sup>和z <sub>2</sub> <sup>2</sup>是每个输入值x <sup>i</sup>与相应权重W <sub>ij</sub> <sup>1</sup>的乘积之和。 <br><br> 这导致z <sup>2的</sup>方程式相同，并证明矩阵表示z <sup>2</sup> ，a <sup>2</sup> ，z <sup>3</sup>和a <sup>3</sup>是正确的。 <br><br><h3> 输出层 </h3><br> 神经网络的最后一部分是输出层，它给出了预测值。 在我们的简单示例中，它以染成蓝色的单个神经元的形式表示，并计算如下： <br><br><img src="https://habrastorage.org/webt/fy/vh/05/fyvh05jvkxbosdqhaqak-vbzn0k.png"><br><br> 同样，我们使用矩阵表示法简化方程。 您可以使用上述方法来了解基本逻辑。 <br><br><h3> 直接分发和评估 </h3><br> 以上等式通过神经网络形成直接分布。 快速概述： <br><br><img src="https://habrastorage.org/webt/pe/ya/fr/peyafrffaxqvrnjeito3i-j-gpk.png"><br><br>  <i>（1）-输入层</i> <i><br></i>  <i>（2）-第一个隐藏层中神经元的值</i> <i><br></i>  <i>（3）-第一个隐藏层上的激活值</i> <i><br></i>  <i>（4）-第二个隐藏层中神经元的值</i> <i><br></i>  <i>（5）-第二隐藏级别的激活值</i> <i><br></i>  <i>（6）-输出层</i> <br><br> 直接通过的最后一步是相对于预期输出值<i>y</i>评估预测输出值<i>s</i> 。 <br><br> 输出y是训练数据集（x，y）的一部分，其中<i>x</i>是输入（如上一节所述）。 <br><br>  <i>s</i>和<i>y</i>之间<i>的</i>估计是通过损失函数进行的。 它可以简单地作为<a href="https://en.wikipedia.org/wiki/Mean_squared_error">标准误差</a> ，也可以复杂地作为<a href="http://neuralnetworksanddeeplearning.com/chap3.html">交叉熵</a> 。 <br><br> 我们将此损失函数称为C并表示如下： <br><br><img src="https://habrastorage.org/webt/eg/s7/wh/egs7whz63c-ryaazd_r-vvuxbae.png"><br><br> 其中<i>成本</i>可以等于标准误差，交叉熵或任何其他损失函数。 <br><br> 基于C的值，模型“知道”需要调整多少参数才能接近<i>y</i>的预期输出值。 使用反向传播方法会发生这种情况。 <br><br><h3> 误差的反向传播和梯度计算 </h3><br> 根据1989年的文章，反向传播方法： <br><br>  <i>不断调整网络中连接的权重，以最小化网络的实际输出矢量和所需输出矢量之间的差异</i> 。 <br> 和 <br>  <i>...使创建有用的新功能成为可能，从而将反向传播与较早和较简单的方法区分开...</i> <br><br> 换句话说，反向传播旨在通过调整网络的权重和偏移来使损耗函数最小化。 调整程度由损耗函数相对于这些参数的梯度确定。 <br><br> 一个问题出现了： <i>为什么要计算梯度</i> ？ <br><br> 要回答这个问题，我们首先需要修改一些计算概念： <br><br> 函数C（x <sup>1</sup> ，x <sup>2</sup> ，...，x <sup>m</sup> ）在x处的<a href="https://en.wikipedia.org/wiki/Partial_derivative">梯度是</a> C <i>相</i>对于<i>x</i> <a href="https://en.wikipedia.org/wiki/Partial_derivative">的偏导数</a>的<a href="https://en.wikipedia.org/wiki/Partial_derivative">向量</a> 。 <br><br><img src="https://habrastorage.org/webt/km/eo/zl/kmeozlylfgdy7nknsa0cq6ytaei.png"><br><br> 函数C的导数反映了相对于函数自变量<i>x</i> （ <a href="https://en.wikipedia.org/wiki/Derivative">输入值</a> ）变化对函数值（输出值）变化的敏感性。 换句话说，导数告诉我们C.向哪个方向移动。 <br><br> 梯度表示为了最小化C，有必要改变参数<i>x</i> （在正方向或负方向上）多少。 <br><br> 这些梯度是使用称为链<a href="https://en.wikipedia.org/wiki/Chain_rule">规则</a>的方法计算的。 <br> 对于一个权重（w <sup>jk</sup> ） <sub>l，</sub>梯度为： <br><br><img src="https://habrastorage.org/webt/y7/97/mu/y797mumguvia31hytpq6gzq3gvy.png"><br><br>  <i>（1）连锁规则</i> <i><br></i>  <i>（2）根据定义，m是每l-1层的神经元数量</i> <i><br></i>  <i>（3）导数计算</i> <i><br></i>  <i>（4）最终价值</i> <i><br></i>  <i>可以将一组相似的方程式应用于（b <sup>j</sup> ） <sub>l</sub></i> ： <br><br><img src="https://habrastorage.org/webt/oo/7_/gz/oo7_gzmr5wpgql73bxefnlfob5u.png"><br><br>  <i>（1）连锁规则</i> <i><br></i>  <i>（2）导数计算</i> <i><br></i>  <i>（3）最终价值</i> <br> 这两个方程的共同部分通常称为“局部梯度”，其表示如下： <br><br><img src="https://habrastorage.org/webt/k9/4a/nc/k94anc1xfk3sjjgk08qf9_48fam.png"><br><br> 使用链式规则可以轻松确定“局部梯度”。 我现在不会画这个过程。 <br><br> 渐变允许优化模型参数： <br><br> 在达到停止标准之前，将执行以下操作： <br><br><img src="https://habrastorage.org/webt/xw/31/1s/xw311s5zex1_sdlnvvucd9qqubk.png"><br><br>  <i>优化权重和偏移量的算法</i> （也称为梯度下降） <br><ul><li>  <i>w</i>和<i>b</i>的初始值是随机选择的。 </li><li>  Epsilon（e）是学习的速度。 它确定渐变的效果。 </li><li>  <i>w</i>和<i>b</i>是权重和偏移量的矩阵表示。 </li><li>  C关于<i>w</i>或<i>b的</i>导数可以使用C关于各个权重或偏移的偏导数来计算。 </li><li> 一旦损失函数最小化，则满足终止条件。 </li></ul><br><br> 我想将本节的最后部分放在一个简单的示例中，在该示例中，我们针对一个权重（w <sup>22</sup> ） <sub>2</sub>计算梯度C。 <br><br> 让我们放大上述神经网络的底部： <br><br><img src="https://habrastorage.org/webt/l7/0w/6d/l70w6d7hhxqjm0wqxtwoj8y8nxq.png"><br><br>  <i>神经网络中反向传播的可视化表示</i> <br> 权重（w <sup>22</sup> ） <sub>2</sub>连接（a <sup>2</sup> ） <sub>2</sub>和（z <sup>2</sup> ） <sub>2</sub> ，因此计算梯度需要在（z <sup>2</sup> ） <sub>3</sub>和（a <sup>2</sup> ） <sub>3</sub>上应用链式规则： <br><br><img src="https://habrastorage.org/webt/n_/mz/nm/n_mznmzn_dt1lqe-nyx7qoplcsa.png"><br><br> 从（a <sup>2</sup> ） <sub>3</sub>计算C的导数的最终值需要了解函数C。由于C取决于（a <sup>2</sup> ） <sub>3</sub> ，所以导数的计算应该很简单。 <br><br> 我希望这个例子能够对计算梯度的数学原理有所启发。 如果您想了解更多信息，我强烈建议您阅读Stanford NLP系列文章，Richard Socher为其中的反向传播提供了4种出色的解释。 <br><br><h3> 结束语 </h3><br> 在本文中，我详细说明了如何使用数学方法（例如计算梯度，链规则等）在后台进行错误的反向传播。 了解该算法的机制将增强您对神经网络的了解，并让您在使用更复杂的模型时感到自在。 祝您在深度学习过程中一切顺利！ <br><br>  <b><i>仅此而已。</i></b>  <b><i>我们邀请所有人参加主题为<a href="https://otus.pw/h0mh/">“细分树：简单快速”</a>的免费网络研讨会。</i></b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN483466/">https://habr.com/ru/post/zh-CN483466/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN483448/index.html">迪士尼-人类历史上最伟大的双向</a></li>
<li><a href="../zh-CN483454/index.html">在Atlassian Bitbucket中从Mercurial切换到GIT，并用西里尔文保存文件</a></li>
<li><a href="../zh-CN483458/index.html">GreenPig数据库助手</a></li>
<li><a href="../zh-CN483460/index.html">SQL HowTo：使用窗口函数构建链</a></li>
<li><a href="../zh-CN483462/index.html">闭嘴拿我的钱</a></li>
<li><a href="../zh-CN483468/index.html">Flutter集成测试-简单</a></li>
<li><a href="../zh-CN483470/index.html">有效地放置图块（Pro CSS，SVG，图案等）</a></li>
<li><a href="../zh-CN483472/index.html">删除所有内容：如何删除数据并将NVMe SSD恢复为出厂设置</a></li>
<li><a href="../zh-CN483476/index.html">机器人运输的道德：手推车的问题，风险和后果</a></li>
<li><a href="../zh-CN483478/index.html">太阳，风和水ver 0.1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>