<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🗝️ 🚭 👂🏼 Uso intuitivo dos métodos de Monte Carlo com cadeias de Markov 🔱 💅🏻 💞</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="É fácil? Eu tentei 
 Alexey Kuzmin, diretor de desenvolvimento de dados e trabalho da DomKlik, palestrante em ciência de dados na Netology, traduziu u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Uso intuitivo dos métodos de Monte Carlo com cadeias de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  É fácil?  Eu tentei </h4><br>  <i>Alexey Kuzmin, diretor de desenvolvimento de dados e trabalho da DomKlik, palestrante em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ciência de dados</a> na Netology, traduziu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um artigo de</a> Rahul Agarwal sobre como os métodos de Monte Carlo trabalham com cadeias de Markov para resolver problemas em um amplo espaço de estados.</i> <br><a name="habracut"></a><br>  Todo mundo associado à Data Science já ouviu falar dos métodos de Monte Carlo com cadeias de Markov (MCMCs).  Às vezes, o tópico é abordado ao estudar as estatísticas bayesianas, às vezes ao trabalhar com ferramentas como o Profeta. <br><br>  Mas o MCMC é difícil de entender.  Toda vez que li sobre esses métodos, notei que a essência do MCMC está oculta nas camadas profundas do ruído matemático, e é difícil perceber por trás desse ruído.  Eu tive que passar muitas horas entendendo esse conceito. <br><br>  Neste artigo, uma tentativa de explicar os métodos de Monte Carlo com cadeias de Markov está disponível, para que fique claro para que eles são usados.  Vou me concentrar em mais algumas maneiras de usar esses métodos no meu próximo post. <br><br>  Então, vamos começar.  O MCMC consiste em dois termos: cadeias de Monte Carlo e Markov.  Vamos falar sobre cada um deles. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  Nos termos mais simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, os métodos de Monte Carlo</a> podem ser definidos como simulações simples. <br><br>  Os métodos Monte Carlo receberam o nome do Monte Carlo Casino, em Mônaco.  Em muitos jogos de cartas, você precisa saber a probabilidade de ganhar o dealer.  Às vezes, o cálculo dessa probabilidade pode ser matematicamente complicado ou intratável.  Mas sempre podemos executar uma simulação de computador para jogar o jogo inteiro muitas vezes e considerar a probabilidade como o número de vitórias dividido pelo número de jogos disputados. <br>  É tudo o que você precisa saber sobre os métodos de Monte Carlo.  Sim, é apenas uma técnica de modelagem simples com um nome sofisticado. <br><br><h2>  Cadeias de Markov </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Como o termo MCMC consiste em duas partes, você ainda precisa entender o que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">são as cadeias de Markov</a> .  Mas antes de passar para as cadeias de Markov, vamos falar um pouco sobre as propriedades de Markov. <br><br>  Suponha que exista um sistema de M-estados possíveis e você mude de um estado para outro.  Não deixe nada te confundir ainda.  Um exemplo específico desse sistema é o clima, que muda de quente para frio a moderado.  Outro exemplo é o mercado de ações, que está saltando de um estado de baixa para um estado de alta e estagnação. <br><br>  <i>A propriedade Markov</i> sugere que, para um determinado processo que esteja no estado X <sub>n</sub> em um determinado momento, a probabilidade X <sub>n + 1</sub> = k (onde k é qualquer um dos estados M aos quais o processo pode ir) depende apenas de qual é essa condição no momento?  E não sobre como atingiu seu estado atual. <br>  Em termos matemáticos, podemos escrever isso na forma da seguinte fórmula: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Para maior clareza, você não se importa com a sequência de condições que o mercado levou para se tornar otimista.  A probabilidade de o próximo estado ser "de baixa" é determinada apenas pelo fato de o mercado estar atualmente em um estado de "alta".  Também faz sentido na prática. <br><br>  Um processo com uma propriedade Markov é chamado de processo Markov.  Por que a cadeia de Markov é importante?  Devido à sua distribuição estacionária. <br><br><h3>  O que é distribuição estacionária? </h3><br>  Vou tentar explicar a distribuição estacionária calculando-a para o exemplo abaixo.  Suponha que você tenha um processo de Markov para o mercado de ações, como mostrado abaixo. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Você tem uma matriz de probabilidade de transição que determina a probabilidade de uma transição do estado X <sub>i</sub> para X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Matriz de Probabilidade de Transição, Q <br><br>  Na matriz de probabilidade transitória Q, a probabilidade de que o próximo estado seja "bull", dado o estado atual de "bull" = 0,9;  a probabilidade de que o próximo estado seja "de baixa" se o estado atual for "bull" = 0,075.  E assim por diante <br><br>  Bem, vamos começar com algum estado em particular.  Nosso estado será definido pelo vetor [touro, urso, estagnação].  Se começarmos com um estado “de baixa”, o vetor será assim: [0,1,0].  Podemos calcular a distribuição de probabilidade para o próximo estado multiplicando o vetor de estado atual pela matriz de probabilidade de transição. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Observe que as probabilidades somam 1.</b> <br><br>  A seguinte distribuição de estados pode ser encontrada pela fórmula: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  E assim por diante  No final, você alcançará um estado estacionário no qual o estado se estabiliza: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  Para a matriz de probabilidade de transição Q descrita acima, a distribuição estacionária s é <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Você pode obter uma distribuição estacionária com o seguinte código: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Você também pode começar de qualquer outro estado - obter a mesma distribuição estacionária.  Altere o estado inicial no código se você quiser ter certeza disso. <br><br>  Agora podemos responder à pergunta de por que a distribuição estacionária é tão importante. <br><br>  A distribuição estacionária é importante porque pode ser usada para determinar a probabilidade de um sistema estar em um determinado estado aleatoriamente. <br><br>  Para o nosso exemplo, podemos dizer que em 62,5% dos casos o mercado estará em um estado "de alta", 31,25% em um estado de "baixa" e 6,25% em estagnação. <br><br>  Intuitivamente, você pode ver isso como um passeio aleatório pela cadeia. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Passeio aleatório <br><br>  Você está em um determinado ponto e escolhe o próximo estado, observando a distribuição de probabilidade do próximo estado, levando em consideração o estado atual.  Podemos visitar alguns nós com mais frequência do que outros, com base nas probabilidades desses nós. <br><br>  Foi assim que o Google resolveu o problema de pesquisa no início da Internet.  O problema era classificar as páginas, dependendo de sua importância.  O Google resolveu o problema usando o algoritmo Pagerank.  O algoritmo do Google Pagerank deve considerar o estado como uma página e a probabilidade de uma página em uma distribuição estacionária como sua importância relativa. <br><br>  Agora nos voltamos diretamente para a consideração dos métodos MCMC. <br><br><h2>  O que são métodos de Monte Carlo com cadeias de Markov (MCMC) </h2><br>  Antes de responder o que é o MCMC, deixe-me fazer uma pergunta.  Nós sabemos sobre a distribuição beta.  Conhecemos sua função de densidade de probabilidade.  Mas podemos tirar uma amostra dessa distribuição?  Você pode criar uma maneira de fazer isso? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Pense ... <br><br>  O MCMC permite escolher entre qualquer distribuição de probabilidade.  Isso é especialmente importante quando você precisa fazer uma seleção na distribuição posterior. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  A figura mostra o teorema de Bayes. <br><br>  Por exemplo, você precisa fazer uma amostra de uma distribuição posterior.  Mas é fácil calcular o componente posterior juntamente com a constante de normalização (evidência)?  Na maioria dos casos, você pode encontrá-los na forma de um produto de probabilidade e probabilidade a priori.  Mas calcular a constante de normalização (p (D)) não funciona.  Porque  Vamos dar uma olhada. <br><br>  Suponha que H use apenas 3 valores: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  Nesse caso, é fácil calcular p (D).  Mas e se o valor de H for contínuo?  Seria possível calcular isso tão facilmente, especialmente se H assumisse valores infinitos?  Para isso, uma integral complexa teria que ser resolvida. <br><br>  Queremos fazer a seleção aleatória a partir da distribuição posterior, mas também queremos considerar p (D) como uma constante. <br><br>  A Wikipedia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escreve</a> : <br><br>  Os métodos de Monte Carlo com cadeias de Markov são uma classe de algoritmos para amostragem de uma distribuição de probabilidade, com base na construção de uma cadeia de Markov, que como distribuição estacionária tem a forma desejada.  O estado da cadeia após uma série de etapas é então usado como uma seleção da distribuição desejada.  A qualidade da amostragem melhora com o aumento do número de etapas. <br><br>  Vejamos um exemplo.  Digamos que você precise de uma amostra da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distribuição beta</a> .  Sua densidade: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  onde C é a constante de normalização.  Na verdade, essa é uma função de α e β, mas quero mostrar que ela não é necessária para uma amostra da distribuição beta; portanto, a consideraremos como uma constante. <br><br>  O problema de distribuição beta é realmente difícil, se não praticamente insolúvel.  Na realidade, talvez você precise trabalhar com funções de distribuição mais complexas e, às vezes, não conhecerá as constantes de normalização. <br><br>  Os métodos MCMC facilitam a vida, fornecendo algoritmos que poderiam criar uma cadeia de Markov com distribuição beta como distribuição estacionária, uma vez que podemos escolher uma distribuição uniforme (que é relativamente simples). <br><br>  Se começarmos com um estado aleatório e passarmos para o próximo estado com base em algum algoritmo várias vezes, criaremos uma cadeia de Markov com uma distribuição beta como distribuição estacionária.  E os estados em que nos encontramos há muito tempo podem ser usados ​​como uma amostra da distribuição beta. <br><br>  Um desses algoritmos MCMC é o algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Metropolis-Hastings.</a> <br><br><h2>  Algoritmo de Metropolis-Hastings </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuição: </h3><br>  Então qual é o propósito? <br><br>  <i>Intuitivamente, queremos caminhar por algum pedaço de superfície (nossa cadeia de Markov) de forma que a quantidade de tempo que gastamos em cada local seja proporcional à altura da superfície naquele local (a densidade de probabilidade desejada da qual queremos fazer uma seleção).</i> <i><br><br></i>  <i>Por exemplo, gostaríamos de gastar o dobro do tempo no topo de uma colina com 100 metros de altura do que em uma colina vizinha de 50 metros.</i>  <i>É bom que possamos fazer isso, mesmo que não saibamos as alturas absolutas dos pontos na superfície: tudo que você precisa saber são as alturas relativas.</i>  <i>Por exemplo, se o topo da colina A for duas vezes mais alto que o topo da colina B, gostaríamos de passar o dobro do tempo em A do que em B.</i> <i><br><br></i>  <i>Existem esquemas mais complexos para propor novos locais e regras para sua adoção, mas a idéia principal é a seguinte:</i> <i><br><br></i> <ol><li>  <i>Escolha um novo local "sugerido".</i> </li><li>  <i>Descubra quanto mais alto ou mais baixo esse local é comparado ao atual.</i> </li><li>  <i>Permanecer no local ou mudar para um novo local com uma probabilidade proporcional à altura dos locais.</i> </li></ol> <i><br></i>  <i>O objetivo do MCMC é selecionar entre algumas distribuições de probabilidade sem precisar saber sua altura exata a qualquer momento (não é necessário conhecer C).</i> <i><br></i>  <i>Se o processo de “errância” estiver configurado corretamente, você poderá garantir que essa proporcionalidade (entre o tempo gasto e a altura da distribuição) seja alcançada</i> . <br><br><h3>  Algoritmo: </h3><br>  Agora vamos definir e descrever a tarefa em termos mais formais.  Seja s = (s1, s2, ..., sM) a distribuição estacionária desejada.  Queremos criar uma cadeia de Markov com uma distribuição tão estacionária.  Começamos com uma cadeia de Markov arbitrária com estados M com a matriz de transição P, de modo que pij representa a probabilidade de transição do estado i para j. <br><br>  Intuitivamente, sabemos como percorrer a cadeia de Markov, mas a cadeia de Markov não possui a distribuição estacionária necessária.  Essa cadeia tem alguma distribuição estacionária (da qual não precisamos).  Nosso objetivo é mudar a maneira como passeamos pela cadeia de Markov para que a cadeia tenha a distribuição estacionária desejada. <br><br>  Para fazer isso: <br><br><ol><li>  Comece com um estado inicial aleatório i. </li><li>  Selecione aleatoriamente um novo estado assumido observando as probabilidades de transição na i-ésima linha da matriz de transição P. </li><li>  Calcule uma medida chamada probabilidade de decisão, que é definida como: aij = min (sj.pji / si.pij, 1). </li><li>  Agora jogue uma moeda que caia na superfície da águia com probabilidade aij.  Se uma águia cair, aceite a oferta, ou seja, vá para o próximo estado, caso contrário, rejeite a oferta, ou seja, permaneça no estado atual. <br></li><li>  Repita várias vezes. <br></li></ol><br>  Após um grande número de testes, essa cadeia convergirá e terá uma distribuição estacionária s.  Em seguida, podemos usar os estados da cadeia como uma amostra de qualquer distribuição. <br><br>  Ao fazer isso para testar a distribuição beta, o único momento em que você precisa usar a densidade de probabilidade é procurar a probabilidade de tomar uma decisão.  Para fazer isso, divida sj por si (ou seja, a constante de normalização C é cancelada). <br><br><h3>  Seleção Beta </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Agora nos voltamos para o problema de amostragem da distribuição beta. <br><br>  Uma distribuição beta é uma distribuição contínua em [0,1] e pode ter valores infinitos em [0,1].  Suponha que uma cadeia de Markov arbitrária P com estados infinitos em [0,1] tenha uma matriz de transição P tal que pij = pji = todos os elementos na matriz. <br><br>  Não precisamos da matriz P, como veremos mais adiante, mas quero que a descrição do problema seja o mais próxima possível do algoritmo que propusemos. <br><br><ul><li>  Comece com um estado inicial aleatório i obtido de uma distribuição uniforme em (0,1). </li><li>  Selecione aleatoriamente um novo estado assumido observando as probabilidades de transição na i-ésima linha da matriz de transição P. Suponha que escolha outro estado Unif (0,1) como o estado assumido j. </li><li>  Calcule a medida, que é chamada de probabilidade de tomar uma decisão: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  O que simplifica para: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Como pji = pij e onde <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Agora jogue uma moeda.  Com probabilidade, uma águia cairá.  Se uma águia cair, você deve aceitar a oferta, ou seja, passar para o próximo estado.  Caso contrário, vale a pena rejeitar a oferta, ou seja, permanecer no mesmo estado. </li><li>  Repita o teste várias vezes. </li></ul><br><h3>  Código: </h3><br>  É hora de passar da teoria para a prática.  Escreveremos nossa amostra beta em Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Compare os resultados com a distribuição beta real. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Como você pode ver, os valores são muito semelhantes à distribuição beta.  Assim, a rede MCMC atingiu um estado estacionário <br><br>  No código acima, criamos um amostrador beta, mas o mesmo conceito se aplica a qualquer outra distribuição a partir da qual queremos fazer uma seleção. <br><br><h2>  Conclusões </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  Foi um ótimo post.  Parabéns se você ler até o final. <br><br>  Em essência, os métodos MCMC podem ser complexos, mas fornecem uma grande flexibilidade.  Você pode selecionar de qualquer função de distribuição usando a seleção através do MCMC.  Normalmente, esses métodos são usados ​​para amostrar a partir de distribuições posteriores. <br><br>  Você também pode usar o MCMC para resolver problemas com um grande espaço de estado.  Por exemplo, em um problema de mochila ou por descriptografia.  Vou tentar fornecer exemplos mais interessantes no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">próximo</a> post.  Fique atento. <br><br><h2>  Dos editores </h2><br><ul><li>  Curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Python para trabalhar com dados</a> </li><li>  Curso Online de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado de Máquina</a> </li><li>  Curso on-line " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BIG DATA from scratch</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt460497/">https://habr.com/ru/post/pt460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt460485/index.html">Como um torneio online pode desencorajar o "fim na próxima semana"</a></li>
<li><a href="../pt460489/index.html">Os 11 principais erros no desenvolvimento do BCP</a></li>
<li><a href="../pt460491/index.html">Sensor de temperatura e umidade do Arduino com envio e plotagem (Parte 1)</a></li>
<li><a href="../pt460493/index.html">"Aplicativos matadores" para PC dos anos 80: VisiCalc e WordStar</a></li>
<li><a href="../pt460495/index.html">Container-to-pipeline: CRI-O agora é o padrão no OpenShift Container Platform 4</a></li>
<li><a href="../pt460499/index.html">Três vencedores do Prêmio Dijkstra: como foram o Hydra 2019 e o SPTDC 2019</a></li>
<li><a href="../pt460501/index.html">Exemplo de implementação de integração contínua usando o BuildBot</a></li>
<li><a href="../pt460503/index.html">Configuração sem fio do Raspberry PI 3 B +</a></li>
<li><a href="../pt460505/index.html">Seduza três cruzamentos, ou por que os projetos são tão difíceis de terminar a tempo</a></li>
<li><a href="../pt460507/index.html">XEN e o futuro do setor automotivo: como um hipervisor de código aberto se torna um concorrente de soluções automotivas comerciais</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>