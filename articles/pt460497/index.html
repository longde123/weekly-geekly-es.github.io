<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üóùÔ∏è üö≠ üëÇüèº Uso intuitivo dos m√©todos de Monte Carlo com cadeias de Markov üî± üíÖüèª üíû</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="√â f√°cil? Eu tentei 
 Alexey Kuzmin, diretor de desenvolvimento de dados e trabalho da DomKlik, palestrante em ci√™ncia de dados na Netology, traduziu u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Uso intuitivo dos m√©todos de Monte Carlo com cadeias de Markov</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/460497/"><h4>  √â f√°cil?  Eu tentei </h4><br>  <i>Alexey Kuzmin, diretor de desenvolvimento de dados e trabalho da DomKlik, palestrante em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ci√™ncia de dados</a> na Netology, traduziu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um artigo de</a> Rahul Agarwal sobre como os m√©todos de Monte Carlo trabalham com cadeias de Markov para resolver problemas em um amplo espa√ßo de estados.</i> <br><a name="habracut"></a><br>  Todo mundo associado √† Data Science j√° ouviu falar dos m√©todos de Monte Carlo com cadeias de Markov (MCMCs).  √Äs vezes, o t√≥pico √© abordado ao estudar as estat√≠sticas bayesianas, √†s vezes ao trabalhar com ferramentas como o Profeta. <br><br>  Mas o MCMC √© dif√≠cil de entender.  Toda vez que li sobre esses m√©todos, notei que a ess√™ncia do MCMC est√° oculta nas camadas profundas do ru√≠do matem√°tico, e √© dif√≠cil perceber por tr√°s desse ru√≠do.  Eu tive que passar muitas horas entendendo esse conceito. <br><br>  Neste artigo, uma tentativa de explicar os m√©todos de Monte Carlo com cadeias de Markov est√° dispon√≠vel, para que fique claro para que eles s√£o usados.  Vou me concentrar em mais algumas maneiras de usar esses m√©todos no meu pr√≥ximo post. <br><br>  Ent√£o, vamos come√ßar.  O MCMC consiste em dois termos: cadeias de Monte Carlo e Markov.  Vamos falar sobre cada um deles. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  Nos termos mais simples <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, os m√©todos de Monte Carlo</a> podem ser definidos como simula√ß√µes simples. <br><br>  Os m√©todos Monte Carlo receberam o nome do Monte Carlo Casino, em M√¥naco.  Em muitos jogos de cartas, voc√™ precisa saber a probabilidade de ganhar o dealer.  √Äs vezes, o c√°lculo dessa probabilidade pode ser matematicamente complicado ou intrat√°vel.  Mas sempre podemos executar uma simula√ß√£o de computador para jogar o jogo inteiro muitas vezes e considerar a probabilidade como o n√∫mero de vit√≥rias dividido pelo n√∫mero de jogos disputados. <br>  √â tudo o que voc√™ precisa saber sobre os m√©todos de Monte Carlo.  Sim, √© apenas uma t√©cnica de modelagem simples com um nome sofisticado. <br><br><h2>  Cadeias de Markov </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Como o termo MCMC consiste em duas partes, voc√™ ainda precisa entender o que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">s√£o as cadeias de Markov</a> .  Mas antes de passar para as cadeias de Markov, vamos falar um pouco sobre as propriedades de Markov. <br><br>  Suponha que exista um sistema de M-estados poss√≠veis e voc√™ mude de um estado para outro.  N√£o deixe nada te confundir ainda.  Um exemplo espec√≠fico desse sistema √© o clima, que muda de quente para frio a moderado.  Outro exemplo √© o mercado de a√ß√µes, que est√° saltando de um estado de baixa para um estado de alta e estagna√ß√£o. <br><br>  <i>A propriedade Markov</i> sugere que, para um determinado processo que esteja no estado X <sub>n</sub> em um determinado momento, a probabilidade X <sub>n + 1</sub> = k (onde k √© qualquer um dos estados M aos quais o processo pode ir) depende apenas de qual √© essa condi√ß√£o no momento?  E n√£o sobre como atingiu seu estado atual. <br>  Em termos matem√°ticos, podemos escrever isso na forma da seguinte f√≥rmula: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  Para maior clareza, voc√™ n√£o se importa com a sequ√™ncia de condi√ß√µes que o mercado levou para se tornar otimista.  A probabilidade de o pr√≥ximo estado ser "de baixa" √© determinada apenas pelo fato de o mercado estar atualmente em um estado de "alta".  Tamb√©m faz sentido na pr√°tica. <br><br>  Um processo com uma propriedade Markov √© chamado de processo Markov.  Por que a cadeia de Markov √© importante?  Devido √† sua distribui√ß√£o estacion√°ria. <br><br><h3>  O que √© distribui√ß√£o estacion√°ria? </h3><br>  Vou tentar explicar a distribui√ß√£o estacion√°ria calculando-a para o exemplo abaixo.  Suponha que voc√™ tenha um processo de Markov para o mercado de a√ß√µes, como mostrado abaixo. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  Voc√™ tem uma matriz de probabilidade de transi√ß√£o que determina a probabilidade de uma transi√ß√£o do estado X <sub>i</sub> para X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Matriz de Probabilidade de Transi√ß√£o, Q <br><br>  Na matriz de probabilidade transit√≥ria Q, a probabilidade de que o pr√≥ximo estado seja "bull", dado o estado atual de "bull" = 0,9;  a probabilidade de que o pr√≥ximo estado seja "de baixa" se o estado atual for "bull" = 0,075.  E assim por diante <br><br>  Bem, vamos come√ßar com algum estado em particular.  Nosso estado ser√° definido pelo vetor [touro, urso, estagna√ß√£o].  Se come√ßarmos com um estado ‚Äúde baixa‚Äù, o vetor ser√° assim: [0,1,0].  Podemos calcular a distribui√ß√£o de probabilidade para o pr√≥ximo estado multiplicando o vetor de estado atual pela matriz de probabilidade de transi√ß√£o. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Observe que as probabilidades somam 1.</b> <br><br>  A seguinte distribui√ß√£o de estados pode ser encontrada pela f√≥rmula: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  E assim por diante  No final, voc√™ alcan√ßar√° um estado estacion√°rio no qual o estado se estabiliza: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  Para a matriz de probabilidade de transi√ß√£o Q descrita acima, a distribui√ß√£o estacion√°ria s √© <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  Voc√™ pode obter uma distribui√ß√£o estacion√°ria com o seguinte c√≥digo: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  Voc√™ tamb√©m pode come√ßar de qualquer outro estado - obter a mesma distribui√ß√£o estacion√°ria.  Altere o estado inicial no c√≥digo se voc√™ quiser ter certeza disso. <br><br>  Agora podemos responder √† pergunta de por que a distribui√ß√£o estacion√°ria √© t√£o importante. <br><br>  A distribui√ß√£o estacion√°ria √© importante porque pode ser usada para determinar a probabilidade de um sistema estar em um determinado estado aleatoriamente. <br><br>  Para o nosso exemplo, podemos dizer que em 62,5% dos casos o mercado estar√° em um estado "de alta", 31,25% em um estado de "baixa" e 6,25% em estagna√ß√£o. <br><br>  Intuitivamente, voc√™ pode ver isso como um passeio aleat√≥rio pela cadeia. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Passeio aleat√≥rio <br><br>  Voc√™ est√° em um determinado ponto e escolhe o pr√≥ximo estado, observando a distribui√ß√£o de probabilidade do pr√≥ximo estado, levando em considera√ß√£o o estado atual.  Podemos visitar alguns n√≥s com mais frequ√™ncia do que outros, com base nas probabilidades desses n√≥s. <br><br>  Foi assim que o Google resolveu o problema de pesquisa no in√≠cio da Internet.  O problema era classificar as p√°ginas, dependendo de sua import√¢ncia.  O Google resolveu o problema usando o algoritmo Pagerank.  O algoritmo do Google Pagerank deve considerar o estado como uma p√°gina e a probabilidade de uma p√°gina em uma distribui√ß√£o estacion√°ria como sua import√¢ncia relativa. <br><br>  Agora nos voltamos diretamente para a considera√ß√£o dos m√©todos MCMC. <br><br><h2>  O que s√£o m√©todos de Monte Carlo com cadeias de Markov (MCMC) </h2><br>  Antes de responder o que √© o MCMC, deixe-me fazer uma pergunta.  N√≥s sabemos sobre a distribui√ß√£o beta.  Conhecemos sua fun√ß√£o de densidade de probabilidade.  Mas podemos tirar uma amostra dessa distribui√ß√£o?  Voc√™ pode criar uma maneira de fazer isso? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Pense ... <br><br>  O MCMC permite escolher entre qualquer distribui√ß√£o de probabilidade.  Isso √© especialmente importante quando voc√™ precisa fazer uma sele√ß√£o na distribui√ß√£o posterior. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  A figura mostra o teorema de Bayes. <br><br>  Por exemplo, voc√™ precisa fazer uma amostra de uma distribui√ß√£o posterior.  Mas √© f√°cil calcular o componente posterior juntamente com a constante de normaliza√ß√£o (evid√™ncia)?  Na maioria dos casos, voc√™ pode encontr√°-los na forma de um produto de probabilidade e probabilidade a priori.  Mas calcular a constante de normaliza√ß√£o (p (D)) n√£o funciona.  Porque  Vamos dar uma olhada. <br><br>  Suponha que H use apenas 3 valores: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  Nesse caso, √© f√°cil calcular p (D).  Mas e se o valor de H for cont√≠nuo?  Seria poss√≠vel calcular isso t√£o facilmente, especialmente se H assumisse valores infinitos?  Para isso, uma integral complexa teria que ser resolvida. <br><br>  Queremos fazer a sele√ß√£o aleat√≥ria a partir da distribui√ß√£o posterior, mas tamb√©m queremos considerar p (D) como uma constante. <br><br>  A Wikipedia <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">escreve</a> : <br><br>  Os m√©todos de Monte Carlo com cadeias de Markov s√£o uma classe de algoritmos para amostragem de uma distribui√ß√£o de probabilidade, com base na constru√ß√£o de uma cadeia de Markov, que como distribui√ß√£o estacion√°ria tem a forma desejada.  O estado da cadeia ap√≥s uma s√©rie de etapas √© ent√£o usado como uma sele√ß√£o da distribui√ß√£o desejada.  A qualidade da amostragem melhora com o aumento do n√∫mero de etapas. <br><br>  Vejamos um exemplo.  Digamos que voc√™ precise de uma amostra da <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distribui√ß√£o beta</a> .  Sua densidade: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  onde C √© a constante de normaliza√ß√£o.  Na verdade, essa √© uma fun√ß√£o de Œ± e Œ≤, mas quero mostrar que ela n√£o √© necess√°ria para uma amostra da distribui√ß√£o beta; portanto, a consideraremos como uma constante. <br><br>  O problema de distribui√ß√£o beta √© realmente dif√≠cil, se n√£o praticamente insol√∫vel.  Na realidade, talvez voc√™ precise trabalhar com fun√ß√µes de distribui√ß√£o mais complexas e, √†s vezes, n√£o conhecer√° as constantes de normaliza√ß√£o. <br><br>  Os m√©todos MCMC facilitam a vida, fornecendo algoritmos que poderiam criar uma cadeia de Markov com distribui√ß√£o beta como distribui√ß√£o estacion√°ria, uma vez que podemos escolher uma distribui√ß√£o uniforme (que √© relativamente simples). <br><br>  Se come√ßarmos com um estado aleat√≥rio e passarmos para o pr√≥ximo estado com base em algum algoritmo v√°rias vezes, criaremos uma cadeia de Markov com uma distribui√ß√£o beta como distribui√ß√£o estacion√°ria.  E os estados em que nos encontramos h√° muito tempo podem ser usados ‚Äã‚Äãcomo uma amostra da distribui√ß√£o beta. <br><br>  Um desses algoritmos MCMC √© o algoritmo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Metropolis-Hastings.</a> <br><br><h2>  Algoritmo de Metropolis-Hastings </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intui√ß√£o: </h3><br>  Ent√£o qual √© o prop√≥sito? <br><br>  <i>Intuitivamente, queremos caminhar por algum peda√ßo de superf√≠cie (nossa cadeia de Markov) de forma que a quantidade de tempo que gastamos em cada local seja proporcional √† altura da superf√≠cie naquele local (a densidade de probabilidade desejada da qual queremos fazer uma sele√ß√£o).</i> <i><br><br></i>  <i>Por exemplo, gostar√≠amos de gastar o dobro do tempo no topo de uma colina com 100 metros de altura do que em uma colina vizinha de 50 metros.</i>  <i>√â bom que possamos fazer isso, mesmo que n√£o saibamos as alturas absolutas dos pontos na superf√≠cie: tudo que voc√™ precisa saber s√£o as alturas relativas.</i>  <i>Por exemplo, se o topo da colina A for duas vezes mais alto que o topo da colina B, gostar√≠amos de passar o dobro do tempo em A do que em B.</i> <i><br><br></i>  <i>Existem esquemas mais complexos para propor novos locais e regras para sua ado√ß√£o, mas a id√©ia principal √© a seguinte:</i> <i><br><br></i> <ol><li>  <i>Escolha um novo local "sugerido".</i> </li><li>  <i>Descubra quanto mais alto ou mais baixo esse local √© comparado ao atual.</i> </li><li>  <i>Permanecer no local ou mudar para um novo local com uma probabilidade proporcional √† altura dos locais.</i> </li></ol> <i><br></i>  <i>O objetivo do MCMC √© selecionar entre algumas distribui√ß√µes de probabilidade sem precisar saber sua altura exata a qualquer momento (n√£o √© necess√°rio conhecer C).</i> <i><br></i>  <i>Se o processo de ‚Äúerr√¢ncia‚Äù estiver configurado corretamente, voc√™ poder√° garantir que essa proporcionalidade (entre o tempo gasto e a altura da distribui√ß√£o) seja alcan√ßada</i> . <br><br><h3>  Algoritmo: </h3><br>  Agora vamos definir e descrever a tarefa em termos mais formais.  Seja s = (s1, s2, ..., sM) a distribui√ß√£o estacion√°ria desejada.  Queremos criar uma cadeia de Markov com uma distribui√ß√£o t√£o estacion√°ria.  Come√ßamos com uma cadeia de Markov arbitr√°ria com estados M com a matriz de transi√ß√£o P, de modo que pij representa a probabilidade de transi√ß√£o do estado i para j. <br><br>  Intuitivamente, sabemos como percorrer a cadeia de Markov, mas a cadeia de Markov n√£o possui a distribui√ß√£o estacion√°ria necess√°ria.  Essa cadeia tem alguma distribui√ß√£o estacion√°ria (da qual n√£o precisamos).  Nosso objetivo √© mudar a maneira como passeamos pela cadeia de Markov para que a cadeia tenha a distribui√ß√£o estacion√°ria desejada. <br><br>  Para fazer isso: <br><br><ol><li>  Comece com um estado inicial aleat√≥rio i. </li><li>  Selecione aleatoriamente um novo estado assumido observando as probabilidades de transi√ß√£o na i-√©sima linha da matriz de transi√ß√£o P. </li><li>  Calcule uma medida chamada probabilidade de decis√£o, que √© definida como: aij = min (sj.pji / si.pij, 1). </li><li>  Agora jogue uma moeda que caia na superf√≠cie da √°guia com probabilidade aij.  Se uma √°guia cair, aceite a oferta, ou seja, v√° para o pr√≥ximo estado, caso contr√°rio, rejeite a oferta, ou seja, permane√ßa no estado atual. <br></li><li>  Repita v√°rias vezes. <br></li></ol><br>  Ap√≥s um grande n√∫mero de testes, essa cadeia convergir√° e ter√° uma distribui√ß√£o estacion√°ria s.  Em seguida, podemos usar os estados da cadeia como uma amostra de qualquer distribui√ß√£o. <br><br>  Ao fazer isso para testar a distribui√ß√£o beta, o √∫nico momento em que voc√™ precisa usar a densidade de probabilidade √© procurar a probabilidade de tomar uma decis√£o.  Para fazer isso, divida sj por si (ou seja, a constante de normaliza√ß√£o C √© cancelada). <br><br><h3>  Sele√ß√£o Beta </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  Agora nos voltamos para o problema de amostragem da distribui√ß√£o beta. <br><br>  Uma distribui√ß√£o beta √© uma distribui√ß√£o cont√≠nua em [0,1] e pode ter valores infinitos em [0,1].  Suponha que uma cadeia de Markov arbitr√°ria P com estados infinitos em [0,1] tenha uma matriz de transi√ß√£o P tal que pij = pji = todos os elementos na matriz. <br><br>  N√£o precisamos da matriz P, como veremos mais adiante, mas quero que a descri√ß√£o do problema seja o mais pr√≥xima poss√≠vel do algoritmo que propusemos. <br><br><ul><li>  Comece com um estado inicial aleat√≥rio i obtido de uma distribui√ß√£o uniforme em (0,1). </li><li>  Selecione aleatoriamente um novo estado assumido observando as probabilidades de transi√ß√£o na i-√©sima linha da matriz de transi√ß√£o P. Suponha que escolha outro estado Unif (0,1) como o estado assumido j. </li><li>  Calcule a medida, que √© chamada de probabilidade de tomar uma decis√£o: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  O que simplifica para: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Como pji = pij e onde <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Agora jogue uma moeda.  Com probabilidade, uma √°guia cair√°.  Se uma √°guia cair, voc√™ deve aceitar a oferta, ou seja, passar para o pr√≥ximo estado.  Caso contr√°rio, vale a pena rejeitar a oferta, ou seja, permanecer no mesmo estado. </li><li>  Repita o teste v√°rias vezes. </li></ul><br><h3>  C√≥digo: </h3><br>  √â hora de passar da teoria para a pr√°tica.  Escreveremos nossa amostra beta em Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Compare os resultados com a distribui√ß√£o beta real. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  Como voc√™ pode ver, os valores s√£o muito semelhantes √† distribui√ß√£o beta.  Assim, a rede MCMC atingiu um estado estacion√°rio <br><br>  No c√≥digo acima, criamos um amostrador beta, mas o mesmo conceito se aplica a qualquer outra distribui√ß√£o a partir da qual queremos fazer uma sele√ß√£o. <br><br><h2>  Conclus√µes </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  Foi um √≥timo post.  Parab√©ns se voc√™ ler at√© o final. <br><br>  Em ess√™ncia, os m√©todos MCMC podem ser complexos, mas fornecem uma grande flexibilidade.  Voc√™ pode selecionar de qualquer fun√ß√£o de distribui√ß√£o usando a sele√ß√£o atrav√©s do MCMC.  Normalmente, esses m√©todos s√£o usados ‚Äã‚Äãpara amostrar a partir de distribui√ß√µes posteriores. <br><br>  Voc√™ tamb√©m pode usar o MCMC para resolver problemas com um grande espa√ßo de estado.  Por exemplo, em um problema de mochila ou por descriptografia.  Vou tentar fornecer exemplos mais interessantes no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pr√≥ximo</a> post.  Fique atento. <br><br><h2>  Dos editores </h2><br><ul><li>  Curso <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Python para trabalhar com dados</a> </li><li>  Curso Online de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Aprendizado de M√°quina</a> </li><li>  Curso on-line " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">BIG DATA from scratch</a> " </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt460497/">https://habr.com/ru/post/pt460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt460485/index.html">Como um torneio online pode desencorajar o "fim na pr√≥xima semana"</a></li>
<li><a href="../pt460489/index.html">Os 11 principais erros no desenvolvimento do BCP</a></li>
<li><a href="../pt460491/index.html">Sensor de temperatura e umidade do Arduino com envio e plotagem (Parte 1)</a></li>
<li><a href="../pt460493/index.html">"Aplicativos matadores" para PC dos anos 80: VisiCalc e WordStar</a></li>
<li><a href="../pt460495/index.html">Container-to-pipeline: CRI-O agora √© o padr√£o no OpenShift Container Platform 4</a></li>
<li><a href="../pt460499/index.html">Tr√™s vencedores do Pr√™mio Dijkstra: como foram o Hydra 2019 e o SPTDC 2019</a></li>
<li><a href="../pt460501/index.html">Exemplo de implementa√ß√£o de integra√ß√£o cont√≠nua usando o BuildBot</a></li>
<li><a href="../pt460503/index.html">Configura√ß√£o sem fio do Raspberry PI 3 B +</a></li>
<li><a href="../pt460505/index.html">Seduza tr√™s cruzamentos, ou por que os projetos s√£o t√£o dif√≠ceis de terminar a tempo</a></li>
<li><a href="../pt460507/index.html">XEN e o futuro do setor automotivo: como um hipervisor de c√≥digo aberto se torna um concorrente de solu√ß√µes automotivas comerciais</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>