<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü¶è ‚úÇÔ∏è ü§ï So nutzen Sie die verf√ºgbare Speicherkapazit√§t richtig üôéüèæ üßìüèæ üåç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir nutzen Cloud-Dienste seit langem: E-Mail, Speicher, soziale Netzwerke, Instant Messenger. Sie arbeiten alle remote - wir senden Nachrichten und Da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So nutzen Sie die verf√ºgbare Speicherkapazit√§t richtig</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/480622/">  Wir nutzen Cloud-Dienste seit langem: E-Mail, Speicher, soziale Netzwerke, Instant Messenger.  Sie arbeiten alle remote - wir senden Nachrichten und Dateien und sie werden auf Remote-Servern gespeichert und verarbeitet.  Cloud-Spiele funktionieren auch: Der Benutzer stellt eine Verbindung zum Dienst her, w√§hlt das Spiel aus und startet.  Dies ist praktisch f√ºr den Spieler, da die Spiele fast sofort starten, keinen Speicher belegen und keinen leistungsstarken Spielecomputer ben√∂tigen. <br><br><img src="https://habrastorage.org/webt/ej/k4/oy/ejk4oyjjh1r3riqgzzd239qu_va.jpeg"><br><br>  Bei einem Cloud-Dienst ist alles anders - er hat Probleme mit der Datenspeicherung.  Jedes Spiel kann Dutzende oder Hunderte von Gigabyte wiegen, zum Beispiel "The Witcher 3" ben√∂tigt 50 GB und "Call of Duty: Black Ops III" - 113. Gleichzeitig wird der Dienst bei 2-3 Spielen nicht genutzt, es werden mindestens mehrere Dutzend ben√∂tigt .  Zus√§tzlich zum Speichern von Hunderten von Spielen muss der Dienst entscheiden, wie viel Speicher pro Spieler zugewiesen werden soll, und bei Tausenden von Spielen skalieren. <br><br>  Sollte dies alles auf ihren Servern gespeichert werden: Wie viele ben√∂tigen sie, wo k√∂nnen Rechenzentren platziert werden, wie k√∂nnen Daten zwischen mehreren Rechenzentren im laufenden Betrieb synchronisiert werden?  "Wolken" kaufen?  Verwenden Sie virtuelle Maschinen?  Ist es m√∂glich, Benutzerdaten f√ºnfmal mit Komprimierung zu speichern und in Echtzeit bereitzustellen?  Wie kann ein gegenseitiger Einfluss von Benutzern bei der konsistenten Verwendung derselben virtuellen Maschine ausgeschlossen werden? <br><br>  All diese Aufgaben wurden in Playkey.net - einer Cloud-basierten Spieleplattform - erfolgreich gel√∂st.  <strong>Vladimir Ryabov</strong> ( <a href="https://habr.com/ru/users/graymansama/" class="user_link">Graymansama</a> ) - Leiter der Systemadministrationsabteilung - wird ausf√ºhrlich √ºber die ZFS-Technologie f√ºr FreeBSD, die dabei geholfen hat, und die neue ZOL-Version (ZFS unter Linux) sprechen. <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SssLwMbMrQ4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Tausend Server des Unternehmens befinden sich in entfernten Rechenzentren in Moskau, London und Frankfurt.  Es gibt mehr als 250 Spiele im Service, die von 100.000 Spielern pro Monat gespielt werden. <br><br><img src="https://habrastorage.org/webt/s6/mv/nw/s6mvnwi5z_b5ltjl_vico2x7ro4.jpeg"><br><br>  Der Dienst funktioniert folgenderma√üen: Das Spiel wird auf den Servern des Unternehmens ausgef√ºhrt, der Benutzer erh√§lt eine Reihe von Steuerelementen √ºber Tastatur, Maus oder Gamepad, und als Antwort wird ein Videostream gesendet.  Auf diese Weise k√∂nnen Sie moderne Top-End-Spiele auf Computern mit schwacher Hardware, Laptops mit integriertem Video oder auf Macs spielen, f√ºr die diese Spiele √ºberhaupt nicht freigegeben sind. <br><br><h2>  Spiele m√ºssen gespeichert und aktualisiert werden </h2><br>  Die Hauptdaten f√ºr den Cloud-Gaming-Dienst sind die Spieldistributionen, die Hunderte von GB √ºberschreiten k√∂nnen, und das Speichern durch Benutzer. <br><br>  Als wir klein waren, hatten wir nur ein Dutzend Server und einen bescheidenen Katalog von 50 Spielen.  Wir haben alle Daten lokal auf den Servern gespeichert, manuell aktualisiert, alles war in Ordnung.  Aber die Zeit ist reif und wir machen uns auf den Weg <strong>zu den AWS-Wolken</strong> . <br><br>  Mit AWS haben wir mehrere hundert Server, aber die Architektur hat sich nicht ge√§ndert.  Sie waren auch Server, aber jetzt virtuell, mit lokalen Festplatten, auf denen die Spieldistributionen lagen.  Die manuelle Aktualisierung von 100 Servern schl√§gt jedoch fehl. <br><br>  Wir suchten nach einer L√∂sung.  Zuerst haben wir versucht, √ºber <strong>rsync</strong> zu aktualisieren.  Es stellte sich jedoch heraus, dass dies extrem langsam ist und die Belastung des Hauptknotens zu hoch ist.  Dies ist jedoch noch nicht einmal das Schlimmste: Als wir nur wenig online waren, haben wir einige der virtuellen Maschinen ausgeschaltet, um nicht daf√ºr zu zahlen, und beim Aktualisieren wurden die Daten nicht auf die deaktivierten Server √ºbertragen.  Alle von ihnen wurden ohne Updates gelassen. <br><br>  Die L√∂sung waren Torrents - das <strong>BTSync-</strong> Programm.  Sie k√∂nnen einen Ordner auf einer gro√üen Anzahl von Knoten synchronisieren, ohne explizit einen zentralen Knoten anzugeben. <br><br><h2>  Wachstumsprobleme </h2><br>  F√ºr eine Weile hat das alles wunderbar funktioniert.  Aber der Dienst entwickelte sich, es gab mehr Spiele und Server.  Die Anzahl der lokalen Speicher nahm ebenfalls zu, wir mussten immer mehr bezahlen.  In den Wolken ist es teuer, vor allem f√ºr SSDs.  Zu einem bestimmten Zeitpunkt dauerte sogar die √ºbliche Indizierung eines Ordners zum Starten der Synchronisierung mehr als eine Stunde, und alle Server konnten mehrere Tage lang aktualisiert werden. <br><br>  BTSync hat ein weiteres Problem mit √ºberm√§√üigem Netzwerkverkehr verursacht.  Damals wurde es bei Amazon sogar zwischen internen Virtuals bezahlt.  Wenn der klassische Game Launcher kleine √Ñnderungen an gro√üen Dateien vornimmt, glaubt BTSync sofort, dass sich die gesamte Datei ge√§ndert hat, und beginnt, sie vollst√§ndig auf alle Knoten zu √ºbertragen.  Selbst ein Upgrade von 15 MB kann daher zig GB Synchronisationsdatenverkehr generieren. <br><br>  Die Situation wurde kritisch, als der Speicher auf 1 TB anstieg.  Habe gerade ein neues Spiel World of Warships ver√∂ffentlicht.  Ihre Distribution hatte mehrere hunderttausend kleine Dateien.  BTSync konnte es nicht verdauen und an alle anderen Server verteilen - dies verlangsamte die Verteilung anderer Spiele. <br><br>  All diese Faktoren verursachten zwei Probleme: <br><br><ul><li>  Die lokale Lagerhaltung ist teuer, unpraktisch und schwierig zu aktualisieren. </li><li>  Die Wolken waren sehr teuer. </li></ul><br>  Wir haben uns entschlossen, zum Konzept unserer physischen Server zur√ºckzukehren. <br><br><h2>  Eigenes Speichersystem </h2><br>  Bevor wir auf physische Server wechseln, m√ºssen wir den lokalen Speicher entfernen.  Dies erfordert ein eigenes <strong>Speichersystem - Speicher</strong> .  Dies ist ein System, das alle Distributionen speichert und zentral an alle Server verteilt. <br><br>  Es scheint, dass die Aufgabe einfach ist - sie wurde bereits wiederholt gel√∂st.  Aber bei Spielen gibt es Nuancen.  Beispielsweise lehnen die meisten Spiele die Arbeit ab, wenn sie nur √ºber Lesezugriff verf√ºgen.  Selbst beim normalen Start schreiben sie gerne etwas in ihre Dateien, ohne dass sie sich weigern zu arbeiten.  Im Gegenteil, wenn einer gro√üen Anzahl von Benutzern Zugriff auf einen Satz von Distributionen gew√§hrt wird, beginnen sie, die Dateien des anderen mit wettbewerbsf√§higem Zugriff zu schlagen. <br><br>  Wir haben √ºber das Problem nachgedacht, verschiedene m√∂gliche L√∂sungen gepr√ºft und sind zu <strong>ZFS - Zettabyte File System auf FreeBSD gekommen</strong> . <br><br><h2>  ZFS unter FreeBSD </h2><br>  Dies ist kein gew√∂hnliches Dateisystem.  Klassische Systeme werden zun√§chst auf einem Ger√§t installiert und erfordern f√ºr das Arbeiten mit mehreren Festplatten bereits einen Volume Manager. <br><blockquote>  ZFS wurde urspr√ºnglich auf virtuellen Pools erstellt. </blockquote>  Sie hei√üen <strong>zpool</strong> und bestehen aus Plattengruppen oder RAID-Arrays.  Das gesamte Volumen dieser Festplatten steht f√ºr jedes Dateisystem innerhalb von zpool zur Verf√ºgung.  Das liegt daran, dass ZFS urspr√ºnglich als System entwickelt wurde, das mit gro√üen Datenmengen arbeitet. <br><br><h3>  Wie ZFS zur L√∂sung unserer Probleme beitrug </h3><br>  Dieses System verf√ºgt √ºber einen wunderbaren <strong>Mechanismus zum Erstellen von Schnappsch√ºssen und Klonen</strong> .  Sie werden <strong>sofort</strong> erstellt und wiegen nur wenige KB.  Wenn wir √Ñnderungen an einem der Klone vornehmen, erh√∂ht sich das Volumen dieser √Ñnderungen.  Gleichzeitig √§ndern sich die Daten in den verbleibenden Klonen nicht und bleiben eindeutig.  Auf diese Weise k√∂nnen Sie eine <strong>10-TB-</strong> Festplatte mit exklusivem Zugriff f√ºr den Endbenutzer mit nur wenigen KB verteilen. <br><br>  Werden Klone, die w√§hrend einer Spielsitzung √Ñnderungen vornehmen, nicht so viel Platz beanspruchen wie alle Spiele?  Nein, wir haben festgestellt, dass selbst in l√§ngeren Spielsitzungen die Anzahl der √Ñnderungen selten 100-200 MB √ºberschreitet - dies ist nicht kritisch.  Aus diesem Grund k√∂nnen wir mehreren hundert Benutzern gleichzeitig vollen Zugriff auf eine vollwertige Festplatte mit hoher Kapazit√§t gew√§hren, wobei wir nur 10 TB mit einem Tail ausgeben. <br><br><h3>  So funktioniert ZFS </h3><br>  Die Beschreibung scheint kompliziert, aber ZFS funktioniert ganz einfach.  Lassen Sie uns seine Arbeit <code>zpool data</code> eines einfachen Beispiels analysieren: Erstellen Sie <code>zpool data</code> von den verf√ºgbaren <code>zpool create data /dev/da /dev/db /dev/dc</code> Datentr√§gern. <code>zpool create data /dev/da /dev/db /dev/dc</code> . <br><br>  <em>Hinweis</em>  <em>Dies ist f√ºr die Produktion nicht erforderlich, da bei Ausfall mindestens einer Festplatte der gesamte Pool in Vergessenheit ger√§t.</em>  <em>Verwenden Sie besser RAID-Gruppen.</em> <br><br>  Wir erstellen das Dateisystem <code>zfs create data/games</code> und darin ein Blockger√§t mit dem Namen <code>data/games/disk</code> von 10 TB.  Das Ger√§t ist unter <code>/dev/zvol/data/games/disk</code> als normale Festplatte verf√ºgbar - Sie k√∂nnen die gleichen Manipulationen damit durchf√ºhren. <br><br>  Dann beginnt der Spa√ü.  Wir geben diese Festplatte √ºber <strong>iSCSI an</strong> unseren Update-Assistenten weiter - eine normale virtuelle Maschine, auf der Windows ausgef√ºhrt wird.  Wir verbinden die Festplatte und legen die Spiele einfach √ºber Steam darauf, wie auf einem normalen Heimcomputer. <br><br>  F√ºlle die Scheibe mit Spielen.  Jetzt m√ºssen diese Daten f√ºr Endbenutzer auf <strong>200 Server</strong> verteilt werden. <br><br><ul><li>  Erstellen Sie einen Snapshot dieser Festplatte und nennen Sie ihn die erste Version - <code>zfs snapshot data/games/disk@ver1</code> .  <strong>Erstellen Sie den Klon</strong> <code>zfs clone data/games/disk@ver1 data/games/disk-vm1</code> , der zur ersten virtuellen Maschine geleitet wird. </li><li>  Wir geben den Klon √ºber iSCSI und <strong>KVM startet eine</strong> virtuelle Maschine <strong>mit dieser Festplatte</strong> .  Es wird geladen, geht in einen Pool von zug√§nglichen Servern f√ºr Benutzer und erwartet einen Spieler. </li><li>  Nach Abschluss der Benutzersitzung werden alle von diesem virtuellen Computer gespeicherten Benutzer <strong>auf einem separaten Server abgelegt</strong> .  Wir <strong>schalten die</strong> virtuelle <strong>Maschine aus und zerst√∂ren den Klon</strong> - <code>zfs destroy data/games/disk-vm1</code> . </li><li>  Wir kehren zum ersten Schritt zur√ºck, erstellen erneut einen Klon und starten die virtuelle Maschine. </li></ul><br>  Dies erm√∂glicht es uns, jedem n√§chsten Benutzer einen <strong>immer sauberen Computer</strong> zur Verf√ºgung zu stellen, auf dem keine √Ñnderungen gegen√ºber dem vorherigen Spieler vorgenommen wurden.  Die Festplatte wird nach jeder Benutzersitzung gel√∂scht, und der auf dem Speichersystem belegte Speicherplatz wird freigegeben.  Wir f√ºhren √§hnliche Vorg√§nge auch mit der Systemfestplatte und mit allen unseren virtuellen Maschinen durch. <br><br>  K√ºrzlich stie√ü ich auf YouTube auf ein Video, in dem ein zufriedener Benutzer w√§hrend einer Spielsitzung unsere Festplatten auf Servern formatierte und sich sehr dar√ºber freute, dass er alles kaputt gemacht hatte.  Ja, bitte, nur um zu bezahlen - er kann spielen und sich verw√∂hnen lassen.  In jedem Fall erh√§lt der n√§chste Benutzer immer eine saubere, funktionsf√§hige virtuelle Maschine, egal was der vorherige tut. <br><br>  Nach diesem Schema werden Spiele auf nur 200 Server verteilt.  Wir haben die Zahl 200 experimentell berechnet: Dies ist die Anzahl der Server, auf denen keine kritischen Belastungen der Speicherlaufwerke auftreten.  Dies liegt daran, dass <strong>Spiele ein ziemlich spezifisches Belastungsprofil haben</strong> : Sie lesen viel in der Startphase oder in der Level-Ladephase und verwenden w√§hrend des Spiels im Gegenteil praktisch keine Diskette.  Wenn Ihr Lastprofil unterschiedlich ist, ist die Abbildung unterschiedlich. <br><br>  Nach dem alten Schema ben√∂tigen wir f√ºr die gleichzeitige Wartung von 200 Benutzern 2.000 TB lokalen Speicher.  Jetzt k√∂nnen wir etwas mehr als 10 TB f√ºr den Hauptdatensatz ausgeben, und es sind noch 0,5 TB f√ºr Benutzerwechsel verf√ºgbar.  Obwohl ZFS es liebt, wenn mindestens 15% des freien Speicherplatzes in seinem Pool vorhanden sind, scheint es mir, dass wir erheblich gespart haben. <br><br><h3>  Was ist, wenn wir mehrere Rechenzentren haben? </h3><br>  Dieser Mechanismus funktioniert nur in einem Rechenzentrum, in dem Server mit einem Speichersystem √ºber mindestens 10-Gigabit-Schnittstellen verbunden sind.  Was tun bei mehreren DCs?  Wie aktualisiere ich die Hauptdiskette mit Spielen (Datensatz) zwischen ihnen? <br><br>  Daf√ºr hat ZFS eine eigene L√∂sung - <strong>den Send / Receive-Mechanismus</strong> .  Der Ausf√ºhrungsbefehl ist sehr einfach: <br><pre> <code class="bash hljs">zfs send -v data/games/disk@ver1 | ssh myzfsuser@myserverip zfs receive data/games/disk</code> </pre> <br>  Mit diesem Mechanismus k√∂nnen Sie einen Snapshot vom Hauptsystem von einem Speichersystem auf ein anderes √ºbertragen.  Zum ersten Mal m√ºssen Sie alle 10 Terabyte an Daten, die auf den Masterknoten geschrieben wurden, an ein leeres Speichersystem senden.  Bei den n√§chsten Aktualisierungen werden √Ñnderungen jedoch erst ab dem Zeitpunkt gesendet, an dem die vorherige Momentaufnahme erstellt wurde. <br><br>  Als Ergebnis erhalten wir: <br><br><ul><li>  <strong>Alle √Ñnderungen werden zentral auf einem Speichersystem vorgenommen</strong> .  Dann verteilen sie sich auf alle anderen Rechenzentren in beliebiger Menge, und die Daten auf allen Knoten sind immer identisch. </li><li>  <strong>Der Sende- / Empfangsmechanismus hat keine Angst vor einer Unterbrechung</strong> .  Daten werden erst dann auf den Hauptdatensatz angewendet, wenn sie vollst√§ndig an den Slave-Knoten √ºbertragen wurden.  Wenn die Verbindung unterbrochen wird, k√∂nnen die Daten nicht besch√§digt werden. Wiederholen Sie einfach den Sendevorgang. </li><li>  <strong>Jeder Knoten kann</strong> w√§hrend eines Unfalls in wenigen Minuten <strong>leicht zum</strong> Masterknoten werden, da die Daten auf allen Knoten immer identisch sind. </li></ul><br><h3>  Deduplizierung und Backups </h3><br>  ZFS bietet eine weitere n√ºtzliche Funktion - die <strong>Deduplizierung</strong> .  Diese Funktion hilft <strong>, zwei identische Datenbl√∂cke nicht zu speichern</strong> .  Stattdessen wird nur der erste Block gespeichert, und anstelle des zweiten wird eine Verkn√ºpfung zum ersten gespeichert.  Zwei identische Dateien belegen einen Speicherplatz und f√ºllen 110% des urspr√ºnglichen Volumens aus, wenn sie zu 90% √ºbereinstimmen. <br><br>  Die Funktion hat uns sehr beim Speichern von Benutzern geholfen.  In einem Spiel haben verschiedene Benutzer eine √§hnliche Speicherung, viele Dateien sind gleich.  Durch die Verwendung der Deduplizierung k√∂nnen wir f√ºnfmal so viele Daten speichern.  Unser Deduplizierungsverh√§ltnis betr√§gt 5,22.  Physikalisch haben wir 4,43 Terabyte, multiplizieren mit einem Faktor und erhalten fast 23 Terabyte echte Daten.  Dies spart Platz, indem doppelte Speicherung vermieden wird. <br><div class="scrollable-table"><table><tbody><tr><td>  NAME </td><td>  Gr√∂√üe </td><td>  ALLOC </td><td>  KOSTENLOS </td><td>  DEDUP </td></tr><tr><td>  Daten </td><td>  7,16 TB </td><td>  4,43 TB </td><td>  2,73 TB </td><td>  5,22x </td></tr></tbody></table></div>  <strong>Snapshots eignen sich gut f√ºr Backups</strong> .  Wir verwenden diese Technologie f√ºr unsere Dateispeicher.  Wenn Sie beispielsweise einen Monat lang jeden Tag ein Bild speichern, k√∂nnen Sie jederzeit an jedem Tag des Monats einen Klon bereitstellen und verlorene oder besch√§digte Dateien abrufen.  Dadurch entf√§llt die Notwendigkeit, den gesamten Speicher zur√ºckzusetzen oder eine vollst√§ndige Kopie davon bereitzustellen. <br><br>  <strong>Wir verwenden Klone, um unseren Entwicklern zu helfen</strong> .  Zum Beispiel m√∂chten sie eine potenziell gef√§hrliche Migration auf einer Kampfbasis erleben.  Es ist nicht schnell, eine klassische Sicherung einer Datenbank bereitzustellen, die sich 1 TB n√§hert.  Daher entfernen wir einfach den Klon von der Basisfestplatte und f√ºgen ihn sofort der neuen Instanz hinzu.  Jetzt k√∂nnen Entwickler dort alles sicher testen. <br><br><h3>  ZFS-API </h3><br>  All dies muss nat√ºrlich automatisiert werden.  Warum auf Server klettern, mit den H√§nden arbeiten, Skripte schreiben, wenn dies Programmierern gegeben werden kann?  Deshalb haben wir unsere einfache <a href="https://github.com/drook/zfsapi">Web-API geschrieben</a> . <br><br>  Wir haben alle Standard-ZFS-Funktionen darin eingeschlossen, den Zugriff auf potenziell gef√§hrliche Funktionen unterbrochen, die das gesamte Speichersystem besch√§digen k√∂nnten, und all dies den Programmierern zur Verf√ºgung gestellt.  Jetzt werden <strong>alle Festplattenvorg√§nge streng zentralisiert</strong> und durch Code ausgef√ºhrt, und wir <strong>kennen immer den Status jeder Festplatte</strong> .  Alles funktioniert super. <br><br><h2>  ZoL - ZFS unter Linux </h2><br>  Wir haben das System zentralisiert und dachten, ist es so gut?  Tats√§chlich m√ºssen wir jetzt f√ºr jede Erweiterung sofort mehrere Server-Racks kaufen: Sie sind an Speichersysteme gebunden, und es ist irrational, das System zu teilen.  Was tun, wenn wir beschlie√üen, einen kleinen Demo-Stand bereitzustellen, um Partnern in anderen L√§ndern die Technologie zu zeigen? <br><br>  Nachdenklich kamen wir zu der alten Idee, <strong>lokale Laufwerke zu verwenden</strong> , aber nur mit all den Erfahrungen und Kenntnissen, die wir erhalten haben.  Wenn Sie die Idee globaler ausbauen, k√∂nnen Sie unseren Benutzern dann nicht nur die M√∂glichkeit geben, unsere Server zu nutzen, sondern auch ihre Computer zu mieten. <br><br>  Die relativ neue <strong>Version</strong> von <strong>ZFS unter Linux - ZoL</strong> hat uns dabei sehr geholfen. <br><blockquote>  Jetzt hat jeder Server seinen eigenen Speicher. </blockquote>  Nur werden nicht 10 Terabyte Daten gespeichert, wie bei einer zentralen Installation, sondern nur 1-2 Distributionen der Spiele, f√ºr die es bereitgestellt wird.  Eine SSD reicht daf√ºr aus.  All dies funktioniert einwandfrei: Jeder n√§chste Benutzer erh√§lt immer eine saubere virtuelle Maschine sowie eine Kampfinstallation. <br><br>  Hier sind wir jedoch auf zwei Probleme gesto√üen. <br><br><h3>  Wie aktualisiere ich? </h3><br>  <strong>Update zentral √ºber SSH, wie wir es in den Rechenzentren tun, wird nicht funktionieren</strong> .  Benutzer k√∂nnen im Gegensatz zu Speichersystemen mit dem lokalen Netzwerk verbunden oder einfach ausgeschaltet werden, und Sie m√∂chten nicht so viele SSH-Verbindungen herstellen. <br><br>  Wir hatten die gleichen Probleme wie bei der Verwendung von rsync.  Torrents auf ZFS k√∂nnen jedoch nicht mehr bezogen werden.  Wir haben uns genau √ºberlegt, wie der Sendemechanismus funktioniert: Er sendet alle ge√§nderten Datenbl√∂cke an den endg√ºltigen Speicher, und Receive wendet sie auf den aktuellen Datensatz an.  Warum nicht die Daten in eine Datei schreiben, anstatt sie an den Endbenutzer zu senden? <br><br>  Das Ergebnis nennen wir <strong>diff</strong> .  Dies ist eine Datei, in die alle zwischen den letzten beiden Snapshots ge√§nderten Bl√∂cke nacheinander geschrieben werden.  Wir haben diesen Diff auf einen CDN gelegt und ihn per HTTP an alle unsere Benutzer gesendet: Er hat den Computer eingeschaltet, festgestellt, dass Aktualisierungen vorgenommen wurden, die Luft abgelassen und ihn mit Receive auf den lokalen Datensatz angewendet. <br><br><h3>  Was tun mit Fahrern? </h3><br>  Zentralisierte Server haben dieselbe Konfiguration, und <strong>Endbenutzer haben immer unterschiedliche Computer und Grafikkarten</strong> .  Selbst wenn wir die Betriebssystemverteilung so weit wie m√∂glich mit allen m√∂glichen Treibern f√ºllen, wird sie beim ersten Start diese Treiber trotzdem installieren wollen, dann wird sie neu gestartet und dann m√∂glicherweise erneut.  Da jedes Mal, wenn wir einen sauberen Klon bereitstellen, alle diese Karussells nach jeder Benutzersitzung auftreten - das ist schlecht. <br><br>  Wir wollten einen Initialisierungslauf durchf√ºhren: Warten Sie, bis Windows hochf√§hrt, alle Treiber installiert, alles tut, was sie will, und erst dann auf diesem Laufwerk arbeiten.  Das Problem ist jedoch, dass die Aktualisierungen unterbrochen werden, wenn Sie √Ñnderungen am Hauptdatensatz vornehmen, da die Daten auf der Quelle und auf dem Empf√§nger unterschiedlich sind und diff einfach nicht angewendet werden. <br><br>  ZFS ist jedoch ein flexibles System, mit dem wir eine kleine Kr√ºcke bauen konnten. <br><br><ul><li>  Erstellen Sie wie gewohnt einen Snapshot: <code>zfs snapshot data/games/os@init</code> . </li><li>  Erstellen Sie den Klon - <code>zfs clone data/games/os@init data/games/os-init</code> - und f√ºhren Sie ihn im Initialisierungsmodus aus. </li><li>  Wir warten darauf, dass alle Treiber installiert werden und alles neu gestartet wird. </li><li>  Schalten Sie die virtuelle Maschine aus und machen Sie erneut einen Schnappschuss.  Diesmal jedoch nicht aus dem urspr√ºnglichen Datensatz, sondern aus dem Initialisierungsklon: <code>zfs snapshot data/games/os-init@ver1</code> . </li><li>  Wir erstellen einen Klon des Schnappschusses mit allen installierten Treibern.  Es wird nicht mehr neu gestartet: <code>zfs clone data/games/os-init@ver1 data/games/os-vm1</code> . </li><li>  Dann arbeiten wir am klassischen Haufen. </li></ul><br>  Jetzt befindet sich dieses System in der Alpha-Testphase.  Wir testen es an echten Benutzern ohne Linux-Kenntnisse, aber sie schaffen es, alles zu Hause bereitzustellen.  Unser oberstes Ziel ist es, dass jeder Benutzer einfach ein bootf√§higes USB-Flash-Laufwerk an seinen Computer anschlie√üt, ein zus√§tzliches SSD-Laufwerk anschlie√üt und es auf unserer Cloud-Plattform ausleiht. <br><br>  Wir haben nur einen kleinen Teil der ZFS-Funktionalit√§t besprochen.  Dieses System kann viel interessanter und unterschiedlicher sein, aber nur wenige kennen sich mit ZFS aus - Benutzer m√∂chten nicht dar√ºber sprechen.  Ich hoffe, dass nach diesem Artikel neue Benutzer in der ZFS-Community auftauchen. <br><br><blockquote>  Abonnieren Sie einen <a href="https://t.me/DevOpsConfChannel">Telegrammkanal</a> oder <a href="http://eepurl.com/bN_0E1">Newsletter</a> , um mehr √ºber neue Artikel und Videos von der <a href="https://devopsconf.io/">DevOpsConf-</a> Konferenz zu erfahren.  Neben dem Newsletter sammeln wir Neuigkeiten von bevorstehenden Konferenzen und berichten beispielsweise, was f√ºr DevOps-Fans auf der <a href="https://www.highload.ru/spb/2020">Saint HighLoad ++</a> interessant sein wird. </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de480622/">https://habr.com/ru/post/de480622/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de480610/index.html">C ++ vtables. Teil 2 (Virtuelle Vererbung + Compiler-generierter Code)</a></li>
<li><a href="../de480612/index.html">Nehmen Sie diese √Ñnderungen vor, um die Barrierefreiheitsstandards f√ºr das Webdesign zu erf√ºllen.</a></li>
<li><a href="../de480614/index.html">Schnelle ENUM</a></li>
<li><a href="../de480618/index.html">Elektronisches Spiel Tic Tac Toe. Zu was bin ich gekommen?</a></li>
<li><a href="../de480620/index.html">SD-WAN und DNA zur Unterst√ºtzung des Administrators: Merkmale von Architekturen und Verfahren</a></li>
<li><a href="../de480626/index.html">Vererbung von Altsystemen und -prozessen oder Die ersten 90 Tage als CTO</a></li>
<li><a href="../de480642/index.html">Einf√ºhrung in Linux ELFs: Verstehen und Analysieren</a></li>
<li><a href="../de480644/index.html">Das Manifest zur Abschaffung von 146 des Strafgesetzbuches und der Boykott von Sberbank und Urheberrechtsinhabern-Parasiten. F√ºr Open Source und Nginx</a></li>
<li><a href="../de480646/index.html">Habr - beste Artikel, Autoren und Statistiken 2019</a></li>
<li><a href="../de480650/index.html">Wessen Haar ist st√§rker: Haarmorphologie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>