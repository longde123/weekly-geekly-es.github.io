<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏿‍🔬 👉🏻 🍥 Organisation de tests sûrs en production. Partie 1 🎁 🙆 🌚</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cet article décrit les différents types de tests en production et les conditions dans lesquelles chacun d'eux est le plus utile, et décrit également c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Organisation de tests sûrs en production. Partie 1</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/funcorp/blog/418081/"><img src="https://habrastorage.org/webt/0q/az/yt/0qazyteu8a-cel_dnpcbmyckzsk.jpeg"><br><br>  Cet article décrit les différents types de tests en production et les conditions dans lesquelles chacun d'eux est le plus utile, et décrit également comment organiser des tests sécurisés de divers services en production. <a name="habracut"></a><br><br>  Il convient de noter que le contenu de cet article s'applique uniquement aux <b>services</b> dont le déploiement est contrôlé par les développeurs.  En outre, vous devez immédiatement avertir que l'utilisation de l'un des types de tests décrits ici n'est pas une tâche facile, ce qui nécessite souvent des modifications sérieuses de la conception, du développement et des tests des systèmes.  Et, malgré le titre de l'article, je ne pense pas que l'un des types de tests en production soit absolument fiable.  Il n'y a qu'une opinion selon laquelle de tels tests peuvent réduire considérablement le niveau de risques à l'avenir, et les coûts d'investissement seront justifiés. <br><br>  <i><b>(Remarque: l'article original étant Longrid, pour la commodité des lecteurs, il est divisé en deux parties).</b></i> <br><br><h2>  Pourquoi les tests en production sont-ils nécessaires si cela peut être fait en staging? </h2><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="974530841190608897"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  L'importance du cluster de mise en scène (ou environnement de mise en scène) par différentes personnes est perçue différemment.  Pour de nombreuses entreprises, le déploiement et le test d'un produit en staging est une étape intégrale précédant sa sortie définitive. <br><br>  De nombreuses organisations bien connues perçoivent la mise en scène comme une copie miniature de l'environnement de travail.  Dans de tels cas, il est nécessaire d'assurer leur synchronisation maximale.  Dans ce cas, il est généralement nécessaire de garantir le fonctionnement de différentes instances de systèmes avec état, tels que des bases de données, et de synchroniser régulièrement les données de l'environnement de production avec le transfert.  L'exception est uniquement les informations confidentielles qui vous permettent d'établir l'identité de l'utilisateur (cela est nécessaire pour se conformer aux exigences du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RGPD</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PCI</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">HIPAA</a> et autres réglementations). <br><br>  Le problème avec cette approche (d'après mon expérience) est que la différence ne réside pas seulement dans l'utilisation d'une instance distincte de la base de données contenant les données réelles de l'environnement de production.  Souvent, la différence s'étend aux aspects suivants: <br><br><ul><li>  La taille du cluster intermédiaire (si vous pouvez l'appeler un «cluster» - il s'agit parfois d'un seul serveur déguisé en cluster); </li><li>  Le fait que le transfert utilise généralement un cluster beaucoup plus petit signifie également que les paramètres de configuration de presque <i>tous les</i> services varient.  Cela s'applique aux configurations d'équilibreurs de charge, de bases de données et de files d'attente, par exemple, le nombre de descripteurs de fichiers ouverts, le nombre de connexions de base de données ouvertes, la taille du pool de threads, etc. Si la configuration est stockée dans une base de données ou un stockage de données de valeurs-clés (par exemple, Zookeeper ou Consul), ces systèmes auxiliaires doivent également être présents dans l'environnement de mise en scène; </li><li>  Le nombre de connexions en ligne traitées par le service sans état ou la méthode de réutilisation des connexions TCP par un serveur proxy (si cette procédure est exécutée); </li><li>  Manque de surveillance dans la mise en scène.  Mais même si la surveillance est effectuée, certains signaux peuvent se révéler totalement inexacts, car un environnement autre que celui de travail est surveillé.  Par exemple, même si vous surveillez la latence ou le temps de réponse d'une requête MySQL, il est difficile de déterminer si le nouveau code contient une requête qui peut lancer une analyse complète de la table dans MySQL, car il est <i>beaucoup</i> plus rapide (et parfois même préférable) d'effectuer une analyse complète de la petite table utilisée dans le test. une base de données plutôt qu'une base de données de production, où une requête peut avoir un profil de performances complètement différent. </li></ul><br>  Bien qu'il soit juste de supposer que toutes les différences ci-dessus ne sont pas des arguments sérieux contre l'utilisation de la mise en scène en tant que telle, contrairement aux antipatterns qui devraient être évités.  Dans le même temps, le désir de tout faire correctement nécessite souvent les énormes coûts de main-d'œuvre des ingénieurs pour garantir un environnement cohérent.  La production est en constante évolution et est influencée par divers facteurs, donc essayer d'atteindre ce match, c'est comme aller nulle part. <br><br>  De plus, même si les conditions de la mise en scène seront aussi similaires que possible à l'environnement de travail, il existe d'autres types de tests qui sont mieux à utiliser sur la base d'informations réelles de production.  Un bon exemple serait le test d'imprégnation, dans lequel la fiabilité et la stabilité d'un service sont testées sur une longue période à des niveaux réels de multitâche et de charge.  Il est utilisé pour détecter les fuites de mémoire, déterminer la durée des pauses dans le CPG, le niveau de charge du processeur et d'autres indicateurs pendant une certaine période de temps. <br><br>  Rien de ce qui précède ne suggère que la mise en scène est <i>complètement</i> inutile (cela deviendra évident après avoir lu la section sur la duplication fantôme des données lors des tests de services).  Cela indique seulement que, bien souvent, ils reposent davantage sur la mise en scène que nécessaire, et dans de nombreuses organisations, il reste le <i>seul</i> type de test effectué avant la sortie complète du produit. <br><br><h2>  L'art du test en production </h2><br>  Il est arrivé historiquement que le concept de «test en production» soit associé à certains stéréotypes et connotations négatives («programmation guérilla», manque ou absence de tests unitaires et d'intégration, négligence ou inattention à la perception du produit par l'utilisateur final). <br><br>  Les tests en production mériteront certainement une telle réputation s'ils sont effectués avec négligence et mal.  Il <i>ne remplace</i> en aucun cas les tests au stade de la pré-production et n'est en aucun cas une <i>tâche simple</i> .  De plus, je soutiens que <i>des</i> tests <i>réussis</i> et <i>sûrs</i> en production nécessitent un niveau d'automatisation important, une bonne compréhension des pratiques établies et la conception de systèmes avec une orientation initiale vers ce type de tests. <br><br>  Afin d'organiser un processus complet et sûr de test efficace des services en production, il est important de ne pas le considérer comme un terme généralisant un ensemble d'outils et de techniques différents.  Malheureusement, cette erreur a également été commise par moi - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans mon article précédent,</a> une classification pas tout à fait scientifique des méthodes de test a été présentée, et dans la section "Test en production", une variété de méthodologies et d'outils ont été regroupés. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2h/bo/od/2hboodum_vrej99d2xm911ifecw.png"></div><br>  <i>Extrait de la note Testing Microservices, the sane way ("A smart approach to testing microservices")</i> <br><br>  Depuis la publication de la note fin décembre 2017, je discute de son contenu et généralement du thème des tests en production avec plusieurs personnes. <br><br>  Au cours de ces discussions, et également après une série de conversations séparées, il m'est apparu clairement que le sujet des tests en production ne pouvait pas être réduit à plusieurs points énumérés ci-dessus. <br><br>  Le concept de «test en production» comprend toute une gamme de techniques appliquées <i>à trois stades différents</i> .  Lesquelles - comprenons. <br><br><img src="https://habrastorage.org/webt/6a/wc/5-/6awc5-yiregc_-2pzrk4i7gjxx0.jpeg"><br><br><h2>  Trois étapes de production </h2><br>  Habituellement, les discussions sur la production sont menées uniquement dans le contexte du déploiement de code dans la production, la surveillance ou dans des situations d'urgence en cas de problème. <br><br>  J'ai moi-même jusqu'à présent utilisé des termes tels que «déploiement», «libération», «livraison», etc., comme synonymes, sans trop réfléchir à leur signification.  Il y a quelques mois, toutes les tentatives pour distinguer ces termes seraient rejetées par moi comme quelque chose d'insignifiant. <br>  Après y avoir réfléchi, j'en suis venu à l'idée qu'il <i>y a un</i> réel besoin de distinguer les différentes étapes de la production. <br><br><h3>  Étape 1. Déploiement </h3><br>  Lorsque les tests (même en production) sont un test de l'atteinte des <i>meilleurs indicateurs possibles</i> , la précision des tests (et en fait de tous les tests) n'est assurée qu'à condition que la méthode de test soit aussi proche que possible de la manière dont le service est réellement utilisé en production. <br><br>  En d'autres termes, les tests doivent être exécutés dans un environnement qui <i>simule le mieux un environnement de travail</i> . <br><br>  Et la <i>meilleure imitation de l'</i> environnement de travail est ... l'environnement de travail lui-même.  Pour effectuer le nombre maximal de tests possible dans un environnement de production, il est nécessaire que le résultat infructueux de l'un d'eux n'affecte pas l'utilisateur final. <br><br>  Ceci, à son tour, n'est possible que si, <b>lors du déploiement du service dans un environnement de production, les utilisateurs n'obtiennent pas un accès direct à ce service</b> . <br><br>  Dans cet article, j'ai décidé d'utiliser la terminologie de l'article <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deploy! = Release</a> écrit par <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Turbine Labs</a> .  Il définit le terme «déploiement» comme suit: <br><br>  «Le déploiement est l'installation par un groupe de travail d'une nouvelle version du code du programme de service dans l'infrastructure de production.  Lorsque nous disons qu'une nouvelle version du logiciel a été <b>déployée</b> , nous voulons dire qu'il s'exécute quelque part dans le cadre de l'infrastructure de travail.  Il peut s'agir d'une nouvelle instance EC2 dans AWS ou d'un conteneur Docker s'exécutant au cœur d'un cluster Kubernetes.  Le service a démarré avec succès, a passé un bilan de santé et est prêt (vous l’espérez!) À traiter les données de l’environnement de production, mais peut ne pas recevoir de données.  C'est un point important, je le souligne à nouveau: <b>pour le déploiement, il n'est pas nécessaire que les utilisateurs aient accès à la nouvelle version de votre service</b> .  Compte tenu de cette définition, le déploiement peut être qualifié de processus avec un risque presque nul. » <br><br>  Les mots «processus à risque zéro» sont simplement un baume pour l'âme de nombreuses personnes qui ont souffert de déploiements infructueux.  La possibilité d'installer un logiciel <i>dans un environnement réel</i> sans y permettre aux utilisateurs présente un certain nombre d'avantages en matière de test. <br>  Premièrement, la nécessité de maintenir des environnements séparés pour le développement, les tests et la mise en scène, qui doivent inévitablement être synchronisés avec la production, est minimisée (et peut même complètement disparaître). <br><br>  De plus, au stade de la conception des services, il devient nécessaire de les isoler les uns des autres afin que l'échec du test d'une instance spécifique du service en production <i>n'entraîne pas</i> une cascade ou n'affecte pas les utilisateurs de l'échec des autres services.  Une solution à cela peut être la conception d'un modèle de données et d'un schéma de base de données dans lesquels les requêtes non idempotentes (principalement les <i>opérations d'écriture</i> ) peuvent: <br><br><ul><li>  A réaliser en relation avec la base de données de l'environnement de production lors de tout lancement de test du service en production (je préfère cette approche); </li><li>  Être rejeté en toute sécurité au niveau de l'application jusqu'à ce qu'ils atteignent le niveau d'écriture ou de sauvegarde; </li><li>  Être alloué ou isolé au niveau de l'enregistrement ou de la sauvegarde d'une manière ou d'une autre (par exemple, en stockant des métadonnées supplémentaires). </li></ul><br><h3>  Étape 2. Libération </h3><br>  Remarque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deploy! = Release</a> définit le terme release comme suit: <br><br>  «Lorsque nous disons que la <b>sortie de</b> la version de service a eu lieu, nous voulons dire qu'elle assure le traitement des données dans l'environnement de production.  En d'autres termes, une <b>version</b> est un processus qui dirige les données de l'environnement de production vers une nouvelle version du logiciel.  Avec cette définition à l'esprit, tous les risques que nous associons à l'envoi de nouveaux flux de données (interruptions, insatisfaction des clients, notes vénéneuses dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">The Register</a> ) concernent la <b>sortie d'un</b> nouveau logiciel, et non son déploiement (dans certaines entreprises, cette étape est également appelée <b>libération</b> . Dans cet article, nous utiliserons le terme <b>version</b> ). " <br><br>  Dans le livre de Google sur SRE, le terme "version" est utilisé dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">chapitre sur l'organisation d'une version logicielle pour le décrire</a> . <br><br>  «Un <b>problème est un élément logique du travail consistant en une ou plusieurs tâches distinctes.</b>  <b>Notre objectif est de coordonner le processus de déploiement avec le profil de risque de ce service</b> . <br><br>  Dans les environnements de développement ou de pré-production, nous pouvons construire toutes les heures et distribuer automatiquement les versions après avoir passé tous les tests.  Pour les grands services orientés utilisateur, nous pouvons commencer la version avec un cluster, puis augmenter son échelle jusqu'à ce que nous mettions à jour tous les clusters.  <b>Pour les éléments d'infrastructure importants, nous pouvons étendre la période de mise en œuvre à plusieurs jours et l'exécuter à tour de rôle dans différentes régions géographiques. »</b> <br><br>  Dans cette terminologie, les mots «version» et «version» signifient ce que le vocabulaire général appelle «déploiement», et les termes souvent utilisés pour décrire diverses stratégies de <i>déploiement</i> (par exemple, déploiement bleu-vert ou déploiement canari) se réfèrent à la <i>publication d'un</i> nouveau logiciel. <br><br>  De plus, une <i>libération</i> infructueuse <i>des</i> applications peut entraîner des interruptions partielles ou importantes du travail.  À ce stade, une <i>restauration</i> ou un <i>correctif</i> est également effectué s'il s'avère que la nouvelle version <i>publiée</i> du service est instable. <br><br>  Le processus de <i>publication</i> fonctionne mieux lorsqu'il est automatisé et s'exécute de manière <i>incrémentielle</i> .  De même, une <i>restauration</i> ou un <i>correctif d'un</i> service est plus utile lorsque le taux d'erreur et la fréquence des demandes sont automatiquement corrélés avec la ligne de base. <br><br><h3>  Étape 3. Après la libération </h3><br>  Si la publication <i>s'est bien déroulée</i> et que la nouvelle version du service traite les données de l'environnement de production sans problèmes évidents, nous pouvons considérer <i>qu'elle a</i> réussi.  Une libération réussie est suivie d'une étape qui peut être appelée «après la libération». <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="961811607759159296"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Tout système suffisamment complexe sera <i>toujours</i> dans un état de perte progressive de performances.  Cela ne signifie pas qu'une <i>restauration</i> ou un <i>correctif est requis</i> .  Au lieu de cela, il est nécessaire de surveiller une telle détérioration (à diverses fins opérationnelles et opérationnelles) et de déboguer si nécessaire.  Pour cette raison, les tests après la publication ne sont plus des routines, mais le <i>débogage</i> ou la collecte de données analytiques. <br><br>  En général, je pense que chaque composant du système doit être créé en tenant compte du fait qu'aucun grand système ne fonctionne parfaitement à 100% et que les dysfonctionnements doivent être reconnus et pris en compte aux étapes de la conception, du développement, des tests, du déploiement et de la surveillance du logiciel fournir. <br><br><hr><br>  Maintenant que nous avons identifié les trois étapes de la production, regardons les différents mécanismes de test disponibles sur chacun d'eux.  Tout le monde n'a pas la possibilité de travailler sur de nouveaux projets ou de réécrire du code à partir de zéro.  Dans cet article, j'ai essayé d'identifier clairement les méthodes qui fonctionneront le mieux lors du développement de nouveaux projets, ainsi que de parler de ce que nous pouvons faire d'autre pour tirer parti des méthodes proposées sans apporter de modifications importantes aux projets de travail. <br><br><h2>  Test de déploiement </h2><br>  Nous avons séparé les étapes de déploiement et de publication les unes des autres, et nous allons maintenant considérer certains types de tests qui peuvent être appliqués après le déploiement du code dans l'environnement de production. <br><br><h3>  Test d'intégration </h3><br>  En règle générale, les tests d'intégration sont effectués par un serveur d'intégration continue dans un environnement de test isolé pour chaque branche Git.  Une copie de la topologie de service <i>complète</i> (y compris les bases de données, les files d'attente, les proxys, etc.) est déployée pour les suites de tests de <i>tous les</i> services qui fonctionneront ensemble. <br><br>  Je pense que ce n'est pas particulièrement efficace pour plusieurs raisons.  Tout d'abord, l'environnement de test, comme l'environnement de transfert, ne peut pas être déployé de manière à être <i>identique à l'</i> environnement de production réel, <i>même si les</i> tests sont exécutés dans le même conteneur Docker qui sera utilisé dans la production.  Cela est particulièrement vrai lorsque la <i>seule chose</i> qui s'exécute dans un environnement de test est les tests eux-mêmes. <br><br>  Que le test soit exécuté en tant que conteneur Docker ou processus POSIX, il établit très probablement <i>une</i> ou plusieurs connexions à un service, une base de données ou un cache supérieur, ce qui est rare si le service se trouve dans un environnement de production où il peut simultanément traiter plusieurs connexions simultanées, en réutilisant souvent des connexions TCP inactives (c'est ce qu'on appelle la réutilisation des connexions HTTP). <br><br>  En outre, le problème est dû au fait que la plupart des tests à chaque démarrage créent une nouvelle table de base de données ou un espace de clé de cache sur <i>le même nœud</i> où ce test est effectué (de cette manière, les tests sont isolés des pannes de réseau).  Ce type de test peut au mieux montrer que le système fonctionne correctement avec une demande très spécifique.  Il est rarement efficace pour simuler des types de défaillances graves et bien répartis, sans parler des différents types de défaillances partielles.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://">Des</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">études</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://">exhaustives</a> existent qui confirment que les systèmes distribués présentent souvent un <i>comportement imprévisible</i> qui ne peut être prévu par une analyse effectuée différemment que pour l'ensemble du système. <br><br>  Mais cela ne signifie pas que les tests d'intégration sont <i>en principe</i> inutiles.  Nous pouvons seulement dire que la mise en œuvre de tests d'intégration dans un <i>environnement artificiel, complètement isolé</i> , n'a généralement pas de sens.  Des tests d'intégration doivent toujours être effectués pour vérifier que la nouvelle version du service: <br><br><ul><li>  N'interfère pas avec l'interaction avec les services en amont ou en aval; </li><li>  N'affecte pas négativement les buts et objectifs des services supérieurs ou inférieurs. </li></ul><br>  Le premier peut être fourni dans une certaine mesure par des tests de contrat.  <i>Étant</i> donné que les <i>interfaces</i> entre les services fonctionnent correctement, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les tests de contrat</a> sont une méthode efficace pour développer et tester des services individuels au <i>stade de la pré-production</i> , ce qui ne nécessite pas le déploiement de la topologie de service complète. <br><br>  Les plates-formes de test de contrat orientées client, telles que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Pact</a> , ne prennent actuellement en charge que l'interopérabilité entre les services via RESTful JSON RPC, bien qu'il soit probable que des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">travaux soient en cours pour prendre en charge l'interaction asynchrone via les sockets Web, les applications non serveur et les files d'attente de messages</a> .  La prise en charge des protocoles gRPC et GraphQL devrait être ajoutée à l'avenir, mais elle n'est pas encore disponible. <br><br>  Cependant, avant la <i>sortie d'une</i> nouvelle version, il peut être nécessaire de vérifier non seulement le bon fonctionnement des <i>interfaces</i> . , , ,   RPC-            .    ,       , ,       . <br><br>  ,     <i></i> ,   — ,       <i>,  </i>    (  ,    ). <br>      :  <i></i>     ? <br><br>     .    – ,       :    -   - ( C)   MySQL ( D)       memcache ( B). <br><br>   ,      (    ),  stateful-  stateless-        . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xl/nc/4e/xlnc4eni3elyx3huaw3trxs5uyc.jpeg"></div><br>  <i></i>  <i></i> ,     <i></i>      . <br><br>  service discovery        <i></i> ( ),      <i></i>  <i></i>        .         <i></i>   . <br><br> ,   <i></i>    C  . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ze/8y/yp/ze8yypuxevqszd_krd3cls4hrqc.jpeg"></div><br>  ,  <i></i>   ,        ,       .      ,       ,       .  ,     <i></i> ,        . <br><br>   Google     <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Just Say No to More End-to-End Tests</a> («      »),      : <br><br> « <i>       ( )      .       ,    ?           ,      </i> . <b>     ,     ,      ,     ».</b> <br><br>    ,          :       <i></i>       .               ,       A        . <br><br>     ,     <i></i>   C     MySQL,     . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/c4/bu/qc/c4buqc8zz2dt4e3pyr6svjnyguc.jpeg"></div><br>  <i> </i>       (   ,   ,         «» ,  <i></i> ).     <i></i>   MySQL   ,    ,      . <br><br>   —        -.      ,      .              -,          . <br><br>    ,   -     <i></i>     ,   /: <br><br><ul><li>       C     /   ; </li><li>         ,   «»   . </li></ul><br>       <i>  </i>    ,             (,   ). <br><br>           ,    ,   <i></i>  .      IP- ,   ,    ,    ,       , ,  ,    ,    . <br><br> ,   ,     ,    ,       .      .     Facebook,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Kraken</a> ,  : <br><br> « <i>   —     ,         .   -      ,    .        ,        .</i> <b>           -     ,     ,     ,       ».</b> <br><br>   ,      ,       ,     ,         ,  . <br><br>     -     .    service mesh             .        -.        -,  ,  ,    : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/kv/cj/ukkvcjxlzn3hdcpt-mxsep3pojm.jpeg"></div><br>  Si nous testons le service B, son serveur proxy sortant peut être configuré pour ajouter un en <code>X-ServiceB-Test</code> tête <code>X-ServiceB-Test</code> spécial à chaque demande de test.  Dans ce cas, le serveur proxy entrant du service supérieur C pourra: <br><br><ul><li>  Détectez cet en-tête et envoyez une réponse standard au service B; </li><li>  Dites au service C que la demande est un <i>test</i> . </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3s/mr/-n/3smr-n7k-_87j3jjpirb02hkaja.jpeg"></div><br>  <i>Test d'intégration de l'interaction de la version déployée du service B avec la version publiée du service C, où les opérations d'écriture n'atteignent jamais la base de données</i> <br><br>  Les tests d'intégration de cette manière permettent également de tester l'interaction du service B avec des services supérieurs <i>lorsqu'ils traitent des données d'environnement de production normales</i> - c'est probablement une imitation plus proche de la façon dont le service B se comportera lors de sa <i>mise</i> en production. <br><br>  Il serait également intéressant que chaque service de cette architecture prenne en charge de véritables appels d'API en mode test ou simulé, vous permettant de tester l'exécution de contrats de service avec des services en aval sans modifier les données réelles.  Cela reviendrait à tester les contrats, mais au niveau du réseau. <br><br><h3>  Duplication de données fantômes (test de flux de données sombres ou mise en miroir) </h3><br>  La duplication des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">clichés instantanés</a> (dans l'article du blog Google, elle est appelée <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lancement sombre</a> , et le terme de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mise en miroir</a> est utilisé dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Istio</a> ) a dans bien des cas plus d'avantages que les tests d'intégration. <br><br>  Les <a href="http://">principes de l'ingénierie du chaos</a> stipulent ce qui suit: <br><br>  «Les <i>systèmes se comportent différemment selon l'environnement et le schéma de transfert de données.</i>  <i>Étant donné que le mode d'utilisation peut changer à tout moment</i> , l' <b>échantillonnage de données réelles est le seul moyen fiable de corriger le chemin de la demande. »</b> <br><br>  La duplication de données fantômes est une méthode par laquelle le flux de données d'environnement de production entrant dans un service donné est capturé et reproduit dans une nouvelle version <i>déployée</i> du service.  Ce processus peut être effectué en temps réel, lorsque le flux de données entrant est divisé et envoyé aux versions <i>publiées</i> et <i>déployées</i> du service, ou de manière asynchrone, lorsqu'une copie des données précédemment capturées est lue dans le service <i>déployé</i> . <br><br>  Lorsque j'ai travaillé chez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">imgix</a> (une start-up <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">employant</a> 7 ingénieurs, dont seulement quatre étaient des ingénieurs système), des flux de données sombres ont été activement utilisés pour tester les changements dans notre infrastructure de visualisation d'images.  Nous avons enregistré un certain pourcentage de toutes les demandes entrantes et les avons envoyées au cluster Kafka - nous avons transmis les journaux d'accès HAProxy au pipeline <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">heka</a> , qui à son tour a transmis le flux de requêtes analysé au cluster Kafka.  Avant la phase de <i>publication, une</i> nouvelle version de notre application de traitement d'image a été testée sur un flux de données sombre capturé, ce qui a permis de vérifier que les demandes étaient correctement traitées.  Cependant, notre système de visualisation d'images était dans l'ensemble un service sans état qui était particulièrement bien adapté à ce type de test. <br><br>  Certaines entreprises préfèrent ne pas capturer une partie du flux de données, mais transmettre une <i>copie complète de</i> ce flux à la nouvelle version de l'application.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le McRouter de Facebook</a> (proxy memcached) prend en charge ce type de duplication fantôme du flux de données memcache. <br><br>  « <i>Lors du test d'une nouvelle installation pour le cache, nous avons trouvé très pratique de pouvoir rediriger une copie complète du flux de données des clients.</i>  <i>McRouter prend en charge les paramètres de duplication d'ombre flexibles.</i>  <i>Il est possible d'effectuer une duplication fantôme d'un pool de différentes tailles (en remettant en cache l'espace clé), de copier uniquement une partie de l'espace clé ou de modifier dynamiquement les paramètres pendant le fonctionnement</i> . <br><br>  L'aspect négatif de la duplication fantôme de l'ensemble du flux de données pour un service <i>déployé</i> dans un environnement de production est que s'il est exécuté au moment de l'intensité de transfert de données maximale, il peut alors nécessiter deux fois plus de puissance. <br>  Des proxys tels qu'Envoy prennent en charge la duplication fantôme du flux de données vers un autre cluster en mode incendie et oubli.  Sa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> dit: <br><br>  « <i>Un routeur peut effectuer une duplication fantôme du flux de données d'un cluster à un autre.</i>  <i>Actuellement, le mode de tir et d'oubli est implémenté, dans lequel le serveur proxy Envoy n'attend pas de réponse du cluster fantôme avant de renvoyer une réponse du cluster principal.</i>  <i>Pour le cluster fantôme, toutes les statistiques habituelles sont collectées, ce qui est utile à des fins de test.</i>  <i>Avec la duplication fantôme, l'option <code>-shadow</code> est ajoutée à l'en- <code>-shadow</code> hôte / autorité.</i>  <i>Ceci est utile pour la journalisation.</i>  <i>Par exemple, <code>cluster1</code> transforme en <code>cluster1-shadow</code></i> . " <br><br>  Cependant, il est souvent impossible ou impossible de créer une réplique d'un cluster synchronisé avec la production pour les tests (pour la même raison qu'il est problématique d'organiser un cluster intermédiaire synchronisé).  Si la duplication miroir est utilisée pour tester un nouveau service <i>déployé</i> qui a de nombreuses dépendances, il peut déclencher des changements imprévus dans l'état des services de niveau supérieur par rapport à celui testé.  La duplication fantôme du volume quotidien d'inscriptions d'utilisateurs dans la version <i>déployée</i> du service avec enregistrement dans la base de données de production peut entraîner une augmentation du taux d'erreur jusqu'à 100% du fait que le flux de données fantôme sera perçu comme des tentatives répétées d'enregistrement et rejetées. <br><br>  Mon expérience personnelle suggère que la duplication d'ombre est la mieux adaptée pour tester les demandes non idempotentes ou les services sans état avec des stubs côté serveur.  Dans ce cas, la duplication fantôme des données est plus souvent utilisée pour tester la charge, la stabilité et les configurations.  Dans le même temps, à l'aide de tests d'intégration ou de transfert, vous pouvez tester la façon dont le service interagit avec un serveur avec état lorsque vous travaillez avec des requêtes non idempotentes. <br><br><h3>  Comparaison TAP </h3><br>  La seule mention de ce terme se trouve dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> du blog Twitter consacré au lancement de services à haut niveau de qualité de service. <br><br>  <i>«Pour vérifier l'exactitude de la nouvelle implémentation du système existant, nous avons utilisé une méthode appelée <b>tap-comparaison</b> .</i>  <i>Notre outil de comparaison de prises reproduit les données de production d'échantillons dans le nouveau système et compare les réponses reçues avec les résultats de l'ancien.</i>  <i>Les résultats obtenus nous ont aidés à trouver et à corriger les erreurs dans le système avant même que les utilisateurs finaux ne les rencontrent. »</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un autre</a> article de blog Twitter définit les comparaisons de prises comme suit: <br><br>  <i>«Envoi de demandes aux instances de service à la fois dans les environnements de production et de transfert avec <b>vérification des résultats</b> et évaluation des caractéristiques de performance.»</i> <br><br>  La différence entre la comparaison de prises et la duplication fantôme est que dans le premier cas, la réponse renvoyée par la version <i>publiée</i> est comparée à la réponse retournée par la version <i>déployée</i> , et dans le second, la demande est dupliquée vers la version <i>déployée</i> en mode hors ligne comme le feu et oublie. <br><br>  Un autre outil pour travailler dans ce domaine est la bibliothèque <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">scientifique</a> , disponible sur GitHub.  Cet outil a été développé pour tester le code Ruby, mais a ensuite été porté dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">plusieurs autres langages</a> .  Il est utile pour certains types de tests, mais présente un certain nombre de problèmes non résolus.  Voici ce qu'un développeur GitHub a écrit dans une communauté Slack professionnelle: <br><br>  <i>«Cet outil exécute simplement deux branches de code et compare les résultats.</i>  <i>Vous devez être prudent avec le code de ces branches.</i>  <i>Il est nécessaire de s'assurer que les requêtes de base de données ne sont pas dupliquées si cela entraîne des problèmes.</i>  <i>Je pense que cela s'applique non seulement à un scientifique, mais aussi à toute situation dans laquelle vous faites quelque chose deux fois, puis comparez les résultats.</i>  <i>L'outil scientifique a été créé pour vérifier que le nouveau système d'autorisation fonctionne de la même manière que l'ancien, et à certains moments, il a été utilisé pour comparer les données typiques de pratiquement chaque demande de Rails.</i>  <i>Je pense que le processus prendra plus de temps, car le traitement est effectué séquentiellement, mais c'est un problème Ruby dans lequel les threads ne sont pas utilisés.</i> <i><br><br></i>  <i>Dans la plupart des cas que je connais, l'outil scientifique a été utilisé pour travailler avec des opérations de lecture plutôt que d'écriture, par exemple, pour savoir si les nouvelles requêtes améliorées et les nouveaux schémas d'autorisation reçoivent la même réponse que les anciens.</i>  <i>Les deux options sont exécutées dans un environnement de production (sur des répliques).</i>  <i>Si les ressources testées ont des effets secondaires, je pense que les tests devront être effectués au niveau de l'application. »</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Diffy</a> est un outil open source écrit Scala introduit par Twitter en 2015.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Un article</a> du blog Twitter intitulé <b>Testing without Writing Tests</b> est probablement la meilleure ressource pour comprendre comment fonctionnent les comparaisons de touches dans la pratique. <br><br>  <i>«Diffy détecte les erreurs potentielles dans le service en lançant simultanément une nouvelle et une ancienne version du code.</i>  <i>Cet outil fonctionne comme un serveur proxy et envoie toutes les demandes reçues à chacune des instances en cours d'exécution.</i>  <i>Il compare ensuite les réponses des instances et rapporte tous les écarts trouvés lors de la comparaison.</i>  <i>Diffy est basé sur l'idée suivante:</i> <b>si deux implémentations d'un service retournent les mêmes réponses avec un ensemble de requêtes suffisamment grand et varié, alors ces deux implémentations peuvent être considérées comme équivalentes, et la plus récente d'entre elles sans dégradation des performances.</b>  <i>La technique innovante de réduction du bruit de Diffy la distingue des autres outils d'analyse de régression comparative. »</i> <br><br>  La comparaison des prises est excellente lorsque vous devez vérifier si deux versions donnent les mêmes résultats.  Selon Mark McBride, <br><br>  <i>«L'outil Diffy était souvent utilisé dans la refonte des systèmes.</i>  <i>Dans notre cas, nous avons divisé la base de code source de Rails en plusieurs services créés à l'aide de Scala, et un grand nombre de clients API ont utilisé les fonctions différemment de ce que nous attendions.</i>  <i>Des fonctions telles que le formatage de la date étaient particulièrement dangereuses. »</i> <br><br>  La comparaison de prises n'est pas la meilleure option pour tester l'activité des utilisateurs ou l'identité du comportement de deux versions du service à la charge maximale.  Comme pour la duplication fantôme, les effets secondaires restent un problème non résolu, en particulier lorsque la version déployée et la version de production écrivent des données dans la même base de données.  Comme pour les tests d'intégration, une solution à ce problème consiste à utiliser des tests de comparaison de prises avec uniquement un ensemble limité de comptes. <br><br><h3>  Test de charge </h3><br>  Pour ceux qui ne sont pas familiers avec les tests de résistance, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cet article</a> peut servir de bon point de départ.  Il ne manque pas d'outils et de plates-formes pour les tests de charge open source.  Les plus populaires d'entre eux sont <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=http://">Apache Bench</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gatling</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">wrk2</a> , <a href="http://">Tsung</a> , écrit en Erlang, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Siege</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Iago</a> de Twitter, écrit en Scala (qui reproduit les journaux d'un serveur HTTP, d'un serveur proxy ou d'un analyseur de paquets réseau dans une instance de test).  Certains experts estiment que le meilleur outil pour générer de la charge est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">mzbench</a> , qui prend en charge une variété de protocoles, y compris MySQL, Postgres, Cassandra, MongoDB, TCP, etc. Le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NDBench</a> de Netflix est un autre outil open source pour tester la charge des entrepôts de données. , qui prend en charge la plupart des protocoles connus. <br><br>  Le blog officiel Twitter d' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Iago</a> décrit plus en détail les fonctionnalités qu'un bon générateur de charge devrait avoir: <br><br>  <i>«Les demandes non bloquantes sont générées à une fréquence donnée sur la base d'une distribution statistique personnalisée interne ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le processus de Poisson est</a> modélisé par défaut).</i>  <i>La fréquence des demandes peut être modifiée au besoin, par exemple pour préparer le cache avant de travailler à pleine charge.</i> <i><br><br></i>  <i>En général, l'accent est mis sur la fréquence des demandes conformément à <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">la loi de Little</a> , et non sur le nombre d'utilisateurs simultanés, qui peut varier en fonction du délai inhérent à ce service.</i>  <i>De ce fait, de nouvelles opportunités apparaissent pour comparer les résultats de plusieurs tests et prévenir la détérioration du service, ralentissant le fonctionnement du générateur de charge.</i> <i><br><br></i>  <i>En d'autres termes, l'outil Iago cherche à simuler un système dans lequel les demandes sont reçues quelle que soit la capacité de votre service à les traiter.</i>  <i>En cela, il diffère des générateurs de charge simulant des systèmes fermés dans lesquels les utilisateurs travailleront patiemment avec le retard existant.</i>  <i>Cette différence nous permet de modéliser assez précisément les modes de défaillance qui peuvent être rencontrés en production. "</i> <br><br>  Un autre type de test de charge est le test de contrainte en redistribuant le flux de données.  Son essence est la suivante: l'ensemble du flux de données de l'environnement de production est dirigé vers un cluster plus petit que celui préparé pour le service;  en cas de problème, le flux de données est retransféré vers le cluster le plus important.  Cette technique est utilisée par Facebook, comme décrit dans l'un des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">articles de son blog officiel</a> : <br><br>  <i>«Nous redirigeons spécifiquement un flux de données plus important vers des clusters ou nœuds individuels, mesurons la consommation de ressources sur ces nœuds et déterminons les limites de la stabilité des services.</i>  <i>Ce type de test, en particulier, est utile pour déterminer les ressources CPU nécessaires pour prendre en charge le nombre maximal de diffusions simultanées de Facebook Live. »</i> <br><br>  Voici ce qu'écrit un ancien ingénieur LinkedIn de la communauté professionnelle Slack: <br><br>  <i>«LinkedIn a également utilisé des tests de redline en production - les serveurs ont été supprimés de l'équilibreur de charge jusqu'à ce que la charge atteigne des valeurs seuil ou que des erreurs commencent à se produire.</i> <br><br>  En effet, une recherche Google fournit un lien vers un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livre blanc complet</a> et un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article de</a> blog LinkedIn sur ce sujet: <br><br>  <i>«La solution Redliner pour les mesures utilise un véritable flux de données de l'environnement de production, ce qui évite les erreurs qui empêchent des mesures précises des performances en laboratoire.</i> <i><br><br></i>  <i>Redliner redirige une partie du flux de données vers le service testé et analyse en temps réel ses performances.</i>  <i>Cette solution a été mise en œuvre dans des centaines de services LinkedIn internes et est utilisée quotidiennement pour divers types d'analyses de performances.</i> <i><br><br></i>  <i>Redliner prend en charge l'exécution de tests parallèles pour les instances canaries et de production.</i>  <b>Cela permet aux ingénieurs de transférer la même quantité de données vers deux instances de service différentes: 1) une instance de service qui contient des innovations, telles que de nouvelles configurations, propriétés ou nouveau code;</b>  <b>2) une instance de service de la version de travail actuelle.</b> <br><br>  <i>"Les résultats des tests de charge sont pris en compte lors de la prise de décisions et aident à empêcher le déploiement de code, ce qui peut entraîner de mauvaises performances."</i> <br><br>  Facebook a fait passer les tests de charge à l'aide de flux de données réels à un tout autre niveau grâce au système Kraken, et sa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">description</a> mérite également d'être lue. <br>  Le test est mis en œuvre en redistribuant le flux de données lors de la modification des valeurs de poids (lues depuis le magasin de configuration distribué) pour les périphériques et les clusters de bordure dans la configuration <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Proxygen</a> (équilibreur de charge Facebook).  Ces valeurs déterminent le volume de données réelles envoyées, respectivement, à chaque cluster et région à un point de présence donné. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/0e/by/wk0ebyvdpznpgdgqcc_6zbu1g6g.png"></div><br>  <i>Données du livre blanc Kraken</i> <br><br>  Le système de surveillance ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gorilla</a> ) affiche des indicateurs de divers services (comme indiqué dans le tableau ci-dessus).  Sur la base des données de surveillance et des seuils, il est décidé d'envoyer des données supplémentaires conformément aux valeurs de poids, ou s'il est nécessaire de réduire, voire d'arrêter complètement le transfert de données vers un cluster spécifique. <br><br><h2>  Tests de configuration </h2><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="916383043933192192"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Une nouvelle vague d'outils d'infrastructure open source a rendu possible la capture de tous les changements dans l'infrastructure sous forme de code, mais aussi relativement <i>facile</i> .  Il est également devenu possible à des degrés divers de <i>tester</i> ces changements, bien que la plupart des tests d'infrastructure en tant que code au stade de la pré-production ne puissent que confirmer les spécifications et la syntaxe correctes. <br><br>  De plus, le refus de tester la nouvelle configuration avant la <i>sortie du</i> code est devenu la cause d'un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nombre important d'interruptions</a> . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="963093541575581696"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Pour un test holistique des changements de configuration, il est important de distinguer les différents types de configurations.  Fred Hebert a suggéré une fois d'utiliser le quadrant suivant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/fi/4i/yvfi4ioub6pkvt1mtzxvz2xuvnk.png"></div><br>  Cette option, bien sûr, n'est pas universelle, mais cette distinction vous permet de décider comment tester au mieux chacune des configurations et à quelle étape le faire.  La configuration du temps de construction est logique si vous pouvez garantir une véritable répétabilité des assemblages.  Toutes les configurations ne sont pas statiques, mais sur les plates-formes modernes, un changement de configuration dynamique est inévitable (même si nous avons affaire à une «infrastructure permanente»). <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-4" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1005924617981005824"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>       ,   ,       blue-green ,        .   ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jamie Wilkinson</a> ),  Google  , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> : <br><br> <i>«        ,   ,   ,  -     .    .</i> <b>    -  ,         —  ,       ,   .        .</b> <br><br> <i>            ,  .     ,    , —    ».</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">  Facebook</a>             : <br><br> <i>«           .    —  ,              .             .        ,      .</i> <br><br><ul><li> <b>   </b> <br><br>             .  Facebook ,          .             ,     . </li><li> <b>   </b> <br><br>         (,  JSON).           ,           .          . <br><br>   (,  Facebook  Thrift)      .   ,           . </li><li> <b> </b> <br><br>         ,     ,    - .       .   — A/B-,        1 % .     A/B-,        .      A/B-    . ,  ,                 ,       ,     .  , A/B-    .      ,    A/B-.     Facebook        . <br><br> ,     A/B-  1% ,   1%     ,          (   «  »).          ,         .               ,      . </li><li> <b>  </b> <br><br>   Facebook            .       ,        .    ,              ,     .   ,  ,           . </li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Annulation simple et pratique des modifications</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Dans certains cas, malgré toutes les mesures préventives, le déploiement d'une configuration inopérante est effectué. </font><font style="vertical-align: inherit;">Trouver et annuler rapidement les modifications est essentiel pour résoudre un tel problème. </font><font style="vertical-align: inherit;">"Les outils de contrôle de version sont disponibles dans notre système de configuration, ce qui facilite beaucoup l'annulation des modifications."</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">À suivre! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UPD: continué </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ici</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr418081/">https://habr.com/ru/post/fr418081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr418069/index.html">Résolution de mots croisés de couleurs japonaises à la vitesse de la lumière</a></li>
<li><a href="../fr418071/index.html">L'industrie informatique pour le peuple: TechTrain Festival à Saint-Pétersbourg</a></li>
<li><a href="../fr418075/index.html">TOP 5 des choses qui peuvent être imprimées sur une imprimante 3D [vidéo]</a></li>
<li><a href="../fr418077/index.html">Accidents de «ne pas regarder»: une justification statistique du mode de fonctionnement du support technique 24/7</a></li>
<li><a href="../fr418079/index.html">Les langages de programmation les plus populaires - 2018</a></li>
<li><a href="../fr418083/index.html">Serveur simple avec GraphQL au lieu de REST, implémentation en java</a></li>
<li><a href="../fr418085/index.html">Utilisation de promesses en JavaScript</a></li>
<li><a href="../fr418087/index.html">80% des caisses libre-service sont à risque</a></li>
<li><a href="../fr418089/index.html">Présentation de la fraiseuse CNC SolidCraft</a></li>
<li><a href="../fr418091/index.html">Liste d'articles et de littérature sur NAS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>