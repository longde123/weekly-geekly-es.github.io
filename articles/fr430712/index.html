<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👲🏿 🚷 🦄 NeurIPS: Comment conquérir la meilleure conférence ML 👩🏿 🎓 🦏</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="NeurIPS –– une conférence qui est actuellement considérée comme l'événement le plus important dans le monde de l'apprentissage automatique. Aujourd'hu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>NeurIPS: Comment conquérir la meilleure conférence ML</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/430712/"><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NeurIPS</a> –– une conférence qui est actuellement considérée comme l'événement le plus important dans le monde de l'apprentissage automatique.  Aujourd'hui, je vais vous parler de mon expérience de participation aux concours NeurIPS: comment rivaliser avec les meilleurs universitaires du monde, remporter un prix et publier un article. </p><br><img src="https://habrastorage.org/webt/hb/kq/-v/hbkq-vnd_xgxhvcixlo-u8b_pmk.jpeg"><a name="habracut"></a><br><hr><br><h1 id="v-chem-sut-konferencii">  Quelle est l'essence de la conférence? </h1><br><p>  NeurIPS soutient l'introduction de méthodes d'apprentissage automatique dans diverses disciplines scientifiques.  Une dizaine de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pistes sont</a> lancées chaque année pour résoudre les problèmes pressants du monde académique.  Selon les résultats du concours, les gagnants interviennent lors de la conférence avec des rapports, de nouveaux développements et des algorithmes.  Surtout, je suis passionné par l'apprentissage renforcé (Reinforcement Learning ou RL), c'est pourquoi je participe à des concours RL dédiés à NeurIPS pour la deuxième année maintenant. </p><br><h1 id="pochemu-neurips">  Pourquoi NeurIPS </h1><br><img src="https://habrastorage.org/webt/ei/c2/us/eic2usvfs-brxmsjczvkvygpfwq.png"><br><br>  NeurIPS se concentre principalement sur la science, pas sur l'argent.  En participant à des concours, vous faites quelque chose de vraiment important, en traitant des problèmes urgents. <br><p>  Deuxièmement, cette conférence est un événement mondial, des scientifiques de différents pays se réunissent en un seul endroit, avec chacun desquels vous pouvez parler. </p><br><p>  De plus, toute la conférence est remplie des dernières réalisations scientifiques et des résultats de pointe, il est extrêmement important pour les personnes du domaine de la science des données de les connaître et de les surveiller. </p><br><h1 id="kak-nachat">  Comment commencer? </h1><br><p>  Commencer à participer à de telles compétitions est assez simple.  Si vous comprenez tellement DL que vous pouvez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">former ResNet</a> –– cela suffit: inscrivez-vous et c'est parti.  Il y a toujours un classement public sur lequel vous pouvez évaluer sobrement votre niveau par rapport aux autres participants.  Et si quelque chose n'est pas clair - il y a toujours des canaux dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">slack</a> / discord / gitter / etc pour discuter de toutes les questions émergentes.  Si le sujet est vraiment «le vôtre», alors rien ne vous empêchera de recevoir le résultat recherché –– dans tous les concours auxquels j'ai participé, toutes les approches et solutions ont été étudiées et mises en œuvre tout au long du concours. </p><br><h1 id="neurips-na-primere-konkretnogo-keysa-learning-to-run">  Étude de cas NeurIPS: apprendre à courir </h1><br><img src="https://habrastorage.org/webt/hu/ws/d5/huwsd5weqocxiuqv3hugygmfqea.jpeg"><br><br><h3 id="problematika">  Problème </h3><br><p>  La démarche d'une personne est le résultat de l'interaction des muscles, des os, des organes de vision et de l'oreille interne.  En cas de perturbation du système nerveux central, certains troubles moteurs peuvent survenir, notamment des troubles de la marche –– abasie. <br>  Des chercheurs du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Stanford Laboratory of Neuromuscular Biomechanics ont</a> décidé de connecter l'apprentissage automatique au problème du traitement afin de pouvoir expérimenter et tester leurs théories sur un modèle virtuel du squelette, et non sur des personnes vivantes. </p><br><h3 id="postanovka-zadachi">  Énoncé du problème </h3><br><p>  Les participants ont reçu un squelette humain virtuel (dans le simulateur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenSim</a> ), qui avait une prothèse à la place d'une jambe.  La tâche consistait à apprendre au squelette à se déplacer dans une certaine direction à une vitesse donnée.  Pendant la simulation, la direction et la vitesse peuvent changer. </p><br><img src="https://habrastorage.org/webt/od/vj/np/odvjnpxb7xogj5h_5ll85iokhp0.jpeg"><br><br>  Pour obtenir un modèle de contrôle de squelette virtuel, il a été proposé d'utiliser l'apprentissage par renforcement.  Le simulateur nous a donné un état du squelette S (un vecteur de ~ 400 nombres).  Il était nécessaire de prédire quelle action A devait être effectuée (les forces d'activation des muscles des jambes sont un vecteur de 19 nombres).  Au cours de la simulation, le squelette a reçu un prix R - comme une sorte de constante moins une pénalité pour avoir dévié d'une vitesse et d'une direction données. <br><div class="spoiler">  <b class="spoiler_title">À propos de la formation de renforcement</b> <div class="spoiler_text"><p>  L'apprentissage par renforcement (RL) est un domaine qui traite de la théorie de la décision et de la recherche de politiques comportementales optimales. </p><br><p>  Rappelez-vous comment ils enseignent <del>  chat </del>  levrette de nouveaux trucs.  Répétez une action, donnez un délicieux pour effectuer un tour et ne donnez pas pour non-accomplissement.  Le chien doit comprendre tout cela et trouver une stratégie comportementale («politique» ou «politique» en termes de RL), qui maximise le nombre de bonbons reçus. </p><br><p>  Formellement, nous avons un agent (chien) qui est formé sur l'histoire des interactions avec l'environnement (personne).  Dans le même temps, l'environnement, évaluant les actions de l'agent, lui fournit une récompense (délicieuse) - meilleur est le comportement de l'agent, meilleure est la récompense.  En conséquence, la tâche de l'agent est de trouver une politique qui maximise bien la récompense pour tout le temps d'interaction avec l'environnement. </p><br><p>  Développer davantage ce sujet, des solutions basées sur des règles - logiciel 1.0, lorsque toutes les règles ont été définies par le développeur, apprentissage supervisé - le même logiciel 2.0, lorsque le système apprend lui-même à l'aide des exemples disponibles et trouve des dépendances de données, l'apprentissage par renforcement est un peu plus loin lorsque le système lui-même apprend à rechercher, expérimenter et trouver les dépendances requises dans ses décisions.  Plus nous allons loin, mieux nous essayons de répéter comment une personne apprend. </p></div></div><br><h3 id="osobennosti-zadachi">  Caractéristiques des tâches </h3><br><p>  Le devoir ressemble à un représentant typique de l'apprentissage renforcé pour les tâches avec un espace d'action continue (RL pour l'espace d'action continue).  Il diffère du RL ordinaire en ce qu'au lieu de choisir une action spécifique (en appuyant sur le bouton du joystick), cette action est nécessaire pour prédire avec précision (et il existe une infinité de possibilités). </p><br><p>  L'approche de base de la solution ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Deterministic Policy Gradient</a> ) a été inventée en 2015, qui pendant longtemps selon les normes de DL, la région continue de développer activement des applications à la robotique et aux applications RL du monde réel.  Il y a quelque chose à améliorer: des approches robustes (pour ne pas casser un vrai robot), l'efficacité des échantillons (pour ne pas collecter des données de vrais robots pendant des mois) et d'autres problèmes de RL (exploration vs compromis d'exploitation, etc.).  Dans cette compétition, ils ne nous ont pas donné un vrai robot - seulement une simulation, mais le simulateur lui-même était 2000 fois plus lent que les homologues Open Source (sur lesquels tout le monde vérifie leurs algorithmes RL), et a donc porté le problème de l'efficacité des échantillons à un nouveau niveau. </p><br><h3 id="etapy-sorevnovaniya">  Étapes de compétition </h3><br><p>  Le concours lui-même s'est déroulé en trois étapes, au cours desquelles la tâche et les conditions ont quelque peu changé. </p><br><ul><li>  Étape 1: le squelette a appris à marcher droit à une vitesse de 3 mètres par seconde.  La tâche était considérée comme terminée si l'agent avait effectué 300 étapes. </li><li>  Étape 2: la vitesse et la direction ont changé avec une fréquence régulière.  La longueur de la distance est passée à 1000 pas. </li><li>  Étape 3: la solution finale devait être emballée dans une image docker et envoyée pour vérification.  Au total, 10 colis ont pu être réalisés. </li></ul><br><p>  La mesure de qualité principale a été considérée comme la récompense totale de la simulation, qui a montré à quel point le squelette adhérait à une direction et une vitesse données sur toute la distance. </p><br><p>  Lors des 1ère et 2ème étapes, la progression de chaque participant a été affichée sur le classement.  La solution finale devait être envoyée sous forme d'image docker.  Il prévoyait des restrictions sur les heures de travail et les ressources. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: classement public et RL</b> <div class="spoiler_text"><p>  En raison de la disponibilité du classement, personne ne montre son meilleur modèle afin de donner «un peu plus que d'habitude» dans le tour final et surprendre ses rivaux. </p></div></div><br><h6 id="pochemu-tak-vazhny-docker-obrazy">  Pourquoi les images Docker sont si importantes </h6><br><p>  L'année dernière, un petit incident s'est produit lors de l'évaluation des décisions du tout premier tour.  À ce moment-là, le chèque est passé par l'interaction http avec la plate-forme, et un visage des conditions de test a été trouvé.  On peut savoir dans quelles situations particulières l'agent a été évalué et le recycler uniquement dans ces conditions.  Ce qui, bien sûr, n'a pas résolu le vrai problème.  C'est pourquoi ils ont décidé de transférer le système de soumissions à docker-images et de le lancer sur les serveurs distants des organisateurs.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dbrain</a> utilise le même système pour calculer le résultat des compétitions précisément pour les mêmes raisons. </p><br><h1 id="klyuchevye-momenty">  Points clés </h1><br><h3 id="komanda">  L'équipe </h3><br><img src="https://habrastorage.org/webt/ty/ur/gp/tyurgpqbzb2zl2wimtzri0mnpwk.jpeg"><br><br>  La première chose qui est importante pour le succès de toute l'entreprise est l'équipe.  Peu importe à quel point vous êtes bon (et à quel point vos pattes sont puissantes) - la participation à l'équipe augmente considérablement les chances de succès.  La raison en est simple - une variété d'opinions et d'approches, une nouvelle vérification des hypothèses, la possibilité de paralléliser le travail et de mener plus d'expériences.  Tout cela est extrêmement important lors de la résolution de nouveaux problèmes auxquels vous devez faire face. <br><p>  Idéalement, vos connaissances et compétences devraient être au même niveau et se compléter.  Ainsi, par exemple, cette année, j'ai implanté notre équipe sur PyTorch, et j'ai eu quelques idées initiales sur la mise en œuvre d'un système de formation d'agent distribué. </p><br><p>  Comment trouver une équipe?  Tout d'abord, vous pouvez rejoindre les rangs des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ods</a> et y rechercher des personnes partageant les mêmes idées.  Deuxièmement, pour les boursiers RL, il existe une salle de discussion séparée dans un télégramme - le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">club RL</a> .  Troisièmement, vous pouvez suivre un merveilleux cours de ShAD - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Practical RL</a> , après quoi vous obtiendrez sûrement quelques connaissances. </p><br><p>  Cependant, il convient de rappeler la politique de «soumission - ou ne l'était pas».  Si vous voulez vous unir, obtenez d'abord votre décision, soumettez, apparaissez dans le classement et montrez votre niveau.  Comme le montre la pratique, ces équipes sont beaucoup plus équilibrées. </p><br><h3 id="motivaciya">  La motivation </h3><br><p>  Comme je l'ai déjà écrit, si le sujet est «le vôtre», alors rien ne vous arrêtera.  Cela signifie que la région ne vous aime pas seulement, mais vous inspire - vous la brûlez, vous voulez en devenir la meilleure. <br>  J'ai rencontré RL il y a 4 ans - lors du passage du <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Berkeley 188x - Intro à l'IA</a> - et je n'arrête pas de m'interroger sur les progrès dans ce domaine. </p><br><h3 id="sistematichnost">  Systématique </h3><br><p>  Troisièmement, mais tout aussi important - vous devez être capable de faire ce que vous avez promis, d'investir dans la compétition tous les jours et juste ... de le résoudre.  Tous les jours.  Aucun talent inné ne peut être comparé à la capacité de faire quelque chose, même un peu, mais tous les jours.  C'est pour cela que la motivation est nécessaire.  Pour réussir, je recommande de lire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DeepWork</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AMA ternaus</a> . </p><br><h3 id="time-management">  Gestion du temps </h3><br><p>  Une autre compétence extrêmement importante est la capacité de répartir sa force et d’utiliser correctement le temps libre.  Combiner travail à plein temps et participation à des compétitions n'est pas une tâche aisée.  La chose la plus importante dans ces conditions est de ne pas brûler et de supporter toute la charge.  Pour ce faire, vous devez gérer correctement votre temps, évaluer sobrement votre force et ne pas oublier de vous détendre à temps. </p><br><h3 id="overwork">  Surmenage </h3><br><p>  Au stade final de la compétition, une situation se présente généralement où littéralement en une semaine, vous devez faire non seulement beaucoup, mais TRÈS beaucoup.  Pour le meilleur résultat, vous devez être en mesure de vous forcer à vous asseoir et à faire le dernier bond vers le prix convoité. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: délai après délai</b> <div class="spoiler_text"><p>  À cause de quoi, en général, vous pourriez avoir besoin de recycler au profit de la concurrence?  La réponse est assez simple - transfert de date limite.  Lors de telles compétitions, les organisateurs ne peuvent souvent pas tout prévoir, car le moyen le plus simple est de donner plus de temps aux participants.  Cette année, le concours a été prolongé 3 fois: d'abord pour un mois, puis pour une semaine et au tout dernier moment (24 heures avant la date limite) - pour encore 2 jours.  Et si au cours des deux premiers transferts, vous aviez juste besoin d'organiser correctement le temps supplémentaire, alors au cours des deux derniers jours, vous n'aviez qu'à labourer. </p></div></div><br><h3 id="theory">  Théorie </h3><br><img src="https://habrastorage.org/webt/gf/rg/9q/gfrg9ql1ukvjmlbglwcizlfcpto.png"><br><p>  Entre autres choses, n'oubliez pas la théorie - être conscient de ce qui se passe sur le terrain et être capable de noter ce qui est pertinent.  Ainsi, par exemple, pour résoudre l'an dernier, notre équipe est partie des articles suivants: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le contrôle continu avec l'apprentissage par renforcement profond</a> est un article de base sur l'apprentissage par renforcement profond pour les tâches avec un espace d'action continue. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Paramètre Space Noise for Exploration</a> - une étude sur l'ajout de bruit aux poids des agents pour une meilleure étude de l'environnement.  Par expérience - l'une des meilleures techniques d'exploration en RL. </li></ul><br><p>  Cette année, quelques autres y ont été ajoutés: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Une perspective distributionnelle sur l'apprentissage par renforcement</a> - Un nouveau regard sur les prédictions d'une récompense possible.  Au lieu de simplement prédire la moyenne, la distribution des récompenses futures est calculée. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'apprentissage par renforcement de la distribution avec régression quantile</a> est une continuation du travail précédent, mais avec la «quantification» de la distribution. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Reprise d'expérience distribuée</a> - travaillez dans le sens d'un apprentissage par renforcement profond à grande échelle.  Comment organiser correctement l'architecture de l'expérience pour maximiser l'utilisation des ressources disponibles et augmenter la vitesse de formation des agents. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Gradients de politique déterministes distribués distribués</a> - une combinaison des trois articles précédents pour les tâches avec un espace d'action continu. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Adressage des erreurs d'approximation des fonctions dans les méthodes des acteurs critiques</a> - excellent travail pour augmenter la robustesse des agents RL.  Je recommande de le lire. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'apprentissage par renforcement hiérarchique efficace des données</a> est le développement d'un article précédent dans le domaine de l'apprentissage par renforcement hiérarchique (HRL). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Lecture complémentaire</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Soft Actor-Critic: Apprentissage par renforcement profond d'entropie maximale hors politique avec un acteur stochastique</a> - les auteurs ont proposé une méthode pour former des politiques stochastiques avec un apprentissage par renforcement hors politique.  Grâce à cet article, il est devenu possible de former des politiciens non déterministes même dans des tâches avec un espace d'action continu. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Politiques d'espace latent pour l'apprentissage de renforcement hiérarchique</a> est une continuation d'un article HRL précédent avec des politiques stochastiques à plusieurs niveaux. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">La diversité est tout ce dont vous avez besoin: apprendre des compétences sans fonction de récompense</a> - cet article contient une approche avec l'apprentissage de nombreuses politiques stochastiques aléatoires de bas niveau sans aucune récompense de l'environnement.  Par la suite, lorsque nous avons défini la fonction de récompense, la plus corrélée avec le prix peut être utilisée pour enseigner la politique de haut niveau. </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Apprentissage et contrôle du renforcement en tant qu'inférence probabiliste: didacticiel et examen</a> - un aperçu de toutes sortes de méthodes d'apprentissage par renforcement d'entropie maximale de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Sergey Levine</a> . </li></ul><br><p>  Je conseille également <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OpenAI une sélection d'articles</a> sur l'apprentissage par renforcement et sa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">version pour mendeley</a> .  Et si vous êtes intéressé par le sujet de la formation de renforcement, rejoignez le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">club</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">RL</a> et les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">papiers RL</a> . </p></div></div><br><h3 id="practice">  Pratique </h3><br><img src="https://habrastorage.org/webt/xp/g7/it/xpg7itebqdpi3cwex33uzrzkidg.jpeg"><br><br>  Connaître la théorie seule ne suffit pas - il est important de pouvoir mettre en pratique toutes ces approches et d'établir le bon système de validation pour évaluer les décisions.  Par exemple, cette année, nous avons appris que notre agent gère mal certains cas régionaux seulement 2 jours avant la fin de la compétition.  Pour cette raison, nous n'avons pas eu le temps de réparer complètement notre modèle et nous n'avons pas obtenu littéralement quelques points pour la deuxième place tant convoitée.  Si nous trouvions cela même en une semaine - le résultat pourrait être meilleur. <br><div class="spoiler">  <b class="spoiler_title">Coolstory: épisode III</b> <div class="spoiler_text"><p>  La récompense moyenne pour 10 épisodes de test a servi d'évaluation finale de la solution. </p><br><img src="https://habrastorage.org/webt/jq/bj/yc/jqbjyctkjettuu2bqd19xcahssk.png"><br><p>  Le graphique montre les résultats des tests de notre agent: 9 épisodes sur 10, notre squelette s'est très bien passé (moyenne - 9955,66), mais un épisode .... L'épisode 3 ne lui a pas été donné (récompense 9870).  C'est cette erreur qui a conduit à la chute de la vitesse finale à 9947 (-8 points). </p></div></div><br><h3 id="udacha">  Bonne chance </h3><br><p>  Et enfin - n'oubliez pas la chance banale.  Ne pensez pas que c'est un point controversé.  Au contraire, un peu de chance contribue grandement au travail constant sur soi: même si la probabilité de chance n'est que de 10%, une personne qui a tenté de participer au concours 100 fois réussira beaucoup plus que quelqu'un qui n'a essayé qu'une seule fois et a abandonné l'idée. </p><br><h1 id="tuda-i-obratno-reshenie-proshlogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2017-learning-to-runwinners">  Aller-retour: décision de l'année dernière - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">troisième place</a> </h1><br><img src="https://habrastorage.org/webt/mq/lx/i_/mqlxi_alc8pt0acnzwoi8twb8oo.jpeg"><br><br>  L'année dernière, notre équipe - Mikhail Pavlov et moi - a participé pour la première fois aux compétitions NeurIPS et la principale motivation était simplement de participer à la première compétition NeurIPS en apprentissage par renforcement.  Ensuite, je viens de terminer le cours <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pratique de RL</a> au SHAD et je voulais tester les compétences acquises.  En conséquence, nous avons pris une honorable troisième place, perdant seulement contre nnaisene (Schmidhuber) et l'équipe universitaire de Chine.  A l'époque, notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">solution</a> était «assez simple» et reposait sur le DDPG distribué avec bruit de paramètre ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">publication</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">présentation sur ml</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Trainings</a> ). <br><h1 id="reshenie-etogo-goda--trete-mestohttpswwwcrowdaiorgchallengesnips-2018-ai-for-prosthetics-challengeleaderboards">  La décision de cette année est la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">troisième place</a> </h1><br><img src="https://habrastorage.org/webt/gf/qq/to/gfqqtoneh51dn47m3f7oicyqixk.jpeg"><br><p>  Il y a eu quelques changements cette année.  Premièrement, il n'y avait aucune envie de simplement participer à ce concours, je voulais le gagner.  Deuxièmement, la composition de l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">équipe a</a> également changé: Alexey Grinchuk, Anton Pechenko et moi.  Prendre et gagner - n'a pas fonctionné, mais nous avons de nouveau pris la 3e place. <br>  Notre solution sera officiellement présentée à NeurIPS, et maintenant nous nous limiterons à un petit nombre de détails.  Sur la base de la décision de l'année dernière et du succès de l'apprentissage de renforcement hors politique de cette année (articles ci-dessus), nous avons ajouté un certain nombre de nos propres développements, dont nous parlerons à NeurIPS, et avons obtenu le Critique d'ensemble quantique distribué, avec lequel nous avons pris la troisième place. </p><br><p>  Toutes nos meilleures pratiques - un système d'apprentissage distribué, des algorithmes, etc. seront publiés et disponibles dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Catalyst.RL</a> après NeurIPS. </p><br><div class="spoiler">  <b class="spoiler_title">Coolstory: grands garçons - gros canons</b> <div class="spoiler_text"><p>  Notre équipe est allée avec confiance à la 1ère place tout au long de la compétition.  Cependant, les gros joueurs avaient d'autres plans - 2 grands joueurs sont entrés en compétition 2 semaines avant la fin de la compétition: FireWork (Baidu) et nnaisense (Schmidhuber).  Et si rien ne pouvait être fait avec Google chinois, alors avec l'équipe Schmidhuber pendant un bon moment, nous avons pu nous battre honnêtement pour la deuxième place, ne perdant qu'avec une marge minimale.  Cela me semble assez bon pour les amoureux. </p></div></div><br><h1 id="zachem-eto-vse">  Pourquoi tout cela? </h1><br><ul><li>  La communication.  Les meilleurs chercheurs viennent à la conférence avec laquelle vous pouvez discuter en direct, qui ne donnera aucune correspondance par e-mail. </li><li>  Publication  Si la solution remporte le prix, l'équipe est invitée à la conférence (ou peut-être plus d'une) pour présenter sa décision et publier l'article. </li><li>  Offre d'emploi et doctorat.  La publication et un prix dans une telle conférence augmentent considérablement vos chances d'obtenir une position dans des entreprises de premier plan telles que OpenAI, DeepMind, Google, Facebook, Microsoft. </li><li>  Valeur réelle.  NeurIPS est réalisé pour résoudre les problèmes pressants du monde académique et réel.  Vous pouvez être sûr que les résultats n'iront pas à la table, mais seront vraiment en demande et aideront à améliorer le monde. </li><li>  Conduire  Résoudre de tels concours ... juste intéressant.  Dans un concours, vous pouvez trouver beaucoup de nouvelles idées, tester différentes approches - juste pour être le meilleur.  Et soyons honnêtes, quand d'autre pouvez-vous conduire des squelettes, jouer à des jeux et tout cela avec un regard sérieux et pour le bien de la science? </li></ul><br><div class="spoiler">  <b class="spoiler_title">Coolstory: visa et RL</b> <div class="spoiler_text"><p>  Je déconseille fortement d'essayer d'expliquer à l'Américain de vous vérifier que vous allez à la conférence, car vous entraînez des squelettes virtuels à s'exécuter dans des simulations.  Allez simplement à la conférence avec une conférence. </p></div></div><br><h1 id="itogi">  Résumé </h1><br><p>  Participer à NeurIPS est une expérience difficile à surestimer.  N'ayez pas peur des gros titres - il vous suffit de vous ressaisir et de commencer à décider. </p><br><p>  Et allez sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Catalyst.RL</a> , alors quoi. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr430712/">https://habr.com/ru/post/fr430712/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr430702/index.html">Formation très étrange</a></li>
<li><a href="../fr430704/index.html">Comment les technologies de l'intelligence artificielle aident les ventes d'avias à se développer: sept exemples</a></li>
<li><a href="../fr430706/index.html">Nouvelle théorie de l'évolution</a></li>
<li><a href="../fr430708/index.html">Tic Tac Toe «Sans Frontières»</a></li>
<li><a href="../fr430710/index.html">Que faire si le Black Friday est demain et que vos serveurs ne sont pas prêts</a></li>
<li><a href="../fr430714/index.html">VMware achète Heptio - qu'est-ce que cela signifie pour Kubernetes</a></li>
<li><a href="../fr430718/index.html">Pour quels objets vaut-il la peine d'utiliser la vidéosurveillance dans le cloud?</a></li>
<li><a href="../fr430720/index.html">Intel RealSense D435i: petite mise à jour et courte digression historique</a></li>
<li><a href="../fr430722/index.html">Performances PHP: planification, profilage, optimisation</a></li>
<li><a href="../fr430724/index.html">DEFCON 21. La conférence DNS peut être dangereuse pour votre santé. Partie 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>