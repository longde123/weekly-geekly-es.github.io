<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòï üë©‚Äçüë©‚Äçüëß‚Äçüëß üè∑Ô∏è Neurobugurt. Comment nous avons appris au r√©seau neuronal √† inventer des m√®mes un an plus t√¥t que Stanford üòó üåΩ üíáüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ces nouvelles (+ recherches ) sur l'invention du g√©n√©rateur de m√®mes par des scientifiques de l'Universit√© de Stanford m'ont incit√© √† √©crire un articl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neurobugurt. Comment nous avons appris au r√©seau neuronal √† inventer des m√®mes un an plus t√¥t que Stanford</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416379/"> Ces <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nouvelles</a> (+ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">recherches</a> ) sur l'invention du g√©n√©rateur de m√®mes par des scientifiques de l'Universit√© de Stanford m'ont incit√© √† √©crire un article.  Dans mon article, je vais essayer de montrer que vous n'avez pas besoin d'√™tre un scientifique de Stanford pour faire des choses int√©ressantes avec les r√©seaux de neurones.  Dans l'article, je d√©cris comment en 2017 nous avons form√© un r√©seau de neurones sur un corps d'environ 30000 textes et l'avons forc√© √† g√©n√©rer de nouveaux m√®mes Internet et des m√®mes (signes de communication) au sens sociologique du terme.  Nous d√©crivons l'algorithme d'apprentissage automatique que nous avons utilis√©, les difficult√©s techniques et administratives que nous avons rencontr√©es. <br><a name="habracut"></a><br>  Un peu d'histoire sur la fa√ßon dont nous sommes arriv√©s √† l'id√©e d'un neuro-√©crivain et en quoi elle consistait exactement.  En 2017, nous avons r√©alis√© un projet pour un site Web public de Vkontakte, dont le nom et les captures d'√©cran √©taient interdits aux mod√©rateurs Habrahabr de publier, consid√©rant sa mention comme ¬´soi¬ª PR.  Le public existe depuis 2013 et unit les messages avec l'id√©e g√©n√©rale de d√©composer l'humour √† travers une ligne et de s√©parer les lignes avec le symbole "@": <br><br> <code> <br> @ <br>   <br> @ <br> </code> <br> <br>  Le nombre de lignes peut varier, l'intrigue peut √™tre quelconque.  Le plus souvent, il s'agit d'humour ou de notes sociales nettes sur les faits rampants de la r√©alit√©.  En g√©n√©ral, cette conception est appel√©e ¬´buhurt¬ª. <br><br><img src="https://habrastorage.org/webt/g9/bb/4n/g9bb4nvtkhh2hkx7wybuutrzr3u.png" alt="image"><br><br>  <i>L'un des buhurts typiques</i> <br><br>  Au fil des ans, le public est devenu une tradition interne (personnages, intrigues, lieux) et le nombre de publications a d√©pass√© les 30 000. Au moment de leur analyse des besoins du projet, le nombre de lignes sources du texte d√©passait le demi-million. <br><br><h3>  Partie 0. L'√©mergence d'id√©es et d'√©quipes </h3><br>  Dans le sillage de la popularit√© de masse des r√©seaux de neurones, l'id√©e de former ANN sur nos textes √©tait dans l'air depuis environ six mois, mais a finalement √©t√© formul√©e √† l'aide de E7su en d√©cembre 2016. En m√™me temps, le nom a √©t√© invent√© (¬´Neurobugurt¬ª).  A cette √©poque, l'√©quipe int√©ress√©e par le projet n'√©tait compos√©e que de trois personnes.  Nous √©tions tous des √©tudiants sans exp√©rience pratique des algorithmes et des r√©seaux de neurones.  Pire encore, nous n'avions m√™me pas un seul GPU adapt√© √† la formation.  Tout ce que nous avions, c'√©tait l'enthousiasme et la confiance que cette histoire pouvait √™tre int√©ressante. <br><br><h3>  Partie 1. La formulation de l'hypoth√®se et des t√¢ches </h3><br>  Notre hypoth√®se s'est av√©r√©e √™tre l'hypoth√®se que si vous m√©langez tous les textes publi√©s sur trois ans et demi et formez le r√©seau neuronal sur ce b√¢timent, vous pouvez obtenir: <br><br>  a) plus cr√©atif que les gens <br>  b) dr√¥le <br><br>  M√™me si les mots ou les lettres dans le buhurt s'av√®rent √™tre confus par la machine et dispos√©s au hasard - nous pensions que cela pourrait fonctionner comme un service de fans et plairait encore aux lecteurs. <br><br>  La t√¢che a √©t√© grandement simplifi√©e par le fait que le format des buhurts est essentiellement textuel.  Nous n'avons donc pas eu √† plonger dans la vision industrielle et d'autres choses complexes.  Une autre bonne nouvelle est que l'ensemble des textes est tr√®s similaire.  Cela a permis de ne pas utiliser l'apprentissage renforc√© - au moins dans les premiers stades.  Dans le m√™me temps, nous avons clairement compris que la cr√©ation d'un r√©dacteur de r√©seau neuronal avec une sortie lisible plus d'une fois n'est pas si facile.  Le risque de donner naissance √† un monstre qui jettera des lettres au hasard √©tait tr√®s grand. <br><br><h3>  Partie 2. Pr√©paration du corps des textes </h3><br>  On pense que la phase de pr√©paration peut prendre beaucoup de temps, car elle est associ√©e √† la collecte et au nettoyage des donn√©es.  Dans notre cas, cela s'est av√©r√© √™tre assez court: un petit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">analyseur a</a> √©t√© √©crit qui a pomp√© environ 30 000 messages du mur de la communaut√© et les a plac√©s dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fichier txt</a> . <br><br>  Nous n'avons pas effac√© les donn√©es avant la premi√®re formation.  √Ä l'avenir, cela a jou√© une blague cruelle avec nous, car en raison de l'erreur qui s'est gliss√©e √† ce stade, nous n'avons pas pu mettre les r√©sultats sous une forme lisible pendant longtemps.  Mais plus √† ce sujet plus tard. <br><br><img src="https://habrastorage.org/webt/jt/9v/uq/jt9vuqk2tdpoi7ycus5udn-lnpo.png" alt="image"><br><br>  <i>Fichier √©cran avec des hamburgers</i> <br><br><h3>  Partie 3. Annonce, raffinement de l'hypoth√®se, choix de l'algorithme </h3><br>  Nous avons utilis√© une ressource accessible - un grand nombre d'abonn√©s publics.  L'hypoth√®se √©tait que parmi 300 000 lecteurs, il y a plusieurs passionn√©s qui poss√®dent des r√©seaux de neurones √† un niveau suffisant pour combler les lacunes dans les connaissances de notre √©quipe.  Nous sommes partis de l'id√©e d'annoncer largement le concours et d'attirer les passionn√©s de machine learning √† la discussion du probl√®me formul√©.  Apr√®s avoir √©crit les textes, nous avons fait part de notre id√©e aux gens et esp√©r√© une r√©ponse. <br><br><img src="https://habrastorage.org/webt/73/0b/jo/730bjoajceycgzoynywra6xcupy.png" alt="image"><br><br>  <i>Annonce d'une discussion th√©matique</i> <br><br>  La r√©action des gens a d√©pass√© nos attentes les plus folles.  La discussion sur le fait que nous allons former un r√©seau de neurones a propag√© l'holivar par pr√®s de 1000 commentaires.  La plupart des lecteurs ont simplement disparu et ont essay√© d'imaginer √† quoi ressemblerait le r√©sultat.  Environ 6 000 personnes ont regard√© la discussion th√©matique et plus de 50 amateurs int√©ress√©s ont laiss√© des commentaires √† qui nous avons donn√© un ensemble de test de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">814 lignes de buhurt</a> pour effectuer les premiers tests et la formation.  Chaque personne int√©ress√©e pourrait prendre un ensemble de donn√©es et apprendre l'algorithme qui lui est le plus int√©ressant, puis discuter avec nous et d'autres passionn√©s.  Nous avons annonc√© √† l'avance que nous continuerons √† travailler avec les participants dont les r√©sultats seront les plus lisibles. <br><br>  Le travail a commenc√©: quelqu'un a assembl√© silencieusement un g√©n√©rateur sur des cha√Ænes de Markov, quelqu'un a essay√© diverses impl√©mentations avec un github, et la plupart sont juste devenus fous dans la discussion et nous ont convaincus avec de la mousse √† la bouche que rien n'en sortirait.  Cela a commenc√© la partie technique du projet. <br><br><img src="https://habrastorage.org/webt/ie/cu/tz/iecutzyxvv__0a2qzo5ailsjk9s.png" alt="image"><br><br>  Quelques suggestions de passionn√©s <br><br>  Les gens ont offert des dizaines d'options pour la mise en ≈ìuvre: <br><br><ul><li>  Cha√Ænes de Markov. </li><li>  Trouvez une impl√©mentation pr√™te √† l'emploi de quelque chose de similaire √† GitHub et entra√Ænez-la. </li><li>  Un g√©n√©rateur de phrases al√©atoires √©crit en Pascal. </li><li>  Obtenez un n√®gre litt√©raire qui √©crira des b√™tises al√©atoires, et nous passerons cela comme une sortie de r√©seau neuronal. </li></ul><br><img src="https://habrastorage.org/webt/ki/tr/ut/kitrutzq2yxrtm_weg1ngm4beow.png" alt="image"><br><br>  <i>√âvaluation de la complexit√© du projet par l'un des abonn√©s</i> <br><br>  La plupart des commentateurs ont convenu que notre projet est vou√© √† l'√©chec et que nous n'atteindrons m√™me pas le stade du prototype.  Comme nous l'avons compris plus tard, les gens sont toujours enclins √† percevoir les r√©seaux de neurones comme une sorte de magie noire qui se produit dans la "t√™te de Zuckerberg" et les divisions secr√®tes de Google. <br><br><h3>  Partie 4. S√©lection d'algorithmes, formation et expansion de l'√©quipe </h3><br>  Apr√®s un certain temps, la campagne que nous avons lanc√©e pour des id√©es de crowdsourcing pour l'algorithme a commenc√© √† porter ses premiers fruits.  Nous avons obtenu environ 30 prototypes fonctionnels, dont la plupart ont donn√© des b√™tises compl√®tement illisibles. <br><br>  A ce stade, nous avons d'abord rencontr√© une d√©motivation de l'√©quipe.  Tous les r√©sultats √©taient tr√®s faiblement similaires aux buhurts et repr√©sentaient le plus souvent l'abracadabra des lettres et des symboles.  Le travail de dizaines de passionn√©s est tomb√© en poussi√®re, ce qui les a d√©motiv√©s, nous et nous. <br><br>  L'algorithme bas√© sur pyTorch s'est montr√© meilleur que les autres.  Il a √©t√© d√©cid√© de prendre cette impl√©mentation et l'algorithme LSTM comme base.  Nous avons reconnu l'abonn√© qui l'a propos√© comme gagnant et avons commenc√© √† travailler avec lui sur l'am√©lioration de l'algorithme.  Notre √©quipe r√©partie est pass√©e √† quatre personnes.  Le fait amusant ici est que le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">gagnant du concours</a> , il s'est av√©r√©, n'avait que 16 ans.  Cette victoire a √©t√© son premier vrai prix dans le domaine de la Data Science. <br><br>  Pour la premi√®re formation, un cluster de 8 cartes graphiques GXT1080 a √©t√© lou√©. <br><br><img src="https://habrastorage.org/webt/mb/_l/ql/mb_lqlodsgm8fyztu36ly0soqo4.jpeg" alt="image"><br><br>  <i>Console de gestion de cluster de cartes</i> <br><br>  Le r√©f√©rentiel d'origine et tous les manuels de projet Torch-rnn sont ici: <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github.com/jcjohnson/torch-rnn</a> .  Plus tard, sur la base de cela, nous avons publi√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">notre r√©f√©rentiel</a> , dans lequel se trouvent nos sources, ReadMe pour l'installation, ainsi que les neurobugurts finis eux-m√™mes. <br><br>  Les premi√®res fois, nous nous sommes entra√Æn√©s √† l'aide d'une configuration pr√©configur√©e sur un cluster GPU payant.  L'installation ne s'est pas av√©r√©e si difficile - seules les instructions du d√©veloppeur Torch et l'aide de l'administration d'h√©bergement, qui est incluse dans le paiement, suffisent. <br><br>  Cependant, tr√®s rapidement, nous avons rencontr√© des difficult√©s: chaque formation a co√ªt√© le temps de location du GPU - ce qui signifie qu'il n'y avait tout simplement pas d'argent dans le projet.  Pour cette raison, en janvier-f√©vrier 2017, nous avons organis√© une formation dans les installations achet√©es et nous avons essay√© de lancer la g√©n√©ration sur nos machines locales. <br><br>  Tout texte convient √† la formation de mod√®le.  Avant la formation, vous devez le pr√©traiter, pour lequel Torch a un algorithme sp√©cial preprocess.py qui convertit votre my_data.txt en deux fichiers: HDF5 et JSON: <br><br>  Le script de pr√©traitement s'ex√©cute comme suit: <br><br><pre> <code class="python hljs">python scripts/preprocess.py \ --input_txt my_data.txt \ --output_h5 my_data.h5 \ --output_json my_data.json</code> </pre> <br><img src="https://habrastorage.org/webt/8u/s8/kj/8us8kjppqy-1wqhpjp87hdiwqyi.png" alt="image"><br><br>  <i>Apr√®s le pr√©traitement, deux fichiers apparaissent sur lesquels le r√©seau de neurones sera form√© √† l'avenir</i> <br><br>  Les diff√©rents indicateurs pouvant √™tre modifi√©s au stade du pr√©traitement sont d√©crits <a href="">ici</a> .  Il est √©galement possible d'ex√©cuter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Torch √† partir de Docker</a> , mais l'auteur de l'article ne l'a pas v√©rifi√©. <br><br><h4>  Formation au r√©seau de neurones </h4><br>  Apr√®s le pr√©traitement, vous pouvez proc√©der √† la formation du mod√®le.  Dans le dossier avec HDF5 et JSON, vous devez ex√©cuter l'utilitaire e, qui est apparu avec vous si vous avez correctement install√© Torch: <br><br><pre> <code class="python hljs">th train.lua -input_h5 my_data.h5 -input_json my_data.json</code> </pre> <br>  La formation prend √©norm√©ment de temps et g√©n√®re des fichiers de la forme cv / checkpoint_1000.t7, qui sont les ¬´poids¬ª de notre r√©seau de neurones.  Ces fichiers p√®sent une quantit√© impressionnante de m√©gaoctets et contiennent la force des liens entre des lettres sp√©cifiques dans votre jeu de donn√©es d'origine. <br><br><img src="https://habrastorage.org/webt/qt/ts/it/qttsit4vulbqojnmxfcfdh1pdzm.png" alt="image"><br><br>  <i>Un r√©seau de neurones est souvent compar√© au cerveau humain, mais il me semble une analogie beaucoup plus claire avec une fonction math√©matique qui prend des param√®tres en entr√©e (votre jeu de donn√©es) et donne le r√©sultat (nouvelles donn√©es) en sortie.</i> <br><br>  Dans notre cas, chaque formation sur un cluster de 8 GTX 1080 dans un ensemble de donn√©es de 500 000 lignes a pris environ une heure ou deux, et une formation similaire sur une sorte de CPU i3-2120 a pris environ 80 √† 100 heures.  Dans le cas d'une formation plus longue, le r√©seau neuronal a commenc√© √† se recycler de mani√®re rigide - les symboles se r√©p√©taient trop souvent, tombant dans de longs cycles de pr√©positions, de conjonctions et de mots introductifs. <br><br>  Il est pratique que vous puissiez choisir la fr√©quence des points de contr√¥le et pendant une formation, vous obtiendrez imm√©diatement de nombreux mod√®les: du moins form√© (checkpoint_1000) au recyclage (checkpoint_1000000).  Seul assez d'espace serait suffisant. <br><br><h4>  Nouvelle g√©n√©ration de texte </h4><br>  Apr√®s avoir re√ßu au moins un fichier pr√™t √† l'emploi avec des poids (checkpoint _ *******), vous pouvez passer √† l'√©tape suivante et la plus int√©ressante: commencer √† g√©n√©rer des textes.  Pour nous, ce fut un vrai moment de v√©rit√©, car pour la premi√®re fois nous avons obtenu un r√©sultat tangible - un bug √©crit par une machine. <br><br>  √Ä ce stade, nous avons finalement cess√© d'utiliser le cluster et toutes les g√©n√©rations ont √©t√© effectu√©es sur nos machines de faible puissance.  Cependant, lors de la tentative de d√©marrage local, nous n'avons tout simplement pas r√©ussi √† suivre les instructions et √† installer Torch.  Le premier obstacle a √©t√© l'utilisation de machines virtuelles.  Sur Ubuntu 16 virtuel, le stick ne d√©colle pas - oubliez-le.  StackOverflow venait souvent √† la rescousse, mais certaines erreurs √©taient si simples que la r√©ponse ne pouvait √™tre trouv√©e qu'avec beaucoup de difficult√©s. <br><br>  L'installation de Torch sur une machine locale a bloqu√© le projet pendant quelques semaines: nous avons rencontr√© toutes sortes d'erreurs lors de l'installation de nombreux packages requis, nous avons √©galement eu des probl√®mes avec la virtualisation (virtualenv .env) et nous ne l'avons finalement pas utilis√©.  Plusieurs fois le stand a √©t√© d√©moli au niveau de sudo rm -rf et a √©t√© simplement r√©install√©. <br><br>  En utilisant le fichier r√©sultant avec des poids, nous avons pu commencer √† g√©n√©rer des textes sur notre machine locale: <br><br><img src="https://habrastorage.org/webt/9z/ve/r_/9zver_8vvx-fknpdj_9-id70hfe.jpeg" alt="image"><br><br>  <i>Une des premi√®res conclusions</i> <br><br><h3>  Partie 5. Effacement des textes </h3><br>  Une autre difficult√© √©vidente √©tait que le sujet des articles est tr√®s diff√©rent, et notre algorithme n'implique aucune division et consid√®re les 500 000 lignes comme un seul texte.  Nous avons envisag√© diff√©rentes options pour regrouper l'ensemble de donn√©es et √©tions m√™me pr√™ts √† d√©composer manuellement le corps des textes par sujet ou √† placer des balises dans plusieurs milliers de buhurts (il y avait une ressource humaine n√©cessaire pour cela), mais nous avons constamment rencontr√© des difficult√©s techniques pour soumettre des clusters lors de l'apprentissage du LSTM.  Changer l'algorithme et refaire le concours ne semble pas √™tre l'id√©e la plus sens√©e en termes de timing du projet et de motivation des participants. <br><br>  Il semblait que nous √©tions dans une impasse - nous ne pouvions pas regrouper les buhurts, et la formation sur un seul ensemble de donn√©es √©norme a donn√© des r√©sultats douteux.  Je ne voulais pas prendre du recul et changer l'algorithme et l'impl√©mentation presque en fl√®che - le projet pourrait simplement tomber dans le coma.  L'√©quipe n'avait d√©sesp√©r√©ment pas suffisamment de connaissances pour r√©soudre la situation normalement, mais le bon vieux SME-KAL-OCHK-A est venu √† la rescousse.  La solution finale √† la <s>b√©quille</s> s'est av√©r√©e √™tre g√©niale: dans le jeu de donn√©es d'origine, s√©parez les buhurts existants les uns des autres avec des lignes vides et entra√Ænez √† nouveau le LSTM. <br><br>  Nous avons organis√© les battements dans 10 espaces verticaux apr√®s chaque buhurt, r√©p√©t√© la formation et pendant la g√©n√©ration, nous avons fix√© une limite au volume de sortie de 500 caract√®res (la longueur moyenne d'un buhurt ¬´plot¬ª dans l'ensemble de donn√©es d'origine). <br><br><img src="https://habrastorage.org/webt/da/2h/x4/da2hx4k9shkumjvxwickcnjfwnu.png" alt="image"><br><br>  <i>Comme c'√©tait.</i>  <i>Les intervalles entre les textes sont minimes.</i> <br><br><img src="https://habrastorage.org/webt/bb/y8/i5/bby8i5skh4u0trqrs94ppe2fal0.png" alt="image"><br><br>  <i>Comment est-ce devenu?</i>  <i>Des intervalles de 10 lignes permettent au LSTM de ¬´comprendre¬ª qu'un bogurt est termin√© et qu'un autre a commenc√©.</i> <br><br>  Ainsi, il a √©t√© possible de r√©aliser qu'environ 60% de tous les buhurts g√©n√©r√©s commen√ßaient √† avoir un trac√© lisible (bien que souvent tr√®s d√©lirant) sur toute la longueur du buhurt du d√©but √† la fin.  La longueur d'une parcelle √©tait en moyenne de 9 √† 13 lignes. <br><br><h3>  Partie 6. Recyclage </h3><br>  Apr√®s avoir estim√© l'√©conomie du projet, nous avons d√©cid√© de ne plus d√©penser d'argent pour la location d'un cluster, mais d'investir dans l'achat de nos propres cartes.  Le temps d'apprentissage augmenterait, mais apr√®s avoir achet√© une carte une fois, nous pourrions g√©n√©rer constamment de nouveaux buhurts.  Dans le m√™me temps, il n'√©tait souvent plus n√©cessaire de dispenser une formation. <br><br><img src="https://habrastorage.org/webt/rr/_e/92/rr_e92il8tdd5ckjdglm7fy8cuy.jpeg" alt="image"><br><br>  <i>Param√®tres de combat sur la machine locale</i> <br><br><h3>  Partie 7. √âquilibrer les r√©sultats </h3><br>  Au tournant de mars-avril 2017, nous avons re-form√© le r√©seau neuronal, en pr√©cisant les param√®tres de temp√©rature et le nombre d'√©poques d'entra√Ænement.  En cons√©quence, la qualit√© de la sortie a l√©g√®rement augment√©. <br><br><img src="https://habrastorage.org/webt/ew/q8/qu/ewq8quy68ixemttz67jkmqeqvgw.png" alt="image"><br><br>  <i>Vitesse d'apprentissage torch-rnn par rapport √† char-rnn</i> <br><br>  Nous avons test√© les deux algorithmes fournis avec Torch: rnn et LSTM.  La seconde s'est av√©r√©e meilleure. <br><br><h3>  Partie 8. Qu'avons-nous r√©alis√©? </h3><br>  Le premier neurobugurt a √©t√© publi√© le 17 janvier 2017 - imm√©diatement apr√®s la formation sur le cluster - et le premier jour, plus de 1000 commentaires ont √©t√© collect√©s. <br><br><img src="https://habrastorage.org/webt/t2/ng/i5/t2ngi5591hhqz7fh0n_snditsuo.png" alt="image"><br><br>  <i>L'un des premiers neurobugurts</i> <br><br>  Les neurobugurts ont si bien atteint le public qu'ils sont devenus une section distincte, qui tout au long de l'ann√©e est sortie sous le hashtag # neurobugurt et a amus√© les abonn√©s.  Au total, en 2017 et d√©but 2018, nous avons g√©n√©r√© plus de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">18000 neurobugurts</a> , avec une moyenne de 500 caract√®res chacun.  De plus, tout un mouvement de parodies publiques est apparu, dont les participants ont d√©peint des neurobugurs, r√©arrangeant au hasard des phrases par endroits. <br><br><img src="https://habrastorage.org/webt/ls/k2/jb/lsk2jbbloh3e7sg-1p-04k5muvq.jpeg" alt="image"><br><br><h3>  Partie 9. Au lieu d'une conclusion </h3><br>  Avec cet article, je voulais montrer que m√™me si vous n'avez pas d'exp√©rience dans les r√©seaux de neurones, ce chagrin n'est pas un probl√®me.  Vous n'avez pas besoin de travailler √† Stanford pour faire des choses simples mais int√©ressantes avec les r√©seaux de neurones.  Tous les participants √† notre projet √©taient des √©tudiants ordinaires avec leurs t√¢ches actuelles, leurs dipl√¥mes, leurs travaux, mais la cause commune nous a permis de mener le projet √† son terme.  Gr√¢ce √† l'id√©e r√©fl√©chie, √† la planification et √† l'√©nergie des participants, nous avons pu obtenir les premiers r√©sultats sens√©s en moins d'un mois apr√®s la formulation finale de l'id√©e (la plupart des travaux techniques et organisationnels sont tomb√©s sur les vacances d'hiver 2017). <br><br><img src="https://habrastorage.org/webt/pu/q2/yj/puq2yjjiq0ynj4ioncm_wn_obrm.png" alt="image"><br><br>  <i>Plus de 18 000 buhurts g√©n√©r√©s par machine</i> <br><br>  J'esp√®re que cet article aide quelqu'un √† planifier son propre projet ambitieux avec des r√©seaux de neurones.  Je demande de ne pas juger strictement car c'est mon premier article sur Habr√©.  Si vous, comme moi, un passionn√© de ML, soyons <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">amis</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr416379/">https://habr.com/ru/post/fr416379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr416367/index.html">Nous commen√ßons ReactOS avec BTRFS de la section</a></li>
<li><a href="../fr416369/index.html">√Ä peu pr√®s compliqu√©. Partie 2, cr√©ation d'une ¬´maison intelligente¬ª sans fil. Bas√© sur la technologie Linux, les logiciels Z-Wave et MajorDoMo</a></li>
<li><a href="../fr416371/index.html">Lampe de camping analogique</a></li>
<li><a href="../fr416375/index.html">Bases de JavaScript pour les d√©butants</a></li>
<li><a href="../fr416377/index.html">Nous devenons des assistants en programmation. Partie 1</a></li>
<li><a href="../fr416381/index.html">Rapport du Club de Rome 2018, chapitre 3.13: philanthropie, investissement, crowdsourcing et blockchain</a></li>
<li><a href="../fr416385/index.html">Si la corr√©lation ressort √† 100%, alors quelque part une erreur s'est gliss√©e quelque part: l'exp√©rience de stage chez Rambler Group</a></li>
<li><a href="../fr416387/index.html">Shrimp: redimensionnez et partagez des images HTTP dans C ++ moderne avec ImageMagic ++, SObjectizer et RESTinio</a></li>
<li><a href="../fr416391/index.html">Optimisation du placement des machines virtuelles sur les serveurs</a></li>
<li><a href="../fr416393/index.html">Conf√©rence IIDF: les entreprises ne sont pas contre les startups</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>