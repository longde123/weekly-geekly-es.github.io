<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§ò üôÖüèΩ üë®üèø‚Äçüåæ Datenbankentwicklung in Dropbox. Der Pfad von einer globalen MySQL-Datenbank zu Tausenden von Servern üë©üèæ‚Äçüîß üë©‚Äçüëß ü§Ωüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Als Dropbox gerade gestartet wurde, kommentierte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git imp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Datenbankentwicklung in Dropbox. Der Pfad von einer globalen MySQL-Datenbank zu Tausenden von Servern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Als Dropbox gerade gestartet wurde, kommentierte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git implementiert werden k√∂nnte.  Dies kann in keiner Weise gesagt werden. Dies ist ein gro√üer Cloud-Dateispeicher mit Milliarden neuer Dateien pro Tag, die nicht nur irgendwie in der Datenbank gespeichert werden, sondern auch so, dass jede Datenbank innerhalb der letzten sechs Tage zu einem beliebigen Zeitpunkt wiederhergestellt werden kann. <br><br>  Unter dem Schnitt das Transkript des Berichts von <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">m0sth8</a> ) auf Highload ++ 2017 dar√ºber, wie sich die Datenbanken in Dropbox entwickelt haben und wie sie jetzt angeordnet sind. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>√úber den Redner:</strong> Ehre sei Bakhmutov - Site Reliability Engineer im Dropbox-Team, liebt Go sehr und erscheint manchmal im Podcast von golangshow.com. <br><br><h2>  Inhalt <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurz √ºber die Dropbox-Architektur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Geschichte der Entwicklung</a> von Datenbanken und wie die aktuelle Dropbox-Architektur funktioniert </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die einfachsten Operationen f√ºr Datenbanken</a> (Feylover, Backups, Klone, Promotions) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Automatisierung</a> - verwaltet alle Datenbanken und f√ºhrt Vorg√§nge aus </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√úberwachung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Testen, Staging und DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Dropbox-Architektur im Klartext </h2><br>  Dropbox erschien im Jahr 2008.  Dies ist im Wesentlichen ein Cloud-Dateispeicher.  Als Dropbox gerade gestartet wurde, sagte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git implementiert werden k√∂nnte.  Trotzdem entwickelt sich Dropbox weiter und ist jetzt ein ziemlich gro√üer Dienst mit mehr als 1,5 Milliarden Benutzern, 200.000 Unternehmen und einer gro√üen Anzahl (mehrere Milliarden!) T√§glich neuer Dateien. <br><br>  <strong>Wie sieht Dropbox aus?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Wir haben mehrere Clients (Webschnittstelle, API f√ºr Anwendungen, die Dropbox verwenden, Desktopanwendungen).  Alle diese Clients verwenden die API und kommunizieren mit zwei gro√üen Diensten, die logisch unterteilt werden k√∂nnen in: <br><br><ol><li>  <strong>Metaserver</strong> </li><li>  <strong>Blockserver</strong> </li></ol><br>  Metaserver speichert Metainformationen √ºber die Datei: Gr√∂√üe, Kommentare dazu, Links zu dieser Datei in Dropbox usw.  Blockserver speichert nur Informationen zu Dateien: Ordner, Pfade usw. <br><br>  <strong>Wie funktioniert es</strong> <br><br>  Zum Beispiel haben Sie eine video.avi-Datei mit einer Art Video. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br><ul><li>  Der Client teilt diese Datei in mehrere Bl√∂cke auf (in diesem Fall jeweils 4 MB), berechnet die Pr√ºfsumme und sendet eine Anfrage an Metaserver: "Ich habe eine * .avi-Datei, ich m√∂chte sie hochladen, die Hash-Betr√§ge sind so und so." </li><li>  Metaserver gibt die Antwort zur√ºck: "Ich habe diese Bl√∂cke nicht, lasst uns herunterladen!"  Oder er kann antworten, dass er alle oder einige der Bl√∂cke hat und nur die verbleibenden geladen werden m√ºssen. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br><ul><li>  Danach geht der Client zu Blockserver, sendet die Hash-Menge und den Datenblock selbst, der auf dem Blockserver gespeichert ist. </li><li>  Blockserver best√§tigt den Vorgang. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  Dies ist nat√ºrlich ein sehr vereinfachtes Schema, das Protokoll ist viel komplizierter: Es gibt eine Synchronisation zwischen Clients innerhalb desselben Netzwerks, es gibt Kerneltreiber, die F√§higkeit, Kollisionen aufzul√∂sen usw.  Dies ist ein ziemlich komplexes Protokoll, aber es funktioniert schematisch so. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Wenn ein Client etwas auf Metaserver speichert, gehen alle Informationen an MySQL.  Blockserver speichert auch Informationen √ºber Dateien, wie sie strukturiert sind, aus welchen Bl√∂cken sie bestehen, in MySQL.  Blockserver speichert die Bl√∂cke auch selbst im Blockspeicher, der wiederum Informationen dar√ºber speichert, wo welcher Block liegt, auf welchem ‚Äã‚ÄãServer und wie er verarbeitet wird, auch in MYSQL. <br><br><blockquote>  Um Exabyte an Benutzerdateien zu speichern, speichern wir gleichzeitig zus√§tzliche Informationen in einer Datenbank mit mehreren Dutzend Petabyte, die auf 6.000 Server verteilt sind. </blockquote><br><a name="history_development"></a><h2>  Datenbankentwicklungsverlauf </h2><br>  Wie haben sich die Datenbanken in Dropbox entwickelt? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  Im Jahr 2008 begann alles mit einem Metaserver und einer globalen Datenbank.  Alle Informationen, die Dropbox irgendwo speichern musste, speicherte er in der einzigen globalen MySQL.  Dies dauerte nicht lange, da die Anzahl der Benutzer zunahm und einzelne Datenbanken und Tablets in den Datenbanken schneller anstiegen als andere. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Daher wurden 2011 mehrere Tabellen an separate Server gesendet: <br><br><ul><li>  <strong>Benutzer</strong> mit Informationen zu Benutzern, z. B. Anmeldungen und oAuth-Token; </li><li>  <strong>Host</strong> mit Dateiinformationen von Blockserver; </li><li>  <strong>Verschiedenes</strong> , das nicht an der Verarbeitung von Anforderungen aus der Produktion beteiligt war, sondern f√ºr Dienstprogrammfunktionen wie Stapeljobs verwendet wurde. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Aber nach 2012 begann Dropbox sehr stark zu wachsen, seitdem sind <strong>wir</strong> um <strong>etwa 100 Millionen Benutzer pro Jahr</strong> gewachsen. <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Es war notwendig, ein so gro√ües Wachstum zu ber√ºcksichtigen, und deshalb hatten wir Ende 2011 Scherben - eine Basis bestehend aus 1.600 Scherben.  Anfangs nur 8 Server mit jeweils 200 Shards.  Jetzt sind es 400 Master-Server mit jeweils 4 Shards. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  Im Jahr 2012 haben wir festgestellt, dass das Erstellen und Aktualisieren von Tabellen in der Datenbank f√ºr jede hinzugef√ºgte Gesch√§ftslogik sehr schwierig, trostlos und problematisch ist.  Aus diesem <strong>Grund haben</strong> wir 2012 unseren eigenen <strong>Grafikspeicher</strong> erfunden, den wir <strong>Edgestore</strong> genannt haben. <strong>Seitdem</strong> werden alle von der Anwendung generierten Gesch√§ftslogiken und Metainformationen in Edgestore gespeichert. <br><br>  Edgestore abstrahiert MySQL im Wesentlichen von Clients.  Clients haben bestimmte Entit√§ten, die durch Links von der gRPC-API zu Edgestore Core verbunden sind, wodurch diese Daten in MySQL konvertiert und irgendwie dort gespeichert werden (im Grunde gibt es all dies aus dem Cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  <strong>2015 verlie√üen wir Amazon S3</strong> und entwickelten unseren eigenen Cloud-Speicher namens Magic Pocket.  Es enth√§lt Informationen dar√ºber, wo sich eine Blockdatei befindet, auf welchem ‚Äã‚ÄãServer, √ºber die Bewegungen dieser Bl√∂cke zwischen Servern, die in MySQL gespeichert sind. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  MySQL wird jedoch auf sehr knifflige Weise verwendet - im Wesentlichen als gro√üe verteilte Hash-Tabelle.  Dies ist eine ganz andere Belastung, haupts√§chlich beim Lesen von zuf√§lligen Datens√§tzen.  90% der Auslastung ist I / O. <br><br><h2>  Datenbankarchitektur </h2><br>  Zun√§chst haben wir sofort einige Prinzipien identifiziert, nach denen wir die Architektur unserer Datenbank erstellen: <br><br><ol><li>  <strong>Zuverl√§ssigkeit und Haltbarkeit</strong> .  Dies ist das wichtigste Prinzip und was Kunden von uns erwarten - Daten sollten nicht verloren gehen. </li><li>  <strong>Die Optimalit√§t der L√∂sung</strong> ist ein ebenso wichtiges Prinzip.  Beispielsweise sollten Sicherungen schnell durchgef√ºhrt und auch schnell wiederhergestellt werden. </li><li>  <strong>Einfachheit der L√∂sung</strong> - sowohl architektonisch als auch in Bezug auf Service und Weiterentwicklungsunterst√ºtzung. </li><li>  <strong>Betriebskosten</strong> .  Wenn etwas die L√∂sung optimiert, aber sehr teuer ist, passt dies nicht zu uns.  Zum Beispiel ist ein Slave, der einen Tag hinter dem Master liegt, f√ºr Backups sehr praktisch, aber dann m√ºssen Sie 1.000 weitere zu 6.000 Servern hinzuf√ºgen - die Betriebskosten eines solchen Slaves sind sehr hoch. </li></ol><br>  Alle Prinzipien m√ºssen <strong>√ºberpr√ºfbar und messbar sein</strong> , dh sie m√ºssen Metriken haben.  Wenn wir √ºber die Betriebskosten sprechen, m√ºssen wir berechnen, wie viele Server wir beispielsweise in Datenbanken haben, wie viele Server in Backups gehen und wie viel es letztendlich f√ºr Dropbox kostet.  Wenn wir eine neue L√∂sung ausw√§hlen, z√§hlen wir alle Metriken und konzentrieren uns auf sie.  Bei der Auswahl einer L√∂sung orientieren wir uns voll und ganz an diesen Grunds√§tzen. <br><br><h2>  Basistopologie </h2><br>  Die Datenbank ist wie folgt aufgebaut: <br><br><ul><li>  Im Hauptdatenzentrum haben wir einen Master, in dem alle Datens√§tze vorkommen. </li><li>  Der Master-Server verf√ºgt √ºber zwei Slave-Server, auf denen eine semisynchrone Replikation erfolgt.  Server sterben oft ab (ungef√§hr 10 pro Woche), daher ben√∂tigen wir zwei Slave-Server. </li><li>  Slave-Server befinden sich in separaten Clustern.  Cluster sind vollst√§ndig getrennte R√§ume im Rechenzentrum, die nicht miteinander verbunden sind.  Wenn ein Raum ausbrennt, bleibt der zweite vollst√§ndig funktionsf√§hig. </li><li>  Auch in einem anderen Rechenzentrum haben wir den sogenannten Pseudo-Master (Intermediate Master), der eigentlich nur ein Slave ist, der einen anderen Slave hat. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Eine solche Topologie wurde gew√§hlt, weil, wenn das erste Rechenzentrum pl√∂tzlich in uns stirbt, wir im zweiten Rechenzentrum eine <strong>fast vollst√§ndige Topologie haben</strong> .  Wir √§ndern einfach alle Adressen in Discovery, und Kunden k√∂nnen arbeiten. <br><br><h3>  Spezialisierte Topologien </h3><br>  Wir haben auch spezialisierte Topologien. <br><br>  Die <strong>Magic Pocket-</strong> Topologie besteht aus einem Master-Server und zwei Slave-Servern.  Dies geschieht, weil Magic Pocket selbst Daten zwischen Zonen dupliziert.  Wenn ein Cluster verloren geht, k√∂nnen alle Daten aus anderen Zonen durch L√∂schcode wiederhergestellt werden. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  Die <strong>Aktiv-Aktiv-</strong> Topologie ist die von Edgestore verwendete benutzerdefinierte Topologie.  Es hat einen Master und zwei Slaves in jedem der beiden Rechenzentren und sie sind Slaves f√ºr einander.  Dies ist ein sehr <strong>gef√§hrliches Schema</strong> , aber Edgestore auf seiner Ebene wei√ü genau, welche Daten auf welchem ‚Äã‚ÄãMaster √ºber welchen Bereich geschrieben werden k√∂nnen.  Daher wird diese Topologie nicht unterbrochen. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Instanz </h3><br>  Wir haben vor 4-5 Jahren ziemlich einfache Server mit einer Konfiguration von installiert: <br><br><ul><li>  <strong>2x Xeon 10 Kerne;</strong> </li><li>  <strong>5 TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 GB Speicher.</strong> </li></ul><br>  * Raid 0 - weil es einfacher und viel schneller ist, einen gesamten Server zu ersetzen als Laufwerke. <br><br><h4>  Einzelne Instanz </h4><br>  Auf diesem Server haben wir eine gro√üe MySQL-Instanz, auf der sich mehrere Shards befinden.  Diese MySQL-Instanz ordnet sich sofort fast den gesamten Speicher zu.  Auf dem Server werden auch andere Prozesse ausgef√ºhrt: Proxy, Statistiksammlung, Protokolle usw. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Diese L√∂sung ist insofern gut: <br><br>  + Es ist <strong>einfach zu verwalten</strong> .  Wenn Sie die MySQL-Instanz ersetzen m√ºssen, ersetzen Sie einfach den Server. <br><br>  + <strong>Mach einfach Faylovers</strong> . <br><br>  Andererseits: <br><br>  - Es ist problematisch, dass Operationen auf der gesamten Instanz von MySQL und sofort auf allen Shards ausgef√ºhrt werden.  Wenn Sie beispielsweise eine Sicherungskopie erstellen m√ºssen, sichern wir alle Shards gleichzeitig.  Wenn Sie einen Faylover machen m√ºssen, machen wir einen Faylover aus allen vier Scherben gleichzeitig.  Dementsprechend leidet die Zug√§nglichkeit viermal mehr. <br><br>  - Probleme beim Replizieren eines Shards betreffen andere Shards.  Die MySQL-Replikation ist nicht parallel und alle Shards arbeiten in einem einzelnen Thread.  Wenn einem Splitter etwas passiert, werden auch die anderen Opfer. <br><br>  Jetzt wechseln wir zu einer anderen Topologie. <br><br><h4>  Multi Instanz </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  In der neuen Version werden mehrere MySQL-Instanzen gleichzeitig auf dem Server mit jeweils einem Shard gestartet.  Was ist besser <br><br>  + Wir k√∂nnen <strong>Operationen nur an einem bestimmten Splitter ausf√ºhren</strong> .  Das hei√üt, wenn Sie einen Faylover ben√∂tigen, wechseln Sie nur einen Shard. Wenn Sie ein Backup ben√∂tigen, sichern wir nur einen Shard.  Dies bedeutet, dass die Vorg√§nge erheblich beschleunigt werden - viermal f√ºr einen Server mit vier Shards. <br><br>  + <strong>Scherben beeinflussen sich kaum gegenseitig</strong> . <br><br>  + <strong>Verbesserung der Replikation.</strong>  Wir k√∂nnen verschiedene Kategorien und Klassen von Datenbanken mischen.  Edgestore nimmt viel Platz ein, zum Beispiel alle 4 TB, und Magic Pocket nimmt nur 1 TB ein, ist aber zu 90% ausgelastet.  Das hei√üt, wir k√∂nnen verschiedene Kategorien kombinieren, die E / A- und Maschinenressourcen auf unterschiedliche Weise verwenden, und 4 Replikationsstr√∂me starten. <br><br>  Nat√ºrlich hat diese L√∂sung ihre Nachteile: <br><br>  - Das gr√∂√üte Minus ist, dass es <strong>viel schwieriger ist, all dies zu verwalten</strong> .  Wir brauchen einen cleveren Planer, der versteht, wo er diese Instanz aufnehmen kann, wo es eine optimale Last gibt. <br><br>  - <strong>H√§rter als die Failover</strong> . <br><br>  Deshalb kommen wir erst jetzt zu dieser Entscheidung. <br><br><h3>  Entdeckung </h3><br>  Clients m√ºssen irgendwie wissen, wie sie sich mit der gew√ºnschten Datenbank verbinden k√∂nnen, also haben wir Discovery, das: <br><br><ol><li>  Benachrichtigen Sie den Client sehr schnell √ºber Topologie√§nderungen.  Wenn wir Master und Slave wechseln, sollten Kunden fast sofort davon erfahren. <br></li><li>  Die Topologie sollte nicht von der MySQL-Replikationstopologie abh√§ngen, da wir bei einigen Operationen die MySQL-Topologie √§ndern.  Wenn wir beispielsweise im vorbereitenden Schritt auf dem Zielmaster, in dem wir einen Teil der Shards √ºbertragen, aufteilen, werden einige der Slave-Server auf diesen Zielmaster neu konfiguriert.  Kunden m√ºssen dies nicht wissen. <br></li><li>  Es ist wichtig, dass die Operationen atomar sind und der Zustand √ºberpr√ºft wird.  Es ist unm√∂glich, dass zwei verschiedene Server derselben Datenbank gleichzeitig Master werden. <br></li></ol><br><h4>  Wie sich Discovery entwickelt hat </h4><br>  Anfangs war alles einfach: die Datenbankadresse im Quellcode in der Konfiguration.  Wenn wir die Adresse aktualisieren mussten, wurde alles nur sehr schnell bereitgestellt. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Leider funktioniert dies nicht, wenn viele Server vorhanden sind. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Oben ist die allererste Entdeckung, die wir haben.  Es gab Datenbankskripte, die das Typenschild in ConfigDB √§nderten - es war ein separates MySQL-Typenschild, und Clients haben diese Datenbank bereits abgeh√∂rt und regelm√§√üig Daten von dort abgerufen. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  Die Tabelle ist sehr einfach, es gibt eine Datenbankkategorie, einen Shard-Schl√ºssel, einen Datenbankklassen-Master / Slave, einen Proxy und eine Datenbankadresse.  Tats√§chlich forderte der Client eine Kategorie, eine DB-Klasse, einen Shard-Schl√ºssel an, und die MySQL-Adresse wurde zur√ºckgegeben, zu der er bereits eine Verbindung herstellen konnte. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  Sobald es viele Server gab, wurde Memcache hinzugef√ºgt und die Clients begannen bereits mit ihm zu kommunizieren. <br><br>  Aber dann haben wir es √ºberarbeitet.  MySQL-Skripte begannen √ºber gRPC zu kommunizieren, √ºber einen Thin Client mit einem Dienst namens RegisterService.  Als einige √Ñnderungen auftraten, hatte RegisterService eine Warteschlange, und er verstand, wie diese √Ñnderungen angewendet werden.  RegisterService hat Daten in AFS gespeichert.  AFS ist unser internes System, das auf ZooKeeper basiert. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  Die zweite L√∂sung, die hier nicht gezeigt wird, verwendete ZooKeeper direkt, und dies verursachte Probleme, da jeder Shard ein Knoten in ZooKeeper war.  Zum Beispiel stellen 100.000 Clients eine Verbindung zu ZooKeeper her. Wenn sie pl√∂tzlich aufgrund eines Fehlers gestorben sind, werden sofort 100.000 Anfragen an ZooKeeper gesendet, die einfach gel√∂scht werden und nicht mehr steigen k√∂nnen. <br><br>  Daher wurde <strong>das AFS-System</strong> entwickelt <strong>, das von der gesamten Dropbox verwendet wird</strong> .  Tats√§chlich wird die Arbeit mit ZooKeeper f√ºr alle Kunden abstrahiert.  Der AFS-D√§mon wird lokal auf jedem Server ausgef√ºhrt und bietet eine sehr einfache Datei-API des Formulars: Erstellen Sie eine Datei, l√∂schen Sie eine Datei, fordern Sie eine Datei an, erhalten Sie eine Benachrichtigung √ºber eine Datei√§nderung und vergleichen und tauschen Sie Vorg√§nge aus.  Das hei√üt, Sie k√∂nnen versuchen, die Datei durch eine Version zu ersetzen. Wenn sich diese Version w√§hrend der √Ñnderung ge√§ndert hat, wird der Vorgang abgebrochen. <br><br>  Im Wesentlichen eine solche Abstraktion √ºber ZooKeeper, in der es einen lokalen Backoff- und Jitter-Algorithmus gibt.  ZooKeeper st√ºrzt unter Last nicht mehr ab.  Mit AFS erstellen wir Backups in S3 und GIT. Anschlie√üend benachrichtigt der lokale AFS die Clients selbst, dass sich die Daten ge√§ndert haben. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  In AFS werden Daten als Dateien gespeichert, dh es handelt sich um eine Dateisystem-API.  Das Obige ist beispielsweise die Datei shard.slave_proxy - die gr√∂√üte Datei ben√∂tigt ungef√§hr 28 KB. Wenn wir die Kategorie der Klasse shard und slave_proxy √§ndern, erhalten alle Clients, die diese Datei abonnieren, eine Benachrichtigung.  Sie lesen diese Datei erneut, die alle erforderlichen Informationen enth√§lt.  Mithilfe des Shard-Schl√ºssels erhalten sie eine Kategorie und konfigurieren den Verbindungspool zur Datenbank neu. <br><br><a name="perations_databases"></a><h2>  Operationen </h2><br>  Wir verwenden sehr einfache Vorg√§nge: Heraufstufen, Klonen, Sichern / Wiederherstellen. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Eine Operation ist eine einfache Zustandsmaschine</strong> .  Wenn wir in die Operation gehen, f√ºhren wir einige √úberpr√ºfungen durch, zum Beispiel eine Schleuderpr√ºfung, die mehrmals anhand des Zeitlimits √ºberpr√ºft, ob wir diese Operation ausf√ºhren k√∂nnen.  Danach f√ºhren wir einige vorbereitende Ma√ünahmen durch, die keine Auswirkungen auf externe Systeme haben.  Als n√§chstes die Operation selbst. <br><br>  Alle Schritte innerhalb einer Operation haben einen <strong>Rollback-Schritt</strong> (R√ºckg√§ngig).  Wenn bei der Operation ein Problem auftritt, versucht die Operation, das System an seiner urspr√ºnglichen Position wiederherzustellen.  Wenn alles in Ordnung ist, erfolgt die Bereinigung und der Vorgang ist abgeschlossen. <br><br>  Wir haben eine so einfache Zustandsmaschine f√ºr jede Operation. <br><br><h4>  <strong>Bef√∂rderung (Masterwechsel)</strong> </h4><br>  Dies ist eine sehr h√§ufige Operation in der Datenbank.  Es gab Fragen, wie man √Ñnderungen an einem funktionierenden Hot-Master-Server vornimmt - es wird einen Einsatz bekommen.  Es ist nur so, dass alle diese Vorg√§nge auf Slave-Servern ausgef√ºhrt werden und dann Slave-√Ñnderungen mit Master-Pl√§tzen vorgenommen werden.  Daher ist die <strong>Werbema√ünahme sehr h√§ufig</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Wir m√ºssen den Kernel aktualisieren - wir tauschen, wir m√ºssen die Version von MySQL aktualisieren - wir aktualisieren auf Slave, wechseln zu Master, aktualisieren dort. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Wir haben eine sehr schnelle Bef√∂rderung erreicht.  Zum Beispiel <strong>haben wir f√ºr vier Scherben jetzt eine Bef√∂rderung f√ºr ungef√§hr 10-15 s.</strong>  Die obige Grafik zeigt, dass die Verf√ºgbarkeit von Werbeaktionen um 0,0003% gelitten hat. <br><br>  Aber normale Werbung ist nicht so interessant, weil dies gew√∂hnliche Operationen sind, die jeden Tag durchgef√ºhrt werden.  Failover sind interessant. <br><br><h4>  <strong>Failover (Ersatz f√ºr einen defekten Master)</strong> <br></h4><br>  Ein Failover bedeutet, dass die Datenbank tot ist. <br><br><ul><li>  Wenn der Server wirklich gestorben ist, ist dies nur ein idealer Fall. </li><li>  In der Tat kommt es vor, dass die Server teilweise am Leben sind. </li><li>  Manchmal stirbt der Server sehr langsam.  Die RAID-Controller, das Festplattensystem fallen aus, einige Anforderungen geben Antworten zur√ºck, aber einige Flows sind blockiert und geben keine Antworten zur√ºck. </li><li>  Es kommt vor, dass der Master einfach √ºberlastet ist und nicht auf unseren Gesundheitscheck reagiert.  Aber wenn wir bef√∂rdert werden, wird auch der neue Meister √ºberlastet und es wird nur noch schlimmer. </li></ul><br>  Der Austausch verstorbener Master-Server erfolgt ca. <strong>2-3 mal am Tag</strong> . Dies ist ein vollautomatischer Prozess, bei dem kein menschliches Eingreifen erforderlich ist.  Der kritische Abschnitt dauert ungef√§hr 30 Sekunden und es gibt eine Reihe zus√§tzlicher √úberpr√ºfungen, um festzustellen, ob der Server tats√§chlich lebt oder m√∂glicherweise bereits gestorben ist. <br><br>  Unten sehen Sie ein Beispieldiagramm, wie der Faylover funktioniert. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  Im ausgew√§hlten Abschnitt <strong>starten</strong> wir <strong>den Master-Server neu</strong> .  Dies ist notwendig, da wir MySQL 5.6 haben und die semisynchrone Replikation nicht verlustfrei ist.  Daher sind Phantom-Lesevorg√§nge m√∂glich, und wir brauchen diesen Master, auch wenn er nicht gestorben ist, so schnell wie m√∂glich zu t√∂ten, damit Clients die Verbindung dazu trennen.  Daher f√ºhren wir einen Hard-Reset √ºber Ipmi durch - dies ist die erste wichtige Operation, die wir ausf√ºhren m√ºssen.  In der MySQL 5.7-Version ist dies nicht so kritisch. <br><br>  <strong>Clustersynchronisation.</strong>  Warum brauchen wir eine Clustersynchronisation? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Wenn wir uns mit unserer Topologie an das vorherige Bild erinnern, hat ein Master-Server drei Slave-Server: zwei in einem Rechenzentrum, einer im anderen.  Bei der Bef√∂rderung muss sich der Master im selben Hauptdatenzentrum befinden.  Aber manchmal, wenn Slaves mit Semisync geladen werden, kommt es vor, dass ein Semisync-Slave ein Slave in einem anderen Rechenzentrum wird, weil er nicht geladen wird.  Daher m√ºssen wir zuerst den gesamten Cluster synchronisieren und dann bereits eine Promotion auf dem Slave in dem von uns ben√∂tigten Rechenzentrum durchf√ºhren.  Dies geschieht ganz einfach: <br><br><ul><li>  Wir stoppen alle E / A-Threads auf allen Slave-Servern. </li><li>  Danach wissen wir bereits mit Sicherheit, dass der Master "schreibgesch√ºtzt" ist, da die Semisync-Verbindung getrennt wurde und niemand anderes dort etwas schreiben kann. </li><li>  Als n√§chstes w√§hlen wir den Slave mit dem gr√∂√üten abgerufenen / ausgef√ºhrten GTID-Satz aus, dh mit der gr√∂√üten Transaktion, die er entweder heruntergeladen oder bereits angewendet hat. </li><li>  Wir konfigurieren alle Slave-Server f√ºr diesen ausgew√§hlten Slave neu, starten den E / A-Thread und sie werden synchronisiert. </li><li>  Wir warten, bis sie synchronisiert sind. Danach wird der gesamte Cluster synchronisiert.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Am Ende √ºberpr√ºfen wir, ob der GTID-Satz √ºberall, wo wir ihn ausgef√ºhrt haben, auf dieselbe Position gesetzt ist. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die zweite wichtige Operation ist die </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Clustersynchronisation</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Die weitere </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Werbung</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> beginnt </font><font style="vertical-align: inherit;">wie folgt:</font></font><br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir w√§hlen jeden Slave im Rechenzentrum aus, den wir ben√∂tigen, sagen ihm, dass er Master ist, und starten den Standard-Promotion-Vorgang. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir konfigurieren alle Slave-Server f√ºr diesen Master neu, stoppen dort die Replikation, wenden ACLs an, fahren Benutzer ein, stoppen einige Proxys und starten m√∂glicherweise etwas neu. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Am Ende machen wir read_only = 0, das hei√üt, wir sagen, dass wir jetzt in den Master schreiben und die Topologie aktualisieren k√∂nnen. </font><font style="vertical-align: inherit;">Von nun an gehen Kunden zu diesem Master und alles funktioniert f√ºr sie.</font></font></li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  ‚Äî change master ‚Äî    master   . <br><br><h4> <strong></strong> </h4><br>  ‚Äî      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> ‚óè   slave <br><br>   ,       slave-,      .   . <br><br> ‚óè       <br><br>     ,     ,      .             . <br><br> ‚óè       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     ‚Äîstream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        ‚Äî  10 ,  45  ‚Äî   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    ‚Äî      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .  Alles ist sehr einfach. <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  ‚Äî   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Automatisierung </h2><br>  ,  6 000      .         ,   ,     ‚Äî : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     ‚Äî ,     -.   ,   . <br><br> <strong>Availability ()</strong> ‚Äî         ,         .      ‚Äî   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat ‚Äî   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> √úberpr√ºfen Sie als N√§chstes, ob ihre Anzahl den Schwellenwert erf√ºllt.  Wenn nicht, stimmt etwas mit dem Server nicht - da er keinen Herzschlag gesendet hat. </li><li>  Danach f√ºhren wir f√ºr alle F√§lle eine umgekehrte √úberpr√ºfung durch - pl√∂tzlich sind diese beiden Dienste ausgefallen, etwas befindet sich im Netzwerk oder die globale Datenbank kann den Heartbeat aus irgendeinem Grund nicht schreiben.  Bei der umgekehrten √úberpr√ºfung stellen wir eine Verbindung zu einer defekten Datenbank her und √ºberpr√ºfen deren Status. </li><li>  Wenn alles andere fehlschl√§gt, pr√ºfen wir, ob die Master-Position voranschreitet oder nicht, ob Datens√§tze darauf vorhanden sind.  Wenn nichts passiert, funktioniert dieser Server definitiv nicht. </li><li>  Der letzte Schritt ist das automatische Ersetzen. </li></ul><br>  Das automatische Ersetzen ist sehr konservativ. Er m√∂chte nie viele automatische Operationen ausf√ºhren. <br><br><ol><li>  Zuerst pr√ºfen wir, ob in letzter Zeit Topologieoperationen stattgefunden haben.  M√∂glicherweise wurde dieser Server gerade hinzugef√ºgt und etwas darauf l√§uft noch nicht. </li><li>  Wir √ºberpr√ºfen, ob zu irgendeinem Zeitpunkt Ersatz im selben Cluster vorhanden war. </li><li>  √úberpr√ºfen Sie, welche Fehlergrenze wir haben.  Wenn wir viele Probleme gleichzeitig haben - 10, 20 -, werden wir nicht alle automatisch l√∂sen, da wir versehentlich den Betrieb aller Datenbanken st√∂ren k√∂nnen. </li></ol><br>  Daher <strong>l√∂sen</strong> wir jeweils <strong>nur ein Problem</strong> . <br><br>  Dementsprechend beginnen wir f√ºr den Slave-Server mit dem Klonen und entfernen ihn einfach aus der Topologie. Wenn es sich um einen Master handelt, starten wir den Feylover, die sogenannte Notfall-Promotion. <br><br><h3>  DBManager </h3><br>  DBManager ist ein Dienst zur Verwaltung unserer Datenbanken.  Es hat: <br><br><ul><li>  Smart Task Scheduler, der genau wei√ü, wann der Job zu starten ist; </li><li>  Protokolle und alle Informationen: Wer, wann und was gestartet - das ist die Quelle der Wahrheit; </li><li>  Synchronisationspunkt. </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  DBManager ist architektonisch recht einfach. <br><br><ul><li>  Es gibt Clients, entweder DBAs, die etwas √ºber die Weboberfl√§che tun, oder Skripte / Dienste, die DBAs geschrieben haben, die √ºber gRPC zugreifen. </li><li>  Es gibt externe Systeme wie Wheelhouse und Naoru, die √ºber gRPC an DBManager gehen. </li><li>  Es gibt einen Planer, der versteht, welche Operation wann und wo er beginnen kann. </li><li>  Es gibt einen sehr dummen Arbeiter, der, wenn eine Operation zu ihm kommt, sie startet und per PID pr√ºft.  Worker kann neu starten, Prozesse werden nicht unterbrochen.  Alle Mitarbeiter befinden sich so nah wie m√∂glich an den Servern, auf denen die Vorg√§nge stattfinden, sodass wir beispielsweise beim Aktualisieren von ACLS nicht viele Roundtrips durchf√ºhren m√ºssen. </li><li>  Auf jedem SQL-Host haben wir einen DBAgent - dies ist ein RPC-Server.  Wenn Sie eine Operation auf dem Server ausf√ºhren m√ºssen, senden wir eine RPC-Anfrage. </li></ul><br>  Wir haben eine Weboberfl√§che f√ºr DBManager, √ºber die Sie die aktuell ausgef√ºhrten Aufgaben, Protokolle f√ºr diese Aufgaben, wer sie wann gestartet hat, welche Vorg√§nge f√ºr den Server einer bestimmten Datenbank usw. ausgef√ºhrt wurden, anzeigen k√∂nnen. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Es gibt eine ziemlich einfache CLI-Oberfl√§che, √ºber die Sie Aufgaben ausf√ºhren und in praktischen Ansichten anzeigen k√∂nnen. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Abhilfema√ünahmen </h3><br>  Wir haben auch ein System, um auf Probleme zu reagieren.  Wenn beispielsweise etwas kaputt geht, das Laufwerk abst√ºrzt oder ein Dienst nicht funktioniert, funktioniert <strong>Naoru.</strong>  Dies ist das System, das in Dropbox funktioniert, von jedem verwendet wird und speziell f√ºr solche kleinen Aufgaben entwickelt wurde.  Ich habe in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bericht</a> 2016 √ºber Naoru gesprochen. <br><br>  <strong>Wheelhouse</strong> basiert auf einer <strong>Zustandsmaschine</strong> und ist f√ºr lange Prozesse ausgelegt.  Zum Beispiel m√ºssen wir den Kernel auf allen MySQL auf unserem gesamten Cluster von 6.000 Computern aktualisieren.  Wheelhouse macht dies klar - Updates auf dem Slave-Server, startet die Promotion, Slave wird Master, Updates auf dem Master-Server.  Dieser Vorgang kann ein oder sogar zwei Monate dauern. <br><br><a name="monitoring"></a><h2>  √úberwachung </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  Es ist sehr wichtig. <br><br><blockquote>  Wenn Sie das System nicht √ºberwachen, funktioniert es h√∂chstwahrscheinlich nicht. </blockquote><br>  Wir √ºberwachen alles in MySQL - alle Informationen, die wir von MySQL erhalten k√∂nnen, werden irgendwo gespeichert, wir k√∂nnen rechtzeitig darauf zugreifen.  Wir speichern Informationen zu InnoDb, Statistiken zu Anfragen, zu Transaktionen, zur L√§nge von Transaktionen, Perzentile zu Transaktionsl√§ngen, zur Replikation, im Netzwerk - alles in allem - eine gro√üe Anzahl von Metriken. <br><br><h3>  Alarm </h3><br>  Wir haben 992 Warnungen konfiguriert.  Tats√§chlich befasst sich niemand mit Metriken. Es scheint mir, dass es keine Leute gibt, die zur Arbeit kommen und anfangen, sich das Metrikdiagramm anzusehen. Es gibt interessantere Aufgaben. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Daher gibt es Warnungen, die funktionieren, wenn bestimmte Schwellenwerte erreicht werden.  <strong>Wir haben 992 Warnungen, egal was passiert, wir werden es herausfinden</strong> . <br><br><h3>  Vorf√§lle </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Wir haben PagerDuty - einen Dienst, √ºber den Benachrichtigungen an verantwortliche Personen gesendet werden, die Ma√ünahmen ergreifen. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  In diesem Fall ist ein Fehler bei der Notfallwerbung aufgetreten, und unmittelbar danach wurde eine Warnung aufgezeichnet, dass der Master gefallen ist.  Danach √ºberpr√ºfte der Dienstoffizier, was eine Notfallf√∂rderung verhinderte, und f√ºhrte die erforderlichen manuellen Operationen durch. <br><br>  Wir werden sicherlich jeden aufgetretenen Vorfall analysieren, f√ºr jeden Vorfall haben wir eine Aufgabe im Task-Tracker.  Selbst wenn dieser Vorfall ein Problem in unseren Warnungen darstellt, erstellen wir auch eine Aufgabe. Wenn das Problem in der Warnungslogik und den Schwellenwerten liegt, m√ºssen diese ge√§ndert werden.  Warnungen sollten nicht nur das Leben der Menschen verderben.  Ein Alarm ist immer schmerzhaft, besonders um 4 Uhr morgens. <br><br><a name="testing"></a><h2>  Testen </h2><br>  Wie bei der √úberwachung bin ich sicher, dass jeder testet.  Zus√§tzlich zu den Komponententests, mit denen wir unseren Code abdecken, haben wir Integrationstests, in denen wir testen: <br><br><ul><li>  alle Topologien, die wir haben; </li><li>  alle Operationen auf diesen Topologien. </li></ul><br>  Wenn wir Bef√∂rderungsvorg√§nge haben, testen wir Bef√∂rderungsvorg√§nge im Integrationstest.  Wenn wir klonen, klonen wir f√ºr alle Topologien, die wir haben. <br><br>  <strong>Topologiebeispiel</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Wir haben Topologien f√ºr alle Gelegenheiten: 2 Rechenzentren mit mehreren Instanzen, mit Shards, keine Shards, mit Clustern, ein Rechenzentrum - im Allgemeinen fast jede Topologie - auch solche, die wir nicht verwenden, nur um zu sehen. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  In dieser Datei haben wir nur die Einstellungen, welche Server und mit was wir erh√∂hen m√ºssen.  Zum Beispiel m√ºssen wir den Master erh√∂hen, und wir sagen, dass wir dies mit solchen und solchen Instanzdaten tun m√ºssen, mit solchen und solchen Datenbanken an solchen und solchen Ports.  Fast alles passt zusammen mit Bazel, das auf der Grundlage dieser Dateien eine Topologie erstellt, den MySQL-Server startet und dann den Test startet. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  Der Test sieht sehr einfach aus: Wir geben an, welche Topologie verwendet wird.  In diesem Test testen wir auto_replace. <br><br><ul><li>  Wir erstellen den auto_replace-Dienst und starten ihn. </li><li>  Wir t√∂ten den Meister in unserer Topologie, warten eine Weile und sehen, dass der Zielsklave Meister geworden ist.  Wenn nicht, ist der Test fehlgeschlagen. </li></ul><br><h3>  Stufen </h3><br>  B√ºhnenumgebungen sind dieselben Datenbanken wie in der Produktion, es gibt jedoch keinen Benutzerverkehr, aber synthetischen Verkehr, der der Produktion √ºber Percona Playback, Sysbench und √§hnliche Systeme √§hnelt. <br><br>  In Percona Playback zeichnen wir Datenverkehr auf, verlieren ihn dann in der B√ºhnenumgebung mit unterschiedlicher Intensit√§t und k√∂nnen 2-3 mal schneller verlieren.  Das hei√üt, es ist k√ºnstlich, aber sehr nahe an der tats√§chlichen Last. <br><br>  Dies ist notwendig, da wir in Integrationstests unsere Produktion nicht testen k√∂nnen.  Wir k√∂nnen die Warnung oder die Tatsache, dass Metriken funktionieren, nicht testen.  In der Testphase testen wir Warnungen, Metriken und Vorg√§nge, beenden die Server regelm√§√üig und stellen sicher, dass sie normal erfasst werden. <br><br>  Au√üerdem testen wir die gesamte Automatisierung zusammen, da bei Integrationstests h√∂chstwahrscheinlich ein Teil des Systems getestet wird und beim Staging alle automatisierten Systeme gleichzeitig funktionieren.  Manchmal denken Sie, dass sich das System so und nicht anders verh√§lt, aber es kann sich ganz anders verhalten. <br><br><h3>  DRT (Disaster Recovery Testing) </h3><br>  Wir f√ºhren auch Tests in der Produktion durch - direkt auf realen Grundlagen.  Dies wird als Disaster Recovery-Test bezeichnet.  Warum brauchen wir das? <br><br>  ‚óè Wir m√∂chten unsere Garantien testen. <br><br>  Dies wird von vielen gro√üen Unternehmen durchgef√ºhrt.  Zum Beispiel hat Google einen Dienst, der so stabil funktioniert - 100% der Zeit -, dass alle Dienste, die ihn verwendet haben, entschieden haben, dass dieser Dienst wirklich 100% stabil ist und niemals abst√ºrzt.  Daher musste Google diesen Dienst absichtlich einstellen, damit die Nutzer diese M√∂glichkeit ber√ºcksichtigen. <br><br>  Wir sind also - wir haben die Garantie, dass MySQL funktioniert - und manchmal funktioniert es nicht!  Und wir haben die Garantie, dass es f√ºr einen bestimmten Zeitraum m√∂glicherweise nicht funktioniert. Kunden sollten dies ber√ºcksichtigen.  Von Zeit zu Zeit t√∂ten wir den Produktionsmaster, oder wenn wir einen Faylover machen wollen, t√∂ten wir alle Slaves, um zu sehen, wie sich die semisynchrone Replikation verh√§lt. <br><br>  ‚óè Kunden sind auf diese Fehler vorbereitet (Austausch und Tod des Masters) <br><br>  Warum ist das gut?  Wir hatten einen Fall, in dem w√§hrend der Promotion 4 von 1600 Shards die Verf√ºgbarkeit auf 20% sank.  Es scheint, dass etwas nicht stimmt, f√ºr 4 Scherben ab 1600 sollte es einige andere Zahlen geben.  Failover f√ºr dieses System waren selten, etwa einmal im Monat, und alle entschieden: "Nun, es ist ein Failover, es passiert." <br><br>  Irgendwann, als wir auf ein neues System umgestiegen sind, hat eine Person beschlossen, diese beiden Heartbeat-Aufzeichnungsdienste zu optimieren und zu einem zu kombinieren.  Dieser Dienst hat etwas anderes getan und ist am Ende gestorben und der Herzschlag hat aufgeh√∂rt aufzuzeichnen.  So kam es, dass wir f√ºr diesen Kunden 8 Faylovers pro Tag hatten.  Alles lag - 20% Verf√ºgbarkeit. <br><br>  Es stellte sich heraus, dass bei diesem Client die Lebensdauer 6 Stunden betr√§gt.  Dementsprechend hatten wir, sobald der Meister starb, alle Verbindungen f√ºr weitere 6 Stunden gehalten.  Der Pool konnte nicht weiter funktionieren - seine Verbindungen bleiben erhalten, er ist begrenzt und funktioniert nicht.  Es wurde repariert. <br><br>  Wir machen den Feylover wieder - nicht mehr 20%, aber immer noch viel.  Irgendwas stimmt immer noch nicht.  Es stellte sich heraus, dass ein Fehler in der Implementierung des Pools.  Auf Anfrage wandte sich der Pool vielen Scherben zu und verband dies alles.  Wenn einige Scherben fieberhaft waren, trat im Go-Code eine Rennbedingung auf, und der gesamte Pool war verstopft.  Alle diese Scherben konnten nicht mehr funktionieren. <br><br>  Disaster Recovery-Tests sind sehr n√ºtzlich, da Clients auf diese Fehler vorbereitet sein m√ºssen und ihren Code √ºberpr√ºfen m√ºssen. <br><br>  ‚óè Plus Disaster Recovery-Tests sind gut, da sie w√§hrend der Gesch√§ftszeiten stattfinden und alles vorhanden ist, weniger Stress, die Leute wissen, was jetzt passieren wird.  Das passiert nachts nicht und es ist gro√üartig. <br><br><h2>  Fazit </h2><br>  1. Alles muss automatisiert werden, niemals in die H√§nde bekommen. <br>  Jedes Mal, wenn jemand mit unseren H√§nden in das System steigt, stirbt und bricht alles in unserem System - jedes Mal!  - auch bei einfachen Operationen.  Zum Beispiel starb ein Sklave, eine Person musste einen zweiten hinzuf√ºgen, entschied sich jedoch, den toten Sklaven mit seinen H√§nden aus der Topologie zu entfernen.  Anstelle des Verstorbenen kopierte er jedoch das Kommando live - der Meister blieb √ºberhaupt ohne Sklaven.  Solche Operationen sollten nicht manuell durchgef√ºhrt werden. <br><br>  2. Die Tests sollten kontinuierlich und automatisiert (und in der Produktion) sein. <br>  Ihr System √§ndert sich, Ihre Infrastruktur √§ndert sich.  Wenn Sie einmal nachgesehen haben und es zu funktionieren schien, bedeutet dies nicht, dass es morgen funktionieren wird.  Daher m√ºssen Sie jeden Tag, auch in der Produktion, st√§ndig automatisierte Tests durchf√ºhren. <br><br>  3. Stellen Sie sicher, dass Sie Clients (Bibliotheken) besitzen. <br>  Benutzer wissen m√∂glicherweise nicht, wie Datenbanken funktionieren.  Sie verstehen m√∂glicherweise nicht, warum Zeit√ºberschreitungen erforderlich sind, um am Leben zu bleiben.  Daher ist es besser, diese Kunden zu besitzen - Sie werden ruhiger sein. <br><br>  4. Es ist notwendig, Ihre Grunds√§tze f√ºr den Aufbau des Systems und Ihre Garantien festzulegen und diese stets einzuhalten. <br><br>  Somit k√∂nnen Sie 6.000 Datenbankserver unterst√ºtzen. <br><br><div class="spoiler">  <b class="spoiler_title">Bei Fragen nach dem Bericht und insbesondere bei den Antworten darauf gibt es auch viele n√ºtzliche Informationen.</b> <div class="spoiler_text"><h2>  Fragen und Antworten <br></h2><br><blockquote>  - Was passiert, wenn die Last der Shards unausgewogen ist - einige Metainformationen zu einer Datei haben sich als beliebter herausgestellt?  Ist es m√∂glich, diese Scherbe zu verbreiten, oder unterscheidet sich die Belastung der Scherben nirgendwo um Gr√∂√üenordnungen? </blockquote><br>  Sie unterscheidet sich nicht um Gr√∂√üenordnungen.  Es ist fast normal verteilt.  Wir haben Drosselung, das hei√üt, wir k√∂nnen den Shard nicht √ºberlasten. Wir drosseln auf Client-Ebene.  Im Allgemeinen kommt es vor, dass ein Stern ein Foto hochl√§dt und die Scherbe praktisch explodiert.  Dann verbieten wir diesen Link <br><br><blockquote>  - Sie sagten, Sie haben 992 Warnungen.  K√∂nnten Sie n√§her erl√§utern, was es ist - ist es sofort einsatzbereit oder wurde es erstellt?  Wenn erstellt, ist es Handarbeit oder so etwas wie maschinelles Lernen? </blockquote><br>  Dies wird alles manuell erstellt.  Wir haben unser eigenes internes System namens Vortex, in dem Metriken gespeichert und Warnungen darin unterst√ºtzt werden.  Es gibt eine Yaml-Datei, die besagt, dass beispielsweise eine Bedingung vorliegt, dass Sicherungen jeden Tag ausgef√ºhrt werden m√ºssen. Wenn diese Bedingung erf√ºllt ist, funktioniert die Warnung nicht.  Wenn nicht ausgef√ºhrt, wird eine Warnung ausgegeben. <br><br>  Dies ist unsere interne Entwicklung, da nur wenige Personen so viele Metriken speichern k√∂nnen, wie wir ben√∂tigen. <br><br><blockquote>  - Wie stark m√ºssen die Nerven sein, um DRT zu machen?  Du bist gefallen, CODERED, steigt nicht auf, mit jeder Minute Panik mehr. </blockquote><br>  Im Allgemeinen ist das Arbeiten in Datenbanken wirklich ein Schmerz.  Wenn die Datenbank abst√ºrzt, funktioniert der Dienst nicht, die gesamte Dropbox funktioniert nicht.  Das ist ein echter Schmerz.  DRT ist insofern n√ºtzlich, als es eine Gesch√§ftsuhr ist.  Das hei√üt, ich bin bereit, ich sitze an meinem Schreibtisch, ich habe Kaffee getrunken, ich bin frisch, ich bin bereit, alles zu tun. <br><br>  Schlimmer noch, wenn es um 4 Uhr morgens passiert und es nicht DRT ist.  Zum Beispiel der letzte gro√üe Fehler, den wir k√ºrzlich hatten.  Beim Injizieren eines neuen Systems haben wir vergessen, den OOM-Score f√ºr MySQL festzulegen.  Es gab einen anderen Dienst, der binlog las.  Irgendwann ist unser Bediener manuell - wieder manuell!  - f√ºhrt den Befehl aus, um einige Informationen in der Percona-Pr√ºfsummentabelle zu l√∂schen.  Nur ein einfaches L√∂schen, eine einfache Operation, aber diese Operation erzeugte ein riesiges Binlog.  Der Dienst las dieses Binlog in den Speicher, OOM Killer kam und √ºberlegte, wen er t√∂ten sollte.  Und wir haben vergessen, den OOM-Score festzulegen, und das t√∂tet MySQL! <br><br>  Wir haben 40 Meister, die um 4 Uhr morgens sterben.  Wenn 40 Meister sterben, ist das wirklich sehr be√§ngstigend und gef√§hrlich.  DRT ist nicht be√§ngstigend und nicht gef√§hrlich.  Wir lagen ungef√§hr eine Stunde. <br><br>  DRT ist √ºbrigens eine gute M√∂glichkeit, solche Momente zu proben, damit wir genau wissen, welche Abfolge von Aktionen erforderlich ist, wenn etwas massenhaft kaputt geht. <br><br><blockquote>  - Ich m√∂chte mehr √ºber das Wechseln von Master-Master erfahren.  Erstens, warum wird beispielsweise ein Cluster nicht verwendet?  Ein Datenbankcluster, dh kein Master-Slave mit Switching, sondern eine Master-Master-Anwendung, sodass es nicht be√§ngstigend ist, wenn einer f√§llt. </blockquote><br>  Meinen Sie so etwas wie Gruppenreplikation, Galera-Cluster usw.?  Es scheint mir, dass die Gruppenanwendung noch nicht lebensbereit ist.  Leider haben wir Galera noch nicht ausprobiert.  Dies ist gro√üartig, wenn sich ein Faylover in Ihrem Protokoll befindet, aber leider so viele andere Probleme hat und es nicht so einfach ist, zu dieser L√∂sung zu wechseln. <br><br><blockquote>  - Es scheint, dass es in MySQL 8 so etwas wie einen InnoDb-Cluster gibt.  Nicht versucht? </blockquote><br>  Wir haben noch 5,6 wert.  Ich wei√ü nicht, wann wir zu 8 wechseln werden. Vielleicht versuchen wir es. <br><br><blockquote>  - Wenn Sie in diesem Fall einen gro√üen Master haben, stellt sich beim Wechsel von einem zum anderen heraus, dass sich die Warteschlange auf den Slave-Servern mit hoher Last ansammelt.  Wenn der Master gel√∂scht ist, muss die Warteschlange erreicht werden, damit der Slave in den Master-Modus wechselt - oder wird dies irgendwie anders gemacht? </blockquote><br>  Die Belastung des Masters wird durch Semisync geregelt.  Semisync beschr√§nkt die Masteraufzeichnung auf die Leistung des Slave-Servers.  Nat√ºrlich kann es sein, dass die Transaktion kam, Semisync funktionierte, aber die Slaves haben diese Transaktion f√ºr eine sehr lange Zeit verloren.  Sie m√ºssen dann warten, bis der Slave diese Transaktion bis zum Ende verliert. <br><br><blockquote>  - Aber dann werden neue Daten gemastert, und es wird notwendig sein ... </blockquote><br>  Wenn wir den Promotionsprozess starten, deaktivieren wir I / O.  Danach kann der Master nichts mehr schreiben, da Semisync repliziert wird.  Phantomlesung kann leider kommen, aber dies ist bereits ein weiteres Problem. <br><br><blockquote>  - Dies sind alles sch√∂ne Zustandsautomaten - worauf sind die Skripte geschrieben und wie schwierig ist es, einen neuen Schritt hinzuzuf√ºgen?  Was muss mit der Person getan werden, die dieses System schreibt? </blockquote><br>  Alle Skripte sind in Python geschrieben, alle Dienste sind in Go geschrieben.  Dies ist unsere Politik.  Das √Ñndern der Logik ist einfach - nur im Python-Code, der das Zustandsdiagramm generiert. <br><br><blockquote>  - Und Sie k√∂nnen mehr √ºber das Testen lesen.  Wie werden Tests geschrieben, wie werden Knoten in einer virtuellen Maschine bereitgestellt - sind dies Container? </blockquote><br>  Ja  Wir werden mit Hilfe von Bazel testen.  Es gibt einige Konfigurationsdateien (json) und Bazel greift auf ein Skript zur√ºck, das mithilfe dieser Konfigurationsdatei die Topologie f√ºr unseren Test erstellt.  Dort werden verschiedene Topologien beschrieben. <br><br>  In Docker-Containern funktioniert alles f√ºr uns: entweder in CI oder in Devbox.  Wir haben ein Devbox-System.  Wir entwickeln alle auf einem Remote-Server, und dies kann beispielsweise funktionieren.  Dort l√§uft es auch in Bazel, in einem Docker-Container oder in der Bazel Sandbox.  Bazel ist sehr kompliziert, macht aber Spa√ü. <br><br><blockquote>  - Wenn Sie 4 Instanzen auf einem Server erstellt haben, haben Sie an Speichereffizienz verloren? </blockquote><br>  Jede Instanz ist kleiner geworden.  Je weniger Speicher MySQL verwendet, desto einfacher ist es f√ºr ihn zu leben.  Jedes System ist mit wenig Speicher einfacher zu bedienen.  An diesem Ort haben wir nichts verloren.  Wir haben die einfachsten C-Gruppen, die diese Instanzen aus dem Speicher beschr√§nken. <br><br><blockquote>  - Wenn Sie 6.000 Server haben, auf denen Datenbanken gespeichert sind, k√∂nnen Sie angeben, wie viele Milliarden Petabyte in Ihren Dateien gespeichert sind? </blockquote><br>  Dies sind Dutzende von Exabytes, wir haben ein Jahr lang Daten von Amazon gegossen. <br><br><blockquote>  - Es stellte sich heraus, dass Sie zuerst 8 Server mit 200 Shards und dann 400 Server mit jeweils 4 Shards hatten.  Sie haben 1600 Shards - ist das eine Art fest codierter Wert?  Schaffst du es nie wieder?  Wird es weh tun, wenn Sie zum Beispiel 3.200 Scherben ben√∂tigen? </blockquote><br>  Ja, es war urspr√ºnglich 1600. Dies wurde vor etwas weniger als 10 Jahren getan, und wir leben immer noch.  Aber wir haben immer noch 4 Scherben - 4 Mal k√∂nnen wir den Raum noch vergr√∂√üern. <br><br><blockquote>  - Wie sterben Server haupts√§chlich aus welchen Gr√ºnden?  Was passiert √∂fter, seltener und es ist besonders interessant, ob spontane Block-Carapters auftreten? </blockquote><br>  Das Wichtigste ist, dass die Festplatten herausfliegen.  Wir haben RAID 0 - die Festplatte ist abgest√ºrzt, der Master ist gestorben.  Dies ist das Hauptproblem, aber es ist f√ºr uns einfacher, diesen Server zu ersetzen.  Google ist einfacher, das Rechenzentrum zu ersetzen, wir haben noch einen Server.  Wir hatten fast nie eine Korruptionspr√ºfsumme.  Um ehrlich zu sein, ich erinnere mich nicht, wann es das letzte Mal war.  Wir aktualisieren den Assistenten nur oft.  Unsere Lebenszeit f√ºr einen Meister ist auf 60 Tage begrenzt.  Es kann nicht l√§nger leben, danach ersetzen wir es durch einen neuen Server, da sich aus irgendeinem Grund st√§ndig etwas in MySQL ansammelt und nach 60 Tagen Probleme auftreten.  Vielleicht nicht in MySQL, vielleicht in Linux. <br><br>  Wir wissen nicht, was dieses Problem ist, und wir wollen uns nicht damit befassen.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben die Zeit nur auf 60 Tage begrenzt und den gesamten Stapel aktualisiert. </font><font style="vertical-align: inherit;">Sie m√ºssen sich nicht an einen Meister halten.</font></font><br><br><blockquote> ‚Äî  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    ‚Äî      ,       ? </blockquote><br>     ,  .   ‚Äî  Dropbox    . <br><br><blockquote> ‚Äî      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">youtube-</a> ‚Äî          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417315/">https://habr.com/ru/post/de417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417303/index.html">Hack zur Unterst√ºtzung von Windows Android-Headset-Tasten</a></li>
<li><a href="../de417305/index.html">Ultima Online: ein Backstage-Look</a></li>
<li><a href="../de417307/index.html">Glaukom - noch nichts von ihr geh√∂rt? Lernen Sie den Serienkiller f√ºr stille Visionen kennen</a></li>
<li><a href="../de417309/index.html">ITSM-Manager Gl√ºcklicherweise: Wie der Beruf der Zukunft dazu beitr√§gt, die Grenzen des Service Desks zu erweitern</a></li>
<li><a href="../de417311/index.html">Erstellen eines Bots zur Teilnahme am AI Mini Cup 2018 basierend auf einem wiederkehrenden neuronalen Netzwerk</a></li>
<li><a href="../de417317/index.html">Bei Highload ++ 2018 volle Kraft voraus</a></li>
<li><a href="../de417319/index.html">Systeme im Geh√§use oder Was befindet sich eigentlich unter der Abdeckung des Mikroprozessors</a></li>
<li><a href="../de417321/index.html">Wie suchen wir Lehrer f√ºr Online-Kurse unter Entwicklern?</a></li>
<li><a href="../de417323/index.html">Probleme bei der Gew√§hrleistung einer 100% igen Projektzug√§nglichkeit</a></li>
<li><a href="../de417325/index.html">Tag der offenen T√ºr in der Netrologie, Thema Data Science</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>