<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤘 🙅🏽 👨🏿‍🌾 Datenbankentwicklung in Dropbox. Der Pfad von einer globalen MySQL-Datenbank zu Tausenden von Servern 👩🏾‍🔧 👩‍👧 🤽🏿</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Als Dropbox gerade gestartet wurde, kommentierte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git imp...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Datenbankentwicklung in Dropbox. Der Pfad von einer globalen MySQL-Datenbank zu Tausenden von Servern</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/417315/">  Als Dropbox gerade gestartet wurde, kommentierte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git implementiert werden könnte.  Dies kann in keiner Weise gesagt werden. Dies ist ein großer Cloud-Dateispeicher mit Milliarden neuer Dateien pro Tag, die nicht nur irgendwie in der Datenbank gespeichert werden, sondern auch so, dass jede Datenbank innerhalb der letzten sechs Tage zu einem beliebigen Zeitpunkt wiederhergestellt werden kann. <br><br>  Unter dem Schnitt das Transkript des Berichts von <strong>Glory Bakhmutov</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" class="user_link">m0sth8</a> ) auf Highload ++ 2017 darüber, wie sich die Datenbanken in Dropbox entwickelt haben und wie sie jetzt angeordnet sind. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/hUFFsLoCRNU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong>Über den Redner:</strong> Ehre sei Bakhmutov - Site Reliability Engineer im Dropbox-Team, liebt Go sehr und erscheint manchmal im Podcast von golangshow.com. <br><br><h2>  Inhalt <br></h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Kurz über die Dropbox-Architektur</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Geschichte der Entwicklung</a> von Datenbanken und wie die aktuelle Dropbox-Architektur funktioniert </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die einfachsten Operationen für Datenbanken</a> (Feylover, Backups, Klone, Promotions) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Automatisierung</a> - verwaltet alle Datenbanken und führt Vorgänge aus </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Überwachung</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Testen, Staging und DRT</a> </li></ul><br><img src="https://habrastorage.org/webt/dh/gt/p7/dhgtp7pgu4eyl6rc3ncfnaf9s3i.jpeg"><br><a name="habracut"></a><br><a name="dropbox_architecture"></a><h2>  Dropbox-Architektur im Klartext </h2><br>  Dropbox erschien im Jahr 2008.  Dies ist im Wesentlichen ein Cloud-Dateispeicher.  Als Dropbox gerade gestartet wurde, sagte ein Benutzer von Hacker News, dass es mit mehreren Bash-Skripten unter Verwendung von FTP und Git implementiert werden könnte.  Trotzdem entwickelt sich Dropbox weiter und ist jetzt ein ziemlich großer Dienst mit mehr als 1,5 Milliarden Benutzern, 200.000 Unternehmen und einer großen Anzahl (mehrere Milliarden!) Täglich neuer Dateien. <br><br>  <strong>Wie sieht Dropbox aus?</strong> <br><img src="https://habrastorage.org/webt/ed/em/km/edemkm1wqvlbv6jb0qobp5c2dgc.jpeg"><br><br>  Wir haben mehrere Clients (Webschnittstelle, API für Anwendungen, die Dropbox verwenden, Desktopanwendungen).  Alle diese Clients verwenden die API und kommunizieren mit zwei großen Diensten, die logisch unterteilt werden können in: <br><br><ol><li>  <strong>Metaserver</strong> </li><li>  <strong>Blockserver</strong> </li></ol><br>  Metaserver speichert Metainformationen über die Datei: Größe, Kommentare dazu, Links zu dieser Datei in Dropbox usw.  Blockserver speichert nur Informationen zu Dateien: Ordner, Pfade usw. <br><br>  <strong>Wie funktioniert es</strong> <br><br>  Zum Beispiel haben Sie eine video.avi-Datei mit einer Art Video. <br><img src="https://habrastorage.org/webt/kc/i9/op/kci9op0l7f_tecaxetg7uzpzemw.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br><ul><li>  Der Client teilt diese Datei in mehrere Blöcke auf (in diesem Fall jeweils 4 MB), berechnet die Prüfsumme und sendet eine Anfrage an Metaserver: "Ich habe eine * .avi-Datei, ich möchte sie hochladen, die Hash-Beträge sind so und so." </li><li>  Metaserver gibt die Antwort zurück: "Ich habe diese Blöcke nicht, lasst uns herunterladen!"  Oder er kann antworten, dass er alle oder einige der Blöcke hat und nur die verbleibenden geladen werden müssen. </li></ul><br><img src="https://habrastorage.org/webt/qa/zr/79/qazr79svhai2ouk6v8lta1zcp-u.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br><ul><li>  Danach geht der Client zu Blockserver, sendet die Hash-Menge und den Datenblock selbst, der auf dem Blockserver gespeichert ist. </li><li>  Blockserver bestätigt den Vorgang. </li></ul><br><img src="https://habrastorage.org/webt/kl/dx/xw/kldxxw1ncgj4ytt1pqq5bh95iue.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  Dies ist natürlich ein sehr vereinfachtes Schema, das Protokoll ist viel komplizierter: Es gibt eine Synchronisation zwischen Clients innerhalb desselben Netzwerks, es gibt Kerneltreiber, die Fähigkeit, Kollisionen aufzulösen usw.  Dies ist ein ziemlich komplexes Protokoll, aber es funktioniert schematisch so. <br><img src="https://habrastorage.org/webt/ev/-x/_h/ev-x_hwfhwpfixld61yqiraelc4.jpeg"><br><br>  Wenn ein Client etwas auf Metaserver speichert, gehen alle Informationen an MySQL.  Blockserver speichert auch Informationen über Dateien, wie sie strukturiert sind, aus welchen Blöcken sie bestehen, in MySQL.  Blockserver speichert die Blöcke auch selbst im Blockspeicher, der wiederum Informationen darüber speichert, wo welcher Block liegt, auf welchem ​​Server und wie er verarbeitet wird, auch in MYSQL. <br><br><blockquote>  Um Exabyte an Benutzerdateien zu speichern, speichern wir gleichzeitig zusätzliche Informationen in einer Datenbank mit mehreren Dutzend Petabyte, die auf 6.000 Server verteilt sind. </blockquote><br><a name="history_development"></a><h2>  Datenbankentwicklungsverlauf </h2><br>  Wie haben sich die Datenbanken in Dropbox entwickelt? <br><img src="https://habrastorage.org/webt/oe/w9/ok/oew9okiqdeivyhvhycznly7kvbe.jpeg"><br><br>  Im Jahr 2008 begann alles mit einem Metaserver und einer globalen Datenbank.  Alle Informationen, die Dropbox irgendwo speichern musste, speicherte er in der einzigen globalen MySQL.  Dies dauerte nicht lange, da die Anzahl der Benutzer zunahm und einzelne Datenbanken und Tablets in den Datenbanken schneller anstiegen als andere. <br><img src="https://habrastorage.org/webt/ry/lt/h9/rylth906zfbbcou6nz7ve_yqib8.jpeg"><br><br>  Daher wurden 2011 mehrere Tabellen an separate Server gesendet: <br><br><ul><li>  <strong>Benutzer</strong> mit Informationen zu Benutzern, z. B. Anmeldungen und oAuth-Token; </li><li>  <strong>Host</strong> mit Dateiinformationen von Blockserver; </li><li>  <strong>Verschiedenes</strong> , das nicht an der Verarbeitung von Anforderungen aus der Produktion beteiligt war, sondern für Dienstprogrammfunktionen wie Stapeljobs verwendet wurde. </li></ul><br><img src="https://habrastorage.org/webt/ja/ec/ja/jaecja2eklv8znsqhf5lt-dyez8.jpeg"><br><br>  Aber nach 2012 begann Dropbox sehr stark zu wachsen, seitdem sind <strong>wir</strong> um <strong>etwa 100 Millionen Benutzer pro Jahr</strong> gewachsen. <br><img src="https://habrastorage.org/webt/ie/cr/-s/iecr-syyi6qprj2zj45qx4l42k4.jpeg"><br><br>  Es war notwendig, ein so großes Wachstum zu berücksichtigen, und deshalb hatten wir Ende 2011 Scherben - eine Basis bestehend aus 1.600 Scherben.  Anfangs nur 8 Server mit jeweils 200 Shards.  Jetzt sind es 400 Master-Server mit jeweils 4 Shards. <br><img src="https://habrastorage.org/webt/b3/v9/vy/b3v9vyjqzwau2kgmadhgb2vg0vo.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  Im Jahr 2012 haben wir festgestellt, dass das Erstellen und Aktualisieren von Tabellen in der Datenbank für jede hinzugefügte Geschäftslogik sehr schwierig, trostlos und problematisch ist.  Aus diesem <strong>Grund haben</strong> wir 2012 unseren eigenen <strong>Grafikspeicher</strong> erfunden, den wir <strong>Edgestore</strong> genannt haben. <strong>Seitdem</strong> werden alle von der Anwendung generierten Geschäftslogiken und Metainformationen in Edgestore gespeichert. <br><br>  Edgestore abstrahiert MySQL im Wesentlichen von Clients.  Clients haben bestimmte Entitäten, die durch Links von der gRPC-API zu Edgestore Core verbunden sind, wodurch diese Daten in MySQL konvertiert und irgendwie dort gespeichert werden (im Grunde gibt es all dies aus dem Cache). <br><img src="https://habrastorage.org/webt/bj/s7/dz/bjs7dz7-cdsvjblcgftjdeybrgk.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  <strong>2015 verließen wir Amazon S3</strong> und entwickelten unseren eigenen Cloud-Speicher namens Magic Pocket.  Es enthält Informationen darüber, wo sich eine Blockdatei befindet, auf welchem ​​Server, über die Bewegungen dieser Blöcke zwischen Servern, die in MySQL gespeichert sind. <br><img src="https://habrastorage.org/webt/f_/bz/lm/f_bzlm3tk9e3lwqo64x4kfuhhok.jpeg">  <em><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link von der Folie</a></em> <br><br>  MySQL wird jedoch auf sehr knifflige Weise verwendet - im Wesentlichen als große verteilte Hash-Tabelle.  Dies ist eine ganz andere Belastung, hauptsächlich beim Lesen von zufälligen Datensätzen.  90% der Auslastung ist I / O. <br><br><h2>  Datenbankarchitektur </h2><br>  Zunächst haben wir sofort einige Prinzipien identifiziert, nach denen wir die Architektur unserer Datenbank erstellen: <br><br><ol><li>  <strong>Zuverlässigkeit und Haltbarkeit</strong> .  Dies ist das wichtigste Prinzip und was Kunden von uns erwarten - Daten sollten nicht verloren gehen. </li><li>  <strong>Die Optimalität der Lösung</strong> ist ein ebenso wichtiges Prinzip.  Beispielsweise sollten Sicherungen schnell durchgeführt und auch schnell wiederhergestellt werden. </li><li>  <strong>Einfachheit der Lösung</strong> - sowohl architektonisch als auch in Bezug auf Service und Weiterentwicklungsunterstützung. </li><li>  <strong>Betriebskosten</strong> .  Wenn etwas die Lösung optimiert, aber sehr teuer ist, passt dies nicht zu uns.  Zum Beispiel ist ein Slave, der einen Tag hinter dem Master liegt, für Backups sehr praktisch, aber dann müssen Sie 1.000 weitere zu 6.000 Servern hinzufügen - die Betriebskosten eines solchen Slaves sind sehr hoch. </li></ol><br>  Alle Prinzipien müssen <strong>überprüfbar und messbar sein</strong> , dh sie müssen Metriken haben.  Wenn wir über die Betriebskosten sprechen, müssen wir berechnen, wie viele Server wir beispielsweise in Datenbanken haben, wie viele Server in Backups gehen und wie viel es letztendlich für Dropbox kostet.  Wenn wir eine neue Lösung auswählen, zählen wir alle Metriken und konzentrieren uns auf sie.  Bei der Auswahl einer Lösung orientieren wir uns voll und ganz an diesen Grundsätzen. <br><br><h2>  Basistopologie </h2><br>  Die Datenbank ist wie folgt aufgebaut: <br><br><ul><li>  Im Hauptdatenzentrum haben wir einen Master, in dem alle Datensätze vorkommen. </li><li>  Der Master-Server verfügt über zwei Slave-Server, auf denen eine semisynchrone Replikation erfolgt.  Server sterben oft ab (ungefähr 10 pro Woche), daher benötigen wir zwei Slave-Server. </li><li>  Slave-Server befinden sich in separaten Clustern.  Cluster sind vollständig getrennte Räume im Rechenzentrum, die nicht miteinander verbunden sind.  Wenn ein Raum ausbrennt, bleibt der zweite vollständig funktionsfähig. </li><li>  Auch in einem anderen Rechenzentrum haben wir den sogenannten Pseudo-Master (Intermediate Master), der eigentlich nur ein Slave ist, der einen anderen Slave hat. </li></ul><br><img src="https://habrastorage.org/webt/k6/6s/x6/k66sx6siyp6efjxxmfrot21ueha.jpeg"><br><br>  Eine solche Topologie wurde gewählt, weil, wenn das erste Rechenzentrum plötzlich in uns stirbt, wir im zweiten Rechenzentrum eine <strong>fast vollständige Topologie haben</strong> .  Wir ändern einfach alle Adressen in Discovery, und Kunden können arbeiten. <br><br><h3>  Spezialisierte Topologien </h3><br>  Wir haben auch spezialisierte Topologien. <br><br>  Die <strong>Magic Pocket-</strong> Topologie besteht aus einem Master-Server und zwei Slave-Servern.  Dies geschieht, weil Magic Pocket selbst Daten zwischen Zonen dupliziert.  Wenn ein Cluster verloren geht, können alle Daten aus anderen Zonen durch Löschcode wiederhergestellt werden. <br><img src="https://habrastorage.org/webt/gk/o7/bi/gko7bifb-4ted4cvwmzgyn5lasw.jpeg"><br><br>  Die <strong>Aktiv-Aktiv-</strong> Topologie ist die von Edgestore verwendete benutzerdefinierte Topologie.  Es hat einen Master und zwei Slaves in jedem der beiden Rechenzentren und sie sind Slaves für einander.  Dies ist ein sehr <strong>gefährliches Schema</strong> , aber Edgestore auf seiner Ebene weiß genau, welche Daten auf welchem ​​Master über welchen Bereich geschrieben werden können.  Daher wird diese Topologie nicht unterbrochen. <br><img src="https://habrastorage.org/webt/xe/nv/wx/xenvwx3ls9ct8fssverq3htck10.jpeg"><br><br><h3>  Instanz </h3><br>  Wir haben vor 4-5 Jahren ziemlich einfache Server mit einer Konfiguration von installiert: <br><br><ul><li>  <strong>2x Xeon 10 Kerne;</strong> </li><li>  <strong>5 TB (8 SSD Raid 0 *);</strong> </li><li>  <strong>384 GB Speicher.</strong> </li></ul><br>  * Raid 0 - weil es einfacher und viel schneller ist, einen gesamten Server zu ersetzen als Laufwerke. <br><br><h4>  Einzelne Instanz </h4><br>  Auf diesem Server haben wir eine große MySQL-Instanz, auf der sich mehrere Shards befinden.  Diese MySQL-Instanz ordnet sich sofort fast den gesamten Speicher zu.  Auf dem Server werden auch andere Prozesse ausgeführt: Proxy, Statistiksammlung, Protokolle usw. <br><br><img src="https://habrastorage.org/webt/z8/yd/vw/z8ydvwabte3v8pytwbrl1vi8yc0.jpeg"><br><br>  Diese Lösung ist insofern gut: <br><br>  + Es ist <strong>einfach zu verwalten</strong> .  Wenn Sie die MySQL-Instanz ersetzen müssen, ersetzen Sie einfach den Server. <br><br>  + <strong>Mach einfach Faylovers</strong> . <br><br>  Andererseits: <br><br>  - Es ist problematisch, dass Operationen auf der gesamten Instanz von MySQL und sofort auf allen Shards ausgeführt werden.  Wenn Sie beispielsweise eine Sicherungskopie erstellen müssen, sichern wir alle Shards gleichzeitig.  Wenn Sie einen Faylover machen müssen, machen wir einen Faylover aus allen vier Scherben gleichzeitig.  Dementsprechend leidet die Zugänglichkeit viermal mehr. <br><br>  - Probleme beim Replizieren eines Shards betreffen andere Shards.  Die MySQL-Replikation ist nicht parallel und alle Shards arbeiten in einem einzelnen Thread.  Wenn einem Splitter etwas passiert, werden auch die anderen Opfer. <br><br>  Jetzt wechseln wir zu einer anderen Topologie. <br><br><h4>  Multi Instanz </h4><br><img src="https://habrastorage.org/webt/lg/7x/ks/lg7xks5vbogjaf6slr7tidlc6ty.jpeg"><br><br>  In der neuen Version werden mehrere MySQL-Instanzen gleichzeitig auf dem Server mit jeweils einem Shard gestartet.  Was ist besser <br><br>  + Wir können <strong>Operationen nur an einem bestimmten Splitter ausführen</strong> .  Das heißt, wenn Sie einen Faylover benötigen, wechseln Sie nur einen Shard. Wenn Sie ein Backup benötigen, sichern wir nur einen Shard.  Dies bedeutet, dass die Vorgänge erheblich beschleunigt werden - viermal für einen Server mit vier Shards. <br><br>  + <strong>Scherben beeinflussen sich kaum gegenseitig</strong> . <br><br>  + <strong>Verbesserung der Replikation.</strong>  Wir können verschiedene Kategorien und Klassen von Datenbanken mischen.  Edgestore nimmt viel Platz ein, zum Beispiel alle 4 TB, und Magic Pocket nimmt nur 1 TB ein, ist aber zu 90% ausgelastet.  Das heißt, wir können verschiedene Kategorien kombinieren, die E / A- und Maschinenressourcen auf unterschiedliche Weise verwenden, und 4 Replikationsströme starten. <br><br>  Natürlich hat diese Lösung ihre Nachteile: <br><br>  - Das größte Minus ist, dass es <strong>viel schwieriger ist, all dies zu verwalten</strong> .  Wir brauchen einen cleveren Planer, der versteht, wo er diese Instanz aufnehmen kann, wo es eine optimale Last gibt. <br><br>  - <strong>Härter als die Failover</strong> . <br><br>  Deshalb kommen wir erst jetzt zu dieser Entscheidung. <br><br><h3>  Entdeckung </h3><br>  Clients müssen irgendwie wissen, wie sie sich mit der gewünschten Datenbank verbinden können, also haben wir Discovery, das: <br><br><ol><li>  Benachrichtigen Sie den Client sehr schnell über Topologieänderungen.  Wenn wir Master und Slave wechseln, sollten Kunden fast sofort davon erfahren. <br></li><li>  Die Topologie sollte nicht von der MySQL-Replikationstopologie abhängen, da wir bei einigen Operationen die MySQL-Topologie ändern.  Wenn wir beispielsweise im vorbereitenden Schritt auf dem Zielmaster, in dem wir einen Teil der Shards übertragen, aufteilen, werden einige der Slave-Server auf diesen Zielmaster neu konfiguriert.  Kunden müssen dies nicht wissen. <br></li><li>  Es ist wichtig, dass die Operationen atomar sind und der Zustand überprüft wird.  Es ist unmöglich, dass zwei verschiedene Server derselben Datenbank gleichzeitig Master werden. <br></li></ol><br><h4>  Wie sich Discovery entwickelt hat </h4><br>  Anfangs war alles einfach: die Datenbankadresse im Quellcode in der Konfiguration.  Wenn wir die Adresse aktualisieren mussten, wurde alles nur sehr schnell bereitgestellt. <br><img src="https://habrastorage.org/webt/7d/yb/oo/7dyboo0h7eo4o9_9xp-n-6lzosy.jpeg"><br><br>  Leider funktioniert dies nicht, wenn viele Server vorhanden sind. <br><img src="https://habrastorage.org/webt/3u/26/pf/3u26pfl_s796zdaoix-zdplb9du.jpeg"><br><br>  Oben ist die allererste Entdeckung, die wir haben.  Es gab Datenbankskripte, die das Typenschild in ConfigDB änderten - es war ein separates MySQL-Typenschild, und Clients haben diese Datenbank bereits abgehört und regelmäßig Daten von dort abgerufen. <br><img src="https://habrastorage.org/webt/ml/qn/mh/mlqnmhmmteylazgl4itjokes_mu.jpeg"><br><br>  Die Tabelle ist sehr einfach, es gibt eine Datenbankkategorie, einen Shard-Schlüssel, einen Datenbankklassen-Master / Slave, einen Proxy und eine Datenbankadresse.  Tatsächlich forderte der Client eine Kategorie, eine DB-Klasse, einen Shard-Schlüssel an, und die MySQL-Adresse wurde zurückgegeben, zu der er bereits eine Verbindung herstellen konnte. <br><img src="https://habrastorage.org/webt/vb/ht/en/vbhteniyw4x7s56a1xwckuz268e.jpeg"><br><br>  Sobald es viele Server gab, wurde Memcache hinzugefügt und die Clients begannen bereits mit ihm zu kommunizieren. <br><br>  Aber dann haben wir es überarbeitet.  MySQL-Skripte begannen über gRPC zu kommunizieren, über einen Thin Client mit einem Dienst namens RegisterService.  Als einige Änderungen auftraten, hatte RegisterService eine Warteschlange, und er verstand, wie diese Änderungen angewendet werden.  RegisterService hat Daten in AFS gespeichert.  AFS ist unser internes System, das auf ZooKeeper basiert. <br><img src="https://habrastorage.org/webt/mi/lm/yz/milmyzvyvayuv2av8pah4neh9zq.jpeg"><br><br>  Die zweite Lösung, die hier nicht gezeigt wird, verwendete ZooKeeper direkt, und dies verursachte Probleme, da jeder Shard ein Knoten in ZooKeeper war.  Zum Beispiel stellen 100.000 Clients eine Verbindung zu ZooKeeper her. Wenn sie plötzlich aufgrund eines Fehlers gestorben sind, werden sofort 100.000 Anfragen an ZooKeeper gesendet, die einfach gelöscht werden und nicht mehr steigen können. <br><br>  Daher wurde <strong>das AFS-System</strong> entwickelt <strong>, das von der gesamten Dropbox verwendet wird</strong> .  Tatsächlich wird die Arbeit mit ZooKeeper für alle Kunden abstrahiert.  Der AFS-Dämon wird lokal auf jedem Server ausgeführt und bietet eine sehr einfache Datei-API des Formulars: Erstellen Sie eine Datei, löschen Sie eine Datei, fordern Sie eine Datei an, erhalten Sie eine Benachrichtigung über eine Dateiänderung und vergleichen und tauschen Sie Vorgänge aus.  Das heißt, Sie können versuchen, die Datei durch eine Version zu ersetzen. Wenn sich diese Version während der Änderung geändert hat, wird der Vorgang abgebrochen. <br><br>  Im Wesentlichen eine solche Abstraktion über ZooKeeper, in der es einen lokalen Backoff- und Jitter-Algorithmus gibt.  ZooKeeper stürzt unter Last nicht mehr ab.  Mit AFS erstellen wir Backups in S3 und GIT. Anschließend benachrichtigt der lokale AFS die Clients selbst, dass sich die Daten geändert haben. <br><img src="https://habrastorage.org/webt/ry/_0/wv/ry_0wvkyux23gicrlwmfceq0eoe.jpeg"><br><br>  In AFS werden Daten als Dateien gespeichert, dh es handelt sich um eine Dateisystem-API.  Das Obige ist beispielsweise die Datei shard.slave_proxy - die größte Datei benötigt ungefähr 28 KB. Wenn wir die Kategorie der Klasse shard und slave_proxy ändern, erhalten alle Clients, die diese Datei abonnieren, eine Benachrichtigung.  Sie lesen diese Datei erneut, die alle erforderlichen Informationen enthält.  Mithilfe des Shard-Schlüssels erhalten sie eine Kategorie und konfigurieren den Verbindungspool zur Datenbank neu. <br><br><a name="perations_databases"></a><h2>  Operationen </h2><br>  Wir verwenden sehr einfache Vorgänge: Heraufstufen, Klonen, Sichern / Wiederherstellen. <br><img src="https://habrastorage.org/webt/q3/5c/df/q35cdfiso51bhhiitqja71qbysc.jpeg"><br><br>  <strong>Eine Operation ist eine einfache Zustandsmaschine</strong> .  Wenn wir in die Operation gehen, führen wir einige Überprüfungen durch, zum Beispiel eine Schleuderprüfung, die mehrmals anhand des Zeitlimits überprüft, ob wir diese Operation ausführen können.  Danach führen wir einige vorbereitende Maßnahmen durch, die keine Auswirkungen auf externe Systeme haben.  Als nächstes die Operation selbst. <br><br>  Alle Schritte innerhalb einer Operation haben einen <strong>Rollback-Schritt</strong> (Rückgängig).  Wenn bei der Operation ein Problem auftritt, versucht die Operation, das System an seiner ursprünglichen Position wiederherzustellen.  Wenn alles in Ordnung ist, erfolgt die Bereinigung und der Vorgang ist abgeschlossen. <br><br>  Wir haben eine so einfache Zustandsmaschine für jede Operation. <br><br><h4>  <strong>Beförderung (Masterwechsel)</strong> </h4><br>  Dies ist eine sehr häufige Operation in der Datenbank.  Es gab Fragen, wie man Änderungen an einem funktionierenden Hot-Master-Server vornimmt - es wird einen Einsatz bekommen.  Es ist nur so, dass alle diese Vorgänge auf Slave-Servern ausgeführt werden und dann Slave-Änderungen mit Master-Plätzen vorgenommen werden.  Daher ist die <strong>Werbemaßnahme sehr häufig</strong> . <br><img src="https://habrastorage.org/webt/xx/79/jv/xx79jvszxb9wjffwqf4ld_euofo.jpeg"><br><br>  Wir müssen den Kernel aktualisieren - wir tauschen, wir müssen die Version von MySQL aktualisieren - wir aktualisieren auf Slave, wechseln zu Master, aktualisieren dort. <br><img src="https://habrastorage.org/webt/wc/op/o9/wcopo9jlmz9aeoynpv-hbbseois.jpeg"><br><br>  Wir haben eine sehr schnelle Beförderung erreicht.  Zum Beispiel <strong>haben wir für vier Scherben jetzt eine Beförderung für ungefähr 10-15 s.</strong>  Die obige Grafik zeigt, dass die Verfügbarkeit von Werbeaktionen um 0,0003% gelitten hat. <br><br>  Aber normale Werbung ist nicht so interessant, weil dies gewöhnliche Operationen sind, die jeden Tag durchgeführt werden.  Failover sind interessant. <br><br><h4>  <strong>Failover (Ersatz für einen defekten Master)</strong> <br></h4><br>  Ein Failover bedeutet, dass die Datenbank tot ist. <br><br><ul><li>  Wenn der Server wirklich gestorben ist, ist dies nur ein idealer Fall. </li><li>  In der Tat kommt es vor, dass die Server teilweise am Leben sind. </li><li>  Manchmal stirbt der Server sehr langsam.  Die RAID-Controller, das Festplattensystem fallen aus, einige Anforderungen geben Antworten zurück, aber einige Flows sind blockiert und geben keine Antworten zurück. </li><li>  Es kommt vor, dass der Master einfach überlastet ist und nicht auf unseren Gesundheitscheck reagiert.  Aber wenn wir befördert werden, wird auch der neue Meister überlastet und es wird nur noch schlimmer. </li></ul><br>  Der Austausch verstorbener Master-Server erfolgt ca. <strong>2-3 mal am Tag</strong> . Dies ist ein vollautomatischer Prozess, bei dem kein menschliches Eingreifen erforderlich ist.  Der kritische Abschnitt dauert ungefähr 30 Sekunden und es gibt eine Reihe zusätzlicher Überprüfungen, um festzustellen, ob der Server tatsächlich lebt oder möglicherweise bereits gestorben ist. <br><br>  Unten sehen Sie ein Beispieldiagramm, wie der Faylover funktioniert. <br><img src="https://habrastorage.org/webt/ks/5d/6o/ks5d6oovtnchnmlr2zlgpwvmt5g.jpeg"><br><br>  Im ausgewählten Abschnitt <strong>starten</strong> wir <strong>den Master-Server neu</strong> .  Dies ist notwendig, da wir MySQL 5.6 haben und die semisynchrone Replikation nicht verlustfrei ist.  Daher sind Phantom-Lesevorgänge möglich, und wir brauchen diesen Master, auch wenn er nicht gestorben ist, so schnell wie möglich zu töten, damit Clients die Verbindung dazu trennen.  Daher führen wir einen Hard-Reset über Ipmi durch - dies ist die erste wichtige Operation, die wir ausführen müssen.  In der MySQL 5.7-Version ist dies nicht so kritisch. <br><br>  <strong>Clustersynchronisation.</strong>  Warum brauchen wir eine Clustersynchronisation? <br><img src="https://habrastorage.org/webt/gh/pa/go/ghpago_p12c1jittnig4rwhc1sg.jpeg"><br><br>  Wenn wir uns mit unserer Topologie an das vorherige Bild erinnern, hat ein Master-Server drei Slave-Server: zwei in einem Rechenzentrum, einer im anderen.  Bei der Beförderung muss sich der Master im selben Hauptdatenzentrum befinden.  Aber manchmal, wenn Slaves mit Semisync geladen werden, kommt es vor, dass ein Semisync-Slave ein Slave in einem anderen Rechenzentrum wird, weil er nicht geladen wird.  Daher müssen wir zuerst den gesamten Cluster synchronisieren und dann bereits eine Promotion auf dem Slave in dem von uns benötigten Rechenzentrum durchführen.  Dies geschieht ganz einfach: <br><br><ul><li>  Wir stoppen alle E / A-Threads auf allen Slave-Servern. </li><li>  Danach wissen wir bereits mit Sicherheit, dass der Master "schreibgeschützt" ist, da die Semisync-Verbindung getrennt wurde und niemand anderes dort etwas schreiben kann. </li><li>  Als nächstes wählen wir den Slave mit dem größten abgerufenen / ausgeführten GTID-Satz aus, dh mit der größten Transaktion, die er entweder heruntergeladen oder bereits angewendet hat. </li><li>  Wir konfigurieren alle Slave-Server für diesen ausgewählten Slave neu, starten den E / A-Thread und sie werden synchronisiert. </li><li>  Wir warten, bis sie synchronisiert sind. Danach wird der gesamte Cluster synchronisiert.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Am Ende überprüfen wir, ob der GTID-Satz überall, wo wir ihn ausgeführt haben, auf dieselbe Position gesetzt ist. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Die zweite wichtige Operation ist die </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Clustersynchronisation</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Die weitere </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Werbung</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> beginnt </font><font style="vertical-align: inherit;">wie folgt:</font></font><br><img src="https://habrastorage.org/webt/bd/s-/b8/bds-b8fbieqxhtc4dy9ookntiri.jpeg"><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir wählen jeden Slave im Rechenzentrum aus, den wir benötigen, sagen ihm, dass er Master ist, und starten den Standard-Promotion-Vorgang. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wir konfigurieren alle Slave-Server für diesen Master neu, stoppen dort die Replikation, wenden ACLs an, fahren Benutzer ein, stoppen einige Proxys und starten möglicherweise etwas neu. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Am Ende machen wir read_only = 0, das heißt, wir sagen, dass wir jetzt in den Master schreiben und die Topologie aktualisieren können. </font><font style="vertical-align: inherit;">Von nun an gehen Kunden zu diesem Master und alles funktioniert für sie.</font></font></li><li>       - .     -    ,  ,   ,    , ,  proxy  . </li><li>     . </li></ul><br>   ,       rollback   ,   .       rollback  reboot.   ,    , ,  — change master —    master   . <br><br><h4> <strong></strong> </h4><br>  —      .   ,    ,   ,    ,    . <br><br> <strong> </strong> <br><br> ●   slave <br><br>   ,       slave-,      .   . <br><br> ●       <br><br>     ,     ,      .             . <br><br> ●       <br><br>  ,     ,      .          .      3  . <br><br><blockquote>    ,   ,   ,     : <br><br><ol><li>      .       1  40 . <br></li><li>            . <br></li></ol></blockquote><br>    ,     .   1   40 ,      ,      ,     . <br><br><h4> <strong></strong> </h4><br>    ,  .           .     4  . <br><img src="https://habrastorage.org/webt/bv/-k/_z/bv-k_znrl7zi2obmotthihjogyo.jpeg"><br><br><ul><li>    <strong> 24 </strong> .         HDFS,      . </li><li> <strong> 6 </strong>     unsharded databases,        Global DB.      , ,  ,     . </li><li> <strong> 3 </strong>          S3. </li><li> <strong> 3 </strong>     S3     . </li></ul><br><img src="https://habrastorage.org/webt/e1/yx/3s/e1yx3s-1ikyhxnuzeympjsvem14.jpeg"><br><br>       . ,    3 ,   HDFS     3 ,   6   S3.     . <br><br>  ,   . <br><img src="https://habrastorage.org/webt/sn/hz/1l/snhz1lmio2naq40wziys-jggaaw.jpeg"><br><br>         ,      ,   .       ,   ,    recovery  -   .  ,      ,  -       .      100  ,   . <br><br>     ,    ,    ,    ,   ,     ,  ,     .        . <br><br><h5>   </h5><br><img src="https://habrastorage.org/webt/4m/4b/kb/4m4bkboro5zunrljkwxbybj7jsi.jpeg"><br><br>     hot-,      Percona xtrabackup.     —stream=xbstream,        ,   .     script-splitter,        ,      . <br><br> MySQL              2x.     3 , ,   ,    1 500 .     ,      ,    HDFS   S3. <br><br>        . <br><img src="https://habrastorage.org/webt/j3/il/jm/j3iljma8c0rekqweak5ngqvaxkk.jpeg"><br><br>  ,    ,    HDFS   S3,    , splitter       xtrabackup,      .   crash-recovery. <br><br>      hot   ,  crash-recovery    .         ,    .     binlog,      master. <br><br> <strong>   binlogs?</strong> <br><br>     binlog'.    master ,    4 ,   100 ,    HDFS. <br><br>      :   Binlog Backuper,         . ,  ,   binlog       HDFS. <br><img src="https://habrastorage.org/webt/on/o3/ce/ono3cesuissuuwuzcglcfautfjo.jpeg"><br><br> ,       4   ,    5 ,    ,    ,    .    HDFS   S3    . <br><br><h5>   </h5><br>      . <br><br>   : <br><br><ol><li>        —  10 ,  45  —   . <br></li><li>      ,       scheduler  multi instance      slave  master    . <br></li><li>    —      ,   .  ,     ,    ,    ,     ,  ,    .  pt-table-checksum   ,      . <br></li></ol><br> <strong></strong> ,        : <br><br><ol><li>       1  10 ,      .    crash-recovery,     . <br></li><li>            . <br></li></ol><br><img src="https://habrastorage.org/webt/2m/bd/ar/2mbdarekbku4hsxygdufshzyhnm.jpeg"><br><br>     slave   -,     .    ,      .  Alles ist sehr einfach. <br><br><h4>  ++ </h4><br>     .       Hardware ,          (HDD)  10 ,       + crash recovery xtrabackup,      . ,         ,    . , ,     ,   ,   HDD  ,    HDFS  . <br><br><h4>  </h4><br>    ,  —   : <br><br><ol><li>         ; <br></li><li>       . <br></li></ol><br>  ,     HDFS,       ,   ,       . <br><br><a name="automation"></a><h2>  Automatisierung </h2><br>  ,  6 000      .         ,   ,     — : <br><br><ul><li> Auto-replace; </li><li> DBManager; </li><li> Naoru, Wheelhouse </li></ul><br><h3> Auto-replace </h3><br>   ,   ,   ,    ,     — ,     -.   ,   . <br><br> <strong>Availability ()</strong> —         ,         .      —   recovery  ,         . <br><img src="https://habrastorage.org/webt/-k/em/tr/-kemtrpvhgocinlvlmnsuj1eq-s.jpeg"><br><br>    MySQL  ,   heartbeat. Heartbeat —   timestamp. <br><img src="https://habrastorage.org/webt/cn/r8/fn/cnr8fn-fpy_ddnthddjem3afr4o.jpeg"><br><br>    ,     , ,  master   read-write.          heartbeat. <br><br>    auto-replace ,    . <br><img src="https://habrastorage.org/webt/3u/r9/sx/3ur9sxfxf8dsxtgsgvjz3ublvye.jpeg"> <em>           <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,   91 .</em> <br><br> <strong>  ?</strong> <br><br><ul><li>   ,     heartbeat    . ,     .  heartbeat', ,    heartbeat'  30 . </li><li> Überprüfen Sie als Nächstes, ob ihre Anzahl den Schwellenwert erfüllt.  Wenn nicht, stimmt etwas mit dem Server nicht - da er keinen Herzschlag gesendet hat. </li><li>  Danach führen wir für alle Fälle eine umgekehrte Überprüfung durch - plötzlich sind diese beiden Dienste ausgefallen, etwas befindet sich im Netzwerk oder die globale Datenbank kann den Heartbeat aus irgendeinem Grund nicht schreiben.  Bei der umgekehrten Überprüfung stellen wir eine Verbindung zu einer defekten Datenbank her und überprüfen deren Status. </li><li>  Wenn alles andere fehlschlägt, prüfen wir, ob die Master-Position voranschreitet oder nicht, ob Datensätze darauf vorhanden sind.  Wenn nichts passiert, funktioniert dieser Server definitiv nicht. </li><li>  Der letzte Schritt ist das automatische Ersetzen. </li></ul><br>  Das automatische Ersetzen ist sehr konservativ. Er möchte nie viele automatische Operationen ausführen. <br><br><ol><li>  Zuerst prüfen wir, ob in letzter Zeit Topologieoperationen stattgefunden haben.  Möglicherweise wurde dieser Server gerade hinzugefügt und etwas darauf läuft noch nicht. </li><li>  Wir überprüfen, ob zu irgendeinem Zeitpunkt Ersatz im selben Cluster vorhanden war. </li><li>  Überprüfen Sie, welche Fehlergrenze wir haben.  Wenn wir viele Probleme gleichzeitig haben - 10, 20 -, werden wir nicht alle automatisch lösen, da wir versehentlich den Betrieb aller Datenbanken stören können. </li></ol><br>  Daher <strong>lösen</strong> wir jeweils <strong>nur ein Problem</strong> . <br><br>  Dementsprechend beginnen wir für den Slave-Server mit dem Klonen und entfernen ihn einfach aus der Topologie. Wenn es sich um einen Master handelt, starten wir den Feylover, die sogenannte Notfall-Promotion. <br><br><h3>  DBManager </h3><br>  DBManager ist ein Dienst zur Verwaltung unserer Datenbanken.  Es hat: <br><br><ul><li>  Smart Task Scheduler, der genau weiß, wann der Job zu starten ist; </li><li>  Protokolle und alle Informationen: Wer, wann und was gestartet - das ist die Quelle der Wahrheit; </li><li>  Synchronisationspunkt. </li></ul><br><img src="https://habrastorage.org/webt/xx/g6/pi/xxg6pitu-pau9ifyqivpa9wul3e.jpeg"><br><br>  DBManager ist architektonisch recht einfach. <br><br><ul><li>  Es gibt Clients, entweder DBAs, die etwas über die Weboberfläche tun, oder Skripte / Dienste, die DBAs geschrieben haben, die über gRPC zugreifen. </li><li>  Es gibt externe Systeme wie Wheelhouse und Naoru, die über gRPC an DBManager gehen. </li><li>  Es gibt einen Planer, der versteht, welche Operation wann und wo er beginnen kann. </li><li>  Es gibt einen sehr dummen Arbeiter, der, wenn eine Operation zu ihm kommt, sie startet und per PID prüft.  Worker kann neu starten, Prozesse werden nicht unterbrochen.  Alle Mitarbeiter befinden sich so nah wie möglich an den Servern, auf denen die Vorgänge stattfinden, sodass wir beispielsweise beim Aktualisieren von ACLS nicht viele Roundtrips durchführen müssen. </li><li>  Auf jedem SQL-Host haben wir einen DBAgent - dies ist ein RPC-Server.  Wenn Sie eine Operation auf dem Server ausführen müssen, senden wir eine RPC-Anfrage. </li></ul><br>  Wir haben eine Weboberfläche für DBManager, über die Sie die aktuell ausgeführten Aufgaben, Protokolle für diese Aufgaben, wer sie wann gestartet hat, welche Vorgänge für den Server einer bestimmten Datenbank usw. ausgeführt wurden, anzeigen können. <br><img src="https://habrastorage.org/webt/yj/tq/3m/yjtq3mrfimptba1kizre2bwie48.jpeg"><br><br>  Es gibt eine ziemlich einfache CLI-Oberfläche, über die Sie Aufgaben ausführen und in praktischen Ansichten anzeigen können. <br><img src="https://habrastorage.org/webt/qi/rc/vp/qircvpuoutswvmcpnuca4wem9cu.jpeg"><br><br><h3>  Abhilfemaßnahmen </h3><br>  Wir haben auch ein System, um auf Probleme zu reagieren.  Wenn beispielsweise etwas kaputt geht, das Laufwerk abstürzt oder ein Dienst nicht funktioniert, funktioniert <strong>Naoru.</strong>  Dies ist das System, das in Dropbox funktioniert, von jedem verwendet wird und speziell für solche kleinen Aufgaben entwickelt wurde.  Ich habe in meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bericht</a> 2016 über Naoru gesprochen. <br><br>  <strong>Wheelhouse</strong> basiert auf einer <strong>Zustandsmaschine</strong> und ist für lange Prozesse ausgelegt.  Zum Beispiel müssen wir den Kernel auf allen MySQL auf unserem gesamten Cluster von 6.000 Computern aktualisieren.  Wheelhouse macht dies klar - Updates auf dem Slave-Server, startet die Promotion, Slave wird Master, Updates auf dem Master-Server.  Dieser Vorgang kann ein oder sogar zwei Monate dauern. <br><br><a name="monitoring"></a><h2>  Überwachung </h2><br><img src="https://habrastorage.org/webt/vo/n_/ys/von_ysxtakb7m5ctiadwzewi3dw.jpeg"><br><br>  Es ist sehr wichtig. <br><br><blockquote>  Wenn Sie das System nicht überwachen, funktioniert es höchstwahrscheinlich nicht. </blockquote><br>  Wir überwachen alles in MySQL - alle Informationen, die wir von MySQL erhalten können, werden irgendwo gespeichert, wir können rechtzeitig darauf zugreifen.  Wir speichern Informationen zu InnoDb, Statistiken zu Anfragen, zu Transaktionen, zur Länge von Transaktionen, Perzentile zu Transaktionslängen, zur Replikation, im Netzwerk - alles in allem - eine große Anzahl von Metriken. <br><br><h3>  Alarm </h3><br>  Wir haben 992 Warnungen konfiguriert.  Tatsächlich befasst sich niemand mit Metriken. Es scheint mir, dass es keine Leute gibt, die zur Arbeit kommen und anfangen, sich das Metrikdiagramm anzusehen. Es gibt interessantere Aufgaben. <br><img src="https://habrastorage.org/webt/q3/nj/sl/q3njslzahbxmh585uzeatrn4plm.jpeg"><br><br>  Daher gibt es Warnungen, die funktionieren, wenn bestimmte Schwellenwerte erreicht werden.  <strong>Wir haben 992 Warnungen, egal was passiert, wir werden es herausfinden</strong> . <br><br><h3>  Vorfälle </h3><br><img src="https://habrastorage.org/webt/bi/ce/tp/bicetpzgstf2lggazas6t27ru10.jpeg"><br><br>  Wir haben PagerDuty - einen Dienst, über den Benachrichtigungen an verantwortliche Personen gesendet werden, die Maßnahmen ergreifen. <br><img src="https://habrastorage.org/webt/6n/hl/sr/6nhlsrj28tloxq4xg5ld3a-mpvy.jpeg"><br><br>  In diesem Fall ist ein Fehler bei der Notfallwerbung aufgetreten, und unmittelbar danach wurde eine Warnung aufgezeichnet, dass der Master gefallen ist.  Danach überprüfte der Dienstoffizier, was eine Notfallförderung verhinderte, und führte die erforderlichen manuellen Operationen durch. <br><br>  Wir werden sicherlich jeden aufgetretenen Vorfall analysieren, für jeden Vorfall haben wir eine Aufgabe im Task-Tracker.  Selbst wenn dieser Vorfall ein Problem in unseren Warnungen darstellt, erstellen wir auch eine Aufgabe. Wenn das Problem in der Warnungslogik und den Schwellenwerten liegt, müssen diese geändert werden.  Warnungen sollten nicht nur das Leben der Menschen verderben.  Ein Alarm ist immer schmerzhaft, besonders um 4 Uhr morgens. <br><br><a name="testing"></a><h2>  Testen </h2><br>  Wie bei der Überwachung bin ich sicher, dass jeder testet.  Zusätzlich zu den Komponententests, mit denen wir unseren Code abdecken, haben wir Integrationstests, in denen wir testen: <br><br><ul><li>  alle Topologien, die wir haben; </li><li>  alle Operationen auf diesen Topologien. </li></ul><br>  Wenn wir Beförderungsvorgänge haben, testen wir Beförderungsvorgänge im Integrationstest.  Wenn wir klonen, klonen wir für alle Topologien, die wir haben. <br><br>  <strong>Topologiebeispiel</strong> <br><img src="https://habrastorage.org/webt/dv/hh/va/dvhhvacdschtqr7s_ynecm0lyk0.jpeg"><br><br>  Wir haben Topologien für alle Gelegenheiten: 2 Rechenzentren mit mehreren Instanzen, mit Shards, keine Shards, mit Clustern, ein Rechenzentrum - im Allgemeinen fast jede Topologie - auch solche, die wir nicht verwenden, nur um zu sehen. <br><img src="https://habrastorage.org/webt/f-/oy/pl/f-oypluoyzosnphjl_aukp4vsks.jpeg"><br><br>  In dieser Datei haben wir nur die Einstellungen, welche Server und mit was wir erhöhen müssen.  Zum Beispiel müssen wir den Master erhöhen, und wir sagen, dass wir dies mit solchen und solchen Instanzdaten tun müssen, mit solchen und solchen Datenbanken an solchen und solchen Ports.  Fast alles passt zusammen mit Bazel, das auf der Grundlage dieser Dateien eine Topologie erstellt, den MySQL-Server startet und dann den Test startet. <br><img src="https://habrastorage.org/webt/bg/zd/b_/bgzdb_dwnggh3kf8uv9f7mn9jpe.jpeg"><br><br>  Der Test sieht sehr einfach aus: Wir geben an, welche Topologie verwendet wird.  In diesem Test testen wir auto_replace. <br><br><ul><li>  Wir erstellen den auto_replace-Dienst und starten ihn. </li><li>  Wir töten den Meister in unserer Topologie, warten eine Weile und sehen, dass der Zielsklave Meister geworden ist.  Wenn nicht, ist der Test fehlgeschlagen. </li></ul><br><h3>  Stufen </h3><br>  Bühnenumgebungen sind dieselben Datenbanken wie in der Produktion, es gibt jedoch keinen Benutzerverkehr, aber synthetischen Verkehr, der der Produktion über Percona Playback, Sysbench und ähnliche Systeme ähnelt. <br><br>  In Percona Playback zeichnen wir Datenverkehr auf, verlieren ihn dann in der Bühnenumgebung mit unterschiedlicher Intensität und können 2-3 mal schneller verlieren.  Das heißt, es ist künstlich, aber sehr nahe an der tatsächlichen Last. <br><br>  Dies ist notwendig, da wir in Integrationstests unsere Produktion nicht testen können.  Wir können die Warnung oder die Tatsache, dass Metriken funktionieren, nicht testen.  In der Testphase testen wir Warnungen, Metriken und Vorgänge, beenden die Server regelmäßig und stellen sicher, dass sie normal erfasst werden. <br><br>  Außerdem testen wir die gesamte Automatisierung zusammen, da bei Integrationstests höchstwahrscheinlich ein Teil des Systems getestet wird und beim Staging alle automatisierten Systeme gleichzeitig funktionieren.  Manchmal denken Sie, dass sich das System so und nicht anders verhält, aber es kann sich ganz anders verhalten. <br><br><h3>  DRT (Disaster Recovery Testing) </h3><br>  Wir führen auch Tests in der Produktion durch - direkt auf realen Grundlagen.  Dies wird als Disaster Recovery-Test bezeichnet.  Warum brauchen wir das? <br><br>  ● Wir möchten unsere Garantien testen. <br><br>  Dies wird von vielen großen Unternehmen durchgeführt.  Zum Beispiel hat Google einen Dienst, der so stabil funktioniert - 100% der Zeit -, dass alle Dienste, die ihn verwendet haben, entschieden haben, dass dieser Dienst wirklich 100% stabil ist und niemals abstürzt.  Daher musste Google diesen Dienst absichtlich einstellen, damit die Nutzer diese Möglichkeit berücksichtigen. <br><br>  Wir sind also - wir haben die Garantie, dass MySQL funktioniert - und manchmal funktioniert es nicht!  Und wir haben die Garantie, dass es für einen bestimmten Zeitraum möglicherweise nicht funktioniert. Kunden sollten dies berücksichtigen.  Von Zeit zu Zeit töten wir den Produktionsmaster, oder wenn wir einen Faylover machen wollen, töten wir alle Slaves, um zu sehen, wie sich die semisynchrone Replikation verhält. <br><br>  ● Kunden sind auf diese Fehler vorbereitet (Austausch und Tod des Masters) <br><br>  Warum ist das gut?  Wir hatten einen Fall, in dem während der Promotion 4 von 1600 Shards die Verfügbarkeit auf 20% sank.  Es scheint, dass etwas nicht stimmt, für 4 Scherben ab 1600 sollte es einige andere Zahlen geben.  Failover für dieses System waren selten, etwa einmal im Monat, und alle entschieden: "Nun, es ist ein Failover, es passiert." <br><br>  Irgendwann, als wir auf ein neues System umgestiegen sind, hat eine Person beschlossen, diese beiden Heartbeat-Aufzeichnungsdienste zu optimieren und zu einem zu kombinieren.  Dieser Dienst hat etwas anderes getan und ist am Ende gestorben und der Herzschlag hat aufgehört aufzuzeichnen.  So kam es, dass wir für diesen Kunden 8 Faylovers pro Tag hatten.  Alles lag - 20% Verfügbarkeit. <br><br>  Es stellte sich heraus, dass bei diesem Client die Lebensdauer 6 Stunden beträgt.  Dementsprechend hatten wir, sobald der Meister starb, alle Verbindungen für weitere 6 Stunden gehalten.  Der Pool konnte nicht weiter funktionieren - seine Verbindungen bleiben erhalten, er ist begrenzt und funktioniert nicht.  Es wurde repariert. <br><br>  Wir machen den Feylover wieder - nicht mehr 20%, aber immer noch viel.  Irgendwas stimmt immer noch nicht.  Es stellte sich heraus, dass ein Fehler in der Implementierung des Pools.  Auf Anfrage wandte sich der Pool vielen Scherben zu und verband dies alles.  Wenn einige Scherben fieberhaft waren, trat im Go-Code eine Rennbedingung auf, und der gesamte Pool war verstopft.  Alle diese Scherben konnten nicht mehr funktionieren. <br><br>  Disaster Recovery-Tests sind sehr nützlich, da Clients auf diese Fehler vorbereitet sein müssen und ihren Code überprüfen müssen. <br><br>  ● Plus Disaster Recovery-Tests sind gut, da sie während der Geschäftszeiten stattfinden und alles vorhanden ist, weniger Stress, die Leute wissen, was jetzt passieren wird.  Das passiert nachts nicht und es ist großartig. <br><br><h2>  Fazit </h2><br>  1. Alles muss automatisiert werden, niemals in die Hände bekommen. <br>  Jedes Mal, wenn jemand mit unseren Händen in das System steigt, stirbt und bricht alles in unserem System - jedes Mal!  - auch bei einfachen Operationen.  Zum Beispiel starb ein Sklave, eine Person musste einen zweiten hinzufügen, entschied sich jedoch, den toten Sklaven mit seinen Händen aus der Topologie zu entfernen.  Anstelle des Verstorbenen kopierte er jedoch das Kommando live - der Meister blieb überhaupt ohne Sklaven.  Solche Operationen sollten nicht manuell durchgeführt werden. <br><br>  2. Die Tests sollten kontinuierlich und automatisiert (und in der Produktion) sein. <br>  Ihr System ändert sich, Ihre Infrastruktur ändert sich.  Wenn Sie einmal nachgesehen haben und es zu funktionieren schien, bedeutet dies nicht, dass es morgen funktionieren wird.  Daher müssen Sie jeden Tag, auch in der Produktion, ständig automatisierte Tests durchführen. <br><br>  3. Stellen Sie sicher, dass Sie Clients (Bibliotheken) besitzen. <br>  Benutzer wissen möglicherweise nicht, wie Datenbanken funktionieren.  Sie verstehen möglicherweise nicht, warum Zeitüberschreitungen erforderlich sind, um am Leben zu bleiben.  Daher ist es besser, diese Kunden zu besitzen - Sie werden ruhiger sein. <br><br>  4. Es ist notwendig, Ihre Grundsätze für den Aufbau des Systems und Ihre Garantien festzulegen und diese stets einzuhalten. <br><br>  Somit können Sie 6.000 Datenbankserver unterstützen. <br><br><div class="spoiler">  <b class="spoiler_title">Bei Fragen nach dem Bericht und insbesondere bei den Antworten darauf gibt es auch viele nützliche Informationen.</b> <div class="spoiler_text"><h2>  Fragen und Antworten <br></h2><br><blockquote>  - Was passiert, wenn die Last der Shards unausgewogen ist - einige Metainformationen zu einer Datei haben sich als beliebter herausgestellt?  Ist es möglich, diese Scherbe zu verbreiten, oder unterscheidet sich die Belastung der Scherben nirgendwo um Größenordnungen? </blockquote><br>  Sie unterscheidet sich nicht um Größenordnungen.  Es ist fast normal verteilt.  Wir haben Drosselung, das heißt, wir können den Shard nicht überlasten. Wir drosseln auf Client-Ebene.  Im Allgemeinen kommt es vor, dass ein Stern ein Foto hochlädt und die Scherbe praktisch explodiert.  Dann verbieten wir diesen Link <br><br><blockquote>  - Sie sagten, Sie haben 992 Warnungen.  Könnten Sie näher erläutern, was es ist - ist es sofort einsatzbereit oder wurde es erstellt?  Wenn erstellt, ist es Handarbeit oder so etwas wie maschinelles Lernen? </blockquote><br>  Dies wird alles manuell erstellt.  Wir haben unser eigenes internes System namens Vortex, in dem Metriken gespeichert und Warnungen darin unterstützt werden.  Es gibt eine Yaml-Datei, die besagt, dass beispielsweise eine Bedingung vorliegt, dass Sicherungen jeden Tag ausgeführt werden müssen. Wenn diese Bedingung erfüllt ist, funktioniert die Warnung nicht.  Wenn nicht ausgeführt, wird eine Warnung ausgegeben. <br><br>  Dies ist unsere interne Entwicklung, da nur wenige Personen so viele Metriken speichern können, wie wir benötigen. <br><br><blockquote>  - Wie stark müssen die Nerven sein, um DRT zu machen?  Du bist gefallen, CODERED, steigt nicht auf, mit jeder Minute Panik mehr. </blockquote><br>  Im Allgemeinen ist das Arbeiten in Datenbanken wirklich ein Schmerz.  Wenn die Datenbank abstürzt, funktioniert der Dienst nicht, die gesamte Dropbox funktioniert nicht.  Das ist ein echter Schmerz.  DRT ist insofern nützlich, als es eine Geschäftsuhr ist.  Das heißt, ich bin bereit, ich sitze an meinem Schreibtisch, ich habe Kaffee getrunken, ich bin frisch, ich bin bereit, alles zu tun. <br><br>  Schlimmer noch, wenn es um 4 Uhr morgens passiert und es nicht DRT ist.  Zum Beispiel der letzte große Fehler, den wir kürzlich hatten.  Beim Injizieren eines neuen Systems haben wir vergessen, den OOM-Score für MySQL festzulegen.  Es gab einen anderen Dienst, der binlog las.  Irgendwann ist unser Bediener manuell - wieder manuell!  - führt den Befehl aus, um einige Informationen in der Percona-Prüfsummentabelle zu löschen.  Nur ein einfaches Löschen, eine einfache Operation, aber diese Operation erzeugte ein riesiges Binlog.  Der Dienst las dieses Binlog in den Speicher, OOM Killer kam und überlegte, wen er töten sollte.  Und wir haben vergessen, den OOM-Score festzulegen, und das tötet MySQL! <br><br>  Wir haben 40 Meister, die um 4 Uhr morgens sterben.  Wenn 40 Meister sterben, ist das wirklich sehr beängstigend und gefährlich.  DRT ist nicht beängstigend und nicht gefährlich.  Wir lagen ungefähr eine Stunde. <br><br>  DRT ist übrigens eine gute Möglichkeit, solche Momente zu proben, damit wir genau wissen, welche Abfolge von Aktionen erforderlich ist, wenn etwas massenhaft kaputt geht. <br><br><blockquote>  - Ich möchte mehr über das Wechseln von Master-Master erfahren.  Erstens, warum wird beispielsweise ein Cluster nicht verwendet?  Ein Datenbankcluster, dh kein Master-Slave mit Switching, sondern eine Master-Master-Anwendung, sodass es nicht beängstigend ist, wenn einer fällt. </blockquote><br>  Meinen Sie so etwas wie Gruppenreplikation, Galera-Cluster usw.?  Es scheint mir, dass die Gruppenanwendung noch nicht lebensbereit ist.  Leider haben wir Galera noch nicht ausprobiert.  Dies ist großartig, wenn sich ein Faylover in Ihrem Protokoll befindet, aber leider so viele andere Probleme hat und es nicht so einfach ist, zu dieser Lösung zu wechseln. <br><br><blockquote>  - Es scheint, dass es in MySQL 8 so etwas wie einen InnoDb-Cluster gibt.  Nicht versucht? </blockquote><br>  Wir haben noch 5,6 wert.  Ich weiß nicht, wann wir zu 8 wechseln werden. Vielleicht versuchen wir es. <br><br><blockquote>  - Wenn Sie in diesem Fall einen großen Master haben, stellt sich beim Wechsel von einem zum anderen heraus, dass sich die Warteschlange auf den Slave-Servern mit hoher Last ansammelt.  Wenn der Master gelöscht ist, muss die Warteschlange erreicht werden, damit der Slave in den Master-Modus wechselt - oder wird dies irgendwie anders gemacht? </blockquote><br>  Die Belastung des Masters wird durch Semisync geregelt.  Semisync beschränkt die Masteraufzeichnung auf die Leistung des Slave-Servers.  Natürlich kann es sein, dass die Transaktion kam, Semisync funktionierte, aber die Slaves haben diese Transaktion für eine sehr lange Zeit verloren.  Sie müssen dann warten, bis der Slave diese Transaktion bis zum Ende verliert. <br><br><blockquote>  - Aber dann werden neue Daten gemastert, und es wird notwendig sein ... </blockquote><br>  Wenn wir den Promotionsprozess starten, deaktivieren wir I / O.  Danach kann der Master nichts mehr schreiben, da Semisync repliziert wird.  Phantomlesung kann leider kommen, aber dies ist bereits ein weiteres Problem. <br><br><blockquote>  - Dies sind alles schöne Zustandsautomaten - worauf sind die Skripte geschrieben und wie schwierig ist es, einen neuen Schritt hinzuzufügen?  Was muss mit der Person getan werden, die dieses System schreibt? </blockquote><br>  Alle Skripte sind in Python geschrieben, alle Dienste sind in Go geschrieben.  Dies ist unsere Politik.  Das Ändern der Logik ist einfach - nur im Python-Code, der das Zustandsdiagramm generiert. <br><br><blockquote>  - Und Sie können mehr über das Testen lesen.  Wie werden Tests geschrieben, wie werden Knoten in einer virtuellen Maschine bereitgestellt - sind dies Container? </blockquote><br>  Ja  Wir werden mit Hilfe von Bazel testen.  Es gibt einige Konfigurationsdateien (json) und Bazel greift auf ein Skript zurück, das mithilfe dieser Konfigurationsdatei die Topologie für unseren Test erstellt.  Dort werden verschiedene Topologien beschrieben. <br><br>  In Docker-Containern funktioniert alles für uns: entweder in CI oder in Devbox.  Wir haben ein Devbox-System.  Wir entwickeln alle auf einem Remote-Server, und dies kann beispielsweise funktionieren.  Dort läuft es auch in Bazel, in einem Docker-Container oder in der Bazel Sandbox.  Bazel ist sehr kompliziert, macht aber Spaß. <br><br><blockquote>  - Wenn Sie 4 Instanzen auf einem Server erstellt haben, haben Sie an Speichereffizienz verloren? </blockquote><br>  Jede Instanz ist kleiner geworden.  Je weniger Speicher MySQL verwendet, desto einfacher ist es für ihn zu leben.  Jedes System ist mit wenig Speicher einfacher zu bedienen.  An diesem Ort haben wir nichts verloren.  Wir haben die einfachsten C-Gruppen, die diese Instanzen aus dem Speicher beschränken. <br><br><blockquote>  - Wenn Sie 6.000 Server haben, auf denen Datenbanken gespeichert sind, können Sie angeben, wie viele Milliarden Petabyte in Ihren Dateien gespeichert sind? </blockquote><br>  Dies sind Dutzende von Exabytes, wir haben ein Jahr lang Daten von Amazon gegossen. <br><br><blockquote>  - Es stellte sich heraus, dass Sie zuerst 8 Server mit 200 Shards und dann 400 Server mit jeweils 4 Shards hatten.  Sie haben 1600 Shards - ist das eine Art fest codierter Wert?  Schaffst du es nie wieder?  Wird es weh tun, wenn Sie zum Beispiel 3.200 Scherben benötigen? </blockquote><br>  Ja, es war ursprünglich 1600. Dies wurde vor etwas weniger als 10 Jahren getan, und wir leben immer noch.  Aber wir haben immer noch 4 Scherben - 4 Mal können wir den Raum noch vergrößern. <br><br><blockquote>  - Wie sterben Server hauptsächlich aus welchen Gründen?  Was passiert öfter, seltener und es ist besonders interessant, ob spontane Block-Carapters auftreten? </blockquote><br>  Das Wichtigste ist, dass die Festplatten herausfliegen.  Wir haben RAID 0 - die Festplatte ist abgestürzt, der Master ist gestorben.  Dies ist das Hauptproblem, aber es ist für uns einfacher, diesen Server zu ersetzen.  Google ist einfacher, das Rechenzentrum zu ersetzen, wir haben noch einen Server.  Wir hatten fast nie eine Korruptionsprüfsumme.  Um ehrlich zu sein, ich erinnere mich nicht, wann es das letzte Mal war.  Wir aktualisieren den Assistenten nur oft.  Unsere Lebenszeit für einen Meister ist auf 60 Tage begrenzt.  Es kann nicht länger leben, danach ersetzen wir es durch einen neuen Server, da sich aus irgendeinem Grund ständig etwas in MySQL ansammelt und nach 60 Tagen Probleme auftreten.  Vielleicht nicht in MySQL, vielleicht in Linux. <br><br>  Wir wissen nicht, was dieses Problem ist, und wir wollen uns nicht damit befassen.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir haben die Zeit nur auf 60 Tage begrenzt und den gesamten Stapel aktualisiert. </font><font style="vertical-align: inherit;">Sie müssen sich nicht an einen Meister halten.</font></font><br><br><blockquote> —  ,    6        . ,   JPEG   ,     JPEG,  ,      ?  , ,      -   ?    —      ,       ? </blockquote><br>     ,  .   —  Dropbox    . <br><br><blockquote> —      ?         ?     , ,  - ,    , ? ,   10   . ,  7     ,    6    ,    .    ? </blockquote><br>   Dropbox  - ,       .   .  ,    ,       ,   -  . <br><br>  ,    .  ,  ,      ,      .        - ,     6 ,   ,     ,    ,    . <br></div></div><br><blockquote>       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="></a> ,  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">facebook</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">youtube-</a> —          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Highload++ 2018</a> .      , <strong> 1 </strong>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">  </a> . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de417315/">https://habr.com/ru/post/de417315/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de417303/index.html">Hack zur Unterstützung von Windows Android-Headset-Tasten</a></li>
<li><a href="../de417305/index.html">Ultima Online: ein Backstage-Look</a></li>
<li><a href="../de417307/index.html">Glaukom - noch nichts von ihr gehört? Lernen Sie den Serienkiller für stille Visionen kennen</a></li>
<li><a href="../de417309/index.html">ITSM-Manager Glücklicherweise: Wie der Beruf der Zukunft dazu beiträgt, die Grenzen des Service Desks zu erweitern</a></li>
<li><a href="../de417311/index.html">Erstellen eines Bots zur Teilnahme am AI Mini Cup 2018 basierend auf einem wiederkehrenden neuronalen Netzwerk</a></li>
<li><a href="../de417317/index.html">Bei Highload ++ 2018 volle Kraft voraus</a></li>
<li><a href="../de417319/index.html">Systeme im Gehäuse oder Was befindet sich eigentlich unter der Abdeckung des Mikroprozessors</a></li>
<li><a href="../de417321/index.html">Wie suchen wir Lehrer für Online-Kurse unter Entwicklern?</a></li>
<li><a href="../de417323/index.html">Probleme bei der Gewährleistung einer 100% igen Projektzugänglichkeit</a></li>
<li><a href="../de417325/index.html">Tag der offenen Tür in der Netrologie, Thema Data Science</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>