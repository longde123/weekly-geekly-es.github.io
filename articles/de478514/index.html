<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üèß üòù üëÜüèø Verwenden verschl√ºsselter Daten f√ºr maschinelles Lernen, ohne sie zu entschl√ºsseln üêÑ üõå üëäüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Verwenden verschl√ºsselter Daten f√ºr maschinelles Lernen, ohne sie zu entschl√ºsseln 
 Dieser Artikel beschreibt fortgeschrittene kryptographische Techn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verwenden verschl√ºsselter Daten f√ºr maschinelles Lernen, ohne sie zu entschl√ºsseln</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/478514/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/_p/ns/yf/_pnsyfz49v6t61j8whitl-q4qw0.jpeg"></div><br>  Verwenden verschl√ºsselter Daten f√ºr maschinelles Lernen, ohne sie zu entschl√ºsseln <br>  Dieser Artikel beschreibt fortgeschrittene kryptographische Techniken.  Dies ist nur ein √úberblick √ºber die von Julia Computing durchgef√ºhrten Forschungsarbeiten.  Verwenden Sie die hier angegebenen Beispiele nicht f√ºr kommerzielle Anwendungen.  Wenden Sie sich immer an Kryptografen, bevor Sie Kryptografie anwenden. <br><br>  <a href="">Hier k√∂nnen</a> Sie das Paket herunterladen, das die ganze Magie implementiert, und <a href="">hier</a> ist der Code, der im Artikel beschrieben wird. <br><a name="habracut"></a><br><h2>  Einleitung </h2><br>  <a href="">Angenommen</a> , Sie haben gerade ein cooles neues Modell f√ºr maschinelles Lernen entwickelt (nat√ºrlich mit <a href="">Flux.jl</a> ).  Und jetzt m√∂chten Sie es f√ºr Ihre Benutzer bereitstellen.  Wie wirst du das machen?  Am einfachsten ist es wahrscheinlich, das Modell den Benutzern zu geben und es lokal auf ihren Daten laufen zu lassen.  Dieser Ansatz hat jedoch Nachteile: <br><br><ol><li>  Modelle f√ºr maschinelles Lernen sind gro√ü und Benutzercomputer verf√ºgen m√∂glicherweise nicht √ºber gen√ºgend Computer- oder Festplattenressourcen. </li><li>  Modelle f√ºr maschinelles Lernen werden h√§ufig aktualisiert, und es ist m√∂glicherweise nicht bequem, regelm√§√üig gro√üe Datenmengen √ºber das Netzwerk zu senden. <br></li><li>  Die Modellentwicklung ist zeitaufw√§ndig und erfordert eine gro√üe Menge an Rechenressourcen.  M√∂glicherweise m√∂chten Sie eine Entsch√§digung in Form einer Geb√ºhr f√ºr die Verwendung Ihres Modells. </li></ol><br>  Dann erinnern sie sich normalerweise daran, dass das Modell √ºber die API in der Cloud bereitgestellt werden kann.  In den letzten Jahren sind viele solcher Dienste erschienen, und jede gro√üe Cloud-Plattform bietet Unternehmensentwicklern √§hnliche Dienste.  Potenzielle Benutzer stehen jedoch vor einem offensichtlichen Dilemma: Jetzt werden ihre Daten auf einem Remote-Server verarbeitet, der m√∂glicherweise nicht vertrauensw√ºrdig ist.  Dies hat klare ethische und rechtliche Auswirkungen, die die Nutzung solcher Dienste einschr√§nken.  In regulierten Branchen, insbesondere im Gesundheits- und Finanzwesen, ist es h√§ufig nicht m√∂glich, Patienten- und Kundendaten zur Verarbeitung an Dritte zu senden. <br><br>  Irgendwelche anderen Optionen? <br><br>  Es stellt sich heraus, dass es gibt!  J√ºngste Entdeckungen in der Kryptographie erm√∂glichen das Rechnen mit Daten, <i>ohne sie zu dekodieren</i> .  Beispielsweise sendet ein Benutzer verschl√ºsselte Daten (z. B. Bilder) an die Cloud-API, die ein Modell f√ºr maschinelles Lernen startet, und sendet dann eine verschl√ºsselte Antwort.  Zu keinem Zeitpunkt werden die Daten entschl√ºsselt, der Cloud-Anbieter erh√§lt keinen Zugriff auf die Quellbilder und kann die berechnete Prognose nicht entschl√ºsseln.  Wie ist das m√∂glich?  Informieren Sie sich am Beispiel der Erstellung eines Dienstes zur Handschrifterkennung f√ºr verschl√ºsselte Bilder aus dem MNIST-Datensatz. <br><br><h2>  √úber homomorphe Verschl√ºsselung </h2><br>  Die F√§higkeit, Berechnungen mit verschl√ºsselten Daten durchzuf√ºhren, wird allgemein als "sicheres Rechnen" bezeichnet.  Dies ist ein gro√ües Forschungsgebiet mit zahlreichen Ans√§tzen f√ºr die Kryptographie, die von allen Arten von Anwendungsszenarien abh√§ngen.  Wir werden uns auf eine Technik namens "homomorphe Verschl√ºsselung" konzentrieren.  In einem solchen System stehen uns normalerweise die folgenden Operationen zur Verf√ºgung: <br><br><ul><li><code>pub_key, eval_key, priv_key = keygen()</code> <br> </li><li> <code>encrypted = encrypt(pub_key, plaintext)</code> <br> </li><li> <code>decrypted = decrypt(priv_key, encrypted)</code> <br> </li><li> <code>encrypted‚Ä≤ = eval(eval_key, f, encrypted)</code> <br> </li></ul><br>  Die ersten drei Vorg√§nge sind f√ºr alle Benutzer einfach und vertraut, die bereits asymmetrische Verschl√ºsselungsalgorithmen verwendet haben (z. B. wenn Sie eine Verbindung √ºber TLS hergestellt haben).  Alle Magie geschieht in der letzten Operation.  W√§hrend der Verschl√ºsselung wertet es die Funktion <code>f</code> und gibt einen anderen verschl√ºsselten Wert zur√ºck, der gem√§√ü dem Ergebnis der Bewertung von <code>f</code> f√ºr den verschl√ºsselten Wert berechnet wurde.  Dieses Feature gab seinem Ansatz seinen Namen.  Die Bewertung bezieht sich auf den Verschl√ºsselungsvorgang: <br><br><pre> <code class="julia hljs">f(decrypt(priv_key, encrypted)) == decrypt(priv_key, eval(eval_key, f, encrypted))</code> </pre> <br>  Ebenso k√∂nnen wir mit einem verschl√ºsselten Wert beliebige Homomorphismen <code>f</code> auswerten. <br><br>  Welche Funktionen <code>f</code> unterst√ºtzt werden, h√§ngt von den Verschl√ºsselungsschemata und den unterst√ºtzten Operationen ab.  Wenn nur ein <code>f</code> unterst√ºtzt wird (zum Beispiel <code>f = +</code> ), wird die Schaltung als "teilweise homomorph" bezeichnet.  Wenn <code>f</code> ein vollst√§ndiger Satz von Gateways sein kann, auf deren Grundlage beliebige Schemata erstellt werden k√∂nnen, wird dies f√ºr eine begrenzte Gr√∂√üe eines Schemas eine andere Art einer teilweise homomorphen Berechnung genannt - "etwas homomorph" und f√ºr eine unbegrenzte Gr√∂√üe - "vollst√§ndig homomorph".  Sie k√∂nnen mithilfe der Bootstrapping-Technik "auf irgendeine Weise" eine vollst√§ndig homomorphe Verschl√ºsselung erstellen, was jedoch den Rahmen unseres Artikels sprengt.  V√∂llig homomorphe Verschl√ºsselung ist eine relativ junge Entdeckung. Das erste (wenn auch unpraktische) Arbeitsschema wurde 2009 von <a href="https://www.cs.cmu.edu/~odonnell/hits09/gentry-homomorphic-encryption.pdf">Craig Gentry ver√∂ffentlicht</a> .  Es gibt eine Reihe sp√§terer (und praktischer) vollst√§ndig homomorpher Schemata.  Es gibt auch Softwarepakete, die diese Schemata qualitativ implementieren.  Am h√§ufigsten verwenden sie <a href="https://github.com/microsoft/SEAL">Microsoft SEAL</a> und <a href="https://palisade-crypto.org/">PALISADE</a> .  Au√üerdem habe ich k√ºrzlich den Implementierungscode f√ºr diese <a href="">Pure Julia-</a> Algorithmen ge√∂ffnet.  In diesem Artikel verwenden wir die darin implementierte CKKS-Verschl√ºsselung. <br><br><h2>  CKS √úbersicht </h2><br>  CKKS (unter den Namen der Autoren der <a href="https://eprint.iacr.org/2016/421.pdf">wissenschaftlichen Arbeit</a> Cheon-Kim-Kim-Song, die den Algorithmus 2016 vorgeschlagen hat) ist ein homomorphes Verschl√ºsselungsschema, das die homomorphe Auswertung der folgenden primitiven Operationen erm√∂glicht: <br><br><ul><li>  Die elementweise Addition der L√§ngen von <code>n</code> Vektoren komplexer Zahlen. <br></li><li>  Elementweise Multiplikation der L√§ngen von <code>n</code> komplexen Vektoren. <br></li><li>  Drehen Sie (im Kontext der <code>circshift</code> ) Elemente in einem Vektor. <br></li><li>  Integrierte Paarung von Vektorelementen. <br></li></ul><br>  Der Parameter <code>n</code> h√§ngt von der gew√ºnschten Sicherheit und Genauigkeit ab und ist normalerweise ziemlich hoch.  In unserem Beispiel ist es 4096 (ein h√∂herer Wert erh√∂ht die Sicherheit, ist aber auch schwieriger zu berechnen, da er ungef√§hr wie <code>n log n</code> skaliert). <br><br>  Dar√ºber hinaus sind Berechnungen mit CKKS <i>verrauscht</i> .  Daher sind die Ergebnisse ungef√§hr und es muss darauf geachtet werden, dass die Ergebnisse mit ausreichender Genauigkeit ausgewertet werden, um die Richtigkeit des Ergebnisses nicht zu beeintr√§chtigen. <br><br>  Andererseits sind solche Einschr√§nkungen f√ºr Entwickler von maschinellen Lernpaketen nicht ungew√∂hnlich.  Spezielle Beschleuniger wie die GPU arbeiten √ºblicherweise auch mit Zahlenvektoren.  Dar√ºber hinaus sehen Gleitkommazahlen f√ºr viele Entwickler aufgrund des Einflusses von Auswahlalgorithmen, Multithreading usw. manchmal verrauscht aus.  Ich m√∂chte betonen, dass der Hauptunterschied hier darin besteht, dass arithmetische Berechnungen mit Gleitkommazahlen anf√§nglich deterministisch sind, auch wenn dies aufgrund der Komplexit√§t der Implementierung nicht offensichtlich ist, obwohl die CKKS-Grundelemente wirklich verrauscht sind.  Aber vielleicht k√∂nnen Benutzer dadurch verstehen, dass das Ger√§usch nicht so furchterregend ist, wie es scheinen mag. <br><br>  Nun wollen wir sehen, wie Sie diese Operationen in Julia ausf√ºhren k√∂nnen (Anmerkung: Es sind sehr unsichere Parameter ausgew√§hlt, mit diesen Operationen wird nur die Verwendung der Bibliothek in REPL veranschaulicht). <br><br><pre> <code class="julia hljs">julia&gt; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> ToyFHE <span class="hljs-comment"><span class="hljs-comment"># Let's play with 8 element vectors julia&gt; N = 8; # Choose some parameters - we'll talk about it later julia&gt; ‚Ñõ = NegacyclicRing(2N, (40, 40, 40)) ‚Ñ§‚ÇÅ‚ÇÉ‚ÇÇ‚Çâ‚ÇÇ‚ÇÇ‚Çá‚Çâ‚Çâ‚Çá‚ÇÖ‚ÇÜ‚Çà‚ÇÄ‚Çà‚ÇÅ‚ÇÑ‚ÇÖ‚Çá‚ÇÑ‚ÇÄ‚ÇÇ‚Çá‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÄ‚Çá‚ÇÅ‚ÇÄ‚ÇÑ‚ÇÇ‚ÇÑ‚Çà‚ÇÇ‚ÇÖ‚Çá/(x¬π‚Å∂ + 1) # We'll use CKKS julia&gt; params = CKKSParams(‚Ñõ) CKKS parameters # We need to pick a scaling factor for a numbers - again we'll talk about that later julia&gt; Tscale = FixedRational{2^40} FixedRational{1099511627776,T} where T # Let's start with a plain Vector of zeros julia&gt; plain = CKKSEncoding{Tscale}(zero(‚Ñõ)) 8-element CKKSEncoding{FixedRational{1099511627776,T} where T} with indices 0:7: 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im 0.0 + 0.0im # Ok, we're ready to get started, but first we'll need some keys julia&gt; kp = keygen(params) CKKS key pair julia&gt; kp.priv CKKS private key julia&gt; kp.pub CKKS public key # Alright, let's encrypt some things: julia&gt; foreach(i-&gt;plain[i] = i+1, 0:7); plain 8-element CKKSEncoding{FixedRational{1099511627776,T} where T} with indices 0:7: 1.0 + 0.0im 2.0 + 0.0im 3.0 + 0.0im 4.0 + 0.0im 5.0 + 0.0im 6.0 + 0.0im 7.0 + 0.0im 8.0 + 0.0im julia&gt; c = encrypt(kp.pub, plain) CKKS ciphertext (length 2, encoding CKKSEncoding{FixedRational{1099511627776,T} where T}) # And decrypt it again julia&gt; decrypt(kp.priv, c) 8-element CKKSEncoding{FixedRational{1099511627776,T} where T} with indices 0:7: 0.9999999999995506 - 2.7335193113350057e-16im 1.9999999999989408 - 3.885780586188048e-16im 3.000000000000205 + 1.6772825551165524e-16im 4.000000000000538 - 3.885780586188048e-16im 4.999999999998865 + 8.382500573679615e-17im 6.000000000000185 + 4.996003610813204e-16im 7.000000000001043 - 2.0024593503998215e-16im 8.000000000000673 + 4.996003610813204e-16im # Note that we had some noise. Let's go through all the primitive operations we'll need: julia&gt; decrypt(kp.priv, c+c) 8-element CKKSEncoding{FixedRational{1099511627776,T} where T} with indices 0:7: 1.9999999999991012 - 5.467038622670011e-16im 3.9999999999978817 - 7.771561172376096e-16im 6.00000000000041 + 3.354565110233105e-16im 8.000000000001076 - 7.771561172376096e-16im 9.99999999999773 + 1.676500114735923e-16im 12.00000000000037 + 9.992007221626409e-16im 14.000000000002085 - 4.004918700799643e-16im 16.000000000001346 + 9.992007221626409e-16im julia&gt; csq = c*c CKKS ciphertext (length 3, encoding CKKSEncoding{FixedRational{1208925819614629174706176,T} where T}) julia&gt; decrypt(kp.priv, csq) 8-element CKKSEncoding{FixedRational{1208925819614629174706176,T} where T} with indices 0:7: 0.9999999999991012 - 2.350516767363621e-15im 3.9999999999957616 - 5.773159728050814e-15im 9.000000000001226 - 2.534464540987068e-15im 16.000000000004306 - 2.220446049250313e-15im 24.99999999998865 + 2.0903753311370056e-15im 36.00000000000222 + 4.884981308350689e-15im 49.000000000014595 + 1.0182491378134327e-15im 64.00000000001077 + 4.884981308350689e-15im</span></span></code> </pre> <br>  So einfach!  Ein aufmerksamer Leser kann feststellen, dass sich CSQ geringf√ºgig vom vorherigen Chiffretext unterscheidet.  Insbesondere der Chiffretext hat die L√§nge 3 und der Ma√üstab ist viel gr√∂√üer.  Eine Erkl√§rung dessen, was dies ist und was ben√∂tigt wird, w√ºrde den Rahmen dieses Artikels sprengen.  Es gen√ºgt zu sagen, dass wir die Werte senken m√ºssen, bevor wir mit den Berechnungen fortfahren, sonst endet die "Stelle" im Chiffretext.  Gl√ºcklicherweise k√∂nnen wir jeden der beiden erh√∂hten Werte reduzieren: <br><br><pre> <code class="julia hljs"><span class="hljs-comment"><span class="hljs-comment"># To get back down to length 2, we need to `keyswitch` (aka # relinerarize), which requires an evaluation key. Generating # this requires the private key. In a real application we would # have generated this up front and sent it along with the encrypted # data, but since we have the private key, we can just do it now. julia&gt; ek = keygen(EvalMultKey, kp.priv) CKKS multiplication key julia&gt; csq_length2 = keyswitch(ek, csq) CKKS ciphertext (length 2, encoding CKKSEncoding{FixedRational{1208925819614629174706176,T} where T}) # Getting the scale back down is done using modswitching. julia&gt; csq_smaller = modswitch(csq_length2) CKKS ciphertext (length 2, encoding CKKSEncoding{FixedRational{1.099511626783e12,T} where T}) # And it still decrypts correctly (though note we've lost some precision) julia&gt; decrypt(kp.priv, csq_smaller) 8-element CKKSEncoding{FixedRational{1.099511626783e12,T} where T} with indices 0:7: 0.9999999999802469 - 5.005163520332181e-11im 3.9999999999957723 - 1.0468514951188039e-11im 8.999999999998249 - 4.7588542623100616e-12im 16.000000000023014 - 1.0413447889166631e-11im 24.999999999955193 - 6.187833723406491e-12im 36.000000000002345 + 1.860733715346631e-13im 49.00000000001647 - 1.442396043149794e-12im 63.999999999988695 - 1.0722489563648028e-10im</span></span></code> </pre> <br>  Dar√ºber hinaus reduziert Modswitching (kurz f√ºr Modulumschaltung, Modulumschaltung) die Gr√∂√üe des Chiffretext-Moduls, sodass wir dies nicht unbegrenzt fortsetzen k√∂nnen (wir verwenden ein etwas homomorphes Verschl√ºsselungsschema): <br><br><pre> <code class="julia hljs">julia&gt; ‚Ñõ <span class="hljs-comment"><span class="hljs-comment"># Remember the ring we initially created ‚Ñ§‚ÇÅ‚ÇÉ‚ÇÇ‚Çâ‚ÇÇ‚ÇÇ‚Çá‚Çâ‚Çâ‚Çá‚ÇÖ‚ÇÜ‚Çà‚ÇÄ‚Çà‚ÇÅ‚ÇÑ‚ÇÖ‚Çá‚ÇÑ‚ÇÄ‚ÇÇ‚Çá‚ÇÄ‚ÇÅ‚ÇÇ‚ÇÄ‚Çá‚ÇÅ‚ÇÄ‚ÇÑ‚ÇÇ‚ÇÑ‚Çà‚ÇÇ‚ÇÖ‚Çá/(x¬π‚Å∂ + 1) julia&gt; ToyFHE.ring(csq_smaller) # It shrunk! ‚Ñ§‚ÇÅ‚ÇÇ‚ÇÄ‚Çà‚Çâ‚ÇÇ‚ÇÖ‚Çà‚ÇÇ‚ÇÄ‚ÇÅ‚ÇÑ‚ÇÑ‚ÇÖ‚Çâ‚ÇÉ‚Çá‚Çá‚Çâ‚ÇÉ‚ÇÉ‚ÇÅ‚ÇÖ‚ÇÖ‚ÇÉ/(x¬π‚Å∂ + 1)&lt;/code&gt;     ‚Äî  (rotations).      keyswitch,       (evaluation key,     ): &lt;source lang="julia"&gt;julia&gt; gk = keygen(GaloisKey, kp.priv; steps=2) CKKS galois key (element 25) julia&gt; decrypt(circshift(c, gk)) decrypt(kp, circshift(c, gk)) 8-element CKKSEncoding{FixedRational{1099511627776,T} where T} with indices 0:7: 7.000000000001042 + 5.68459112632516e-16im 8.000000000000673 + 5.551115123125783e-17im 0.999999999999551 - 2.308655353580721e-16im 1.9999999999989408 + 2.7755575615628914e-16im 3.000000000000205 - 6.009767921608429e-16im 4.000000000000538 + 5.551115123125783e-17im 4.999999999998865 + 4.133860996136768e-17im 6.000000000000185 - 1.6653345369377348e-16im # And let's compare to doing the same on the plaintext julia&gt; circshift(plain, 2) 8-element OffsetArray(::Array{Complex{Float64},1}, 0:7) with eltype Complex{Float64} with indices 0:7: 7.0 + 0.0im 8.0 + 0.0im 1.0 + 0.0im 2.0 + 0.0im 3.0 + 0.0im 4.0 + 0.0im 5.0 + 0.0im 6.0 + 0.0im</span></span></code> </pre> <br>  Wir haben die Grundlagen der Nutzung der HE-Bibliothek behandelt.  Bevor wir jedoch mit der Berechnung von Vorhersagen f√ºr neuronale Netze fortfahren, schauen wir uns den Lernprozess an. <br><br><h2>  Modell des maschinellen Lernens </h2><br>  Wenn Sie nicht mit maschinellem Lernen oder der Flux.jl-Bibliothek vertraut sind, empfehle ich einen kurzen Durchlauf der <a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl-Dokumentation</a> oder eine kostenlose <a href="https://juliaacademy.com/p/introduction-to-machine-learning">Einf√ºhrung in maschinelles Lernen</a> , da wir nur √Ñnderungen bei der Anwendung des Modells auf verschl√ºsselte Daten diskutieren. <br><br>  Beginnen wir mit dem Faltungsnetzwerk <a href="">aus dem Flux-Zoo</a> .  Wir werden den gleichen Trainingszyklus durchf√ºhren, mit Datenaufbereitung und so weiter, nur das Modell ein wenig einrichten.  Da ist sie: <br><br><pre> <code class="julia hljs"><span class="hljs-keyword"><span class="hljs-keyword">function</span></span> reshape_and_vcat(x) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> y=reshape(x, <span class="hljs-number"><span class="hljs-number">64</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, size(x, <span class="hljs-number"><span class="hljs-number">4</span></span>)) vcat((y[:,i,:] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i=axes(y,<span class="hljs-number"><span class="hljs-number">2</span></span>))...) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> model = Chain( <span class="hljs-comment"><span class="hljs-comment"># First convolution, operating upon a 28x28 image Conv((7, 7), 1=&gt;4, stride=(3,3), x-&gt;x.^2), reshape_and_vcat, Dense(256, 64, x-&gt;x.^2), Dense(64, 10), )</span></span></code> </pre><br>  Dies ist das gleiche Modell wie in der Arbeit <a href="https://eprint.iacr.org/2018/1041.pdf">‚ÄûSichere ausgelagerte Matrixberechnung und Anwendung auf neuronale Netze‚Äú</a> , die dasselbe kryptografische Schema mit zwei Unterschieden verwendet: 1) Der Einfachheit halber haben wir das Modell selbst nicht verschl√ºsselt und 2) nach jeder Schicht, die wir haben Bayesianische Vektoren werden verwendet (in Flux wird dies standardm√§√üig durchgef√ºhrt), ich bin mir nicht sicher, was es in der erw√§hnten Arbeit war.  M√∂glicherweise stellte sich aufgrund des zweiten Punktes heraus, dass die Genauigkeit des Testsatzes unseres Modells geringf√ºgig h√∂her war (98,6% gegen√ºber 98,1%), aber auch hyperparametrische Unterschiede k√∂nnten der Grund sein. <br><br>  Ungew√∂hnlich (f√ºr diejenigen, die Erfahrung im maschinellen Lernen haben) ist die <code>x.^2</code> Aktivierung von Funktionen.  Meistens benutzen sie in solchen F√§llen <code>relu</code> , <code>relu</code> oder etwas Phantasievolleres.  Obwohl diese Funktionen (insbesondere <code>relu</code> ) f√ºr gew√∂hnliche <code>relu</code> leicht berechnet werden k√∂nnen, erfordern sie m√∂glicherweise eine Menge Rechenressourcen, um sie in verschl√ºsselter Form auszuwerten (wir sch√§tzen normalerweise die Polynomapproximation).  Zum Gl√ºck funktioniert in diesem Fall <code>x.^2</code> sehr gut. <br><br>  Der Rest des Lernzyklus blieb gleich.  Wir haben <code>softmax</code> aus dem Modell f√ºr die Verlustfunktion <code>logitcrossentropy</code> (Sie k√∂nnen es verlassen und softmax nach der Entschl√ºsselung auf dem Client auswerten).  Der komplette Code zum Trainieren des Modells liegt <a href="">auf GitHub</a> , es l√§uft in wenigen Minuten auf jeder neuen Grafikkarte. <br><br><h2>  Effektive Operationen </h2><br>  Jetzt wissen wir, welche Operationen wir durchf√ºhren m√ºssen: <br><br><ul><li>  Koagulation. <br></li><li>  Quadratur der Elemente. <br></li><li>  Matrixmultiplikation. <br></li></ul><br>  Beim Quadrieren ist alles einfach, wir haben es bereits oben untersucht, also werden wir zwei andere Operationen betrachten.  Wir nehmen an, dass die L√§nge des Datenpakets 64 betr√§gt (Sie werden m√∂glicherweise bemerken, dass die Modellparameter und die Paketgr√∂√üe so gew√§hlt werden, dass der 4096-Elemente-Vektor, den wir als Ergebnis einer realistischen Auswahl von Parametern erhalten haben, ausgenutzt wird). <br><br><h3>  Koagulation </h3><br>  Denken Sie daran, wie die Gerinnung funktioniert.  Nehmen Sie ein Fenster (in unserem Fall 7x7) des urspr√ºnglichen Eingabearrays, und jedes Fensterelement wird mit einem Faltungsmaskenelement multipliziert.  Dann bewegen wir das Fenster zu einem Schritt (in unserem Fall ist der Schritt 3, dh wir bewegen 3 Elemente) und wiederholen den Vorgang (mit der gleichen Faltungsmaske).  Die Animation des Prozesses ( <a href="https://github.com/vdumoulin/conv_arithmetic">Quelle</a> ) f√ºr die 3x3-Faltung mit dem Schritt <code>(2, 2)</code> unten dargestellt (blaues Array - Eingang, gr√ºner Ausgang): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/h5/a3/q3/h5a3q3ea0pljgdagz1wd-k2igdy.gif"></div><br>  Au√üerdem f√ºhren wir die Faltung in vier verschiedenen ‚ÄûKan√§len‚Äú durch (dh, wir wiederholen die Faltung dreimal mit verschiedenen Masken). <br><br>  Jetzt wissen wir, was zu tun ist, es bleibt zu verstehen, wie.  Wir haben das Gl√ºck, dass die Faltung die erste Operation in unserem Modell ist.  Um Ressourcen zu sparen, k√∂nnen wir die Daten auf dem Client vorverarbeiten und dann verschl√ºsseln (ohne Gewichte zu verwenden).  Lass uns das machen: <br><br><ul><li>  Zuerst berechnen wir jedes Faltungsfenster (dh ein 7x7-Sample aus den Quellbildern), wodurch wir 64 7x7-Matrizen f√ºr jedes Eingabebild erhalten.  Beachten Sie, dass f√ºr ein 7x7-Fenster in Schritten von 2 8x8-Faltungsfenster zur Auswertung des 28x28-Eingabebilds vorhanden sind. <br></li><li>  Sammeln wir in einem Vektor die gleichen Positionen in jedem Fenster.  Das hei√üt, f√ºr jedes Bild haben wir einen 64-Element-Vektor oder einen Vektor von 64 √ó 64 Elementen f√ºr ein Paket der Gr√∂√üe 64 (insgesamt 49 64 √ó 64 Matrizen). <br></li><li>  Wir werden verschl√ºsseln. <br></li></ul><br>  Die Koagulation wird dann einfach zu einer skalaren Multiplikation der gesamten Matrix mit dem entsprechenden Maskenelement.  Wenn wir sp√§ter alle 49 Elemente zusammenfassen, erhalten wir das Ergebnis der Faltung.  So k√∂nnte die Umsetzung dieser Strategie aussehen (im Klartext): <br><br><pre> <code class="julia hljs"><span class="hljs-keyword"><span class="hljs-keyword">function</span></span> public_preprocess(batch) ka = OffsetArray(<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Create feature extracted matrix I = [[batch[i‚Ä≤*3 .+ (1:7), j‚Ä≤*3 .+ (1:7), 1, k] for i‚Ä≤=ka, j‚Ä≤=ka] for k = 1:64] # Reshape into the ciphertext I·µ¢‚±º = [[I[k][l...][i,j] for k=1:64, l=product(ka, ka)] for i=1:7, j=1:7] end I·µ¢‚±º = public_preprocess(batch) # Evaluate the convolution weights = model.layers[1].weight conv_weights = reverse(reverse(weights, dims=1), dims=2) conved = [sum(I·µ¢‚±º[i,j]*conv_weights[i,j,1,channel] for i=1:7, j=1:7) for channel = 1:4] conved = map(((x,b),)-&gt;x .+ b, zip(conved, model.layers[1].bias))</span></span></code> </pre> <br>  Dies (Modul zum √Ñndern der Abmessung) (Modulo - √Ñndern der <code>model.layers[1](batch)</code> ) gibt die gleiche Antwort wie die Operation <code>model.layers[1](batch)</code> . <br><br>  Hinzuf√ºgen von Verschl√ºsselungsvorg√§ngen: <br><br><pre> <code class="julia hljs">I·µ¢‚±º = public_preprocess(batch) C_I·µ¢‚±º = map(I·µ¢‚±º) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> Iij plain = CKKSEncoding{Tscale}(zero(plaintext_space(ckks_params))) plain .= OffsetArray(vec(Iij), <span class="hljs-number"><span class="hljs-number">0</span></span>:(N√∑<span class="hljs-number"><span class="hljs-number">2</span></span>-<span class="hljs-number"><span class="hljs-number">1</span></span>)) encrypt(kp, plain) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> weights = model.layers[<span class="hljs-number"><span class="hljs-number">1</span></span>].weight conv_weights = reverse(reverse(weights, dims=<span class="hljs-number"><span class="hljs-number">1</span></span>), dims=<span class="hljs-number"><span class="hljs-number">2</span></span>) conved3 = [sum(C_I·µ¢‚±º[i,j]*conv_weights[i,j,<span class="hljs-number"><span class="hljs-number">1</span></span>,channel] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i=<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>, j=<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> channel = <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">4</span></span>] conved2 = map(((x,b),)-&gt;x .+ b, zip(conved3, model.layers[<span class="hljs-number"><span class="hljs-number">1</span></span>].bias)) conved1 = map(ToyFHE.modswitch, conved2)</code> </pre> <br>  Bitte beachten Sie, dass hier kein Schl√ºsselschalter erforderlich ist, da die Gewichte √∂ffentlich sind.  Wir verl√§ngern den Chiffretext also nicht. <br><br><h3>  Matrixmultiplikation </h3><br>  Um zur Matrixmultiplikation √ºberzugehen, k√∂nnen wir die Rotation von Elementen im Vektor verwenden, um die Reihenfolge der Multiplikationsindizes zu √§ndern.  Betrachten Sie die zeilenweise Platzierung von Matrixelementen in einem Vektor.  Wenn wir den Vektor um ein Vielfaches der Zeilengr√∂√üe verschieben, erhalten wir den Effekt der Spaltendrehung, was f√ºr die Implementierung der Matrixmultiplikation (mindestens quadratische Matrizen) ausreicht.  Lass es uns versuchen: <br><br><pre> <code class="julia hljs"><span class="hljs-keyword"><span class="hljs-keyword">function</span></span> matmul_square_reordered(weights, x) sum(<span class="hljs-number"><span class="hljs-number">1</span></span>:size(weights, <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> k <span class="hljs-comment"><span class="hljs-comment"># We rotate the columns of the LHS and take the diagonal weight_diag = diag(circshift(weights, (0,(k-1)))) # We rotate the rows of the RHS x_rotated = circshift(x, (k-1,0)) # We do an elementwise, broadcast multiply weight_diag .* x_rotated end end function matmul_reorderd(weights, x) sum(partition(1:256, 64)) do range matmul_square_reordered(weights[:, range], x[range, :]) end end fc1_weights = model.layers[3].W x = rand(Float64, 256, 64) @assert (fc1_weights*x) ‚âà matmul_reorderd(fc1_weights, x)</span></span></code> </pre> <br>  Nat√ºrlich ist f√ºr die allgemeine Matrixmultiplikation etwas Komplizierteres erforderlich, aber f√ºr den Moment ist dies ausreichend. <br><br><h2>  Verbesserung der Technik </h2><br>  Jetzt funktionieren alle Komponenten unserer Technik.  Hier ist der gesamte Code (au√üer zum Festlegen von Auswahloptionen und √§hnlichen Dingen): <br><br><pre> <code class="julia hljs">ek = keygen(EvalMultKey, kp.priv) gk = keygen(GaloisKey, kp.priv; steps=<span class="hljs-number"><span class="hljs-number">64</span></span>) I·µ¢‚±º = public_preprocess(batch) C_I·µ¢‚±º = map(I·µ¢‚±º) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> Iij plain = CKKSEncoding{Tscale}(zero(plaintext_space(ckks_params))) plain .= OffsetArray(vec(Iij), <span class="hljs-number"><span class="hljs-number">0</span></span>:(N√∑<span class="hljs-number"><span class="hljs-number">2</span></span>-<span class="hljs-number"><span class="hljs-number">1</span></span>)) encrypt(kp, plain) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> weights = model.layers[<span class="hljs-number"><span class="hljs-number">1</span></span>].weight conv_weights = reverse(reverse(weights, dims=<span class="hljs-number"><span class="hljs-number">1</span></span>), dims=<span class="hljs-number"><span class="hljs-number">2</span></span>) conved3 = [sum(C_I·µ¢‚±º[i,j]*conv_weights[i,j,<span class="hljs-number"><span class="hljs-number">1</span></span>,channel] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i=<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>, j=<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">7</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> channel = <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">4</span></span>] conved2 = map(((x,b),)-&gt;x .+ b, zip(conved3, model.layers[<span class="hljs-number"><span class="hljs-number">1</span></span>].bias)) conved1 = map(ToyFHE.modswitch, conved2) Csqed1 = map(x-&gt;x*x, conved1) Csqed1 = map(x-&gt;keyswitch(ek, x), Csqed1) Csqed1 = map(ToyFHE.modswitch, Csqed1) <span class="hljs-keyword"><span class="hljs-keyword">function</span></span> encrypted_matmul(gk, weights, x::ToyFHE.CipherText) result = repeat(diag(weights), inner=<span class="hljs-number"><span class="hljs-number">64</span></span>).*x rotated = x <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k = <span class="hljs-number"><span class="hljs-number">2</span></span>:<span class="hljs-number"><span class="hljs-number">64</span></span> rotated = ToyFHE.rotate(gk, rotated) result += repeat(diag(circshift(weights, (<span class="hljs-number"><span class="hljs-number">0</span></span>,(k-<span class="hljs-number"><span class="hljs-number">1</span></span>)))), inner=<span class="hljs-number"><span class="hljs-number">64</span></span>) .* rotated <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> result <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> fq1_weights = model.layers[<span class="hljs-number"><span class="hljs-number">3</span></span>].W Cfq1 = sum(enumerate(partition(<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>))) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> (i,range) encrypted_matmul(gk, fq1_weights[:, range], Csqed1[i]) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> Cfq1 = Cfq1 .+ OffsetArray(repeat(model.layers[<span class="hljs-number"><span class="hljs-number">3</span></span>].b, inner=<span class="hljs-number"><span class="hljs-number">64</span></span>), <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">4095</span></span>) Cfq1 = modswitch(Cfq1) Csqed2 = Cfq1*Cfq1 Csqed2 = keyswitch(ek, Csqed2) Csqed2 = modswitch(Csqed2) <span class="hljs-keyword"><span class="hljs-keyword">function</span></span> naive_rectangular_matmul(gk, weights, x) <span class="hljs-meta"><span class="hljs-meta">@assert</span></span> size(weights, <span class="hljs-number"><span class="hljs-number">1</span></span>) &lt; size(weights, <span class="hljs-number"><span class="hljs-number">2</span></span>) weights = vcat(weights, zeros(eltype(weights), size(weights, <span class="hljs-number"><span class="hljs-number">2</span></span>)-size(weights, <span class="hljs-number"><span class="hljs-number">1</span></span>), size(weights, <span class="hljs-number"><span class="hljs-number">2</span></span>))) encrypted_matmul(gk, weights, x) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> fq2_weights = model.layers[<span class="hljs-number"><span class="hljs-number">4</span></span>].W Cresult = naive_rectangular_matmul(gk, fq2_weights, Csqed2) Cresult = Cresult .+ OffsetArray(repeat(vcat(model.layers[<span class="hljs-number"><span class="hljs-number">4</span></span>].b, zeros(<span class="hljs-number"><span class="hljs-number">54</span></span>)), inner=<span class="hljs-number"><span class="hljs-number">64</span></span>), <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">4095</span></span>)</code> </pre> <br>  Es sieht nicht allzu ordentlich aus, aber wenn Sie dies alles getan haben, sollten Sie jeden Schritt verstehen. <br>  √úberlegen wir uns nun, welche Abstraktionen unser Leben vereinfachen k√∂nnten.  Wir verlassen das Gebiet der Kartografie und des maschinellen Lernens und wechseln zur Architektur der Programmiersprache. Nutzen wir also die Tatsache, dass Julia es Ihnen erm√∂glicht, leistungsstarke Abstraktionen zu verwenden und zu erstellen.  Beispielsweise k√∂nnen Sie den gesamten Prozess des Extrahierens von Windungen in Ihren Array-Typ einkapseln: <br><br><pre> <code class="julia hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> BlockArrays <span class="hljs-string"><span class="hljs-string">""" ExplodedConvArray{T, Dims, Storage} &lt;: AbstractArray{T, 4} Represents a an `nxmx1xb` array of images, but rearranged into a series of convolution windows. Evaluating a convolution compatible with `Dims` on this array is achievable through a sequence of scalar multiplications and sums on the underling storage. """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> ExplodedConvArray{T, <span class="hljs-built_in"><span class="hljs-built_in">Dims</span></span>, Storage} &lt;: <span class="hljs-built_in"><span class="hljs-built_in">AbstractArray</span></span>{T, <span class="hljs-number"><span class="hljs-number">4</span></span>} <span class="hljs-comment"><span class="hljs-comment"># sx*sy matrix of b*(dx*dy) matrices of extracted elements # where (sx, sy) = kernel_size(Dims) # (dx, dy) = output_size(DenseConvDims(...)) cdims::Dims x::Matrix{Storage} function ExplodedConvArray{T, Dims, Storage}(cdims::Dims, storage::Matrix{Storage}) where {T, Dims, Storage} @assert all(==(size(storage[1])), size.(storage)) new{T, Dims, Storage}(cdims, storage) end end Base.size(ex::ExplodedConvArray) = (NNlib.input_size(ex.cdims)..., 1, size(ex.x[1], 1)) function ExplodedConvArray{T}(cdims, batch::AbstractArray{T, 4}) where {T} x, y = NNlib.output_size(cdims) kx, ky = NNlib.kernel_size(cdims) stridex, stridey = NNlib.stride(cdims) kax = OffsetArray(0:x-1, 0:x-1) kay = OffsetArray(0:x-1, 0:x-1) I = [[batch[i‚Ä≤*stridex .+ (1:kx), j‚Ä≤*stridey .+ (1:ky), 1, k] for i‚Ä≤=kax, j‚Ä≤=kay] for k = 1:size(batch, 4)] I·µ¢‚±º = [[I[k][l...][i,j] for k=1:size(batch, 4), l=product(kax, kay)] for (i,j) in product(1:kx, 1:ky)] ExplodedConvArray{T, typeof(cdims), eltype(I·µ¢‚±º)}(cdims, I·µ¢‚±º) end function NNlib.conv(x::ExplodedConvArray{&lt;:Any, Dims}, weights::AbstractArray{&lt;:Any, 4}, cdims::Dims) where {Dims&lt;:ConvDims} blocks = reshape([ Base.ReshapedArray(sum(xx[i,j]*weights[i,j,1,channel] for i=1:7, j=1:7), (NNlib.output_size(cdims)...,1,size(x, 4)), ()) for channel = 1:4 ],(1,1,4,1)) BlockArrays._BlockArray(blocks, BlockArrays.BlockSizes([8], [8], [1,1,1,1], [64])) end</span></span></code> </pre><br>  Hier haben wir wieder <code>BlockArrays</code> , um ein <code>8x8x4x64</code> Array als vier <code>8x8x1x64</code> Arrays wie im Quellcode <code>8x8x1x64</code> .  Jetzt ist die Darstellung der ersten Stufe, zumindest bei unverschl√ºsselten Arrays, viel sch√∂ner geworden: <br><br><pre> <code class="julia hljs">julia&gt; cdims = DenseConvDims(batch, model.layers[<span class="hljs-number"><span class="hljs-number">1</span></span>].weight; stride=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>), dilation=(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)) DenseConvDims: (<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) * (<span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>) -&gt; (<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>), stride: (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) pad: (<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>), dil: (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), flip: <span class="hljs-literal"><span class="hljs-literal">false</span></span> julia&gt; a = ExplodedConvArray{eltype(batch)}(cdims, batch); julia&gt; model(a) <span class="hljs-number"><span class="hljs-number">10</span></span>√ó<span class="hljs-number"><span class="hljs-number">64</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>{<span class="hljs-built_in"><span class="hljs-built_in">Float32</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>}: [snip]</code> </pre><br>  Wie verbinden wir das nun mit Verschl√ºsselung?  Dazu ben√∂tigen Sie: <br><br><ol><li>  Verschl√ºsseln Sie die Struktur ( <code>ExplodedConvArray</code> ), damit wir den Chiffretext f√ºr jedes Feld erhalten.  Operationen mit einer solchen verschl√ºsselten Struktur werden √ºberpr√ºfen, was die Funktion mit der urspr√ºnglichen Struktur tun w√ºrde, und dasselbe homomorph tun. <br></li><li>  Fangen Sie bestimmte Vorg√§nge ab, um sie in einem verschl√ºsselten Kontext anders auszuf√ºhren. </li></ol><br>  Zum Gl√ºck liefert uns Julia daf√ºr eine Abstraktion: ein Compiler-Plugin, das den <a href="">Cassette.jl-</a> Mechanismus verwendet.  Ich werde Ihnen nicht sagen, was es ist und wie es funktioniert. Ich m√∂chte kurz sagen, dass es den Kontext bestimmen kann, z. B. " <code>Encrypted</code> , und dann definiert es die Regeln, wie Vorg√§nge in diesem Kontext funktionieren sollen.  Beispielsweise k√∂nnen Sie dies f√ºr die zweite Anforderung schreiben: <br><br><pre> <code class="julia hljs"><span class="hljs-comment"><span class="hljs-comment"># Define Matrix multiplication between an array and an encrypted block array function (*::Encrypted{typeof(*)})(a::Array{T, 2}, b::Encrypted{&lt;:BlockArray{T, 2}}) where {T} sum(a*b for (i,range) in enumerate(partition(1:size(a, 2), size(b.blocks[1], 1)))) end # Define Matrix multiplication between an array and an encrypted array function (*::Encrypted{typeof(*)})(a::Array{T, 2}, b::Encrypted{Array{T, 2}}) where {T} result = repeat(diag(a), inner=size(a, 1)).*x rotated = b for k = 2:size(a, 2) rotated = ToyFHE.rotate(GaloisKey(*), rotated) result += repeat(diag(circshift(a, (0,(k-1)))), inner=size(a, 1)) .* rotated end result end</span></span></code> </pre><br>  Infolgedessen ist der Benutzer in der Lage, alles oben Genannte mit einem Minimum an manueller Arbeit zu schreiben: <br><br><pre> <code class="julia hljs">kp = keygen(ckks_params) ek = keygen(EvalMultKey, kp.priv) gk = keygen(GaloisKey, kp.priv; steps=<span class="hljs-number"><span class="hljs-number">64</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Create evaluation context ctx = Encrypted(ek, gk) # Do public preprocessing batch = ExplodedConvArray{eltype(batch)}(cdims, batch); # Run on encrypted data under the encryption context Cresult = ctx(model)(encrypt(kp.pub, batch)) # Decrypt the answer decrypt(kp, Cresult)</span></span></code> </pre> <br> ,     .   (   ‚Ñõ,   modswitch, keyswitch  ..)       ,      .  ,    ,    ,         ,        . <br><br><h2>  Fazit </h2><br>          ‚Äî      .     Julia          .  RAMPARTS ( <a href="https://eprint.iacr.org/2019/988.pdf">paper</a> , <a href="https://www.youtube.com/watch%3Fv%3D_KLlMg6jKQg">JuliaCon talk</a> )       :  Julia-   -  PALISADE. Julia Computing    RAMPARTS    Verona, <a href="https://galois.com/news/15m-iarpa-hector-contract-privacy-preserving-technology/"> </a>     .             ,     .  .     ,   ,          . <br><br>        ,   <a href=""> ToyFHE</a> .   <a href="https://juliacomputing.github.io/ToyFHE.jl/dev/man/background/"></a> , ,   ,         . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de478514/">https://habr.com/ru/post/de478514/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de478500/index.html">Warum gibt es im Sommer so wenige Konferenzen?</a></li>
<li><a href="../de478502/index.html">Intelligente Technologie f√ºr alle</a></li>
<li><a href="../de478504/index.html">Wie sich der Arbeitsplatz mit der Entwicklung des Laptops ver√§ndert</a></li>
<li><a href="../de478508/index.html">Telegramm als Dienstleistung</a></li>
<li><a href="../de478510/index.html">Wir laden Sie zum DINS QA EVENING 12/12/19 ein: Erstellen Sie eine Jenkins-Pipeline und lernen Sie, wie Sie mit deren Hilfe den Start von Tests parallelisieren k√∂nnen</a></li>
<li><a href="../de478516/index.html">Verfahrensstra√üen in Houdini und Unity</a></li>
<li><a href="../de478518/index.html">Erfahren Sie, wie Sie Office-Infrastrukturen unter Zextras / Zimbra OSE bereitstellen</a></li>
<li><a href="../de478522/index.html">Gib es zu, Watson, bist du v√∂llig verbl√ºfft?</a></li>
<li><a href="../de478528/index.html">Haustier (eine fantastische Geschichte)</a></li>
<li><a href="../de478530/index.html">TechnoText-2019: Wer hat am Ende gewonnen und wof√ºr waren sie?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>