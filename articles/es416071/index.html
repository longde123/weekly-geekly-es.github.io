<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüè´ üöΩ üëêüèæ Redes neuronales, principios fundamentales de funcionamiento, diversidad y topolog√≠a. üöµüèº üë©üèø‚Äçüíª ü§¶üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Las redes neuronales han revolucionado el campo del reconocimiento de patrones, pero debido a la interpretaci√≥n no obvia del principio de funcionamien...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neuronales, principios fundamentales de funcionamiento, diversidad y topolog√≠a.</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416071/">  Las redes neuronales han revolucionado el campo del reconocimiento de patrones, pero debido a la interpretaci√≥n no obvia del principio de funcionamiento, no se utilizan en √°reas como la medicina y la evaluaci√≥n de riesgos.  Requiere una representaci√≥n visual de la red, lo que har√° que no sea una caja negra, sino al menos "transl√∫cida".  <b>Cristopher Olah, en Redes neuronales, colectores y topolog√≠a, demostr√≥ los principios del funcionamiento de la red neuronal y los conect√≥ con la teor√≠a matem√°tica de la topolog√≠a y la diversidad, que sirvi√≥ de base para este art√≠culo.</b>  Para demostrar el funcionamiento de una red neuronal, se utilizan redes neuronales profundas de baja dimensi√≥n. <br><br>  Comprender el comportamiento de las redes neuronales profundas generalmente no es una tarea trivial.  Es m√°s f√°cil explorar redes neuronales profundas de baja dimensi√≥n, redes en las que solo hay unas pocas neuronas en cada capa.  Para redes de baja dimensi√≥n, puede crear visualizaciones para comprender el comportamiento y la capacitaci√≥n de dichas redes.  Esta perspectiva proporcionar√° una comprensi√≥n m√°s profunda del comportamiento de las redes neuronales y observar√° la conexi√≥n que combina las redes neuronales con un campo de las matem√°ticas llamado topolog√≠a. <br><br>  De esto se desprenden varias cosas interesantes, incluidos los l√≠mites inferiores fundamentales sobre la complejidad de una red neuronal capaz de clasificar ciertos conjuntos de datos. <br><br>  Considere el principio de la red usando un ejemplo <br><a name="habracut"></a><br>  Comencemos con un conjunto de datos simple: dos curvas en un plano.  La tarea de red aprender√° a clasificar los puntos que pertenecen a las curvas. <br><br><img src="https://habrastorage.org/webt/m4/od/cv/m4odcvui3bls-vx-zzfhprjatzg.png"><br><br>  Una forma obvia de visualizar el comportamiento de una red neuronal, para ver c√≥mo el algoritmo clasifica todos los objetos posibles (en nuestro ejemplo, puntos) de un conjunto de datos. <br><br>  Comencemos con la clase m√°s simple de red neuronal, con una capa de entrada y salida.  Dicha red intenta separar dos clases de datos dividi√©ndolos por una l√≠nea. <br><br><img src="https://habrastorage.org/webt/ii/sz/eo/iiszeobw-_fe8mam77ku7o62g7g.png"><br><br>  Tal red no se usa en la pr√°ctica.  Las redes neuronales modernas generalmente tienen varias capas entre su entrada y salida, llamadas capas "ocultas". <br><br><img src="https://habrastorage.org/webt/ej/bp/dw/ejbpdwhjcvzouhxnir9a-zeavye.jpeg"><br><br><h3>  Diagrama de red simple </h3><br>  Visualizamos el comportamiento de esta red, observando lo que hace con diferentes puntos en su campo.  Una red de capa oculta separa los datos de una curva m√°s compleja que una l√≠nea. <br><br><img src="https://habrastorage.org/webt/wp/cr/vf/wpcrvf_a0tiqrhftmxhddovy8tk.png"><br><br>  Con cada capa, la red transforma los datos, creando una nueva vista.  Podemos ver los datos en cada una de estas vistas y c√≥mo la red con una capa oculta los clasifica.  Cuando el algoritmo llega a la presentaci√≥n final, la red neuronal dibujar√° una l√≠nea a trav√©s de los datos (o en dimensiones m√°s altas, un hiperplano). <br><br>  En la visualizaci√≥n anterior, se consideran los datos en una vista sin procesar.  Puedes imaginar esto mirando la capa de entrada.  Ahora, consid√©relo despu√©s de convertirlo a la primera capa.  Puedes imaginar esto mirando la capa oculta. <br>  Cada medida corresponde a la activaci√≥n de una neurona en la capa. <br><br><img src="https://habrastorage.org/webt/fc/ha/ch/fchachtyqdnmtcgaxonj6d1xiu0.png"><br><br>  La capa oculta se entrena en la vista para que los datos sean linealmente separables. <br><br>  <b>Representaci√≥n continua de capas</b> <br><br>  En el enfoque descrito en la secci√≥n anterior, aprendemos a comprender las redes al observar la presentaci√≥n correspondiente a cada capa.  Esto nos da una lista discreta de puntos de vista. <br><br>  La parte no trivial es comprender c√≥mo nos movemos de uno a otro.  Afortunadamente, los niveles de la red neuronal tienen propiedades que lo hacen posible. <br>  Hay muchos tipos diferentes de capas usadas en redes neuronales. <br><br>  Considere una capa de tanh para un ejemplo espec√≠fico.  Tanh-tanh-layer (Wx + b) consta de: <br><br><ol><li>  La transformaci√≥n lineal de la matriz de "peso" W </li><li>  Traducci√≥n usando el vector b </li><li>  Aplicaci√≥n puntual de tanh. </li></ol><br>  Podemos representar esto como una transformaci√≥n continua de la siguiente manera: <br><br><img src="https://habrastorage.org/webt/cn/vl/rb/cnvlrblwsnmp9mxxzbawagywuj0.gif"><br><br>  Este principio de funcionamiento es muy similar a otras capas est√°ndar que consisten en una transformaci√≥n af√≠n, seguida de la aplicaci√≥n puntual de una funci√≥n de activaci√≥n monot√≥nica. <br>  Este m√©todo puede usarse para comprender redes m√°s complejas.  Entonces, la siguiente red clasifica dos espirales que est√°n ligeramente enredadas usando cuatro capas ocultas.  Con el tiempo, se puede ver que la red neuronal se mueve desde una vista sin procesar a un nivel superior que la red ha estudiado para clasificar los datos.  Si bien las espirales se enredan inicialmente, hacia el final son linealmente separables. <br><br><img src="https://habrastorage.org/webt/g0/-y/kg/g0-ykgrem8udnrx-xiua2rolthq.gif"><br><br>  Por otro lado, la siguiente red, que tambi√©n usa varios niveles, pero no puede clasificar dos espirales, que est√°n m√°s enredados. <br><br><img src="https://habrastorage.org/webt/gv/f3/wc/gvf3wc_-a7yw2h-3odoobbsie7u.gif"><br><br>  Cabe se√±alar que estas tareas tienen una complejidad limitada, porque se utilizan redes neuronales de baja dimensi√≥n.  Si se utilizaron redes m√°s amplias, la resoluci√≥n de problemas se simplific√≥. <br><br><h3>  Capas Tang </h3><br>  Cada capa estira y comprime el espacio, pero nunca corta, no se rompe y no lo dobla.  Intuitivamente, vemos que las propiedades topol√≥gicas se conservan en cada capa. <br><br>  Tales transformaciones que no afectan la topolog√≠a se denominan homomorfismos (Wiki - Este es un mapeo del sistema algebraico A que preserva las operaciones b√°sicas y las relaciones b√°sicas).  Formalmente, son biyecciones que son funciones continuas en ambas direcciones.  En una asignaci√≥n biyectiva, cada elemento de un conjunto corresponde exactamente a un elemento de otro conjunto, y se define una asignaci√≥n inversa que tiene la misma propiedad. <br><br>  <b>El teorema</b> <br><br>  Las capas con N entradas y N salidas son homomorfismos si la matriz de peso W no est√° degenerada.  (Debe tener cuidado con el dominio y el rango). <br><br><div class="spoiler">  <b class="spoiler_title">Prueba:</b> <div class="spoiler_text">  1. Suponga que W tiene un determinante distinto de cero.  Entonces es una funci√≥n lineal biyectiva con un inverso lineal.  Las funciones lineales son continuas.  Entonces, la multiplicaci√≥n por W es un homeomorfismo. <br>  2. Mapeos - homomorfismos <br>  3. tanh (tanto sigmoide como softplus, pero no ReLU) son funciones continuas con inversas continuas.  Son biyecciones si tenemos cuidado con el √°rea y el rango que estamos considerando.  Su uso puntual es un homomorfismo. <br><br>  Por lo tanto, si W tiene un determinante distinto de cero, la fibra es homeom√≥rfica. <br></div></div><br><h3>  Topolog√≠a y clasificaci√≥n. </h3><br>  Considere un conjunto de datos bidimensionales con dos clases A, B‚äÇR2: <br><br>  A = {x |  d (x, 0) &lt;1/3} <br><br>  B = {x |  2/3 &lt;d (x, 0) &lt;1} <br><br><img src="https://habrastorage.org/webt/ow/2l/b1/ow2lb1ozxk5p4d-s4ho_lq6_ds8.png"><br><br>  A rojo, B azul <br><br>  Requisito: una red neuronal no puede clasificar este conjunto de datos sin 3 o m√°s capas ocultas, independientemente del ancho. <br><br>  Como se mencion√≥ anteriormente, la clasificaci√≥n con una funci√≥n sigmoidea o capa softmax es equivalente a tratar de encontrar el hiperplano (o en este caso la l√≠nea) que separa A y B en la representaci√≥n final.  Con solo dos capas ocultas, la red es topol√≥gicamente incapaz de compartir datos de esta manera, y est√° condenada al fracaso en este conjunto de datos. <br>  En la siguiente visualizaci√≥n, observamos una vista latente mientras la red est√° entrenando junto con la l√≠nea de clasificaci√≥n. <br><br><img src="https://habrastorage.org/webt/ap/nt/xe/apntxeprybgn8yf_jchwskwes44.gif"><br><br>  Para esta red de capacitaci√≥n no es suficiente para lograr un resultado cien por ciento. <br>  El algoritmo cae en un m√≠nimo local no productivo, pero puede lograr una precisi√≥n de clasificaci√≥n de ~ 80%. <br><br>  En este ejemplo, solo hab√≠a una capa oculta, pero no funcion√≥. <br>  Declaraci√≥n  O cada capa es un homomorfismo, o la matriz de peso de la capa tiene determinante 0. <br><br><div class="spoiler">  <b class="spoiler_title">Prueba:</b> <div class="spoiler_text">  Si esto es un homomorfismo, entonces A todav√≠a est√° rodeado por B, y la l√≠nea no puede separarlos.  Pero supongamos que tiene un determinante de 0: entonces el conjunto de datos colapsa en alg√∫n eje.  Dado que estamos tratando con algo homeomorfo al conjunto de datos original, A est√° rodeado por B, y el colapso en cualquier eje significa que tendremos algunos puntos de A y B mezclados, y esto hace que sea imposible distinguirlos. <br></div></div><br>  Si agregamos un tercer elemento oculto, el problema se volver√° trivial.  La red neuronal reconoce la siguiente representaci√≥n: <br><br><img src="https://habrastorage.org/webt/y1/p8/ol/y1p8olobp-zdo3shlaooa62hy8k.png"><br><br>  La vista permite separar conjuntos de datos con un hiperplano. <br>  Para comprender mejor lo que est√° sucediendo, veamos un conjunto de datos a√∫n m√°s simple, que es unidimensional: <br><br><img src="https://habrastorage.org/webt/ud/by/hr/udbyhrfnqgfdr9r9lvh4cg7apw0.png"><br><br>  A = [- 1 / 3,1 / 3] <br>  B = [- 1, ‚àí2 / 3] ‚à™ [2 / 3,1] <br>  Sin usar una capa de dos o m√°s elementos ocultos, no podemos clasificar este conjunto de datos.  Pero, si usamos una red con dos elementos, aprenderemos c√≥mo representar los datos como una buena curva que nos permite separar las clases usando una l√≠nea: <br><br><img src="https://habrastorage.org/webt/8_/w-/bo/8_w-boyjlwhtufafsnqpljuo8u0.gif"><br><br>  Que esta pasando  Un elemento oculto aprende a disparar cuando x&gt; -1/2, y uno aprende a disparar cuando x&gt; 1/2.  Cuando se activa el primero, pero no el segundo, sabemos que estamos en A. <br><br><h3>  Conjetura de la variedad </h3><br>  ¬øSe aplica esto a los conjuntos de datos del mundo real, como los conjuntos de im√°genes?  Si te tomas en serio la hip√≥tesis de la diversidad, creo que es importante. <br><br>  La hip√≥tesis multidimensional es que los datos naturales forman m√∫ltiples de baja dimensi√≥n en el espacio de implantaci√≥n.  Hay razones te√≥ricas [1] y experimentales [2] para creer que esto es cierto.  Si es as√≠, entonces la tarea del algoritmo de clasificaci√≥n es separar el paquete de m√∫ltiples enredados. <br><br>  En los ejemplos anteriores, una clase rodeaba completamente a la otra.  Sin embargo, es poco probable que la variedad de im√°genes de perros est√© completamente rodeada por una colecci√≥n de im√°genes de gatos.  Pero hay otras situaciones topol√≥gicas m√°s plausibles que a√∫n pueden surgir, como veremos en la siguiente secci√≥n. <br><br><h3>  Conexiones y homotop√≠as </h3><br>  Otro conjunto de datos interesante son los dos toros conectados A y B. <br><br><img src="https://habrastorage.org/webt/wl/er/ns/wlernsr6ibyfnuq2cbkepwgxxq8.png"><br><br>  Al igual que los conjuntos de datos anteriores que examinamos, este conjunto de datos no se puede dividir sin usar n + 1 dimensiones, es decir, la cuarta dimensi√≥n. <br><br>  Las conexiones se estudian en la teor√≠a de los nudos, en el campo de la topolog√≠a.  A veces, cuando vemos una conexi√≥n, no est√° claro de inmediato si es incoherencia (muchas cosas que se enredan pero que pueden separarse por deformaci√≥n continua) o no. <br><br><img src="https://habrastorage.org/webt/lg/1h/bu/lg1hbu1mehjrj872e61gd73_3vi.png"><br><br>  Incoherencia relativamente simple. <br><br>  Si una red neuronal que usa capas con solo tres unidades puede clasificarla, entonces es incoherente.  (Pregunta: ¬øPueden te√≥ricamente clasificarse todas las incoherencias en la red con solo tres incoherencias?) <br><br>  Desde el punto de vista de este nodo, la visualizaci√≥n continua de representaciones creadas por una red neuronal es un procedimiento para desentra√±ar conexiones.  En topolog√≠a, llamaremos a esta isotop√≠a ambiental entre el enlace original y los separados. <br><br>  Formalmente, la isotop√≠a del espacio circundante entre los colectores A y B es una funci√≥n continua F: [0,1] √ó X ‚Üí Y, de modo que cada Ft es un homeomorfismo de X a su rango, F0 es una funci√≥n de identidad y F1 asigna A a B. T .e.  Ft va continuamente del mapa A a s√≠ mismo, al mapa A a B. <br><br>  Teorema: hay una isotop√≠a del espacio circundante entre la entrada y la representaci√≥n del nivel de red si: a) W no est√° degenerado, b) estamos listos para transferir neuronas a la capa oculta yc) hay m√°s de 1 elemento oculto. <br><br><div class="spoiler">  <b class="spoiler_title">Prueba:</b> <div class="spoiler_text">  1. La parte m√°s dif√≠cil es la transformaci√≥n lineal.  Para hacer esto posible, necesitamos que W tenga un determinante positivo.  Nuestra premisa es que no es igual a cero, y podemos revertir el signo si es negativo cambiando dos neuronas ocultas, y por lo tanto podemos garantizar que el determinante sea positivo.  El espacio de las matrices determinantes positivas est√° conectado, por lo tanto, existe p: [0,1] ‚Üí GLn ¬Æ5 tal que p (0) = Id y p (1) = W. Podemos pasar continuamente de la funci√≥n de identidad a la transformaci√≥n W usando funciona x ‚Üí p (t) x, multiplicando x en cada punto de tiempo t por una matriz que pasa continuamente p (t). <br>  2. Podemos movernos continuamente desde la funci√≥n de identidad al mapa b usando la funci√≥n x ‚Üí x + tb. <br>  3. Podemos pasar continuamente de la funci√≥n id√©ntica al uso puntual de œÉ con la funci√≥n: x ‚Üí (1-t) x + tœÉ (x) <br></div></div><br>  Hasta ahora, es poco probable que las relaciones de las que hablamos aparezcan en datos reales, pero hay generalizaciones de un nivel superior.  Es plausible que tales caracter√≠sticas puedan existir en datos reales. <br><br>  Las conexiones y los nodos son m√∫ltiples unidimensionales, pero necesitamos 4 dimensiones para que las redes puedan desentra√±arlos a todos.  De manera similar, se puede requerir un espacio dimensional a√∫n mayor para poder expandir m√∫ltiples n-dimensionales.  Todos los colectores n-dimensionales se pueden expandir en 2n + 2 dimensiones.  [3] <br><br><h3>  Salida f√°cil </h3><br>  La manera m√°s f√°cil es tratar de separar los colectores y estirar las partes que est√©n lo m√°s enredadas posible.  Aunque esto no estar√° cerca de una soluci√≥n genuina, dicha soluci√≥n puede lograr una precisi√≥n de clasificaci√≥n relativamente alta y ser un m√≠nimo local aceptable. <br><br><img src="https://habrastorage.org/webt/7x/mf/fp/7xmffpy2eilztftxrcbv69xhsf4.png"><br><br>  Tales m√≠nimos locales son absolutamente in√∫tiles en t√©rminos de tratar de resolver problemas topol√≥gicos, pero los problemas topol√≥gicos pueden proporcionar una buena motivaci√≥n para estudiar estos problemas. <br><br>  Por otro lado, si solo estamos interesados ‚Äã‚Äãen lograr buenos resultados de clasificaci√≥n, el enfoque es aceptable.  Si una peque√±a cantidad de una variedad de datos queda atrapada en otra variedad, ¬øes esto un problema?  Es probable que sea posible obtener resultados de clasificaci√≥n arbitrariamente buenos, a pesar de este problema. <br><br>  ¬øCapas mejoradas para manipular m√∫ltiples? <br><br>  Es dif√≠cil imaginar que las capas est√°ndar con transformaciones afines sean realmente buenas para manipular m√∫ltiples. <br><br>  ¬øQuiz√°s tiene sentido tener una capa completamente diferente, que podemos usar en la composici√≥n con otras m√°s tradicionales? <br><br>  El estudio de un campo vectorial con una direcci√≥n en la que queremos cambiar la variedad es prometedor: <br><br><img src="https://habrastorage.org/webt/2z/iw/at/2ziwat9d2bjwlclnjrixwmm5llc.png"><br><br>  Y luego deformamos el espacio en funci√≥n del campo vectorial: <br><br><img src="https://habrastorage.org/webt/ig/qb/dr/igqbdrqbcl3gy85kv4rzbhirflk.png"><br><br>  Se podr√≠a estudiar el campo vectorial en puntos fijos (solo tomar algunos puntos fijos del conjunto de datos de prueba para usar como anclajes) e interpolar de alguna manera. <br><br><div class="spoiler">  <b class="spoiler_title">El campo vectorial de arriba tiene la forma:</b> <div class="spoiler_text">  P (x) = (v0f0 (x) + v1f1 (x)) / (1 + 0 (x) + f1 (x)) <br></div></div><br>  Donde v0 y v1 son vectores, y f0 (x) y f1 (x) son gaussianos n-dimensionales. <br><br><h3>  K-Capas vecinas m√°s cercanas </h3><br>  La separabilidad lineal puede ser una necesidad enorme y posiblemente irrazonable de redes neuronales.  Es natural utilizar el m√©todo k-vecinos m√°s cercanos (k-NN).  Sin embargo, el √©xito de k-NN depende en gran medida de la presentaci√≥n que clasifica, por lo que se requiere una buena presentaci√≥n antes de que k-NN pueda funcionar bien. <br><br>  k-NN es diferenciable con respecto a la representaci√≥n sobre la que act√∫a.  De esta manera, podemos entrenar directamente a la red para clasificar k-NN.  Esto puede verse como un tipo de capa de "vecino m√°s cercano" que act√∫a como una alternativa a softmax. <br>  No queremos advertir con todo nuestro conjunto de entrenamiento para cada mini-fiesta, porque ser√° un procedimiento muy costoso.  El enfoque adaptado consiste en clasificar cada elemento del mini lote en funci√≥n de las clases de los otros elementos del mini lote, dando a cada unidad de peso dividido por la distancia desde el objetivo de clasificaci√≥n. <br><br>  Desafortunadamente, incluso con arquitecturas complejas, el uso de k-NN reduce la probabilidad de error, y el uso de arquitecturas m√°s simples degrada los resultados. <br><br><h3>  Conclusi√≥n </h3><br>  Las propiedades topol√≥gicas de los datos, como las relaciones, pueden hacer imposible la divisi√≥n lineal de clases usando redes de baja dimensi√≥n, independientemente de la profundidad.  Incluso en casos donde es t√©cnicamente posible.  Por ejemplo, espirales, que pueden ser muy dif√≠ciles de separar. <br><br>  Para una clasificaci√≥n de datos precisa, las redes neuronales necesitan capas anchas.  Adem√°s, las capas tradicionales de la red neuronal son poco adecuadas para representar manipulaciones importantes con m√∫ltiples;  incluso si establecemos los pesos manualmente, ser√≠a dif√≠cil representar de forma compacta las transformaciones que queremos. <br><br><div class="spoiler">  <b class="spoiler_title">Enlaces a fuentes y explicaciones</b> <div class="spoiler_text">  [1] Muchas de las transformaciones naturales que tal vez quieras realizar en una imagen, como traducir o escalar un objeto en ella, o cambiar la iluminaci√≥n, formar√≠an curvas continuas en el espacio de la imagen si las realizaras continuamente. <br><br>  [2] Carlsson y col.  descubri√≥ que parches locales de im√°genes forman una botella de Klein. <br>  [3] Este resultado se menciona en la subsecci√≥n de Wikipedia sobre las versiones de Isotopy. <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es416071/">https://habr.com/ru/post/es416071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es416059/index.html">Mobio habla con Daniil Shuleiko (Yandex.Taxi) sobre fusi√≥n con Uber, mercado de taxis y competencia</a></li>
<li><a href="../es416061/index.html">M√°s o menos, lo veo todo</a></li>
<li><a href="../es416063/index.html">Las negociaciones de los rusos no tienen d√≥nde registrar</a></li>
<li><a href="../es416067/index.html">Descripci√≥n general de la vulnerabilidad de Mikrotik Winbox. O un archivo grande</a></li>
<li><a href="../es416069/index.html">Migraci√≥n de datos sin p√©rdida de ElasticSearch</a></li>
<li><a href="../es416073/index.html">Un simple bot de comercio de criptomonedas</a></li>
<li><a href="../es416075/index.html">El FSB quiere presentar la responsabilidad por el uso oculto de grabadoras de voz y c√°maras en tel√©fonos inteligentes [y no solo]</a></li>
<li><a href="../es416077/index.html">PlantUML: todo lo que los analistas empresariales necesitan para crear gr√°ficos en la documentaci√≥n del software</a></li>
<li><a href="../es416079/index.html">Corona Native para Android: uso de c√≥digo Java personalizado en un juego escrito en Corona</a></li>
<li><a href="../es416081/index.html">Algo sigue mal con el regreso a Habr</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>