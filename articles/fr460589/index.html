<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§• üë®‚Äçüé® üë©üèø‚Äçüç≥ √âcrire un r√©seau neuronal simple en utilisant les math√©matiques et Numpy üë©‚Äçüë©‚Äçüë¶‚Äçüë¶ üçò üë®üèæ‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pourquoi le prochain article sur la fa√ßon d'√©crire des r√©seaux de neurones √† partir de z√©ro? H√©las, je n'ai pas pu trouver d'articles o√π la th√©orie et...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>√âcrire un r√©seau neuronal simple en utilisant les math√©matiques et Numpy</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/460589/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/5_/h5/0t/5_h50teeaqngyx0ccp8cqjqfumm.jpeg" width="350"></div><br>  Pourquoi le prochain article sur la fa√ßon d'√©crire des r√©seaux de neurones √† partir de z√©ro?  H√©las, je n'ai pas pu trouver d'articles o√π la th√©orie et le code ont √©t√© d√©crits de A √† Z pour un mod√®le pleinement fonctionnel.  J'avertis imm√©diatement qu'il y aura beaucoup de math√©matiques.  Je suppose que le lecteur conna√Æt les bases de l'alg√®bre lin√©aire, les d√©riv√©es partielles et au moins partiellement la th√©orie des probabilit√©s, ainsi que Python et Numpy.  Nous traiterons d'un r√©seau neuronal enti√®rement connect√© et du MNIST. <br><a name="habracut"></a><br><h2>  Math  Partie 1 (simple) </h2><br>  Qu'est-ce qu'une couche enti√®rement connect√©e (couche FC)?  Habituellement, ils disent quelque chose comme ¬´Une couche enti√®rement connect√©e est une couche, dont chaque neurone est connect√© √† tous les neurones de la couche pr√©c√©dente¬ª.  Ce n'est tout simplement pas clair ce que sont les neurones, comment ils sont connect√©s, en particulier dans le code.  Maintenant, je vais essayer d'analyser cela avec un exemple.  Soit une couche de 100 neurones.  Je sais que je n'ai pas encore expliqu√© de quoi il s'agit, mais imaginons simplement qu'il y a 100 neurones et qu'ils ont une entr√©e d'o√π les donn√©es sont envoy√©es et une sortie d'o√π ils donnent les donn√©es.  Et une image en noir et blanc de 28x28 pixels est envoy√©e √† l'entr√©e - seulement 784 valeurs, si vous l'√©tirez en un vecteur.  Une image peut √™tre appel√©e couche d'entr√©e.  Ensuite, pour que chacun des 100 neurones se connecte avec chaque "neurone" ou, si vous le souhaitez, la valeur de la couche pr√©c√©dente (c'est-√†-dire l'image), il est n√©cessaire que chacun des 100 neurones accepte 784 valeurs de l'image d'origine.  Par exemple, pour chacun des 100 neurones, il suffira de multiplier 784 valeurs de l'image par quelques 784 nombres et de les additionner, par cons√©quent, un nombre sort.  Autrement dit, ceci est un neurone: <br><p><math> </math> $$ afficher $$ \ text {sortie Neuron} = \ text {un certain nombre} _ {1} \ cdot \ text {valeur d'image} _1 ~ + \\ + ~ ... ~ + ~ \ text {some- ce nombre} _ {784} \ cdot \ text {valeur d'image} _ {784} $$ display $$ </p><br>  Ensuite, il s'av√®re que chaque neurone a 784 nombres, et tous ces nombres: (nombre de neurones sur cette couche) x (nombre de neurones sur la couche pr√©c√©dente) = <math> </math> $ en ligne $ 100 \ times784 $ en ligne $   = 78 400 chiffres.  Ces nombres sont commun√©ment appel√©s poids des couches.  Chaque neurone donnera son num√©ro et par cons√©quent nous obtenons un vecteur √† 100 dimensions, et en fait, nous pouvons √©crire que ce vecteur √† 100 dimensions est obtenu en multipliant le vecteur √† 784 dimensions (notre image originale) par une matrice de poids de taille <math> </math> $ en ligne $ 100 \ times784 $ en ligne $   : <br><p><math> </math> $$ afficher $$ \ boldsymbol {x} ^ {100} = W_ {100 \ times784} \ cdot \ boldsymbol {x} ^ {784} $$ afficher $$ </p><br><br>  De plus, les 100 nombres r√©sultants sont transmis √† la fonction d'activation - une fonction non lin√©aire - qui affecte chaque nombre s√©par√©ment.  Par exemple, sigmo√Øde, tangente hyperbolique, ReLU et autres.  La fonction d'activation est n√©cessairement non lin√©aire, sinon le r√©seau neuronal n'apprendra que des transformations simples. <br><br><img src="https://habrastorage.org/webt/0j/rl/ba/0jrlbaqv0486mryhqj32u8et0cw.png"><br><br>  Ensuite, les donn√©es r√©sultantes sont √† nouveau transmises √† une couche enti√®rement connect√©e, mais avec un nombre diff√©rent de neurones, et √† nouveau √† la fonction d'activation.  Cela se produit plusieurs fois.  La derni√®re couche du r√©seau est la couche qui produit la r√©ponse.  Dans ce cas, la r√©ponse est une information sur le num√©ro dans l'image. <br><br><img src="https://habrastorage.org/webt/9f/73/q-/9f73q-feve3kb5k9u5fxvmz4kxk.png"><br><br>  Lors de la formation du r√©seau, il est n√©cessaire que nous sachions quelle figure est montr√©e sur l'image.  Autrement dit, l'ensemble de donn√©es est balis√©.  Ensuite, vous pouvez utiliser un autre √©l√©ment - la fonction d'erreur.  Elle regarde la r√©ponse du r√©seau neuronal et la compare √† la vraie r√©ponse.  Gr√¢ce √† cela, le r√©seau neuronal apprend. <br><br><h2>  √ânonc√© g√©n√©ral du probl√®me </h2><br>  L'ensemble de donn√©es est un grand tenseur (nous appellerons un tableau de donn√©es multidimensionnel un tenseur) <math> </math> $ inline $ \ boldsymbol {X} = \ left [\ boldsymbol {x} _1, \ boldsymbol {x} _2, \ ldots, \ boldsymbol {x} _n \ right] $ inline $   o√π <math> </math> $ inline $ \ boldsymbol {x} _i $ inline $   - i-√®me objet, par exemple, une image, qui est aussi un tenseur.  Pour chaque objet il y a <math> </math> $ inline $ y_i $ inline $   - la bonne r√©ponse sur le i-√®me objet.  Dans ce cas, un r√©seau neuronal peut √™tre repr√©sent√© comme une fonction qui prend un objet en entr√©e et donne une r√©ponse √† ce sujet: <br><p><math> </math> $$ afficher $$ F (\ boldsymbol {x} _i) = \ chapeau {y} _i $$ afficher $$ </p><br>  Examinons maintenant de plus pr√®s la fonction <math> </math> $ inline $ F (\ boldsymbol {x} _i) $ inline $   .  Puisque le r√©seau neuronal est constitu√© de couches, chaque couche individuelle est une fonction.  Et cela signifie <br><p><math> </math> $$ afficher $$ F (\ boldsymbol {x} _i) = f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i)))) = \ hat {y} _i $$ afficher $ $ </p><br>  Autrement dit, dans la toute premi√®re fonction - la premi√®re couche - une image est pr√©sent√©e sous la forme d'un tenseur.  Fonction <math> </math> $ inline $ f_1 $ inline $   donne une r√©ponse - aussi un tenseur, mais d'une dimension diff√©rente.  Ce tenseur sera appel√© la repr√©sentation interne.  Maintenant, cette repr√©sentation interne est aliment√©e √† l'entr√©e de la fonction <math> </math> $ inline $ f_2 $ inline $   , qui donne sa repr√©sentation interne.  Et ainsi de suite, jusqu'√† ce que la fonction <math> </math> $ inline $ f_k $ inline $   - derni√®re couche - ne donnera pas de r√©ponse <math> </math> $ inline $ \ hat {y} _i $ inline $   . <br><br>  Maintenant, la t√¢che est de former le r√©seau - pour que la r√©ponse du r√©seau corresponde √† la bonne r√©ponse.  Vous devez d'abord mesurer √† quel point le r√©seau de neurones est incorrect.  Mesurer cela est une fonction d'erreur. <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   .  Et nous imposons des restrictions: <br><br>  1. <math> </math> $ inline $ \ hat {y} _i \ xrightarrow {} y_i \ Rightarrow L (\ hat {y} _i, y_i) \ xrightarrow {} 0 $ inline $ <br>  2. <math> </math> $ inline $ \ existe ~ dL (\ hat {y} _i, y_i) $ inline $ <br>  3. <math> </math> $ inline $ L (\ hat {y} _i, y_i) \ geq 0 $ inline $ <br><br>  La restriction 2 est impos√©e sur toutes les fonctions des couches <math> </math> $ inline $ f_j $ inline $   - qu'ils soient tous diff√©renciables. <br><br>  De plus, en fait (je n'ai pas mentionn√© cela) certaines de ces fonctions d√©pendent des param√®tres - les poids du r√©seau neuronal - <math> </math> $ inline $ f_j (\ boldsymbol {x} _i | \ boldsymbol {\ omega} _j) $ inline $   .  Et l'id√©e est de prendre de tels poids pour que <math> </math> $ inline $ \ hat {y} _i $ inline $   co√Øncid√© avec <math> </math> $ inline $ y_i $ inline $   sur tous les objets d'un jeu de donn√©es.  Je note que toutes les fonctions n'ont pas de poids. <br><br>  Alors, o√π en sommes-nous?  Toutes les fonctions du r√©seau neuronal sont diff√©renciables, la fonction d'erreur est √©galement diff√©renciable.  Rappelez-vous l'une des propri√©t√©s du gradient - montrer la direction de croissance de la fonction.  Nous utilisons cela, les restrictions 1 et 3, le fait que <br><p><math> </math> $$ affiche $$ L (F (\ boldsymbol {x} _i)) = L (f_k (f_ {k-1} (\ ldots (f_1 (\ boldsymbol {x} _i))))))) = L (\ hat {y} _i) $$ afficher $$ </p><br>  et le fait que je puisse consid√©rer les d√©riv√©es partielles et les d√©riv√©es d'une fonction complexe.  Maintenant, il y a tout ce dont vous avez besoin pour calculer <br><p><math> </math> $$ afficher $$ \ frac {\ partiel L (F (\ boldsymbol {x} _i))} {\ partiel \ boldsymbol {\ omega_j}} $$ afficher $$ </p><br>  pour tout i et j.  Cette d√©riv√©e partielle montre la direction dans laquelle changer <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   agrandir <math> </math> $ en ligne $ L $ en ligne $   .  Pour r√©duire, vous devez faire un pas de c√¥t√© <math> </math> $ inline $ - \ frac {\ partial L (F (\ boldsymbol {x} _i))} {\ partial \ boldsymbol {\ omega_j}} $ inline $   rien de compliqu√©. <br><br>  Ainsi, le processus de formation du r√©seau est construit comme suit: plusieurs fois dans un cycle, nous parcourons l'ensemble de donn√©es (c'est ce qu'on appelle une √®re), pour chaque objet de l'ensemble de donn√©es que nous consid√©rons <math> </math> $ inline $ L (\ hat {y} _i, y_i) $ inline $   (c'est ce que l'on appelle la passe avant) et consid√©rons la d√©riv√©e partielle <math> </math> $ inline $ \ partiel L $ inline $   pour tous les poids <math> </math> $ inline $ \ boldsymbol {\ omega_j} $ inline $   , puis mettez √† jour les poids (c'est ce que l'on appelle le passage en arri√®re). <br><br>  Je note que je n'ai pas encore introduit de fonctions et de couches sp√©cifiques.  Si √† ce stade, on ne sait pas quoi faire de tout cela, je propose de continuer √† lire - il y aura plus de math√©matiques, mais maintenant il ira avec des exemples. <br><br><h2>  Math  Partie 2 (difficile) </h2><br><h3>  Fonction d'erreur </h3><br>  Je vais commencer par la fin et d√©river la fonction d'erreur pour le probl√®me de classification.  Pour le probl√®me de r√©gression, la d√©rivation de la fonction d'erreur est bien d√©crite dans le livre ¬´Deep Learning.  Immersion dans le monde des r√©seaux de neurones. " <br><br>  Pour simplifier, il existe un r√©seau de neurones (NN) qui s√©pare les photos de chats des photos de chiens, et il existe un ensemble de photos de chats et de chiens pour lesquelles il existe une bonne r√©ponse. <math> </math> $ inline $ y_ {true} $ inline $   . <br><p><math> </math> $$ afficher $$ NN (image | \ Omega) = y_ {pred} $$ afficher $$ </p><br>  Tout ce que je vais faire ensuite est tr√®s similaire √† la m√©thode du maximum de vraisemblance.  Par cons√©quent, la t√¢che principale consiste √† trouver la fonction de vraisemblance.  Si nous omettons les d√©tails, alors une telle fonction qui compare la pr√©diction du r√©seau neuronal et la bonne r√©ponse, et si elles co√Øncident, donne une grande valeur, sinon, vice versa.  La probabilit√© d'une r√©ponse correcte vient √† l'esprit avec les param√®tres donn√©s: <br><p><math> </math> $$ afficher $$ p (y_ {pred} = y_ {vrai} | \ Omega) $$ afficher $$ </p><br>  Et maintenant, nous allons faire une feinte, qui, semble-t-il, ne suit nulle part.  Que le r√©seau neuronal donne une r√©ponse sous la forme d'un vecteur bidimensionnel dont la somme des valeurs est 1. Le premier √©l√©ment de ce vecteur peut √™tre appel√© une mesure de confiance que le chat est sur la photo et le deuxi√®me √©l√©ment la mesure de confiance que le chien est sur la photo.  Oui, c'est presque une probabilit√©! <br><p><math> </math> $$ afficher $$ NN (image | \ Omega) = \ gauche [\ commencer {matrice} p_0 \\ p_1 \\\ fin {matrice} \ droite] $$ afficher $$ </p><br>  Maintenant, la fonction de vraisemblance peut √™tre r√©√©crite comme suit: <br><p><math> </math> $$ affiche $$ p (y_ {pred} = y_ {true} | \ Omega) = p_ \ Omega (y_ {pred}) ^ t_ {0} * (1 - p_ \ Omega (y_ {pred})) ^ t_ {1} = \\ p_0 ^ {t_0} * p_1 ^ {t_1} $$ afficher $$ </p><br>  O Where <math> </math> $ inline $ t_0, t_1 $ inline $   les √©tiquettes de la bonne classe, par exemple, si <math> </math> $ inline $ y_ {true} = cat $ inline $   alors <math> </math> $ inline $ t_0 == 1, t_1 == 0 $ inline $   si <math> </math> $ inline $ y_ {true} = chien $ inline $   alors <math> </math> $ inline $ t_0 == 0, t_1 == 1 $ inline $   .  Ainsi, la probabilit√© d'une classe qui aurait d√ª √™tre pr√©dite par un r√©seau de neurones (mais pas n√©cessairement pr√©dite par lui) est toujours consid√©r√©e.  Maintenant, cela peut √™tre g√©n√©ralis√© √† n'importe quel nombre de classes (par exemple, m classes): <br><p><math> </math> $$ afficher $$ p (y_ {pred} = y_ {vrai} | \ Omega) = \ prod_0 ^ m p_i ^ {t_i} $$ afficher $$ </p><br>  Cependant, dans tout jeu de donn√©es, il existe de nombreux objets (par exemple, N objets).  Je veux que le r√©seau neuronal donne la bonne r√©ponse sur chacun ou la plupart des objets.  Et pour cela, vous devez multiplier les r√©sultats de la formule ci-dessus pour chaque objet de l'ensemble de donn√©es. <br><p><math> </math> $$ afficher $$ MaximumL vraisemblance = \ prod_ {j = 0} ^ N \ prod_ {i = 0} ^ m p_ {i, j} ^ {t_ {i, j}} $$ afficher $$ </p><br>  Pour obtenir de bons r√©sultats, cette fonction doit √™tre maximis√©e.  Mais, premi√®rement, il est plus raide √† minimiser, car nous avons une descente de gradient stochastique et tous les petits pains pour cela - attribuez simplement un moins, et deuxi√®mement, il est difficile de travailler avec un travail √©norme - c'est le logarithme. <br><p><math> </math> $$ afficher $$ CrossEntropyLoss = - \ sum \ limits_ {j = 0} ^ {N} \ sum \ limits_ {i = 0} ^ {m} t_ {i, j} \ cdot \ log (p_ {i, j }) $$ afficher $$ </p><br>  Super!  Le r√©sultat √©tait une entropie crois√©e ou, dans le cas binaire, une perte de log.  Cette fonction est facile √† compter et encore plus facile √† diff√©rencier: <br><p><math> </math> $$ display $$ \ frac {\ partial CrossEntropyLoss} {\ partial p_j} = - \ frac {\ boldsymbol {t_j}} {\ boldsymbol {p_ {j}}} $$ display $$ </p><br>  Vous devez diff√©rencier l'algorithme de r√©tropropagation.  Je note que la fonction d'erreur ne change pas la dimension du vecteur.  Si, comme dans le cas de MNIST, la sortie est un vecteur de r√©ponses √† 10 dimensions, alors lors du calcul de la d√©riv√©e, nous obtenons un vecteur √† 10 dimensions de d√©riv√©es.  Une autre chose int√©ressante est qu'un seul √©l√©ment de la d√©riv√©e ne sera pas nul, auquel <math> </math> $ inline $ t_ {i, j} \ neq 0 $ inline $   , c'est-√†-dire avec la bonne r√©ponse.  Et moins la probabilit√© d'une r√©ponse correcte est pr√©dite par un r√©seau de neurones sur un objet donn√©, plus la fonction d'erreur y sera. <br><br><h3>  Fonctionnalit√©s d'activation </h3><br>  √Ä la sortie de chaque couche enti√®rement connect√©e d'un r√©seau de neurones, une fonction d'activation non lin√©aire doit √™tre pr√©sente.  Sans lui, il est impossible de former un r√©seau neuronal significatif.  Pour l'avenir, une couche enti√®rement connect√©e d'un r√©seau de neurones est simplement une multiplication des donn√©es d'entr√©e par une matrice de poids.  En alg√®bre lin√©aire, cela s'appelle une carte lin√©aire - une fonction lin√©aire.  La combinaison de fonctions lin√©aires est √©galement une fonction lin√©aire.  Mais cela signifie qu'une telle fonction ne peut qu'approcher des fonctions lin√©aires.  H√©las, ce n'est pas pourquoi des r√©seaux de neurones sont n√©cessaires. <br><br><h4>  Softmax </h4><br>  Habituellement, cette fonction est utilis√©e sur la derni√®re couche du r√©seau, car elle transforme le vecteur de la derni√®re couche en un vecteur de ¬´probabilit√©s¬ª: chaque √©l√©ment du vecteur se situe de 0 √† 1 et leur somme est 1. Elle ne change pas la dimension du vecteur. <br><p><math> </math> $$ afficher $$ Softmax_i = \ frac {e ^ {x_i}} {\ sum \ limits_ {j} e ^ {x_j}} $$ afficher $$ </p><br>  Passons maintenant √† la recherche d√©riv√©e.  Depuis <math> </math> $ inline $ \ boldsymbol {x} $ inline $   Est un vecteur, et tous ses √©l√©ments sont toujours pr√©sents dans le d√©nominateur, puis en prenant la d√©riv√©e on obtient le jacobien: <br><p><math> </math> $$ display $$ J_ {Softmax} = \ begin {cases} x_i - x_i \ cdot x_j, i = j \\ - x_i \ cdot x_j, i \ neq j \ end {cases} $$ display $$ </p><br>  Maintenant sur la r√©tropropagation.  Le vecteur des d√©riv√©s provient de la couche pr√©c√©dente (il s'agit g√©n√©ralement d'une fonction d'erreur) <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   .  Au cas o√π <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   provenait d'une fonction d'erreur sur mnist, <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   - Vecteur √† 10 dimensions.  Ensuite, le jacobien a une dimension de 10x10.  Pour obtenir <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   , qui va plus loin dans la couche pr√©c√©dente (n'oubliez pas que l'on va de la fin au d√©but du r√©seau quand l'erreur se propage), il faut multiplier <math> </math> $ inline $ \ boldsymbol {dz} $ inline $   sur <math> </math> $ inline $ J_ {Softmax} $ inline $   (ligne par colonne): <br><p><math> </math> $$ afficher $$ dz_ {nouveau} = \ boldsymbol {dz} \ fois J_ {Softmax} $$ afficher $$ </p><br>  En sortie, nous obtenons un vecteur √† 10 dimensions de d√©riv√©es <math> </math> $ inline $ \ boldsymbol {dz_ {new}} $ inline $   . <br><br><h4>  Relu </h4><br><p><math> </math> $$ display $$ ReLU (x) = \ begin {cases} x, x&gt; 0 \\ 0, x &lt;0 \ end {cases} $$ display $$ </p><br>  ReLU a commenc√© √† √™tre massivement utilis√© apr√®s 2011, lorsque l'article ¬´Deep Sparse Rectifier Neural Networks¬ª a √©t√© publi√©.  Cependant, une telle fonction √©tait auparavant connue.  Le concept de ¬´puissance d'activation¬ª est applicable √† ReLU (pour plus de d√©tails, voir le livre ¬´Deep Learning. Immersion dans le monde des r√©seaux de neurones¬ª).  Mais la principale caract√©ristique qui rend ReLU plus attrayant que les autres fonctions d'activation est son calcul d√©riv√© simple: <br><p><math> </math> $$ display $$ d (ReLU (x)) = \ begin {cases} 1, x&gt; 0 \\ 0, x &lt;0 \ end {cases} $$ display $$ </p><br>  Ainsi, ReLU est plus efficace sur le plan informatique que les autres fonctions d'activation (sigmo√Øde, tangente hyperbolique, etc.). <br><br><h3>  Couche enti√®rement connect√©e </h3><br>  Il est maintenant temps de discuter d'une couche enti√®rement connect√©e.  Le plus important de tous les autres, car c'est dans cette couche que se trouvent tous les poids, qui doivent √™tre param√©tr√©s pour que le r√©seau neuronal fonctionne bien.  Une couche enti√®rement connect√©e est simplement une matrice de poids: <br><p><math> </math> $$ afficher $$ W = | w_ {i, j} | $$ afficher $$ </p><br>  Une nouvelle repr√©sentation interne est obtenue lorsque la matrice de poids est multipli√©e par la colonne d'entr√©e: <br><p><math> </math> $$ afficher $$ \ boldsymbol {x} _ {nouveau} = W \ cdot \ boldsymbol {x} $$ afficher $$ </p><br>  O Where <math> </math> $ inline $ \ boldsymbol {x} $ inline $   a la taille <math> </math> $ inline $ input \ _shape $ inline $   et <math> </math> $ inline $ x_ {new} $ inline $   - <math> </math> $ inline $ output \ _shape $ inline $   .  Par exemple <math> </math> $ inline $ \ boldsymbol {x} $ inline $   - vecteur 784 dimensions, et <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   Est un vecteur √† 100 dimensions, alors la matrice W a une taille de 100x784.  Il se trouve que sur cette couche est 100x784 = 78 400 poids. <br><br>  Avec la r√©tropropagation de l'erreur, il faut prendre la d√©riv√©e par rapport √† chaque poids de cette matrice.  Simplifiez le probl√®me et ne prenez que la d√©riv√©e par rapport √† <math> </math> $ inline $ w_ {1,1} $ inline $   .  Lors de la multiplication de la matrice et du vecteur, le premier √©l√©ment du nouveau vecteur <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   est √©gal √† <math> </math> $ inline $ x_ {new ~ 1} = w_ {1,1} \ cdot x_1 + ... + w_ {1,784} \ cdot x_ {784} $ inline $   et le d√©riv√© <math> </math> $ inline $ x_ {new ~ 1} $ inline $   par <math> </math> $ inline $ w_ {1,1} $ inline $   sera simple <math> </math> $ en ligne $ x_1 $ en ligne $   , il vous suffit de prendre le d√©riv√© du montant ci-dessus.  Il en va de m√™me pour tous les autres poids.  Mais ce n'est pas un algorithme de propagation de retour d'erreur, tant qu'il ne s'agit que d'une matrice de d√©riv√©s.  Vous devez vous rappeler que de la couche suivante √† celle-ci (l'erreur va de la fin au d√©but) vient un vecteur de gradient √† 100 dimensions <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   .  Premier √©l√©ment de ce vecteur <math> </math> $ inline $ dz_1 $ inline $   sera multipli√© par tous les √©l√©ments de la matrice des d√©riv√©s qui "ont particip√©" √† la cr√©ation <math> </math> $ inline $ x_ {new ~ 1} $ inline $   , c'est-√†-dire le <math> </math> $ en ligne $ x_1, x_2, ..., x_ {784} $ en ligne $   .  De m√™me, le reste des √©l√©ments.  Si vous traduisez cela dans le langage de l'alg√®bre lin√©aire, alors c'est √©crit comme ceci: <br><p><math> </math> $$ display $$ \ frac {\ partial L} {\ partial W} = (d \ boldsymbol {z}, ~ dW) = \ left (\ begin {matrix} dz_ {1} \ cdot \ boldsymbol {x} \ \ ... \\ dz_ {100} \ cdot \ boldsymbol {x} \ end {matrix} \ right) _ {100} $$ display $$ </p><br>  La sortie est une matrice 100x784. <br><img src="https://habrastorage.org/webt/1m/8_/hl/1m8_hljpr28gm3dikkgpsk4zss8.png"><br><br>  Vous devez maintenant comprendre quoi transf√©rer vers la couche pr√©c√©dente.  Pour cela et pour une meilleure compr√©hension de ce qui s'est pass√© maintenant, je veux √©crire ce qui s'est pass√© lors de la prise de d√©riv√©s sur cette couche dans un langage l√©g√®rement diff√©rent, pour √©chapper aux sp√©cificit√©s de ¬´ce qui est multipli√© par¬ª aux fonctions (encore). <br><br>  Quand je voulais ajuster les poids, je voulais prendre la d√©riv√©e de la fonction d'erreur pour ces poids: <math> </math> $ inline $ \ frac {\ partial L} {\ partial W} $ inline $   .  Il a √©t√© montr√© ci-dessus comment prendre des d√©riv√©es des fonctions d'erreur et des fonctions d'activation.  Par cons√©quent, nous pouvons consid√©rer un tel cas (en <math> </math> $ inline $ d \ boldsymbol {z} $ inline $   toutes les d√©riv√©es de la fonction d'erreur et des fonctions d'activation sont d√©j√† en place): <br><p><math> </math> $$ afficher $$ \ frac {\ partial L} {\ partial W} = d \ boldsymbol {z} \ cdot \ frac {\ partial \ boldsymbol {x} _ {new} (W)} {\ partial W} $ $ afficher $$ </p><br>  Cela peut √™tre fait, car vous pouvez envisager <math> </math> $ inline $ \ boldsymbol {x} _ {new} $ inline $   en fonction de W: <math> </math> $ inline $ \ boldsymbol {x} _ {new} = W \ cdot \ boldsymbol {x} $ inline $   . <br>  Vous pouvez remplacer cela dans la formule ci-dessus: <br><br><p><math> </math> $$ afficher $$ \ frac {\ partial L} {\ partial W} = d \ boldsymbol {z} \ cdot \ frac {\ partial W \ cdot \ boldsymbol {x}} {\ partial W} = d \ boldsymbol { z} \ cdot E \ cdot \ boldsymbol {x} $$ display $$ </p><br>  O√π E est une matrice compos√©e d'unit√©s (PAS une matrice d'unit√©s). <br><br>  Maintenant, lorsque vous devez prendre la d√©riv√©e de la couche pr√©c√©dente (m√™me si pour la simplicit√© des calculs, ce sera √©galement une couche enti√®rement connect√©e, mais dans le cas g√©n√©ral, cela ne change rien), alors vous devez consid√©rer <math> </math> $ inline $ \ boldsymbol {x} $ inline $   en fonction de la couche pr√©c√©dente <math> </math> $ inline $ \ boldsymbol {x} (W_ {old}) $ inline $   : <br><p><math> </math> $$ afficher $$ \ begin {r√©unis} \ frac {\ partial L} {\ partial W_ {old}} = d \ boldsymbol {z} \ cdot \ frac {\ partial \ boldsymbol {x} _ {nouveau} (W )} {\ partial W_ {old}} = d \ boldsymbol {z} \ cdot \ frac {\ partial W \ cdot \ boldsymbol {x} (W_ {old})} {\ partial W_ {old}} = \\ = d \ boldsymbol {z} \ cdot \ frac {\ partial W \ cdot W_ {old} \ cdot \ boldsymbol {x} _ {old}} {\ partial W_ {old}} = d \ boldsymbol {z} \ cdot W \ cdot E \ cdot \ boldsymbol {x} _ {old} = \\ = d \ boldsymbol {z} _ {new} \ cdot E \ cdot \ boldsymbol {x} _ {old} \ end {r√©unis} $$ afficher $$ </p><br>  Exactement <math> </math> $ inline $ d \ boldsymbol {z} _ {new} = d \ boldsymbol {z} \ cdot W $ inline $   et vous devez envoyer √† la couche pr√©c√©dente. <br><br><h2>  Code </h2><br><blockquote>  Cet article vise principalement √† expliquer les math√©matiques des r√©seaux de neurones.  Je vais consacrer tr√®s peu de temps au code. </blockquote><br>  Voici un exemple d'impl√©mentation de la fonction d'erreur: <br><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CrossEntropy</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, y_true, y_hat)</span></span></span><span class="hljs-function">:</span></span> self.y_hat = y_hat self.y_true = y_true self.loss = -np.sum(self.y_true * np.log(y_hat)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.loss <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> dz = -self.y_true / self.y_hat <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dz</code> </pre> <br>  La classe a des m√©thodes pour le passage direct et inverse.  Au moment de la passe directe, l'instance de classe stocke les donn√©es √† l'int√©rieur de la couche et, au moment de la passe de retour, elle les utilise pour calculer le gradient.  Les couches restantes sont construites de la m√™me mani√®re.  Gr√¢ce √† cela, il devient possible d'√©crire un neurone enti√®rement connect√© dans ce style: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MnistNet</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.d1_layer = Dense(<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) self.a1_layer = ReLu() self.drop1_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>) self.d2_layer = Dense(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>) self.a2_layer = ReLu() self.drop2_layer = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) self.d3_layer = Dense(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) self.a3_layer = Softmax() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x, train=True)</span></span></span><span class="hljs-function">:</span></span> ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">backward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, dz, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.01</span></span></span></span><span class="hljs-function"><span class="hljs-params">, mini_batch=True, update=False, len_mini_batch=None)</span></span></span><span class="hljs-function">:</span></span> ...</code> </pre><br>  Le code complet peut √™tre trouv√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br>  Je conseille √©galement d'√©tudier cet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article sur Habr√©</a> . <br><br><h2>  Conclusion </h2><br>  J'esp√®re avoir pu expliquer et montrer que les math√©matiques assez simples sont derri√®re les r√©seaux de neurones et que ce n'est pas du tout effrayant.  N√©anmoins, pour une compr√©hension plus approfondie, il vaut la peine d'essayer d'√©crire votre propre ¬´v√©lo¬ª.  Les corrections et suggestions sont heureuses de lire dans les commentaires. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr460589/">https://habr.com/ru/post/fr460589/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr460567/index.html">React Native: cr√©ez une liste glissable et glissable</a></li>
<li><a href="../fr460569/index.html">Logiciel d'√©criture avec la fonctionnalit√© des utilitaires client-serveur Windows, partie 01</a></li>
<li><a href="../fr460573/index.html">Google d√©clare que ¬´reCAPTCHA¬ª n'abuse pas des donn√©es utilisateur. Vaut-il la peine de croire?</a></li>
<li><a href="../fr460577/index.html">Vive le roi: monde cruel de la hi√©rarchie dans une meute de chiens errants</a></li>
<li><a href="../fr460587/index.html">Module sans fil pour capteur capacitif d'humidit√© du sol sur nRF52832</a></li>
<li><a href="../fr460591/index.html">Obtention de la racine sur un routeur Tenda Nova MW6</a></li>
<li><a href="../fr460593/index.html">"Universel" dans l'√©quipe de d√©veloppement: avantage ou inconv√©nient?</a></li>
<li><a href="../fr460597/index.html">Comment diagnostiquer les probl√®mes d'int√©gration du SDK. L'exp√©rience de l'√©quipe de d√©veloppement du SDK Yandex Mobile Ads</a></li>
<li><a href="../fr460599/index.html">Nouvelles du monde d'OpenStreetMap n ¬∞ 468 (07/02/2019 - 08/07/2019)</a></li>
<li><a href="../fr460603/index.html">V2G. Les voitures √©lectriques aideront √† √©quilibrer la production et la consommation d'√©lectricit√©</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>