<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîÑ üèîÔ∏è ü§µüèΩ Cassandra Sink para Spark Structured Streaming ‚öíÔ∏è üë©üèº‚Äçüöí üöÇ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hace un par de meses comenc√© a estudiar Spark, y en alg√∫n momento me encontr√© con el problema de guardar los c√°lculos de Streaming Estructurado en la ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cassandra Sink para Spark Structured Streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/425503/">  Hace un par de meses comenc√© a estudiar Spark, y en alg√∫n momento me encontr√© con el problema de guardar los c√°lculos de Streaming Estructurado en la base de datos de Cassandra. <br><br>  En esta publicaci√≥n, doy un ejemplo simple de creaci√≥n y uso de Cassandra Sink para Spark Structured Streaming.  Espero que la publicaci√≥n sea √∫til para aquellos que recientemente comenzaron a trabajar con Spark Structured Streaming y se preguntan c√≥mo cargar los resultados del c√°lculo en la base de datos. <br><br>  La idea de la aplicaci√≥n es muy simple: recibir y analizar mensajes de Kafka, realizar transformaciones simples en un par y guardar los resultados en cassandra. <br><a name="habracut"></a><br><h3>  Ventajas de la transmisi√≥n estructurada </h3><br>  Puede leer m√°s sobre la transmisi√≥n estructurada en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">documentaci√≥n</a> .  En resumen, Structured Streaming es un motor de procesamiento de informaci√≥n de transmisi√≥n bien escalable que se basa en el motor Spark SQL.  Le permite usar un conjunto de datos / marco de datos para agregar datos, calcular funciones de ventana, conexiones, etc. Es decir, la transmisi√≥n estructurada le permite usar el buen SQL antiguo para trabajar con flujos de datos. <br><br><h3>  Cual es el problema </h3><br>  El lanzamiento estable de Spark Structured Streaming se lanz√≥ en 2017.  Es decir, esta es una API bastante nueva que implementa la funcionalidad b√°sica, pero algunas cosas deber√°n ser realizadas por nosotros mismos.  Por ejemplo, Structured Streaming tiene funciones est√°ndar para escribir resultados en un archivo, mosaico, consola o memoria, pero para guardar los datos en la base de datos, debe usar el receptor <i>foreach</i> disponible en Structured Streaming e implementar la interfaz <i>ForeachWriter</i> .  <b>A partir de Spark 2.3.1, esta funcionalidad solo se puede implementar en Scala y Java</b> . <br><br>  Supongo que el lector ya sabe c√≥mo funciona Structured Streaming en t√©rminos generales, sabe c√≥mo implementar las transformaciones necesarias y ahora est√° listo para cargar los resultados en la base de datos.  Si alguno de los pasos anteriores no est√° claro, la documentaci√≥n oficial puede servir como un buen punto de partida para aprender la Transmisi√≥n Estructurada.  En este art√≠culo, me gustar√≠a centrarme en el √∫ltimo paso cuando necesite guardar los resultados en una base de datos. <br><br>  A continuaci√≥n, describir√© un ejemplo de implementaci√≥n del sumidero Cassandra para la transmisi√≥n estructurada y explicar√© c√≥mo ejecutarlo en un cl√∫ster.  El c√≥digo completo est√° disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . <br><br>  Cuando encontr√© por primera vez el problema anterior, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">este proyecto</a> result√≥ ser muy √∫til.  Sin embargo, puede parecer un poco complicado si el lector acaba de comenzar a trabajar con Structured Streaming y est√° buscando un ejemplo simple de c√≥mo cargar datos en Cassandra.  Adem√°s, el proyecto est√° escrito para funcionar en modo local y requiere algunos cambios para ejecutarse en el cl√∫ster. <br><br>  Tambi√©n quiero dar ejemplos de c√≥mo guardar datos en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MongoDB</a> y cualquier otra base de datos usando <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">JDBC</a> . <br><br><h3>  Soluci√≥n simple </h3><br>  Para cargar datos a un sistema externo, debe usar el receptor <i>foreach</i> .  Lea m√°s sobre esto <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> .  En resumen, se debe implementar la interfaz <i>ForeachWriter</i> .  Es decir, es necesario determinar c√≥mo abrir la conexi√≥n, c√≥mo procesar cada dato y c√≥mo cerrar la conexi√≥n al final del procesamiento.  El c√≥digo fuente es el siguiente: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  La definici√≥n de <i>CassandraDriver</i> y la estructura de la tabla de salida que describir√© m√°s adelante, pero por ahora, echemos un vistazo m√°s de cerca a c√≥mo funciona el c√≥digo anterior.  Para conectarme a Kasandra desde Spark, creo un objeto <i>CassandraDriver</i> que proporciona acceso a <i>CassandraConnector</i> , un conector desarrollado por <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">DataStax</a> .  CassandraConnector es responsable de abrir y cerrar la conexi√≥n a la base de datos, por lo que simplemente visualizo mensajes de depuraci√≥n en los m√©todos de <i>apertura</i> y <i>cierre</i> de la clase <i>CassandraSinkForeach</i> . <br><br>  El c√≥digo anterior se llama desde la aplicaci√≥n principal de la siguiente manera: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> se crea para cada fila de datos, por lo que cada nodo de trabajo inserta su parte de las filas en la base de datos.  Es decir, cada nodo de trabajo ejecuta <i>val cassandraDriver = new CassandraDriver ();</i>  As√≠ es como se ve CassandraDriver: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Echemos un vistazo m√°s de cerca al objeto de <i>chispa</i> .  El c√≥digo para <i>SparkSessionBuilder es el</i> siguiente: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  En cada nodo de trabajo, <i>SparkSessionBuilder</i> proporciona acceso a la <i>SparkSession</i> que se cre√≥ en el controlador.  Para hacer posible dicho acceso, es necesario serializar <i>SparkSessionBuilder</i> y usar el valor <i>vago <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=" class="user_link">transitorio</a></i> , que permite que el sistema de serializaci√≥n ignore los objetos <i>conf</i> y <i>spark</i> cuando el programa se inicializa y hasta que se accede a los objetos.  Por lo tanto, cuando se inicia el programa, <i>buildSparkSession se</i> serializa y se env√≠a a cada nodo de trabajo, pero los objetos <i>conf</i> y <i>spark</i> solo se permiten cuando el nodo de trabajo est√° accediendo a ellos. <br><br>  Ahora veamos el c√≥digo de la aplicaci√≥n principal: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  Cuando la aplicaci√≥n se env√≠a para su ejecuci√≥n, <i>buildSparkSession se</i> serializa y se env√≠a a los nodos de trabajo, sin embargo, los objetos <i>conf</i> y <i>spark</i> permanecen sin resolver.  Luego, el controlador crea un objeto de chispa dentro de <i>KafkaToCassandra</i> y distribuye el trabajo entre los nodos de trabajo.  Cada nodo de trabajo lee datos de Kafka, realiza transformaciones simples en la parte recibida de los registros, y cuando el nodo de trabajo est√° listo para escribir los resultados en la base de datos, permite <i>conf</i> y objetos de <i>chispa</i> , obteniendo as√≠ acceso a la <i>SparkSession</i> creada en el controlador. <br><br><h3>  ¬øC√≥mo construir y ejecutar la aplicaci√≥n? </h3><br>  Cuando me mud√© de PySpark a Scala, me llev√≥ un tiempo descubrir c√≥mo construir la aplicaci√≥n.  Por lo tanto, <i>inclu√≠</i> Maven <i>pom.xml</i> en mi proyecto.  El lector puede construir la aplicaci√≥n usando Maven ejecutando el comando <i>mvn package</i> .  Despu√©s de que la aplicaci√≥n se puede enviar para su ejecuci√≥n usando <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  Para construir y ejecutar la aplicaci√≥n, es necesario reemplazar los nombres de mis m√°quinas AWS con los suyos (es decir, reemplazar todo lo que parece ec2-xx-xxx-xx-xx.compute-1.amazonaws.com). <br><br>  Spark and Structured Streaming en particular es un tema nuevo para m√≠, por lo que agradecer√© mucho a los lectores por sus comentarios, discusiones y correcciones. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es425503/">https://habr.com/ru/post/es425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es425489/index.html">Automatizaci√≥n: amenaza exagerada del robot</a></li>
<li><a href="../es425493/index.html">Configuraci√≥n de MikroTik hAP mini para IPTV Beeline</a></li>
<li><a href="../es425497/index.html">Tutu PHP Meetup # 2: transmisi√≥n de eventos en vivo</a></li>
<li><a href="../es425499/index.html">HyperX Impact DDR4: ¬°SO-DIMM que podr√≠a! ¬øO por qu√© en una computadora port√°til 64 GB de memoria con una frecuencia de 3200 MHz?</a></li>
<li><a href="../es425501/index.html">Pruebas A / B en Android de la A a la Z</a></li>
<li><a href="../es425505/index.html">An√°lisis del proceso de arranque del kernel de Linux</a></li>
<li><a href="../es425507/index.html">Parsim Wikipedia para tareas de PNL en 4 equipos</a></li>
<li><a href="../es425511/index.html">Caracter√≠sticas no obvias de la aplicaci√≥n Rotativa para generar PDF en la aplicaci√≥n ASP.NET MVC</a></li>
<li><a href="../es425515/index.html">Apple bloquea la reparaci√≥n independiente de los nuevos modelos de MacBook</a></li>
<li><a href="../es425517/index.html">C√≥mo Yandex cre√≥ un pron√≥stico de precipitaci√≥n global utilizando radares y sat√©lites</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>