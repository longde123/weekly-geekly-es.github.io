<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üå∫ ü§æüèø üî£ Analyse de la coloration √©motionnelle des critiques de Kinopoisk üñ≤Ô∏è üó®Ô∏è üëò</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Entr√©e 
 Le traitement du langage naturel (PNL) est un domaine populaire et important de l'apprentissage automatique. Dans ce hub, je d√©crirai mon pre...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analyse de la coloration √©motionnelle des critiques de Kinopoisk</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467081/"><h3>  Entr√©e </h3><br>  Le traitement du langage naturel (PNL) est un domaine populaire et important de l'apprentissage automatique.  Dans ce hub, je d√©crirai mon premier projet li√© √† l'analyse de la coloration √©motionnelle des critiques de films √©crites en Python.  La t√¢che de l'analyse sentimentale est assez courante chez ceux qui veulent ma√Ætriser les concepts de base de la PNL, et peut devenir un analogue du ¬´Hello world¬ª dans ce domaine. <br><br>  Dans cet article, nous passerons par toutes les √©tapes principales du processus Data Science: de la cr√©ation de votre propre ensemble de donn√©es, du traitement et de l'extraction des fonctionnalit√©s √† l'aide de la biblioth√®que NLTK, et enfin √† l'apprentissage et au r√©glage du mod√®le √† l'aide de scikit-learn.  La t√¢che elle-m√™me consiste √† classer les avis en trois classes: n√©gatifs, neutres et positifs. <br><a name="habracut"></a><br><h3>  Formation de Corpus de donn√©es </h3><br>  Pour r√©soudre ce probl√®me, on pourrait utiliser un corps de donn√©es pr√™t √† l'emploi et annot√© avec des critiques d'IMDB, dont il y en a beaucoup sur GitHub.  Mais il a √©t√© d√©cid√© de cr√©er le v√¥tre avec des critiques en russe tir√©es de Kinopoisk.  Afin de ne pas les copier manuellement, nous √©crirons un analyseur Web.  J'utiliserai la biblioth√®que de <i>requ√™tes</i> pour envoyer des <i>requ√™tes</i> http et <i>BeautifulSoup</i> pour traiter les fichiers html.  Tout d'abord, d√©finissons une fonction qui prendra un lien vers les critiques de films et les r√©cup√©rera.  Pour que Kinopoisk ne reconnaisse pas le bot en nous, vous devez sp√©cifier l'argument <i>headers</i> dans la fonction requests.get, qui simulera le navigateur.  Il est n√©cessaire de lui passer un dictionnaire avec les cl√©s User-Agent, Accept-language et Accept, dont les valeurs se trouvent dans les outils de d√©veloppement du navigateur.  Ensuite, un analyseur est cr√©√© et les avis sont r√©cup√©r√©s √† partir de la page, qui sont stock√©s dans la classe de balisage html _reachbanner_. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> r = requests.get(url, headers = headers) <span class="hljs-comment"><span class="hljs-comment">#  http  soup = BeautifulSoup(r.text, 'html.parser')#  html  reviews = soup.find_all(class_='_reachbanner_')#    reviews_clean = [] for review in reviews:#    html  reviews_clean.append(review.find_all(text=True)) return reviews_clean</span></span></code> </pre> <br>  Nous nous sommes d√©barrass√©s du balisage html, cependant, nos avis sont toujours des objets <i>BeautifulSoup</i> , mais nous devons les convertir en cha√Ænes.  La fonction de <i>conversion</i> fait exactement cela.  Nous allons √©galement √©crire une fonction qui r√©cup√®re le nom du film, qui sera ensuite utilis√©e pour enregistrer les critiques. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convert</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(reviews)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     review_converted = [] for review in reviews: for i in review: map(str, i) review = ''.join(review) review_converted.append(review) return review_converted def get_name(url): #    r = requests.get(url, headers = headers) soup = BeautifulSoup(r.text, 'html.parser') name = soup.find(class_='alternativeHeadline') name_clean = name.find_all(text = True) #   , . .     return str(name_clean[0])</span></span></code> </pre><br>  La derni√®re fonction de l'analyseur prendra un lien vers la page principale du film, une classe de r√©vision et un moyen de sauvegarder les critiques.  La fonction d√©finit √©galement les <i>d√©lais</i> entre les demandes qui sont n√©cessaires pour √©viter une interdiction.  La fonction contient une boucle qui r√©cup√®re et stocke les avis √† partir de la premi√®re page, jusqu'√† ce qu'elle rencontre une page inexistante √† partir de laquelle la fonction <i>load_data</i> extraira une liste vide et la boucle se rompra. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parsing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url, status, path)</span></span></span><span class="hljs-function">:</span></span> page = <span class="hljs-number"><span class="hljs-number">1</span></span> delays = [<span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">13</span></span>, <span class="hljs-number"><span class="hljs-number">11.5</span></span>, <span class="hljs-number"><span class="hljs-number">12.5</span></span>, <span class="hljs-number"><span class="hljs-number">13.5</span></span>, <span class="hljs-number"><span class="hljs-number">11.2</span></span>, <span class="hljs-number"><span class="hljs-number">12.3</span></span>, <span class="hljs-number"><span class="hljs-number">11.8</span></span>] name = get_name(url) time.sleep(np.random.choice(delays)) <span class="hljs-comment"><span class="hljs-comment">#    while True: loaded_data = load_data(url + 'reviews/ord/date/status/{}/perpage/200/page/{}/'.format(status, page)) if loaded_data == []: break else: # E     ,    if not os.path.exists(path + r'\{}'.format(status)): os.makedirs(path + r'\{}'.format(status)) converted_data = convert(loaded_data) #   for i, review in enumerate(converted_data): with open(path + r'\{}\{}_{}_{}.txt'.format(status, name, page, i), 'w', encoding = 'utf-8') as output: output.write(review) page += 1 time.sleep(np.random.choice(delays))</span></span></code> </pre><br>  Ensuite, en utilisant le cycle suivant, vous pouvez extraire des critiques de films qui sont dans la liste des <i>urles</i> .  Une liste de films devra √™tre cr√©√©e manuellement.  Il serait possible, par exemple, d'obtenir une liste de liens vers des films en √©crivant une fonction qui les extrairait des 250 meilleurs films d'une recherche de film, afin de ne pas le faire manuellement, mais 15-20 films suffiraient pour former un petit ensemble de donn√©es de mille critiques pour chaque classe.  De plus, si vous obtenez une interdiction, le programme affichera sur quel film et quelle classe l'analyseur s'est arr√™t√© pour continuer au m√™me endroit apr√®s avoir pass√© l'interdiction. <br><br><pre> <code class="python hljs">path = <span class="hljs-comment"><span class="hljs-comment">#    urles = #    statuses = ['good', 'bad', 'neutral'] delays = [15, 20, 13, 18, 12.5, 13.5, 25, 12.3, 23] for url in urles: for status in statuses: try: parsing(url = url, status = status, path=path) print('one category done') time.sleep(np.random.choice(delays)) #       AttributeError except AttributeError: print(' : {}, {}'.format(url, status)) break #  else  ,      #    ,     else: print('one url done') continue break</span></span></code> </pre><br><h3>  Pr√©traitement </h3><br>  Apr√®s avoir √©crit l'analyseur, se rappelant des films al√©atoires pour lui et plusieurs interdictions de la recherche de film, j'ai m√©lang√© les critiques dans des dossiers et s√©lectionn√© 900 critiques de chaque classe pour la formation et le reste pour le groupe t√©moin.  Maintenant, il est n√©cessaire de pr√©traiter le bo√Ætier, √† savoir le tokenize et le normaliser.  Tokeniser signifie d√©composer le texte en composants, en l'occurrence en mots, puisque nous utiliserons la repr√©sentation d'un sac de mots.  Et la normalisation consiste √† convertir les mots en minuscules, √† supprimer les mots vides et les bruits excessifs, le stamming et toutes autres astuces qui aident √† r√©duire l'espace des signes. <br><br>  Nous importons les biblioth√®ques n√©cessaires. <br><br><div class="spoiler">  <b class="spoiler_title">Texte masqu√©</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.corpus <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PlaintextCorpusReader <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.probability <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FreqDist <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.tokenize <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RegexpTokenizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> bigrams <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pos_tag <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OrderedDict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report, accuracy_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.naive_bayes <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MultinomialNB <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shuffle <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pool <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix</code> </pre><br></div></div><br>  Nous commen√ßons par d√©finir quelques petites fonctions de pr√©traitement de texte.  Le premier, appel√© <i>lower_pos_tag,</i> prendra une liste de mots, les convertira en minuscules et enregistrera chaque jeton dans un tuple avec sa partie de discours.  L'op√©ration consistant √† ajouter une partie de la parole √† un mot est appel√©e balisage de partie de la parole (POS) et est souvent utilis√©e dans la PNL pour extraire des entit√©s.  Dans notre cas, nous utiliserons des parties du discours dans la fonction suivante pour filtrer les mots. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lower_pos_tag</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> lower_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: lower_words.append(i.lower()) pos_words = pos_tag(lower_words, lang=<span class="hljs-string"><span class="hljs-string">'rus'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> pos_words</code> </pre><br>  Les textes contiennent un grand nombre de mots jug√©s trop souvent utiles au mod√®le (les mots dits stop).  Fondamentalement, ce sont des pr√©positions, des conjonctions, des pronoms par lesquels il est impossible de d√©terminer √† quelle classe se r√©f√®re le rappel.  La fonction <i>propre</i> ne laisse que des noms, des adjectifs, des verbes et des adverbes.  Notez qu'il supprime des parties du discours, car elles ne sont pas n√©cessaires pour le mod√®le lui-m√™me.  Vous pouvez √©galement remarquer que cette fonction utilise le stamming, dont l'essence est de supprimer les suffixes et les pr√©fixes des mots.  Cela vous permet de r√©duire la dimension des signes, car les mots avec des genres et des cas diff√©rents seront r√©duits au m√™me jeton.  Il existe un analogue plus puissant du stamming - la lemmatisation, il vous permet de restaurer la forme initiale du mot.  Cependant, cela fonctionne plus lentement que le stamming, et, en plus, NLTK n'a pas de lemmatiseur russe. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clean</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(words)</span></span></span><span class="hljs-function">:</span></span> stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) cleaned_words = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> [<span class="hljs-string"><span class="hljs-string">'S'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'V'</span></span>, <span class="hljs-string"><span class="hljs-string">'ADV'</span></span>]: cleaned_words.append(stemmer.stem(i[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cleaned_words</code> </pre><br>  Ensuite, nous √©crivons la fonction finale qui prendra l'√©tiquette de classe et r√©cup√©rera toutes les revues avec cette classe.  Pour lire le cas, nous utiliserons la m√©thode <i>brute</i> de l'objet <i>PlaintextCorpusReader</i> , qui vous permet d'extraire du texte du fichier sp√©cifi√©.  Ensuite, la tokenisation est utilis√©e RegexpTokenizer, fonctionnant sur la base d'une expression r√©guli√®re.  En plus des mots individuels, j'ai ajout√© au mod√®le des bigrammes, qui sont des combinaisons de tous les mots voisins.  Cette fonction utilise √©galement l'objet <i>FreqDist</i> , qui renvoie la fr√©quence d'occurrence des mots.  Il est utilis√© ici pour supprimer les mots qui n'apparaissent dans toutes les critiques d'une classe particuli√®re qu'une seule fois (ils sont √©galement appel√©s hapaks).  Ainsi, la fonction renverra un dictionnaire contenant des documents pr√©sent√©s comme un sac de mots et une liste de tous les mots pour une classe particuli√®re. <br><br><pre> <code class="python hljs">corpus_root = <span class="hljs-comment"><span class="hljs-comment">#    def process(label): # Wordmatrix -     # All words -    data = {'Word_matrix': [], 'All_words': []} #      templist_allwords = [] #        corpus = PlaintextCorpusReader(corpus_root + '\\' + label, '.*', encoding='utf-8') #       names = corpus.fileids() #   tokenizer = RegexpTokenizer(r'\w+|[^\w\s]+') for i in range(len(names)): #   bag_words = tokenizer.tokenize(corpus.raw(names[i])) lower_words = lower_pos_tag(bag_words) cleaned_words = clean(lower_words) finalist = list(bigrams(cleaned_words)) + cleaned_words data['Word_matrix'].append(final_words) templist_allwords.extend(cleaned_words) #   templistfreq = FreqDist(templist_allwords) hapaxes = templistfreq.hapaxes() #    for word in templist_allwords: if word not in hapaxes: data['All_words'].append(word) return {label: data}</span></span></code> </pre><br>  La phase de pr√©traitement est la plus longue, il est donc logique de parall√©liser le traitement de notre dossier.  Cela peut √™tre fait √† l'aide du module de <i>multitraitement</i> .  Dans le prochain morceau de code de programme, je lance trois processus qui traiteront simultan√©ment trois dossiers avec des classes diff√©rentes.  Ensuite, les r√©sultats seront rassembl√©s dans un dictionnaire.  Ce pr√©traitement est termin√©. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: data = {} labels = [<span class="hljs-string"><span class="hljs-string">'neutral'</span></span>, <span class="hljs-string"><span class="hljs-string">'bad'</span></span>, <span class="hljs-string"><span class="hljs-string">'good'</span></span>] p = Pool(<span class="hljs-number"><span class="hljs-number">3</span></span>) result = p.map(process, labels) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> result: data.update(i) p.close()</code> </pre><br><h3>  Vectorisation </h3><br>  Apr√®s avoir pr√©trait√© le cas, nous avons un dictionnaire o√π pour chaque √©tiquette de classe contient une liste avec des critiques que nous avons tokenis√©es, normalis√©es et enrichies avec des bigrammes, ainsi qu'une liste de mots de toutes les critiques de cette classe.  Comme le mod√®le ne peut pas percevoir le langage naturel comme nous le faisons, la t√¢che consiste maintenant √† pr√©senter nos revues sous forme num√©rique.  Pour ce faire, nous allons cr√©er un vocabulaire commun, compos√© de jetons uniques, et avec lui nous allons vectoriser chaque revue. <br><br>  Pour commencer, nous cr√©ons une liste qui contient les avis de toutes les classes ainsi que leurs √©tiquettes.  Ensuite, nous cr√©ons un vocabulaire commun, en prenant dans chaque classe 10 000 des mots les plus courants en utilisant la m√©thode <i>most_common</i> du m√™me <i>FreqDist</i> .  En cons√©quence, j'ai obtenu un vocabulaire compos√© d'environ 17 000 mots. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     : # [([  ], _)] labels = ['neutral', 'bad', 'good'] labeled_data = [] for label in labels: for document in data[label]['Word_matrix']: labeled_data.append((document, label)) #      all_words = [] for label in labels: frequency = FreqDist(data[label]['All_words'] common_words = frequency.most_common(10000) words = [i[0] for i in common_words] all_words.extend(words) #    unique_words = list(OrderedDict.fromkeys(all_words))</span></span></code> </pre><br>  Il existe plusieurs fa√ßons de vectoriser du texte.  Les plus populaires d'entre eux: TF-IDF, codage direct et fr√©quence.  J'ai utilis√© le codage fr√©quentiel, dont l'essentiel est de pr√©senter chaque revue comme un vecteur dont les √©l√©ments sont le nombre d'occurrences de chaque mot du vocabulaire.  <i>NLTK</i> a ses propres classificateurs, vous pouvez les utiliser, mais ils fonctionnent plus lentement que leurs homologues de <i>scikit-learn</i> et ont moins de param√®tres.  Vous trouverez ci-dessous le code de codage pour <i>NLTK</i> .  Cependant, j'utiliserai le mod√®le Naive Bayes de <i>scikit-learn</i> et encoderai les critiques, en stockant les attributs dans une matrice clairsem√©e de <i>SciPy</i> et les √©tiquettes de classe dans un tableau <i>NumPy</i> s√©par√©. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     nltk  : # # [({ : -   },  )] prepared_data = [] for x in labeled_data: d = defaultdict(int) for word in unique_words: if word in x[0]: d[word] += 1 if word not in x[0]: d[word] = 0 prepared_data.append((d, x[1])) #     scikit-learn #     matrix_vec = csr_matrix((len(labeled_data), len(unique_words)), dtype=np.int8).toarray() #     target = np.zeros(len(labeled_data), 'str') for index_doc, document in enumerate(labeled_data): for index_word, word in enumerate(unique_words): #  -     matrix_vec[index_doc, index_word] = document[0].count(word) target[index_doc] = document[1] #   X, Y = shuffle(matrix_vec, target)</span></span></code> </pre><br>  √âtant donn√© que dans le jeu de donn√©es, les avis avec certaines balises se succ√®dent, c'est-√†-dire tout d'abord neutres, puis tous n√©gatifs et ainsi de suite, vous devez les m√©langer.  Pour ce faire, vous pouvez utiliser la fonction de <i>lecture</i> <i>al√©atoire</i> de <i>scikit-learn</i> .  Il convient uniquement aux situations o√π les signes et les √©tiquettes de classe sont dans des tableaux diff√©rents, car il vous permet de m√©langer deux tableaux √† l'unisson. <br><br><h3>  Formation mod√®le </h3><br>  Il reste maintenant √† former le mod√®le et √† v√©rifier sa pr√©cision dans le groupe t√©moin.  Comme mod√®le, nous utiliserons le mod√®le du classifieur Naive Bayes.  <i>Scikit-learn</i> a trois mod√®les Naive Bayes en fonction de la distribution des donn√©es: binaire, discret et continu.  La distribution de nos fonctionnalit√©s √©tant discr√®te, nous choisissons <i>MultinomialNB</i> . <br><br>  Le classifieur bay√©sien a le <i>param√®tre alpha</i> hyper, qui est responsable du lissage du mod√®le.  Naive Bayes calcule les probabilit√©s de chaque revue appartenant √† toutes les classes, pour cela multipliant les probabilit√©s conditionnelles d'apparition de tous les mots de revue, √† condition qu'ils appartiennent √† une classe particuli√®re.  Mais si aucun mot de r√©vision n'a √©t√© trouv√© dans l'ensemble de donn√©es de formation, sa probabilit√© conditionnelle est √©gale √† z√©ro, ce qui annule la probabilit√© que la r√©vision appartienne √† n'importe quelle classe.  Pour √©viter cela, par d√©faut, une unit√© est ajout√©e √† toutes les probabilit√©s de mots conditionnelles, c'est <i>-√†-</i> dire que <i>alpha</i> est √©gal √† un.  Cependant, cette valeur peut ne pas √™tre optimale.  Vous pouvez essayer de s√©lectionner <i>alpha en</i> utilisant la recherche dans la grille et la validation crois√©e. <br><br><pre> <code class="python hljs">parameter = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-number"><span class="hljs-number">0.001</span></span>, <span class="hljs-number"><span class="hljs-number">0.0001</span></span>] param_grid = {<span class="hljs-string"><span class="hljs-string">'alpha'</span></span>: parameter} grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>) grid_search.fit(X, Y) Alpha, best_score = grid_search.best_params_, grid_search.best_score_</code> </pre><br>  Dans mon cas, le foyer de la grille donne la valeur optimale de l'hyperparam√®tre √©gale √† 0 avec une pr√©cision de 0,965.  Cependant, cette valeur ne sera √©videmment pas optimale pour l'ensemble de donn√©es de contr√¥le, car il y aura un grand nombre de mots qui n'ont pas √©t√© trouv√©s pr√©c√©demment dans l'ensemble d'apprentissage.  Pour un ensemble de donn√©es de r√©f√©rence, ce mod√®le a une pr√©cision de 0,598.  Cependant, si vous augmentez l' <i>alpha</i> √† 0,1, la pr√©cision des donn√©es d'entra√Ænement chutera √† 0,82 et des donn√©es de contr√¥le, elle augmentera √† 0,62.  Tr√®s probablement, sur un ensemble de donn√©es plus important, la diff√©rence sera plus importante. <br><br><pre> <code class="python hljs">model = MultinomialNB(<span class="hljs-number"><span class="hljs-number">0.1</span></span>) model.fit(X, Y) <span class="hljs-comment"><span class="hljs-comment"># X_control, Y_control   ,   X  Y #        predicted = model.predict(X_control) #     score_test = accuracy_score(Y_control, predicted) #   report = classification_report(Y_control, predicted)</span></span></code> </pre><br><br><h3>  Conclusion </h3><br>  On suppose que le mod√®le devrait √™tre utilis√© pour pr√©dire les r√©visions dont les mots n'ont pas √©t√© utilis√©s pour former un vocabulaire.  Par cons√©quent, la qualit√© du mod√®le peut √™tre √©valu√©e par sa pr√©cision sur la partie contr√¥le des donn√©es, qui est de 0,62.  C'est presque deux fois mieux que de deviner, mais la pr√©cision est encore assez faible. <br><br>  Selon le rapport de classification, il est clair que le mod√®le est le moins performant avec des avis de couleur neutre (pr√©cision 0,47 contre 0,68 pour le positif et 0,76 pour le n√©gatif).  En effet, les avis neutres contiennent des mots caract√©ristiques des avis positifs et n√©gatifs.  Probablement, la pr√©cision du mod√®le peut √™tre am√©lior√©e en augmentant le volume de l'ensemble de donn√©es, car le trois milli√®me ensemble de donn√©es est plut√¥t modeste.  En outre, il serait possible de r√©duire le probl√®me √† une classification binaire des avis en positifs et n√©gatifs, ce qui augmenterait √©galement la pr√©cision. <br><br>  Merci d'avoir lu. <br><br>  PS Si vous voulez vous entra√Æner, mon jeu de donn√©es peut √™tre t√©l√©charg√© sous le lien. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Lien vers l'ensemble de donn√©es</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr467081/">https://habr.com/ru/post/fr467081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr467061/index.html">Rang babylonien: 5 probl√®mes de s√©curit√© dans le secteur de la construction</a></li>
<li><a href="../fr467063/index.html">Surveillance du carburant pour les g√©n√©rateurs diesel du centre de donn√©es - comment le faire et pourquoi est-ce si important?</a></li>
<li><a href="../fr467065/index.html">Archive des probl√®mes d'olympiades en physique pour les √©coliers</a></li>
<li><a href="../fr467073/index.html">¬´En Occident, il n'y a pas de directeurs artistiques de moins de 40 ans. Avec nous, cela peut aller jusqu'√† 30. " Comment est-ce d'√™tre designer en informatique</a></li>
<li><a href="../fr467079/index.html">Carrousel CSS et Javascript Ant</a></li>
<li><a href="../fr467083/index.html">Comment l'√©trange instruction popcount est utilis√©e dans les processeurs modernes</a></li>
<li><a href="../fr467085/index.html">La d√©compilation C, C ++ et DotNet sont les bases de l'inverse. R√©solution des probl√®mes de marche arri√®re avec r0ot-mi. partie 1</a></li>
<li><a href="../fr467087/index.html">Comment j'ai pr√©par√© et r√©ussi la certification Oracle Database SQL (1Z0-071)</a></li>
<li><a href="../fr467089/index.html">Exim patch√© - patchez √† nouveau. Nouvelle ex√©cution de commande √† distance dans Exim 4.92 en une seule demande</a></li>
<li><a href="../fr467091/index.html">Une introduction rapide √† Svelte du point de vue d'un d√©veloppeur angulaire</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>