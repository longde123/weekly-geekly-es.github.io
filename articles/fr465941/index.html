<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏅 👩‍💻 🛍️ Synthèse vocale multilingue avec clonage 🎚️ 👩🏾‍⚕️ 🧑🏾‍🤝‍🧑🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bien que les réseaux de neurones aient commencé à être utilisés pour la synthèse de la parole il n'y a pas si longtemps ( par exemple ), ils ont déjà ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Synthèse vocale multilingue avec clonage</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465941/"><p>  Bien que les réseaux de neurones aient commencé à être utilisés pour la synthèse de la parole il n'y a pas si longtemps ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">par exemple</a> ), ils ont déjà réussi à dépasser les approches classiques et chaque année, ils font l'expérience de tâches de plus en plus récentes. </p><br><p>  Par exemple, il y a quelques mois, la mise en œuvre de la synthèse vocale en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">temps réel-Voice-Cloning</a> est apparue.  Essayons de comprendre de quoi il s'agit et réalisons notre modèle de phonème multilingue (russe-anglais). </p><br><h2 id="stroenie">  Immeuble </h2><br><p><img src="https://habrastorage.org/webt/4b/k7/e_/4bk7e_qeb-qw-clbaxsqs14f7cg.png"></p><br><p>  Notre modèle comprendra quatre réseaux de neurones.  Le premier convertira le texte en phonèmes (g2p), le second convertira la parole que nous voulons cloner en un vecteur de signes (nombres).  Le troisième synthétisera les spectrogrammes Mel sur la base des sorties des deux premiers.  Et enfin, le quatrième recevra le son des spectrogrammes. </p><a name="habracut"></a><br><h2 id="nabory-dannyh">  Jeux de données </h2><br><p>  Ce modèle a besoin de beaucoup de discours.  Voici les bases qui vous aideront à cet égard. </p><br><div class="scrollable-table"><table><thead><tr><th>  Prénom </th><th>  La langue </th><th>  Lien </th><th>  Commentaires </th><th>  Mon lien </th><th>  Commentaires </th></tr></thead><tbody><tr><td>  Dictionnaire des phonèmes </td><td>  En ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">En</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ru</a> </td><td></td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  Dictionnaire phonème combiné russe et anglais </td></tr><tr><td>  Libripepeech </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  300 votes, 360 heures de discours pur </td><td></td><td></td></tr><tr><td>  VoxCeleb </td><td>  En </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  7000 votes, plusieurs heures de mauvais son </td><td></td><td></td></tr><tr><td>  M-AILABS </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  3 votes, 46 heures de discours pur </td><td></td><td></td></tr><tr><td>  open_tts, open_stt </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">open_tts</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">open_stt</a> </td><td>  beaucoup de voix, plusieurs heures de mauvais son </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  J'ai nettoyé 4 heures de discours d'un orateur.  Annotation corrigée, divisée en segments jusqu'à 7 secondes </td></tr><tr><td>  Livre audio Voxforge + </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  de nombreux votes, 25h de qualité différente </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  Bons fichiers sélectionnés.  Divisé en segments.  Ajout de livres audio sur Internet.  Il s'est avéré 200 haut-parleurs en quelques minutes pour chaque </td></tr><tr><td>  RUSLAN </td><td>  Ru </td><td>  <a href="">lien</a> </td><td>  Une seule voix, 40 heures de discours pur </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  Recodé à 16 kHz </td></tr><tr><td>  Mozilla </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  50 votes, 30 heures de qualité normale </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  Recodé à 16 kHz, différents utilisateurs dispersés dans des dossiers </td></tr><tr><td>  Russe célibataire </td><td>  Ru </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td>  Une seule voix, 9 heures de discours pur </td><td>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">lien</a> </td><td></td></tr></tbody></table></div><br><h2 id="obrabotka-teksta">  Traitement de texte </h2><br><p>  La première tâche sera le traitement de texte.  Imaginez le texte sous la forme dans laquelle il sera davantage exprimé.  Nous représenterons les nombres en mots et ouvrirons des abréviations.  En savoir plus dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article sur la synthèse</a> .  C'est une tâche difficile, alors supposons que nous avons déjà traité du texte (dans les bases de données ci-dessus, il a été traité). </p><br><p>  La prochaine question à se poser est de savoir s'il faut utiliser l'enregistrement graphème ou phonème.  Pour une voix monophonique et monolingue, un modèle de lettre convient également.  Si vous souhaitez travailler avec un modèle multi-voix multi-voix, je vous conseille d'utiliser la transcription (Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">aussi</a> ). </p><br><h3 id="g2p">  G2p </h3><br><p>  Pour la langue russe, il existe une implémentation appelée <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">russian_g2p</a> .  Il est construit sur les règles de la langue russe et fait bien face à la tâche, mais présente des inconvénients.  Tous les mots ne soulignent pas et ne conviennent pas non plus à un modèle multilingue.  Par conséquent, prenez le dictionnaire créé pour elle, ajoutez le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dictionnaire pour la langue anglaise</a> et alimentez le réseau neuronal (par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> ) </p><br><p>  Avant de former le réseau, il convient de considérer les sons de différentes langues qui semblent similaires, et vous pouvez sélectionner un caractère pour eux, et pour lequel cela est impossible.  Plus il y a de sons, plus le modèle est difficile à apprendre, et s'il y en a trop peu, le modèle aura un accent.  N'oubliez pas de souligner les caractères individuels avec des voyelles accentuées.  Pour la langue anglaise, le stress secondaire joue un petit rôle, et je ne le distinguerais pas. </p><br><h2 id="kodirovanie-spikerov">  Codage des haut-parleurs </h2><br><p>  Le réseau est similaire à la tâche d'identifier un utilisateur par la voix.  En sortie, différents utilisateurs obtiennent différents vecteurs avec des nombres.  Je suggère d'utiliser l'implémentation de CorentinJ elle-même, qui est basée sur l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> .  Le modèle est un LSTM à trois couches avec 768 nœuds, suivi d'une couche entièrement connectée de 256 neurones, donnant un vecteur de 256 nombres. </p><br><p>  L'expérience a montré qu'un réseau formé en anglais parle bien le russe.  Cela simplifie considérablement la vie, car la formation nécessite beaucoup de données.  Je recommande de prendre un modèle déjà formé et de se recycler en anglais à partir de VoxCeleb et LibriSpeech, ainsi que de tout le discours russe que vous trouvez.  L'encodeur n'a pas besoin d'annotation textuelle des fragments de parole. </p><br><h3 id="trenirovka">  La formation </h3><br><ol><li> Exécutez <code>python encoder_preprocess.py &lt;datasets_root&gt;</code> pour traiter les données </li><li>  Exécutez "visdom" dans un terminal séparé. </li><li>  Exécutez <code>python encoder_train.py my_run &lt;datasets_root&gt;</code> pour entraîner l'encodeur </li></ol><br><h2 id="sintez">  La synthèse </h2><br><p>  Passons à la synthèse.  Les modèles que je connais n'obtiennent pas de son directement du texte, car c'est difficile (trop de données).  Premièrement, le texte produit un son sous forme spectrale, et alors seulement le quatrième réseau se traduira par une voix familière.  Par conséquent, nous comprenons d'abord comment la forme spectrale est associée à la voix.  Il est plus facile de comprendre le problème inverse de la façon d'obtenir un spectrogramme à partir du son. </p><br><p>  Le son est divisé en segments de 25 ms par incréments de 10 ms (valeur par défaut dans la plupart des modèles).  Ensuite, en utilisant la transformée de Fourier pour chaque pièce, le spectre est calculé (oscillations harmoniques, dont la somme donne le signal d'origine) et présenté sous la forme d'un graphique, où la bande verticale est le spectre d'un segment (en fréquence), et à l'horizontale - une séquence de segments (dans le temps).  Ce graphique est appelé un spectrogramme.  Si la fréquence est codée de façon non linéaire (les fréquences inférieures sont meilleures que les fréquences supérieures), alors l'échelle verticale changera (nécessaire pour réduire les données), alors ce graphique est appelé le spectrogramme Mel.  C'est ainsi que fonctionne l'audition humaine, que nous entendons une légère déviation aux basses fréquences mieux qu'aux fréquences supérieures, donc la qualité du son ne souffrira pas </p><br><p><img src="https://habrastorage.org/webt/dx/mv/fs/dxmvfst1objf8dmdglu7rlbhaiq.jpeg"></p><br><p>  Il existe plusieurs bonnes implémentations de synthèse de spectrogrammes telles que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tacotron 2</a> et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deepvoice 3</a> .  Chacun de ces modèles a ses propres implémentations, par exemple <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">1</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">2</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">3</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">4</a> .  Nous utiliserons (comme CorentinJ) le modèle Tacotron de Rayhane-mamah. </p><br><p><img src="https://habrastorage.org/webt/u9/jm/h0/u9jmh0ldgpelp8qoouinbx9ptfi.png"></p><br><p>  Tacotron est basé sur le réseau seq2seq avec un mécanisme d'attention.  Lisez les détails dans l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> . </p><br><h3 id="trenirovka-1">  La formation </h3><br><p>  N'oubliez pas d'éditer utils / symboles.py si vous synthétisez non seulement la parole anglaise, hparams.p, mais aussi preprocess.py. </p><br><p>  La synthèse nécessite beaucoup de son clair et bien marqué provenant de différents haut-parleurs.  Ici, une langue étrangère n'aidera pas. </p><br><ol><li>  Exécutez <code>python synthesizer_preprocess_audio.py &lt;datasets_root&gt;</code> pour créer un son et des spectrogrammes traités </li><li>  Exécutez <code>python synthesizer_preprocess_embeds.py &lt;datasets_root&gt;</code> pour encoder le son (obtenir les signes d'une voix) </li><li>  Exécutez <code>python synthesizer_train.py my_run &lt;datasets_root&gt;</code> pour entraîner le synthétiseur </li></ol><br><h2 id="vokoder">  Vocoder </h2><br><p>  Maintenant, il ne reste plus qu'à convertir les spectrogrammes en sons.  Pour cela, le dernier réseau est le vocodeur.  La question se pose, si les spectrogrammes sont obtenus à partir du son en utilisant la transformée de Fourier, est-il possible d'obtenir à nouveau le son en utilisant la transformation inverse?  La réponse est oui et non.  Les oscillations harmoniques qui composent le signal d'origine contiennent à la fois l'amplitude et la phase, et nos spectrogrammes contiennent uniquement des informations sur l'amplitude (dans le but de réduire les paramètres et de travailler avec les spectrogrammes), donc si nous faisons la transformée de Fourier inverse, nous obtenons un mauvais son. </p><br><p>  Pour résoudre ce problème, ils ont inventé un algorithme Griffin-Lim rapide.  Il fait la transformée de Fourier inverse du spectrogramme, obtenant un "mauvais" son.  Il effectue ensuite une conversion directe de ce son et reçoit un spectre qui contient déjà un peu d'informations sur la phase, et l'amplitude ne change pas au cours du processus.  Ensuite, la transformation inverse est reprise et un son plus net est obtenu.  Malheureusement, la qualité de la parole générée par un tel algorithme laisse beaucoup à désirer. </p><br><p>  Il a été remplacé par des vocodeurs neuronaux tels que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WaveNet</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WaveRNN</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WaveGlow</a> et autres.  CorentinJ a utilisé le modèle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WaveRNN</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">fatchord</a> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2fe/b25/464/2feb25464260ad8a96c983bb85340fee.gif" alt="image"></p><br><p>  Pour le prétraitement des données, deux approches sont utilisées.  Soit obtenir des spectrogrammes à partir du son (en utilisant la transformée de Fourier), ou du texte (en utilisant le modèle de synthèse).  Google recommande une deuxième approche. </p><br><h3 id="trenirovka-2">  La formation </h3><br><ol><li>  Exécutez <code>python vocoder_preprocess.py &lt;datasets_root&gt;</code> pour synthétiser les spectrogrammes </li><li>  Exécutez <code>python vocoder_train.py &lt;datasets_root&gt;</code> pour vocoder </li></ol><br><h2 id="itogo">  Total </h2><br><p>  Nous avons obtenu un modèle de synthèse vocale multilingue qui peut cloner une voix. <br>  Exécutez la boîte à outils: <code>python demo_toolbox.py -d &lt;datasets_root&gt;</code> <br>  Des exemples peuvent être entendus ici </p><br><div class="oembed">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://soundcloud.com/fn5va3vghrkh/sets/multi-tacotron</a> </div><br><h4 id="sovety-i-vyvody">  Conseils et conclusions </h4><br><ul><li>  Besoin de beaucoup de données (&gt; 1000 votes,&gt; 1000 heures) </li><li>  La vitesse de fonctionnement n'est comparable au temps réel que dans la synthèse d'au moins 4 phrases </li><li>  Pour l'encodeur, utilisez le modèle pré-formé pour la langue anglaise, un peu de recyclage.  Elle va bien </li><li>  Un synthétiseur formé sur des données "propres" fonctionne mieux, mais clone moins bien qu'un synthétiseur formé sur un volume plus important, mais des données sales </li><li>  Le modèle ne fonctionne bien que sur les données sur lesquelles j'ai étudié </li></ul><br><p>  Vous pouvez synthétiser votre voix en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ligne en utilisant colab</a> , ou voir mon implémentation sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">github</a> et télécharger mes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">poids</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr465941/">https://habr.com/ru/post/fr465941/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr465921/index.html">Du cyberpunk au DevSecOps: 7 livres pour lesquels l'ingénieur DevSecOps mérite encore d'apprendre l'anglais</a></li>
<li><a href="../fr465923/index.html">3 erreurs qui peuvent coûter la vie de votre startup</a></li>
<li><a href="../fr465927/index.html">Count Scoring de la Fer ou une étude sur la notation du crédit dans le cadre de l'élargissement de ses horizons. 3e partie</a></li>
<li><a href="../fr465929/index.html">Expériences d'infrastructure A / B dans la grande recherche. Rapport Yandex</a></li>
<li><a href="../fr465937/index.html">Technostream: une nouvelle sélection de vidéos de formation pour la rentrée</a></li>
<li><a href="../fr465943/index.html">La startup Unicorn Bolt organisera un championnat pour les développeurs avec un prix de 350 mille roubles et la possibilité d'une relocalisation en Europe</a></li>
<li><a href="../fr465945/index.html">À propos de l'installation et de l'utilisation de LineageOS 16, F-Droid</a></li>
<li><a href="../fr465947/index.html">Formation Cisco 200-125 CCNA v3.0. Jour 30. Architecture réseau et dépannage Cisco</a></li>
<li><a href="../fr465949/index.html">Applications pour les livres électroniques sur le système d'exploitation Android. Partie 5. Stockage dans le cloud et joueurs</a></li>
<li><a href="../fr465951/index.html">Nous avons tous besoin d'un helpdesk</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>