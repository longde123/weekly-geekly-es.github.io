<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ“ ğŸ“· ğŸ§”ğŸ½ Evolusi interaksi cluster. Bagaimana kami menerapkan ActiveMQ dan Hazelcast ğŸš ğŸ‘©ğŸ»â€ğŸ¤ ğŸ§›ğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Selama 7 tahun terakhir, bersama dengan tim, saya telah mendukung dan mengembangkan inti dari produk Miro (ex-RealtimeBoard): interaksi client-server ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Evolusi interaksi cluster. Bagaimana kami menerapkan ActiveMQ dan Hazelcast</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/miro/blog/441590/">  Selama 7 tahun terakhir, bersama dengan tim, saya telah mendukung dan mengembangkan inti dari produk Miro (ex-RealtimeBoard): interaksi client-server dan cluster, bekerja dengan database. <br><br>  Kami memiliki Java dengan berbagai pustaka.  Semuanya diluncurkan di luar wadah, melalui plugin Maven.  Ini didasarkan pada platform mitra kami, yang memungkinkan kami untuk bekerja dengan basis data dan arus, mengelola interaksi klien-server, dll.  DB - Redis dan PostgreSQL (kolega saya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">menulis tentang bagaimana kami berpindah dari satu database ke yang lain</a> ). <br><br>  Dalam hal logika bisnis, aplikasi berisi: <br><br><ul><li>  bekerja dengan papan khusus dan isinya; </li><li>  fungsionalitas untuk pendaftaran pengguna, pembuatan dan pengelolaan papan; </li><li> generator sumber daya kustom.  Misalnya, ini mengoptimalkan gambar besar yang diunggah ke aplikasi sehingga mereka tidak memperlambat klien kami; </li><li>  banyak integrasi dengan layanan pihak ketiga. </li></ul><br>  Pada 2011, ketika kami baru memulai, seluruh Miro berada di server yang sama.  Semuanya ada di sana: Nginx di mana php untuk situs berubah, aplikasi Java dan database. <br><br>  Produk dikembangkan, jumlah pengguna dan konten yang mereka tambahkan ke papan bertambah, sehingga beban di server juga bertambah.  Karena banyaknya aplikasi di server kami, pada saat itu kami tidak dapat memahami apa yang sebenarnya memberi beban, dan karenanya, tidak dapat mengoptimalkannya. Untuk memperbaikinya, kami membagi semuanya menjadi server yang berbeda, dan kami mendapat server web, server dengan aplikasi dan server database kami. <br><br>  Sayangnya, setelah beberapa waktu, masalah muncul lagi, karena beban pada aplikasi terus bertambah.  Kemudian kami memikirkan bagaimana skala infrastruktur. <br><br><img src="https://habrastorage.org/webt/_5/zq/_3/_5zq_3c16pydjklapiqamfzyxcg.png"><br><br>  Selanjutnya, saya akan berbicara tentang kesulitan yang kami temui dalam mengembangkan cluster dan meningkatkan skala aplikasi dan infrastruktur Java. <a name="habracut"></a><br><br><h2>  Skala infrastruktur secara horizontal </h2><br>  Kami mulai dengan mengumpulkan metrik: penggunaan memori dan CPU, waktu yang diperlukan untuk mengeksekusi permintaan pengguna, penggunaan sumber daya sistem, dan bekerja dengan database.  Dari metrik, jelas bahwa generasi sumber daya pengguna adalah proses yang tidak dapat diprediksi.  Kami dapat memuat prosesor 100% dan menunggu puluhan detik hingga semuanya selesai.  Permintaan pengguna untuk papan juga terkadang memberi beban yang tidak terduga.  Misalnya, ketika pengguna memilih seribu widget dan mulai memindahkannya secara spontan. <br><br>  Kami mulai berpikir tentang bagaimana skala bagian-bagian sistem ini dan sampai pada solusi yang jelas. <br><br>  <b>Skala bekerja dengan papan dan konten</b> .  Pengguna membuka papan seperti ini: pengguna membuka klien â†’ menunjukkan papan mana yang ia ingin buka â†’ terhubung ke server â†’ aliran dibuat di server â†’ semua pengguna papan ini terhubung ke satu aliran â†’ setiap perubahan atau pembuatan widget terjadi dalam aliran ini.  Ternyata semua pekerjaan dengan dewan dibatasi secara ketat oleh arus, yang berarti bahwa kita dapat mendistribusikan arus ini di antara server. <br><br>  <b>Skala generasi sumber daya pengguna</b> .  Kita dapat mengeluarkan server untuk menghasilkan sumber daya secara terpisah, dan ia akan menerima pesan untuk dibuat, dan kemudian menjawab bahwa semuanya dihasilkan. <br><br>  Segalanya tampak sederhana.  Tetapi begitu kami mulai mempelajari topik ini lebih dalam, ternyata kami perlu menyelesaikan beberapa masalah tidak langsung.  Misalnya, jika pengguna kedaluwarsa berlangganan, maka kami harus memberi tahu mereka tentang hal ini, apa pun papannya.  Atau, jika pengguna telah memperbarui versi sumber daya, Anda perlu memastikan bahwa cache dibilas dengan benar di semua server dan kami memberikan versi yang tepat. <br><br>  Kami telah mengidentifikasi persyaratan sistem.  Langkah selanjutnya adalah memahami bagaimana mempraktikkannya.  Faktanya, kami membutuhkan sistem yang memungkinkan server dalam cluster untuk berkomunikasi satu sama lain dan berdasarkan itu kami akan mewujudkan semua ide kami. <br><br><h2>  Cluster pertama di luar kotak </h2><br>  Kami tidak memilih versi sistem yang pertama, karena sudah diterapkan sebagian di platform mitra yang kami gunakan.  Di dalamnya, semua server terhubung satu sama lain melalui TCP, dan menggunakan koneksi ini kita bisa mengirim pesan RPC ke satu atau semua server sekaligus. <br><br>  Sebagai contoh, kami memiliki tiga server, mereka terhubung satu sama lain melalui TCP, dan di Redis kami memiliki daftar server ini.  Kami memulai server baru di gugus â†’ menambahkan dirinya ke daftar di Redis â†’ membaca daftar untuk mencari tahu tentang semua server di gugus â†’ terhubung ke semua. <br><br><img src="https://habrastorage.org/webt/yj/9c/hv/yj9chvfavcbrixqnn_12jho2k7k.png"><br><br>  Berdasarkan RPC, dukungan untuk membilas cache dan mengarahkan pengguna ke server yang diinginkan telah dilaksanakan.  Kami harus melakukan generasi sumber daya pengguna dan memberi tahu pengguna bahwa sesuatu telah terjadi (misalnya, akun telah kedaluwarsa).  Untuk menghasilkan sumber daya, kami memilih server yang sewenang-wenang dan mengirimnya permintaan untuk dibuat, dan untuk pemberitahuan tentang berakhirnya langganan, kami mengirim perintah ke semua server dengan harapan pesan akan mencapai tujuan. <br><br><h3>  Server itu sendiri menentukan kepada siapa untuk mengirim pesan. </h3><br>  Kedengarannya seperti fitur, bukan masalah.  Tetapi server hanya berfokus pada koneksi ke server lain.  Jika ada koneksi, maka ada kandidat untuk mengirim pesan. <br><br>  Masalahnya adalah bahwa server nomor 1 tidak tahu bahwa server nomor 4 berada di bawah beban tinggi sekarang dan tidak dapat menjawabnya dengan cukup cepat.  Akibatnya, permintaan server # 1 diproses lebih lambat dari yang mereka bisa. <br><br><img src="https://habrastorage.org/webt/g7/mw/ez/g7mwezzba78vsgcvx8mpa_fzdou.png"><br><br><h3>  Server tidak tahu bahwa server kedua dibekukan </h3><br>  Tetapi bagaimana jika server tidak hanya dimuat banyak, tetapi umumnya membeku?  Selain itu, hang sehingga tidak lagi hidup.  Sebagai contoh, saya telah kehabisan semua memori yang tersedia. <br><br>  Dalam hal ini, server # 1 tidak tahu apa masalahnya, sehingga ia terus menunggu jawaban.  Server yang tersisa di gugus juga tidak tahu tentang situasi dengan server No. 4, sehingga mereka akan mengirim banyak pesan ke server No. 4 dan menunggu jawaban.  Jadi akan sampai server nomor 4 mati. <br><br><img src="https://habrastorage.org/webt/5y/et/pg/5yetpgodx1zi38he2nchwnnniiq.png"><br><br>  Apa yang harus dilakukan  Kami dapat secara independen menambahkan pemeriksaan status server ke sistem.  Atau kita dapat mengarahkan pesan dari server "sakit" ke server "sehat".  Semua ini akan memakan waktu terlalu lama bagi pengembang.  Pada 2012, kami memiliki sedikit pengalaman dalam bidang ini, jadi kami mulai mencari solusi yang siap pakai untuk semua masalah kami sekaligus. <br><br><h2>  Pialang pesan.  Activemq </h2><br>  Kami memutuskan untuk pergi ke broker pesan untuk mengkonfigurasi komunikasi antara server dengan benar.  Mereka memilih ActiveMQ karena kemampuan untuk mengkonfigurasi menerima pesan pada konsumen pada waktu tertentu.  Benar, kami tidak pernah mengambil kesempatan ini, jadi kami bisa memilih RabbitMQ, misalnya. <br><br>  Akibatnya, kami mentransfer seluruh sistem kluster kami ke ActiveMQ.  Apa yang diberikannya: <br><br><ol><li>  Server tidak lagi menentukan sendiri kepada siapa pesan dikirim, karena semua pesan melewati antrian. </li><li>  Toleransi kesalahan dikonfigurasi.  Untuk membaca antrian, Anda dapat menjalankan tidak hanya satu, tetapi beberapa server.  Bahkan jika salah satunya jatuh, sistem akan terus bekerja. </li><li>  Server muncul peran, yang memungkinkan untuk membagi server berdasarkan jenis beban.  Sebagai contoh, generator sumber daya hanya dapat terhubung ke antrian untuk membaca pesan untuk menghasilkan sumber daya, dan server dengan papan dapat terhubung ke antrian untuk membuka papan. </li><li>  Apakah komunikasi RPC, mis.  setiap server memiliki antrian pribadi sendiri, di mana server lain mengirim acara ke sana. </li><li>  Anda dapat mengirim pesan ke semua server melalui Topik, yang kami gunakan untuk mengatur ulang langganan. </li></ol><br><br>  Skema ini terlihat sederhana: semua server terhubung ke broker, dan mengelola komunikasi di antara mereka.  Semuanya berfungsi, pesan dikirim dan diterima, sumber daya dibuat.  Tetapi ada masalah baru. <br><br><h3>  Apa yang harus dilakukan ketika semua server yang diperlukan berbohong? </h3><br>  Katakanlah server # 3 ingin mengirim pesan untuk menghasilkan sumber daya dalam antrian.  Dia berharap pesannya diproses.  Tetapi dia tidak tahu bahwa karena suatu alasan tidak ada satu pun penerima pesan.  Misalnya, penerima macet karena kesalahan. <br><br>  Untuk semua waktu tunggu, server mengirim banyak pesan dengan permintaan, itulah sebabnya antrian pesan muncul.  Oleh karena itu, ketika server yang berfungsi muncul, mereka dipaksa untuk terlebih dahulu memproses akumulasi antrian, yang membutuhkan waktu.  Di sisi pengguna, ini mengarah pada fakta bahwa gambar yang mereka unggah tidak segera muncul.  Dia tidak siap untuk menunggu, jadi dia meninggalkan papan tulis. <br><br>  Akibatnya, kami menghabiskan kapasitas server untuk menghasilkan sumber daya, dan tidak ada yang membutuhkan hasilnya. <br><br><img src="https://habrastorage.org/webt/oq/p8/fd/oqp8fd0mdctqarlauh84jdjn8rc.png"><br><br>  Bagaimana saya bisa menyelesaikan masalah?  Kami dapat mengatur pemantauan, yang akan memberi tahu Anda tentang apa yang terjadi.  Tetapi dari saat pemantauan melaporkan sesuatu, sampai saat kita memahami bahwa server kita buruk, waktu akan berlalu.  Ini tidak cocok untuk kita. <br><br>  Pilihan lain adalah menjalankan Service Discovery, atau registri layanan yang akan mengetahui server mana yang menjalankan peran.  Dalam hal ini, kami akan segera menerima pesan kesalahan jika tidak ada server gratis. <br><br><h3>  Beberapa layanan tidak dapat diskalakan secara horizontal </h3><br>  Ini adalah masalah kode awal kami, bukan ActiveMQ.  Izinkan saya menunjukkan kepada Anda sebuah contoh: <br><br><pre><code class="plaintext hljs">Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER);</code> </pre> <br>  Kami memiliki layanan untuk bekerja dengan hak pengguna di papan tulis: pengguna dapat menjadi pemilik papan atau editornya.  Hanya ada satu pemilik di dewan.  Misalkan kita memiliki skenario di mana kita ingin mentransfer kepemilikan papan dari satu pengguna ke pengguna lain.  Pada baris pertama kita mendapatkan pemilik papan saat ini, pada baris kedua - kita mengambil pengguna yang merupakan editor, dan sekarang menjadi pemilik.  Selanjutnya, pemilik saat ini kami menempatkan peran EDITOR, dan mantan editor - peran PEMILIK. <br><br>  Mari kita lihat bagaimana ini akan bekerja di lingkungan multi-threaded.  Ketika utas pertama menetapkan peran EDITOR, dan utas kedua mencoba untuk mengambil PEMILIK saat ini, mungkin saja PEMILIK itu tidak ada, tetapi ada dua EDITOR. <br><br>  Alasannya adalah kurangnya sinkronisasi.  Kami dapat memecahkan masalah dengan menambahkan blok sinkronisasi di papan tulis. <br><br><pre> <code class="plaintext hljs">synchronized (board) { Permission ownerPermission = service.getOwnerPermission(board); Permission permission = service.getPermission(board,user); ownerPermission.setRole(EDITOR); permission.setRole(OWNER); }</code> </pre><br>  Solusi ini tidak akan berfungsi di cluster.  Basis data SQL dapat membantu kami dalam hal ini dengan bantuan transaksi.  Tapi kami punya Redis. <br><br>  Solusi lain adalah menambahkan kunci terdistribusi ke cluster sehingga sinkronisasi ada di dalam seluruh cluster, dan bukan hanya satu server. <br><br><h2>  Satu titik kegagalan saat memasuki papan tulis </h2><br>  Model interaksi antara klien dan server stateful.  Jadi kita harus menyimpan status board di server.  Oleh karena itu, kami membuat peran terpisah untuk server - BoardServer, yang menangani permintaan pengguna terkait dengan papan. <br><br>  Bayangkan kita memiliki tiga BoardServer, salah satunya adalah yang utama.  Pengguna mengiriminya permintaan "Buka saya papan dengan id = 123" â†’ server melihat dalam database-nya apakah papan terbuka dan di server mana itu.  Dalam contoh ini, papan terbuka. <br><br><img src="https://habrastorage.org/webt/ej/kf/sd/ejkfsdptym30e-gdvkycc225zpw.png"><br><br>  Server utama menjawab bahwa Anda harus terhubung ke server No. 1 â†’ pengguna terhubung.  Jelas, jika server utama mati, maka pengguna tidak akan lagi dapat mengakses papan baru. <br><br>  Lalu mengapa kita membutuhkan server yang tahu di mana papan terbuka?  Sehingga kita memiliki satu titik keputusan.  Jika terjadi sesuatu pada server, kita perlu memahami apakah papan itu sebenarnya tersedia untuk menghapus papan dari registri atau membuka kembali di tempat lain.  Mungkin untuk mengatur ini dengan bantuan kuorum, ketika beberapa server memecahkan masalah yang sama, tetapi pada saat itu kami tidak memiliki pengetahuan untuk mengimplementasikan kuorum secara independen. <br><br><h2>  Beralih ke Hazelcast </h2><br>  Dengan satu atau lain cara, kami mengatasi masalah yang muncul, tetapi itu mungkin bukan cara yang paling indah.  Sekarang kami perlu memahami bagaimana menyelesaikannya dengan benar, jadi kami merumuskan daftar persyaratan untuk solusi cluster baru: <br><br><ol><li>  Kami membutuhkan sesuatu yang akan memantau status semua server dan peran mereka.  Sebut saja Service Discovery. </li><li>  Kami membutuhkan kunci kluster yang akan membantu memastikan konsistensi saat menjalankan kueri berbahaya. </li><li>  Kami membutuhkan struktur data terdistribusi yang akan memastikan bahwa papan ada di server tertentu dan menginformasikan jika ada kesalahan. </li></ol><br>  Itu adalah tahun 2015.  Kami memilih Hazelcast - In-Memory Data Grid, sistem cluster untuk menyimpan informasi dalam RAM.  Kemudian kami berpikir bahwa kami telah menemukan solusi ajaib, grail suci dunia interaksi cluster, kerangka kerja keajaiban yang dapat melakukan segalanya dan menggabungkan struktur data terdistribusi, kunci, pesan RPC dan antrian. <br><br><img src="https://habrastorage.org/webt/ce/ws/c9/cewsc9gdgzsmtebczs9jxbs4j2e.png"><br><br>  Seperti dengan ActiveMQ, kami mentransfer hampir semuanya ke Hazelcast: <br><br><ul><li>  generasi sumber daya pengguna melalui ExecutorService; </li><li>  kunci didistribusikan ketika hak diubah; </li><li>  peran dan atribut server (Penemuan Layanan); </li><li>  satu registri papan terbuka, dll. </li></ul><br><h3>  Topologi Hazelcast </h3><br>  Hazelcast dapat dikonfigurasi dalam dua topologi.  Opsi pertama adalah Client-Server, ketika anggota berada secara terpisah dari aplikasi utama, mereka sendiri membentuk sebuah cluster, dan semua aplikasi terhubung ke mereka sebagai database. <br><br><img src="https://habrastorage.org/webt/r4/lg/vm/r4lgvmm7ni0dmyb6yp60cueklwm.png"><br><br>  Topologi kedua adalah Tertanam, ketika anggota Hazelcast tertanam dalam aplikasi itu sendiri.  Dalam hal ini, kita dapat menggunakan contoh lebih sedikit, akses ke data lebih cepat, karena data dan logika bisnis itu sendiri berada di tempat yang sama. <br><br><img src="https://habrastorage.org/webt/gq/rz/fa/gqrzfappt3yspdlfpfe5sm3mhyg.png"><br><br>  Kami memilih solusi kedua karena kami menganggapnya lebih efektif dan ekonomis untuk diterapkan.  Efektif, karena kecepatan mengakses data Hazelcast akan lebih rendah, karena  mungkin data ini ada di server saat ini.  Ekonomis, karena kita tidak perlu mengeluarkan uang untuk contoh tambahan. <br><br><h3>  Cluster hang ketika anggota hang </h3><br>  Beberapa minggu setelah menyalakan Hazelcast, muncul masalah pada prod. <br><br>  Pada awalnya, pemantauan kami menunjukkan bahwa salah satu server mulai secara bertahap membebani memori.  Saat kami menonton server ini, seluruh server mulai memuat juga: CPU bertambah, lalu RAM, dan setelah lima menit semua server menggunakan semua memori yang tersedia. <br><br>  Pada titik ini di konsol kami melihat pesan-pesan ini: <br><br><pre> <code class="java hljs"><span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">466</span></span> [WARN] (cached18) com.hazelcast.spi.impl.operationservice.impl.Invocation: [my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] Asking ifoperation execution has been started: com.hazelcast.spi.impl.operationservice.impl.IsStillRunningService$InvokeIsStillRunningOperationRunnable@<span class="hljs-number"><span class="hljs-number">6</span></span>d4274d7 <span class="hljs-number"><span class="hljs-number">2015</span></span>-<span class="hljs-number"><span class="hljs-number">07</span></span>-<span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">35</span></span>:<span class="hljs-number"><span class="hljs-number">51</span></span>,<span class="hljs-number"><span class="hljs-number">467</span></span> [WARN] (hz._hzInstance_1_dev.async.thread-<span class="hljs-number"><span class="hljs-number">3</span></span>) com.hazelcast.spi.impl.operationservice.impl.Invocation:[my.host.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span> [dev] [<span class="hljs-number"><span class="hljs-number">3.5</span></span>] <span class="hljs-string"><span class="hljs-string">'is-executing'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">true</span></span> -&gt; Invocation{ serviceName=<span class="hljs-string"><span class="hljs-string">'hz:impl:executorService'</span></span>, op=com.hazelcast.executor.impl.operations.MemberCallableTaskOperation{serviceName=<span class="hljs-string"><span class="hljs-string">'null'</span></span>, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, callId=<span class="hljs-number"><span class="hljs-number">18062</span></span>, invocationTime=<span class="hljs-number"><span class="hljs-number">1436974430783</span></span>, waitTimeout=-<span class="hljs-number"><span class="hljs-number">1</span></span>,callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>}, partitionId=-<span class="hljs-number"><span class="hljs-number">1</span></span>, replicaIndex=<span class="hljs-number"><span class="hljs-number">0</span></span>, tryCount=<span class="hljs-number"><span class="hljs-number">250</span></span>, tryPauseMillis=<span class="hljs-number"><span class="hljs-number">500</span></span>, invokeCount=<span class="hljs-number"><span class="hljs-number">1</span></span>, callTimeout=<span class="hljs-number"><span class="hljs-number">60000</span></span>,target=Address[my.host2.address.com]:<span class="hljs-number"><span class="hljs-number">5701</span></span>, backupsExpected=<span class="hljs-number"><span class="hljs-number">0</span></span>, backupsCompleted=<span class="hljs-number"><span class="hljs-number">0</span></span>}</code> </pre><br>  Di sini, Hazelcast memeriksa untuk melihat apakah operasi yang dikirim ke server "sekarat" pertama sedang berlangsung.  Hazelcast mencoba mengikuti dan memeriksa status operasi beberapa kali per detik.  Akibatnya, ia mem-spammed semua server lain dengan operasi ini, dan setelah beberapa menit mereka kehabisan memori, dan kami mengumpulkan beberapa GB log dari masing-masing server. <br><br>  Situasi diulang beberapa kali.  Ternyata ini adalah kesalahan dalam Hazelcast versi 3.5, di mana mekanisme detak jantung dilaksanakan, yang memeriksa status permintaan.  Itu tidak memeriksa beberapa kasus batas yang kami temui.  Saya harus mengoptimalkan aplikasi agar tidak masuk ke dalam kasus ini, dan setelah beberapa minggu Hazelcast memperbaiki kesalahan di rumah. <br><br><h3>  Sering menambahkan dan menghapus anggota dari Hazelcast </h3><br>  Masalah berikutnya yang kami temukan adalah menambah dan menghapus anggota dari Hazelcast. <br><br>  Pertama, saya akan menjelaskan secara singkat bagaimana Hazelcast bekerja dengan partisi.  Misalnya, ada empat server, dan masing-masing menyimpan sebagian data (pada gambar mereka berbeda warna).  Unit adalah partisi primer, deuce adalah partisi sekunder, yaitu  cadangan dari partisi utama. <br><br><img src="https://habrastorage.org/webt/ex/qz/vj/exqzvjxs9rxlmfgssghnrnqxnn8.png"><br><br>  Ketika server dimatikan, partisi dikirim ke server lain.  Dalam hal server mati, partisi tidak ditransfer dari itu, tetapi dari server yang masih hidup dan menyimpan cadangan dari partisi ini. <br><br><img src="https://habrastorage.org/webt/eu/ds/-0/euds-0xurnqjlbhisjoj8k9ucis.png"><br><br>  Ini adalah mekanisme yang andal.  Masalahnya adalah kita sering menghidupkan dan mematikan server untuk menyeimbangkan beban, dan menyeimbangkan partisi juga membutuhkan waktu.  Dan semakin banyak server berjalan dan semakin banyak data yang kami simpan di Hazelcast, semakin banyak waktu yang dibutuhkan untuk menyeimbangkan kembali partisi. <br><br>  Tentu saja, kita dapat mengurangi jumlah cadangan, mis.  partisi sekunder.  Tapi ini tidak aman, karena sesuatu pasti akan salah. <br><br>  Solusi lain adalah beralih ke topologi Client-Server sehingga menghidupkan dan mematikan server tidak mempengaruhi cluster Hazelcast inti.  Kami mencoba melakukan ini, dan ternyata permintaan RPC tidak dapat dilakukan pada klien.  Mari kita lihat mengapa. <br><br>  Untuk melakukan ini, perhatikan contoh mengirim satu permintaan RPC ke server lain.  Kami mengambil ExecutorService, yang memungkinkan Anda mengirim pesan RPC, dan mengirimkannya dengan tugas baru. <br><br><pre> <code class="plaintext hljs">hazelcastInstance .getExecutorService(...) .submit(new Task(), ...);</code> </pre><br>  Tugas itu sendiri terlihat seperti kelas Java biasa yang mengimplementasikan Callable. <br><pre> <code class="plaintext hljs">public class Task implements Callable&lt;Long&gt; { @Override public Long call() { return 42; } }</code> </pre><br>  Masalahnya adalah klien Hazelcast tidak hanya aplikasi Java, tetapi juga aplikasi C ++, .NET, dan lainnya.  Secara alami, kami tidak dapat membuat dan mengkonversi kelas Java kami ke platform lain. <br><br>  Salah satu opsi adalah beralih menggunakan permintaan http jika kami ingin mengirim sesuatu dari satu server ke server lain dan mendapatkan jawaban.  Tetapi kemudian kita harus meninggalkan sebagian Hazelcast. <br><br>  Oleh karena itu, sebagai solusi, kami memilih untuk menggunakan antrian alih-alih ExecutorService.  Untuk melakukan ini, kami secara mandiri menerapkan mekanisme menunggu elemen dieksekusi dalam antrian, yang memproses kasus batas dan mengembalikan hasilnya ke server yang meminta. <br><br><h2>  Apa yang telah kita pelajari </h2><br>  <b>Letakkan fleksibilitas dalam sistem.</b>  Masa depan terus berubah, jadi tidak ada solusi yang sempurna.  Untuk melakukannya dengan benar, "benar" tidak berfungsi, tetapi Anda dapat mencoba menjadi fleksibel dan memasukkannya ke dalam sistem.  Ini memungkinkan kami untuk menunda keputusan arsitektur yang penting sampai saat ketika tidak lagi mustahil untuk menerimanya. <br><br>  Robert Martin dalam Arsitektur Bersih menulis tentang prinsip ini: <br><blockquote>  â€œTujuan arsitek adalah menciptakan bentuk untuk sistem yang akan menjadikan politik elemen yang paling penting, dan detail yang tidak terkait dengan politik.  Ini akan menunda dan menunda keputusan tentang perincian. " </blockquote><br><br>  <b>Alat dan solusi universal tidak ada.</b>  Jika Anda merasa bahwa beberapa kerangka kerja menyelesaikan semua masalah Anda, maka kemungkinan besar tidak demikian.  Karena itu, ketika menerapkan kerangka kerja apa pun, penting untuk memahami tidak hanya masalah apa yang akan dipecahkan, tetapi juga masalah apa yang akan dibawanya. <br><br>  <b>Jangan langsung menulis ulang semuanya.</b>  Jika Anda dihadapkan dengan masalah dalam arsitektur dan tampaknya satu-satunya solusi yang tepat adalah menulis semuanya dari awal, tunggu.  Jika masalahnya benar-benar serius, temukan perbaikan cepat dan perhatikan bagaimana sistem akan bekerja di masa depan.  Kemungkinan besar, ini bukan satu-satunya masalah dalam arsitektur, seiring waktu Anda akan menemukan lebih banyak.  Dan hanya ketika Anda mengambil sejumlah masalah yang cukup Anda dapat mulai melakukan refactoring.  Hanya dalam hal ini akan ada lebih banyak keuntungan darinya daripada nilainya. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id441590/">https://habr.com/ru/post/id441590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id441578/index.html">Bereaksi Tutorial Bagian 19: Metode Siklus Hidup Komponen</a></li>
<li><a href="../id441580/index.html">React Tutorial Bagian 20: Pelajaran Rendering Bersyarat Pertama</a></li>
<li><a href="../id441582/index.html">Optimalisasi sistem kontrol LQR</a></li>
<li><a href="../id441584/index.html">PHP Digest No. 150 (11 - 25 Februari 2019)</a></li>
<li><a href="../id441586/index.html">Cara merekomendasikan musik yang hampir tidak ada yang mendengarkan. Laporan Yandex</a></li>
<li><a href="../id441594/index.html">Napalm perusahaan</a></li>
<li><a href="../id441596/index.html">Pelabuhan antariksa swasta pertama akan dibangun di Rusia</a></li>
<li><a href="../id441598/index.html">Misi Lunar "Bereshit" - portal online dengan simulator lintasan dan pemantauan parameter penerbangan saat ini</a></li>
<li><a href="../id441600/index.html">UI lemah, programmer lemah</a></li>
<li><a href="../id441602/index.html">Mengapa mobil otomatis klasik tidak mungkin dan tidak memiliki prospek komersial</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>