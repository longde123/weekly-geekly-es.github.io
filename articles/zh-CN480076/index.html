<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐀 👨🏿‍🚒 🎺 对来自各种来源的数据进行多处理和核对 👨‍👩‍👦‍👦 🕴🏿 🧘</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="哈Ha！ 

 考虑到分布式系统的多样性，目标存储中已验证信息的可用性是数据一致性的重要标准。 

 有许多方法和方法可以达到这种效果，我们将重点放在和解上，本文在本文中讨论了其理论方面。 我建议考虑该系统的实际实现，该系统可扩展并适用于大量数据。 

 如何在良好的旧Python上实现这种情况-请...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>对来自各种来源的数据进行多处理和核对</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480076/">哈Ha！ <br><br> 考虑到分布式系统的多样性，目标存储中已验证信息的可用性是数据一致性的重要标准。 <br><br> 有许多方法和方法可以达到这种效果，我们将重点放在和解上，本文<a href="https://habr.com/ru/post/428443/">在本文</a>中讨论了其理论方面<a href="https://habr.com/ru/post/428443/">。</a> 我建议考虑该系统的实际实现，该系统可扩展并适用于大量数据。 <br><br> 如何在良好的旧Python上实现这种情况-请切入阅读！ 走吧 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ic/zx/hg/iczxhgu9zvlumwggetuoblxm1ra.jpeg"></div><br>  <a href="https://www.megapixl.com/alexdobysh-stock-images-videos-portfolio" rel="nofollow">（图片来源）</a> <br><a name="habracut"></a><br><h2> 引言 </h2><br> 假设一个金融机构有几个分布式系统，我们面临着验证这些系统中的交易并将对帐后的数据上传到目标存储的任务。 <br><br> 作为数据源，在PostgreSQL数据库中获取一个大文本文件和一个表。 假设这些源中的数据具有相同的事务，但是它们可能具有差异，因此需要对其进行验证并将其写入最终存储中的已验证数据以进行分析。 <br><br> 此外，有必要在同一数据库上并行启动多个对帐，并使用多处理功能使系统适应大量需求。 <br><br>  <a href="https://docs.python.org/dev/library/multiprocessing.html" rel="nofollow">多处理</a>模块非常适合在Python中并行化操作，从某种意义上说，它规避了某些GIL缺陷。 我们将在下面使用此库的功能。 <br><br><h2> 正在开发的系统架构 </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/je/dm/hu/jedmhumxsx9d-mxu-bbfbzqbulq.png"></div><br> 使用的组件： <br><br><ul><li>  <b>随机数据生成器</b> -生成CSV文件并在其基础上填充数据库表的Python脚本； </li><li>  <b>数据源</b> -PostgreSQL数据库中的CSV文件和表； </li><li>  <b>适配器</b> -在这种情况下，我们使用两个适配器，将从它们的源（CSV或数据库）中提取数据并将信息输入中间数据库； </li><li>  <b>数据库</b> -共有三部分：原始数据，存储适配器获取的信息的中间数据库以及包含来自这两个源的已协调事务的“干净”数据库。 </li></ul><br><h2> 初步训练 </h2><br> 作为数据存储工具，我们将<a href="https://hub.docker.com/_/postgres" rel="nofollow">在Docker容器中</a>使用<a href="https://hub.docker.com/_/postgres" rel="nofollow">PostgreSQL数据库，</a>并通过<a href="https://hub.docker.com/r/dpage/pgadmin4/" rel="nofollow">在容器中运行的pgAdmin</a>与我们的数据库进行交互： <br><br><pre><code class="bash hljs">docker run --name pg -d -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_USER=my_user"</span></span> -e <span class="hljs-string"><span class="hljs-string">"POSTGRES_PASSWORD=my_password"</span></span> postgres</code> </pre> <br> 运行pgAdmin： <br><br><pre> <code class="bash hljs">docker run -p 80:80 -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_EMAIL=user@domain.com"</span></span> -e <span class="hljs-string"><span class="hljs-string">"PGADMIN_DEFAULT_PASSWORD=12345"</span></span> -d dpage/pgadmin4</code> </pre> <br> 一切开始之后，请不要忘记在配置文件（conf / db.ini）中指定数据库的连接字符串（对于培训示例，可以！）： <br><br><pre> <code class="bash hljs">[POSTGRESQL] db_url=postgresql://my_user:my_password@172.17.0.2:5432/my_user</code> </pre><br> 原则上，容器的使用是可选的，您可以使用数据库服务器。 <br><br><h2> 输入产生 </h2><br>  Python脚本<b>generate_test_data</b>负责生成测试数据，该数据需要生成所需数量的条目。 可以通过<b>GenerateTestData</b>类的主要功能轻松跟踪操作顺序： <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta"> @m.timing def run(self, num_rows): """ Run the process """ m.info('START!') self.create_db_schema() self.create_folder('data') self.create_csv_file(num_rows) self.bulk_copy_to_db() self.random_delete_rows() self.random_update_rows() m.info('END!')</span></span></code> </pre> <br> 因此，该函数执行以下步骤： <br><br><ul><li> 在数据库中创建架构（我们创建所有主要架构和表）； </li><li> 创建一个用于存储测试文件的文件夹； </li><li> 生成具有给定行数的测试文件； </li><li> 将数据批量插入目标表transaction_db_raw.transaction_log; </li><li> 意外删除该表中的多行； </li><li> 此表中几行的随机更新。 </li></ul><br> 删除和修改是必需的，以便比较的对象至少有一些差异。 能够找到这些差异很重要！ <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing @m.wrapper(m.entering, m.exiting) def random_delete_rows(self): """ Random deleting some rows from the table """ sql_command = sql.SQL(""" delete from {0}.{1} where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been deleted [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror) @m.timing @m.wrapper(m.entering, m.exiting) def random_update_rows(self): """ Random update some rows from the table """ sql_command = sql.SQL(""" update {0}.{1} set transaction_amount = round(random()::numeric, 2) where ctid = any(array( select ctid from {0}.{1} tablesample bernoulli (1) ))""").format(sql.Identifier(self.schema_raw), sql.Identifier(self.raw_table_name)) try: rows = self.database.execute(sql_command) m.info('Has been updated [%s rows] from table %s' % (rows, self.raw_table_name)) except psycopg2.Error as err: m.error('Oops! Delete random rows has been FAILED. Reason: %s' % err.pgerror)</span></span></code> </pre> <br> 生成测试数据集并随后将其记录为CSV格式的文本文件如下： <br><br><ul><li> 创建一个随机事务UID； </li><li> 创建一个随机的UID帐号（默认情况下，我们使用十个唯一的帐号，但是可以使用配置文件通过更改“ random_accounts”参数来更改此值）； </li><li> 交易日期-从配置文件中指定的日期（initial_date）开始的随机日期； </li><li> 交易类型（交易/佣金）； </li><li> 交易金额； </li><li> 数据生成的主要工作由<b>TestDataCreator</b>类的<i>generate_test_data_by_chunk</i>方法执行： </li></ul><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def generate_test_data_by_chunk(self, chunk_start, chunk_end): """ Generating and saving to the file """ num_rows_mp = chunk_end - chunk_start new_rows = [] for _ in range(num_rows_mp): transaction_uid = uuid.uuid4() account_uid = choice(self.list_acc) transaction_date = (self.get_random_date(self.date_in, 0) .__next__() .strftime('%Y-%m-%d %H:%M:%S')) type_deal = choice(self.list_type_deal) transaction_amount = randint(-1000, 1000) new_rows.append([transaction_uid, account_uid, transaction_date, type_deal, transaction_amount]) self.write_in_file(new_rows, chunk_start, chunk_end)</span></span></code> </pre> <br><blockquote> 此功能的一个特点是在多个并行异步进程中启动，每个进程都会生成自己的50K记录部分。 这个“芯片”将使您足够快地在几百万行上创建文件 </blockquote><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run_csv_writing</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Writing the test data into csv file """</span></span> pool = mp.Pool(mp.cpu_count()) jobs = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> chunk_start, chunk_end <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.divide_into_chunks(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.num_rows): jobs.append(pool.apply_async(self.generate_test_data_by_chunk, (chunk_start, chunk_end))) <span class="hljs-comment"><span class="hljs-comment"># wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join()</span></span></code> </pre> <br> 文本文件完成后，处理bulk_insert命令，并且该文件中的所有数据均落入<b>transaction_db_raw.transaction_log</b>表中<b>。</b> <br><br> 此外，这两个源将包含完全相同的数据，对帐不会发现任何有趣的内容，因此我们删除并更改了数据库中的几个随机行。 <br><br> 运行脚本并生成包含10,000行交易的测试CSV文件： <br><br><pre> <code class="bash hljs">./generate_test_data.py 10000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4c/cp/hm/4ccphmc5dcjcgxuy54p_9limlz4.png"></div><br> 屏幕截图显示已接收到10K行的文件，将10K行加载到数据库中，然后从数据库中删除了112行，并更改了108行，结果：数据库中的文件和表相差220个条目。 <br><br> 您问：“嗯，多处理在哪里？” <br> 当您生成更大的文件时，可以看到它的工作，而不是10K条记录，而是1M条记录。 我们会尝试吗？ <br><br><pre> <code class="bash hljs">./generate_test_data.py 1000000</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rw/_a/ne/rw_aneqnairixqk-wglpqxgjvkc.png"></div><br> 加载数据，删除和更改随机记录后，我们从表中看到了文本文件的区别：19,939行（其中10022条是随机删除的，而9917条是更改的）。 <br><br><blockquote> 图片显示记录的生成是异步的，不一致的。 这意味着下一个过程可以在不考虑开始顺序的情况下立即开始，而前一个过程就完成了。 不能保证结果与输入的顺序相同。 </blockquote><br><div class="spoiler">  <b class="spoiler_title">肯定更快吗？</b> <div class="spoiler_text"> 在15.5秒内“发明”了不在最快的虚拟机上的100万行-这是一个值得选择的选择。 在不使用多处理的情况下按顺序启动了同一代，我得到的结果是：文件生成的速度慢了三倍多（超过了52秒而不是15.5秒）： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sb/kb/ck/sbkbckylzluoyyslyflwak5udrq.png"></div><br></div></div><br><h2>  CSV适配器 </h2><br> 该适配器对行进行哈希处理，仅保留第一列（事务标识符）不变，并将接收到的<i>数据</i>保存到<i>data / transaction_hashed.csv</i>文件中。 他的工作的最后一步是使用COPY命令将此文件加载到<b>reconciliation_db</b>模式的临时表中<b>。</b> <br><br> 最佳的文件读取是通过几个并行过程执行的。 我们逐行读取，每个读取5兆字节。 通过经验方法获得数字“ 5兆字节”。 正是由于只有一小段文字，我们才能够在最短的时间内读取虚拟机上的大文件。 您可以使用此参数在您的环境中进行实验，并查看运行时间将如何变化： <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@m.timing def process_wrapper(self, chunk_start, chunk_size): """ Read a particular chunk """ with open(self.file_name_raw, newline='\n') as file: file.seek(chunk_start) lines = file.read(chunk_size).splitlines() for line in lines: self.process(line) def chunkify(self, size=1024*1024*5): """ Return a new chunk """ with open(self.file_name_raw, 'rb') as file: chunk_end = file.tell() while True: chunk_start = chunk_end file.seek(size, 1) file.readline() chunk_end = file.tell() if chunk_end &gt; self.file_end: chunk_end = self.file_end yield chunk_start, chunk_end - chunk_start break else: yield chunk_start, chunk_end - chunk_start @m.timing def run_reading(self): """ The main method for the reading """ # init objects pool = mp.Pool(mp.cpu_count()) jobs = [] m.info('Run csv reading...') # create jobs for chunk_start, chunk_size in self.chunkify(): jobs.append(pool.apply_async(self.process_wrapper, (chunk_start, chunk_size))) # wait for all jobs to finish for job in jobs: job.get() # clean up pool.close() pool.join() m.info('CSV file reading has been completed')</span></span></code> </pre> <br> 读取1M记录上先前创建的文件的示例： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9p/z1/zr/9pz1zrkzeelnep_r8oppk0sxhok.png"></div><br> 屏幕快照显示了如何为当前对帐运行创建一个具有唯一名称的临时表。 接下来是部分异步读取文件并获取每一行的哈希值。 将数据从适配器插入目标表即可完成此适配器的工作。 <br><blockquote> 为每个对帐过程使用具有唯一名称的临时表，可以使您在一个数据库中额外并行化对帐过程。 </blockquote><br><h2>  PostgreSQL适配器 </h2><br> 用于处理表中存储的数据的适配器与文件适配器的逻辑大致相同： <br><br><ul><li> 读取表的某些部分（如果很大，则超过10万个条目），并对除事务标识符之外的所有列进行哈希处理； </li><li> 然后将已处理的数据插入到<b>reconciliation_db</b>表中<b>。</b>  <b>存储_ $（int（time.time（））</b> 。 </li></ul><br> 该适配器的一个有趣特性是它使用一个到数据库的连接池，该池将通过索引在表中搜索必要的数据并进行处理。 <br><br> 根据表的大小，计算处理所需的进程数，并且在每个进程中将其划分为10个任务。 <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Read the data from the postgres and shared those records with each processor to perform their operation using threads """</span></span> threads_array = self.get_threads(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.max_id_num_row, self.pid_max) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> pid <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(threads_array) + <span class="hljs-number"><span class="hljs-number">1</span></span>): m.info(<span class="hljs-string"><span class="hljs-string">'Process %s'</span></span> % pid) <span class="hljs-comment"><span class="hljs-comment"># Getting connection from the connection pool select_conn = self._select_conn_pool.getconn() select_conn.autocommit = 1 # Creating 10 process to perform the operation process = Process(target=self.process_data, args=(self.data_queque, pid, threads_array[pid-1][0], threads_array[pid-1][1], select_conn)) process.daemon = True process.start() process.join() select_conn.close()</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/kt/kk/pwktkkisxg3sud4dyss_gtyi4gy.png"></div><br><h2> 搜索差异 </h2><br> 我们继续验证从两个适配器接收到的数据。 <br><br> 使用SQL语言的所有功能，对帐（或接收差异报告）发生在数据库的服务器端。 <br><br>  SQL查询非常简单-它只是一个表联接，通过事务ID将数据从适配器传递到自身： <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" select s1.adapter_name, count(s1.transaction_uid) as tran_count from {0}.{1} s1 full join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name and s2.hash = s1.hash where s2.transaction_uid is null group by s1.adapter_name;"""</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table))</code> </pre><br> 输出为报告： <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5c/ou/gy/5cougys1gkflsplq2hvkoleooto.png"></div><br> 检查上面图片中的所有内容是否正确。 我们记得从数据库表中删除了9917，并更改了10022行。 报告中共显示19939行。 <br><br><h2> 汇总表 </h2><br> 仅保留将在各个方面（通过哈希）匹配的不同适配器中的“干净”事务插入存储表中。 此过程由以下SQL查询执行： <br><br><pre> <code class="python hljs">sql_command = sql.SQL(<span class="hljs-string"><span class="hljs-string">""" with reconcil_data as ( select s1.transaction_uid from {0}.{1} s1 join {0}.{1} s2 on s2.transaction_uid = s1.transaction_uid and s2.adapter_name != s1.adapter_name where s2.hash = s1.hash and s1.adapter_name = 'postresql_adapter' ) insert into {2}.transaction_log select t.transaction_uid, t.account_uid, t.transaction_date, t.type_deal, t.transaction_amount from {3}.transaction_log t join reconcil_data r on t.transaction_uid = r.transaction_uid where not exists ( select 1 from {2}.transaction_log tl where tl.transaction_uid = t.transaction_uid ) """</span></span>).format(sql.Identifier(self.schema_target), sql.Identifier(self.storage_table), sql.Identifier(self.schema_db_clean), sql.Identifier(self.schema_raw))</code> </pre><br> 我们用作适配器的中间数据存储的临时表可以删除。 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uq/sr/te/uqsrte2g0thu2woasaxqojdbc88.png"></div><br><h2> 结论 </h2><br> 在完成工作的过程中，开发了一种用于协调来自各种来源的数据的系统：文本文件和数据库中的表格。 最少使用其他工具。 <br><br> 也许老练的读者可能会注意到，使用诸如Apache Spark之类的框架，再加上将源数据转换为镶木地板格式，可以大大加快此过程，特别是对于大批量交易而言。 但是这项工作的主要目标是用裸Python编写系统并研究多处理数据处理。 我认为我们已经处理了什么。 <br><br> 整个项目的源代码位于<a href="https://github.com/igorgorbenko/transact_reconciliation" rel="nofollow">我在GitHub上的存储库中</a> ，建议您熟悉一下它。 <br><br> 我很乐意回答所有问题并结识您的意见。 <br><br> 祝你成功！ </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN480076/">https://habr.com/ru/post/zh-CN480076/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN480062/index.html">10个控制系统。 在任务上进行交流和共享文件在哪里更方便？</a></li>
<li><a href="../zh-CN480064/index.html">学习按主题分组的单词</a></li>
<li><a href="../zh-CN480068/index.html">[更新]我们的人民遭到殴打，我们会保持沉默吗？</a></li>
<li><a href="../zh-CN480070/index.html">反应收益：企业的福气？</a></li>
<li><a href="../zh-CN480072/index.html">Kubernetes：为什么设置系统资源管理如此重要？</a></li>
<li><a href="../zh-CN480078/index.html">React外设上的新前端库</a></li>
<li><a href="../zh-CN480080/index.html">您在做笔记应用程序时需要什么？</a></li>
<li><a href="../zh-CN480082/index.html">在MySQL中为Zabbix使用分区和大量监视对象</a></li>
<li><a href="../zh-CN480086/index.html">如何符合152-FZ的要求，保护客户的个人数据，而不是踩我们的耙子</a></li>
<li><a href="../zh-CN480088/index.html">DevOps-好，但是该怎么办？ 如何减少体力劳动并取得理想的结果</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>