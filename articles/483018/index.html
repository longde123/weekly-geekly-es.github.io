<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ôäÔ∏è ‚òÆÔ∏è üíë M√°scara-R CNN desde principiante hasta profesional üç• üëèüèø üõë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Una vez que necesitaba analizar la informaci√≥n de la imagen y en la salida para tener el tipo del objeto, su tipo, y tambi√©n, analizar el conjunto de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>M√°scara-R CNN desde principiante hasta profesional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/483018/"><p><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><p>  Una vez que necesitaba analizar la informaci√≥n de la imagen y en la salida para tener el tipo del objeto, su tipo, y tambi√©n, analizar el conjunto de cuadros, necesitaba dar el identificador del objeto y el tiempo que pas√≥ en el cuadro, era necesario determinar c√≥mo se mov√≠a el objeto y qu√© c√°maras aparec√≠an a la vista.  Comencemos, quiz√°s, con los dos primeros, el an√°lisis del personal en conjunto se discutir√° en la siguiente parte. </p><a name="habracut"></a><br><p>  Bueno, describiremos con m√°s detalle nuestras tareas: </p><br><ul><li>  Repare personas y autom√≥viles: selecci√≥nelos en la imagen y genere las instancias de clase correspondientes con los campos necesarios. </li><li>  Determine el n√∫mero del autom√≥vil, si cay√≥ en el marco de una c√°mara espec√≠fica </li><li>  Compare el marco actual con el anterior para la igualdad de objetos, para que podamos descubrir </li></ul><br><p>  Ok, pens√©, y tom√© una serpiente gruesa, pit√≥n, eso significa.  Se decidi√≥ utilizar la red neuronal <a href="https://github.com/matterport/Mask_RCNN">Mask R-Cnn</a> en relaci√≥n con su simplicidad y <a href="https://habr.com/ru/post/421299/">caracter√≠sticas modernas</a> .  Adem√°s, por supuesto, utilizaremos OpenCV para la manipulaci√≥n de im√°genes. </p><br><h2 id="ustanovka-sredy">  Configuraci√≥n del entorno </h2><br><p>  Usaremos Windows 10, porque es m√°s probable que lo use. <br>  Se entiende que ya tienes Python de 64 bits.  De lo contrario, puede descargar el paquete, por ejemplo, <a href="https://www.python.org/downloads/release/python-374/">desde aqu√≠.</a> </p><br><h3 id="ustanovka-paketov">  Instalaci√≥n de paquete </h3><br><pre><code class="plaintext hljs">git clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip3 install -r requirements.txt python3 setup.py install</code> </pre> <br><p>  Si por alguna raz√≥n no es posible compilar desde la fuente, hay una versi√≥n de pip: </p><br><pre> <code class="plaintext hljs">pip3 install mrcnn --user</code> </pre> <br><p>  El paquete, por supuesto, viene con todas las <a href="https://github.com/matterport/Mask_RCNN/blob/master/requirements.txt">dependencias</a> . </p><br><h2 id="etap-1-sozdanie-prosteyshey-programmy-raspoznavatelya">  Etapa 1. Crear un reconocedor simple. </h2><br><p>  Haremos las importaciones necesarias </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN</code> </pre> <br><p>  La red neuronal requiere crear una configuraci√≥n con campos anulados </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MaskRCNNConfig</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(mrcnn.config.Config)</span></span></span><span class="hljs-class">:</span></span> NAME = <span class="hljs-string"><span class="hljs-string">"coco_pretrained_model_config"</span></span> GPU_COUNT = <span class="hljs-number"><span class="hljs-number">1</span></span> IMAGES_PER_GPU = <span class="hljs-number"><span class="hljs-number">1</span></span> DETECTION_MIN_CONFIDENCE = <span class="hljs-number"><span class="hljs-number">0.8</span></span> <span class="hljs-comment"><span class="hljs-comment">#     NUM_CLASSES = 81</span></span></code> </pre> <br><p>  Indique la ubicaci√≥n del archivo con las escalas.  Deje que en este ejemplo est√© en la carpeta con este archivo.  Si no es as√≠, se descargar√°. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils DATASET_FILE = <span class="hljs-string"><span class="hljs-string">"mask_rcnn_coco.h5"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(DATASET_FILE): mrcnn.utils.download_trained_weights(DATASET_FILE)</code> </pre> <br><p>  Creemos nuestro modelo con la configuraci√≥n anterior </p><br><pre> <code class="python hljs">model = MaskRCNN(mode=<span class="hljs-string"><span class="hljs-string">"inference"</span></span>, model_dir=<span class="hljs-string"><span class="hljs-string">"logs"</span></span>, config=MaskRCNNConfig()) model.load_weights(DATASET_FILE, by_name=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  Y quiz√°s comencemos a procesar todas las im√°genes en el directorio de <code>images</code> en el directorio actual. </p><br><pre> <code class="python hljs">IMAGE_DIR = os.path.join(os.getcwd(), <span class="hljs-string"><span class="hljs-string">"images"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(IMAGE_DIR): image = cv2.imread(os.path.join(IMAGE_DIR, filename)) rgb_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] detections = model.detect([rgb_image], verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  ¬øQu√© veremos en las detecciones? </p><br><pre> <code class="python hljs"> print(detections)</code> </pre> <br><p>  Por ejemplo, algo similar: </p><br><pre> <code class="plaintext hljs">{'rois': array([[ 303, 649, 542, 1176],[ 405, 2, 701, 319]]), 'class_ids': array([3, 3]), 'scores': array([0.99896, 0.99770015], dtype=float32), 'masks': array()}</code> </pre> <br><p>  En este caso, se encontraron 2 objetos. <br>  <code>rois</code> : conjuntos de coordenadas de la esquina inferior izquierda y superior derecha <br>  <code>class_ids</code> son los identificadores num√©ricos de los objetos encontrados, mientras que necesitamos saber que 1 es una persona, 3 es un autom√≥vil, 8 es un cami√≥n. <br>  <code>scores</code> : en la medida en que el modelo conf√≠e en la soluci√≥n, este par√°metro puede <code>DETECTION_MIN_CONFIDENCE</code> mediante <code>DETECTION_MIN_CONFIDENCE</code> en la configuraci√≥n, cortando todas las opciones inapropiadas. <br>  <code>masks</code> : el contorno del objeto.  Los datos se usan para dibujar una m√°scara de objeto.  Porque  son bastante voluminosos y no est√°n destinados a la comprensi√≥n humana; no los citar√© en el art√≠culo. </p><br><p>  Ok, podr√≠amos detenernos all√≠, pero ¬øqueremos ver la imagen que gu√≠a sobre el uso de redes neuronales con objetos bellamente seleccionados? </p><br><p>  Ser√≠a m√°s simple llamar a la funci√≥n <code>mrcnn.visualize.display_instances</code> , pero no haremos esto, escribiremos la nuestra. </p><br><p>  La funci√≥n tomar√° una imagen y los principales par√°metros obtenidos del diccionario de los primeros pasos. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">visualize_detections</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, masks, boxes, class_ids, scores)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np bgr_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] CLASS_NAMES = [<span class="hljs-string"><span class="hljs-string">'BG'</span></span>,<span class="hljs-string"><span class="hljs-string">"person"</span></span>, <span class="hljs-string"><span class="hljs-string">"bicycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"car"</span></span>, <span class="hljs-string"><span class="hljs-string">"motorcycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"bus"</span></span>, <span class="hljs-string"><span class="hljs-string">"truck"</span></span>] COLORS = mrcnn.visualize.random_colors(len(CLASS_NAMES)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(boxes.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]): y1, x1, y2, x2 = boxes[i] classID = class_ids[i] label = CLASS_NAMES[classID] font = cv2.FONT_HERSHEY_DUPLEX color = [int(c) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> c <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.array(COLORS[classID]) * <span class="hljs-number"><span class="hljs-number">255</span></span>] text = <span class="hljs-string"><span class="hljs-string">"{}: {:.3f}"</span></span>.format(label, scores[i]) size = <span class="hljs-number"><span class="hljs-number">0.8</span></span> width = <span class="hljs-number"><span class="hljs-number">2</span></span> cv2.rectangle(bgr_image, (x1, y1), (x2, y2), color, width) cv2.putText(bgr_image, text, (x1, y1<span class="hljs-number"><span class="hljs-number">-20</span></span>), font, size, color, width)</code> </pre><br><p><img src="https://lh3.googleusercontent.com/dnSAiGZW32zMK92_T8yyk2nXCFCKECQ_eSdNiVv5Bzpz9TOsBZT6_jyAY-LfUT4c0jCzfdgFOCpy6_0HzT54CAPo3vvkZ6VgR1U5cdmSTb0zLLpAxJjX-_pTNUnpIExXsao_u29b"></p><br><div class="spoiler">  <b class="spoiler_title">Imagen de origen</b> <div class="spoiler_text"><p><img src="https://sun9-10.userapi.com/c854324/v854324789/e2f73/nJHcTGLnWI4.jpg"></p></div></div><br><p>  Aunque una de las principales ventajas de esta red neuronal es la soluci√≥n a los problemas de segmentaci√≥n de instancias: obtener los contornos de los objetos, a√∫n no lo hemos utilizado, lo analizaremos. </p><br><p>  Para implementar m√°scaras, agregue un par de l√≠neas antes de dibujar un rect√°ngulo para cada objeto encontrado. </p><br><pre> <code class="python hljs">mask = masks[:, :, i] <span class="hljs-comment"><span class="hljs-comment">#   image = mrcnn.visualize.apply_mask(image, mask, color, alpha=0.6) #  </span></span></code> </pre> <br><p>  Resultado: <br><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><div class="spoiler">  <b class="spoiler_title">Versi√≥n con m√°scaras blancas.</b> <div class="spoiler_text"><p><img src="https://lh5.googleusercontent.com/JUUlUQfOCc9jeeuzMDiSc2Hd06P2aMVli2UPSRkUoxlNVwdlwEfi-BHmuyzOsx-nlvm19lPmYlyxFGYV9xGzpppXRORmDBLtLYH6UcYRh1zO47ROLb04HgUggDoz14Zk2AWZ3ta3"></p></div></div><br><h2 id="etap-ii-pervye-uspehi-raspoznavanie-nomerov-mashin">  Etapa II  Primeros √©xitos.  Reconocimiento de n√∫meros de autom√≥viles. </h2><br><p>  Para el reconocimiento, necesitamos un cuadro claro del autom√≥vil cerca, por lo que se decidi√≥ tomar solo cuadros del punto de control y luego compararlos con la similitud (m√°s sobre eso en el pr√≥ximo cap√≠tulo).  Este m√©todo, sin embargo, da demasiada imprecisi√≥n, porque  Las m√°quinas pueden ser muy similares visualmente y mi algoritmo a√∫n no puede evitar tales situaciones. </p><br><p>  Se decidi√≥ utilizar una lib preparada del fabricante ucraniano <a href="https://github.com/ria-com/nomeroff-net">nomeroff-net</a> (no publicidad).  Porque  casi todo el c√≥digo se puede encontrar en los ejemplos para el modelo, luego no dar√© una descripci√≥n completa. </p><br><p>  Solo puedo decir que esta funci√≥n se puede iniciar con la imagen original o que la m√°quina reconocida se puede cortar del marco y pasar a esta funci√≥n. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os sys.path.append(cfg.NOMEROFF_NET_DIR) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> NomeroffNet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> filters, RectDetector, TextDetector, OptionsDetector, Detector, textPostprocessing nnet = Detector(cfg.MASK_RCNN_DIR, cfg.MASK_RCNN_LOG_DIR) nnet.loadModel(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) rectDetector = RectDetector() optionsDetector = OptionsDetector() optionsDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) textDetector = TextDetector.get_static_module(<span class="hljs-string"><span class="hljs-string">"ru"</span></span>)() textDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectCarNumber</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(imgPath: str)</span></span></span><span class="hljs-function"> -&gt; str:</span></span> img = mpimg.imread(imgPath) NP = nnet.detect([img]) cvImgMasks = filters.cv_img_mask(NP) arrPoints = rectDetector.detect(cvImgMasks) zones = rectDetector.get_cv_zonesBGR(img, arrPoints) regionIds, stateIds, _c = optionsDetector.predict(zones) regionNames = optionsDetector.getRegionLabels(regionIds) <span class="hljs-comment"><span class="hljs-comment"># find text with postprocessing by standart textArr = textDetector.predict(zones) textArr = textPostprocessing(textArr, regionNames) return textArr</span></span></code> </pre> <br><p>  La salida textArr representar√° una serie de cadenas con los n√∫meros de m√°quinas que se encuentran en el marco, por ejemplo: <br>  <code>["293163"]</code> , o <code>[""]</code> , <code>[]</code> - si no se encontraron n√∫meros coincidentes. </p><br><h2 id="etap-iii-opoznaem-obekty-na-shozhest">  Etapa III  Identificar objetos por similitud. </h2><br><p>  Ahora necesitamos entender c√≥mo arreglar un objeto una vez, entender que es √©l en el siguiente cuadro.  En esta etapa, asumiremos que tenemos una sola c√°mara y solo distinguiremos entre diferentes marcos. </p><br><p>  Para hacer esto, debe averiguar c√≥mo compararemos los dos objetos. </p><br><p>  Propondr√© un algoritmo de <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">cribado</a> para estos fines.  Hacemos una reserva de que no es parte de la parte principal de OpenCV, por lo que necesitamos entregar m√≥dulos contrib adicionales.  Desafortunadamente, el algoritmo est√° patentado y su uso en programas comerciales es limitado.  Pero estamos enfocados en actividades de investigaci√≥n, ¬øverdad? </p><br><pre> <code class="plaintext hljs">pip3 install opencv-contrib-python --user</code> </pre> <br><p>  ~~ Sobrecargar el operador == ~~ Escribimos una funci√≥n que toma 2 objetos comparados en forma de matrices.  Por ejemplo, los obtenemos despu√©s de llamar a la funci√≥n <code>cv2.open(path)</code> </p><br><p>  Escribiremos una implementaci√≥n de nuestro algoritmo. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img1, img2)</span></span></span><span class="hljs-function"> -&gt; bool:</span></span> sift = cv2.xfeatures2d.SIFT_create()</code> </pre> <br><p>  Encuentre puntos clave y descriptores utilizando SIFT.  Quiz√°s no proporcione ayuda para estas funciones, porque siempre puede llamarlo en el shell interactivo como <code>help(somefunc)</code> </p><br><pre> <code class="python hljs"> kp1, des1 = sift.detectAndCompute(img1, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) kp2, des2 = sift.detectAndCompute(img2, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)</code> </pre> <br><p>  Configura nuestro algoritmo. </p><br><pre> <code class="python hljs"> FLANN_INDEX_KDTREE = <span class="hljs-number"><span class="hljs-number">0</span></span> indexParams = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number"><span class="hljs-number">5</span></span>) searchParams = dict(checks=<span class="hljs-number"><span class="hljs-number">50</span></span>) flann = cv2.FlannBasedMatcher(indexParams, searchParams)</code> </pre> <br><p>  Ahora ejec√∫talo. </p><br><pre> <code class="python hljs"> matches = flann.knnMatch(des1, des2, k=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><p>  Cuenta las similitudes entre las im√°genes. </p><br><pre> <code class="python hljs"> matchesCount = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> m, n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> matches: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> m.distance &lt; cfg.cencitivity*n.distance: matchesCount += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> matchesCount &gt; cfg.MIN_MATCH_COUNT</code> </pre> <br><p>  Ahora intenta usarlo <br>  Para hacer esto, despu√©s de detectar objetos, necesitamos cortarlos de la imagen original. </p><br><p>  No podr√≠a escribir nada mejor que guardarlo para memoria lenta, y luego leer desde all√≠. </p><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">extractObjects</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(objects, binaryImage, outputImageDirectory, filename=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> objects: y1, x1, y2, x2 = item.coordinates <span class="hljs-comment"><span class="hljs-comment">#       cropped = binaryImage[y1:y2, x1:x2] beforePoint, afterPoint = filename.split(".") outputDirPath = os.path.join(os.path.split(outputImageDirectory)[0], "objectsOn" + beforePoint) if not os.path.exists(outputDirPath): os.mkdir(outputDirPath) coordinates = str(item).replace(" ", ",") pathToObjectImage = "{}{}.jpg".format(item.type, coordinates) cv2.imwrite(os.path.join(outputDirPath, str(pathToObjectImage)), cropped)</span></span></code> </pre> <br><p>  Ahora tenemos los objetos en el <code>&lt;outputImageDirectory&gt;/objectsOn&lt;imageFilename&gt;</code> </p><br><p>  Ahora, si tenemos al menos 2 de esos directorios, podemos comparar los objetos en ellos.  Ejecute la funci√≥n escrita anteriormente </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> compareImages(previousObjects, currentObjects): print(‚Äú  !‚Äù)</code> </pre> <br><p>  O podemos hacer otra acci√≥n, como marcar estos objetos con el mismo identificador. </p><br><p>  Por supuesto, como todas las redes neuronales, esta tiende a dar resultados a veces err√≥neos. </p><br><p>  En general, hemos completado las 3 tareas establecidas al principio, por lo que terminaremos.  Dudo que este art√≠culo haya abierto los ojos de personas que han escrito al menos un programa que resuelve los problemas de reconocimiento de im√°genes / segmentaci√≥n de im√°genes, pero espero haber ayudado al menos a un desarrollador novato). </p><br><p>  El c√≥digo fuente completo del proyecto se puede encontrar <a href="https://github.com/Sapfir0/premier-eye">aqu√≠</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/483018/">https://habr.com/ru/post/483018/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../483004/index.html">Efecto Kuleshov en Disco Elysium: c√≥mo el contexto crea significado</a></li>
<li><a href="../483008/index.html">Otro futuro: una divisi√≥n de la humanidad</a></li>
<li><a href="../483012/index.html">Antig√ºedades: Roland MT-32, un sonido alternativo para juegos de DOS</a></li>
<li><a href="../483014/index.html">Colas de mensajes PostgreSQL usando PgQ</a></li>
<li><a href="../483016/index.html">Una breve historia de los microprocesadores espaciales, segunda parte</a></li>
<li><a href="../483024/index.html">"¬øQu√© hicieron las corporaciones con su privacidad?", Arthur Khachuyan (Tazeros Global)</a></li>
<li><a href="../483026/index.html">Java / Spring: C√≥mo generar completamente una API CRUD REST usando Speedment</a></li>
<li><a href="../483030/index.html">API que te hace llorar</a></li>
<li><a href="../483032/index.html">Mudarse de la CEI a la Rep√∫blica Checa, experiencia propia (parte 2)</a></li>
<li><a href="../483036/index.html">Problema de finalizaci√≥n de n-Queens: algoritmo de soluci√≥n lineal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>