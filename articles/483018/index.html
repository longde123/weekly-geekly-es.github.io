<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>♊️ ☮️ 💑 Máscara-R CNN desde principiante hasta profesional 🍥 👏🏿 🛑</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Una vez que necesitaba analizar la información de la imagen y en la salida para tener el tipo del objeto, su tipo, y también, analizar el conjunto de ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Máscara-R CNN desde principiante hasta profesional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/483018/"><p><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><p>  Una vez que necesitaba analizar la información de la imagen y en la salida para tener el tipo del objeto, su tipo, y también, analizar el conjunto de cuadros, necesitaba dar el identificador del objeto y el tiempo que pasó en el cuadro, era necesario determinar cómo se movía el objeto y qué cámaras aparecían a la vista.  Comencemos, quizás, con los dos primeros, el análisis del personal en conjunto se discutirá en la siguiente parte. </p><a name="habracut"></a><br><p>  Bueno, describiremos con más detalle nuestras tareas: </p><br><ul><li>  Repare personas y automóviles: selecciónelos en la imagen y genere las instancias de clase correspondientes con los campos necesarios. </li><li>  Determine el número del automóvil, si cayó en el marco de una cámara específica </li><li>  Compare el marco actual con el anterior para la igualdad de objetos, para que podamos descubrir </li></ul><br><p>  Ok, pensé, y tomé una serpiente gruesa, pitón, eso significa.  Se decidió utilizar la red neuronal <a href="https://github.com/matterport/Mask_RCNN">Mask R-Cnn</a> en relación con su simplicidad y <a href="https://habr.com/ru/post/421299/">características modernas</a> .  Además, por supuesto, utilizaremos OpenCV para la manipulación de imágenes. </p><br><h2 id="ustanovka-sredy">  Configuración del entorno </h2><br><p>  Usaremos Windows 10, porque es más probable que lo use. <br>  Se entiende que ya tienes Python de 64 bits.  De lo contrario, puede descargar el paquete, por ejemplo, <a href="https://www.python.org/downloads/release/python-374/">desde aquí.</a> </p><br><h3 id="ustanovka-paketov">  Instalación de paquete </h3><br><pre><code class="plaintext hljs">git clone https://github.com/matterport/Mask_RCNN cd Mask_RCNN pip3 install -r requirements.txt python3 setup.py install</code> </pre> <br><p>  Si por alguna razón no es posible compilar desde la fuente, hay una versión de pip: </p><br><pre> <code class="plaintext hljs">pip3 install mrcnn --user</code> </pre> <br><p>  El paquete, por supuesto, viene con todas las <a href="https://github.com/matterport/Mask_RCNN/blob/master/requirements.txt">dependencias</a> . </p><br><h2 id="etap-1-sozdanie-prosteyshey-programmy-raspoznavatelya">  Etapa 1. Crear un reconocedor simple. </h2><br><p>  Haremos las importaciones necesarias </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.config <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mrcnn.model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaskRCNN</code> </pre> <br><p>  La red neuronal requiere crear una configuración con campos anulados </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MaskRCNNConfig</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(mrcnn.config.Config)</span></span></span><span class="hljs-class">:</span></span> NAME = <span class="hljs-string"><span class="hljs-string">"coco_pretrained_model_config"</span></span> GPU_COUNT = <span class="hljs-number"><span class="hljs-number">1</span></span> IMAGES_PER_GPU = <span class="hljs-number"><span class="hljs-number">1</span></span> DETECTION_MIN_CONFIDENCE = <span class="hljs-number"><span class="hljs-number">0.8</span></span> <span class="hljs-comment"><span class="hljs-comment">#     NUM_CLASSES = 81</span></span></code> </pre> <br><p>  Indique la ubicación del archivo con las escalas.  Deje que en este ejemplo esté en la carpeta con este archivo.  Si no es así, se descargará. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mrcnn.utils DATASET_FILE = <span class="hljs-string"><span class="hljs-string">"mask_rcnn_coco.h5"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(DATASET_FILE): mrcnn.utils.download_trained_weights(DATASET_FILE)</code> </pre> <br><p>  Creemos nuestro modelo con la configuración anterior </p><br><pre> <code class="python hljs">model = MaskRCNN(mode=<span class="hljs-string"><span class="hljs-string">"inference"</span></span>, model_dir=<span class="hljs-string"><span class="hljs-string">"logs"</span></span>, config=MaskRCNNConfig()) model.load_weights(DATASET_FILE, by_name=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><p>  Y quizás comencemos a procesar todas las imágenes en el directorio de <code>images</code> en el directorio actual. </p><br><pre> <code class="python hljs">IMAGE_DIR = os.path.join(os.getcwd(), <span class="hljs-string"><span class="hljs-string">"images"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(IMAGE_DIR): image = cv2.imread(os.path.join(IMAGE_DIR, filename)) rgb_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] detections = model.detect([rgb_image], verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br><p>  ¿Qué veremos en las detecciones? </p><br><pre> <code class="python hljs"> print(detections)</code> </pre> <br><p>  Por ejemplo, algo similar: </p><br><pre> <code class="plaintext hljs">{'rois': array([[ 303, 649, 542, 1176],[ 405, 2, 701, 319]]), 'class_ids': array([3, 3]), 'scores': array([0.99896, 0.99770015], dtype=float32), 'masks': array()}</code> </pre> <br><p>  En este caso, se encontraron 2 objetos. <br>  <code>rois</code> : conjuntos de coordenadas de la esquina inferior izquierda y superior derecha <br>  <code>class_ids</code> son los identificadores numéricos de los objetos encontrados, mientras que necesitamos saber que 1 es una persona, 3 es un automóvil, 8 es un camión. <br>  <code>scores</code> : en la medida en que el modelo confíe en la solución, este parámetro puede <code>DETECTION_MIN_CONFIDENCE</code> mediante <code>DETECTION_MIN_CONFIDENCE</code> en la configuración, cortando todas las opciones inapropiadas. <br>  <code>masks</code> : el contorno del objeto.  Los datos se usan para dibujar una máscara de objeto.  Porque  son bastante voluminosos y no están destinados a la comprensión humana; no los citaré en el artículo. </p><br><p>  Ok, podríamos detenernos allí, pero ¿queremos ver la imagen que guía sobre el uso de redes neuronales con objetos bellamente seleccionados? </p><br><p>  Sería más simple llamar a la función <code>mrcnn.visualize.display_instances</code> , pero no haremos esto, escribiremos la nuestra. </p><br><p>  La función tomará una imagen y los principales parámetros obtenidos del diccionario de los primeros pasos. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">visualize_detections</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, masks, boxes, class_ids, scores)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np bgr_image = image[:, :, ::<span class="hljs-number"><span class="hljs-number">-1</span></span>] CLASS_NAMES = [<span class="hljs-string"><span class="hljs-string">'BG'</span></span>,<span class="hljs-string"><span class="hljs-string">"person"</span></span>, <span class="hljs-string"><span class="hljs-string">"bicycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"car"</span></span>, <span class="hljs-string"><span class="hljs-string">"motorcycle"</span></span>, <span class="hljs-string"><span class="hljs-string">"bus"</span></span>, <span class="hljs-string"><span class="hljs-string">"truck"</span></span>] COLORS = mrcnn.visualize.random_colors(len(CLASS_NAMES)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(boxes.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]): y1, x1, y2, x2 = boxes[i] classID = class_ids[i] label = CLASS_NAMES[classID] font = cv2.FONT_HERSHEY_DUPLEX color = [int(c) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> c <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> np.array(COLORS[classID]) * <span class="hljs-number"><span class="hljs-number">255</span></span>] text = <span class="hljs-string"><span class="hljs-string">"{}: {:.3f}"</span></span>.format(label, scores[i]) size = <span class="hljs-number"><span class="hljs-number">0.8</span></span> width = <span class="hljs-number"><span class="hljs-number">2</span></span> cv2.rectangle(bgr_image, (x1, y1), (x2, y2), color, width) cv2.putText(bgr_image, text, (x1, y1<span class="hljs-number"><span class="hljs-number">-20</span></span>), font, size, color, width)</code> </pre><br><p><img src="https://lh3.googleusercontent.com/dnSAiGZW32zMK92_T8yyk2nXCFCKECQ_eSdNiVv5Bzpz9TOsBZT6_jyAY-LfUT4c0jCzfdgFOCpy6_0HzT54CAPo3vvkZ6VgR1U5cdmSTb0zLLpAxJjX-_pTNUnpIExXsao_u29b"></p><br><div class="spoiler">  <b class="spoiler_title">Imagen de origen</b> <div class="spoiler_text"><p><img src="https://sun9-10.userapi.com/c854324/v854324789/e2f73/nJHcTGLnWI4.jpg"></p></div></div><br><p>  Aunque una de las principales ventajas de esta red neuronal es la solución a los problemas de segmentación de instancias: obtener los contornos de los objetos, aún no lo hemos utilizado, lo analizaremos. </p><br><p>  Para implementar máscaras, agregue un par de líneas antes de dibujar un rectángulo para cada objeto encontrado. </p><br><pre> <code class="python hljs">mask = masks[:, :, i] <span class="hljs-comment"><span class="hljs-comment">#   image = mrcnn.visualize.apply_mask(image, mask, color, alpha=0.6) #  </span></span></code> </pre> <br><p>  Resultado: <br><img src="https://lh6.googleusercontent.com/2ZHVaccuXIFgG1Z5qnHROW6cKyxjEe1RLo0SwRAaOxxyQI7Q_Ymbs7ZuZ6bOs56v7oMaCInsarFjOPZRDWL5hNdKpVhlVtHINo9u3gae4utQB-FARZPFxEV5UmkYY8LVJdTjZ0PK"></p><br><div class="spoiler">  <b class="spoiler_title">Versión con máscaras blancas.</b> <div class="spoiler_text"><p><img src="https://lh5.googleusercontent.com/JUUlUQfOCc9jeeuzMDiSc2Hd06P2aMVli2UPSRkUoxlNVwdlwEfi-BHmuyzOsx-nlvm19lPmYlyxFGYV9xGzpppXRORmDBLtLYH6UcYRh1zO47ROLb04HgUggDoz14Zk2AWZ3ta3"></p></div></div><br><h2 id="etap-ii-pervye-uspehi-raspoznavanie-nomerov-mashin">  Etapa II  Primeros éxitos.  Reconocimiento de números de automóviles. </h2><br><p>  Para el reconocimiento, necesitamos un cuadro claro del automóvil cerca, por lo que se decidió tomar solo cuadros del punto de control y luego compararlos con la similitud (más sobre eso en el próximo capítulo).  Este método, sin embargo, da demasiada imprecisión, porque  Las máquinas pueden ser muy similares visualmente y mi algoritmo aún no puede evitar tales situaciones. </p><br><p>  Se decidió utilizar una lib preparada del fabricante ucraniano <a href="https://github.com/ria-com/nomeroff-net">nomeroff-net</a> (no publicidad).  Porque  casi todo el código se puede encontrar en los ejemplos para el modelo, luego no daré una descripción completa. </p><br><p>  Solo puedo decir que esta función se puede iniciar con la imagen original o que la máquina reconocida se puede cortar del marco y pasar a esta función. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os sys.path.append(cfg.NOMEROFF_NET_DIR) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> NomeroffNet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> filters, RectDetector, TextDetector, OptionsDetector, Detector, textPostprocessing nnet = Detector(cfg.MASK_RCNN_DIR, cfg.MASK_RCNN_LOG_DIR) nnet.loadModel(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) rectDetector = RectDetector() optionsDetector = OptionsDetector() optionsDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) textDetector = TextDetector.get_static_module(<span class="hljs-string"><span class="hljs-string">"ru"</span></span>)() textDetector.load(<span class="hljs-string"><span class="hljs-string">"latest"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectCarNumber</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(imgPath: str)</span></span></span><span class="hljs-function"> -&gt; str:</span></span> img = mpimg.imread(imgPath) NP = nnet.detect([img]) cvImgMasks = filters.cv_img_mask(NP) arrPoints = rectDetector.detect(cvImgMasks) zones = rectDetector.get_cv_zonesBGR(img, arrPoints) regionIds, stateIds, _c = optionsDetector.predict(zones) regionNames = optionsDetector.getRegionLabels(regionIds) <span class="hljs-comment"><span class="hljs-comment"># find text with postprocessing by standart textArr = textDetector.predict(zones) textArr = textPostprocessing(textArr, regionNames) return textArr</span></span></code> </pre> <br><p>  La salida textArr representará una serie de cadenas con los números de máquinas que se encuentran en el marco, por ejemplo: <br>  <code>["293163"]</code> , o <code>[""]</code> , <code>[]</code> - si no se encontraron números coincidentes. </p><br><h2 id="etap-iii-opoznaem-obekty-na-shozhest">  Etapa III  Identificar objetos por similitud. </h2><br><p>  Ahora necesitamos entender cómo arreglar un objeto una vez, entender que es él en el siguiente cuadro.  En esta etapa, asumiremos que tenemos una sola cámara y solo distinguiremos entre diferentes marcos. </p><br><p>  Para hacer esto, debe averiguar cómo compararemos los dos objetos. </p><br><p>  Propondré un algoritmo de <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">cribado</a> para estos fines.  Hacemos una reserva de que no es parte de la parte principal de OpenCV, por lo que necesitamos entregar módulos contrib adicionales.  Desafortunadamente, el algoritmo está patentado y su uso en programas comerciales es limitado.  Pero estamos enfocados en actividades de investigación, ¿verdad? </p><br><pre> <code class="plaintext hljs">pip3 install opencv-contrib-python --user</code> </pre> <br><p>  ~~ Sobrecargar el operador == ~~ Escribimos una función que toma 2 objetos comparados en forma de matrices.  Por ejemplo, los obtenemos después de llamar a la función <code>cv2.open(path)</code> </p><br><p>  Escribiremos una implementación de nuestro algoritmo. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compareImages</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img1, img2)</span></span></span><span class="hljs-function"> -&gt; bool:</span></span> sift = cv2.xfeatures2d.SIFT_create()</code> </pre> <br><p>  Encuentre puntos clave y descriptores utilizando SIFT.  Quizás no proporcione ayuda para estas funciones, porque siempre puede llamarlo en el shell interactivo como <code>help(somefunc)</code> </p><br><pre> <code class="python hljs"> kp1, des1 = sift.detectAndCompute(img1, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) kp2, des2 = sift.detectAndCompute(img2, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)</code> </pre> <br><p>  Configura nuestro algoritmo. </p><br><pre> <code class="python hljs"> FLANN_INDEX_KDTREE = <span class="hljs-number"><span class="hljs-number">0</span></span> indexParams = dict(algorithm=FLANN_INDEX_KDTREE, trees=<span class="hljs-number"><span class="hljs-number">5</span></span>) searchParams = dict(checks=<span class="hljs-number"><span class="hljs-number">50</span></span>) flann = cv2.FlannBasedMatcher(indexParams, searchParams)</code> </pre> <br><p>  Ahora ejecútalo. </p><br><pre> <code class="python hljs"> matches = flann.knnMatch(des1, des2, k=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br><p>  Cuenta las similitudes entre las imágenes. </p><br><pre> <code class="python hljs"> matchesCount = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> m, n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> matches: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> m.distance &lt; cfg.cencitivity*n.distance: matchesCount += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> matchesCount &gt; cfg.MIN_MATCH_COUNT</code> </pre> <br><p>  Ahora intenta usarlo <br>  Para hacer esto, después de detectar objetos, necesitamos cortarlos de la imagen original. </p><br><p>  No podría escribir nada mejor que guardarlo para memoria lenta, y luego leer desde allí. </p><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">extractObjects</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(objects, binaryImage, outputImageDirectory, filename=None)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> objects: y1, x1, y2, x2 = item.coordinates <span class="hljs-comment"><span class="hljs-comment">#       cropped = binaryImage[y1:y2, x1:x2] beforePoint, afterPoint = filename.split(".") outputDirPath = os.path.join(os.path.split(outputImageDirectory)[0], "objectsOn" + beforePoint) if not os.path.exists(outputDirPath): os.mkdir(outputDirPath) coordinates = str(item).replace(" ", ",") pathToObjectImage = "{}{}.jpg".format(item.type, coordinates) cv2.imwrite(os.path.join(outputDirPath, str(pathToObjectImage)), cropped)</span></span></code> </pre> <br><p>  Ahora tenemos los objetos en el <code>&lt;outputImageDirectory&gt;/objectsOn&lt;imageFilename&gt;</code> </p><br><p>  Ahora, si tenemos al menos 2 de esos directorios, podemos comparar los objetos en ellos.  Ejecute la función escrita anteriormente </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> compareImages(previousObjects, currentObjects): print(“  !”)</code> </pre> <br><p>  O podemos hacer otra acción, como marcar estos objetos con el mismo identificador. </p><br><p>  Por supuesto, como todas las redes neuronales, esta tiende a dar resultados a veces erróneos. </p><br><p>  En general, hemos completado las 3 tareas establecidas al principio, por lo que terminaremos.  Dudo que este artículo haya abierto los ojos de personas que han escrito al menos un programa que resuelve los problemas de reconocimiento de imágenes / segmentación de imágenes, pero espero haber ayudado al menos a un desarrollador novato). </p><br><p>  El código fuente completo del proyecto se puede encontrar <a href="https://github.com/Sapfir0/premier-eye">aquí</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/483018/">https://habr.com/ru/post/483018/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../483004/index.html">Efecto Kuleshov en Disco Elysium: cómo el contexto crea significado</a></li>
<li><a href="../483008/index.html">Otro futuro: una división de la humanidad</a></li>
<li><a href="../483012/index.html">Antigüedades: Roland MT-32, un sonido alternativo para juegos de DOS</a></li>
<li><a href="../483014/index.html">Colas de mensajes PostgreSQL usando PgQ</a></li>
<li><a href="../483016/index.html">Una breve historia de los microprocesadores espaciales, segunda parte</a></li>
<li><a href="../483024/index.html">"¿Qué hicieron las corporaciones con su privacidad?", Arthur Khachuyan (Tazeros Global)</a></li>
<li><a href="../483026/index.html">Java / Spring: Cómo generar completamente una API CRUD REST usando Speedment</a></li>
<li><a href="../483030/index.html">API que te hace llorar</a></li>
<li><a href="../483032/index.html">Mudarse de la CEI a la República Checa, experiencia propia (parte 2)</a></li>
<li><a href="../483036/index.html">Problema de finalización de n-Queens: algoritmo de solución lineal</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>