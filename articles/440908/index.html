<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👈🏾 🔲 🕵🏾 Servir todo 👩🏽‍🚀 👷🏻 🐫</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="No hace mucho tiempo, en una galaxia bastante distante, en un planeta provincial había descendientes famosos de monos que eran tan vagos que decidiero...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Servir todo</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440908/"> No hace mucho tiempo, en una galaxia bastante distante, en un planeta provincial había descendientes famosos de monos que eran tan vagos que decidieron inventar inteligencia artificial.  "Bueno, ¿qué?"  Ellos pensaron.  Es bueno tener en los asesores de la <s>Overmind un</s> "cerebro" que pensará por ti cuando sea necesario, tus problemas se pueden resolver rápidamente, y es incluso mejor de lo que una criatura viviente puede hacer ... Y, sin pensar en las consecuencias, comenzaron sus monos. Los cerebros inversos y el proceso cognitivo en los bloques de construcción se desmontan.  Pensaron, pensaron y pensaron, no lo creerán: un modelo de neurona, un algoritmo de aprendizaje matemático y luego redes neuronales con diferentes topologías desplegadas.  Por supuesto, esto no funcionó para decirlo muy bien.  Hubo muchas deficiencias, en comparación con la inteligencia natural, pero una cierta gama de problemas, estos modelos nos permitieron resolver con una precisión razonable.  Y lentamente, las habilidades digitalizadas y serializadas comenzaron a aparecer en forma de modelos de redes neuronales.  Hoy, queridos amantes de la historia del universo, tocaremos la organización e implementación de varias habilidades de inteligencia artificial. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sa/gk/cs/sagkcsc7kookhhmkxeppqvi7zue.jpeg"></div><a name="habracut"></a><br>  Sobre la creación y capacitación de modelos de redes neuronales (habilidades) en Habré se escribe mucho, por lo tanto, no hablaremos de eso hoy.  Habiendo entrenado o recibido habilidades de IA serializadas, esperamos usarlas en nuestros sistemas de información objetivo, y aquí surge un problema.  Lo que funciona en el stand de laboratorio no puede transferirse a la producción en su forma original, es necesario implementar toda la pila de tecnología asociada e incluso realizar modificaciones significativas en la plataforma objetivo (por supuesto, hay excepciones en la forma de CoreML, pero este es un caso especial y solo para equipos Apple).  Además, existen muchas herramientas para desarrollar y serializar modelos, ¿es realmente necesario que todos desarrollen una solución de integración separada?  Además, incluso en el laboratorio, a menudo es necesario obtener una conclusión rápida del modelo, sin esperar la carga de toda la pila de desarrollo asociada. <br>  Como sugerencia para resolver estos problemas, me gustaría contarles acerca de una herramienta de código abierto relativamente nueva, que, tal vez, será útil para ustedes al desarrollar proyectos relacionados con la IA. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">0Mind</a> (leer ZeroMind) es un servidor de habilidades gratuito.  La solución es un servidor de aplicaciones modular, universal y fácilmente extensible con elementos de un marco para servir modelos de aprendizaje automático heterogéneos (salida altamente accesible).  El servidor es feo en Python 3 y usa Tornado para el procesamiento de solicitudes asíncronas.  Independientemente del marco de aprendizaje automático que se utilizó para preparar y serializar el modelo, 0Mind facilita el uso de una habilidad o grupo de habilidades utilizando la API REST universal.  De hecho, la solución es un servidor web asíncrono con una API REST, unificada para trabajar con modelos de habilidades de IA y un conjunto de adaptadores para varios marcos de aprendizaje automático.  Es posible que haya trabajado con el servicio de tensorflow: esta es una solución similar, pero 0Mind no está apilado y puede servir varios modelos de diferentes marcos en el mismo puerto.  Por lo tanto, en lugar de implementar toda la pila de tecnología para derivar modelos de IA en el sistema de información de destino, puede usar la API REST simple y familiar para la habilidad de interés, además, el modelo preparado permanece en el servidor y no termina en la distribución de software.  Para no confundir una vez más con términos complejos, pasemos a ejemplos de uso y comencemos a lanzar hechizos de consola. <br><br><h1>  Instalación </h1><br>  Aquí todo es simple: <br><br><pre><code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> git@github.com:MisteryX/0Mind.git 0Mind</code> </pre> <br>  Ahora tenemos una instancia de servidor en funcionamiento.  Instalar las dependencias: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> 0Mind pip3 install -r requirements.txt</code> </pre><br>  O si usas Conda: <br><br><pre> <code class="bash hljs">conda install --yes --file requirements.txt</code> </pre> <br>  Una advertencia importante es que el <a href="">servidor admite varios marcos de</a> aprendizaje automático y, para no agregarlos todos y no instalarlos, usted mismo decide qué marcos de marcos cargará en el host con la instancia de 0Mind, instale y configure estas herramientas de forma independiente. <br><br><h1>  Personalización </h1><br>  El punto de entrada o el ejecutable del servidor principal es <b>model_pool.py</b> . <br>  Las posibles opciones de inicio son <b>-c</b> o <b>--config_file</b> con la ruta al archivo de configuración.  De manera predeterminada, 0Mind usa el archivo <b>configs / model_pool_config.json</b> como archivo de configuración.  El servidor también usa el <b>archivo config / logger.json</b> para controlar el registro estándar del módulo de registro de Python. <br><br>  Con el fin de demostrar las capacidades, podemos dejar intacto el archivo de configuración predeterminado.  Lea más sobre la configuración en la <a href="">documentación oficial</a> . <br><br>  La configuración principal del servidor es: id, host, puerto, tareas. <br><br>  <b>id</b> : (número) identificador único del grupo de modelos (utilizado para equilibrar y direccionar en una red distribuida de grupos) <br>  <b>host</b> - (cadena) dirección de red o nombre de dominio de este host <br>  <b>puerto</b> : (número) en qué puerto desea alojar el servicio 0Mind (debe estar libre en este host) <br>  <b>tareas</b> : (lista de objetos) una lista de tareas cargadas con el servicio (puede estar vacía).  En la configuración predeterminada, se carga el modelo de demostración CNN_MNIST preparado por Keras, y lo usaremos para demostrar las capacidades. <br><br>  Parámetros de configuración adicionales (opcionales): <br><br>  <b>model_types</b> : (lista de cadenas) puede limitar los tipos de modelos cargados a este grupo al especificarlos en esta lista.  Si la lista está vacía, entonces no hay restricciones. <br><br>  <b>debug</b> : (tipo booleano) es responsable de habilitar o deshabilitar el modo de depuración de Tornado.  En modo de depuración, en caso de errores, la información de error extendida se devuelve a stdout, lo cual es útil cuando se desarrollan extensiones. <br><br><h1>  Las posibilidades </h1><br>  Lo principal en 0Mind es la <a href="">lista de marcos compatibles</a> y las <a href="">características de la API REST</a> . <br><br>  Las solicitudes a la API REST se pueden realizar utilizando un navegador o utilidades http.  En esta guía, así como en la documentación del servidor, utilizaremos cURL como la herramienta más simple y asequible para sistemas abiertos. <br><br>  Actualmente, 0Mind API tiene un total de 10 solicitudes: <br><br>  1. http: // $ HOST: $ PORT / info - información general sobre la instancia de 0Mind <br>  2. http: // $ HOST: $ PORT / info / system: información del sistema sobre el host en el que se ejecuta 0Mind <br>  3. http: // $ HOST: $ PORT / info / task: información sobre la tarea especificada <br>  4. http: // $ HOST: $ PORT / info / tareas - lista de tareas de instancia 0Mind <br>  5. http: // $ HOST: $ PORT / model / list: una lista de identificadores de modelos cargados en el grupo <br>  6. http: // $ HOST: $ PORT / model / info - muestra información de la interfaz sobre el modelo <br>  7. http: // $ HOST: $ PORT / model / load: carga un nuevo modelo en el grupo <br>  8. http: // $ HOST: $ PORT / model / drop: descarga un modelo cargado previamente del grupo <br>  9. http: // $ HOST: $ PORT / model / predict - solicita salida del modelo <br>  10.http: // $ HOST: $ PORT / command / stop - detiene el servicio 0Mind y finaliza su proceso <br><br><h2>  Informacion </h2><br>  Puede iniciar una instancia de servidor, por ejemplo, así: <br><br><pre> <code class="bash hljs">python3 model_pool.py</code> </pre> <br>  Por ejemplo, obtendremos información general sobre una instancia de servidor en ejecución: <br><br><pre> <code class="bash hljs">curl http://127.0.0.1:5885/info</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"service"</span></span>: <span class="hljs-string"><span class="hljs-string">"ModelPool"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"id"</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-attr"><span class="hljs-attr">"options"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"debug"</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span>}, <span class="hljs-attr"><span class="hljs-attr">"version"</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>]}</code> </pre> <br>  Ok, ahora descubrimos qué modelos están cargados en el grupo: <br><br><pre> <code class="bash hljs">curl http://127.0.0.1:5885/model/list</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"id"</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-attr"><span class="hljs-attr">"check_sum"</span></span>: <span class="hljs-string"><span class="hljs-string">"4d8a15e3cc35750f016ce15a43937620"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"models"</span></span>: [<span class="hljs-string"><span class="hljs-string">"1"</span></span>]}</code> </pre> <br>  Ahora aclaremos la interfaz del modelo cargado con el identificador "1": <br><br><pre> <code class="bash hljs">curl http://127.0.0.1:5885/model/info?id=1</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"inputs"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"0"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"conv2d_1_input:0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"float32"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"shape"</span></span>: [<span class="hljs-literal"><span class="hljs-literal">null</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]}}, <span class="hljs-attr"><span class="hljs-attr">"outputs"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"0"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"dense_2/Softmax:0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"float32"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"shape"</span></span>: [<span class="hljs-literal"><span class="hljs-literal">null</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]}}, <span class="hljs-attr"><span class="hljs-attr">"tool"</span></span>: <span class="hljs-string"><span class="hljs-string">"keras"</span></span>}</code> </pre> <br>  Queda por averiguar con qué filtros se carga el modelo.  Para hacer esto, aclaramos los detalles de la tarea de cargar el modelo con el identificador "1": <br><br><pre> <code class="bash hljs">curl http://127.0.0.1:5885/info/task?id=1</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"id"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"model_file"</span></span>: <span class="hljs-string"><span class="hljs-string">"ML/models/mnist_cnn_model.keras"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"model_type"</span></span>: <span class="hljs-string"><span class="hljs-string">"keras"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"input_filters"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"conv2d_1_input:0"</span></span>: [<span class="hljs-string"><span class="hljs-string">"i_img_file_to_ns_arr.ImageFileToNormAndScaledNPArrayFilter"</span></span>]}, <span class="hljs-attr"><span class="hljs-attr">"output_filters"</span></span>: {}}</code> </pre> <br>  Como puede ver, nuestro modelo tiene un filtro de entrada: i_img_file_to_ns_arr.ImageFileToNormAndScaledNPArrayFilter y filtra la entrada con el nombre - conv2d_1_input: 0.  Este filtro simplemente convierte el archivo de imagen especificado en un tensor y lo escala de acuerdo con la entrada del modelo.  <a href="">Los filtros</a> son otra gran herramienta generalizada de 0Mind.  Dado que el procesamiento previo y posterior del procesamiento de datos para los modelos es el mismo, simplemente puede acumular estos filtros para un uso rápido en el trabajo posterior con otros modelos, indicando la tarea deseada como un atributo para cargar el modelo. <br><br><h2>  Salida de datos del modelo (inferencia) </h2><br>  Bueno, ahora que tenemos toda la información necesaria para la inferencia, podemos sacar una conclusión del modelo.  Como entrada, usamos la imagen del conjunto de pruebas incluido en la distribución 0Mind <b>samples / image5.png</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o0/rr/h7/o0rrh7cyclpin4vxxjx9nqaytxc.png"></div><br><br><pre> <code class="bash hljs">curl -d <span class="hljs-string"><span class="hljs-string">'{"conv2d_1_input:0": [{"image_file": "samples/image5.png"}]}'</span></span> -H <span class="hljs-string"><span class="hljs-string">"Content-Type:application/json"</span></span> -X POST http://127.0.0.1:5885/model/predict?id=1</code> </pre> <br>  La única entrada del modelo conv2d_1_input: 0 con el filtro i_img_file_to_ns_arr.ImageFileToNormAndScaledNPArrayFilter son los datos en el formato aceptado por el filtro - [{"image_file": "samples / image5.png"}].  En respuesta de 0Mind, obtenemos la salida del modelo: <br><br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"result"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"dense_2/Softmax:0"</span></span>: [[<span class="hljs-number"><span class="hljs-number">2.190017217283827e-21</span></span>, <span class="hljs-number"><span class="hljs-number">1.6761866200587505e-11</span></span>, <span class="hljs-number"><span class="hljs-number">2.2447325167271673e-14</span></span>, <span class="hljs-number"><span class="hljs-number">0.00011080023978138342</span></span>, <span class="hljs-number"><span class="hljs-number">1.881280855367115e-17</span></span>, <span class="hljs-number"><span class="hljs-number">0.9998891353607178</span></span>, <span class="hljs-number"><span class="hljs-number">1.6690393796396863e-16</span></span>, <span class="hljs-number"><span class="hljs-number">9.67975005705668e-12</span></span>, <span class="hljs-number"><span class="hljs-number">1.1265206161566871e-13</span></span>, <span class="hljs-number"><span class="hljs-number">2.086113400079359e-13</span></span>]]}, <span class="hljs-attr"><span class="hljs-attr">"model_time"</span></span>: <span class="hljs-number"><span class="hljs-number">0.002135753631591797</span></span>}</code> </pre> <br>  Entonces, la única salida del modelo "dense_2 / Softmax: 0" (ver información sobre el modelo anterior) nos dio el vector de confianza del modelo en la clasificación de esta imagen.  Como puede ver, la probabilidad más alta es 0.99 para una clase con un índice de 6 (las clases son números 0-9), que corresponde al número <b>5</b> .  Por lo tanto, el modelo hizo frente con éxito al reconocimiento del manuscrito y dio una conclusión con gran confianza.  El tiempo de inferencia del modelo en el host 0Mind fue 0.002135753631591797 segundos, porque  la salida estaba en una CPU x86 normal. <br><br><h2>  Carga y descarga dinámica de modelos. </h2><br>  Ahora descargue nuestro modelo del grupo: <br><br><pre> <code class="bash hljs">curl http://127.0.0.1:5885/model/drop?id=1</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"result"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"unload_time"</span></span>: <span class="hljs-number"><span class="hljs-number">0.000152587890625</span></span>, <span class="hljs-attr"><span class="hljs-attr">"memory_released"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"model_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>}</code> </pre> <br>  Volvemos a cargar el mismo modelo, pero ahora con un identificador diferente ("nuevo") y un filtro de salida del modelo io_argmax.ArgMaxFilter, que probablemente derivará el índice del vector de confianza del modelo.  Tendremos que cambiar los índices de las entradas y salidas del modelo, esto se debe a las características de Keras: <br><br><pre> <code class="bash hljs">curl -d <span class="hljs-string"><span class="hljs-string">'{"id": "new", "output_filters": {"dense_2_1/Softmax:0": ["io_argmax.ArgMaxFilter"]}, "model_file": "ML/models/mnist_cnn_model.keras", "input_filters": {"conv2d_1_input_1:0": ["i_img_file_to_ns_arr.ImageFileToNormAndScaledNPArrayFilter"]}, "model_type": "keras"}'</span></span> -H <span class="hljs-string"><span class="hljs-string">"Content-Type:application/json"</span></span> -X POST http://127.0.0.1:5885/model/load</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"result"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"load_time"</span></span>: <span class="hljs-number"><span class="hljs-number">0.45618462562561035</span></span>, <span class="hljs-attr"><span class="hljs-attr">"memory_consumed"</span></span>: <span class="hljs-number"><span class="hljs-number">16183296</span></span>, <span class="hljs-attr"><span class="hljs-attr">"model_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"new"</span></span>}</code> </pre> <br>  Y ahora le pedimos al modelo que reconozca para nosotros dos imágenes a la vez en una solicitud <b>samples / image5.png</b> y <b>samples / image1.png</b> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/o0/rr/h7/o0rrh7cyclpin4vxxjx9nqaytxc.png"></div><div style="text-align:center;"><img src="https://habrastorage.org/webt/bv/ha/79/bvha79zohijfpxiilvn1w12wze4.png"></div><br><pre> <code class="bash hljs">curl -d <span class="hljs-string"><span class="hljs-string">'{"conv2d_1_input:0": [{"image_file": "samples/image5.png"}, {"image_file": "samples/image1.png"}]}'</span></span> -H <span class="hljs-string"><span class="hljs-string">"Content-Type:application/json"</span></span> -X POST http://127.0.0.1:5885/model/predict?id=new</code> </pre> <br><pre> <code class="json hljs">{<span class="hljs-attr"><span class="hljs-attr">"result"</span></span>: {<span class="hljs-attr"><span class="hljs-attr">"dense_2_1/Softmax:0"</span></span>: [<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]}, <span class="hljs-attr"><span class="hljs-attr">"model_time"</span></span>: <span class="hljs-number"><span class="hljs-number">0.003907206535339355</span></span>}</code> </pre> <br>  El modelo de demostración no volvió a confundirse. <br><br><h1>  Extensión </h1><br>  Ampliar las capacidades de 0Mind no es difícil, gracias a su arquitectura modular, el uso de herramientas populares y buenas convenciones de código en el proyecto.  Los principales vectores de extensión pueden ser: <br><br><ol><li>  <a href="">Los adaptadores</a> son clases entre capas para trabajar con nuevos marcos de redes neuronales y aprendizaje automático. </li><li>  <a href="">Los filtros</a> son manejadores de datos para entrar y salir de modelos de habilidades. </li><li>  <a href="">Controladores de solicitud</a> : le permiten agregar nueva funcionalidad a las solicitudes y respuestas de la API 0Mind. </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/440908/">https://habr.com/ru/post/440908/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../440898/index.html">Marketing de contenidos, publicidad contextual, mejora de la conversión: 6 guías útiles de promoción de inicio</a></li>
<li><a href="../440900/index.html">RESTO pasión por 200</a></li>
<li><a href="../440902/index.html">La mitad del reino para la IA: cuánto ahorran los bancos en aprendizaje automático, redes neuronales y bots de chat</a></li>
<li><a href="../440904/index.html">Comparación de las arquitecturas Viper y MVVM: cómo aplicar ambas</a></li>
<li><a href="../440906/index.html">Seminario web "167-ФЗ. Cómo los bancos pueden cumplir los requisitos del Banco Central para los sistemas antifraude ”- 26 de febrero de 2019, 11:00 hora de Moscú</a></li>
<li><a href="../440910/index.html">¿Por qué los bancos monopolizan blockchain?</a></li>
<li><a href="../440912/index.html">Tal dolor, tal dolor, infraestructura como servicio 1: 0</a></li>
<li><a href="../440914/index.html">Perdí la fe en la industria, me quemé, pero el culto a la herramienta me salvó</a></li>
<li><a href="../440916/index.html">Radiación: unidades</a></li>
<li><a href="../440918/index.html">Semana de la seguridad 08: pirateo de VFEMail en vivo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>