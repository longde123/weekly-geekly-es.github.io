<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§í üë©üèº‚Äçü§ù‚Äçüë®üèø üöê Buch "Maschinelles Lernen f√ºr Wirtschaft und Marketing" üõãÔ∏è üëñ ‚ñ´Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Datenwissenschaft wird zu einem integralen Bestandteil jeder Marketingaktivit√§t, und dieses Buch ist ein lebendiges Portr√§t der digitalen Transfor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Buch "Maschinelles Lernen f√ºr Wirtschaft und Marketing"</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/460375/"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/gq/td/mc/gqtdmc8joactk6gu7xrzcdr0f4i.jpeg" align="left" alt="Bild"></a>  Die Datenwissenschaft wird zu einem integralen Bestandteil jeder Marketingaktivit√§t, und dieses Buch ist ein lebendiges Portr√§t der digitalen Transformation im Marketing.  Datenanalyse und intelligente Algorithmen automatisieren zeitaufw√§ndige Marketingaufgaben.  Der Entscheidungsprozess wird nicht nur perfekter, sondern auch schneller, was in einem sich st√§ndig beschleunigenden Wettbewerbsumfeld von gro√üer Bedeutung ist. <br><br>  ‚ÄûDieses Buch ist ein lebendiges Portr√§t der digitalen Transformation im Marketing.  Es zeigt, wie Data Science zu einem integralen Bestandteil jeder Marketingaktivit√§t wird.  Es wird detailliert beschrieben, wie auf Datenanalyse und intelligenten Algorithmen basierende Ans√§tze zur tiefgreifenden Automatisierung traditionell arbeitsintensiver Marketingaufgaben beitragen.  Der Entscheidungsprozess wird nicht nur fortschrittlicher, sondern auch schneller, was in unserem sich st√§ndig beschleunigenden Wettbewerbsumfeld wichtig ist.  Dieses Buch muss von Datenverarbeitungs- und Marketingfachleuten gelesen werden, und es ist besser, wenn sie es gemeinsam lesen. ‚Äú  Andrey Sebrant, Direktor f√ºr strategisches Marketing, Yandex. <br><a name="habracut"></a><br><h3>  Auszug.  5.8.3.  Modelle mit versteckten Faktoren </h3><br>  In den bisher diskutierten gemeinsamen Filteralgorithmen basieren die meisten Berechnungen auf den einzelnen Elementen der Bewertungsmatrix.  Proximity-basierte Methoden bewerten fehlende Bewertungen direkt anhand bekannter Werte in der Bewertungsmatrix.  Modellbasierte Methoden f√ºgen der Bewertungsmatrix eine Abstraktionsschicht hinzu und erstellen ein Vorhersagemodell, das bestimmte Beziehungsmuster zwischen Benutzern und Elementen erfasst. Das Modelltraining h√§ngt jedoch weiterhin stark von den Eigenschaften der Bewertungsmatrix ab.  Infolgedessen sind diese kollaborativen Filtertechniken normalerweise mit den folgenden Problemen konfrontiert: <br><br>  Die Bewertungsmatrix kann Millionen von Benutzern, Millionen von Elementen und Milliarden bekannter Bewertungen enthalten, was zu ernsthaften Problemen hinsichtlich der Komplexit√§t und Skalierbarkeit der Berechnungen f√ºhrt. <br><br>  Die Bewertungsmatrix ist normalerweise sehr sp√§rlich (in der Praxis fehlen m√∂glicherweise 99% der Bewertungen).  Dies wirkt sich auf die Rechenstabilit√§t der Empfehlungsalgorithmen aus und f√ºhrt zu unzuverl√§ssigen Sch√§tzungen, wenn der Benutzer oder das Element keine wirklich √§hnlichen Nachbarn hat.  Dieses Problem wird h√§ufig durch die Tatsache versch√§rft, dass die meisten grundlegenden Algorithmen entweder benutzer- oder elementorientiert sind, was ihre F√§higkeit einschr√§nkt, alle Arten von √Ñhnlichkeiten und Beziehungen aufzuzeichnen, die in der Bewertungsmatrix verf√ºgbar sind. <br><br>  Die Daten in der Bewertungsmatrix sind aufgrund von √Ñhnlichkeiten zwischen Benutzern und Elementen normalerweise stark korreliert.  Dies bedeutet, dass die in der Bewertungsmatrix verf√ºgbaren Signale nicht nur sp√§rlich, sondern auch redundant sind, was zur Versch√§rfung des Skalierbarkeitsproblems beitr√§gt. <br><br>  Die obigen √úberlegungen weisen darauf hin, dass die urspr√ºngliche Bewertungsmatrix m√∂glicherweise nicht die optimalste Darstellung von Signalen ist, und andere alternative Darstellungen, die f√ºr die gemeinsame Filterung besser geeignet sind, sollten in Betracht gezogen werden.  Um diese Idee zu untersuchen, kehren wir zum Ausgangspunkt zur√ºck und denken ein wenig √ºber die Art der Empfehlungsdienste nach.  Tats√§chlich kann der Empfehlungsdienst als ein Algorithmus betrachtet werden, der Bewertungen basierend auf einem gewissen Ma√ü an √Ñhnlichkeit zwischen dem Benutzer und dem Element vorhersagt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/at/bc/6n/atbc6no-aj2vssp1mrgyctgu_oy.png" alt="Bild"></div><br>  Eine M√∂glichkeit, dieses √Ñhnlichkeitsma√ü zu bestimmen, besteht darin, den Hidden-Factor-Ansatz zu verwenden und Benutzer und Elemente auf Punkte in einem k-dimensionalen Raum abzubilden, sodass jeder Benutzer und jedes Element durch einen k-dimensionalen Vektor dargestellt wird: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9e/ck/no/9eckno4ntrf2yl0hw1q2tr2irbk.png" alt="Bild"></div><br>  Vektoren m√ºssen so konstruiert sein, dass die entsprechenden Dimensionen p und q miteinander vergleichbar sind.  Mit anderen Worten kann jede Dimension als Zeichen oder Konzept betrachtet werden, dh puj ist ein Ma√ü f√ºr die N√§he von Benutzer u und Konzept j, und qij ist ein Ma√ü f√ºr Element i und Konzept j.  In der Praxis werden diese Dimensionen h√§ufig als Genres, Stile und andere Attribute interpretiert, die gleichzeitig f√ºr Benutzer und Elemente gelten.  Die √Ñhnlichkeit zwischen dem Benutzer und dem Element und dementsprechend die Bewertung kann als das Produkt der entsprechenden Vektoren definiert werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k3/1w/9w/k31w9wqmwas5pfvhr8_mopo7ibg.png" alt="Bild"></div><br>  Da jede Bewertung in ein Produkt aus zwei Vektoren zerlegt werden kann, die zu einem Konzeptraum geh√∂ren, der in der urspr√ºnglichen Bewertungsmatrix nicht direkt beobachtet wird, werden p und q als versteckte Faktoren bezeichnet.  Der Erfolg dieses abstrakten Ansatzes h√§ngt nat√ºrlich ganz davon ab, wie die verborgenen Faktoren bestimmt und konstruiert werden.  Um diese Frage zu beantworten, stellen wir fest, dass der Ausdruck 5.92 wie folgt in Matrixform umgeschrieben werden kann: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2i/bp/j9/2ibpj9pmptpun2amylvxf41okjw.png" alt="Bild"></div><br>  wobei P die aus den Vektoren p zusammengesetzte n √ó k-Matrix ist und Q die aus den Vektoren q zusammengesetzte m √ó k-Matrix ist, wie in Fig. 1 gezeigt.  5.13.  Das Hauptziel eines gemeinsamen Filtersystems besteht normalerweise darin, die Vorhersagefehler der Bewertung zu minimieren, wodurch Sie das Optimierungsproblem in Bezug auf die Matrix der verborgenen Faktoren direkt bestimmen k√∂nnen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9h/nm/mx/9hnmmxregnvn9snp9empxqf91qk.png" alt="Bild"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z-/vl/nu/z-vlnuz-v9git5dmm0nj79etl5g.png" alt="Bild"></div><br>  Unter der Annahme, dass die Anzahl der verborgenen Dimensionen k fest ist und k ‚â§ n und k ‚â§ m ist, reduziert sich das Optimierungsproblem 5.94 auf das in Kapitel 2 ber√ºcksichtigte niedrigrangige Approximationsproblem. Um den L√∂sungsansatz zu demonstrieren, nehmen wir f√ºr einen Moment an, dass die Bewertungsmatrix vollst√§ndig ist.  In diesem Fall hat das Optimierungsproblem eine analytische L√∂sung hinsichtlich der Singular Value Decomposition (SVD) der Bewertungsmatrix.  Insbesondere kann unter Verwendung des Standard-SVD-Algorithmus die Matrix in das Produkt von drei Matrizen zerlegt werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7z/ic/x0/7zicx0iwp156hctp6t7axd5bzu4.png" alt="Bild"></div><br>  wobei U die durch Spalten orthonormalisierte n √ó n-Matrix ist, Œ£ die n √ó m-Diagonalmatrix ist und V die durch Spalten orthonormalisierte m √ó m-Matrix ist.  Eine optimale L√∂sung f√ºr Problem 5.94 kann in Bezug auf diese Faktoren erhalten werden, abgeschnitten auf die k wichtigsten Dimensionen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/i-/os/mwi-oskhgcs-ehf_bhepytyo_os.png" alt="Bild"></div><br>  Folglich k√∂nnen versteckte Faktoren, die hinsichtlich der Vorhersagegenauigkeit optimal sind, durch singul√§re Zerlegung erhalten werden, wie unten gezeigt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uc/ih/yl/ucihyltxuqye29vkocknsbzyguw.png" alt="Bild"></div><br>  Dieses SVD-basierte Hidden-Factor-Modell hilft bei der L√∂sung der am Anfang dieses Abschnitts beschriebenen Co-Filtering-Probleme.  Erstens ersetzt es die gro√üe n √ó m-Bewertungsmatrix durch n √ó k- und m √ó k-Faktormatrizen, die normalerweise viel kleiner sind, da in der Praxis die optimale Anzahl versteckter Dimensionen k h√§ufig klein ist.  Es gibt zum Beispiel einen Fall, in dem die Bewertungsmatrix mit 500.000 Benutzern und 17.000 Elementen mit 40 Messungen ziemlich gut angen√§hert werden konnte [Funk, 2016].  Ferner eliminiert SVD die Korrelation in der Bewertungsmatrix: Die durch 5,97 definierten Latentfaktormatrizen sind in Spalten orthonormal, d. H. Versteckte Dimensionen sind nicht korreliert.  Wenn SVD, was in der Praxis normalerweise der Fall ist, auch das Problem der Sp√§rlichkeit l√∂st, weil das in der urspr√ºnglichen Bewertungsmatrix vorhandene Signal effektiv konzentriert ist (denken Sie daran, dass wir k Dimensionen mit der h√∂chsten Signalenergie ausw√§hlen) und die Matrix der verborgenen Faktoren nicht d√ºnn ist.  Abbildung 5.14 zeigt diese Eigenschaft.  Der benutzerbasierte N√§herungsalgorithmus (5.14, a) reduziert sp√§rliche Bewertungsvektoren f√ºr ein bestimmtes Element und einen bestimmten Benutzer, um eine Bewertungsbewertung zu erhalten.  Das Hidden-Factor-Modell (5.14, b) sch√§tzt dagegen die Bewertung durch Faltung zweier Vektoren mit reduzierter Dimension und h√∂herer Energiedichte. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/v3/te/fm/v3tefmto-k2yf54og0xn4rpbxlq.png" alt="Bild"></div><br>  Der soeben beschriebene Ansatz scheint eine koh√§rente L√∂sung f√ºr das Problem der versteckten Faktoren zu sein, hat jedoch aufgrund der Annahme, dass die Ratingmatrix vollst√§ndig ist, einen schwerwiegenden Nachteil.  Wenn die Bewertungsmatrix d√ºnn ist, was fast immer der Fall ist, kann der Standard-SVD-Algorithmus nicht direkt angewendet werden, da er fehlende (undefinierte) Elemente nicht verarbeiten kann.  Die einfachste L√∂sung in diesem Fall besteht darin, die fehlenden Bewertungen mit einem Standardwert zu f√ºllen. Dies kann jedoch zu einer ernsthaften Verzerrung der Prognose f√ºhren.  Dar√ºber hinaus ist es rechnerisch ineffizient, da die rechnerische Komplexit√§t einer solchen L√∂sung gleich der SVD-Komplexit√§t f√ºr die vollst√§ndige n √ó m-Matrix ist, w√§hrend es w√ºnschenswert ist, ein Verfahren mit einer Komplexit√§t zu haben, die proportional zur Anzahl bekannter Bewertungen ist.  Diese Probleme k√∂nnen mit den in den folgenden Abschnitten beschriebenen alternativen Zerlegungsmethoden gel√∂st werden. <br><br><h3>  5.8.3.1.  Unbegrenzte Zersetzung </h3><br>  Der Standard-SVD-Algorithmus ist eine analytische L√∂sung f√ºr das niedrigrangige Approximationsproblem.  Dieses Problem kann jedoch als Optimierungsproblem betrachtet werden, und es k√∂nnen auch universelle Optimierungsmethoden darauf angewendet werden.  Einer der einfachsten Ans√§tze besteht darin, die Gradientenabstiegsmethode zu verwenden, um die Werte versteckter Faktoren iterativ zu verfeinern.  Ausgangspunkt ist die Definition der Kostenfunktion J als verbleibender Prognosefehler: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/g8/d_/gk/g8d_gkpksz8-d3wyg0xs9ca60a8.png" alt="Bild"></div><br>  Bitte beachten Sie, dass wir diesmal der Matrix der versteckten Faktoren keine Einschr√§nkungen wie Orthogonalit√§t auferlegen.  Wenn wir den Gradienten der Kostenfunktion in Bezug auf versteckte Faktoren berechnen, erhalten wir das folgende Ergebnis: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jn/_l/cu/jn_lcurj48vk0kuluh8mojadgcy.png" alt="Bild"></div><br>  wobei E die Restfehlermatrix ist: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6z/wt/at/6zwtat7l4h9qoo0fq7ejzatlobi.png" alt="Bild"></div><br>  Der Gradientenabstiegsalgorithmus minimiert die Kostenfunktion, indem er sich bei jedem Schritt in die negative Richtung des Gradienten bewegt.  Daher k√∂nnen Sie versteckte Faktoren finden, die den quadratischen Fehler der Bewertungsvorhersage minimieren, indem Sie die Matrizen P und Q iterativ √§ndern, um gem√§√ü den folgenden Ausdr√ºcken zu konvergieren: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f9/uw/gu/f9uwguspa118fw__nuchwxu1ycy.png" alt="Bild"></div><br>  wobei Œ± die Lerngeschwindigkeit ist.  Der Nachteil der Gradientenabstiegsmethode ist die Notwendigkeit, die gesamte Matrix der Restfehler zu berechnen und gleichzeitig alle Werte der verborgenen Faktoren in jeder Iteration zu √§ndern.  Ein alternativer Ansatz, der m√∂glicherweise besser f√ºr gro√üe Matrizen geeignet ist, ist der stochastische Gradientenabstieg [Funk, 2016].  Der stochastische Gradientenabstiegsalgorithmus verwendet die Tatsache, dass der Gesamtprognosefehler J die Summe der Fehler f√ºr einzelne Elemente der Bewertungsmatrix ist, daher kann der allgemeine Gradient J durch einen Gradienten an einem Datenpunkt angen√§hert werden und die verborgenen Faktoren k√∂nnen elementweise ge√§ndert werden.  Die vollst√§ndige Umsetzung dieser Idee ist in Algorithmus 5.1 dargestellt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/le/c5/ct/lec5ctumggjq0twtb30wnamvj0g.png" alt="Bild"></div><br>  Die erste Stufe des Algorithmus ist die Initialisierung der Matrix versteckter Faktoren.  Die Wahl dieser Anfangswerte ist nicht sehr wichtig, aber in diesem Fall wird eine gleichm√§√üige Verteilung der Energie bekannter Bewertungen unter zuf√§llig erzeugten versteckten Faktoren gew√§hlt.  Dann optimiert der Algorithmus nacheinander die Dimensionen des Konzepts.  Bei jeder Messung werden alle Bewertungen im Trainingssatz wiederholt umgangen, jede Bewertung anhand der aktuellen Werte der verborgenen Faktoren vorhergesagt, der Fehler gesch√§tzt und die Werte der Faktoren gem√§√ü den Ausdr√ºcken 5.101 korrigiert.  Die Messoptimierung ist abgeschlossen, wenn die Konvergenzbedingung erf√ºllt ist, wonach der Algorithmus mit der n√§chsten Messung fortf√§hrt. <br><br>  Algorithmus 5.1 hilft, die Einschr√§nkungen der Standard-SVD-Methode zu √ºberwinden.  Es optimiert versteckte Faktoren durch Durchlaufen einzelner Datenpunkte und vermeidet so Probleme mit fehlenden Bewertungen und algebraischen Operationen mit riesigen Matrizen.  Der iterative Ansatz macht den stochastischen Gradientenabstieg f√ºr praktische Anwendungen auch bequemer als den Gradientenabstieg, bei dem ganze Matrizen unter Verwendung der Ausdr√ºcke 5.101 modifiziert werden. <br><br><h3>  BEISPIEL 5.6 </h3><br>  Tats√§chlich ist ein Ansatz, der auf verborgenen Faktoren basiert, eine ganze Gruppe von Methoden zum Unterrichten von Darstellungen, mit denen Muster, die in der Bewertungsmatrix enthalten sind, identifiziert und explizit in Form von Konzepten dargestellt werden k√∂nnen.  Manchmal haben Konzepte eine v√∂llig bedeutungsvolle Interpretation, insbesondere solche mit hoher Energie, obwohl dies nicht bedeutet, dass alle Konzepte immer eine bedeutungsvolle Bedeutung haben.  Wenn Sie beispielsweise den Matrixzerlegungsalgorithmus auf eine Filmbewertungsdatenbank anwenden, k√∂nnen Faktoren erzeugt werden, die ungef√§hr den psychografischen Dimensionen entsprechen, z. B. Melodram, Kom√∂die, Horror usw. Lassen Sie uns dieses Ph√§nomen anhand eines kleinen numerischen Beispiels veranschaulichen, das die Bewertungsmatrix aus Tabelle verwendet.  5.3: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/6a/vu/xw6avuizjd2x8aku8k-0r89ieem.png" alt="Bild"></div><br>  Subtrahieren Sie zuerst den globalen Durchschnitt Œº = 2,82 von allen Elementen, um die Matrix zu zentrieren, und f√ºhren Sie dann den Algorithmus 5.1 mit k = 3 versteckten Messungen und der Lernrate Œ± = 0,01 aus, um die folgenden zwei Faktorenmatrix zu erhalten: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/k-/rr/vd/k-rrvde1sezar3qo0e2vqfb7jh8.png" alt="Bild"></div><br>  Jede Zeile in diesen Matrizen entspricht einem Benutzer oder einem Film, und alle 12 Zeilenvektoren sind in Fig. 4 gezeigt.  5.15.  Bitte beachten Sie, dass die Elemente in der ersten Spalte (der erste Vektor von Konzepten) die gr√∂√üten Werte haben und die Werte in den nachfolgenden Spalten allm√§hlich abnehmen.  Dies wird durch die Tatsache erkl√§rt, dass der erste Konzeptvektor so viel Signalenergie erfasst, wie mit einer Messung erfasst werden kann, der zweite Konzeptvektor nur einen Teil der Restenergie erfasst usw. Beachten Sie ferner, dass das erste Konzept semantisch als Dramaachse interpretiert werden kann - Actionfilm, wobei die positive Richtung dem Actionfilm-Genre und die negative - dem Drama-Genre entspricht.  Die Bewertungen in diesem Beispiel sind stark korreliert, so dass deutlich zu sehen ist, dass die ersten drei Benutzer und die ersten drei Filme im ersten Vektorkonzept (Dramafilme und Benutzer, die solche Filme m√∂gen) gro√üe negative Werte aufweisen, w√§hrend die letzten drei Benutzer und die letzten drei Filme haben in derselben Spalte gro√üe positive Bedeutungen (Actionfilme und Benutzer, die dieses Genre bevorzugen).  Die zweite Dimension in diesem speziellen Fall entspricht haupts√§chlich der Tendenz des Benutzers oder Elements, die als psychografisches Attribut interpretiert werden kann (Kritikalit√§t der Urteile des Benutzers? Filmpopularit√§t?).  Andere Konzepte k√∂nnen als Rauschen betrachtet werden. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z0/s6/t9/z0s6t9vbnwdtgonx0p6rswo_ufe.png" alt="Bild"></div><br>  Die resultierende Matrix von Faktoren ist in den Spalten nicht vollst√§ndig orthogonal, sondern tendenziell orthogonal, da dies aus der Optimalit√§t der SVD-L√∂sung folgt.  Dies l√§sst sich anhand der Produkte von PTP und QTQ erkennen, die nahe an den Diagonalmatrizen liegen: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/zr/sf/pbzrsfbqm0hjvmoqgh1xlurslnm.png" alt="Bild"></div><br>  Die Matrizen 5.103 sind im Wesentlichen ein Vorhersagemodell, mit dem sowohl bekannte als auch fehlende Bewertungen bewertet werden k√∂nnen.  Sch√§tzungen k√∂nnen erhalten werden, indem zwei Faktoren multipliziert und der globale Durchschnitt addiert werden: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qg/du/d2/qgdud2i8fqm_ut6doi-8jnpllq8.png" alt="Bild"></div><br>  Die Ergebnisse geben die bekannten genau wieder und prognostizieren die fehlenden Bewertungen gem√§√ü den intuitiven Erwartungen.  Die Genauigkeit der Sch√§tzungen kann durch √Ñndern der Anzahl der Messungen erh√∂ht oder verringert werden, und die optimale Anzahl von Messungen kann in der Praxis durch Gegenpr√ºfung und Auswahl eines angemessenen Kompromisses zwischen Rechenkomplexit√§t und Genauigkeit bestimmt werden. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  25% Rabatt-Gutschein f√ºr Stra√üenh√§ndler - <b>Maschinelles Lernen</b> <br><br>  Nach Bezahlung der Papierversion des Buches wird ein elektronisches Buch per E-Mail verschickt. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460375/">https://habr.com/ru/post/de460375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460361/index.html">Tolle FAQ zur Cybersicherheit von medizinischen Informationssystemen</a></li>
<li><a href="../de460363/index.html">7 fehlende Faktoren im Ansatz 12 Faktor App</a></li>
<li><a href="../de460365/index.html">Verteilte Spur: Wir haben alles falsch gemacht</a></li>
<li><a href="../de460367/index.html">Chaos Engineering: die Kunst der absichtlichen Zerst√∂rung. Teil 1</a></li>
<li><a href="../de460373/index.html">Unter der Haube Turbo-Seiten: Architektur der Webseite Fast Download-Technologie</a></li>
<li><a href="../de460377/index.html">Verwenden von Liquibase zum Verwalten der Datenbankstruktur in einer Spring Boot-Anwendung. Teil 1</a></li>
<li><a href="../de460381/index.html">Was ist Durchsetzungsverm√∂gen und warum wird es ben√∂tigt?</a></li>
<li><a href="../de460383/index.html">Bildschirm√ºberg√§nge in Legend of Zelda verwenden die undokumentierten Funktionen von NES</a></li>
<li><a href="../de460387/index.html">SELinux Anf√§ngerleitfaden</a></li>
<li><a href="../de460393/index.html">Hintergrund: Was erwartet Sie von Fedora Silverblue?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>