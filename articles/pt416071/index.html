<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÅ üëÇ ü§ôüèΩ Redes neurais, princ√≠pios fundamentais de opera√ß√£o, diversidade e topologia üßúüèæ üÜì üë©üèø‚Äç‚úàÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As redes neurais revolucionaram o campo do reconhecimento de padr√µes, mas devido √† interpretabilidade n√£o √≥bvia do princ√≠pio de opera√ß√£o, elas n√£o s√£o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neurais, princ√≠pios fundamentais de opera√ß√£o, diversidade e topologia</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/416071/">  As redes neurais revolucionaram o campo do reconhecimento de padr√µes, mas devido √† interpretabilidade n√£o √≥bvia do princ√≠pio de opera√ß√£o, elas n√£o s√£o usadas em √°reas como medicina e avalia√ß√£o de riscos.  Requer uma representa√ß√£o visual da rede, o que a tornar√° n√£o uma caixa preta, mas pelo menos "transl√∫cida".  <b>Cristopher Olah, em Redes Neurais, Distribuidores e Topologia, demonstrou os princ√≠pios da opera√ß√£o de redes neurais e os conectou √† teoria matem√°tica da topologia e diversidade, que serviu de base para este artigo.</b>  Para demonstrar a opera√ß√£o de uma rede neural, s√£o usadas redes neurais profundas de baixa dimens√£o. <br><br>  Compreender o comportamento de redes neurais profundas geralmente n√£o √© uma tarefa trivial.  √â mais f√°cil explorar redes neurais profundas de baixa dimens√£o - redes nas quais existem apenas alguns neur√¥nios em cada camada.  Para redes de baixa dimens√£o, voc√™ pode criar visualiza√ß√µes para entender o comportamento e o treinamento dessas redes.  Essa perspectiva fornecer√° uma compreens√£o mais profunda do comportamento das redes neurais e observar√° a conex√£o que combina redes neurais com um campo de matem√°tica chamado topologia. <br><br>  Uma s√©rie de coisas interessantes segue-se a isso, incluindo os limites inferiores fundamentais da complexidade de uma rede neural capaz de classificar determinados conjuntos de dados. <br><br>  Considere o princ√≠pio da rede usando um exemplo <br><a name="habracut"></a><br>  Vamos come√ßar com um conjunto de dados simples - duas curvas em um avi√£o.  A tarefa de rede aprender√° a classificar os pontos pertencentes √†s curvas. <br><br><img src="https://habrastorage.org/webt/m4/od/cv/m4odcvui3bls-vx-zzfhprjatzg.png"><br><br>  Uma maneira √≥bvia de visualizar o comportamento de uma rede neural, para ver como o algoritmo classifica todos os objetos poss√≠veis (em nosso exemplo, pontos) de um conjunto de dados. <br><br>  Vamos come√ßar com a classe mais simples de rede neural, com uma camada de entrada e sa√≠da.  Essa rede tenta separar duas classes de dados, dividindo-os por uma linha. <br><br><img src="https://habrastorage.org/webt/ii/sz/eo/iiszeobw-_fe8mam77ku7o62g7g.png"><br><br>  Essa rede n√£o √© usada na pr√°tica.  As redes neurais modernas geralmente t√™m v√°rias camadas entre sua entrada e sa√≠da, chamadas camadas "ocultas". <br><br><img src="https://habrastorage.org/webt/ej/bp/dw/ejbpdwhjcvzouhxnir9a-zeavye.jpeg"><br><br><h3>  Diagrama de rede simples </h3><br>  Visualizamos o comportamento dessa rede, observando o que ela faz com diferentes pontos em seu campo.  Uma rede de camada oculta separa os dados de uma curva mais complexa do que uma linha. <br><br><img src="https://habrastorage.org/webt/wp/cr/vf/wpcrvf_a0tiqrhftmxhddovy8tk.png"><br><br>  Com cada camada, a rede transforma os dados, criando uma nova visualiza√ß√£o.  Podemos ver os dados em cada uma dessas visualiza√ß√µes e como a rede com uma camada oculta os classifica.  Quando o algoritmo atinge a apresenta√ß√£o final, a rede neural desenha uma linha atrav√©s dos dados (ou em dimens√µes superiores - um hiperplano). <br><br>  Na visualiza√ß√£o anterior, os dados em uma visualiza√ß√£o bruta s√£o considerados.  Voc√™ pode imaginar isso olhando para a camada de entrada.  Agora, considere-o depois de convertido para a primeira camada.  Voc√™ pode imaginar isso olhando para a camada oculta. <br>  Cada medida corresponde √† ativa√ß√£o de um neur√¥nio na camada. <br><br><img src="https://habrastorage.org/webt/fc/ha/ch/fchachtyqdnmtcgaxonj6d1xiu0.png"><br><br>  A camada oculta √© treinada na visualiza√ß√£o para que os dados sejam separ√°veis ‚Äã‚Äãlinearmente. <br><br>  <b>Renderiza√ß√£o de camada cont√≠nua</b> <br><br>  Na abordagem descrita na se√ß√£o anterior, aprendemos a entender redes observando a apresenta√ß√£o correspondente a cada camada.  Isso nos d√° uma lista discreta de visualiza√ß√µes. <br><br>  A parte n√£o trivial √© entender como passamos de um para o outro.  Felizmente, os n√≠veis de redes neurais t√™m propriedades que tornam isso poss√≠vel. <br>  Existem muitos tipos diferentes de camadas usadas em redes neurais. <br><br>  Considere uma camada tanh para um exemplo espec√≠fico.  A camada tanh-tanh (Wx + b) consiste em: <br><br><ol><li>  A transforma√ß√£o linear da matriz "peso" W </li><li>  Tradu√ß√£o usando o vetor b </li><li>  Aplica√ß√£o local de tanh. </li></ol><br>  Podemos representar isso como uma transforma√ß√£o cont√≠nua da seguinte maneira: <br><br><img src="https://habrastorage.org/webt/cn/vl/rb/cnvlrblwsnmp9mxxzbawagywuj0.gif"><br><br>  Este princ√≠pio de opera√ß√£o √© muito semelhante a outras camadas padr√£o que consistem em uma transforma√ß√£o afim, seguida pela aplica√ß√£o pontual de uma fun√ß√£o de ativa√ß√£o monot√¥nica. <br>  Este m√©todo pode ser usado para entender redes mais complexas.  Portanto, a rede a seguir classifica duas espirais levemente emaranhadas usando quatro camadas ocultas.  Com o tempo, pode-se observar que a rede neural passa de uma vis√£o bruta para um n√≠vel superior que a rede estudou para classificar dados.  Enquanto as espirais s√£o inicialmente emaranhadas, no final elas s√£o linearmente separ√°veis. <br><br><img src="https://habrastorage.org/webt/g0/-y/kg/g0-ykgrem8udnrx-xiua2rolthq.gif"><br><br>  Por outro lado, a pr√≥xima rede, que tamb√©m usa v√°rios n√≠veis, mas n√£o pode classificar duas espirais, que s√£o mais emaranhadas. <br><br><img src="https://habrastorage.org/webt/gv/f3/wc/gvf3wc_-a7yw2h-3odoobbsie7u.gif"><br><br>  Deve-se notar que essas tarefas t√™m complexidade limitada, porque redes neurais de baixa dimens√£o s√£o usadas.  Se redes mais amplas fossem usadas, a solu√ß√£o de problemas seria simplificada. <br><br><h3>  Camadas de Tang </h3><br>  Cada camada estica e comprime o espa√ßo, mas nunca corta, n√£o quebra e n√£o dobra.  Intuitivamente, vemos que as propriedades topol√≥gicas s√£o preservadas em cada camada. <br><br>  Tais transforma√ß√µes que n√£o afetam a topologia s√£o chamadas homomorfismos (Wiki - Este √© um mapeamento do sistema alg√©brico A que preserva as opera√ß√µes b√°sicas e as rela√ß√µes b√°sicas).  Formalmente, s√£o bijections que s√£o fun√ß√µes cont√≠nuas em ambas as dire√ß√µes.  Em um mapeamento bijetivo, cada elemento de um conjunto corresponde exatamente a um elemento de outro conjunto, e um mapeamento inverso que possui a mesma propriedade √© definido. <br><br>  <b>O teorema</b> <br><br>  Camadas com N entradas e sa√≠das N s√£o homomorfismos se a matriz de pesos W n√£o for degenerada.  (Voc√™ precisa ter cuidado com o dom√≠nio e o alcance.) <br><br><div class="spoiler">  <b class="spoiler_title">Prova:</b> <div class="spoiler_text">  1. Suponha que W tenha um determinante diferente de zero.  Ent√£o √© uma fun√ß√£o linear bijetiva com uma inversa linear.  As fun√ß√µes lineares s√£o cont√≠nuas.  Ent√£o, multiplica√ß√£o por W √© um homeomorfismo. <br>  2. Mapeamentos - homomorfismos <br>  3. tanh (sigmoid e softplus, mas n√£o ReLU) s√£o fun√ß√µes cont√≠nuas com inversos cont√≠nuos.  S√£o bijections se tivermos cuidado com a √°rea e o alcance que estamos considerando.  Seu uso no sentido dos ponteiros √© um homomorfismo. <br><br>  Assim, se W tem um determinante diferente de zero, a fibra √© homeom√≥rfica. <br></div></div><br><h3>  Topologia e classifica√ß√£o </h3><br>  Considere um conjunto de dados bidimensional com duas classes A, B‚äÇR2: <br><br>  A = {x |  d (x, 0) &lt;1/3} <br><br>  B = {x |  2/3 &lt;d (x, 0) &lt;1} <br><br><img src="https://habrastorage.org/webt/ow/2l/b1/ow2lb1ozxk5p4d-s4ho_lq6_ds8.png"><br><br>  A vermelho, B azul <br><br>  Requisito: Uma rede neural n√£o pode classificar esse conjunto de dados sem 3 ou mais camadas ocultas, independentemente da largura. <br><br>  Como mencionado anteriormente, a classifica√ß√£o com uma fun√ß√£o sigm√≥ide ou camada softmax √© equivalente a tentar encontrar o hiperplano (ou, nesse caso, a linha) que separa A e B na representa√ß√£o final.  Com apenas duas camadas ocultas, a rede √© topologicamente incapaz de compartilhar dados dessa maneira e est√° fadada ao fracasso nesse conjunto de dados. <br>  Na pr√≥xima visualiza√ß√£o, observamos uma visualiza√ß√£o latente enquanto a rede est√° treinando junto com a linha de classifica√ß√£o. <br><br><img src="https://habrastorage.org/webt/ap/nt/xe/apntxeprybgn8yf_jchwskwes44.gif"><br><br>  Para essa rede de treinamento, n√£o basta atingir um resultado de cem por cento. <br>  O algoritmo cai para um m√≠nimo local n√£o produtivo, mas √© capaz de atingir ~ 80% de precis√£o na classifica√ß√£o. <br><br>  Neste exemplo, havia apenas uma camada oculta, mas n√£o funcionou. <br>  Declara√ß√£o.  Cada camada √© um homomorfismo ou a matriz de peso da camada tem um determinante 0. <br><br><div class="spoiler">  <b class="spoiler_title">Prova:</b> <div class="spoiler_text">  Se este √© um homomorfismo, ent√£o A ainda est√° cercado por B, e a linha n√£o pode separ√°-los.  Mas suponha que ele tenha um determinante de 0: o conjunto de dados entra em colapso em algum eixo.  Como estamos lidando com algo homeom√≥rfico no conjunto de dados original, A √© cercado por B, e colapsar em qualquer eixo significa que teremos alguns pontos de A e B misturados, e isso torna imposs√≠vel distinguir. <br></div></div><br>  Se adicionarmos um terceiro elemento oculto, o problema se tornar√° trivial.  A rede neural reconhece a seguinte representa√ß√£o: <br><br><img src="https://habrastorage.org/webt/y1/p8/ol/y1p8olobp-zdo3shlaooa62hy8k.png"><br><br>  A visualiza√ß√£o torna poss√≠vel separar conjuntos de dados com um hiperplano. <br>  Para entender melhor o que est√° acontecendo, vejamos um conjunto de dados ainda mais simples, unidimensional: <br><br><img src="https://habrastorage.org/webt/ud/by/hr/udbyhrfnqgfdr9r9lvh4cg7apw0.png"><br><br>  A = [- 1/3,1 / 3] <br>  B = [- 1, ‚àí2 / 3] ‚à™ [2 / 3,1] <br>  Sem usar uma camada de dois ou mais elementos ocultos, n√£o podemos classificar esse conjunto de dados.  Mas, se usarmos uma rede com dois elementos, aprenderemos como representar os dados como uma boa curva que nos permite separar classes usando uma linha: <br><br><img src="https://habrastorage.org/webt/8_/w-/bo/8_w-boyjlwhtufafsnqpljuo8u0.gif"><br><br>  O que est√° havendo?  Um elemento oculto aprende a disparar quando x&gt; -1/2, e um aprende a disparar quando x&gt; 1/2.  Quando o primeiro √© acionado, mas n√£o o segundo, sabemos que estamos em A. <br><br><h3>  Conjectura de variedade </h3><br>  Isso se aplica a conjuntos de dados do mundo real, como conjuntos de imagens?  Se voc√™ √© s√©rio sobre a hip√≥tese da diversidade, acho que isso importa. <br><br>  A hip√≥tese multidimensional √© que os dados naturais formam coletores de baixa dimens√£o no espa√ßo da implanta√ß√£o.  Existem raz√µes te√≥ricas [1] e experimentais [2] para acreditar que isso √© verdade.  Nesse caso, a tarefa do algoritmo de classifica√ß√£o √© separar o pacote de variedades entrela√ßadas. <br><br>  Nos exemplos anteriores, uma classe cercava completamente a outra.  No entanto, √© improv√°vel que a variedade de imagens de c√£es esteja completamente cercada por uma cole√ß√£o de imagens de gatos.  Mas existem outras situa√ß√µes topol√≥gicas mais plaus√≠veis que ainda podem surgir, como veremos na pr√≥xima se√ß√£o. <br><br><h3>  Conex√µes e homotopias </h3><br>  Outro conjunto de dados interessante s√£o os dois tori A e B. <br><br><img src="https://habrastorage.org/webt/wl/er/ns/wlernsr6ibyfnuq2cbkepwgxxq8.png"><br><br>  Como os conjuntos de dados anteriores que examinamos, esse conjunto de dados n√£o pode ser dividido sem o uso de n + 1 dimens√µes, a quarta dimens√£o. <br><br>  As conex√µes s√£o estudadas na teoria dos n√≥s, no campo da topologia.  √Äs vezes, quando vemos uma conex√£o, n√£o fica imediatamente claro se √© incoer√™ncia (muitas coisas que se enroscam mas podem ser separadas por deforma√ß√£o cont√≠nua) ou n√£o. <br><br><img src="https://habrastorage.org/webt/lg/1h/bu/lg1hbu1mehjrj872e61gd73_3vi.png"><br><br>  Incoer√™ncia relativamente simples. <br><br>  Se uma rede neural usando camadas com apenas tr√™s unidades pode classific√°-la, ela √© incoerente.  (Pergunta: Todas as incoer√™ncias podem ser classificadas na rede com apenas tr√™s incoer√™ncias, teoricamente?) <br><br>  Do ponto de vista desse n√≥, a visualiza√ß√£o cont√≠nua de representa√ß√µes criadas por uma rede neural √© um procedimento para desvendar conex√µes.  Em topologia, chamaremos essa isotopia do ambiente entre o link original e os separados. <br><br>  Formalmente, a isotopia do espa√ßo circundante entre as variedades A e B √© uma fun√ß√£o cont√≠nua F: [0,1] √ó X ‚Üí Y, de modo que cada Ft √© um homeomorfismo de X at√© seu intervalo, F0 √© uma fun√ß√£o de identidade e F1 mapeia A para B. T .e.  Ft vai continuamente do mapa A para si mesmo, para o mapa A para B. <br><br>  Teorema: existe uma isotopia do espa√ßo circundante entre a entrada e a representa√ß√£o do n√≠vel da rede se: a) W n√£o √© degenerado, b) estamos prontos para transferir neur√¥nios para a camada oculta ec) h√° mais de um elemento oculto. <br><br><div class="spoiler">  <b class="spoiler_title">Prova:</b> <div class="spoiler_text">  1. A parte mais dif√≠cil √© a transforma√ß√£o linear.  Para tornar isso poss√≠vel, precisamos que W tenha um determinante positivo.  Nossa premissa √© que n√£o √© igual a zero, e podemos reverter o sinal se for negativo trocando dois neur√¥nios ocultos e, portanto, podemos garantir que o determinante seja positivo.  O espa√ßo das matrizes determinantes positivas est√° conectado; portanto, existe p: [0,1] ‚Üí GLn ¬Æ5, de modo que p (0) = Id ep (1) = W. Podemos passar continuamente da fun√ß√£o de identidade para a transforma√ß√£o W usando fun√ß√µes x ‚Üí p (t) x, multiplicando x em cada momento t por uma matriz que passa continuamente p (t). <br>  2. Podemos mover continuamente da fun√ß√£o de identidade para o mapa b usando a fun√ß√£o x ‚Üí x + tb. <br>  3. Podemos mover continuamente da fun√ß√£o id√™ntica para o uso pontual de œÉ com a fun√ß√£o: x ‚Üí (1-t) x + tœÉ (x) <br></div></div><br>  At√© agora, √© improv√°vel que os relacionamentos de que falamos apare√ßam em dados reais, mas h√° generaliza√ß√µes de um n√≠vel superior.  √â plaus√≠vel que esses recursos possam existir em dados reais. <br><br>  Conex√µes e n√≥s s√£o coletores unidimensionais, mas precisamos de 4 dimens√µes para que as redes possam desvendar todos eles.  Da mesma forma, pode ser necess√°rio um espa√ßo dimensional ainda mais alto para expandir as variedades n-dimensionais.  Todos os coletores n-dimensionais podem ser expandidos em 2n + 2 dimens√µes.  [3] <br><br><h3>  Sa√≠da f√°cil </h3><br>  A maneira mais f√°cil √© tentar separar os coletores e esticar as pe√ßas o mais emaranhadas poss√≠vel.  Embora isso n√£o esteja pr√≥ximo de uma solu√ß√£o genu√≠na, essa solu√ß√£o pode atingir uma precis√£o de classifica√ß√£o relativamente alta e ser um m√≠nimo local aceit√°vel. <br><br><img src="https://habrastorage.org/webt/7x/mf/fp/7xmffpy2eilztftxrcbv69xhsf4.png"><br><br>  Esses m√≠nimos locais s√£o absolutamente in√∫teis em termos de tentativa de resolver problemas topol√≥gicos, mas os problemas topol√≥gicos podem fornecer uma boa motiva√ß√£o para o estudo desses problemas. <br><br>  Por outro lado, se estamos interessados ‚Äã‚Äãapenas em obter bons resultados de classifica√ß√£o, a abordagem √© aceit√°vel.  Se um pequeno peda√ßo de um coletor de dados for capturado em outro coletor, isso √© um problema?  √â prov√°vel que seja poss√≠vel obter bons resultados de classifica√ß√£o arbitrariamente, apesar desse problema. <br><br>  Camadas aprimoradas para manipula√ß√£o de manifolds? <br><br>  √â dif√≠cil imaginar que camadas padr√£o com transforma√ß√µes afins s√£o realmente boas para manipular variedades. <br><br>  Talvez fa√ßa sentido ter uma camada completamente diferente, que possamos usar em composi√ß√£o com as mais tradicionais? <br><br>  O estudo de um campo vetorial com uma dire√ß√£o na qual queremos mudar o coletor √© promissor: <br><br><img src="https://habrastorage.org/webt/2z/iw/at/2ziwat9d2bjwlclnjrixwmm5llc.png"><br><br>  E ent√£o deformamos o espa√ßo com base no campo vetorial: <br><br><img src="https://habrastorage.org/webt/ig/qb/dr/igqbdrqbcl3gy85kv4rzbhirflk.png"><br><br>  Pode-se estudar o campo vetorial em pontos fixos (basta pegar alguns pontos fixos do conjunto de dados de teste para usar como √¢ncoras) e interpolar de alguma forma. <br><br><div class="spoiler">  <b class="spoiler_title">O campo vetorial acima tem o formato:</b> <div class="spoiler_text">  P (x) = (v0f0 (x) + v1f1 (x)) / (1 + 0 (x) + f1 (x)) <br></div></div><br>  Onde v0 e v1 s√£o vetores, e f0 (x) e f1 (x) s√£o Gaussianos n-dimensionais. <br><br><h3>  K-Camadas dos vizinhos mais pr√≥ximos </h3><br>  A separabilidade linear pode ser uma necessidade enorme e possivelmente irracional de redes neurais.  √â natural usar o m√©todo k-vizinhos mais pr√≥ximos (k-NN).  No entanto, o sucesso do k-NN depende muito da apresenta√ß√£o que classifica; portanto, √© necess√°ria uma boa apresenta√ß√£o antes que o k-NN possa funcionar bem. <br><br>  O k-NN √© diferenci√°vel em rela√ß√£o √† representa√ß√£o em que atua.  Dessa forma, podemos treinar diretamente a rede para classificar o k-NN.  Isso pode ser visto como uma esp√©cie de camada ‚Äúvizinho mais pr√≥ximo‚Äù que atua como uma alternativa ao softmax. <br>  N√£o queremos avisar com todo o nosso conjunto de treinamento para cada minipartideira, porque ser√° um procedimento muito caro.  A abordagem adaptada √© classificar cada elemento do minilote com base nas classes dos outros elementos do minilote, atribuindo a cada unidade o peso dividido pela dist√¢ncia da meta de classifica√ß√£o. <br><br>  Infelizmente, mesmo com arquiteturas complexas, o uso do k-NN reduz a probabilidade de erro - e o uso de arquiteturas mais simples degrada os resultados. <br><br><h3>  Conclus√£o </h3><br>  As propriedades topol√≥gicas dos dados, como relacionamentos, podem tornar imposs√≠vel a divis√£o linear de classes usando redes de baixa dimens√£o, independentemente da profundidade.  Mesmo nos casos em que √© tecnicamente poss√≠vel.  Por exemplo, espirais, que podem ser muito dif√≠ceis de separar. <br><br>  Para uma classifica√ß√£o precisa dos dados, as redes neurais precisam de amplas camadas.  Al√©m disso, as camadas tradicionais da rede neural s√£o pouco adequadas para representar manipula√ß√µes importantes com variedades;  mesmo se definirmos os pesos manualmente, seria dif√≠cil representar de forma compacta as transforma√ß√µes que queremos. <br><br><div class="spoiler">  <b class="spoiler_title">Links para fontes e explica√ß√µes</b> <div class="spoiler_text">  [1] Muitas das transforma√ß√µes naturais que voc√™ deseja realizar em uma imagem, como traduzir ou dimensionar um objeto nela ou alterar a ilumina√ß√£o, formariam curvas cont√≠nuas no espa√ßo da imagem se voc√™ as executasse continuamente. <br><br>  [2] Carlsson et al.  descobriram que manchas locais de imagens formam uma garrafa klein. <br>  [3] Este resultado √© mencionado na subse√ß√£o da Wikipedia em vers√µes Isotopy. <br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt416071/">https://habr.com/ru/post/pt416071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt416059/index.html">Mobio conversa com Daniil Shuleiko (Yandex.Taxi) sobre fus√£o com Uber, mercado de t√°xi e concorr√™ncia</a></li>
<li><a href="../pt416061/index.html">Ent√£o, eu vejo tudo</a></li>
<li><a href="../pt416063/index.html">As negocia√ß√µes dos russos n√£o t√™m onde registrar</a></li>
<li><a href="../pt416067/index.html">Vis√£o geral da vulnerabilidade do Mikrotik Winbox. Ou um arquivo grande</a></li>
<li><a href="../pt416069/index.html">Migra√ß√£o de dados ElasticSearch sem perdas</a></li>
<li><a href="../pt416073/index.html">Um bot simples de negocia√ß√£o de criptomoedas</a></li>
<li><a href="../pt416075/index.html">O FSB quer introduzir a responsabilidade pelo uso oculto de gravadores de voz e c√¢meras em smartphones [e n√£o apenas]</a></li>
<li><a href="../pt416077/index.html">PlantUML - Tudo o que os analistas de neg√≥cios precisam para criar gr√°ficos em documenta√ß√£o de software</a></li>
<li><a href="../pt416079/index.html">Corona Native para Android - usando c√≥digo Java personalizado em um jogo escrito em Corona</a></li>
<li><a href="../pt416081/index.html">Algo ainda est√° errado com o retorno a Habr</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>