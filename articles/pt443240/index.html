<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèª‚Äçüè´ üë©üèø‚Äçü§ù‚Äçüë©üèæ üòÜ Entendendo o Q-learning, o problema ‚ÄúAndar sobre uma pedra‚Äù üíÆ üë®‚Äçüîß üë©‚ÄçüöÄ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° Habr! Trago √† sua aten√ß√£o uma tradu√ß√£o do artigo "Entendendo o Q-Learning, o problema da caminhada no penhasco", de Lucas Vazquez . 




 No √∫ltim...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Entendendo o Q-learning, o problema ‚ÄúAndar sobre uma pedra‚Äù</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/443240/">  Ol√° Habr!  Trago √† sua aten√ß√£o uma tradu√ß√£o do artigo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">"Entendendo o Q-Learning, o problema da caminhada no penhasco",</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Lucas Vazquez</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/dl/ol/ezdlol2oj19smcejxtdlt4aon68.jpeg"></div><br><p> No <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">√∫ltimo post,</a> apresentamos o problema ‚ÄúAndar sobre uma pedra‚Äù e nos debru√ßamos sobre um algoritmo terr√≠vel que n√£o fazia sentido.  Desta vez, vamos revelar os segredos desta caixa cinza e ver que ela n√£o √© t√£o assustadora. </p><br><h3>  Sum√°rio </h3><br><p>  Conclu√≠mos que, maximizando a quantidade de recompensas futuras, tamb√©m encontramos o caminho mais r√°pido para a meta; portanto, nossa meta agora √© encontrar uma maneira de fazer isso! </p><br><a name="habracut"></a><br><h3>  Introdu√ß√£o ao Q-Learning </h3><br><ul><li>  Vamos come√ßar criando uma tabela que mede o desempenho de uma determinada a√ß√£o em qualquer estado (podemos medi-la com um valor escalar simples; portanto, quanto maior o valor, melhor a a√ß√£o) </li><li>  Esta tabela ter√° uma linha para cada estado e uma coluna para cada a√ß√£o.  Em nosso mundo, a grade possui 48 (4 por Y por 12 por X) estados e 4 a√ß√µes s√£o permitidas (cima, baixo, esquerda, direita), portanto a tabela ser√° 48 x 4. </li><li>  Os valores armazenados nesta tabela s√£o chamados de "valores Q". </li><li>  Essas s√£o estimativas da quantidade de recompensas futuras.  Em outras palavras, eles estimam quanto mais recompensas podemos obter antes do final do jogo, estando no estado S e realizando a a√ß√£o A. </li><li>  Inicializamos a tabela com valores aleat√≥rios (ou alguma constante, por exemplo, todos os zeros). </li></ul><br><p>  A ‚Äútabela Q‚Äù ideal possui valores que nos permitem realizar as melhores a√ß√µes em cada estado, dando-nos a melhor maneira de vencer no final.  O problema est√° resolvido, aplausos, Robot Lords :). </p><br><img src="https://habrastorage.org/webt/5a/ii/wl/5aiiwljmx4igtrsrhrc3qoymoge.png"><br>  <i>Valores Q dos cinco primeiros estados.</i> <br><br><h4>  Q-learning </h4><br><ul><li>  Q-learning √© um algoritmo que "aprende" esses valores. </li><li>  A cada passo, obtemos mais informa√ß√µes sobre o mundo. </li><li>  Esta informa√ß√£o √© usada para atualizar os valores na tabela. </li></ul><br><p>  Por exemplo, suponha que estamos a um passo do alvo (quadrado [2, 11]) e, se decidirmos descer, obteremos uma recompensa de 0 em vez de -1. <br>  Podemos usar essas informa√ß√µes para atualizar o valor do par de a√ß√£o de estado em nossa tabela e, na pr√≥xima vez em que o visitarmos, j√° saberemos que a descida nos d√° uma recompensa de 0. </p><br><img src="https://habrastorage.org/webt/7a/iq/u3/7aiqu3ttrz1qnypctrqvlgyd93e.png"><br><p>  Agora podemos espalhar essas informa√ß√µes ainda mais!  Como agora sabemos o caminho para o objetivo a partir do quadrado [2, 11], qualquer a√ß√£o que nos leve ao quadrado [2, 11] tamb√©m ser√° boa, portanto atualizamos o valor Q do quadrado, o que nos leva a [2, 11] estar mais perto de 0. </p><br><p>  <b>E isso, senhoras e senhores, √© a ess√™ncia do Q-learning!</b> </p><br><p>  Observe que toda vez que atingimos a meta, aumentamos nosso "mapa" de como alcan√ß√°-la em um quadrado. Portanto, ap√≥s um n√∫mero suficiente de itera√ß√µes, teremos um mapa completo que nos mostrar√° como chegar √† meta de cada estado. </p><br><img src="https://habrastorage.org/webt/mj/q0/sh/mjq0shtkn3u37zlhdpptbh2oppe.gif"><br>  <i>Um caminho √© gerado executando a melhor a√ß√£o em todos os estados.</i>  <i>Chave verde representa o significado de uma a√ß√£o melhor, chaves mais saturadas representam um valor mais alto.</i>  <i>O texto representa um valor para cada a√ß√£o (cima, baixo, direita, esquerda).</i> <br><br><h3>  Equa√ß√£o de Bellman </h3><br><p>  Antes de falarmos sobre c√≥digo, vamos falar sobre matem√°tica: o conceito b√°sico de Q-learning, a equa√ß√£o de Bellman. </p><br><img src="https://habrastorage.org/webt/i2/_u/gx/i2_ugxlinshqmsyzkawlbmirxri.png"><br><br><ul><li>  Primeiro vamos esquecer Œ≥ nesta equa√ß√£o </li><li>  A equa√ß√£o afirma que o valor Q para um determinado par de a√ß√£o e estado deve ser a recompensa recebida na transi√ß√£o para um novo estado (executando esta a√ß√£o), adicionado ao valor da melhor a√ß√£o no pr√≥ximo estado. </li></ul><br><p>  <b>Em outras palavras, divulgamos informa√ß√µes sobre os valores das a√ß√µes, um passo de cada vez!</b> </p><br><p>  Mas podemos decidir que receber uma recompensa agora √© mais valioso do que receber uma recompensa no futuro e, portanto, temos Œ≥, um n√∫mero de 0 a 1 (geralmente de 0,9 a 0,99) que √© multiplicado por uma recompensa no futuro descontando recompensas futuras. </p><br><p>  Portanto, dado Œ≥ = 0,9 e aplicando isso a alguns estados do nosso mundo (grade), temos: </p><br><img src="https://habrastorage.org/webt/e7/yp/gi/e7ypginhzc-cetmrcbqa6ar3byg.png"><br><br><p>  Podemos comparar esses valores com os itens acima no GIF e ver se eles s√£o iguais. </p><br><br><h3>  Implementa√ß√£o </h3><br><p>  Agora que temos um entendimento intuitivo de como o Q-learning funciona, podemos come√ßar a pensar em implementar tudo isso, e usaremos o pseudoc√≥digo do Q-learning <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">do livro</a> de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Sutton</a> como um guia. </p><br><img src="https://habrastorage.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png"><br>  <i>Pseudoc√≥digo do livro de Sutton.</i> <br><br><p>  C√≥digo: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize Q arbitrarily, in this case a table full of zeros q_values = np.zeros((num_states, num_actions)) # Iterate over 500 episodes for _ in range(500): state = env.reset() done = False # While episode is not over while not done: # Choose action action = egreedy_policy(q_values, state, epsilon=0.1) # Do the action next_state, reward, done = env.step(action) # Update q_values td_target = reward + gamma * np.max(q_values[next_state]) td_error = td_target - q_values[state][action] q_values[state][action] += learning_rate * td_error # Update state state = next_state</span></span></code> </pre> <br><ul><li>  Primeiro, dizemos: ‚ÄúPara todos os estados e a√ß√µes, inicializamos Q (s) a arbitrariamente‚Äù, isso significa que podemos criar nossa tabela de valores Q com quaisquer valores que desejamos, eles podem ser aleat√≥rios, podem ser qualquer tipo de permanente n√£o importa.  Vemos que na linha 2 criamos uma tabela cheia de zeros. </li></ul><br><p>  <b>Tamb√©m dizemos: ‚ÄúO valor de Q para os estados finais √© zero‚Äù, n√£o podemos executar nenhuma a√ß√£o nos estados finais; portanto, consideramos o valor de todas as a√ß√µes nesse estado como zero.</b> </p><br><ul><li>  Para cada epis√≥dio, precisamos ‚Äúinicializar S‚Äù, essa √© apenas uma maneira elegante de dizer ‚Äúreinicie o jogo‚Äù; no nosso caso, significa colocar o jogador na posi√ß√£o inicial;  em nosso mundo, existe um m√©todo que faz exatamente isso, e o chamamos na linha 6. </li><li>  Para cada etapa do tempo (toda vez que precisamos agir), devemos escolher a a√ß√£o obtida de Q. </li></ul><br><p>  Lembre-se, eu disse, ‚Äúestamos tomando as a√ß√µes que s√£o mais valiosas em todas as condi√ß√µes? </p><br><p>  Quando fazemos isso, usamos nossos valores Q para criar a pol√≠tica;  nesse caso, ser√° uma pol√≠tica gananciosa, porque sempre tomamos a√ß√µes que, em nossa opini√£o, s√£o as melhores em todos os estados; portanto, diz-se que agimos com avidez. </p><br><br><h3>  Lixo eletr√¥nico </h3><br><p>  Mas h√° um problema com essa abordagem: imagine que estamos em um labirinto que possui duas recompensas, uma delas √© +1 e a outra √© +100 (e toda vez que encontramos uma delas, o jogo termina).  Como sempre realizamos a√ß√µes que consideramos as melhores, ficaremos presos ao primeiro pr√™mio encontrado, sempre retornando a ele; portanto, se reconhecermos primeiro o pr√™mio +1, perderemos o grande pr√™mio +100. </p><br><br><h3>  Solu√ß√£o </h3><br><p>  Precisamos garantir que estudemos o mundo o suficiente (essa √© uma tarefa incrivelmente dif√≠cil).  √â aqui que Œµ entra em jogo.  Œµ no algoritmo ganancioso significa que devemos agir ansiosamente, MAS executar a√ß√µes aleat√≥rias como uma porcentagem de Œµ ao longo do tempo, portanto, com um n√∫mero infinito de tentativas, devemos examinar todos os estados. </p><br><p>  A a√ß√£o √© selecionada de acordo com esta estrat√©gia na linha 12, com epsilon = 0,1, o que significa que estamos pesquisando o mundo 10% do tempo.  A implementa√ß√£o da pol√≠tica √© a seguinte: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">egreedy_policy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(q_values, state, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get a random number from a uniform distribution between 0 and 1, # if the number is lower than epsilon choose a random action if np.random.random() &lt; epsilon: return np.random.choice(4) # Else choose the action with the highest value else: return np.argmax(q_values[state])</span></span></code> </pre><br><p>  Na linha 14 da primeira listagem, chamamos o m√©todo step para executar a a√ß√£o, o mundo nos devolve o pr√≥ximo estado, recompensa e informa√ß√µes sobre o final do jogo. </p><br><p>  De volta √† matem√°tica: </p><br><p>  Temos uma equa√ß√£o longa, vamos pensar sobre isso: </p><br><img src="https://habrastorage.org/webt/9v/bn/ws/9vbnws8g-1gclwefuvtpjv_yqpm.png"><br><br><p>  Se tomarmos Œ± = 1: </p><br><img src="https://habrastorage.org/webt/7r/aw/er/7rawertkpcilxbfzrorhpygtrok.png"><br><br><p>  O que corresponde exatamente √† equa√ß√£o de Bellman, que vimos alguns par√°grafos atr√°s!  Portanto, j√° sabemos que essa √© a linha respons√°vel pela dissemina√ß√£o de informa√ß√µes sobre os valores do estado. </p><br><p>  Mas geralmente Œ± (conhecido principalmente como velocidade de aprendizado) √© muito menor que 1, seu objetivo principal √© evitar grandes altera√ß√µes em uma atualiza√ß√£o; portanto, em vez de voar para o objetivo, abordamos-o lentamente.  Em nossa abordagem tabular, definir Œ± = 1 n√£o causa problemas, mas ao trabalhar com redes neurais (mais sobre isso nos artigos a seguir), tudo pode facilmente sair do controle. </p><br><p>  Observando o c√≥digo, vemos que na linha 16 da primeira listagem que definimos td_target, esse √© o valor que devemos aproximar, mas em vez de ir diretamente para esse valor na linha 17, calculamos td_error, usaremos esse valor em combina√ß√£o com a velocidade aprendendo a avan√ßar lentamente em dire√ß√£o √† meta. </p><br><p>  <b>Lembre-se de que esta equa√ß√£o √© uma entidade de Q-Learning.</b> </p><br><p>  Agora s√≥ precisamos atualizar nosso estado, e tudo est√° pronto, esta √© a linha 20. Repetimos esse processo at√© chegarmos ao final do epis√≥dio, morrendo nas rochas ou atingindo a meta. </p><br><br><h3>  Conclus√£o </h3><br><p>  Agora, intuitivamente entendemos e sabemos como codificar o Q-Learning (pelo menos uma vers√£o tabular), verifique todo o c√≥digo usado para esta postagem, dispon√≠vel no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GitHub</a> . </p><br><p>  Visualiza√ß√£o do teste do processo de aprendizagem: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Vto8n9C7DSQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Observe que todas as a√ß√µes come√ßam com um valor que excede seu valor final; esse √© um truque para estimular a explora√ß√£o do mundo. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt443240/">https://habr.com/ru/post/pt443240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt443222/index.html">Como o Protonmail √© bloqueado na R√∫ssia</a></li>
<li><a href="../pt443224/index.html">Esteira com um elefante e um cavalo. M√©todo TWIX</a></li>
<li><a href="../pt443234/index.html">O que os engenheiros da Apple e da Intel fazem no escrit√≥rio: um curso on-line voltado para a carreira em microeletr√¥nica moderna para estudantes</a></li>
<li><a href="../pt443236/index.html">Desmistificar redes neurais convolucionais</a></li>
<li><a href="../pt443238/index.html">Como n√£o se transformar em uma lib√©lula se voc√™ tem muitos bancos de dados diferentes</a></li>
<li><a href="../pt443242/index.html">Quarkus √© um Java subat√¥mico supers√¥nico. Uma breve vis√£o geral da estrutura</a></li>
<li><a href="../pt443244/index.html">Tarefas de revis√£o. Beanpoisk_1</a></li>
<li><a href="../pt443246/index.html">Como reinventamos o Askozia IP PBX depois que o projeto foi vendido e fechado pelo desenvolvedor</a></li>
<li><a href="../pt443250/index.html">Coletor de Lixo Caseiro para OpenJDK</a></li>
<li><a href="../pt443252/index.html">Bots de formiga modulares com mem√≥ria</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>