<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñïüèΩ üë®üèª‚Äçüåæ ‚úùÔ∏è Servicio inform√°tico de GPU altamente cargado üôä üòß üöµüèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola Habr! Dirijo el desarrollo de la plataforma Vision : esta es nuestra plataforma p√∫blica, que proporciona acceso a modelos de visi√≥n por computado...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Servicio inform√°tico de GPU altamente cargado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/472928/"><img src="https://habrastorage.org/webt/_v/yk/sa/_vyksasjmhcbsn1feox_egbqs_4.jpeg"><br><br>  Hola Habr!  Dirijo el desarrollo de la plataforma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Vision</a> : esta es nuestra plataforma p√∫blica, que proporciona acceso a modelos de visi√≥n por computadora y le permite resolver tareas tales como reconocer caras, n√∫meros, objetos y escenas completas.  Y hoy quiero decir con el ejemplo de Vision c√≥mo implementar un servicio r√°pido y altamente cargado usando tarjetas de video, c√≥mo implementarlo y operarlo. <br><a name="habracut"></a><br><h1>  ¬øQu√© es la visi√≥n? </h1><br>  Esto es esencialmente una API REST.  El usuario genera una solicitud HTTP con una foto y la env√≠a al servidor. <br><br>  Suponga que necesita reconocer una cara en una imagen.  El sistema lo encuentra, lo corta, extrae algunas propiedades de la cara, lo guarda en la base de datos y le asigna un n√∫mero condicional.  Por ejemplo, persona42.  El usuario luego carga la siguiente foto, que tiene la misma persona.  El sistema extrae propiedades de su cara, busca en la base de datos y devuelve el n√∫mero condicional que se asign√≥ inicialmente a la persona, es decir,  persona42. <br><br>  Hoy, los principales usuarios de Vision son varios proyectos del Grupo Mail.ru.  La mayor√≠a de las solicitudes provienen de Mail and Cloud. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c77/448/9a8/c774489a8bcd0a3badf0a8413d95e97c.png" width="400"></div><br>  En la nube, los usuarios tienen carpetas en las que se cargan las fotos.  La nube ejecuta archivos a trav√©s de Vision y los agrupa en categor√≠as.  Despu√©s de eso, el usuario puede hojear convenientemente sus fotos.  Por ejemplo, cuando desea mostrar fotos a amigos o familiares, puede encontrar r√°pidamente las que necesita. <br><br>  Tanto Mail como Cloud son servicios muy grandes con millones de personas, por lo que Vision procesa cientos de miles de solicitudes por minuto.  Es decir, es un servicio cl√°sico de alta carga, pero con un giro: tiene nginx, un servidor web, una base de datos y colas, pero en el nivel m√°s bajo de este servicio hay inferencia: ejecutar im√°genes a trav√©s de redes neuronales.  Es la ejecuci√≥n de redes neuronales que ocupa la mayor parte del tiempo y requiere recursos.  Las redes de computaci√≥n consisten en una secuencia de operaciones matriciales que generalmente requieren mucho tiempo en la CPU, pero est√°n perfectamente paralelas en la GPU.  Para ejecutar redes de manera efectiva, utilizamos un grupo de servidores con tarjetas de video. <br><br>  En este art√≠culo quiero compartir un conjunto de consejos que pueden ser √∫tiles al crear dicho servicio. <br><br><h1>  Desarrollo de servicios </h1><br><h3>  Tiempo de procesamiento para una solicitud </h3><br>  Para un sistema con una carga pesada, el tiempo de procesamiento de una solicitud y el rendimiento del sistema son importantes.  La selecci√≥n correcta de la arquitectura de la red neuronal proporciona, en primer lugar, una alta velocidad de procesamiento de consultas.  En ML, como en cualquier otra tarea de programaci√≥n, las mismas tareas pueden resolverse de diferentes maneras.  Tomemos la detecci√≥n de rostros: para resolver este problema, primero tomamos redes neuronales con arquitectura R-FCN.  Muestran una calidad bastante alta, pero tomaron alrededor de 40 ms en una imagen, lo que no nos conven√≠a. Luego recurrimos a la arquitectura MTCNN y obtuvimos un doble aumento de velocidad con una ligera p√©rdida de calidad. <br><br>  A veces, para optimizar el tiempo de c√°lculo de las redes neuronales, puede ser ventajoso implementar la inferencia en otro marco, no en el que se ense√±√≥.  Por ejemplo, a veces tiene sentido convertir su modelo a NVIDIA TensorRT.  Aplica una serie de optimizaciones y es especialmente bueno en modelos bastante complejos.  Por ejemplo, de alguna manera puede reorganizar algunas capas, fusionarlas e incluso tirarlas;  el resultado no cambiar√° y la velocidad de c√°lculo de inferencia aumentar√°.  TensorRT tambi√©n le permite administrar mejor la memoria y, despu√©s de algunos trucos, puede reducirla a calcular n√∫meros con menos precisi√≥n, lo que tambi√©n aumenta la velocidad de c√°lculo de la inferencia. <br><br><h3>  Descargar tarjeta de video </h3><br>  La inferencia de red se lleva a cabo en la GPU, la tarjeta de video es la parte m√°s cara del servidor, por lo que es importante usarla de la manera m√°s eficiente posible.  ¬øC√≥mo entender, hemos cargado completamente la GPU o podemos aumentar la carga?  Esta pregunta puede responderse, por ejemplo, utilizando el par√°metro de Utilizaci√≥n GPU en la utilidad nvidia-smi del paquete de controladores de video est√°ndar.  Esta figura, por supuesto, no muestra cu√°ntos n√∫cleos CUDA se cargan directamente en la tarjeta de video, sino cu√°ntos est√°n inactivos, pero le permite evaluar de alguna manera la carga de la GPU.  Por experiencia, podemos decir que una carga del 80-90% es buena.  Si tiene una carga del 10-20%, entonces esto es malo y todav√≠a hay potencial. <br><br>  Una consecuencia importante de esta observaci√≥n: debe intentar organizar el sistema para maximizar la carga de las tarjetas de video.  Adem√°s, si tiene 10 tarjetas de video, cada una de las cuales se carga al 10-20%, lo m√°s probable es que dos tarjetas de video de alta carga puedan resolver el mismo problema. <br><br><h3>  Rendimiento del sistema </h3><br>  Cuando env√≠a una imagen a la entrada de una red neuronal, el procesamiento de la imagen se reduce a una variedad de operaciones matriciales.  La tarjeta de video es un sistema multin√∫cleo, y las im√°genes de entrada que generalmente enviamos son peque√±as.  Digamos que hay 1,000 n√∫cleos en nuestra tarjeta de video, y tenemos 250 x 250 p√≠xeles en la imagen.  Solo, no podr√°n cargar todos los n√∫cleos debido a su tama√±o modesto.  Y si enviamos esas im√°genes al modelo de una en una, la carga de la tarjeta de video no exceder√° el 25%. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b9/b25/191/4b9b2519170a86b151420cc0a746a2fe.png"></div><br>  Por lo tanto, debe cargar varias im√°genes para inferencia a la vez y formar un lote a partir de ellas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/367/51b/04c36751b27fa428e9828a6fa007e25d.png"></div><br>  En este caso, la carga de la tarjeta de video aumenta al 95%, y el c√°lculo de la inferencia llevar√° tiempo como para una sola imagen. <br><br>  Pero, ¬øqu√© pasa si no hay 10 im√°genes en la cola para que podamos combinarlas en un lote?  Puede esperar un poco, por ejemplo, 50-100 ms con la esperanza de que lleguen las solicitudes.  Esta estrategia se llama estrategia de latencia fija.  Le permite combinar solicitudes de clientes en un b√∫fer interno.  Como resultado, aumentamos nuestro retraso en una cantidad fija, pero aumentamos significativamente el rendimiento del sistema. <br><br><h3>  Lanzamiento de inferencia </h3><br>  Capacitamos modelos en im√°genes de un formato y tama√±o fijos (por ejemplo, 200 x 200 p√≠xeles), pero el servicio debe admitir la capacidad de cargar varias im√°genes.  Por lo tanto, todas las im√°genes antes de enviarlas a inferencia, debe prepararse adecuadamente (cambiar el tama√±o, centrar, normalizar, traducir a flotante, etc.).  Si todas estas operaciones se realizan en un proceso que inicia inferencia, entonces su ciclo de trabajo se ver√° as√≠: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f75/5be/112/f755be112bc557d3ab602aee47e40d2e.png"><br><br>  Pasa algo de tiempo en el procesador, preparando los datos de entrada, durante alg√∫n tiempo esperando una respuesta de la GPU.  Es mejor minimizar los intervalos entre inferencias para que la GPU est√© menos inactiva. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/783/cd7/fdb783cd70c90fcc06ba2cfca5f9c7f2.png"><br><br>  Para hacer esto, puede iniciar otra transmisi√≥n o transferir la preparaci√≥n de im√°genes a otros servidores, sin tarjetas de video, pero con procesadores potentes. <br><br>  Si es posible, el proceso responsable de la inferencia solo debe ocuparse de ella: acceder a la memoria compartida, recopilar datos de entrada, copiarlos inmediatamente a la memoria de la tarjeta de video y ejecutar la inferencia. <br><br><h3>  Turbo boost </h3><br>  Lanzar redes neuronales es una operaci√≥n que consume recursos no solo de la GPU, sino tambi√©n del procesador.  Incluso si todo est√° organizado correctamente en t√©rminos de ancho de banda, y el hilo que realiza la inferencia ya est√° esperando nuevos datos, en un procesador d√©bil simplemente no tendr√° tiempo para saturar este flujo con nuevos datos. <br><br>  Muchos procesadores admiten la tecnolog√≠a Turbo Boost.  Le permite aumentar la frecuencia del procesador, pero no siempre est√° habilitado de forma predeterminada.  Vale la pena echarle un vistazo.  Para esto, Linux tiene la utilidad CPU Power: <code>$ cpupower frequency-info -m</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fd4/78f/ca4/fd478fca423620ee3bd684d2adfe9d73.png"></div><br>  Los procesadores tambi√©n tienen un modo de consumo de energ√≠a que puede ser reconocido por tal CPU Power: comando de <code>performance</code> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/12a/ed4/a1b/12aed4a1b0dec7240bd744232b5d4770.png"></div><br>  En el modo de ahorro de energ√≠a, el procesador puede acelerar su frecuencia y funcionar m√°s lentamente.  Debe ingresar al BIOS y seleccionar el modo de rendimiento.  Entonces el procesador siempre funcionar√° a la frecuencia m√°xima. <br><br><h1>  Implementaci√≥n de aplicaciones </h1><br>  Docker es excelente para implementar la aplicaci√≥n, le permite ejecutar aplicaciones en la GPU dentro del contenedor.  Para acceder a las tarjetas de video, primero debe instalar los controladores para la tarjeta de video en el sistema host: un servidor f√≠sico.  Luego, para iniciar el contenedor, debe hacer mucho trabajo manual: arroje correctamente las tarjetas de video dentro del contenedor con los par√°metros correctos.  Despu√©s de iniciar el contenedor, a√∫n ser√° necesario instalar controladores de video dentro de √©l.  Y solo despu√©s de eso puedes usar tu aplicaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20a/ae0/cb2/20aae0cb286e3cef415f2a32f4f12bfa.png"><br><br>  Este enfoque tiene una advertencia.  Los servidores pueden desaparecer del cl√∫ster y agregarse.  Es posible que diferentes servidores tengan diferentes versiones de controladores, y diferir√°n de la versi√≥n que est√° instalada dentro del contenedor.  En este caso, un Docker simple se romper√°: la aplicaci√≥n recibir√° un error de desajuste de la versi√≥n del controlador cuando intente acceder a la tarjeta de video. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb4/1af/0b2/bb41af0b281cf8a660d2cc35db133101.png"><br><br>  ¬øC√≥mo lidiar con eso?  Hay una versi√≥n de Docker de NVIDIA, gracias a la cual es m√°s f√°cil y m√°s agradable usar el contenedor.  Seg√∫n la propia NVIDIA y seg√∫n las observaciones pr√°cticas, la sobrecarga del uso de nvidia-docker es de aproximadamente el 1%. <br><br>  En este caso, los controladores deben instalarse solo en la m√°quina host.  Cuando inicie el contenedor, no necesita tirar nada dentro, y la aplicaci√≥n tendr√° acceso inmediato a las tarjetas de video. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c95/967/a3a/c95967a3a8ac668acaed1fc14550dcb8.png"><br><br>  La "independencia" de nvidia-docker de los controladores le permite ejecutar un contenedor desde la misma imagen en diferentes m√°quinas en las que est√°n instaladas diferentes versiones de los controladores.  ¬øC√≥mo se implementa esto?  Docker tiene un concepto llamado docker-runtime: es un conjunto de est√°ndares que describe c√≥mo un contenedor debe comunicarse con el kernel host, c√≥mo debe comenzar y detenerse, c√≥mo interactuar con el kernel y el controlador.  Comenzando con una versi√≥n espec√≠fica de Docker, es posible reemplazar este tiempo de ejecuci√≥n.  Esto es lo que hizo NVIDIA: reemplazan el tiempo de ejecuci√≥n, capturan las llamadas al controlador de video interno y convierten la versi√≥n correcta en llamadas al controlador de video. <br><br><h1>  Orquestaci√≥n </h1><br>  Elegimos a Kubernetes como la orquesta.  Es compatible con muchas caracter√≠sticas muy buenas que son √∫tiles para cualquier sistema muy cargado.  Por ejemplo, la detecci√≥n autom√°tica permite que los servicios accedan entre s√≠ dentro de un cl√∫ster sin reglas de enrutamiento complejas.  O tolerancia a fallas: cuando Kubernetes siempre tiene varios contenedores listos, y si algo le sucede a los suyos, Kubernetes lanzar√° inmediatamente un nuevo contenedor. <br><br>  Si ya tiene un cl√∫ster de Kubernetes configurado, no necesita tanto para comenzar a usar tarjetas de video dentro del cl√∫ster: <br><br><ul><li>  conductores relativamente nuevos <br></li><li>  instalado nvidia-docker versi√≥n 2 <br></li><li>  Docker Runtime configurado de forma predeterminada en `nvidia` en /etc/docker/daemon.json: <br> <code>"default-runtime": "nvidia"</code> <br> </li><li>  <code>kubectl create -f https://githubusercontent.com/k8s-device-plugin/v1.12/plugin.yml</code> instalado <code>kubectl create -f https://githubusercontent.com/k8s-device-plugin/v1.12/plugin.yml</code> </li></ul><br>  Despu√©s de configurar su cl√∫ster e instalar el complemento del dispositivo, puede especificar una tarjeta de video como recurso. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/414/c90/8f6/414c908f6d3ba6d4a9bded55921790c6.png"><br><br>  ¬øQu√© afecta esto?  Digamos que tenemos dos nodos, m√°quinas f√≠sicas.  En una hay una tarjeta de video, en la otra no.  Kubernetes detectar√° una m√°quina con una tarjeta de video y recoger√° nuestro pod en ella. <br><br>  Es importante tener en cuenta que Kubernetes no sabe c√≥mo manipular una tarjeta de video de forma competente entre las c√°psulas.  Si tiene 4 tarjetas de video y necesita 1 GPU para iniciar el contenedor, entonces no puede aumentar m√°s de 4 pods en su cl√∫ster. <br><br>  Tomamos como regla 1 Pod = 1 Modelo = 1 GPU. <br><br>  Hay una opci√≥n para ejecutar m√°s instancias en 4 tarjetas de video, pero no lo consideraremos en este art√≠culo, ya que esta opci√≥n no sale de la caja. <br><br>  Si varios modelos deben girar a la vez, es conveniente crear Implementaci√≥n en Kubernetes para cada modelo.  En su archivo de configuraci√≥n, puede especificar el n√∫mero de hogares para cada modelo, teniendo en cuenta la popularidad del modelo.  Si llegan muchas solicitudes al modelo, debe especificar muchos pods para √©l, si hay pocas solicitudes, hay pocos pods.  En total, el n√∫mero de hogares debe ser igual al n√∫mero de tarjetas de video en el cl√∫ster. <br><br>  Considera un punto interesante.  Digamos que tenemos 4 tarjetas de video y 3 modelos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04c/767/23a/04c76723a17fef7311eb41462db1d5a7.png"></div><br>  En las dos primeras tarjetas de video, deje que aumente la inferencia del modelo de reconocimiento facial, en otro reconocimiento de objetos y en otro reconocimiento de n√∫meros de autom√≥viles. <br><br>  Usted trabaja, los clientes van y vienen, y una vez, por ejemplo en la noche, surge una situaci√≥n en la que una tarjeta de video con objetos de inferencia simplemente no se carga, una peque√±a cantidad de solicitudes llega y las tarjetas de video con reconocimiento facial se sobrecargan.  Me gustar√≠a sacar un modelo con objetos en este momento y lanzar caras en su lugar para descargar las l√≠neas. <br><br>  Para el escalado autom√°tico de modelos en tarjetas de video, hay herramientas dentro de Kubernetes: escalado autom√°tico de solera horizontal (HPA, escalador autom√°tico de pod horizontal). <br>  Fuera de la caja, Kubernetes admite el autoescalado en la utilizaci√≥n de la CPU.  Pero en una tarea con tarjetas de video, ser√° mucho m√°s razonable usar informaci√≥n sobre la cantidad de tareas para cada modelo de escala. <br><br>  Hacemos esto: poner solicitudes para cada modelo en una cola.  Cuando se completan las solicitudes, las eliminamos de esta cola.  Si logramos procesar r√°pidamente las solicitudes de modelos populares, entonces la cola no aumenta.  Si el n√∫mero de solicitudes para un modelo en particular aumenta repentinamente, entonces la cola comienza a crecer.  Queda claro que necesita agregar tarjetas de video que ayuden a alinear la l√≠nea. <br><br>  Informaci√≥n sobre las colas que enviamos a trav√©s de HPA a trav√©s de Prometheus: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aa3/da3/6ba/aa3da36baec71d52dcd0bee93c8e52cf.png"><br><br>  Y luego hacemos un autoescalado de los modelos en las tarjetas de video en el cl√∫ster, dependiendo de la cantidad de solicitudes que les env√≠en. <br><br><h3>  CI / CD </h3><br>  Despu√©s de que haya adjuntado la aplicaci√≥n y la haya envuelto en Kubernetes, literalmente le queda un paso hacia la parte superior del proyecto.  Puede agregar CI / CD, aqu√≠ hay un ejemplo de nuestra tuber√≠a: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d3/982/b23/3d3982b23d9ff4f1d413cee86c601d8d.png"><br><br>  Aqu√≠ el programador lanz√≥ el nuevo c√≥digo en la rama maestra, despu√©s de lo cual la imagen de Docker con nuestros demonios de fondo se recopila autom√°ticamente y se ejecutan las pruebas.  Si todas las marcas de verificaci√≥n son verdes, la aplicaci√≥n se vierte en el entorno de prueba.  Si no hay problemas, puede enviar la imagen a la operaci√≥n sin ninguna dificultad. <br><br><h1>  Conclusi√≥n </h1><br>  En mi art√≠culo, mencion√© algunos aspectos del trabajo de un servicio altamente cargado utilizando una GPU.  Hablamos sobre formas de reducir el tiempo de respuesta de un servicio, como: <br><br><ul><li>  selecci√≥n de la arquitectura de red neuronal √≥ptima para reducir la latencia; </li><li>  Aplicaciones de marcos de optimizaci√≥n como TensorRT. </li></ul><br>  Plante√≥ los problemas de aumentar el rendimiento: <br><br><ul><li>  el uso de procesamiento por lotes de im√°genes; </li><li>  aplicando una estrategia de latencia fija para reducir el n√∫mero de ejecuciones de inferencia, pero cada inferencia procesar√° un mayor n√∫mero de im√°genes; </li><li>  optimizaci√≥n de la tuber√≠a de entrada de datos para minimizar el tiempo de inactividad de la GPU; </li><li>  "Lucha" con trote de procesador, eliminaci√≥n de operaciones vinculadas a la CPU a otros servidores. </li></ul><br>  Analizamos el proceso de implementaci√≥n de una aplicaci√≥n con una GPU: <br><br><ul><li>  Usando nvidia-docker dentro de Kubernetes </li><li>  escalado basado en el n√∫mero de solicitudes y HPA (escalador autom√°tico de pod horizontal). </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/472928/">https://habr.com/ru/post/472928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../472914/index.html">Wolfram Function Repository: plataforma de acceso abierto para extensiones de lenguaje Wolfram</a></li>
<li><a href="../472916/index.html">Backend, machine learning y serverless son los m√°s interesantes de la conferencia de julio Habr</a></li>
<li><a href="../472918/index.html">ZX Spectrum en Rusia y la CEI: c√≥mo se transform√≥ la b√∫squeda en l√≠nea fuera de l√≠nea</a></li>
<li><a href="../472922/index.html">Programador defensor m√°s fuerte que la entrop√≠a</a></li>
<li><a href="../472926/index.html">La ley de los retornos acelerados (parte 1)</a></li>
<li><a href="../472930/index.html">Astrof√≠sicos de Silicon Valley cuantificando la moda</a></li>
<li><a href="../472932/index.html">IntelliJ IDEA An√°lisis est√°tico vs mente humana</a></li>
<li><a href="../472934/index.html">¬øQu√© es un fideicomiso cero? Modelo de seguridad</a></li>
<li><a href="../472936/index.html">Operaci√≥n TA505: Infraestructura de red de agrupaci√≥n. Parte 3</a></li>
<li><a href="../472944/index.html">Data as a Service: qu√© es, dificultades t√©cnicas y c√≥mo sortearlas usando proxies residentes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>