<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåá üëéüèº üôÖüèæ Wie wir ein Ger√§t zur √úberwachung der Aufmerksamkeit von Fahrern entwickelt haben. Erleben Sie Yandex.Taxi üó∫Ô∏è üòÖ üë®‚Äçüåæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Das Taxi sollte bequem und sicher sein. Dies h√§ngt nicht nur von der Qualit√§t des Fahrzeugs und des Service ab, sondern auch von der Konzentration der...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie wir ein Ger√§t zur √úberwachung der Aufmerksamkeit von Fahrern entwickelt haben. Erleben Sie Yandex.Taxi</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/461137/"><img src="https://habrastorage.org/webt/fu/ag/ow/fuagowhmp0mr-5j1p_xvxe4vlkw.jpeg"><br><br>  Das Taxi sollte bequem und sicher sein.  Dies h√§ngt nicht nur von der Qualit√§t des Fahrzeugs und des Service ab, sondern auch von der Konzentration der Aufmerksamkeit des Fahrers, die bei √úberlastung abf√§llt.  Daher begrenzen wir auf Serviceebene die Zeit, die der Fahrer hinter dem Lenkrad verbringt. <br><br>  Aber manchmal sind die Fahrer schon m√ºde in der Leitung - zum Beispiel war eine Person den ganzen Tag mit einem anderen Job besch√§ftigt und entschied sich abends, zu ‚Äûlenken‚Äú.  Was tun?  Wie kann man verstehen, dass der Fahrer eingreift, ohne den Schlaf zu √§ndern?  Sie k√∂nnen beispielsweise versuchen, zu beurteilen, wie genau er die Stra√üe √ºberwacht, und Anzeichen von M√ºdigkeit erkennen, z. B. anhand der Art des Blinkens.  Klingt das einfach?  Alles ist komplizierter als es scheint. <br><br>  Heute werden wir den Lesern von Habr zun√§chst erz√§hlen, wie wir auf eine Kamera gekommen sind, die dies kann. <br><br>  Es ist also gegeben: Die H√§ufigkeit und Dauer von Blinzeln h√§ngt vom Erm√ºdungsgrad ab.  Wenn wir m√ºde sind, ist der Kopf weniger beweglich, die Blickrichtung √§ndert sich seltener, wir blinken h√§ufiger und lassen unsere Augen f√ºr l√§ngere Zeit geschlossen - der Unterschied kann in Bruchteilen einer Sekunde oder mehreren Rotationsgraden gemessen werden, existiert aber.  Unsere Aufgabe war es, ein Ger√§t zu entwickeln, mit dem wir Blinzeln sowie die Blickrichtung, das G√§hnen und die Kopfbewegungen analysieren k√∂nnen, um den Grad der Aufmerksamkeit und M√ºdigkeit des Fahrers zu beurteilen. <br><br><a name="habracut"></a>  Zuerst haben wir uns entschieden: Lassen Sie uns eine Laptop-Anwendung erstellen, sie den Freiwilligen unter den Mitarbeitern zur Verf√ºgung stellen und die eingebaute Kamera verwenden, um die von uns ben√∂tigten Zeichen zu verfolgen.  So werden wir sofort eine gro√üe Menge an Informationen zur Analyse sammeln und unsere Hypothesen schnell testen. <br><br>  Spoiler: nichts ist passiert!  Ziemlich schnell wurde klar, dass die meisten Leute, die an einem Computer arbeiten, st√§ndig auf die Tastatur schauen und den Kopf neigen.  Das hei√üt, die Augen sind nicht sichtbar und es ist nicht einmal klar, ob sie geschlossen oder ge√∂ffnet sind, eine Person blinkt oder schaut einfach vom Bildschirm zur Tastatur und umgekehrt. <br><br><img src="https://habrastorage.org/webt/l9/la/n6/l9lan6o4wjrpctaeelbsnuosnso.jpeg"><br><br>  Dann wurde uns klar, dass wir selbst f√ºr die Herstellung eines Prototyps eine Art Ger√§t ben√∂tigen.  Wir haben das erste verf√ºgbare IP-Kameramodell gekauft, das im Infrarotbereich funktioniert. <br><br>  Warum brauchen wir Infrarot?  Die Beleuchtung kann unterschiedlich sein, manchmal befindet sich der Benutzer im Schatten, manchmal kommt das Licht von hinten, von oben oder es gibt √ºberhaupt keine.  Wenn wir ein Messger√§t herstellen, sollte es unter allen Umst√§nden gleich funktionieren. <br><br>  F√ºr das Experiment kam eine ziemlich beliebte Kamera von Xiaomi - CHUANGMI. <br><br><img src="https://habrastorage.org/webt/tr/qt/f9/trqtf9jukb0tuzste_5rq-nsiwk.jpeg"><br><br>  Es stellte sich heraus, dass sie mit einer Frequenz von 15 Bildern pro Sekunde schie√üt, und wir brauchen doppelt so viel: Das Blinken dauert 30 bis 150 ms, bei 15 Bildern pro Sekunde riskierten wir, dass das Blinken k√ºrzer als 60 bis 70 ms ist.  Daher mussten wir die Firmware √§ndern, um die IR-Beleuchtung zwangsweise einzuschalten, direkten Zugriff auf den Videostream zu erhalten und die erforderlichen 30 Bilder pro Sekunde aufzunehmen.  Nachdem wir die Kamera an den Laptop angeschlossen und f√ºr den Empfang des Videostreams √ºber das RTSP-Protokoll konfiguriert hatten, begannen wir mit der Aufnahme der ersten Videos.  Die Kamera wurde 15 cm unter der Kamera des Laptops platziert, wodurch die Augen des Benutzers besser ‚Äûgesehen‚Äú werden konnten. <br><br>  Erfolg?  Und wieder nein.  Nachdem wir mehrere hundert Videos gesammelt hatten, stellten wir fest, dass nichts passierte.  Das Verhalten des Laptop-Benutzers w√§hrend des Tages unterscheidet sich vom Verhalten des Fahrers: Eine Person kann jederzeit aufstehen, einen Bissen nehmen, einfach gehen und sich aufw√§rmen, w√§hrend der Fahrer viel mehr Zeit in sitzender Position verbringt.  Daher passen solche Daten nicht zu uns. <br><br>  Es wurde klar, dass der einzige Weg darin besteht, eine geeignete Kamera herzustellen oder zu kaufen und sie im Auto zu installieren. <br><br>  Es scheint, dass alles elementar ist: Wir kaufen einen DVR, wenden uns dem Fahrer zu, befestigen uns im Auto und holen einmal pro Woche SD-Karten mit Videoaufnahmen ab.  Aber hier stellte sich in Wirklichkeit heraus, dass alles nicht so einfach war. <br><br>  Erstens ist es √§u√üerst schwierig, einen DVR mit IR-Beleuchtung zu finden, und wir m√ºssen das Gesicht besonders nachts gut sehen. <br><br>  Zweitens verf√ºgen alle DVRs √ºber ein Weitwinkelobjektiv, sodass der Bereich mit dem Gesicht des Fahrers recht klein ist und Sie auf der Aufzeichnung nichts erkennen k√∂nnen.  Ja, und Verzerrungen durch das Objektiv beeintr√§chtigen die Analyse der Position des Kopfes und der Blickrichtung. <br><br>  Drittens l√§sst sich dieses Unternehmen auf zehn, einhundert oder mehr Maschinen nicht gut skalieren.  Wir m√ºssen viele Daten von verschiedenen Fahrern sammeln, um sie zu analysieren und Schlussfolgerungen zu ziehen.  Das manuelle Wechseln der Speicherkarten auf hundert Maschinen pro Woche oder jeden Tag ist eine enorme Zeitverschwendung.  Wir haben sogar versucht, eine Kamera zu finden, mit der Videos in die Cloud hochgeladen werden k√∂nnen, aber es gab nichts Vergleichbares auf dem Markt. <br><br>  Es gab sogar die Idee, "Ihren eigenen DVR" aus dem Raspberry Pi zu machen, einer Kamera mit IR-Beleuchtung und Halterungen. <br><br><img src="https://habrastorage.org/webt/zr/ba/3z/zrba3zrobipczwlnn8u0swwvdoi.jpeg"><br><br>  Das Ergebnis war nicht ganz das, was wir erwartet hatten: umst√§ndlich, es ist unm√∂glich, die Kamera getrennt vom Computer zu installieren.  Tatsache ist, dass bei einer Kabell√§nge von mehr als 50 cm Probleme mit dem Signal auftreten und das CSI-Kabel selbst sehr zerbrechlich, zu breit und daher f√ºr die Installation in einer Maschine nicht sehr geeignet ist. <br><br>  Wir m√ºssen nach Hongkong, beschlossen wir.  Der Zweck der Reise war ziemlich abstrakt: zu sehen, was verschiedene Hersteller im Bereich der Analyse des Fahrerverhaltens tun, Produktmuster zu kaufen, wenn wir sie finden, und nach geeigneten technischen L√∂sungen / Komponenten zu suchen, die wir in Autos einbauen k√∂nnten. <br><br>  Wir gingen sofort zu zwei beliebten Ausstellungen von Elektronik und Komponenten.  Im Pavillon f√ºr Automobilelektronik sahen wir eine beispiellose Dominanz von Videorecordern, R√ºckfahrkameras und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ADAS-</a> Systemen, aber fast niemand war mit der Analyse des Fahrerverhaltens besch√§ftigt.  Die Prototypen mehrerer Hersteller stellten fest, dass sie einschliefen, ablenkten, rauchten und telefonierten, aber niemand dachte an M√ºdigkeit. <br><br>  Infolgedessen haben wir mehrere Muster von Kameras und Einplatinencomputern gekauft.  Es wurde klar, dass 1) es keine geeigneten fertigen Produkte f√ºr uns gibt;  2) Der Computer und die Kamera m√ºssen getrennt werden, um die Sicht des Fahrers nicht zu beeintr√§chtigen.  Aus diesem Grund haben wir eine Kamerakarte mit USB-Schnittstelle und als Recheneinheit einen Banana Pi-Computer mit einer Karte sowie mehrere Android-Player mit Amlogic-Prozessoren verwendet. <br><br><img src="https://habrastorage.org/webt/mh/yy/rq/mhyyrqsrpdv8ypl30wacadprkne.jpeg"><br><br>  "Warum sind die Spieler?"  - Du fragst.  Tats√§chlich sind der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">S912 und sogar der S905</a> in Bezug auf die Leistung ziemlich leistungsstark und k√∂nnen Videoaufnahmen f√ºr unsere Zwecke problemlos abrufen, selbst wenn die Bildanalyse direkt vor Ort erfolgt.  Eine Bildanalyse vor Ort war erforderlich, um nicht den gesamten Videostream an den Server zu senden. <br><br>  Z√§hlen wir: Eine Minute gut komprimiertes Video in einer H.264-Aufl√∂sung von 640 √ó 480 (30 FPS) ben√∂tigt mindestens 5 Megabyte.  In einer Stunde sind es also 300 Megabyte und f√ºr eine 8-Stunden-Standardschicht etwa 2-3 Gigabyte. <br><br>  Das t√§gliche Hochladen von 3 Gigabyte Video mit Hilfe eines LTE-Modems ist sehr ‚Äûteuer‚Äú.  Aus diesem Grund haben wir uns entschlossen, regelm√§√üig 5-min√ºtige Videos aufzunehmen und alles, was im Auto passiert, genau dort zu analysieren und in Form eines analysierten Ereignisstroms auf unsere Server hochzuladen: eine Reihe von Gesichtspunkten, eine Blickrichtung, eine Kopfdrehung usw. <br><br>  Wir kehrten gut gelaunt von den Ausstellungen zur√ºck, brachten eine Menge notwendigen (und unn√∂tigen) M√ºll mit und erkannten, wie wir den Prototyp weiter herstellen w√ºrden. <br><br>  Die USB-Kamera, die wir in Hongkong gefunden haben, war fast perfekt f√ºr uns: Gr√∂√üe 38 √ó 38 mm, Standardobjektive (12 mm), die M√∂glichkeit, IR-Strahler direkt auf die Platine zu l√∂ten. <br><br><img src="https://habrastorage.org/webt/vw/r9/fs/vwr9fs6x4vzo304bkvfw9phoqse.jpeg"><br><br>  Deshalb haben wir den Hersteller sofort gebeten, uns einen Prototyp mit den notwendigen Komponenten zu machen.  Jetzt haben wir verstanden: Wir brauchen eine USB-Kamera mit Hintergrundbeleuchtung und einen Einplatinen-PC f√ºr die Videoverarbeitung.  Wir beschlossen, alles auszuprobieren, was auf dem Markt pr√§sentiert wurde, und arrangierten eine Einkaufssitzung bei AliExpress.  Wir haben vier Dutzend verschiedene Kameras, ein Dutzend Single-Board-PCs, Android-Player, eine Sammlung von 12-mm-Objektiven und viele andere seltsame Ger√§te gekauft. <br><br><img src="https://habrastorage.org/webt/ck/6m/tb/ck6mtbk3ytd-8iucodljfkod-gs.jpeg"><br><br>  Das Problem mit der Hardware wurde behoben.  Und was ist mit Software? <br><br>  Sehr schnell konnten wir einen einfachen Prototyp auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenCV-</a> Basis <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erstellen</a> , der ein Video schreibt, das Gesicht des Fahrers findet, es analysiert, 68 wichtige Punkte im Gesicht markiert, Blinken, G√§hnen, Drehen des Kopfes usw. erkennt. <br><br>  Die n√§chste Aufgabe bestand darin, unseren Prototyp auf einem Einplatinen-PC zum Laufen zu bringen.  Raspberry PI fiel sofort ab: Wenige Kerne, ein schwacher Prozessor, mehr als sieben Frames pro Sekunde k√∂nnen nicht herausgezogen werden.  Und wie man gleichzeitig ein Video schreibt, ein Gesicht erkennt und analysiert, stand au√üer Frage.  Aus den gleichen Gr√ºnden passten Set-Top-Boxen und Single-Board-Computer auf Allwinner (H2, H3, H5), Amlogic S905 und Rockchip RK3328 nicht zu uns, obwohl letztere der gew√ºnschten Leistung sehr nahe kamen.  Daher haben wir noch zwei potenzielle SoCs: Amlogic S912 und Rockchip RK3399. <br><br>  Bei Amlogic war die Auswahl an Ger√§ten gering: eine TV-Box oder Khadas VIM2.  Bei der TV-Box und Khadas funktionierte alles gleich, aber die K√ºhlung der Set-Top-Boxen lie√ü zu w√ºnschen √ºbrig, und die Konfiguration von Linux auf ihnen ist oft nichts f√ºr schwache Nerven: Wi-Fi, BT zum Laufen bringen, das Betriebssystem den gesamten Speicher sehen lassen, - Es ist lang, schwierig und unvorhersehbar.  Aus diesem Grund haben wir uns f√ºr Khadas VIM2 entschieden: Es verf√ºgt √ºber einen Standardk√ºhler und die Platine ist kompakt genug, um sie hinter dem Armaturenbrett der Maschine zu verstecken. <br><br><img src="https://habrastorage.org/webt/f_/_1/lt/f__1ltkkzkqy08jygtdgs-jrooi.jpeg"><br><br>  Zu diesem Zeitpunkt hatte uns der Hersteller der Kamerakarte bereits eine Testcharge von einhundert St√ºck geschickt, und wir waren gespannt auf den Kampf: einen Prototyp herstellen, ihn in ein Auto stecken und Daten sammeln. <br><br>  Wir hatten eine Kamera, es gab Software, es gab einen Einplatinen-PC, aber es gab nicht die geringste Idee, wie man all dies in das Auto steckt und es an das Bordnetzteil anschlie√üt. <br><br>  Offensichtlich brauchte die Kamera einen K√∂rper und eine Halterung.  Wir haben zwei 3D-Drucker gleichzeitig gekauft, um Teile zu drucken, und der Auftragnehmer hat uns zum ersten primitiven Modell des Geh√§uses gemacht. <br><br><img src="https://habrastorage.org/webt/ju/jt/ct/jujtct45fz8xpuv-pd_xvr18qmm.jpeg"><br><br>  Jetzt hat sich die schwierige Aufgabe der Wahl ergeben: Wo muss die Kamera im Auto montiert werden, um ein gutes Bild zu erhalten, ohne die Sicht des Fahrers zu beeintr√§chtigen?  Es gab genau drei M√∂glichkeiten: <br><br><ol><li>  Mitten auf der Windschutzscheibe. </li><li>  Am linken Gestell. </li><li>  Auf dem R√ºckspiegel. </li></ol><br><img src="https://habrastorage.org/webt/ou/nv/en/ounven5ffmbmk-fnzvfqqy6oyls.jpeg"><br><br>  In diesem Moment schien es uns am besten, die Kamera direkt am R√ºckspiegel anzubringen: Sie ist immer auf das Gesicht des Fahrers gerichtet, damit die Kamera genau das aufnimmt, was wir brauchen.  Leider haben die Hersteller von R√ºckspiegeln nicht sichergestellt, dass etwas bequem und zuverl√§ssig an ihnen befestigt werden kann.  Die Kameras hielten nicht gut, fielen und schlossen die Bewertung ab. <br><br><img src="https://habrastorage.org/webt/9h/oo/gc/9hoogck8s_vhjdzabffnwni1n78.jpeg"><br><br>  Trotzdem haben wir mehrere Maschinen ausger√ºstet und begonnen, Daten von ihnen zu sammeln.  Es wurde deutlich, dass das Design nicht perfekt war und die Probleme in Bezug auf Leistung und Erw√§rmung zunahmen, w√§hrend gleichzeitig das Gesicht aufgezeichnet und analysiert wurde. <br><br>  Dann haben wir beschlossen, die Kamera auf Augenh√∂he am linken Rack zu montieren: Wir schlie√üen den Test weniger und haben einen guten Winkel f√ºr die Kamera, damit der Fahrer gesehen werden kann.  Das Geh√§use musste erneuert werden, da sich Befestigungselemente mit Scharnieren als √§u√üerst unzuverl√§ssig erwiesen: Sie brechen beim Sch√ºtteln auseinander, brechen und die Saugn√§pfe l√∂sen sich vom Glas. <br><br><img src="https://habrastorage.org/webt/qj/mt/ko/qjmtkocxocrcpmzhkzw3urszdem.jpeg"><br><br>  Wir haben beschlossen, dass es f√ºr den Prototyp und die Datenerfassung besser ist, die Kameras fest am Glas zu befestigen, damit keine Ersch√ºtterungen und √§u√üeren Einfl√ºsse ihre Position √§ndern k√∂nnen.  Wir haben das Geh√§use leicht modifiziert und gleichzeitig einen Belastungstest der Installation mit einem speziellen doppelseitigen Klebeband durchgef√ºhrt.  Zum Testen wurden komplexe und hochpr√§zise Ger√§te verwendet. <br><br><img src="https://habrastorage.org/webt/7r/c-/cb/7rc-cbdojhkigygkl9kbwucodek.jpeg"><br><br>  Aufgrund von Leistungsproblemen haben wir uns entschlossen, den SoC durch einen leistungsst√§rkeren zu ersetzen. Deshalb haben wir uns f√ºr den NanoPI M4-Einplatinen-PC auf dem Rockchip RK3399-Prozessor entschieden. <br><br>  Im Vergleich zu Khadas VIM2 ist es etwa ein Drittel produktiver, verf√ºgt √ºber Hardwarekomprimierung und Videodecodierung und verh√§lt sich unter schwierigen Temperaturbedingungen viel stabiler.  Ja, wir haben versucht, Kameras und Leiterplatten im Gefrierschrank zu betreiben, sie im Ofen zu erhitzen und viele andere unmenschliche Tests durchzuf√ºhren. <br><br><img src="https://habrastorage.org/webt/ro/ia/ty/roiatyksmkri083bp6dsk2soye4.jpeg"><br><br>  Da wir Videos nicht nur so aufnehmen, sondern den ganzen Tag √ºber dynamisch, war es wichtig, dass die Systemzeit auf dem Ger√§t genau war.  Leider sind die meisten Einplatinencomputer nicht mit einer Uhr mit eigener Stromversorgung ausgestattet.  Wir hatten Gl√ºck, dass unser NanoPI einen Batterieanschluss hatte. <br><br>  Ich musste einen Koffer f√ºr einen Computer entwerfen, der ihn physisch sch√ºtzt und als Halter f√ºr WLAN- und BT-Antennen fungiert.  Dort haben wir auch einen Platz f√ºr die Montage der Uhrenbatterie mit einem Halter bereitgestellt. <br><br><img src="https://habrastorage.org/webt/cy/94/6w/cy946wwax2lf0lt9szdcx-7-qbe.jpeg"><img src="https://habrastorage.org/webt/ba/s8/xu/bas8xuo0kr2v5_ti2gczvaups_i.jpeg"><br><br>  Au√üerdem wollten wir einhundert Maschinen mit Prototypen ausstatten, die Videos aufzeichnen und die gesamte Telemetrie online in die Cloud √ºbertragen: Gibt es einen Fahrer, der so oft und lange blinkt, g√§hnt, von der Stra√üe abgelenkt wird, den Kopf dreht usw. All dies ( und nicht nur) die Parameter erm√∂glichen es uns, ein Modell zu trainieren, das bewertet, wie konzentriert der Fahrer auf der Stra√üe ist, ob er abgelenkt oder m√ºde ist.  Um dies alles direkt auf dem Ger√§t im Auto zu tun, mussten wir den Code komplett neu schreiben, Hardware-Videokomprimierung durchf√ºhren, Protokolle und Videoaufzeichnungen drehen, ihn regelm√§√üig an den Server senden, Software aus der Ferne aktualisieren und vieles mehr. <br><br>  Gleichzeitig wurde uns klar, dass unsere Berechnungen und Algorithmen mit einer genaueren grundlegenden Gesichtsanalyse viel besser funktionieren w√ºrden.  In den ersten Prototypen verwendeten wir den in OpenCV integrierten Gesichtsdetektor basierend auf dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Haarkaskadenmodell</a> und das Modell zum Markieren von 68 Gesichtspunkten basierend auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dlib-</a> Bibliothek.  Wir haben die Position des Kopfes selbst <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">berechnet,</a> indem wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> Projektion der Gesichtspunkte auf die Fokusebene <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">berechnet haben</a> .  Open-Source-L√∂sungen zum Erkennen und Markieren von Gesichtern funktionieren gut bei Bildern, bei denen das Gesicht vor oder im Profil aufgenommen wird, aber unter Zwischenbedingungen sind sie h√§ufig falsch. <br><br>  Aus diesem Grund haben wir uns entschlossen, eine gute Gesichtserkennungs- und Markierungsl√∂sung von Drittanbietern zu lizenzieren - das VisionLabs SDK.  Im Vergleich zu fr√ºheren Algorithmen ist es ressourcenintensiver, erh√∂ht jedoch die Qualit√§t der Gesichtserkennung und -markierung sp√ºrbar, was zu einer genaueren Extraktion von Faktoren f√ºr das maschinelle Lernen f√ºhrt.  Mithilfe von Kollegen von VisionLabs konnten wir schnell zu ihrem SDK wechseln und die Leistung erzielen, die wir brauchten: 30 Bilder / Sek.  bei einer Aufl√∂sung von 640x480. <br><br>  Das VisionLabs SDK verwendet neuronale Netze zur Gesichtserkennung.  Die Technologie verarbeitet jeden Frame, findet das Gesicht des Fahrers darauf und gibt die Koordinaten von Augen, Nase, Mund und anderen wichtigen Punkten aus.  Die erhaltenen Daten werden verwendet, um einen normalisierten Rahmen mit einer Gr√∂√üe von 250 x 250 zu erstellen, bei dem sich das Gesicht streng in der Mitte befindet.  Dieser Rahmen kann bereits verwendet werden, um die Kopfposition in Grad entlang drei Achsen zu berechnen: Gieren, Neigen und Rollen.  Um den Status der Augen des Fahrers zu verfolgen, analysiert das System das Bild der Augen und entscheidet f√ºr jedes Auge, ob es geschlossen oder offen ist.  Das System kann mithilfe der IR Liveness-Technologie feststellen, ob sich eine lebende Person vor der Kamera befindet oder der Fahrer ein Foto angeh√§ngt hat.  F√ºr die Analyse wird ein normalisierter Frame verwendet, und am Ausgang erhalten wir das Ergebnis lebendig oder notaliv. <br><br><h4>  Fazit </h4><br>  W√§hrend wir Software umschrieben und debuggten, druckten unsere 3D-Drucker Tag und Nacht H√ºllen f√ºr Kameras und Single-Board-PCs.  Das Drucken des Kits (Kamerageh√§use + PC-Geh√§use) dauerte ca. 3-4 Stunden, sodass wir die Produktionskapazit√§ten erweitern mussten: Wir verwendeten vier Drucker.  Aber wir haben es geschafft, alles im Zeitplan zu erledigen. <br><br><img src="https://habrastorage.org/webt/ob/rs/b5/obrsb5c3yhdnisat6cn5jvatoa4.jpeg"><br><br>  In zwei Wochen haben wir die ersten hundert Autos in mehreren Taxiflotten - Yandex.Taxi-Partner - vollst√§ndig ausgestattet.  Jetzt sammeln wir mit ihrer Hilfe Videos, analysieren das Fahrerverhalten, Anzeichen von M√ºdigkeit, verbessern Algorithmen und trainieren Modelle, die den Grad der Aufmerksamkeit und M√ºdigkeit bewerten.  Und erst danach (unter Ber√ºcksichtigung aller Daten, R√ºckmeldungen von Fahrern und Passagieren) sind wir bereit, mit der n√§chsten Stufe fortzufahren - der Massenproduktion und -implementierung. <br><br>  Leider ist die derzeitige technische L√∂sung aus mehreren Gr√ºnden f√ºr die Skalierung auf mehrere tausend oder zehntausend Installationen nicht sehr geeignet.  Alles, wor√ºber wir in diesem Artikel gesprochen haben, ist ein kurzes Experiment, dessen Zweck es war, schnell zu lernen, wie man Daten direkt von den Maschinen sammelt, um Modelle zu trainieren.  Der n√§chste gro√üe Schritt f√ºr uns ist die Entwicklung und Produktion eines Ger√§ts mit den gleichen Abmessungen, das jedoch aus einer Einheit besteht: Kamera, Sensoren und Modem werden in einem kompakten Geh√§use untergebracht, das wir massiv in Maschinen installieren werden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461137/">https://habr.com/ru/post/de461137/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461125/index.html">Die Aufgabe, sequentielle numerische Codes zum Nummerieren von Nachrichten im Quellcode in Visual Studio zu erstellen (z. B. C #)</a></li>
<li><a href="../de461127/index.html">VM-Leistungsanalyse in VMware vSphere. Teil 3: Lagerung</a></li>
<li><a href="../de461129/index.html">√úber Kote, Frau, zwei S√∂hne, die Idee ... und nicht nur. Geschichte mit Fortsetzung</a></li>
<li><a href="../de461131/index.html">ROS Wagen LKW Teil 2. Software</a></li>
<li><a href="../de461133/index.html">Testen von SQL Server-Code mit tSQLt</a></li>
<li><a href="../de461141/index.html">Mein erster Tag mit Haiku: Sie ist unerwartet gut</a></li>
<li><a href="../de461143/index.html">Zu aktuellen Problemen des Spieldesigns und M√∂glichkeiten, diese zu l√∂sen. Blick von unten</a></li>
<li><a href="../de461145/index.html">Was sollte ein Team f√ºhren: Rollen, Verantwortlichkeiten und F√§higkeiten</a></li>
<li><a href="../de461147/index.html">So sparen Sie 64 Stunden, indem Sie Schl√ºssel in PowerPoint kombinieren</a></li>
<li><a href="../de461149/index.html">Ungehinderte MongoDB-Migration nach Kubernetes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>