<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåÇ üíï üëå Fechando furos em um cluster Kubernetes. Relat√≥rio e transcri√ß√£o com DevOpsConf üêù üöú üë®üèº‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pavel Selivanov, arquiteto de solu√ß√µes da Southbridge e palestrante da Slurm, fez uma apresenta√ß√£o no DevOpsConf 2019. Este relat√≥rio √© parte do curso...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Fechando furos em um cluster Kubernetes. Relat√≥rio e transcri√ß√£o com DevOpsConf</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/southbridge/blog/472484/"><p>  Pavel Selivanov, arquiteto de solu√ß√µes da Southbridge e palestrante da Slurm, fez uma apresenta√ß√£o no DevOpsConf 2019. Este relat√≥rio √© parte do curso aprofundado do Kubernetes, Slur Mega. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Slurm Basic: Uma introdu√ß√£o ao Kubernetes</a> ocorre em Moscou, de 18 a 20 de novembro. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Slurm Mega: N√≥s olhamos sob o cap√¥ de Kubernetes</a> - Moscou, de 22 a 24 de novembro. <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Slurm Online: Os dois cursos do Kubernetes est√£o sempre</a> dispon√≠veis. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Gt4Q1du5FXk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Sob o recorte - transcri√ß√£o do relat√≥rio. </p><a name="habracut"></a><br><p>  Boa tarde, colegas e simpatizantes.  Hoje vou falar sobre seguran√ßa. </p><br><p>  Vejo que hoje existem muitos seguran√ßas no sal√£o.  Pe√ßo desculpas antecipadamente se n√£o usarei os termos do mundo da seguran√ßa da mesma maneira que voc√™ aceitou. </p><br><p>  Aconteceu que cerca de seis meses atr√°s eu entrei nas m√£os de um grupo p√∫blico de Kubernetes.  P√∫blico - significa que existe um en√©simo n√∫mero de espa√ßos para nome; nesse espa√ßo para nome, h√° usu√°rios isolados em seu espa√ßo para nome.  Todos esses usu√°rios pertencem a diferentes empresas.  Bem, supunha-se que esse cluster deveria ser usado como uma CDN.  Ou seja, eles fornecem um cluster, eles fornecem o usu√°rio l√°, voc√™ vai l√° no seu namespace, implementa suas frentes. </p><br><p>  Eles tentaram vender esse servi√ßo para minha empresa anterior.  E me pediram para cutucar um cluster sobre o assunto - essa solu√ß√£o √© adequada ou n√£o. </p><br><p>  Eu vim para este cluster.  Eu recebi direitos limitados, espa√ßo para nome limitado.  L√°, os caras entenderam o que era seguran√ßa.  Eles leram o que o Kubernetes tinha RBAC (Controle de Acesso Baseado em Fun√ß√£o) - e o distorceram para que eu n√£o pudesse executar pods separadamente da implanta√ß√£o.  N√£o me lembro da tarefa que estava tentando resolver executando abaixo sem implanta√ß√£o, mas eu realmente queria executar logo abaixo.  Decidi, com sorte, ver quais direitos tenho no cluster, o que posso, o que n√£o posso, o que eles estragaram l√°.  Ao mesmo tempo, mostrarei o que eles configuraram no RBAC incorretamente. </p><br><p>  Aconteceu que, dois minutos depois, consegui um administrador para o cluster, examinei todos os namespaces vizinhos, vi as frentes de produ√ß√£o de empresas que j√° haviam comprado o servi√ßo e ficaram presas l√°.  Mal me detive, para n√£o encontrar algu√©m na frente e n√£o colocar nenhuma palavra obscena na p√°gina principal. </p><br><p>  Vou lhe contar com exemplos como eu fiz isso e como me proteger disso. </p><br><p>  Mas primeiro, me apresento.  Meu nome √© Pavel Selivanov.  Sou arquiteto em Southbridge.  Eu entendo Kubernetes, DevOps e todo tipo de coisas sofisticadas.  Os engenheiros da Southbridge e eu estamos construindo tudo isso, e eu aconselho. </p><br><p>  Al√©m do nosso neg√≥cio principal, lan√ßamos recentemente projetos chamados Slory.  Estamos tentando levar nossa capacidade de trabalhar com os Kubernetes para as massas, para ensinar outras pessoas a trabalhar com os K8s tamb√©m. </p><br><p> Sobre o que vou falar hoje.  O t√≥pico do relat√≥rio √© √≥bvio - sobre a seguran√ßa do cluster Kubernetes.  Mas quero dizer imediatamente que esse t√≥pico √© muito grande - e, portanto, quero estipular imediatamente o que n√£o falarei com certeza.  N√£o falarei sobre termos hackeados que j√° s√£o cem vezes oprimidos na Internet.  Qualquer RBAC e certificados. </p><br><p>  Vou falar sobre como meus colegas e eu estamos cansados ‚Äã‚Äãde seguran√ßa no cluster Kubernetes.  Vemos esses problemas tanto com fornecedores que fornecem clusters Kubernetes quanto com clientes que v√™m at√© n√≥s.  E mesmo com clientes que chegam at√© n√≥s de outras empresas de consultoria em administra√ß√£o.  Ou seja, a escala da trag√©dia √© realmente muito grande. </p><br><p>  Literalmente tr√™s pontos, sobre os quais falarei hoje: </p><br><ol><li>  Direitos do usu√°rio versus direitos do pod.  Direitos do usu√°rio e direitos da lareira n√£o s√£o a mesma coisa. </li><li>  Coleta de informa√ß√µes de cluster.  Mostrarei que no cluster voc√™ pode coletar todas as informa√ß√µes necess√°rias sem ter direitos especiais nesse cluster. </li><li>  Ataque de nega√ß√£o de servi√ßo no cluster.  Se n√£o podemos coletar informa√ß√µes, podemos colocar o cluster em qualquer caso.  Vou falar sobre ataques de DoS em controles de cluster. </li></ol><br><p>  Outra coisa comum que mencionarei √© onde testei tudo e posso dizer com certeza que tudo funciona. </p><br><p>  Como base, tomamos a instala√ß√£o de um cluster Kubernetes usando o Kubespray.  Se algu√©m n√£o sabe, esse √© realmente um conjunto de pap√©is para o Ansible.  Estamos constantemente usando-o em nosso trabalho.  O bom √© que voc√™ pode rolar em qualquer lugar - nas gl√¢ndulas e em algum lugar na nuvem.  Um m√©todo de instala√ß√£o √© adequado em princ√≠pio para tudo. </p><br><p>  Neste cluster, terei o Kubernetes v1.14.5.  Todo o cluster de Cuba, que consideraremos, √© dividido em namespaces, cada namespace pertence a uma equipe separada e os membros dessa equipe t√™m acesso a cada namespace.  Eles n√£o podem ir para namespaces diferentes, apenas para os seus pr√≥prios.  Mas h√° uma conta de administrador que possui direitos para todo o cluster. </p><br><p><img src="https://habrastorage.org/webt/xm/rj/rx/xmrjrxw6toktjj1ukm-tir_remm.jpeg"></p><br><p>  Prometi que a primeira coisa que teremos √© obter direitos de administrador no cluster.  Precisamos de um pod especialmente preparado que quebre o cluster Kubernetes.  Tudo o que precisamos fazer √© aplic√°-lo ao cluster Kubernetes. </p><br><pre><code class="plaintext hljs">kubectl apply -f pod.yaml</code> </pre> <br><p>  Este pod chegar√° a um dos mestres do cluster Kubernetes.  E depois disso, o cluster retornar√° felizmente um arquivo chamado admin.conf para n√≥s.  Em Cuba, todos os certificados de administrador s√£o armazenados nesse arquivo e, ao mesmo tempo, a API do cluster est√° configurada.  Acho que √© assim que voc√™ pode obter acesso de administrador a 98% dos clusters do Kubernetes. </p><br><p>  Repito, esse pod foi feito por um desenvolvedor em seu cluster que tem acesso para implantar suas propostas em um pequeno espa√ßo para nome; ele √© todo preso pelo RBAC.  Ele n√£o tinha direitos.  No entanto, o certificado retornou. </p><br><p>  E agora sobre a lareira especialmente preparada.  Execute em qualquer imagem.  Por exemplo, considere debian: jessie. </p><br><p>  Temos uma coisa dessas: </p><br><pre> <code class="plaintext hljs">tolerations: - effect: NoSchedule operator: Exists nodeSelector: node-role.kubernetes.io/master: ""</code> </pre> <br><p>  O que √© toler√¢ncia?  Os mestres no cluster Kubernetes geralmente s√£o marcados com uma coisa chamada contamina√ß√£o ("infec√ß√£o" em ingl√™s).  E a ess√™ncia dessa "infec√ß√£o" - ela diz que os pods n√£o podem ser atribu√≠dos aos n√≥s principais.  Mas ningu√©m se preocupa em indicar de qualquer maneira que ele √© tolerante com a "infec√ß√£o".  A se√ß√£o Toleration apenas diz que se o NoSchedule estiver em algum n√≥, a infec√ß√£o por essa infec√ß√£o ser√° tolerante - e sem problemas. </p><br><p>  Al√©m disso, dizemos que nosso baixo n√£o √© apenas tolerante, mas tamb√©m quer cair especificamente sobre o mestre.  Porque os mestres s√£o os mais deliciosos que precisamos - todos os certificados.  Portanto, dizemos nodeSelector - e temos um r√≥tulo padr√£o nos assistentes, o que nos permite selecionar exatamente os n√≥s que s√£o assistentes de todos os n√≥s do cluster. </p><br><p>  Com essas duas se√ß√µes, ele definitivamente chegar√° ao mestre.  E ele poder√° morar l√°. </p><br><p>  Mas apenas vir ao mestre n√£o √© suficiente para n√≥s.  N√£o vai nos dar nada.  Portanto, ainda temos essas duas coisas: </p><br><pre> <code class="plaintext hljs">hostNetwork: true hostPID: true</code> </pre> <br><p>  Indicamos que nosso under, que estamos iniciando, viver√° no namespace do kernel, no namespace da rede e no namespace PID.  Assim que for iniciado no assistente, ele poder√° ver todas as interfaces reais e ativas desse n√≥, ouvir todo o tr√°fego e ver o PID de todos os processos. </p><br><p>  Em seguida, √© pequeno.  Pegue o etcd e leia o que voc√™ deseja. </p><br><p>  O mais interessante √© esse recurso do Kubernetes, que est√° presente l√° por padr√£o. </p><br><pre> <code class="plaintext hljs">volumeMounts: - mountPath: /host name: host volumes: - hostPath: path: / type: Directory name: host</code> </pre> <br><p>  E sua ess√™ncia √© que podemos dizer que queremos criar um volume do tipo hostPath no pod que executamos, mesmo sem os direitos para esse cluster.  Significa seguir o caminho do host no qual iniciaremos - e tom√°-lo como volume.  E depois chame-o de nome: host.  Todo esse hostPath que montamos dentro da lareira.  Neste exemplo, para o diret√≥rio / host. </p><br><p>  Repito mais uma vez.  Dissemos ao pod para chegar ao mestre, obter hostNetwork e hostPID l√° - e montar toda a raiz do mestre dentro desse pod. </p><br><p>  Voc√™ entende que no debian temos o bash rodando, e esse bash funciona sob a nossa raiz.  Ou seja, acabamos de obter a raiz do mestre, embora n√£o tenhamos nenhum direito no cluster Kubernetes. </p><br><p>  A tarefa toda √© entrar no subdiret√≥rio / host / etc / kubernetes / pki; se n√£o me engano, pegue todos os certificados mestres do cluster e, consequentemente, torne-se o administrador do cluster. </p><br><p>  Se voc√™ olhar dessa maneira, esses s√£o alguns dos direitos mais perigosos dos pods, apesar dos direitos do usu√°rio: <br><img src="https://habrastorage.org/webt/ax/07/cj/ax07cjhm7y0dwpueikrj-qwqv2k.jpeg"></p><br><p>  Se eu tiver direitos para executar em algum espa√ßo para nome do cluster, esse sub possui esses direitos por padr√£o.  Eu posso executar pods privilegiados, e isso geralmente √© todos os direitos, praticamente root no n√≥. </p><br><p>  O meu favorito √© usu√°rio root.  E o Kubernetes tem essa op√ß√£o Executar como N√£o-Raiz.  Este √© um tipo de prote√ß√£o contra hackers.  Voc√™ sabe o que √© o "v√≠rus da Mold√°via"?  Se voc√™ √© um hacker e vem ao meu cluster do Kubernetes, n√≥s, administradores pobres, perguntamos: ‚ÄúPor favor, indique nos seus pods com os quais voc√™ invadir√° meu cluster, execute como n√£o raiz.  E acontece que voc√™ inicia o processo em seu cora√ß√£o sob a raiz, e ser√° muito f√°cil para voc√™ me invadir.  Por favor, proteja-se de si mesmo. </p><br><p>  Volume do caminho do host - na minha opini√£o, a maneira mais r√°pida de obter o resultado desejado do cluster Kubernetes. </p><br><p>  Mas o que fazer com tudo isso? </p><br><p>  Pensamentos que devem chegar a qualquer administrador normal que encontre o Kubernetes: ‚ÄúSim, eu lhe disse, o Kubernetes n√£o funciona.  Existem buracos nele.  E todo o cubo √© besteira. "  De fato, existe documenta√ß√£o e, se voc√™ procurar l√°, h√° uma se√ß√£o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pol√≠tica de Seguran√ßa do Pod</a> . </p><br><p>  Este √© um objeto yaml - podemos cri√°-lo no cluster Kubernetes - que controla os aspectos de seguran√ßa na descri√ß√£o dos lares.  Na verdade, ele controla esses direitos para usar todos os tipos de hostNetwork, hostPID, certos tipos de volume, que est√£o nos pods na inicializa√ß√£o.  Com a Pol√≠tica de Seguran√ßa do Pod, tudo isso pode ser descrito. </p><br><p>  O mais interessante na Pol√≠tica de Seguran√ßa do Pod √© que, no cluster Kubernetes, todos os instaladores do PSP simplesmente n√£o s√£o descritos de forma alguma, eles s√£o simplesmente desativados por padr√£o.  A Pol√≠tica de Seguran√ßa do Pod √© ativada usando o plug-in de admiss√£o. </p><br><p>  Ok, vamos terminar com um cluster da Pol√≠tica de Seguran√ßa do Pod, digamos que temos algum tipo de pod de servi√ßo no espa√ßo para nome, ao qual apenas os administradores t√™m acesso.  Digamos que em todos os outros pods eles t√™m direitos limitados.  Como provavelmente, os desenvolvedores n√£o precisam executar pods privilegiados no seu cluster. </p><br><p>  E tudo parece estar bem conosco.  E nosso cluster Kubernetes n√£o pode ser invadido em dois minutos. </p><br><p>  H√° um problema.  Provavelmente, se voc√™ tiver um cluster Kubernetes, o monitoramento ser√° instalado no seu cluster.  Eu at√© presumo prever que, se houver monitoramento no seu cluster, ele ser√° chamado de Prometheus. </p><br><p>  O que vou lhe dizer agora ser√° v√°lido tanto para o operador Prometheus quanto para o Prometheus entregue em sua forma pura.  A quest√£o √© que, se n√£o consigo acessar o administrador t√£o rapidamente no cluster, significa que preciso procurar mais.  E eu posso pesquisar usando seu monitoramento. </p><br><p>  Provavelmente, todo mundo l√™ os mesmos artigos sobre Habr√©, e o monitoramento est√° em monitoramento.  O gr√°fico de leme √© chamado aproximadamente o mesmo para todos.  Presumo que, se voc√™ instalar o stable / prometheus, obter√° aproximadamente os mesmos nomes.  E ainda mais provavelmente n√£o precisarei adivinhar o nome DNS no seu cluster.  Porque √© padr√£o. </p><br><p><img src="https://habrastorage.org/webt/o6/rv/fw/o6rvfw0idykw0wyxivpfmwkd_vy.jpeg"></p><br><p>  Al√©m disso, temos alguns dev ns, nele √© poss√≠vel lan√ßar um certo abaixo.  Al√©m disso, √© muito f√°cil fazer assim: </p><br><pre> <code class="plaintext hljs">$ curl http://prometheus-kube-state-metrics.monitoring</code> </pre> <br><p>  prometheus-kube-state-metrics √© um dos exportadores de prometheus que coleta m√©tricas da API do Kubernetes.  H√° muitos dados em execu√ß√£o no cluster, quais s√£o, quais problemas voc√™ tem com ele. </p><br><p>  Como um exemplo simples: </p><br><p>  kube_pod_container_info {namespace = "kube-system", pod = "kube-apiserver-k8s-1", container = "kube-apiserver", imagem = </p><br><p>  <strong>"gcr.io/google-containers/kube-apiserver:v1.14.5"</strong> </p><br><p>  , Image_id = "docker-pullable: //gcr.io/google-containers/kube- apiserver @ sha256: e29561119a52adad9edc72bfe0e7fcab308501313b09bf99df4a96 38ee634989", container_id = "janela de encaixe: // 7cbe7b1fea33f811fdd8f7e0e079191110268f2 853397d7daf08e72c22d3cf8b"} 1 </p><br><p>  Depois de fazer uma solicita√ß√£o de curvatura simples a partir de um arquivo n√£o privilegiado, voc√™ pode obter essas informa√ß√µes.  Se voc√™ n√£o souber em qual vers√£o do Kubernetes est√° executando, ser√° f√°cil informar. </p><br><p>  E o mais interessante √© que, al√©m do fato de voc√™ recorrer √†s m√©tricas de estado de cubo, tamb√©m pode se aplicar diretamente ao pr√≥prio Prometheus.  Voc√™ pode coletar m√©tricas a partir da√≠.  Voc√™ pode at√© criar m√©tricas a partir da√≠.  Mesmo teoricamente, voc√™ pode criar essa solicita√ß√£o a partir de um cluster no Prometheus, que simplesmente a desativa.  E seu monitoramento geralmente deixa de funcionar no cluster. </p><br><p>  E aqui surge a quest√£o de saber se algum monitoramento externo monitora seu monitoramento.  Acabei de ter a oportunidade de atuar no cluster Kubernetes sem nenhuma consequ√™ncia para mim.  Voc√™ nem sabe que estou atuando l√°, j√° que n√£o h√° mais monitoramento. </p><br><p>  Assim como no PSP, parece que o problema √© que todas essas tecnologias da moda - Kubernetes, Prometheus - elas simplesmente n√£o funcionam e est√£o cheias de buracos.  Na verdade n√£o. </p><br><p>  Existe uma coisa - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Pol√≠tica de Rede</a> . </p><br><p>  Se voc√™ √© um administrador normal, provavelmente na Pol√≠tica de Rede sabe que esse √© outro yaml, que no cluster j√° est√° em execu√ß√£o.  E algumas pol√≠ticas de rede definitivamente n√£o s√£o necess√°rias.  E mesmo que voc√™ leia o que √© a Pol√≠tica de Rede, qual √© o yaml-firewall do Kubernetes, ele permite restringir os direitos de acesso entre os espa√ßos para nome, entre os pods, e certamente voc√™ decidiu que o firewall do tipo yaml no Kubernetes est√° nas pr√≥ximas abstra√ß√µes ... N√£o, n√£o .  Definitivamente, isso n√£o √© necess√°rio. </p><br><p>  Mesmo que seus especialistas em seguran√ßa n√£o tenham informado de que, usando o Kubernetes, voc√™ pode criar um firewall com muita facilidade e simplicidade, e √© muito granular.  Se eles ainda n√£o sabem disso e n√£o o puxam: "Bem, d√™, d√™ ..." Em qualquer caso, voc√™ precisar√° da Pol√≠tica de Rede para bloquear o acesso a alguns locais de servi√ßo que voc√™ pode extrair do cluster sem nenhuma autoriza√ß√£o. </p><br><p>  Como no exemplo que citei, voc√™ pode extrair as m√©tricas do estado do kube de qualquer espa√ßo para nome no cluster Kubernetes sem ter nenhum direito.  As pol√≠ticas de rede fecharam o acesso de todos os outros namespaces ao monitoramento do namespace e, por assim dizer, tudo: sem acesso, sem problemas.  Em todos os gr√°ficos existentes, tanto o prometeus padr√£o quanto o prometeus que est√° no operador, simplesmente nos valores do leme h√° uma op√ß√£o para simplesmente ativar as pol√≠ticas de rede para eles.  Voc√™ s√≥ precisa lig√°-lo e eles funcionar√£o. </p><br><p>  H√° realmente um problema aqui.  Sendo um administrador barbudo normal, voc√™ provavelmente decidiu que n√£o s√£o necess√°rias pol√≠ticas de rede.  E depois de ler todos os tipos de artigos sobre recursos como Habr, voc√™ decidiu que a flanela, especialmente com o modo host-gateway, √© a melhor coisa que voc√™ pode escolher. </p><br><p>  O que fazer </p><br><p>  Voc√™ pode tentar reimplementar a solu√ß√£o de rede que est√° no seu cluster Kubernetes, substitu√≠-la por algo mais funcional.  Na mesma chita, por exemplo.  Mas imediatamente quero dizer que a tarefa de alterar a solu√ß√£o de rede no cluster de trabalho do Kubernetes n√£o √© trivial.  Eu o resolvi duas vezes (no entanto, teoricamente, ambas as vezes), mas at√© mostramos como fazer isso nos Slurms.  Para nossos alunos, mostramos como alterar a solu√ß√£o de rede no cluster Kubernetes.  Em princ√≠pio, voc√™ pode tentar garantir que n√£o haja tempo de inatividade no cluster de produ√ß√£o.  Mas voc√™ provavelmente n√£o ter√° sucesso. </p><br><p>  E o problema √© realmente resolvido de maneira muito simples.  Existem certificados no cluster e voc√™ sabe que seus certificados ficar√£o ruins em um ano.  Bem, e geralmente uma solu√ß√£o normal com certificados no cluster - por que vamos usar o vapor, criaremos um novo cluster ao lado, deixaremos apodrecer no antigo e refazeremos tudo.  √â verdade que, quando tudo der errado, tudo se acalmar√° em nossos dias, mas depois um novo cluster. </p><br><p>  Quando voc√™ criar um novo cluster, ao mesmo tempo, insira Calico em vez de flanela. </p><br><p>  O que fazer se voc√™ possui certificados emitidos por cem anos e n√£o deseja agrupar novamente o cluster?  Existe uma coisa chamada Kube-RBAC-Proxy.  Este √© um desenvolvimento muito interessante, pois permite incorporar-se como um cont√™iner lateral a qualquer lareira no cluster Kubernetes.  E ela realmente adiciona autoriza√ß√£o atrav√©s do Kubernetes RBAC a este pod. </p><br><p>  H√° um problema.  Anteriormente, o Kube-RBAC-Proxy era incorporado ao prometheus do operador.  Mas ent√£o ele se foi.  Agora, as vers√µes modernas dependem do fato de voc√™ ter uma diretiva de rede e fechar usando-as.  E ent√£o voc√™ tem que reescrever um pouco o gr√°fico.  De fato, se voc√™ for a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">este reposit√≥rio</a> , h√° exemplos de como us√°-lo como sidecars e precisar√° reescrever os gr√°ficos minimamente. </p><br><p>  H√° outro pequeno problema.  N√£o apenas Prometheus fornece suas m√©tricas para quem as recebe.  Tamb√©m temos todos os componentes do cluster Kubernetes, eles podem fornecer suas m√©tricas. </p><br><p>  Mas como eu disse, se voc√™ n√£o conseguir acessar o cluster e coletar informa√ß√µes, poder√° pelo menos causar algum dano. </p><br><p>  Ent√£o, mostrarei rapidamente duas maneiras de estragar o cluster do Kubernetes. </p><br><p>  Voc√™ vai rir quando eu lhe contar, esses s√£o dois casos da vida real. </p><br><p>  O primeiro caminho.  Ficar sem recursos. </p><br><p>  Lan√ßamos mais um especial.  Ele ter√° essa se√ß√£o. </p><br><pre> <code class="plaintext hljs">resources: requests: cpu: 4 memory: 4Gi</code> </pre> <br><p>  Como voc√™ sabe, as solicita√ß√µes s√£o a quantidade de CPU e mem√≥ria reservada no host para pods espec√≠ficos com solicita√ß√µes.  Se tivermos um host de quatro n√∫cleos no cluster Kubernetes e quatro CPUs chegarem com solicita√ß√µes, significa que n√£o haver√° mais pods com solicita√ß√µes para esse host. </p><br><p>  Se eu executar isso em baixo, vou fazer um comando: </p><br><pre> <code class="plaintext hljs">$ kubectl scale special-pod --replicas=...</code> </pre> <br><p>  Ent√£o ningu√©m mais poder√° implantar no cluster Kubernetes.  Porque em todos os n√≥s as solicita√ß√µes terminar√£o.  E assim eu paro seu cluster Kubernetes.  Se fizer isso √† noite, posso parar a implanta√ß√£o por algum tempo. </p><br><p>  Se olharmos novamente para a documenta√ß√£o do Kubernetes, veremos uma coisa chamada Limit Range.  Ele define recursos para objetos de cluster.  Voc√™ pode escrever um objeto Limit Range no yaml, aplic√°-lo a determinados namespaces - e ainda mais nesse namespace, voc√™ pode dizer que possui os recursos para os pods padr√£o, m√°ximo e m√≠nimo. </p><br><p>  Com a ajuda de uma coisa dessas, podemos limitar os usu√°rios em espa√ßos de nomes de produtos espec√≠ficos de equipes na capacidade de indicar qualquer coisa desagrad√°vel em seus pods.  Mas, infelizmente, mesmo se voc√™ disser ao usu√°rio que √© imposs√≠vel executar pods com solicita√ß√µes de mais de uma CPU, existe um comando de escala maravilhoso, bem, ou atrav√©s do painel eles podem escalar. </p><br><p>  E daqui vem o m√©todo n√∫mero dois.  Lan√ßamos 11 111 111 111 111 lares.  S√£o onze bilh√µes.  N√£o √© porque eu criei esse n√∫mero, mas porque eu mesmo o vi. </p><br><p>  A verdadeira hist√≥ria.  No final da noite, eu estava prestes a sair do escrit√≥rio.  Eu vejo, um grupo de desenvolvedores est√° sentado no canto e fazendo algo freneticamente com laptops.  Vou at√© os caras e pergunto: "O que aconteceu com voc√™?" </p><br><p>  Um pouco antes, √†s nove da noite, um dos desenvolvedores estava indo para casa.  E ele decidiu: "Vou pular minha inscri√ß√£o at√© uma agora".  Eu cliquei um pouco, e a Internet um pouco chata.  Ele mais uma vez clicou na unidade, pressionou a unidade, clicou em Enter.  Brincou com tudo o que p√¥de.  Ent√£o a Internet ganhou vida - e tudo come√ßou a crescer at√© essa data. </p><br><p>  √â verdade que essa hist√≥ria n√£o ocorreu em Kubernetes, na √©poca era n√¥made.  Terminou com o fato de que, depois de uma hora de nossas tentativas de impedir Nomad de tentativas obstinadas de permanecer juntos, Nomad respondeu que ele n√£o parava de grudar e que n√£o fazia mais nada.  "Estou cansado, estou indo embora."  E enrolado. </p><br><p>  Naturalmente, tentei fazer o mesmo no Kubernetes.  Os onze bilh√µes de pods de Kubernetes n√£o ficaram satisfeitos, ele disse: "N√£o posso.  Excede os protetores bucais internos. "  Mas 1.000.000.000 de lares podiam. </p><br><p>  Em resposta a um bilh√£o, o Cubo n√£o entrou.  Ele realmente come√ßou a escalar.  Quanto mais o processo passava, mais tempo levava para ele criar novos lares.  Mas ainda assim o processo continuou.  O √∫nico problema √© que, se eu puder executar pods indefinidamente no meu espa√ßo para nome, mesmo sem solicita√ß√µes e limites, posso iniciar um n√∫mero t√£o grande de pods com algumas tarefas que, com essas tarefas, os n√≥s come√ßar√£o a adicionar da mem√≥ria a partir da CPU.  Quando eu corro tantos lares, as informa√ß√µes deles devem ir para o reposit√≥rio, ou seja, etcd.  E quando muita informa√ß√£o chega l√°, o armaz√©m come√ßa a ceder muito lentamente - e em Kubernetes, as coisas mon√≥tonas come√ßam. </p><br><p>  E mais um problema ... Como voc√™ sabe, os elementos de controle do Kubernetes n√£o s√£o apenas uma coisa central, mas v√°rios componentes.  L√°, em particular, h√° um gerenciador de controladores, agendador e assim por diante.  Todos esses caras come√ßar√£o a fazer um trabalho est√∫pido desnecess√°rio ao mesmo tempo, o que com o tempo come√ßar√° a demorar cada vez mais.  O gerente do controlador criar√° novos pods.  O Agendador tentar√° encontrar um novo n√≥ para eles.  √â prov√°vel que novos n√≥s no seu cluster terminem em breve.  O cluster Kubernetes come√ßar√° a trabalhar mais devagar e lentamente. </p><br><p>  Mas eu decidi ir ainda mais longe.  Como voc√™ sabe, no Kubernetes existe uma coisa chamada servi√ßo.  Bem, e por padr√£o em seus clusters, provavelmente, o servi√ßo funciona usando tabelas IP. </p><br><p>  Se voc√™ administra um bilh√£o de lares, por exemplo, e usa o script para for√ßar o Kubernetis a criar novos servi√ßos: </p><br><pre> <code class="plaintext hljs">for i in {1..1111111}; do kubectl expose deployment test --port 80 \ --overrides="{\"apiVersion\": \"v1\", \"metadata\": {\"name\": \"nginx$i\"}}"; done</code> </pre> <br><p>  Em todos os n√≥s do cluster, aproximadamente novas regras do iptables ser√£o geradas aproximadamente simultaneamente.  Al√©m disso, para cada servi√ßo, um bilh√£o de regras de tabelas de ip ser√° gerado. </p><br><p>  Eu verifiquei tudo isso em v√°rios milhares, at√© uma d√∫zia.  E o problema √© que j√° nesse limite o ssh no n√≥ √© bastante problem√°tico.  Como os pacotes, passando por um n√∫mero t√£o grande de cadeias, come√ßam a n√£o se sentir muito bem. </p><br><p>  E tudo isso tamb√©m √© resolvido com a ajuda do Kubernetes.  Existe um objeto de cota de recurso.  Define o n√∫mero de recursos e objetos dispon√≠veis para um espa√ßo para nome em um cluster.  Podemos criar um objeto yaml em cada espa√ßo para nome do cluster Kubernetes.  Usando esse objeto, podemos dizer que alocamos um certo n√∫mero de solicita√ß√µes, limites para esse espa√ßo para nome e, em seguida, podemos dizer que √© poss√≠vel criar 10 servi√ßos e 10 pods nesse espa√ßo para nome.  E um √∫nico desenvolvedor pode pelo menos se espremer √† noite.  Kubernetes dir√° a ele: "Voc√™ n√£o pode aumentar seus pods a tal quantidade porque excede a cota de recursos".  Tudo, o problema est√° resolvido.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">A documenta√ß√£o est√° aqui</a> . </p><br><p>  Um ponto problem√°tico surge em conex√£o com isso.  Voc√™ sente como √© dif√≠cil criar um espa√ßo para nome no Kubernetes.  Para cri√°-lo, precisamos considerar v√°rias coisas. </p><br><p>  Cota de recursos + Intervalo de limite + RBAC <br>  ‚Ä¢ Crie um espa√ßo para nome <br>  ‚Ä¢ Criar limite interno <br>  ‚Ä¢ Criar dentro da quota de recursos <br>  ‚Ä¢ Crie uma conta de servi√ßo para o IC <br>  ‚Ä¢ Crie uma associa√ß√£o de rolagem para IC e usu√°rios <br>  ‚Ä¢ Opcionalmente, execute os pods de servi√ßo necess√°rios </p><br><p>  Portanto, aproveitando esta oportunidade, gostaria de compartilhar meus desenvolvimentos.  Existe uma coisa chamada operador de SDK.  Esta √© uma maneira no cluster Kubernetes de escrever operadores para ele.  Voc√™ pode escrever instru√ß√µes usando Ansible. </p><br><p>  Primeiro, ele foi escrito em Ansible e, em seguida, verifiquei que havia um operador SDK e reescrevi a fun√ß√£o Ansible no operador.  Este operador permite criar um objeto no cluster Kubernetes chamado equipe.       yaml    .       ,    - . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">    </a> . </p><br><p>   .     ? <br> . Pod Security Policy ‚Äî  .     ,            , -      . </p><br><p> Network Policy ‚Äî   -    .  ,     . </p><br><p> LimitRange/ResourceQuota ‚Äî   .     ,     ,     . ,   . </p><br><p>  ,      ,   ,    .   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">   </a> . </p><br><p>      .  ,           warlocks ,   . </p><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a>  ,   ,   .      ,  ResourceQuota, Pod Security Policy .     . </p><br><p>  . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt472484/">https://habr.com/ru/post/pt472484/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt472468/index.html">ClusterJ - trabalhando com o MySQL NDB Cluster de Java</a></li>
<li><a href="../pt472470/index.html">Camundongos transg√™nicos e antienvelhecimento</a></li>
<li><a href="../pt472472/index.html">Casa de campo no inverno: ser ou n√£o ser?</a></li>
<li><a href="../pt472474/index.html">Erro cosm√©tico engra√ßado no Google Chrome</a></li>
<li><a href="../pt472482/index.html">Acidente radioativo: descoberta de uma fase s√≥lida e est√°vel do plut√¥nio</a></li>
<li><a href="../pt472486/index.html">Armazenamento de dados a longo prazo. (Artigo - discuss√£o)</a></li>
<li><a href="../pt472488/index.html">Trinta relat√≥rios do DevOops 2019: Tim Lister, Hadi Hariri, Roman Shaposhnik e outras estrelas do DevOps internacional</a></li>
<li><a href="../pt472490/index.html">Como pesquisei um padr√£o de beleza usando o Processamento de linguagem natural (e n√£o o encontrei)</a></li>
<li><a href="../pt472492/index.html">Analisando o C√≥digo de ROOT, Estrutura de An√°lise de Dados Cient√≠ficos</a></li>
<li><a href="../pt472494/index.html">An√°lise de c√≥digo ROOT - estrutura de an√°lise de dados de pesquisa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>