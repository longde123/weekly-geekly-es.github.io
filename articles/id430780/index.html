<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏿‍⚕️ 👫 🌋 Sequence-to-Sequence Bagian 1 Model 🚈 🏼 🤥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hari baik untuk semua! 

 Dan kami kembali membuka aliran baru ke kursus Data Scientist yang direvisi: guru luar biasa lainnya, program yang sedikit d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sequence-to-Sequence Bagian 1 Model</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/430780/">  Hari baik untuk semua! <br><br>  Dan kami kembali membuka aliran baru ke kursus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Data Scientist yang</a> direvisi: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">guru luar biasa</a> lainnya, program yang sedikit disempurnakan berdasarkan pembaruan.  Nah, seperti biasa, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pelajaran terbuka</a> menarik dan koleksi bahan-bahan menarik.  Hari ini kita akan memulai analisis model seq2seq dari Tensor Flow. <br><br>  Ayo pergi. <br><br>  Seperti yang sudah dibahas dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tutorial RNN</a> (kami sarankan Anda membiasakan diri dengannya sebelum membaca artikel ini), jaringan saraf berulang dapat diajarkan untuk memodelkan bahasa.  Dan muncul pertanyaan yang menarik: apakah mungkin untuk melatih jaringan pada data tertentu untuk menghasilkan jawaban yang bermakna?  Misalnya, dapatkah kita mengajarkan jaringan saraf untuk menerjemahkan dari Bahasa Inggris ke Bahasa Prancis?  Ternyata kita bisa. <br><br>  Panduan ini akan menunjukkan cara membuat dan melatih sistem end-to-end.  Salin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori inti aliran Tensor</a> dan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">repositori model TensorFlow dari GitHub</a> .  Kemudian, Anda dapat memulai dengan memulai program terjemahan: <br><br><pre><code class="python hljs">cd models/tutorials/rnn/translate python translate.py --data_dir [your_data_directory]</code> </pre> <br><img src="https://habrastorage.org/webt/ra/j0/rr/raj0rraitsp6itojzydkhrk2yoi.png"><a name="habracut"></a><br><br>  Dia akan mengunduh data untuk diterjemahkan dari bahasa Inggris ke bahasa Prancis dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">situs web WMT'15</a> , mempersiapkannya untuk pelatihan dan pelatihan.  Ini akan membutuhkan sekitar 20GB pada hard drive dan cukup banyak waktu untuk mengunduh dan menyiapkan, sehingga Anda dapat memulai proses sekarang dan melanjutkan membaca tutorial ini. <br><br>  Manual akan mengakses file-file berikut: <br><br><table><tbody><tr><th>  File </th><th>  Apa isinya? </th></tr><tr><td>  tensorflow / tensorflow / python / ops / seq2seq.py </td><td>  Perpustakaan untuk membuat model urutan-ke-urutan </td></tr><tr><td>  model / tutorial / rnn / terjemahkan / seq2seq_model.py </td><td>  Model terjemahan neural urutan-ke-urutan </td></tr><tr><td>  models / tutorials / rnn / translate / data_utils.py </td><td>  Fungsi pembantu untuk menyiapkan data terjemahan </td></tr><tr><td>  model / tutorial / rnn / terjemahkan / translate.py </td><td>  Biner yang melatih dan menjalankan model terjemahan </td></tr></tbody></table><br>  <b>Dasar-dasar urutan-ke-urutan</b> <br><br>  Model urutan-ke-urutan dasar, seperti yang disajikan oleh <a href="">Cho et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pdf</a> ), terdiri dari dua jaringan saraf berulang (RNNs): enkoder (enkoder) yang memproses input data dan decoder (decoder) yang menghasilkan data keluaran.  Arsitektur dasar ditunjukkan di bawah ini: <br><br><img src="https://habrastorage.org/webt/e-/df/cu/e-dfcuvlsbykvyxvzac9rc0nrow.png"><br><br>  Setiap persegi panjang pada gambar di atas mewakili sel dalam RNN, biasanya sel GRU - blok pengulangan terkontrol, atau sel LSTM - memori jangka pendek jangka panjang (baca <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">tutorial RNN</a> untuk mempelajari lebih lanjut tentang mereka).  Encoder dan decoder dapat memiliki bobot yang sama atau, lebih sering, menggunakan set parameter yang berbeda.  Sel multilayer telah berhasil digunakan dalam model urutan-ke-urutan, misalnya, untuk menerjemahkan <a href="">Sutskever et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pdf</a> ). <br><br>  Dalam model dasar yang dijelaskan di atas, setiap input harus dikodekan ke dalam vektor keadaan ukuran tetap, karena ini adalah satu-satunya hal yang ditransmisikan ke dekoder.  Untuk memberikan decoder lebih banyak akses langsung ke input data, mekanisme perhatian diperkenalkan di <a href="">Bahdanau et al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pdf</a> ).  Kami tidak akan merinci mekanisme perhatian (untuk ini Anda dapat membiasakan diri dengan pekerjaan di sini);  Cukuplah untuk mengatakan bahwa itu memungkinkan decoder untuk melihat data input pada setiap langkah decoding.  Jaringan urutan-ke-urutan multilayer dengan sel LSTM dan mekanisme perhatian dalam dekoder adalah sebagai berikut: <br><br><img src="https://habrastorage.org/webt/c4/ro/0z/c4ro0zvzu8m-y4qjlnfbhv9x4qa.png"><br><br>  <b>Pustaka TensorFlow seq2seq</b> <br><br>  Seperti yang Anda lihat di atas, ada berbagai model urutan-ke-urutan.  Semuanya dapat menggunakan sel RNN yang berbeda, tetapi semuanya menerima data input encoder dan data input decoder.  Ini adalah dasar dari antarmuka perpustakaan TensorFlow seq2seq (tensorflow / tensorflow / python / ops / seq2seq.py).  Model dasar, RNN, codec, urutan-ke-urutan ini berfungsi sebagai berikut. <br><br><pre> <code class="python hljs">outputs, states = basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)</code> </pre> <br>  Dalam panggilan di atas, <code>encoder_inputs</code> adalah daftar tensor yang mewakili data input encoder, sesuai dengan huruf A, B, C dari gambar di atas.  Demikian pula, <code>decoder_inputs</code> adalah tensor yang mewakili data input decoder.  GO, W, X, Y, Z dari gambar pertama. <br><br>  Argumen <code>cell</code> adalah turunan dari kelas <code>tf.contrib.rnn.RNNCell</code> , yang menentukan sel mana yang akan digunakan dalam model.  Anda dapat menggunakan sel yang ada, misalnya, <code>GRUCell</code> atau <code>LSTMCell</code> , atau Anda dapat menulis sendiri.  Selain itu, <code>tf.contrib.rnn</code> menyediakan shell untuk membuat sel multilayer, menambahkan pengecualian pada input dan output sel, atau transformasi lainnya.  Lihatlah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tutorial RNN</a> sebagai contoh. <br><br>  Panggilan <code>basic_rnn_seq2seq</code> mengembalikan dua argumen: <code>outputs</code> dan <code>states</code> .  Keduanya mewakili daftar tensor dengan panjang yang sama dengan <code>decoder_inputs</code> .  <code>outputs</code> sesuai dengan data output decoder pada setiap langkah waktu, pada gambar pertama adalah W, X, Y, Z, EOS.  Status yang dikembalikan mewakili keadaan internal decoder pada setiap langkah waktu. <br><br>  Dalam banyak aplikasi menggunakan model urutan-ke-urutan, output decoder pada waktu t ditransmisikan kembali ke input ke decoder pada waktu t +1.  Selama pengujian, selama decoding urutan, ini adalah bagaimana yang baru dibangun.  Di sisi lain, selama pelatihan biasanya mengirimkan ke decoder data input yang benar pada setiap langkah waktu, bahkan jika decoder sebelumnya salah.  Fungsi di <code>seq2seq.py</code> mendukung kedua mode dengan argumen <code>feed_previous</code> .  Misalnya, pertimbangkan penggunaan model RNN bersarang berikut ini. <br><br><pre> <code class="python hljs">outputs, states = embedding_rnn_seq2seq( encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, output_projection=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, feed_previous=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre> <br>  Dalam model <code>embedding_rnn_seq2seq</code> , semua data input (baik <code>encoder_inputs</code> dan <code>decoder_inputs</code> ) adalah tensor bilangan bulat yang mencerminkan nilai diskrit.  Mereka akan bersarang dalam representasi ketat (untuk detail pada lampiran, lihat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Panduan Tampilan Vektor</a> ), tetapi untuk membuat lampiran ini, Anda perlu menentukan jumlah maksimum karakter diskrit: <code>num_encoder_symbols</code> di sisi encoder dan <code>num_decoder_symbols</code> di sisi decoder. <br><br>  Dalam panggilan di atas, kami menetapkan <code>feed_previous</code> ke False.  Ini berarti bahwa decoder akan menggunakan tensor <code>decoder_inputs</code> dalam bentuk yang disediakan.  Jika kita menetapkan <code>feed_previous</code> ke True, decoder hanya akan menggunakan elemen <code>decoder_inputs</code> pertama.  Semua tensor lain dari daftar akan diabaikan, dan nilai keluaran dekoder sebelumnya akan digunakan sebagai gantinya.  Ini digunakan untuk memecahkan kode terjemahan dalam model terjemahan kami, tetapi juga dapat digunakan selama pelatihan, untuk meningkatkan stabilitas model terhadap kesalahannya.  Kira-kira seperti dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bengio et al., 2015</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pdf</a> ). <br><br>  Argumen penting lain yang digunakan di atas adalah <code>output_projection</code> .  Tanpa klarifikasi, kesimpulan dari model tertanam akan menjadi tensor dari jumlah sampel pelatihan per <code>num_decoder_symbols</code> , karena mereka mewakili logiths dari setiap simbol yang dihasilkan.  Saat melatih model dengan kamus keluaran besar, misalnya dengan <code>num_decoder_symbols</code> besar, menyimpan tensor besar ini menjadi tidak praktis.  Sebagai gantinya, lebih baik mengembalikan tensor yang lebih kecil, yang selanjutnya akan diproyeksikan ke tensor besar menggunakan <code>output_projection</code> .  Hal ini memungkinkan kami untuk menggunakan model seq2seq kami dengan kerugian softmax sampel, seperti yang dijelaskan oleh <a href="">Jean et.</a>  <a href="">al., 2014</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">pdf</a> ). <br><br>  Selain <code>basic_rnn_seq2seq</code> dan <code>embedding_rnn_seq2seq</code> , ada beberapa model urutan-ke-urutan lainnya di <code>seq2seq.py</code> .  Perhatikan mereka.  Semuanya memiliki antarmuka yang sama, jadi kami tidak akan menyelidiki detailnya.  Untuk model terjemahan kami di bawah ini, gunakan <code>embedding_attention_seq2seq</code> . <br><br>  Untuk dilanjutkan. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id430780/">https://habr.com/ru/post/id430780/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id430768/index.html">Wawancara dengan pencipta ADOM Thomas Biscap</a></li>
<li><a href="../id430770/index.html">Cadangkan untuk Linux, atau cara membuat snapshot</a></li>
<li><a href="../id430774/index.html">Apakah Anda siap untuk AI di papan reklame?</a></li>
<li><a href="../id430776/index.html">Membuat IP adalah satu-satunya cara</a></li>
<li><a href="../id430778/index.html">3DEXPERIENCE proses desain sistem kelistrikan end-to-end</a></li>
<li><a href="../id430782/index.html">Berapa banyak programmer yang Anda butuhkan untuk mendukung kode yang ditulis sebelumnya?</a></li>
<li><a href="../id430784/index.html">Dari junior menjadi sutradara: kisah satu penjaga</a></li>
<li><a href="../id430788/index.html">Riwayat wawancara saya di IB IT (pengembang Java, bank investasi) di London dengan contoh-contoh tugas yang khas</a></li>
<li><a href="../id430790/index.html">Ledger Nano S: kunci ruangan tempat 710 token dan cryptocurrency dapat berbohong</a></li>
<li><a href="../id430792/index.html">Membuat garis besar pada LWRP di Unity</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>