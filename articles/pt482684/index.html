<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèº‚Äçüé§ üçΩÔ∏è üë®üèø‚Äç‚úàÔ∏è Criei meu pr√≥prio dipfake em duas semanas e US $ 552 üíáüèº üêÆ üßù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ao criar este v√≠deo, eu aprendi muito 

 A tecnologia Dipfake usa redes neurais profundas para substituir de forma convincente uma pessoa por outra em...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Criei meu pr√≥prio dipfake em duas semanas e US $ 552</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/482684/"><h3>  Ao criar este v√≠deo, eu aprendi muito </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/N6y0CA2Mcx8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <a href="https://ru.wikipedia.org/wiki/Deepfake" rel="nofollow">A</a> tecnologia <a href="https://ru.wikipedia.org/wiki/Deepfake" rel="nofollow">Dipfake</a> usa <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D1%2583%25D0%25B1%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5" rel="nofollow">redes neurais profundas</a> para substituir de forma convincente uma pessoa por outra em v√≠deo.  Essa tecnologia tem um potencial √≥bvio para uso malicioso e est√° se tornando mais comum.  Quanto √†s conseq√º√™ncias sociais e pol√≠ticas dessa tend√™ncia, muitos bons artigos j√° foram escritos. <br><br>  E este n√£o √© um deles.  Em vez disso, examinarei mais de perto essa tecnologia: como o software diphey funciona?  Qu√£o dif√≠cil √© cri√°-los e qu√£o bons s√£o os resultados? <br><br>  Decidi que era melhor responder a essas perguntas criando meu pr√≥prio v√≠deo de mergulho.  Os editores me deram alguns dias para jogar com o software e US $ 1000 para pagar pela computa√ß√£o em nuvem.  Ap√≥s algumas semanas, obtive o resultado apresentado no v√≠deo no in√≠cio do artigo.  Comecei com um v√≠deo de Mark Zuckerberg discursando no Congresso e substitu√≠ o rosto dele por Tenente Comandante Data (Brent Spiner) de Star Trek: The Next Generation.  Um total de US $ 552 foi gasto. <br><a name="habracut"></a><br>  O v√≠deo n√£o foi perfeito.  Todos os detalhes do rosto dos Dados n√£o s√£o transmitidos e, se voc√™ observar com aten√ß√£o, os artefatos s√£o vis√≠veis nas bordas. <br><br>  Ainda assim, √© not√°vel que um rec√©m-chegado como eu possa criar um v√≠deo convincente, de forma r√°pida e barata.  H√° todos os motivos para acreditar que a tecnologia dipfeyk nos pr√≥ximos anos s√≥ ficar√° melhor, mais r√°pida e mais barata. <br><br>  Neste artigo, vou gui√°-lo pela m√£o no meu caminho do dipfake.  Explicarei todas as etapas necess√°rias para criar um v√≠deo deepfake.  Ao longo do caminho, explicarei como essa tecnologia funciona e quais limita√ß√µes ela possui. <br><br><h2>  Dipfeyks precisam de muita energia e dados de computa√ß√£o </h2><br>  Chamamos esses v√≠deos de diphakes ["falsifica√ß√µes profundas"] porque s√£o criados usando redes neurais profundas.  Na √∫ltima d√©cada, os cientistas da computa√ß√£o descobriram que as redes neurais est√£o se tornando mais poderosas com a adi√ß√£o de camadas adicionais de neur√¥nios.  Mas, para liberar todo o potencial das redes neurais profundas, voc√™ precisa de muitos dados e enorme poder de computa√ß√£o. <br><br>  O mesmo vale para os dipfakes.  Para este projeto, aluguei uma m√°quina virtual com quatro poderosas placas gr√°ficas.  E mesmo com todos esses cavalos, levei quase uma semana para treinar meu modelo. <br><br>  Eu tamb√©m precisava de uma montanha de imagens de Mark Zuckerberg e Data.  Eu consegui um v√≠deo com 38 segundos de dura√ß√£o, mas para o treinamento eu precisava de v√≠deos muito mais longos, Zuckerberg e Data. <br><br>  Para fazer isso, baixei v√°rios v√≠deos contendo seus rostos: 14 clipes com clipes de Star Trek e nove com Mark Zuckerberg.  Entre os √∫ltimos, havia relatos formais, v√°rias entrevistas na TV e at√© um v√≠deo em que Zuckerberg preparava um churrasco no quintal. <br><br>  Carreguei todos esses clipes no iMovie e apaguei os quadros que n√£o continham os rostos de Zuckerberg e Data.  Tamb√©m cortei em peda√ßos as passagens mais longas.  Um programa dipfake precisa n√£o apenas de um grande n√∫mero de imagens, mas de um grande n√∫mero de imagens diferentes.  Precis√°vamos fotografar rostos de √¢ngulos diferentes, com express√µes diferentes e com ilumina√ß√£o diferente.  Um v√≠deo de uma hora em que Zuckerberg l√™ o relat√≥rio n√£o pode produzir fotos mais valiosas do que um segmento de cinco minutos, uma vez que √© gravado do mesmo √¢ngulo, na mesma luz e mostra a mesma express√£o facial.  Ent√£o, cortei algumas horas de v√≠deo em 9 minutos com Data e em at√© 7 minutos com Zuckerberg. <br><br><h2>  Faceswap: um pacote de software para criar dipfakes </h2><br>  Ent√£o √© hora de usar o software para dipheyka.  No come√ßo, tentei usar o programa DeepFaceLab e consegui criar um v√≠deo bastante dif√≠cil.  Depois, pedi conselhos no f√≥rum SFWdeepfakes e algumas pessoas me aconselharam no Faceswap.  As pessoas observaram que este programa possui mais recursos, melhor documenta√ß√£o e melhor suporte online.  Eu decidi seguir o conselho deles. <br><br>  O Faceswap √© executado no Linux, Windows e Mac.  O pacote possui ferramentas para trabalhar em todas as etapas da cria√ß√£o de um dipfake, desde a importa√ß√£o dos v√≠deos originais at√© a cria√ß√£o de um v√≠deo dipfake finalizado.  O software n√£o √© intuitivo, mas com ele vem um material de treinamento detalhado que cobre todas as etapas do processo.  O material foi escrito pelo criador do Faceswap, Matt Torah, que tamb√©m me ajudou bastante a conversar no canal Deepfake do Discord. <br><br>  O Faceswap requer uma poderosa placa gr√°fica.  Eu sabia que o meu MacBook Pro n√£o aguentava.  Pedi aos t√©cnicos de nosso escrit√≥rio editorial que me alugassem uma m√°quina virtual para Linux de um provedor l√≠der de servi√ßos em nuvem.  Comecei com uma m√°quina virtual com uma GPU Nvidia K80 e 12 GB de mem√≥ria de v√≠deo.  Alguns dias depois, mudei para um modelo com duas GPUs e depois para 4 GPUs.  Ela tinha quatro GPUs Nvidia T4 Tensor Core com 16 Gb de mem√≥ria cada (e outras 48 CPU e 192 RAM, que estavam praticamente inativas). <br><br>  Ap√≥s duas semanas de trabalho, recebi uma fatura de US $ 522.  Claro, gastei uma quantia bastante grande para a conveni√™ncia de alugar um computador.  A Tor√° me disse que, no momento, a op√ß√£o de hardware mais lucrativa para um dipfake √© uma placa Nvidia GTX 1070 ou 1080 com 8 GB de mem√≥ria.  Esse cart√£o usado vale v√°rias centenas de d√≥lares.  Uma placa 1080 n√£o ensina uma rede neural t√£o r√°pido quanto quatro das minhas GPUs, mas se voc√™ estiver pronto para esperar algumas semanas, obter√° resultados semelhantes. <br><br>  O fluxo de trabalho no Faceswap consiste em tr√™s etapas b√°sicas: <br><br><ul><li>  Extra√ß√£o: corte o v√≠deo em quadros, encontre rostos em cada quadro, exiba imagens bem alinhadas e cuidadosamente cortadas de cada rosto. </li><li>  Treinamento: use as imagens obtidas para treinar a rede neural dipfake.  Tira uma imagem do rosto de uma pessoa e produz uma imagem do rosto de outra pessoa com a mesma express√£o, ilumina√ß√£o e na mesma posi√ß√£o. </li><li>  Transforma√ß√£o: aplique o modelo treinado na etapa anterior a um v√≠deo espec√≠fico para fornecer um dipfake.  Depois de treinar o modelo, ele pode ser aplicado a qualquer v√≠deo em que essas pessoas estejam presentes em cujas faces ele foi treinado. </li></ul><br>  Para cada uma das tr√™s etapas, √© necess√°ria uma quantidade de tempo completamente diferente da pessoa e da m√°quina.  O software de recupera√ß√£o de imagem √© executado por v√°rios minutos, mas pode levar horas para uma pessoa verificar os resultados.  O software registra todos os rostos de cada imagem, al√©m de alguns falsos positivos.  Para obter bons resultados, uma pessoa precisa passar por todos os resultados, removendo rostos desnecess√°rios e tudo o que o software levou para uma pessoa. <br><br>  A aprendizagem √© f√°cil de configurar e praticamente n√£o requer envolvimento humano.  No entanto, pode levar dias ou at√© semanas de tempo no computador para obter bons resultados.  Comecei a treinar meu modelo final em 7 de dezembro e funcionou at√© 13 de dezembro.  √â poss√≠vel que, ap√≥s mais uma semana de trabalho, a qualidade do meu dipfake melhore.  E tamb√©m usei meu monstro na nuvem com quatro placas gr√°ficas avan√ßadas.  Se voc√™ trabalha no seu computador com uma √∫nica GPU de menor pot√™ncia, pode levar v√°rias semanas para treinar um bom modelo. <br><br>  A etapa final, transforma√ß√£o, √© r√°pida para uma pessoa e um computador.  Ao receber um modelo adequadamente treinado, voc√™ pode entregar v√≠deos dipfake em menos de um minuto. <br><br><h2>  Como funcionam os diphakes </h2><br>  Antes de descrever o processo de aprendizado do Faceswap, voc√™ precisa explicar como a tecnologia subjacente funciona. <br><br>  No cora√ß√£o do Faceswap - e outros pacotes de software l√≠deres para a cria√ß√£o de diphakes - est√° o codificador autom√°tico.  Esta √© uma rede neural treinada para receber uma imagem de entrada e produzir uma imagem id√™ntica.  Essa habilidade em si pode n√£o ser t√£o √∫til, mas, como veremos mais adiante, √© um elemento essencial no processo de cria√ß√£o de um dipfake. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/63f/e44/76b/63fe4476bd395c863cd0c9b71bf4cfbf.jpg"><br><br>  O codificador autom√°tico √© estruturado de acordo com o princ√≠pio de dois funis conectados por uma extremidade estreita.  De um lado da rede, h√° um codificador que recebe uma imagem e a comprime em um pequeno n√∫mero de vari√°veis.  No modelo que usei no Faceswap, esses s√£o 1024 n√∫meros de ponto flutuante de 32 bits.  Do outro lado da rede neural est√° um decodificador.  Ele pega essa representa√ß√£o compacta, conhecida como ‚Äúespa√ßo latente‚Äù, e tenta expandi-la, tendo recebido a imagem inicial. <br><br>  Limitar artificialmente a quantidade de dados transmitidos do codificador para o decodificador faz com que essas duas redes desenvolvam uma representa√ß√£o compacta do rosto humano.  Um codificador √© algo como um algoritmo de compacta√ß√£o com perdas que tenta salvar o m√°ximo poss√≠vel de informa√ß√µes sobre um rosto, limitando a quantidade de armazenamento.  O espa√ßo latente deve de alguma forma extrair detalhes importantes, por exemplo, em que dire√ß√£o o sujeito est√° olhando, seus olhos est√£o abertos ou fechados, ele est√° sorrindo ou franzindo a testa. <br><br>  √â importante que o codificador autom√°tico precise salvar apenas os recursos do rosto que mudam com o tempo.  Ele n√£o precisa armazenar coisas inalteradas, como cor dos olhos ou formato do nariz.  Se ele tiver olhos azuis em todas as fotografias de Zuckerberg, seu decodificador de rede aprender√° a desenhar automaticamente seu rosto com olhos azuis.  N√£o h√° necessidade de colocar as informa√ß√µes em um espa√ßo latente apertado que n√£o muda durante a transi√ß√£o de uma imagem para outra.  Como veremos mais adiante, o fato de os codificadores autom√°ticos terem atitudes diferentes em rela√ß√£o √†s caracter√≠sticas faciais constantes e vari√°veis ‚Äã‚Äã√© extremamente importante para sua capacidade de emitir difusores de frequ√™ncia. <br><br>  Cada algoritmo para treinar uma rede neural precisa de alguma maneira de avaliar a qualidade da rede para que ela possa ser aprimorada.  Em muitos casos, isso √© feito por meio de treinamento com o professor, quando a pessoa fornece a resposta correta para cada elemento do conjunto de dados de treinamento.  Os codificadores autom√°ticos funcionam de maneira diferente.  Como eles est√£o simplesmente tentando reproduzir seus pr√≥prios dados de entrada, o software de treinamento pode avaliar automaticamente a qualidade do trabalho.  No jarg√£o do aprendizado de m√°quina, isso √© chamado de aprendizado sem professor. <br><br>  Como qualquer rede neural, os auto-codificadores no Faceswap s√£o treinados usando retropropaga√ß√£o.  O algoritmo de treinamento alimenta uma imagem espec√≠fica na rede neural e examina quais pixels na sa√≠da n√£o correspondem √† entrada.  Em seguida, ele calcula quais neur√¥nios da √∫ltima camada deram a maior contribui√ß√£o para os erros e corrige levemente os par√¢metros de cada neur√¥nio, a fim de obter melhores resultados. <br><br>  Em seguida, esses erros se propagam de volta √† camada anterior, onde os par√¢metros de cada neur√¥nio s√£o corrigidos novamente.  Os erros se propagam dessa maneira mais para tr√°s at√© que cada um dos par√¢metros da rede neural - tanto o codificador quanto o decodificador - seja corrigido. <br><br>  Em seguida, o algoritmo de treinamento alimenta outra imagem da rede e todo o processo √© repetido novamente.  Podem ser necess√°rias centenas de milhares de repeti√ß√µes para criar um codificador autom√°tico que reproduza bem sua pr√≥pria entrada. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/335/661/bb1/335661bb13300b80164987a1aa1dd420.jpg"><br><br>  O software Dipfake funciona treinando simultaneamente dois codificadores autom√°ticos, um para o rosto original e o segundo para o novo.  Durante o processo de treinamento, cada codificador autom√°tico recebe imagens de apenas uma pessoa e ele √© treinado para produzir imagens muito semelhantes √†s originais. <br><br>  H√°, no entanto, um problema: ambas as redes usam o mesmo codificador.  Os decodificadores - neur√¥nios do lado direito da rede - permanecem separados e cada um deles √© treinado para mostrar uma face diferente.  Mas os neur√¥nios no lado esquerdo da rede t√™m par√¢metros comuns que mudam toda vez que qualquer um dos codificadores autom√°ticos √© treinado.  Quando a rede Zuckerberg √© treinada na face Zuckerberg, isso altera metade da rede pertencente ao codificador e na rede de Dados.  Cada vez que a rede da Data √© treinada na face da Data, o codificador Zuckerberg herda essas altera√ß√µes. <br><br>  Como resultado, dois codificadores autom√°ticos t√™m um codificador comum que pode "ler" o rosto de Zuckerberg ou o rosto de Data.  O objetivo do codificador √© usar a mesma representa√ß√£o de coisas como o √¢ngulo da cabe√ßa ou a localiza√ß√£o das sobrancelhas, independentemente de ele ter recebido uma foto de Zuckerberg ou uma foto de dados na entrada.  E isso, por sua vez, significa que quando voc√™ aperta seu rosto com o codificador, pode descompact√°-lo usando qualquer decodificador. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ce8/0b6/670/ce80b6670b49be4b905f9ce520810931.jpg"><br><br>  Portanto, tendo treinado alguns codificadores autom√°ticos dessa maneira, resta um passo simples para criar um falso falso: voc√™ troca os decodificadores.  Voc√™ est√° codificando uma foto de Zuckerberg, mas usando o decodificador de dados na etapa de decodifica√ß√£o.  O resultado √© uma fotografia reconstru√≠da de Data - mas com a mesma posi√ß√£o da cabe√ßa e express√£o facial da fotografia original de Zuckerberg. <br><br>  Lembre-se de que mencionei que o espa√ßo latente captura as caracter√≠sticas faciais vari√°veis ‚Äã‚Äãde uma pessoa - express√£o, dire√ß√£o da vis√£o, a localiza√ß√£o das sobrancelhas - e coisas constantes como a cor dos olhos ou o formato da boca fornece o decodificador.  Isso significa que, se voc√™ codificar a imagem de Zuckerberg e decodific√°-la usando o decodificador de dados, obter√° um rosto com recursos permanentes de dados - por exemplo, um formato de rosto - mas com a express√£o e orienta√ß√£o da face original de Zuckerberg. <br><br>  Ao aplicar essa t√©cnica a quadros sucessivos de um v√≠deo com Zuckerberg, voc√™ obt√©m um novo v√≠deo em que o rosto de Data realiza os mesmos movimentos - sorri, pisca, vira a cabe√ßa -, o que Zuckerberg fez no v√≠deo original. <br><br>  Esta situa√ß√£o √© sim√©trica.  Ao treinar uma rede neural para receber uma foto de Zuckerberg e emitir uma foto de Data, voc√™ a treina simultaneamente para receber uma foto de Data e emitir uma foto de Zuckerberg.  A ferramenta de convers√£o de v√≠deo do Faceswap - o √∫ltimo passo no processo de cria√ß√£o de um dipfake - inclui uma caixa de sele√ß√£o √∫til "trocar modelos", permitindo ao usu√°rio trocar decodificadores.  Como resultado, em vez de substituir o rosto de Data no lugar do rosto de Zuckerberg, o programa faz o contr√°rio, produzindo v√≠deos muito engra√ßados como este: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Fi7sj7SU1Vg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Dados de treinamento </h2><br>  Na pr√°tica, obter bons resultados ao criar um dipfake n√£o √© f√°cil. <br><br>  Como mencionei, recebi sete minutos de v√≠deo para Data e nove minutos para Zuckerberg.  Depois, usei a ferramenta de extra√ß√£o de imagem Faceswap para cortar o v√≠deo e obter imagens cortadas dos rostos dos dois homens.  O v√≠deo cont√©m cerca de 30 quadros por segundo, mas extra√≠ apenas a cada sexto - essa pr√°tica √© recomendada na documenta√ß√£o do Faceswap.  Isso ocorre porque uma variedade de imagens significa mais do que apenas seu n√∫mero, e salvar cada quadro levaria a um grande n√∫mero de imagens muito semelhantes. <br><br>  A ferramenta de extra√ß√£o Faceswap produziu muitos falsos positivos.  Ele tamb√©m encontrou rostos reais no fundo de algumas fotos.  Durante algumas horas, apaguei manualmente todas as fotos extra√≠das que n√£o pertenciam a nenhum dos meus dois assuntos experimentais.  Como resultado, obtive 2598 imagens do rosto de Data e 2224 imagens do rosto de Zuckerberg. <br><br>  E, naquele momento, finalmente, chegou a hora de seguir para o treinamento de modelos reais.  Agora, o Faceswap vem com 10 algoritmos dipfake diferentes, que oferecem suporte a tamanhos de imagem diferentes e exigem poder de computa√ß√£o diferente.  Entre os mais despretensiosos, h√° um modelo "leve" que trabalha com imagens de rosto com tamanho n√£o superior a 64 pixels.  Pode ser executado em uma m√°quina com no m√°ximo 2 GB de mem√≥ria de v√≠deo.  Outros modelos trabalham com imagens de 128, 256 ou at√© 512 pixels de tamanho - no entanto, exigem muito mais mem√≥ria de v√≠deo e mais tempo de treinamento. <br><br>  Comecei a treinar o modelo DFL-SAE, derivado de algoritmos do DeepFaceLab.  No entanto, houve um aviso na documenta√ß√£o do Faceswap de que este modelo sofre de um "vazamento de identidade" no qual alguns recursos de um rosto podem se infiltrar em outro.  Pareceu-me que vi algo parecido com isso em alguns dos primeiros v√≠deos de teste, ent√£o um dia depois mudei para o modelo Villain, que funciona com imagens de 128 pixels.  O manual do Faceswap o descreve como muito exigente na VRAM e como "uma boa op√ß√£o para quem deseja obter um modelo de resolu√ß√£o mais alta sem ajustar nenhum par√¢metro". <br><br>  Ent√£o eu esperei.  E ele esperou.  O processo de aprendizado ainda n√£o havia terminado quando meu prazo chegou na sexta-feira - e ap√≥s seis dias de treinamento.  Naquela √©poca, meu modelo produzia um bom dipfake.  A velocidade do progresso diminuiu, mas √© poss√≠vel que eu tivesse um resultado melhor se tivesse mais uma semana de tempo no computador. <br><br>  O Faceswap est√° bem adaptado para trabalhos de computa√ß√£o longos.  Se voc√™ iniciar a equipe de treinamento a partir da interface gr√°fica, a interface do programa atualiza regularmente a tela de visualiza√ß√£o, onde √© poss√≠vel ver exemplos de como o software cria retratos de Data e Zuckerberg.  Se voc√™ preferir realizar um treinamento na linha de comando, isso tamb√©m √© poss√≠vel.  A interface do Faceswap possui um √∫til bot√£o "gerar" que fornece o comando exato que voc√™ precisa executar para treinar o modelo com as configura√ß√µes atuais feitas na interface. <br><br><h2>  Qu√£o bom foi o dipfake? </h2><br>  No processo de aprendizado, o Faceswap exibe constantemente uma estimativa num√©rica da "perda" para cada um dos dois codificadores autom√°ticos.  Essas estimativas mostram qu√£o bem o codificador autom√°tico de Zuckerberg pode reproduzir as fotos de Zuckerberg - e qu√£o bem o codificador autom√°tico de Data pode reproduzir as fotos de Data.  E esses n√∫meros ainda estavam diminuindo quando eu parei de aprender na sexta-feira, embora a velocidade do progresso tenha diminu√≠do significativamente. <br><br>  Naturalmente, √© importante para n√≥s o qu√£o bem o decodificador de Data pode transformar o rosto de Zuckerberg em Data.  N√£o sabemos como deve ser o "resultado final", portanto, √© imposs√≠vel medir a qualidade do trabalho em n√∫meros exatos.  O melhor que podemos fazer √© revisar o v√≠deo e decidir se ele parece realista. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zZEi1lHTdpY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O v√≠deo acima mostra a qualidade do dipfake nas quatro etapas do processo de aprendizado. Os v√≠deos de 10 e 12 de dezembro mostram o modelo de vil√£o parcialmente treinado. O v√≠deo de 6 de dezembro no canto superior esquerdo √© um teste inicial com um modelo diferente. O canto inferior direito √© o resultado final. No processo de treinamento, os detalhes de seu rosto se tornaram mais claros e mais cr√≠veis. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em 9 de dezembro, ap√≥s tr√™s dias de treinamento, publiquei um v√≠deo preliminar no canal interno da reda√ß√£o em Slak. O v√≠deo foi semelhante ao que est√° localizado no canto superior esquerdo. Nosso guru do design, Aurich Lawson, reagiu sarcasticamente a ele. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúEm geral, parece ruim‚Äù, ele escreveu, acrescentando que ‚Äún√£o parece convincente. Estou esperando por um desses v√≠deos que n√£o parecem falsos. "</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Penso que nas suas cr√≠ticas existe um n√∫cleo racional. Fiquei surpreso com a rapidez com que o Faceswap foi capaz de criar imagens de rostos que se pareciam muito com Brent Spiner, mais do que Zuckerberg. No entanto, se voc√™ observar de perto, ver√° os sinais caracter√≠sticos da fraude digital. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em alguns quadros, a fronteira entre o rosto falso de Data e a cabe√ßa de Zuckerberg n√£o parece muito certa. √Äs vezes, a sobrancelha de Zuckerberg espreita por baixo do rosto de Data. Em outros lugares, as bordas do rosto falso s√£o cobertas com alguns pixels nas orelhas de Zuckerberg. Pode ser poss√≠vel corrigir esses problemas com a composi√ß√£o no p√≥s-processamento manual de uma pessoa - mas algu√©m precisar√° rolar o v√≠deo quadro a quadro e corrigir a m√°scara de cada um.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/d8XknbnMs4c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, um problema mais fundamental √© que os algoritmos diphfake ainda n√£o s√£o capazes de reproduzir os m√≠nimos detalhes dos rostos humanos o suficiente. Isso √© bastante √≥bvio quando voc√™ olha para os v√≠deos inicial e final em paralelo. O Faceswap surpreendentemente bem transmitiu a estrutura geral do rosto de Data. Mas mesmo depois de uma semana de treinamento, o rosto parece desfocado e n√£o h√° detalhes importantes suficientes nele. Por exemplo, o software para dipheykas dificilmente consegue lidar com o desenho de dentes humanos. √Äs vezes, os dentes ficam claramente vis√≠veis e, no pr√≥ximo quadro, desaparecem, deixando escurid√£o.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uma das principais raz√µes para isso √© porque a tarefa Faceswap fica exponencialmente mais complicada com resolu√ß√µes mais altas. Os codificadores autom√°ticos fazem um bom trabalho com imagens de 64x64 pixels. Mas reproduzir os detalhes mais finos das imagens de 128x128 pixels - para n√£o mencionar imagens de 256 pixels ou mais - j√° √© muito mais dif√≠cil. Talvez essa seja uma das raz√µes pelas quais os difusores mais impressionantes t√™m um √¢ngulo de vis√£o bastante amplo, sem close de rostos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, voc√™ n√£o deve considerar isso uma limita√ß√£o fundamental da tecnologia diphake. Nos pr√≥ximos anos, os pesquisadores poder√£o desenvolver tecnologias que possam superar essas limita√ß√µes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Freq√ºentemente, a base do software para um dipheyka √© erroneamente descrita como redes generativas-advers√°rias (GSS) ou redes neurais que permitem ao software "representar"</font></font><a href="https://thispersondoesnotexist.com/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pessoas</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , objetos ou paisagens </font><a href="https://thispersondoesnotexist.com/" rel="nofollow"><font style="vertical-align: inherit;">inexistentes</font></a><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">De fato, o dipfeyki trabalha usando autoencoders. </font><font style="vertical-align: inherit;">No entanto, os √∫ltimos avan√ßos na tecnologia GSS sugerem que os dipfakes ainda t√™m espa√ßo para melhorias. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O GSS, que apareceu pela primeira vez em 2014, s√≥ podia produzir imagens grosseiras e de baixa resolu√ß√£o. </font><font style="vertical-align: inherit;">Mas, recentemente, os pesquisadores </font></font><a href="https://arxiv.org/abs/1710.10196" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">descobriram</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como criar um GSS que produz imagens fotorrealistas com tamanho de at√© 1024 pixels. </font><font style="vertical-align: inherit;">As t√©cnicas espec√≠ficas usadas nesses trabalhos cient√≠ficos podem n√£o ser aplic√°veis ‚Äã‚Äã√† cria√ß√£o de um diphake, mas √© f√°cil imaginar como algu√©m desenvolver√° uma tecnologia semelhante para codificadores autom√°ticos - ou talvez uma arquitetura de rede neural completamente nova projetada para substituir faces.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Perspectiva Dipfake </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O aumento da popularidade dos dipfakes √© obviamente alarmante. At√© recentemente, as pessoas podiam facilmente gravar um v√≠deo com uma pessoa pelo valor de face. O advento do software dipheyka e de outras ferramentas digitais nos deixou c√©ticos em rela√ß√£o aos v√≠deos agora. Se virmos um v√≠deo em que uma pessoa reivindica algo escandaloso - ou retira-se - devemos considerar a possibilidade de algu√©m falsificar esse v√≠deo para desacreditar essa pessoa. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, meu experimento enfatiza as limita√ß√µes da tecnologia dipfake - pelo menos em sua forma atual. √â necess√°rio amplo conhecimento e esfor√ßo para criar uma face virtual totalmente convincente. N√£o tive sucesso e n√£o tenho certeza se algu√©m j√° foi capaz de produzir um v√≠deo dipfake que √© realmente indistingu√≠vel do real.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al√©m disso, hoje ferramentas como o Faceswap lidam apenas com altera√ß√µes de rosto. Eles n√£o mudam a testa, cabelos, bra√ßos e pernas. E mesmo que o rosto seja perfeito, ser√° poss√≠vel determinar o v√≠deo dipfake com base em elementos que n√£o parecem corretos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No entanto, essas limita√ß√µes da tecnologia dipfake podem desaparecer. Em alguns anos, o software poder√° aprender a produzir v√≠deos que n√£o podem ser distinguidos dos reais. O que ent√£o?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nesse caso, ser√° √∫til lembrar que outros tipos de m√≠dia h√° muito s√£o f√°ceis de falsificar. </font><font style="vertical-align: inherit;">A tarefa trivial seria tirar uma captura de tela de um e-mail, onde algu√©m escreve algo que ele realmente n√£o escreveu. </font><font style="vertical-align: inherit;">E isso n√£o levou a um aumento no n√∫mero de pedreiras quebradas devido a e-mails fraudulentos, nem desacreditou as capturas de tela das cartas como evid√™ncia usada em discuss√µes p√∫blicas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas as pessoas sabem que os e-mails podem ser falsificados e est√£o procurando por confirma√ß√£o adicional nesses casos. </font><font style="vertical-align: inherit;">Que cadeia de eventos atraiu a aten√ß√£o do p√∫blico para as cartas? </font><font style="vertical-align: inherit;">Outras pessoas receberam c√≥pias deste e-mail no momento em que deveria ser escrito? </font><font style="vertical-align: inherit;">O suposto autor da carta reconheceu sua autoria ou alegou falsifica√ß√£o? </font><font style="vertical-align: inherit;">As respostas a essas perguntas ajudam as pessoas a decidirem com que seriedade podem levar uma carta publicada.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Voc√™ pode ser enganado uma vez </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O mesmo acontece com os v√≠deos. </font><font style="vertical-align: inherit;">Talvez haja um breve per√≠odo de tempo em que os enganadores possam destruir a carreira de uma pessoa postando um v√≠deo em que ele diz ou faz algo ultrajante. </font><font style="vertical-align: inherit;">Mas em breve a sociedade aprender√° a tratar v√≠deos com ceticismo, a menos que o videoclipe possua qualquer tipo de evid√™ncia documental, testemunhas ou outros fatores de apoio. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acho que isso funcionar√° mesmo nos casos dos abusos mais ultrajantes da tecnologia diphey: inserir o rosto de uma pessoa em um v√≠deo pornogr√°fico. </font><font style="vertical-align: inherit;">Isso √© obviamente desrespeitoso e inaceit√°vel. </font><font style="vertical-align: inherit;">Mas as pessoas temem que esses v√≠deos possam destruir sua reputa√ß√£o e carreira. </font><font style="vertical-align: inherit;">Eu acho que n√£o √© assim.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">De fato, na Internet voc√™ pode encontrar imagens completas de personalidades famosas (principalmente mulheres) cujas cabe√ßas est√£o presas aos corpos de estrelas porno com a ajuda do Photoshop. O sofrimento das mulheres √© compreens√≠vel. Mas o p√∫blico n√£o conclui automaticamente que essas mulheres posam nuas - sabemos sobre a exist√™ncia do Photoshop e sobre a possibilidade de criar fotos falsas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O mesmo vale para a pornografia profunda. Obviamente, n√£o √© bom fazer porn√¥ falso com a sua participa√ß√£o. Mas o lan√ßamento de um v√≠deo dipfake com algum tipo de pessoa n√£o ter√° um efeito t√£o devastador quanto um v√≠deo real de sexo. Na aus√™ncia de evid√™ncias da autenticidade do v√≠deo, o p√∫blico concluir√° que √© falso.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Matt Torah, autor do Faceswap, conta que essa considera√ß√£o foi um dos componentes de sua motiva√ß√£o para criar o pacote. Ele acredita que o software para mudar as pessoas ser√° inevitavelmente desenvolvido. Ele espera que, ao criar uma ferramenta amig√°vel para mudar as pessoas de c√≥digo aberto, ele ajude a remover o v√©u de sigilo com essa tecnologia e informe o p√∫blico sobre suas capacidades e limita√ß√µes. E isso, por sua vez, nos ajudar√° a chegar rapidamente ao ponto em que o p√∫blico ficar√° c√©tico em rela√ß√£o a v√≠deos que podem ser falsos.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A longo prazo, arriscamos que o p√™ndulo das rela√ß√µes p√∫blicas mude demais para o outro lado, e a possibilidade de criar dipfakes destrua a cren√ßa no poder evidencial dos v√≠deos. </font><font style="vertical-align: inherit;">Alguns pol√≠ticos j√° adotaram o h√°bito de rejeitar as cr√≠ticas da m√≠dia como "not√≠cias falsas". </font><font style="vertical-align: inherit;">Essa t√°tica se tornar√° mais eficaz com a crescente conscientiza√ß√£o da sociedade sobre a tecnologia dos dipfakes.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt482684/">https://habr.com/ru/post/pt482684/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt482672/index.html">Relat√≥rio anual Habrapost - 2019</a></li>
<li><a href="../pt482674/index.html">‚ÄúImediatamente ap√≥s as f√©rias‚Äù: semin√°rios, master classes e competi√ß√µes de tecnologia na Universidade ITMO</a></li>
<li><a href="../pt482676/index.html">Classifica√ß√£o eficaz dos dados do tipo Struct</a></li>
<li><a href="../pt482678/index.html">Como eu decidi fazer uma busca de texto para iOS e o que veio dela</a></li>
<li><a href="../pt482680/index.html">Crypt, XOR, hackers ZIP e PRSP n√£o criptografados. Solu√ß√£o de problemas com o r0ot-mi Crypto. Parte 2</a></li>
<li><a href="../pt482686/index.html">Cientistas automatizam pesquisa de comportamento animal para decodificar a fun√ß√£o cerebral</a></li>
<li><a href="../pt482690/index.html">Crie seu editor em Combine</a></li>
<li><a href="../pt482698/index.html">[Ensaio] Dedicado ao pl√¢ncton do Office. Eu n√£o sou inspirado pelo meu trabalho</a></li>
<li><a href="../pt482700/index.html">Como "foder" o Google e o Yandex: promo√ß√£o de sites em preto e branco de SEO. Shestakov Pessoas PRO # 74</a></li>
<li><a href="../pt482702/index.html">Realmente precisamos do TypeScript em 2020?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>