<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘©â€ğŸ¤ ğŸº â¬…ï¸ Prinsip untuk membangun sistem analisis streaming ğŸ”® ğŸ…¿ï¸ âš•ï¸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Merancang analitik streaming dan sistem pemrosesan data streaming memiliki nuansa tersendiri, masalah tersendiri, dan tumpukan teknologinya sendiri. K...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Prinsip untuk membangun sistem analisis streaming</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477834/"><img src="https://habrastorage.org/webt/rk/pz/c5/rkpzc5nw7uyyv0vsp_00trtag44.jpeg" alt="gambar"><br><br>  Merancang analitik streaming dan sistem pemrosesan data streaming memiliki nuansa tersendiri, masalah tersendiri, dan tumpukan teknologinya sendiri.  Kami membicarakan hal ini dalam <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">pelajaran terbuka</a> berikutnya, yang diadakan menjelang peluncuran kursus <a href="https://otus.pw/IxY2/">Insinyur Data</a> . <br><br>  Di webinar dibahas: <br><br><ul><li>  saat pemrosesan streaming diperlukan; </li><li>  elemen apa yang ada di SPOD, alat apa yang bisa kita gunakan untuk mengimplementasikan elemen-elemen ini; </li><li>  cara membangun sistem analisis clickstream Anda sendiri. </li></ul><br>  Dosen - <a href="https://otus.ru/teacher/370/">Yegor Mateshuk</a> , Insinyur Data Senior di MaximaTelecom. <br><a name="habracut"></a><br><h3>  Kapan streaming diperlukan?  Streaming vs Gelombang </h3><br>  Pertama-tama, kita perlu mencari tahu kapan kita perlu streaming, dan kapan pemrosesan batch.  Mari kita jelaskan kekuatan dan kelemahan pendekatan ini. <br><br>  <b>Jadi, kerugian dari pemrosesan batch:</b> <br><br><ul><li>  data dikirimkan dengan penundaan.  Karena kita memiliki periode perhitungan tertentu, maka untuk periode ini kita selalu ketinggalan waktu nyata.  Dan semakin banyak iterasi, semakin kita tertinggal.  Dengan demikian, kami mendapat penundaan waktu, yang dalam beberapa kasus sangat penting; </li><li>  beban puncak pada besi dibuat.  Jika kami menghitung banyak dalam mode batch, pada akhir periode (hari, minggu, bulan) kami memiliki beban puncak, karena Anda perlu menghitung banyak hal.  Apa yang menyebabkan ini?  Pertama, kita mulai bersandar pada batasan, yang, seperti yang Anda tahu, tidak terbatas.  Akibatnya, sistem berjalan secara berkala hingga batas, yang sering mengakibatkan kegagalan.  Kedua, karena semua pekerjaan ini dimulai pada saat yang sama, mereka bersaing dan dihitung cukup lambat, yaitu, Anda tidak dapat mengandalkan hasil yang cepat. </li></ul><br>  <b>Tetapi pemrosesan batch memiliki kelebihan:</b> <br><br><ul><li>  efisiensi tinggi.  Kami tidak akan masuk lebih dalam, karena efisiensi terkait dengan kompresi, dan dengan kerangka kerja, dan dengan penggunaan format kolom, dll. Faktanya adalah bahwa pemrosesan batch, jika Anda mengambil jumlah catatan yang diproses per unit waktu, akan lebih efisien; </li><li> kemudahan pengembangan dan dukungan.  Anda dapat memproses bagian mana pun dari data dengan menguji dan menghitung ulang seperlunya. </li></ul><br>  <b>Keuntungan pemrosesan data streaming (streaming):</b> <br><br><ul><li>  menghasilkan waktu nyata.  Kami tidak menunggu akhir periode: segera setelah data (bahkan jumlah yang sangat kecil) datang kepada kami, kami dapat segera memprosesnya dan meneruskannya.  Artinya, hasilnya, menurut definisi, berjuang untuk waktu nyata; </li><li>  beban seragam pada besi.  Jelas bahwa ada siklus harian, dll., Namun, bebannya masih didistribusikan sepanjang hari dan ternyata lebih seragam dan dapat diprediksi. </li></ul><br>  <b>Kerugian utama dari pemrosesan streaming:</b> <br><ul><li>  kompleksitas pengembangan dan dukungan.  Pertama, menguji, mengelola, dan mengambil data sedikit lebih sulit jika dibandingkan dengan batch.  Kesulitan kedua (sebenarnya, ini adalah masalah paling mendasar) dikaitkan dengan rollback.  Jika pekerjaan tidak berhasil, dan ada kegagalan, sangat sulit untuk menangkap momen di mana semuanya rusak.  Dan memecahkan masalah akan membutuhkan lebih banyak upaya dan sumber daya daripada pemrosesan batch. </li></ul><br>  Jadi, jika Anda berpikir tentang <b>apakah Anda perlu streaming</b> , jawablah pertanyaan berikut untuk diri sendiri: <br><br><ol><li>  Apakah Anda benar-benar membutuhkan waktu nyata? </li><li>  Apakah ada banyak sumber streaming? </li><li>  Apakah kehilangan satu catatan penting? </li></ol><br>  Mari kita lihat <b>dua contoh</b> : <br><br>  <i>Contoh 1. Analisis stok untuk ritel:</i> <br><ul><li>  tampilan barang tidak berubah secara real time; </li><li>  data paling sering dikirim dalam mode batch; </li><li>  kehilangan informasi sangat penting. </li></ul><br>  Dalam contoh ini, lebih baik menggunakan batch. <br><br>  <i>Contoh 2. Analisis untuk portal web:</i> <br><br><ul><li>  kecepatan analitik menentukan waktu respons terhadap suatu masalah; </li><li>  data datang secara real time; </li><li>  Kehilangan sejumlah kecil informasi aktivitas pengguna dapat diterima. </li></ul><br>  Bayangkan analitik mencerminkan bagaimana perasaan pengunjung ke portal web menggunakan produk Anda.  Misalnya, Anda meluncurkan rilis baru dan Anda perlu memahami dalam 10-30 menit apakah semuanya beres, jika ada fitur kustom yang rusak.  Katakanlah teks dari tombol "Pesan" hilang - analitik akan memungkinkan Anda untuk dengan cepat merespons penurunan tajam dalam jumlah pesanan, dan Anda akan segera memahami bahwa Anda harus mundur. <br><br>  Jadi, dalam contoh kedua, lebih baik menggunakan stream. <br><br><h3>  Elemen SPOD </h3><br>  Insinyur pemrosesan data menangkap, memindahkan, mengirim, mengonversi, dan menyimpan data ini (ya, penyimpanan data juga merupakan proses yang aktif!). <br>  Oleh karena itu, untuk membangun sistem pemrosesan data streaming (SPOD), kita perlu elemen-elemen berikut: <br><br><ol><li>  <b>pemuat data</b> (sarana pengiriman data ke penyimpanan); </li><li>  <b>bus pertukaran data</b> (tidak selalu diperlukan, tetapi tidak ada cara untuk mengalirkannya, karena Anda membutuhkan sistem yang melaluinya Anda akan bertukar data secara langsung); </li><li>  <b>penyimpanan data</b> (seperti tanpa itu); </li><li>  <b>Mesin ETL</b> (diperlukan untuk melakukan berbagai penyaringan, penyortiran dan operasi lainnya); </li><li>  <b>BI</b> (untuk menampilkan hasil); </li><li>  <b>orchestrator</b> (menghubungkan seluruh proses bersama-sama, mengatur pemrosesan data multi-tahap). </li></ol><br>  Dalam kasus kami, kami akan mempertimbangkan situasi paling sederhana dan hanya fokus pada tiga elemen pertama. <br><br><h3>  Alat Pemroses Streaming Data </h3><br>  Kami memiliki beberapa "kandidat" untuk peran <b>pemuat data</b> : <br><br><ul><li>  Apache flume </li><li>  Apache nifi </li><li>  Streamset </li></ul><br><h4>  Apache flume </h4><br>  Yang pertama akan kita bicarakan adalah <b>Apache Flume</b> , alat untuk mengangkut data antara berbagai sumber dan repositori. <br><br><img src="https://habrastorage.org/webt/dg/by/a3/dgbya30snrkaceq0y7bvtct59wc.png" alt="gambar"><br><br>  Pro: <br><br><ul><li>  hampir di mana-mana </li><li>  lama digunakan </li><li>  cukup fleksibel dan dapat diperluas </li></ul><br>  Cons: <br><br><ul><li>  konfigurasi tidak nyaman </li><li>  sulit dipantau </li></ul><br>  Adapun konfigurasinya, tampilannya seperti ini: <br><br><img src="https://habrastorage.org/webt/hf/-i/bz/hf-ibz-bp5n8ydo3c1viwsxw9qe.png" alt="gambar"><br><br>  Di atas, kami membuat satu saluran sederhana yang berada di port, mengambil data dari sana dan hanya mencatatnya.  Pada prinsipnya, untuk menggambarkan satu proses, ini masih normal, tetapi ketika Anda memiliki lusinan proses seperti itu, file konfigurasi berubah menjadi neraka.  Seseorang menambahkan beberapa konfigurator visual, tetapi mengapa repot-repot jika ada alat yang membuatnya keluar dari kotak?  Misalnya, NiFi dan StreamSets yang sama. <br><br><h4>  Apache nifi </h4><br>  Bahkan, ia melakukan peran yang sama dengan Flume, tetapi dengan antarmuka visual, yang merupakan nilai tambah besar, terutama ketika ada banyak proses. <br><br>  Beberapa fakta tentang NiFi <br><br><ul><li>  awalnya dikembangkan di NSA; </li><li>  Hortonworks sekarang didukung dan dikembangkan; </li><li>  bagian dari HDF dari Hortonworks; </li><li>  memiliki versi khusus MiNiFi untuk mengumpulkan data dari perangkat. </li></ul><br>  Sistemnya terlihat seperti ini: <br><br><img src="https://habrastorage.org/webt/jz/1k/l7/jz1kl7seqymd9rxx2tog4k88wni.png" alt="gambar"><br><br>  Kami memiliki bidang kreativitas dan tahapan pemrosesan data yang kami lemparkan ke sana.  Ada banyak konektor untuk semua sistem yang memungkinkan, dll. <br><br><h4>  Streamset </h4><br>  Ini juga merupakan sistem kontrol aliran data dengan antarmuka visual.  Ini dikembangkan oleh orang-orang dari Cloudera, mudah diinstal sebagai Parcel di CDH, ia memiliki versi khusus dari SDC Edge untuk mengumpulkan data dari perangkat. <br><br>  Terdiri dari dua komponen: <br><br><ul><li>  SDC - sistem yang melakukan pemrosesan data langsung (gratis); </li><li>  StreamSets Control Hub - pusat kontrol untuk beberapa SDC dengan fitur tambahan untuk pengembangan paylines (berbayar). </li></ul><br>  Itu terlihat seperti ini: <br><br><img src="https://habrastorage.org/webt/kx/3z/jw/kx3zjwqx_ijbfxdlg7hvizllnt4.png" alt="gambar"><br><br>  Momen tidak menyenangkan - StreamSets memiliki bagian gratis dan berbayar. <br><br><h4>  Bus data </h4><br>  Sekarang mari kita cari tahu di mana kita akan mengunggah data ini.  Pelamar: <br><br><ul><li>  Apache kafka </li><li>  Rabbitmq </li><li>  NATS </li></ul><br>  Apache Kafka adalah pilihan terbaik, tetapi jika Anda memiliki RabbitMQ atau NATS di perusahaan Anda, dan Anda perlu menambahkan sedikit analitik, maka menggunakan Kafka dari awal tidak akan sangat menguntungkan. <br><br>  Dalam semua kasus lain, Kafka adalah pilihan yang bagus.  Bahkan, ini adalah broker pesan dengan penskalaan horizontal dan bandwidth besar.  Ini sangat terintegrasi ke dalam seluruh ekosistem alat untuk bekerja dengan data dan dapat menahan beban berat.  Ini memiliki antarmuka universal dan merupakan sistem sirkulasi pemrosesan data kami. <br><br>  Di dalam, Kafka dibagi menjadi Topik - aliran data terpisah dari pesan dengan skema yang sama atau, setidaknya, dengan tujuan yang sama. <br><br>  Untuk membahas nuansa berikutnya, Anda harus ingat bahwa sumber data mungkin sedikit berbeda.  Format data sangat penting: <br><br><img src="https://habrastorage.org/webt/fq/kn/ci/fqkncilmsox7hmoye289ronvbyk.png" alt="gambar"><br><br>  Format serialisasi data Apache Avro patut disebutkan secara khusus.  Sistem menggunakan JSON untuk menentukan struktur data (skema) yang diserialisasi ke dalam <b>format biner yang ringkas</b> .  Oleh karena itu, kami menyimpan sejumlah besar data, dan serialisasi / deserialisasi lebih murah. <br><br>  Semuanya tampak baik-baik saja, tetapi keberadaan file terpisah dengan sirkuit menimbulkan masalah, karena kita perlu bertukar file antara sistem yang berbeda.  Tampaknya itu sederhana, tetapi ketika Anda bekerja di departemen yang berbeda, orang-orang di ujung sana dapat mengubah sesuatu dan tenang, dan semuanya akan rusak untuk Anda. <br><br>  Agar tidak mentransfer semua file ini ke flash drive, floppy disk dan lukisan gua, ada layanan khusus - Schema registry.  Ini adalah layanan untuk menyinkronkan skema-avro antara layanan yang menulis dan membaca dari Kafka. <br><br><img src="https://habrastorage.org/webt/do/jf/qd/dojfqd1m6nf5wr53xmvmg5hee_a.png" alt="gambar"><br><br>  Dalam hal Kafka, produsen adalah orang yang menulis, konsumen adalah orang yang mengkonsumsi (membaca) data. <br><br><h4>  Gudang data </h4><br>  Penantang (pada kenyataannya, ada banyak opsi lagi, tetapi hanya mengambil beberapa): <br><br><ul><li>  HDFS + Sarang </li><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  Sebelum memilih repositori, ingat <b>idempotensi</b> apa <b>itu</b> .  Wikipedia mengatakan bahwa idempotensi (idem Latin - yang sama + potens - mampu) - properti dari suatu objek atau operasi ketika menerapkan operasi ke objek lagi, memberikan hasil yang sama seperti yang pertama.  Dalam kasus kami, proses pemrosesan streaming harus dibangun sehingga saat mengisi ulang sumber data, hasilnya tetap benar. <br><br>  <b>Cara mencapai ini</b> dalam sistem streaming: <br><br><ul><li>  mengidentifikasi id unik (dapat berupa gabungan) </li><li>  gunakan id ini untuk mendeduplikasi data </li></ul><br>  Penyimpanan HDFS + Hive <b>tidak memberikan idempotensi</b> untuk streaming rekaman "out of the box", jadi kami memiliki: <br><br><ul><li>  Kudu + Impala </li><li>  Clickhouse </li></ul><br>  <b>Kudu</b> adalah repositori yang cocok untuk kueri analitik, tetapi dengan Kunci Utama, untuk deduplikasi.  <b>Impala</b> adalah antarmuka SQL ke repositori ini (dan beberapa lainnya). <br><br>  Adapun ClickHouse, ini adalah database analitik dari Yandex.  Tujuan utamanya adalah analitik pada tabel yang diisi dengan aliran besar data mentah.  Dari kelebihan - ada mesin ReplacingMergeTree untuk deduplikasi kunci (deduplikasi dirancang untuk menghemat ruang dan dapat meninggalkan duplikat dalam beberapa kasus, Anda perlu mempertimbangkan <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/">nuansa</a> ). <br><br>  Tetap menambahkan beberapa kata tentang <b>Divolte</b> .  Jika Anda ingat, kami berbicara tentang fakta bahwa beberapa data perlu ditangkap.  Jika Anda perlu mengatur analisis untuk portal dengan cepat dan mudah, maka Divolte adalah layanan yang sangat baik untuk menangkap acara pengguna di halaman web melalui JavaScript. <br><br><img src="https://habrastorage.org/webt/wo/7m/ol/wo7molaoelkzjjatvyzzaihkrmm.png" alt="gambar"><br><br><h3>  Contoh praktis </h3><br>  Apa yang kita coba lakukan?  <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4016">Mari kita coba membangun</a> saluran untuk mengumpulkan data Clickstream secara real time.  <b>Clickstream</b> adalah jejak virtual yang ditinggalkan pengguna saat berada di situs Anda.  Kami akan mengambil data menggunakan Divolte, dan menulisnya di Kafka. <br><br><img src="https://habrastorage.org/webt/uj/mt/h-/ujmth-4jh9catnqqxt0ikp0w9xa.png" alt="gambar"><br><br>  Anda perlu Docker untuk bekerja, ditambah Anda harus mengkloning <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example">repositori berikut</a> .  Segala sesuatu yang terjadi akan diluncurkan dalam wadah.  Untuk secara konsisten menjalankan banyak wadah sekaligus, <a href="">docker-compose.yml</a> akan digunakan.  Selain itu, ada <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/blob/master/Dockerfile">Dockerfile yang</a> mengkompilasi StreamSet kami dengan dependensi tertentu. <br><br>  Ada juga tiga folder: <br><br><ol><li>  <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">data clickhouse</a> akan ditulis ke <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/clickhouse-data">clickhouse-data</a> </li><li>  persis ayah yang sama ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/sdc-data">sdc-data</a> ) yang akan kita miliki untuk StreamSets, di mana sistem dapat menyimpan konfigurasi </li><li>  folder ketiga ( <a href="https://github.com/Gorini4/divolte-streamsets-clickhouse-example/tree/master/examples">contoh</a> ) termasuk file permintaan dan file konfigurasi pipa untuk StreamSets </li></ol><br><br><img src="https://habrastorage.org/webt/-r/xt/2r/-rxt2rlhxzdqqeyz7qv3z3dugxw.png" alt="gambar"><br><br>  Untuk memulai, masukkan perintah berikut: <br><br><pre><code class="bash hljs">docker-compose up</code> </pre> <br>  Dan kami menikmati betapa lambat tapi pasti wadah mulai.  Setelah memulai, kita bisa pergi ke alamat <a href="http://localhost:8290/">http: // localhost: 18630 â€‹â€‹/</a> dan segera sentuh Divolte: <br><br><img src="https://habrastorage.org/webt/cc/1m/im/cc1mimjszmzdoyiwb-elb2c_igu.png" alt="gambar"><br><br>  Jadi, kami memiliki Divolte, yang telah menerima beberapa acara dan merekamnya di Kafka.  Mari kita coba menghitungnya menggunakan StreamSets: <a href="http://localhost:18630/">http: // localhost: 18630 â€‹â€‹/</a> (kata sandi / login - admin / admin). <br><br><img src="https://habrastorage.org/webt/fc/hk/qz/fchkqzqe9pzpdws-xilz3ftdcb8.png" alt="gambar"><br><br>  Agar tidak menderita, lebih baik <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4425">mengimpor</a> <b>Pipeline</b> , menamainya, misalnya, <b>clickstream_pipeline</b> .  Dan dari folder contoh kita mengimpor <b>clickstream.json</b> .  Jika semuanya baik-baik saja, <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4701">kita akan melihat gambar berikut</a> : <br><br><img src="https://habrastorage.org/webt/o8/z9/x5/o8z9x5eaacqkehcfcgxattekpba.png" alt="gambar"><br><br>  Jadi, kami membuat koneksi ke Kafka, mendaftarkan Kafka mana yang kami butuhkan, mendaftarkan topik mana yang menarik minat kami, kemudian memilih bidang yang menarik minat kami, kemudian menguras Kafka, mendaftarkan Kafka mana dan topik mana.  Perbedaannya adalah bahwa dalam satu kasus, format Data adalah Avro, dan yang kedua hanya JSON. <br><br>  Mari kita lanjutkan.  Kami dapat, misalnya, <a href="https://youtu.be/NFjL8YQKuVg%3Ft%3D4768">membuat pratinjau</a> yang menangkap catatan tertentu secara real time dari Kafka.  Lalu kami menuliskan semuanya. <br><br>  Setelah diluncurkan, kita akan melihat bahwa aliran acara terbang ke Kafka, dan ini terjadi secara real time: <br><br><img src="https://habrastorage.org/webt/kr/a7/0n/kra70nxdaplug8-oywal1wb23oi.png" alt="gambar"><br><br>  Sekarang Anda dapat membuat repositori untuk data ini di ClickHouse.  Untuk bekerja dengan ClickHouse, Anda dapat menggunakan klien asli yang sederhana dengan menjalankan perintah berikut: <br><br><pre> <code class="bash hljs">docker run -it --rm --network divolte-ss-ch_default yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Harap perhatikan bahwa baris ini menunjukkan jaringan yang ingin Anda sambungkan.  Dan tergantung pada bagaimana Anda memberi nama folder dengan repositori, nama jaringan Anda mungkin berbeda.  Secara umum, perintahnya adalah sebagai berikut: <br><br><pre> <code class="bash hljs">docker run -it --rm --network {your_network_name} yandex/clickhouse-client --host clickhouse</code> </pre> <br>  Daftar jaringan dapat dilihat dengan perintah: <br><br><pre> <code class="bash hljs">docker network ls</code> </pre> <br>  Tidak ada yang tersisa: <br><br>  1. <b>Pertama, "tandatangani" ClickHouse kami ke Kafka</b> , "jelaskan kepadanya" format data apa yang kami butuhkan di sana: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> clickstream_topic ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = Kafka <span class="hljs-keyword"><span class="hljs-keyword">SETTINGS</span></span> kafka_broker_list = <span class="hljs-string"><span class="hljs-string">'kafka:9092'</span></span>, kafka_topic_list = <span class="hljs-string"><span class="hljs-string">'clickstream'</span></span>, kafka_group_name = <span class="hljs-string"><span class="hljs-string">'clickhouse'</span></span>, kafka_format = <span class="hljs-string"><span class="hljs-string">'JSONEachRow'</span></span>;</code> </pre><br>  2. <b>Sekarang kita akan membuat tabel nyata di</b> mana kita akan meletakkan data akhir: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> clickstream ( firstInSession UInt8, <span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span> UInt64, location <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, partyId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, sessionId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, pageViewId <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, eventType <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, userAgentString <span class="hljs-keyword"><span class="hljs-keyword">String</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = ReplacingMergeTree() <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">timestamp</span></span>, pageViewId);</code> </pre> <br>  3. <b>Dan kemudian kami akan memberikan hubungan antara dua tabel ini</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">MATERIALIZED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">VIEW</span></span> clickstream_consumer <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> clickstream <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream_topic;</code> </pre> <br>  4. <b>Dan sekarang kita akan memilih bidang yang diperlukan</b> : <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> clickstream;</code> </pre> <br>  Akibatnya, pilihan dari tabel target akan memberi kita hasil yang kita butuhkan. <br><br><img src="https://habrastorage.org/webt/wn/sr/qe/wnsrqei2fo7iydrf41zattwbddy.png"><br><br>  Itu saja, itu Clickstream paling sederhana yang bisa Anda buat.  Jika Anda ingin menyelesaikan sendiri langkah-langkah di atas, <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">tonton</a> seluruh <a href="https://www.youtube.com/watch%3Fv%3DNFjL8YQKuVg">video</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id477834/">https://habr.com/ru/post/id477834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id477818/index.html">Bagaimana cara menjadi Ilmuwan Data pada tahun 2019</a></li>
<li><a href="../id477820/index.html">VMware, Hyper-V, OpenStack, Kubernetes, Swarm - pemantauan dari satu antarmuka tunggal di Quest Foglight</a></li>
<li><a href="../id477822/index.html">PHP 7.4 dirilis! Bagaimana Badoo Meng-upgrade</a></li>
<li><a href="../id477824/index.html">Mari kita hidup sampai Senin atau bagaimana bertahan Jumat hitam</a></li>
<li><a href="../id477832/index.html">Cara bergaul dengan generasi Z</a></li>
<li><a href="../id477840/index.html">Alat analisis kode statis PVS-Studio sebagai perlindungan terhadap kerentanan zero-day</a></li>
<li><a href="../id477842/index.html">Kisah Gennady Zelenko dan Sergey Popov - populariser teknologi di Uni Soviet</a></li>
<li><a href="../id477844/index.html">5 langkah dari ide hingga aplikasi praktis pembelajaran mesin dengan SAP Data Intelligence</a></li>
<li><a href="../id477846/index.html">Intisari acara untuk profesional SDM di bidang TI untuk Desember 2019</a></li>
<li><a href="../id477848/index.html">Rahasia kecil hati besar: kardiogram paus biru pertama dalam sejarah</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>