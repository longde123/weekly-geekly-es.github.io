<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëèüèø üë©üèø‚Äçü§ù‚Äçüë®üèª üåã Las redes neuronales prefieren las texturas y c√≥mo lidiar con ellas. üë®üèª‚Äç‚öñÔ∏è üéÆ ü•ñ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recientemente, ha habido varios art√≠culos que critican a ImageNet, quiz√°s el conjunto de im√°genes m√°s famoso utilizado para entrenar redes neuronales....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Las redes neuronales prefieren las texturas y c√≥mo lidiar con ellas.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/453788/"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  Recientemente, ha habido varios art√≠culos que critican a ImageNet, quiz√°s el conjunto de im√°genes m√°s famoso utilizado para entrenar redes neuronales. </p><br><p>  En el primer art√≠culo, La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aproximaci√≥n de CNN con modelos de bolsa de caracter√≠sticas locales funciona sorprendentemente bien en ImageNet, los</a> autores toman un modelo similar a la bolsa de palabras y usan fragmentos de la imagen como "palabras".  Estos fragmentos pueden tener hasta 9x9 p√≠xeles.  Y al mismo tiempo, en dicho modelo, donde la informaci√≥n sobre la disposici√≥n espacial de estos fragmentos est√° completamente ausente, los autores obtienen una precisi√≥n del 70 al 86% (por ejemplo, la precisi√≥n de un ResNet-50 regular es ~ 93%). </p><br><p>  En el segundo art√≠culo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CNNs entrenados en ImageNet est√°n sesgados hacia la textura, los</a> autores concluyen que el conjunto de datos de ImageNet en s√≠ mismo y la forma en que las personas y las redes neuronales perciben las im√°genes son culpables de esto y sugieren usar un nuevo conjunto de datos: Stylized-ImageNet. </p><br><p>  Con m√°s detalle sobre lo que la gente ve en las im√°genes y qu√© redes neuronales <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  El conjunto de datos ImageNet comenz√≥ a crearse en 2006 por los esfuerzos del profesor Fei-Fei Li y contin√∫a evolucionando hasta nuestros d√≠as.  Por el momento, contiene alrededor de 14 millones de im√°genes pertenecientes a m√°s de 20 mil categor√≠as diferentes. </p><br><p>  Desde 2010, un subconjunto de este conjunto de datos, conocido como ImageNet 1K con ~ 1 mill√≥n de im√°genes y miles de clases, se ha utilizado en el ImageNet Large Scale Visual Recognition Challenge (ILSVRC).  En esta competencia, en 2012, AlexNet, una red neuronal convolucional, dispar√≥ con una precisi√≥n de top 1 del 60% y top 5 del 80%. <br>  Es en este subconjunto del conjunto de datos que las personas del entorno acad√©mico miden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">su SOTA</a> cuando ofrecen nuevas arquitecturas de red. </p><br><p>  Un poco sobre el proceso de aprendizaje en este conjunto de datos.  Hablaremos sobre el protocolo de capacitaci√≥n en ImageNet en el entorno acad√©mico.  Es decir, cuando en el art√≠culo se nos muestran los resultados de un bloque SE, una red ResNeXt o DenseNet, el proceso se ve as√≠: la red aprende durante 90 eras, la velocidad de aprendizaje disminuye en las eras 30 y 60, cada 10 veces, como optimizador se selecciona un SGD ordinario con una peque√±a disminuci√≥n de peso, solo se usan RandomCrop y HorizontalFlip de los aumentos, la imagen generalmente se redimensiona a 224x224 p√≠xeles. </p><br><p>  Aqu√≠ hay un ejemplo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">script pytorch</a> para capacitaci√≥n en ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Volvamos a los art√≠culos mencionados anteriormente.  En el primero de ellos, los autores quer√≠an un modelo que fuera m√°s f√°cil de interpretar que las redes profundas comunes.  Inspirados en la idea de los modelos de bolsa de caracter√≠sticas, crean su propia familia de modelos: BagNets.  Utilizando como base la red ResNet-50 habitual. </p><br><p>  Reemplazando algunas convoluciones de 3x3 con 1x1 en ResNet-50, aseguran que el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">campo receptivo de</a> neuronas en la √∫ltima capa convolucional se reduzca significativamente, hasta 9x9 p√≠xeles.  Por lo tanto, limitan la informaci√≥n disponible para una neurona individual a un fragmento muy peque√±o de toda la imagen, un parche de varios p√≠xeles.  Cabe se√±alar que para el ResNet-50 pr√≠stino, el tama√±o del campo receptivo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">es de m√°s de 400 p√≠xeles, lo</a> que cubre completamente la imagen, que generalmente cambia a 224x224 p√≠xeles. </p><br><p>  Este parche es el fragmento <strong>m√°ximo</strong> de la imagen de la que el modelo podr√≠a extraer datos espaciales.  Al final del modelo, todos los datos simplemente se resumieron y el modelo de ninguna manera pudo saber d√≥nde se encuentra cada parche en relaci√≥n con otros parches. <br>  En total, se probaron tres variantes de redes con campo receptivo 9x9, 17x17 y 33x33.  Y, a pesar de la completa falta de informaci√≥n espacial, tales modelos pudieron lograr una buena precisi√≥n en la clasificaci√≥n en ImageNet.  La precisi√≥n del Top 5 para los parches 9x9 fue del 70%, para 17x17 - 80%, para 33x33 - 86%.  A modo de comparaci√≥n, la precisi√≥n de ResNet-50 top-5 es aproximadamente del 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  La estructura del modelo se muestra en la figura anterior.  Cada parche de qxqx3 p√≠xeles cortados de la imagen se convierte en un vector 2048. A continuaci√≥n, este vector se alimenta a la entrada de un clasificador lineal, que produce puntuaciones para cada una de las 1000 clases.  Al recopilar las puntuaciones de cada parche en una matriz 2D, puede obtener un mapa de calor para cada clase y cada p√≠xel de la imagen original.  Los puntajes finales de la imagen se obtuvieron sumando el mapa de calor de cada clase. </p><br><p>  Ejemplos de mapas de calor para algunas clases: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="Mapas de calor"><br>  Como puede ver, la mayor contribuci√≥n al beneficio de una clase en particular se realiza mediante parches ubicados en los bordes de los objetos.  Los parches del fondo son casi ignorados.  Hasta ahora, todo va bien. </p><br><p>  Veamos los parches m√°s informativos: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="Parches informativos"></p><br><p>  Por ejemplo, los autores tomaron cuatro clases.  Para cada uno de ellos, se seleccionaron 2x7 parches m√°s significativos (es decir, parches donde el puntaje de esta clase fue el m√°s alto).  La fila superior de 7 parches se toma de im√°genes de solo la clase correspondiente, la inferior, de toda la muestra de im√°genes. </p><br><p>  Lo que se puede ver en estas im√°genes es notable.  Por ejemplo, para la clase de tencas (tencas, peces), los dedos son un rasgo caracter√≠stico.  S√≠, dedos humanos comunes sobre un fondo verde.  Y todo porque hay un pescador en casi todas las im√°genes con esta clase, que, de hecho, sostiene este pez en sus manos, mostrando un trofeo. </p><br><div class="spoiler">  <b class="spoiler_title">Ejemplos de ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  Para las computadoras port√°tiles, un rasgo caracter√≠stico son las teclas de letras.  Las teclas de m√°quina de escribir tambi√©n cuentan para esta clase. </p><br><p>  Un rasgo caracter√≠stico de la portada de un libro son las letras sobre un fondo de color.  Que sea incluso una inscripci√≥n en una camiseta o en una bolsa. </p><br><p>  Parece que este problema no deber√≠a molestarnos.  Dado que es inherente solo en una clase estrecha de redes con un campo receptivo muy limitado.  Pero adem√°s, los autores calcularon la correlaci√≥n entre logits (salidas de red antes del softmax final) asignado a cada clase BagNet con un campo receptivo diferente, y logits de VGG-16, que tiene un campo receptivo bastante grande.  Y la encontraron bastante alta. </p><br><div class="spoiler">  <b class="spoiler_title">Correlaci√≥n entre BagNets y VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  Los autores se preguntaron si BagNet contiene alguna pista sobre c√≥mo otras redes toman decisiones. </p><br><p>  Para una de las pruebas, utilizaron una t√©cnica como Image Scrambling.  Que consist√≠a en usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un generador de texturas basado en matrices de gramo</a> para componer una imagen donde se guardan las texturas, pero falta informaci√≥n espacial. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="Im√°genes revueltas"></p><br><p>  VGG-16, entrenado en im√°genes normales, hizo frente a esas im√°genes revueltas bastante bien.  Su precisi√≥n de top 5 cay√≥ del 90% al 80%.  Es decir, incluso las redes con un campo receptivo bastante grande prefirieron recordar texturas e ignorar la informaci√≥n espacial.  Por lo tanto, su precisi√≥n no recay√≥ en gran medida en las im√°genes codificadas. </p><br><p>  Los autores realizaron una serie de experimentos en los que compararon qu√© partes de las im√°genes son m√°s significativas para BagNet y otras redes (VGG-16, ResNet-50, ResNet-152 y DenseNet-169).  Todo insinu√≥ que otras redes, como BagNet, dependen de peque√±os fragmentos de im√°genes y cometen los mismos errores al tomar decisiones.  Esto fue especialmente notable para redes no muy profundas como VGG. </p><br><p>  Esta tendencia de las redes a tomar decisiones basadas en texturas, a diferencia de nosotros, las personas que prefieren la forma (ver la figura a continuaci√≥n), llev√≥ a los autores del segundo art√≠culo a crear un nuevo conjunto de datos basado en ImageNet. </p><br><h3 id="stylized-imagenet">  ImageNet estilizada </h3><br><p>  En primer lugar, los autores del art√≠culo que usaron la transferencia de estilo crearon un conjunto de im√°genes donde la forma (datos espaciales) y las texturas en una imagen se contradec√≠an.  Y comparamos los resultados de las personas y las redes de convoluci√≥n profunda de diferentes arquitecturas en un conjunto de datos sintetizados de 16 clases. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  En la figura de la extrema derecha, la gente ve un gato, una red, un elefante. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Comparaci√≥n de los resultados de personas y redes neuronales. </p><br><p>  Como puede ver, las personas al asignar un objeto a una clase en particular se basaban en la forma de los objetos, las redes neuronales en las texturas.  En la figura anterior, la gente vio un gato, una red, un elefante. </p><br><blockquote>  S√≠, aqu√≠ puede encontrar fallas en el hecho de que las redes tambi√©n son algo correctas y esto, por ejemplo, podr√≠a ser un elefante fotografiado de cerca con un tatuaje de un gato querido.  Pero el hecho de que las redes cuando toman decisiones se comportan de manera diferente a las personas, los autores consideraron el problema y comenzaron a buscar formas de resolverlo. </blockquote><p>  Como se mencion√≥ anteriormente, confiando solo en las texturas, la red puede lograr un buen resultado con un 86% de precisi√≥n entre los 5 mejores.  Y no se trata de varias clases, donde las texturas ayudan a clasificar las im√°genes correctamente, sino de la mayor√≠a de las clases. </p><br><p>  El problema est√° en ImageNet, ya que se mostrar√° m√°s adelante que la red puede aprender la forma, pero no lo hace, ya que las texturas son suficientes para este conjunto de datos y las neuronas responsables de las texturas est√°n en capas poco profundas, que son mucho m√°s f√°ciles de entrenar </p><br><p>  Utilizando esta vez un mecanismo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">transferencia de estilo r√°pido AdaIN</a> ligeramente diferente, los autores crearon un nuevo conjunto de datos: ImageNet estilizada.  La forma de los objetos fue tomada de ImageNet y el conjunto de texturas de esta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">competencia en Kaggle</a> .  El script para la generaci√≥n est√° disponible <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">en el enlace</a> . </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  Adem√°s, por brevedad, ImageNet se denominar√° <strong>IN</strong> , ImageNet estilizada como <strong>SIN</strong> . </p><br><p>  Los autores tomaron ResNet-50 y tres BagNet con diferentes campos receptivos y entrenaron en un modelo separado para cada conjunto de datos. </p><br><p>  Y esto es lo que hicieron: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  Lo que vemos aqu√≠.  ResNet-50 entrenado en IN est√° completamente incapacitado en SIN.  Lo que confirma en parte que cuando se entrena en IN, la red se adapta a las texturas e ignora la forma de los objetos.  Al mismo tiempo, el ResNet-50 entrenado en SIN se adapta perfectamente tanto a SIN como a IN.  Es decir, si se ve privado de una ruta simple, la red sigue una ruta dif√≠cil: ense√±a la forma de los objetos. <br>  BagNet finalmente ha comenzado a comportarse como se esperaba, especialmente en parches peque√±os, ya que no tiene nada a lo que aferrarse: la informaci√≥n de textura simplemente falta en el SIN. </p><br><p>  En esas diecis√©is clases que se mencionaron anteriormente, ResNet-50, capacitado en SIN, comenz√≥ a dar respuestas m√°s similares a las que dan las personas: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  Adem√°s de simplemente entrenar ResNet-50 en SIN, los autores trataron de entrenar la red en un conjunto mixto de SIN e IN, incluida la optimizaci√≥n por separado en IN puro. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  Como puede ver, al usar SIN + IN para capacitaci√≥n, los resultados mejoraron no solo en la tarea principal: la clasificaci√≥n de im√°genes en ImageNet, sino tambi√©n en la tarea de detectar objetos en el conjunto de datos PASCAL VOC 2007. </p><br><p>  Adem√°s, las redes capacitadas por SIN se han vuelto m√°s resistentes a diversos ruidos en los datos. </p><br><h3 id="zaklyuchenie">  Conclusi√≥n </h3><br><p>  Incluso ahora, en 2019, despu√©s de siete a√±os de √©xito con AlexNet, cuando las redes neuronales se utilizan ampliamente en la visi√≥n por computadora, cuando ImageNet 1K se convirti√≥ de hecho en el est√°ndar para evaluar el rendimiento de los modelos en el entorno acad√©mico, el mecanismo de c√≥mo las redes neuronales toman decisiones no est√° del todo claro .  Y c√≥mo los conjuntos de datos en los que se formaron estas redes influyen en esto. </p><br><p>  Los autores del primer art√≠culo intentaron arrojar luz sobre c√≥mo se toman tales decisiones en redes con arquitectura de bolsa de caracter√≠sticas con un campo receptivo limitado, que es m√°s f√°cil de interpretar.  Y, al comparar las respuestas de BagNet y las redes neuronales profundas habituales, llegamos a la conclusi√≥n de que los procesos de toma de decisiones en ellas son bastante similares. </p><br><p>  Los autores del segundo art√≠culo compararon c√≥mo las personas y las redes neuronales perciben im√°genes en las que la forma y las texturas se contradicen entre s√≠.  Y sugirieron usar un nuevo conjunto de datos, Stylized ImageNet, para reducir las diferencias de percepci√≥n.  Despu√©s de recibir como bonificaci√≥n un aumento en la precisi√≥n de la clasificaci√≥n en ImageNet y la detecci√≥n en conjuntos de datos de terceros. </p><br><p>  La conclusi√≥n principal se puede hacer de la siguiente manera: las redes que aprenden de las im√°genes, que tienen la capacidad de recordar propiedades espaciales de los objetos de mayor nivel, prefieren una forma m√°s f√°cil de lograr el objetivo: sobreajustar las texturas.  Si el conjunto de datos en el que entrenan lo permite. </p><br><p>  Adem√°s del inter√©s acad√©mico, el problema del sobreajuste de texturas es importante para todos los que usamos modelos pre-entrenados para transferir el aprendizaje en sus tareas. <br>  Una consecuencia importante de todo esto para nosotros es que no debe confiar en el peso de los modelos que generalmente se entrenan previamente en ImageNet, ya que para la mayor√≠a de ellos se utilizaron aumentos bastante simples, que de ninguna manera contribuyen a deshacerse del sobreajuste.  Y es mejor, si es posible, tener modelos entrenados con aumentos m√°s serios o ImageNet + ImageNet estilizados en el nido.  Para poder comparar cu√°l es el m√°s adecuado para nuestra tarea actual. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/453788/">https://habr.com/ru/post/453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../453776/index.html">Bombeamos dise√±adores en la empresa: de junior a director de arte</a></li>
<li><a href="../453778/index.html">C√≥mo creamos un banco en l√≠nea para negocios. Primera parte: cambio de marca</a></li>
<li><a href="../453780/index.html">¬øC√≥mo elegir un tel√©fono Grandstream SIP, en general y en particular?</a></li>
<li><a href="../453782/index.html">Infinito UIScrollView</a></li>
<li><a href="../453784/index.html">C√≥mo DevOps Specialist es v√≠ctima de la automatizaci√≥n</a></li>
<li><a href="../453790/index.html">"El cliente se fue, ¬øes para siempre?" C√≥mo contar la p√©rdida de clientes en SaaS y qu√© hay de malo con las m√©tricas b√°sicas</a></li>
<li><a href="../453792/index.html">Sistemas de recomendaci√≥n: ideas, enfoques, tareas.</a></li>
<li><a href="../453796/index.html">¬øLa gente necesita matem√°ticas?</a></li>
<li><a href="../453800/index.html">C√≥mo resolver "Buscaminas" (y hacerlo mejor)</a></li>
<li><a href="../453804/index.html">El libro "Competitividad y concurrencia en la plataforma .NET. Patrones de dise√±o efectivos "</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>