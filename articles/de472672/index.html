<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õΩÔ∏è üö£üèΩ üç∂ Die √úberschrift "Artikel f√ºr Sie lesen." Juli - September 2019 üë¥ üßîüèº üíÉ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Wir ver√∂ffentlichen weiterhin Rezensionen zu wissenschaftlichen Artikeln von Mitgliedern der Open Data Science-Community √ºber den Kanal #ar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Die √úberschrift "Artikel f√ºr Sie lesen." Juli - September 2019</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/472672/"><img src="https://habrastorage.org/webt/gx/-y/xl/gx-yxlo7xiz-5y8krpyoj3rgswq.png"><br><p><br>  Hallo habr  Wir ver√∂ffentlichen weiterhin Rezensionen zu wissenschaftlichen Artikeln von Mitgliedern der Open Data Science-Community √ºber den Kanal #article_essense.  Wenn Sie sie vor allen anderen erhalten m√∂chten, treten Sie der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Community bei</a> ! </p><br><p>  Artikel f√ºr heute: </p><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schichtrotation: ein √ºberraschend starker Indikator f√ºr die Verallgemeinerung in tiefen Netzwerken?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(Universit√© catholique de Louvain, Belgien, 2018)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parametereffizientes Transferlernen f√ºr NLP (Google Research, Jagiellonian University, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">RoBERTa: Ein robust optimierter BERT-Pretraining-Ansatz (University of Washington, Facebook AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">EfficientNet: √úberdenken der Modellskalierung f√ºr Faltungs-Neuronale Netze (Google Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wie das Gehirn von der bewussten zur unterschwelligen Wahrnehmung √ºbergeht (USA, Argentinien, Spanien, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gro√üe Speicherschichten mit Produktschl√ºsseln (Facebook AI Research, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Machen wir wirklich gro√üe Fortschritte?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine besorgniserregende Analyse der j√ºngsten neuronalen Empfehlungsans√§tze (Politecnico di Milano, Universit√§t Klagenfurt, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Omni-Scale Feature Learning zur erneuten Identifizierung von Personen (Universit√§t von Surrey, Queen Mary University, Samsung AI, 2019)</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Neuronale Reparametrisierung verbessert die Strukturoptimierung (Google Research, 2019)</a> </li></ol><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Links zu fr√ºheren Sammlungen der Serie:</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Januar - Juni 2019</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Februar - M√§rz 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dezember 2017 - Januar 2018</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Oktober - November 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">September 2017</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">August 2017</a> </li></ul></div></div><br><h3 id="1-layer-rotation-a-surprisingly-powerful-indicator-of-generalization-in-deep-networks">  1. Schichtrotation: Ein √ºberraschend starker Indikator f√ºr die Verallgemeinerung in tiefen Netzwerken? </h3><br><p>  Autoren: Simon Carbonnelle, Christophe De Vleeschouwer (Universit√© catholique de Louvain, Belgien, 2018) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: Svyatoslav Skoblov (in slack error_derivative) </p><br><img src="https://habrastorage.org/webt/tt/n5/g8/ttn5g8j27-ihyqnwk0rowhg8oie.png" width="500" height="250"><br><p><br>  In diesem Artikel machten die Autoren auf eine recht einfache Beobachtung aufmerksam: den Kosinusabstand zwischen den Schichtgewichten w√§hrend der Initialisierung und nach dem Training (der Vorgang des Erh√∂hens des Abstands w√§hrend des Trainings wird als Schichtrotation bezeichnet).  Die Herren sagen, dass in den meisten Experimenten Netzwerke, die in allen Schichten einen Abstand von 1 erreicht haben, anderen Konfigurationen durchweg √ºberlegen sind.  In diesem <strong>Artikel wird</strong> auch der <strong>Layca-</strong> Algorithmus (Controlled Amount of Weight Rotation auf Gewichtsebene) vorgestellt, mit dem diese schichtweise Lernrate zur Steuerung derselben Schichtrotation verwendet werden kann.  Tats√§chlich unterscheidet es sich vom √ºblichen SGD-Algorithmus durch das Vorhandensein einer orthogonalen Projektion und Normalisierung.  Eine detaillierte Auflistung des Algorithmus zusammen mit dem Trainingsschema finden Sie im Artikel. </p><br><p>  Die Hauptidee, auf die die Autoren schlie√üen, ist: Je <strong>gr√∂√üer die Schichtrotationen sind, desto besser ist die Generalisierungsleistung</strong> .  Der gr√∂√üte Teil des Artikels ist eine Aufzeichnung von Experimenten, bei denen verschiedene Trainingsszenarien untersucht wurden: MNIST, CIFAR-10 / CIFAR-100, winziges ImageNet mit unterschiedlichen Architekturen, von einem einschichtigen Netzwerk bis zur ResNet-Familie. </p><br><p>  Eine Reihe von Experimenten wurde in mehrere Phasen unterteilt: </p><br><ol><li>  <strong>Vanilla SGD</strong> Es <strong>stellte sich</strong> heraus, dass das Verhalten der Skalen insgesamt mit der Hypothese √ºbereinstimmt (gro√üe √Ñnderungen im Abstand entsprachen den besten metrischen Werten), es wurden jedoch auch Probleme festgestellt: Die Schichtrotation wurde lange vor den gew√ºnschten Werten gestoppt;  Es wurde auch eine Instabilit√§t beim √Ñndern der Entfernung festgestellt. </li><li>  <strong>SGD + Gewichtsabfall Durch</strong> Verringern der <strong>Gewichtsnorm wurde</strong> das Trainingsbild erheblich verbessert: Die meisten Schichten erreichten die maximale Entfernung, und die Testleistung √§hnelt der vorgeschlagenen Layca.  Der zweifelsfreie Vorteil der Methode des Autors ist das Fehlen eines zus√§tzlichen Hyperparameters. </li><li>  <strong>LR-Aufw√§rm√ºbungen</strong> Es stellte sich heraus, dass das Aufw√§rmen SGD hilft, das Problem der instabilen Schichtrotation zu √ºberwinden, jedoch keine Auswirkungen auf Layca hat. </li><li>  <strong>Adaptive Gradientenmethoden</strong> Zus√§tzlich zu der bekannten Wahrheit (dass es mit diesen Methoden schwieriger ist, den Grad der Verallgemeinerung zu erreichen, den SGD + Gewichtsabfall ergeben kann), stellte sich heraus, dass die Auswirkungen der Schichtrotation sehr unterschiedlich sind: Die erste Erh√∂hung der Rotation in den letzten Schichten, w√§hrend SGD in den Anfangsschichten .  Die Autoren weisen darauf hin, dass dies die Gemeinheit adaptiver Methoden sein k√∂nnte.  Und sie schlagen vor, Layca in Verbindung mit ihnen zu verwenden (Verbesserung der F√§higkeit zur Verallgemeinerung adaptiver Methoden und Beschleunigung des Lernens bei SGD). </li></ol><br><p>  Der Artikel schlie√üt mit einem Versuch, das Ph√§nomen zu interpretieren.  Zu diesem Zweck trainierten die Autoren ein Netzwerk mit einer verborgenen Schicht auf einer abgespeckten Version von MNIST. Anschlie√üend visualisierten sie zuf√§llige Neuronen und kamen zu einer logischen Schlussfolgerung: Ein h√∂herer Grad an Schichtrotation entspricht einem geringeren Effekt der Initialisierung und einer besseren Untersuchung der Merkmale, was zu einer verbesserten Generalisierung beitr√§gt. </p><br><p>  Der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code des implementierten Algorithmus (tf / keras)</a> und der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code zur Reproduktion von Experimenten werden</a> hochgeladen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">.</a> </p><br><h3 id="2-parameter-efficient-transfer-learning-for-nlp">  2. Parametereffizientes Transferlernen f√ºr NLP </h3><br><p>  Autoren des Artikels: Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly (Google Research, Jagiellonen-Universit√§t, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: Alexey Karnachev (in lockerem zhirzemli) </p><br><img src="https://habrastorage.org/webt/ka/lg/gp/kalggpbjkmd8zc7ep451lysxpc8.png"><br><p><br>  Hier bieten die Herren eine einfache, aber effektive Feinabstimmungstechnik f√ºr NLP-Modelle (in diesem Fall BERT) an.  Die Idee ist, Lernschichten (Adapter) direkt in das Netzwerk einzubetten.  Jede solche Schicht ist ein Netzwerk mit einem Engpass, das die latenten Zust√§nde des urspr√ºnglichen Modells an eine bestimmte nachgelagerte Aufgabe anpasst.  Die Gewichte des Originalmodells bleiben wiederum eingefroren. </p><br><p>  <strong>Motivation</strong> <br>  Unter den Bedingungen des Streaming-Trainings (oder des Near-Online-Trainings), bei denen es viele Downstream-Aufgaben gibt, m√∂chte ich nicht wirklich das gesamte Modell einreichen.  Erstens ist es f√ºr eine lange Zeit schwierig, und zweitens ist es schwierig, und drittens muss das Modell, selbst wenn es eng ist, irgendwie gespeichert werden: um es zu sichern oder im Speicher zu behalten.  Und wir werden dieses Modell nicht f√ºr die folgende Aufgabe wiederverwenden k√∂nnen: Jedes Mal m√ºssen wir auf eine neue Weise abstimmen.  Infolgedessen k√∂nnen wir versuchen, die verborgenen Netzwerkzust√§nde an das aktuelle Problem anzupassen.  Dar√ºber hinaus bleibt das urspr√ºngliche Modell unber√ºhrt, und die Adapter selbst sind viel umfangreicher als das Hauptmodell (~ 4% der Gesamtzahl der Parameter). </p><br><p>  <strong>Implementierung</strong> <br>  Das Problem wird auf unglaublich einfache Weise gel√∂st: Wir f√ºgen jeder Ebene des Modells 2 Adapter hinzu.  Vor der Schichtnormalisierung in transformatorbasierten Modellen erfolgt eine Sprungverbindung: Der transformierte Eingang (aktueller verborgener Zustand) wird zum urspr√ºnglichen Eingang hinzugef√ºgt. </p><br><p>  In jeder Transformatorschicht befinden sich zwei solcher Abschnitte: einer nach Mehrkopfaufmerksamkeit, der zweite nach Vorw√§rtskopplung.  Somit werden die verborgenen Zust√§nde dieser Abschnitte zus√§tzlich durch den Adapter geleitet: ein flaches Netzwerk mit einer verborgenen Schicht mit einem Engpass und einer Ausgabe mit derselben Dimension wie die Eingabe.  Nichtlinearit√§t wird auf den Engpasszustand angewendet, und Eingabe (Sprungverbindung) wird zur Ausgabe hinzugef√ºgt.  Es stellt sich heraus, dass die Gesamtzahl der trainierten Parameter 2 md + m + d betr√§gt, wobei d die Dimension des verborgenen Zustands des Originalmodells und m die Gr√∂√üe des Engpasses des Adapters ist.  Es stellt sich heraus, dass f√ºr das BERT-Basismodell (12 Schichten, 110 Millionen Parameter) und f√ºr die Gr√∂√üe des Adapter-Bottlneck'a 128 4,3% der Gesamtzahl der Parameter erhalten werden </p><br><p>  <strong>Ergebnisse</strong> <br>  Der Vergleich wurde mit der vollst√§ndigen Modellabstimmung durchgef√ºhrt.  Bei allen Aufgaben zeigte dieser Ansatz einen geringen Verlust an Metriken (im Durchschnitt weniger als 1 Punkt), wobei die Anzahl der trainierten Gewichte 3% der Gesamtzahl betrug.  Ich werde die Aufgaben selbst nicht auflisten, es gibt viele davon, es gibt ein Tablet im Artikel. </p><br><p>  <strong>Feinabstimmung</strong> <br>  In diesem Modell ist nur der Adapterteil abgestimmt (+ der Ausgabeklassifikator selbst).  F√ºr Adapterskalen schlagen sie eine identit√§tsnahe Initialisierung vor.  Somit wird ein nicht trainiertes Modell die verborgenen Netzwerkzust√§nde in keiner Weise √§ndern, und dies wird es bereits w√§hrend des Trainings des Modells erm√∂glichen, zu entscheiden, welche Zust√§nde f√ºr die Aufgabe angepasst und welche unver√§ndert bleiben sollen. </p><br><p>  Die Lernrate empfiehlt mehr als bei der Standard-BERT-Feinabstimmung.  Pers√∂nlich hat 1e-04 lr bei meiner Aufgabe gut funktioniert.  Au√üerdem explodiert das Modell (bereits pers√∂nlich meine Beobachtung) w√§hrend des Abstimmungsprozesses fast immer Gradienten, sodass Sie daran denken m√ºssen, Clipping durchzuf√ºhren.  Optimierer - Adam mit Aufw√§rmen 10% </p><br><p>  <strong>Code</strong> <br>  Der Code in ihrem Artikel ist beigef√ºgt.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Implementierung auf Tensorflow</a> . <br>  F√ºr Torch gab der Autor der Rezension <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pytorch-Transformatoren ab und f√ºgte eine Adapterschicht hinzu</a> (am Anfang der Datei README.md befindet sich ein kleines Starthandbuch). </p><br><h3 id="3-roberta-a-robustly-optimized-bert-pretraining-approach">  3. RoBERTa: Ein robust optimierter BERT-Pretraining-Ansatz </h3><br><p>  Artikelautoren: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov (Universit√§t Washington, Facebook AI, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: Artem Rodichev (in Slack Fuckai) </p><br><p>  Die Qualit√§t der BERT-Modelle wurde dramatisch gesteigert, der erste Platz in der GLUE-Rangliste und SOTA bei vielen NLP-Aufgaben.  Sie schlugen eine Reihe von M√∂glichkeiten vor, um das BERT-Modell so gut wie m√∂glich zu trainieren, ohne die Modellarchitektur selbst zu √§ndern. </p><br><p>  Hauptunterschiede zum Original-BERT: </p><br><ol><li>  Der Zugbau wurde um das Zehnfache von 16 GB Rohtext auf 160 GB erh√∂ht </li><li>  Dynamische Maskierung f√ºr jede Probe </li><li>  Die Verwendung der Vorhersage des Verlusts im n√§chsten Satz wurde entfernt </li><li>  Die Gr√∂√üe der Mini-Charge wurde von 256 Proben auf 8 KB erh√∂ht </li><li>  Verbesserte BPE-Codierung durch √úbersetzung der Datenbank von Unicode in Bytes. </li></ol><br><p>  Das beste endg√ºltige Modell wurde 5 Tage lang auf 1024 Nvidia V100-Karten (128 DGX-1-Server) trainiert. </p><br><p>  <strong>Das Wesentliche des Ansatzes:</strong> </p><br><p>  <em>Daten.</em>  Zus√§tzlich zu den Wiki-Shells und BookCorpus (insgesamt 16 GB), in denen das urspr√ºngliche BERT unterrichtet wurde, wurden drei weitere gr√∂√üere Shells hinzugef√ºgt, alle in englischer Sprache: </p><br><ol><li>  SS-News 63 Millionen Nachrichten in 2,5 Jahren auf 76 GB </li><li>  OpenWebText ist das Framework, auf dem OpenAI das GPT2-Modell beigebracht wurde.  Dies sind gecrawlte Artikel, zu denen Links in Posts auf einem Reddit mit mindestens drei Updates gegeben wurden.  38 GB Daten </li><li>  Geschichten - 31 GB CommonCrawl Story Case </li></ol><br><p>  <em>Dynamische Maskierung.</em>  Im urspr√ºnglichen BERT sind 15% der Token in jeder Probe maskiert, und diese Token werden unter Verwendung des nicht maskierten Teils der Sequenz vorhergesagt.  W√§hrend der Vorverarbeitung wird f√ºr jede Probe einmal eine Maske generiert, die sich nicht √§ndert.  Gleichzeitig kann dieselbe Probe im Zug je nach Anzahl der Epochen im K√∂rper mehrmals auftreten.  Die Idee der dynamischen Maskierung besteht darin, jedes Mal eine neue Maske f√ºr die Sequenz zu erstellen, anstatt bei der Vorverarbeitung eine feste zu verwenden. </p><br><p>  <em>Ziel der n√§chsten Satzvorhersage.</em>  Lassen Sie uns einfach dieses Objektiv abschneiden und sehen, ob es schlimmer wurde?  Ist es besser geworden oder ist es auch geblieben - bei SQuAD-, MNLI-, SST- und RACE-Aufgaben? </p><br><p>  <em>Erh√∂hen Sie die Gr√∂√üe der Mini-Charge.</em>  An vielen Stellen, insbesondere in der maschinellen √úbersetzung, wurde gezeigt, dass die Endergebnisse des Zuges umso besser sind, je gr√∂√üer die Mini-Charge ist.  Sie zeigten, dass, wenn Sie den Minibatch von 256 Proben wie im urspr√ºnglichen BERT auf 2k und dann auf 8k erh√∂hen, die Ratlosigkeit bei der Validierung abnimmt und die Metriken f√ºr MNLI und SST-2 zunehmen. </p><br><p>  <em>BPE</em>  Die BPE aus der urspr√ºnglichen BERT-Implementierung verwendet Unicode-Zeichen als Basis f√ºr Unterworteinheiten.  Dies f√ºhrt dazu, dass in gro√üen und unterschiedlichen F√§llen ein erheblicher Teil des W√∂rterbuchs von einzelnen Unicode-Zeichen belegt wird.  OpenAI in GPT2 schlug vor, nicht Unicode-Zeichen, sondern Bytes als Basis f√ºr Unterw√∂rter zu verwenden.  Wenn wir ein 50k BPE-W√∂rterbuch verwenden, haben wir keine unbekannten Token.  Im Vergleich zum urspr√ºnglichen BERT ist die Modellgr√∂√üe f√ºr das Basismodell um 15 Millionen Parameter und f√ºr gro√üe Modelle um 20 Millionen gewachsen, dh um 5-10% mehr. </p><br><p>  <strong>Ergebnisse:</strong> <br>  BERT-Large und XLNet-Large werden als Vergleichsmodelle verwendet.  RoBERTa selbst hat die gleichen Parameter wie BERT-large. Damit haben sie den ersten Platz im GLUE-Benchmark gewonnen.  Im Gegensatz zu vielen anderen Ans√§tzen des GLUE-Benchmarks, die die Optimierung von Dateien f√ºr mehrere Aufgaben durchf√ºhren, haben wir die Optimierung von Dateien f√ºr einzelne Aufgaben verwendet.  Bei den M√§dchen in GLUE werden einzelne Modellergebnisse verglichen, sie erhielten SOTA f√ºr alle 9 Aufgaben.  Auf dem Testset wird das Ensemble von Modellen verglichen, SOTA f√ºr 4 von 9 Aufgaben und die endg√ºltige Klebegeschwindigkeit.  Auf zwei Versionen von SQuAD im SOTA-Entwicklungsnetzwerk, auf dem Testset auf XLNet-Ebene.  Dar√ºber hinaus werden sie im Gegensatz zu XLNet nicht von zus√§tzlichen QS-Paketen erfasst, bevor sie SQuAD l√∂sen. </p><br><img src="https://habrastorage.org/webt/5x/ue/u9/5xueu9hpmqwowfuf0yn1_zopqxy.png" width="500" height="250"><br><p><br>  SOTA on RACE-Aufgabe, in der ein Textst√ºck gegeben wird, eine Frage zu diesem Text und 4 Antwortoptionen, bei denen Sie die richtige ausw√§hlen m√ºssen.  Um diese Aufgabe zu l√∂sen, verketten sie den Text, stellen Fragen und Antworten, durchlaufen BERT, erhalten eine Darstellung vom CLF-Token, wenden sie auf eine vollst√§ndig verbundene Ebene an und sagen voraus, ob die Antwort korrekt ist.  Dies geschieht viermal - f√ºr jede der Antwortoptionen. </p><br><p>  Wir haben den Code und den Pretrain des RoBERTa-Modells in <a href="">Fairseq-R√ºbe ver√∂ffentlicht</a> .  Sie k√∂nnen es verwenden, alles sieht ordentlich und einfach aus. </p><br><h3 id="4-efficientnet-rethinking-model-scaling-for-convolutional-neural-networks">  4. EfficientNet: Modellskalierung f√ºr Faltungs-Neuronale Netze √ºberdenken </h3><br><p>  Autoren: Mingxing Tan, Quoc V. Le (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: Alexander Denisenko (in lockerem Alexander Denisenko) </p><br><img src="https://habrastorage.org/webt/ey/se/0k/eyse0kouanmvflpgz9ev--x1oqm.png" width="500" height="250"><br><p><br>  Sie untersuchen die Skalierung (Skalierung) von Modellen und das Gleichgewicht zwischen Tiefe und Breite (Anzahl der Kan√§le) des Netzwerks sowie die Aufl√∂sung von Bildern im Raster.  Sie bieten eine neue Skalierungsmethode, mit der Tiefe / Breite / Aufl√∂sung gleichm√§√üig skaliert werden k√∂nnen.  Zeigen Sie seine Wirksamkeit auf MobileNet und ResNet. </p><br><p>  Sie verwenden auch die Suche nach neuronaler Architektur, um ein neues Netz zu erstellen und es zu skalieren, wodurch eine Klasse neuer Modelle erhalten wird - EfficientNets.  Sie sind besser und wirtschaftlicher als fr√ºhere Netze.  In ImageNet erreicht EfficientNet-B7 eine Genauigkeit von 84,4% Top-1 und 97,1% Top-5 auf dem neuesten Stand der Technik, w√§hrend es 8,4-mal weniger und 6,1-mal schneller bei Inferenz ist als das derzeit beste ConvNet seiner Klasse.  Es l√§sst sich gut auf andere Datens√§tze √ºbertragen - sie haben SOTA f√ºr 5 der 8 beliebtesten Datens√§tze erhalten. </p><br><p>  <strong>Zusammengesetzte Modellskalierung</strong> <br>  Die Skalierung erfolgt, wenn Operationen innerhalb des Gitters festgelegt sind und nur die Tiefe (Anzahl der Wiederholungen derselben Module) d, die Breite (Anzahl der Kan√§le in Faltung) w und die Aufl√∂sung r ge√§ndert werden.  Im Pager wird die Skalierung als Optimierungsproblem formuliert - wir wollen maximale Genauigkeit (Net (d, w, r)), obwohl wir im Speicher und in FLOPS nicht aus dem Rahmen kriechen. </p><br><p>  Wir haben Experimente durchgef√ºhrt und sichergestellt, dass es auch beim Skalieren in der Breite wirklich hilft, die Tiefe und Aufl√∂sung zu skalieren.  Mit den gleichen FLOPS erzielen wir mit ImageNet ein deutlich besseres Ergebnis (siehe Bild oben).  Im Allgemeinen ist dies vern√ºnftig, da es den Anschein hat, dass mit zunehmender Aufl√∂sung des Netzwerkbildes mehr Schichten in der Tiefe ben√∂tigt werden, um das Empfangsfeld zu vergr√∂√üern, und mehr Kan√§le, um alle Muster im Bild mit einer h√∂heren Aufl√∂sung zu erfassen. </p><br><p>  Die Essenz der zusammengesetzten Skalierung: Wir nehmen den zusammengesetzten Koeffizienten phi, der d, w und r mit diesem Koeffizienten gleichm√§√üig skaliert: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>d</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mtext>&amp;#xA0;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="47.52ex" height="2.419ex" viewBox="0 -780.1 20460 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-3D" x="801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="2107" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6C" x="2637" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="2935" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="3439" y="0"></use><g transform="translate(4015,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="4895" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="5398" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="5975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="6320" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-77" x="6765" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-3D" x="7760" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-62" x="9066" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-65" x="9495" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-74" x="9962" y="0"></use><g transform="translate(10323,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="11203" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="11706" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="12283" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="12628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-72" x="13073" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-3D" x="13803" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-67" x="15109" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="15589" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="16119" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="16998" y="0"></use><g transform="translate(17876,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="0" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="18756" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="19259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="19836" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="20181" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>w</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo><mi>r</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mtext>&nbsp;</mtext></msup><mi>p</mi><mi>h</mi><mi>i</mi><mo>,</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> d = \ alpha ^ \ phi, w = \ beta ^ \ phi, r = \ gamma ^ \ phi, </script>  wo <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ alpha, \ beta, \ gamma </script>  - Konstanten, die aus einer kleinen Gitteransicht im Quellgitter erhalten wurden. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ phi </script>  - Koeffizient, der die Menge der verf√ºgbaren Rechenressourcen kennzeichnet. </p><br><p>  <strong>Effizientes Netz</strong> <br>  Um das Raster zu erstellen, haben wir die Suche nach neuronalen Architekturen mit mehreren Objektiven, die optimierte Genauigkeit und FLOPS mit dem Parameter verwendet, der f√ºr den Kompromiss zwischen ihnen verantwortlich ist.  Eine solche Suche ergab EfficientNet-B0.  Kurz gesagt - Conv, gefolgt von mehreren MBConv am Ende von Conv1x1, Pool, FC. </p><br><p>  F√ºhren Sie dann die Skalierung in zwei Schritten durch: </p><br><ol><li>  Zun√§chst reparieren wir <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.152ex" height="2.419ex" viewBox="0 -780.1 3510.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="1330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-3D" x="1953" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-31" x="3009" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ phi = 1 </script>  F√ºhren Sie eine Rastersuche f√ºr die Suche durch <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.278ex" height="2.419ex" viewBox="0 -780.1 9161.3 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6C" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="1078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="1581" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="2158" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="2687" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-62" x="3382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-65" x="3812" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-74" x="4278" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="4640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMAIN-2C" x="5169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-67" x="5864" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="6345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="6874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-6D" x="7753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-61" x="8631" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>a</mi><mi>l</mi><mi>p</mi><mi>h</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>e</mi><mi>t</mi><mi>a</mi><mo>,</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ alpha, \ beta, \ gamma </script>  . </li><li>  Skalieren Sie das Raster mit den Formeln f√ºr d, w und r.  Erhielt EffiientNet-B1.  Ebenso steigend <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.891ex" height="2.419ex" viewBox="0 -780.1 1675.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-70" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-68" x="753" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://habr.com/ru/company/ods/blog/472672/&amp;usg=ALkJrhiPbFMCA3DoRLVvgAxSzSc0uoISjw#MJMATHI-69" x="1330" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>p</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ phi </script>  , erhalten EfficientNet-B2, ... B7. </li></ol><br><p>  F√ºr verschiedene ResNet- und MobileNet-Modelle skaliert, wurden √ºberall signifikante Verbesserungen gegen√ºber ImageNet erzielt. Die zusammengesetzte Skalierung f√ºhrte zu einer signifikanten Steigerung im Vergleich zur Skalierung in nur einer Dimension.  Wir haben auch Experimente mit EfficientNet an acht popul√§reren Datens√§tzen durchgef√ºhrt, √ºberall dort, wo wir SOTA oder ein Ergebnis in der N√§he mit einer signifikant geringeren Anzahl von Parametern erhalten haben. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Code</a> </p><br><h3 id="5-how-the-brain-transitions-from-conscious-to-subliminal-perception">  5. Wie das Gehirn von der bewussten zur unterschwelligen Wahrnehmung √ºbergeht </h3><br><p>  Autoren des Artikels: Francesca Arese Lucini, Gino Del Ferraro, Mariano Sigman, Hernan A. Makse (USA, Argentinien, Spanien, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: Svyatoslav Skoblov (in slack error_derivative) </p><br><p>  Dieser Artikel ist eine Fortsetzung und ein Umdenken der Arbeit von <em>Dehaene, S, Naccache, L, Cohen, L, Le Bihan, D, Mangin, JF, Poline, JB und Rivie`re, D. Zerebrale Mechanismen der Wortmaskierung und der unbewussten Wiederholungsgrundierung</em> in die Autoren versuchten, die Modi der bewussten und unbewussten Gehirnfunktion zu ber√ºcksichtigen. </p><br><img src="https://habrastorage.org/webt/fi/o4/te/fio4terok3udte6bcmpajzfs_um.png" width="500" height="250"><br><p><br>  <strong>Experiment:</strong> <br>  Freiwilligen werden Bilder gezeigt (W√∂rter mit 4 Buchstaben oder ein leerer Bildschirm oder Kritzeleien).  Jeder von ihnen wird 30 ms lang angezeigt. Im Allgemeinen dauert die gesamte Aktion 5 Minuten. </p><br><ol><li>  Im "bewussten" Modus des Experiments wechselt ein leerer Bildschirm mit W√∂rtern ab, wodurch eine Person den Text bewusst wahrnehmen kann. </li><li>  Im ‚Äûunbewussten‚Äú Modus wechseln sich W√∂rter mit Kritzeleien ab, was die Wahrnehmung des Textes auf bewusster Ebene sehr effektiv beeintr√§chtigt. </li></ol><br><p>  <strong>Daten:</strong> <br>  W√§hrend dieser Pr√§sentation wurden die Gehirne unserer Primaten mit fMRI gescannt.  Insgesamt hatten die Forscher 15 Freiwillige, die das Experiment jeweils f√ºnfmal wiederholten, insgesamt 75 fMRI-Str√∂me.  Es ist erw√§hnenswert, dass der Voxel-Scan ziemlich gro√ü war (sehr vereinfacht: Voxel ist ein 3D-W√ºrfel, der eine ziemlich gro√üe Anzahl von Zellen enth√§lt) - 4 x 4 x 4 mm. </p><br><p>  <strong>Magie:</strong> <br>  Rufen wir den Node Active Voxel aus unserem Stream auf.  Da das Gehirn ein modularer Waschlappen ist, f√ºhren wir zwei Arten von Verbindungen ein: externe und interne (entsprechend der r√§umlichen Anordnung der Knoten).  Verbindungen werden auf interessante Weise zusammengesetzt: Wir erstellen eine Kreuzkorrelationsmatrix zwischen Knoten und verbinden die Knoten mit einer Verbindung, wenn die Korrelation gr√∂√üer als ein adaptiver Parameter Lambda ist.  Dieser Parameter wirkt sich auf die Entladung unseres Netzwerks aus. </p><br><p>  Die Parametereinstellung erfolgt nach dem "Filter" -Verfahren.  Wenn wir unser Lambda ein wenig beeinflussen, werden scharfe √úberg√§nge zwischen den endg√ºltigen Dimensionen des Netzwerks bemerkbar (d. H. Eine ausreichend kleine Parameter√§nderung entspricht einem gro√üen Gr√∂√üenzuwachs). </p><br><p>  Also: Interne Verbindungen werden durch den Lambda-1-Wert aktiviert, der dem Lambda-Wert unmittelbar vor einem scharfen √úbergang entspricht.  Extern - Lambda-2-Wert, der dem Lambda-Wert unmittelbar nach einem scharfen √úbergang entspricht. </p><br><p>  <strong>Magie 2:</strong> <br>  K-Core-Filterung.  Das k-Core-Konzept beschreibt die Netzwerkkonnektivit√§t und ist ganz einfach formuliert: das maximale Subnetz, dessen Knoten alle mindestens k Nachbarn haben.  Ein solches Subnetz kann durch iteratives Entfernen von Knoten mit weniger als k Nachbarn erhalten werden.  Da die verbleibenden Knoten Nachbarn verlieren, wird der Vorgang fortgesetzt, bis nichts mehr zu l√∂schen ist.  Was bleibt, ist das k-Core-Netzwerk. </p><br><p>  <strong>Ergebnisse:</strong> <br>  Wenn Sie diese Artillerie auf unser Gehirn anwenden, k√∂nnen Sie eine Reihe sehr interessanter Merkmale erkennen. </p><br><ol><li>  Die Anzahl der Knoten im k-Kern mit kleinem / sehr gro√üem k ist extrem gro√ü.  F√ºr Medium k ist es im Gegenteil nicht genug.  Auf dem Bild sieht es aus wie eine U-Form, dh eine solche Netzwerkkonfiguration bietet die gr√∂√üte Stabilit√§t des Systems (Best√§ndigkeit gegen lokale und globale Fehler). </li><li>  <strong>und die wichtigsten</strong> Knoten, die zum k-Kern mit kleinem k geh√∂ren, k√∂nnen in fast jedem Zustand des Netzwerks gesehen werden.  Ein k-Kern mit sehr gro√üem k ist jedoch nur f√ºr diejenigen Teile des Gehirns charakteristisch, die im unbewussten Zustand <em>Fusiform Gyrus &amp; Left Precentral Gyrus aktiv sind</em> .          . </li></ol><br><p>           ,   rewiring,       (  ,      ).          k.  , U shape          ,       ,        . </p><br><p>  <strong>Schlussfolgerungen:</strong> <br> ,  ,   ,              .     ,     ,      ,    -     (     , , ,     ). </p><br><p>  ,   ,         ,       ,        ,         , ,    - . , ,            qualia. </p><br><h3 id="6-large-memory-layers-with-product-keys"> 6. Large Memory Layers with Product Keys </h3><br><p>  : Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, Herv√© J√©gou (Facebook AI Research, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí  </a> <br>  :   (  belerafon) </p><br><img src="https://habrastorage.org/webt/4b/_q/nm/4b_qnmw2tilj5zyscirkrogerrg.png"><br><p><br> ,       key-value        ,   . </p><br><p>     -    attention.    q,     k   v.  q,    k,    ,      value  .  ,       .    ,         .      ,         ,   .     -    q       (, -10).        .      . </p><br><p>    ‚Äî     q   k   .   ,    "Product Keys".      ,         q   ,     .        -10   , ,     O(N)    ""  ,   (sqrt(N)). </p><br><p>         key-value .      ,    (  ,     ). ,   BERT      28  . ,          ,     .  : 12-       2  ,  24-  ,    perplexity     . </p><br><p>            (      self-attention). ,     -         .  ,         multy-head attention.  Das hei√üt,    query  ,      value,     .         -. </p><br><p>        , ,        ,    ,    BERT  .      . </p><br><h3 id="7-are-we-really-making-much-progress-a-worrying-analysis-of-recent-neural-recommendation-approaches"> 7. Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches </h3><br><p>  : Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach (Politecnico di Milano, University of Klagenfurt, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí  </a> <br>  :   (  netcitizen) </p><br><img src="https://habrastorage.org/webt/zk/89/wu/zk89wuyudxpggdl6-0tyxe2wwrw.png"><br><p><br>       DL    , ,             . </p><br><p> <strong></strong> <br>             DL       top-n.    DL      KDD, SIGIR, TheWebConf (WWW)  RecSys     : </p><br><ol><li>     </li><li>    -       </li><li>       </li></ol><br><p> <strong></strong> </p><br><ol><li>    7/18 (39%) </li><li>        ‚Äú‚Äù    train/test,     .,   , ,   . </li><li>     (Variational Autoencoders for Collaborative Filtering (Mult-VAE)  ¬±   )     KNN, SVD, PR. </li></ol><br><p> <strong></strong> <br>  DL,       CV, NLP      ,       . </p><br><h3 id="8-omni-scale-feature-learning-for-person-re-identification"> 8. Omni-Scale Feature Learning for Person Re-Identification </h3><br><p>  : Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang (University of Surrey, Queen Mary University, Samsung AI, 2019) <br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí  </a> <br>  : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> (  graviton) </p><br><p>  Person Re-Identification,    Face Recognition,    ,          .  (Kaiyang Zhou)        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">deep-person-reid</a>     ,      (OSNet),          Person Re-Identification.     . </p><br><p> <strong>  </strong> Person Re-Identification: </p><br><img src="https://habrastorage.org/webt/6d/rq/jv/6drqjvwsyg33s_e7zv2t4nov0ts.png" width="500" height="250"><br><p><br> <strong> :</strong> </p><br><ol><li>   conv1x1  deepwise conv3x3   conv3x3   (figure 3). </li><li>  ,      .    ResNeXt         ,     Inception      (figure 4). </li><li>      ‚Äúaggregation gate‚Äù       .  ,    Inception     . </li></ol><br><img src="https://habrastorage.org/webt/ic/tc/9z/ictc9z6mcdkv3o0i23swu_f5zty.png"><br><p><br>  OSNet       , ..      ,    :      (  ,  )   . </p><br><p> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ReID-Testergebnisse</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> f√ºr OSNet (ca. 2 Millionen Parameter) zeigen den Vorteil dieser Architektur gegen√ºber anderen Lichtmodellen (Markt: R1 93,6%, mAP 81,0% f√ºr OSNet und R1 87,0%, mAP 69,5% f√ºr MobileNetV2) und das Fehlen eines signifikanten Unterschieds in der Genauigkeit mit schwere Modelle von ResNet und DenseNet (Markt: R1 94,8%, mAP 84,9% f√ºr OSNet und R1 94,8%, mAP 86,0% f√ºr ResNet).</font></font></p><br><p> Eine weitere Herausforderung ist die <strong>Dom√§nenanpassung</strong> : Modelle, die auf einem Datensatz trainiert wurden, weisen auf einem anderen eine schlechte Qualit√§t auf.  OSNet zeigt auch in diesem Segment gute Ergebnisse ohne die Verwendung einer ‚Äûunbeaufsichtigten Dom√§nenanpassung‚Äú (Verwendung von Testdaten in nicht zugeordneter Form, um die Verteilung von Daten auszugleichen). </p><br><p>  Die Architektur wurde auch in ImageNet getestet, wo sie mit MobileNetV2 eine √§hnliche Genauigkeit mit weniger Parametern, aber mehr Operationen erreichte. </p><br><h3 id="9-neural-reparameterization-improves-structural-optimization">  9. Neuronale Reparametrisierung verbessert die Strukturoptimierung </h3><br><p>  Autoren: Stephan Hoyer, Jascha Sohl-Dickstein, Sam Greydanus (Google Research, 2019) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Üí Originalartikel</a> <br>  Rezensionsautor: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alexey</a> (in Arech locker) </p><br><img src="https://habrastorage.org/webt/ga/sd/hd/gasdhdqgy5febp9elrdgdhy9nqg.png"><br><p><br>  In der Konstruktion und anderen Technologien gibt es Aufgaben zur Optimierung der Struktur / Topologie einer L√∂sung.  Grob gesagt ist dies eine Computerantwort auf eine Frage wie zum Beispiel, wie eine Br√ºcke / ein Geb√§ude / ein Fl√ºgel eines Flugzeugs / einer Turbinenschaufel / einer Blablabla so konstruiert werden soll, dass bestimmte Einschr√§nkungen erf√ºllt werden und die Struktur stark genug ist.  Es gibt eine Reihe von "Standard" -L√∂sungsmethoden - es funktioniert, aber dort ist nicht immer alles reibungslos. </p><br><p>  Was haben sich diese Leute von Google ausgedacht?  Sie sagten: Lassen Sie uns eine L√∂sung durch ein neuronales Netzwerk (den Upsampling-Teil von UNet) generieren und dann unter Verwendung eines differenzierbaren physikalischen Modells, das das Verhalten einer L√∂sung unter dem Einfluss aller Kr√§fte und der Schwerkraft berechnet, die Zielfunktion - St√§rke (genauer gesagt die Umkehrung davon - Compliance) berechnen. ) Designs.  Da dann alles automatisch differenzierbar ist, erhalten wir den Gradienten der Zielfunktion, der durch die gesamte Struktur zur√ºck zu den Gewichten und der Eingabe des neuronalen Netzwerks geschoben wird.  Wir √§ndern Gewichte und Eingabe und setzen den Zyklus bis zur Konvergenz zu einer stabilen L√∂sung fort. </p><br><p>  Es stellte sich heraus, dass die Ergebnisse kleine (in Bezug auf die Gr√∂√üe des Raums m√∂glicher L√∂sungen) Probleme betrafen, die mit herk√∂mmlichen Methoden zur Optimierung von Topologien vergleichbar sind, und dass gro√üe Probleme deutlich besser sind als herk√∂mmliche (√úbergewicht bei 99 gegen√ºber 66 von 116 Problemen).  Dar√ºber hinaus sind die resultierenden L√∂sungen oft wesentlich technologischer und optimaler als die Entscheidungen von Baselines. </p><br><p>  Das hei√üt,  Tats√§chlich verwendeten sie den NS als eine schwierige Methode zur Parametrisierung des physikalischen Modells der Struktur, die implizit (dank der Architektur des NS) einige n√ºtzliche Einschr√§nkungen f√ºr Parameterwerte auferlegen kann (gesteuert durch Entfernen des NS aus der Methode und direkte Optimierung der Pixelwerte). </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quellcode.</a> </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine detailliertere √úbersicht √ºber diesen Artikel auf habr.</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de472672/">https://habr.com/ru/post/de472672/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de472658/index.html">Fast alles √ºber die Zukunft HolyJS 2019 Moskau</a></li>
<li><a href="../de472660/index.html">Wie man schnell Prototypen von Ger√§ten erstellt und warum dies wichtig ist. Bericht Yandex.Taxi</a></li>
<li><a href="../de472662/index.html">Mit wem f√ºr den Export gehen</a></li>
<li><a href="../de472668/index.html">Produktdenken. Was ist das und wie kann man es entwickeln?</a></li>
<li><a href="../de472670/index.html">Limely Herbst, Limely Winter ...</a></li>
<li><a href="../de472674/index.html">Umgebungsvariablen f√ºr Python-Projekte</a></li>
<li><a href="../de472676/index.html">Wir erstellen die Abteilung f√ºr Jones, um den Hauptteams zu helfen, indem wir nur Slack, Jira und das blaue Isolierband verwenden</a></li>
<li><a href="../de472682/index.html">Verlangsamung des Alterns mit Arzneimittelsynergien bei C. elegans</a></li>
<li><a href="../de472684/index.html">√úberraschen Sie fsync () PostgreSQL</a></li>
<li><a href="../de472686/index.html">Videostudio basierend auf i486</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>