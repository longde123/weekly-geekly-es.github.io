<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍿 💓 🧤 Python vs. Scala para Apache Spark: referencia esperada con resultado inesperado 💊 👩🏿‍🚀 🎶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Apache Spark hoy es quizás la plataforma más popular para analizar datos de gran volumen. La posibilidad de usarlo desde debajo de Python hace una con...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Python vs. Scala para Apache Spark: referencia esperada con resultado inesperado</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/443324/"><p><img src="https://habrastorage.org/webt/uk/hc/yv/ukhcyvudckoey9ttm1libhqkyv8.jpeg"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Apache Spark</a> hoy es quizás la plataforma más popular para analizar datos de gran volumen.  La posibilidad de usarlo desde debajo de Python hace una contribución considerable a su popularidad.  Al mismo tiempo, todos están de acuerdo en que, dentro del marco de la API estándar, el rendimiento del código Python y Scala / Java es comparable, pero no existe un punto de vista único con respecto a las funciones definidas por el usuario (Función definida por el usuario, UDF).  Intentemos descubrir cómo aumentan los costos generales en este caso, utilizando el ejemplo de la tarea de verificar la solución <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SNA Hackathon 2019</a> . </p><a name="habracut"></a><br><p>  Como parte de la competencia, los participantes resuelven el problema de clasificar las noticias de una red social y suben soluciones en forma de un conjunto de listas ordenadas.  Para verificar la calidad de la solución obtenida, primero, para cada una de las listas cargadas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">se</a> calcula el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ROC AUC</a> y luego se muestra el valor promedio.  Tenga en cuenta que necesita calcular no un AUC ROC común, sino uno personal para cada usuario: no hay un diseño listo para resolver este problema, por lo que deberá escribir una función especializada.  Una buena razón para comparar los dos enfoques en la práctica. </p><br><p>  Como plataforma de comparación, utilizaremos un contenedor en la nube con cuatro núcleos y Spark lanzado en modo local, y trabajaremos con él a través de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Apache Zeppelin</a> .  Para comparar la funcionalidad, reflejaremos el mismo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">código</a> en PySpark y Scala Spark.  [aquí] Comencemos cargando los datos. </p><br><pre><code class="python hljs">data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) \ .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) \ .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) \ .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> data = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/modelCappedSubmit"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trueData = sqlContext.read.csv(<span class="hljs-string"><span class="hljs-string">"sna2019/collabGt"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> toValidate = data.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"submit"</span></span>) .join(trueData.withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c1"</span></span>, <span class="hljs-string"><span class="hljs-string">"real"</span></span>), <span class="hljs-string"><span class="hljs-string">"_c0"</span></span>) .withColumnRenamed(<span class="hljs-string"><span class="hljs-string">"_c0"</span></span>, <span class="hljs-string"><span class="hljs-string">"user"</span></span>) .repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() toValidate.count()</code> </pre> <br><p>  Cuando se usa la API estándar, la identidad casi completa del código es notable, hasta la palabra clave <code>val</code> .  El tiempo de operación no es significativamente diferente.  Ahora intentemos determinar el UDF que necesitamos. </p><br><pre> <code class="python hljs">parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)], ArrayType(IntegerType())) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores)) auc_udf = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, auc, DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> parse = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"parse"</span></span>, (x : <span class="hljs-type"><span class="hljs-type">String</span></span>) =&gt; x.slice(<span class="hljs-number"><span class="hljs-number">1</span></span>,x.size - <span class="hljs-number"><span class="hljs-number">1</span></span>).split(<span class="hljs-string"><span class="hljs-string">","</span></span>).map(_.trim.toInt)) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AucAccumulator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">height: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, area: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-class"><span class="hljs-params">, negatives: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">val</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">auc_udf</span></span></span><span class="hljs-class"> </span></span>= sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc"</span></span>, (byScore: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>], gt: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> byLabel = gt.toSet <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> accumulator = byScore.foldLeft(<span class="hljs-type"><span class="hljs-type">AucAccumulator</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>))((accumulated, current) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (byLabel.contains(current)) { accumulated.copy(height = accumulated.height + <span class="hljs-number"><span class="hljs-number">1</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { accumulated.copy(area = accumulated.area + accumulated.height, negatives = accumulated.negatives + <span class="hljs-number"><span class="hljs-number">1</span></span>) } }) (accumulator.area).toDouble / (accumulator.negatives * accumulator.height) })</code> </pre> <br><p>  Al implementar una función específica, está claro que Python es más conciso, principalmente debido a la capacidad de usar la función incorporada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">scikit-learn</a> .  Sin embargo, hay momentos desagradables: debe especificar explícitamente el tipo del valor de retorno, mientras que en Scala se determina automáticamente.  Realicemos la operación: </p><br><pre> <code class="python hljs">toValidate.select(auc_udf(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><pre> <code class="scala hljs">toValidate.select(auc_udf(parse($<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse($<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p>  El código parece casi idéntico, pero los resultados son desalentadores. </p><br><p><img src="https://habrastorage.org/webt/vc/jw/y_/vcjwy_zwvfbkb_jlw6mimlogdwi.png"></p><br><p>  La implementación en PySpark funcionó un minuto y medio en lugar de dos segundos en Scala, es decir, <strong>Python resultó ser 45 veces más lento</strong> .  Mientras se ejecuta, la parte superior muestra 4 procesos activos de Python que se ejecutan a toda velocidad, y esto sugiere que el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">bloqueo global del intérprete</a> no crea problemas aquí.  Pero!  Quizás el problema esté en la implementación interna de scikit-learn: intentemos reproducir el código de Python literalmente, sin recurrir a bibliotecas estándar. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) auc_udf_modified = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"auc_modified"</span></span>, auc, DoubleType()) toValidate.select(auc_udf_modified(parse(<span class="hljs-string"><span class="hljs-string">"submit"</span></span>), parse(<span class="hljs-string"><span class="hljs-string">"real"</span></span>))).groupBy().avg().show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/cx/o2/io/cxo2ioxdl18a6djwsmgr6dravhy.png"></p><br><p>  El experimento muestra resultados interesantes.  Por un lado, con este enfoque, la productividad se niveló, pero por otro, el laconismo desapareció.  Los resultados obtenidos pueden indicar que cuando se trabaja en Python usando módulos C ++ adicionales, aparecen gastos generales significativos para cambiar entre contextos.  Por supuesto, hay una sobrecarga similar al usar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">JNI</a> en Java / Scala, sin embargo, no tuve que lidiar con ejemplos de degradación 45 veces al usarlos. </p><br><p>  Para un análisis más detallado, realizaremos dos experimentos adicionales: usar Python puro sin Spark para medir la contribución de la llamada del paquete, y con un mayor tamaño de datos en Spark para amortizar los gastos generales y obtener una comparación más precisa. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [int(s.strip()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">-1</span></span>].split(<span class="hljs-string"><span class="hljs-string">","</span></span>)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) height = <span class="hljs-number"><span class="hljs-number">0</span></span> area = <span class="hljs-number"><span class="hljs-number">0</span></span> negatives = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> candidate <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet: height = height + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: area = area + height negatives = negatives + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(area) / (negatives * height) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sklearn_auc</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(submit, real)</span></span></span><span class="hljs-function">:</span></span> trueSet = set(real) scores = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> / (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(submit)] labels = [<span class="hljs-number"><span class="hljs-number">1.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trueSet <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> submit] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> float(roc_auc_score(labels, scores))</code> </pre> <br><p><img src="https://habrastorage.org/webt/bk/ul/m1/bkulm1xarplv2-f4ilyg5dtxixw.png"></p><br><p>  El experimento con Python y Pandas locales confirmó la suposición de una sobrecarga significativa al usar paquetes adicionales: cuando se usa scikit-learn, la velocidad disminuye en más de 20 veces.  Sin embargo, 20 no es 45: intentemos "inflar" los datos y comparar el rendimiento de Spark nuevamente. </p><br><pre> <code class="python hljs">k4 = toValidate.union(toValidate) k8 = k4.union(k4) m1 = k8.union(k8) m2 = m1.union(m1) m4 = m2.union(m2).repartition(<span class="hljs-number"><span class="hljs-number">4</span></span>).cache() m4.count()</code> </pre> <br><p><img src="https://habrastorage.org/webt/zp/sf/po/zpsfpoabev7w3_xjcn0vqobwqf4.png"></p><br><p>  La nueva comparación muestra la ventaja de velocidad de una implementación de Scala sobre Python en 7-8 veces, 7 segundos frente a 55. Finalmente, intentemos "lo más rápido que hay en Python", <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">numpy</a> para calcular la suma de la matriz: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy numpy_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"numpy_sum"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: float(numpy.sum(x)), DoubleType())</code> </pre> <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> my_sum = sqlContext.udf.register(<span class="hljs-string"><span class="hljs-string">"my_sum"</span></span>, (x: <span class="hljs-type"><span class="hljs-type">Seq</span></span>[<span class="hljs-type"><span class="hljs-type">Int</span></span>]) =&gt; x.map(_.toDouble).sum)</code> </pre> <br><p><img src="https://habrastorage.org/webt/og/tr/gx/ogtrgx-kkuzx4kmkh4pd5cbyd-k.png"></p><br><p>  Nuevamente, una desaceleración significativa: 5 segundos de Scala frente a 80 segundos de Python.  En resumen, podemos sacar las siguientes conclusiones: </p><br><ul><li>  Si bien PySpark opera dentro del marco de la API estándar, realmente puede ser comparable en velocidad a Scala. </li><li>  Cuando aparece una lógica específica en forma de funciones definidas por el usuario, el rendimiento de PySpark disminuye notablemente.  Con suficiente información, cuando el tiempo de procesamiento para un bloque de datos excede varios segundos, la implementación de Python es 5-10 más lenta debido a la necesidad de mover datos entre procesos y desperdiciar recursos al interpretar Python. </li><li>  Si aparece el uso de funciones adicionales implementadas en módulos C ++, entonces surgen costos de llamadas adicionales, y la diferencia entre Python y Scala aumenta hasta 10-50 veces. </li></ul><br><p>  Como resultado, a pesar de todo el encanto de Python, su uso en conjunto con Spark no siempre parece justificado.  Si no hay tantos datos para hacer que la sobrecarga de Python sea significativa, entonces debería pensar si se necesita Spark aquí.  Si hay muchos datos, pero el procesamiento se produce dentro del marco de la API estándar de Spark SQL, ¿se necesita Python aquí? </p><br><p>  Si hay una gran cantidad de datos y a menudo tiene que lidiar con tareas que van más allá de los límites de la API de SQL, entonces para realizar la misma cantidad de trabajo cuando se usa PySpark, tendrá que aumentar el clúster en ocasiones.  Por ejemplo, para Odnoklassniki, el costo de los gastos de capital para el grupo Spark aumentaría en muchos cientos de millones de rublos.  Y si intenta aprovechar las capacidades avanzadas de las bibliotecas del ecosistema Python, es decir, el riesgo de desaceleración no es solo a veces, sino un orden de magnitud. </p><br><p>  Se puede obtener cierta aceleración utilizando la funcionalidad relativamente nueva de las funciones vectorizadas.  En este caso, no se alimenta una sola fila a la entrada UDF, sino un paquete de varias filas en forma de un marco de datos de Pandas.  Sin embargo, el desarrollo de esta funcionalidad <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aún no se ha completado</a> , e incluso en este caso la diferencia será <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">significativa</a> . </p><br><p>  Una alternativa sería mantener un amplio equipo de ingenieros de datos, capaces de abordar rápidamente las necesidades de los científicos de datos con funciones adicionales.  O sumergirse en el mundo Scala, ya que no es tan difícil: muchas de las herramientas necesarias ya <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">existen</a> , aparecen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">programas de capacitación</a> que van más allá de PySpark. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/443324/">https://habr.com/ru/post/443324/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443312/index.html">Gestión de memoria Python</a></li>
<li><a href="../443316/index.html">Prueba de ReactJS: qué tan profunda es la madriguera del conejo</a></li>
<li><a href="../443318/index.html">Escribir un cargador de wasm para Ghidra. Parte 1: planteamiento del problema y entorno de configuración</a></li>
<li><a href="../443320/index.html">Sistema de gestión electrónica de documentos "Visir"</a></li>
<li><a href="../443322/index.html">GitLab 11.8 lanzado con SAST para JavaScript, páginas de GitLab para subgrupos y seguimiento de errores</a></li>
<li><a href="../443326/index.html">Python y Arduino. Simple, rapido y hermoso</a></li>
<li><a href="../443330/index.html">Semana de la seguridad 11: RSA 2019 y un futuro mejor</a></li>
<li><a href="../443332/index.html">Robot de cocina</a></li>
<li><a href="../443334/index.html">Cara amarilla</a></li>
<li><a href="../443336/index.html">Trabaja con la cámara en Flutter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>