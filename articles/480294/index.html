<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêâ ‚ôüÔ∏è üèñÔ∏è Visi√≥n artificial de alta velocidad en el vers√°til dispositivo de clasificaci√≥n de piezas LEGO üéÇ üë®üèø‚Äçüîß üçá</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Durante los √∫ltimos a√±os, he estado dise√±ando y fabricando una m√°quina que puede reconocer y clasificar piezas LEGO. La parte m√°s importante de la m√°q...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Visi√≥n artificial de alta velocidad en el vers√°til dispositivo de clasificaci√≥n de piezas LEGO</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/480294/"> Durante los √∫ltimos a√±os, he estado dise√±ando y fabricando una m√°quina que puede reconocer y clasificar piezas LEGO.  La parte m√°s importante de la m√°quina es la <strong>Unidad de captura</strong> , un compartimento peque√±o, casi completamente cerrado, en el que hay una cinta transportadora, iluminaci√≥n y una c√°mara. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/6c7/5d8/6416c75d85738aceecf1162f1d48718d.jpg"></div><br>  <i>Iluminando ver√°s un poco m√°s abajo.</i> <br><br>  La c√°mara toma fotos de las partes de LEGO que pasan por el transportador y luego transfiere las im√°genes de forma inal√°mbrica a un servidor que ejecuta un algoritmo de inteligencia artificial para reconocer la parte entre miles de posibles elementos de LEGO.  Te contar√© m√°s sobre el algoritmo de IA en futuros art√≠culos, y este art√≠culo se centrar√° en el procesamiento que se realiza entre la salida sin formato de la c√°mara de video y la entrada a la red neuronal. <br><br>  El principal problema que ten√≠a que resolver era convertir la transmisi√≥n de video del transportador en im√°genes separadas de partes que una red neuronal podr√≠a usar. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca8/feb/2ea/ca8feb2eafab64d7d835d50e090e7bc5.gif"></div><br>  <i>El objetivo final: cambiar de un video sin formato (a la izquierda) a un conjunto de im√°genes del mismo tama√±o (a la derecha) para transferirlas a una red neuronal.</i>  <i>(en comparaci√≥n con el trabajo real, el gif es aproximadamente la mitad de lento)</i> <br><br>  Este es un gran ejemplo de una tarea que en la superficie parece simple, pero en realidad plantea muchos obst√°culos √∫nicos e interesantes, muchos de los cuales son exclusivos de las plataformas de visi√≥n artificial. <br><br>  La recuperaci√≥n de las partes correctas de una imagen de esta manera a menudo se denomina detecci√≥n de objetos.  Eso es exactamente lo que necesito hacer: reconocer la presencia de objetos, su ubicaci√≥n y tama√±o, para que pueda generar <strong>rect√°ngulos delimitadores</strong> para cada parte en cada marco. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cfc/265/c31/cfc265c31250940a6170ad404f89de08.gif"></div><br>  <i>Lo m√°s importante es encontrar buenos cuadros delimitadores (mostrados arriba en verde)</i> <br><br>  Considerar√© tres aspectos para resolver el problema: <br><br><ul><li>  Prepararse para eliminar variables innecesarias </li><li>  Crear un proceso a partir de operaciones simples de visi√≥n artificial </li><li>  Mantener un rendimiento suficiente en una plataforma Raspberry Pi con recursos limitados </li></ul><br><h2>  Eliminaci√≥n de variables innecesarias. </h2><br>  En el caso de tales tareas, es mejor eliminar tantas variables como sea posible antes de usar t√©cnicas de visi√≥n artificial.  Por ejemplo, no deber√≠a preocuparme por las condiciones ambientales, las diferentes posiciones de la c√°mara, la p√©rdida de informaci√≥n debido a la superposici√≥n de algunas partes por otras.  Por supuesto, es posible (aunque muy dif√≠cil) resolver todas estas variables mediante programaci√≥n, pero afortunadamente para m√≠, esta m√°quina se cre√≥ desde cero.  Yo mismo puedo prepararme para una soluci√≥n exitosa, eliminando toda la interferencia incluso antes de comenzar a escribir c√≥digo. <br><br>  El primer paso es fijar firmemente la posici√≥n, el √°ngulo y el enfoque de la c√°mara.  Con esto, todo es simple: en el sistema, la c√°mara se monta sobre el transportador.  No necesito preocuparme por la interferencia de otras partes;  Los objetos no deseados casi no tienen posibilidades de entrar en la unidad de captura.  Un poco m√°s complicado, pero es muy importante garantizar <strong>condiciones de iluminaci√≥n constantes</strong> .  No necesito el reconocedor de objetos para interpretar err√≥neamente la sombra de una parte m√≥vil a lo largo de la cinta como un objeto f√≠sico.  Afortunadamente, la unidad de captura es muy peque√±a (todo el campo de visi√≥n de la c√°mara es m√°s peque√±o que una barra de pan), por lo que ten√≠a un control m√°s que suficiente sobre las condiciones del entorno. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8ec/9da/652/8ec9da6525ba0a6f906d3e1ef6309647.jpg"></div><br>  <i>Unidad de captura, vista interior.</i>  <i>La c√°mara est√° en el tercio superior del marco.</i> <br><br>  Una soluci√≥n es hacer que el compartimento est√© completamente cerrado para que no entre la iluminaci√≥n exterior.  Intent√© este enfoque usando tiras de LED como fuente de iluminaci√≥n.  Desafortunadamente, el sistema result√≥ ser muy malhumorado: solo un peque√±o agujero en la caja es suficiente y la luz penetra en el compartimento, lo que hace imposible reconocer objetos. <br><br>  Al final, la mejor soluci√≥n era "obstruir" todas las otras fuentes de luz llenando el compartimento peque√±o con luz fuerte.  Result√≥ que las fuentes de luz que se pueden utilizar para iluminar locales residenciales son muy baratas y f√°ciles de usar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/16a/031/025/16a031025c08531ff3ef2f0ef20a2394.jpg"></div><br>  <i>¬°Consigue las sombras!</i> <br><br>  Cuando la fuente se dirige al compartimento peque√±o, obstruye completamente toda posible interferencia de luz externa.  Tal sistema tambi√©n tiene un efecto secundario conveniente: debido a la gran cantidad de luz en la c√°mara, puede usar una velocidad de obturaci√≥n muy alta, obteniendo im√°genes perfectamente claras de las piezas incluso cuando se mueve r√°pidamente a lo largo del transportador. <br><br><h2>  Reconocimiento de objetos </h2><br>  ¬øC√≥mo logr√© convertir este hermoso video con iluminaci√≥n uniforme en los cuadros delimitadores que necesitaba?  Si trabaja con IA, podr√≠a sugerirme que implemente una red neuronal para el reconocimiento de objetos como <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="nofollow">YOLO</a> o <a href="https://arxiv.org/abs/1506.01497" rel="nofollow">Faster R-CNN</a> .  Estas redes neuronales pueden hacer frente f√°cilmente a la tarea.  Desafortunadamente, estoy ejecutando el c√≥digo de reconocimiento de objetos en <a href="https://www.raspberrypi.org/" rel="nofollow">Raspberry pi</a> .  Incluso una computadora poderosa tendr√≠a problemas para ejecutar estas redes neuronales convolucionales a la frecuencia que necesitaba alrededor de 90FPS.  Y Raspberry pi, que no tiene una GPU compatible con AI, no podr√≠a hacer frente a una versi√≥n muy reducida de uno de esos algoritmos de AI.  Puedo transmitir video desde Pi a otra computadora, pero la transmisi√≥n de video en tiempo real es un proceso muy cambiante, y las demoras y las limitaciones de ancho de banda causan serios problemas, especialmente cuando se necesita una alta velocidad de transferencia de datos. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/K9a6mGNmhbc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>¬°YOLO es muy genial!</i>  <i>Pero no necesito todas sus funciones.</i> <br><br>  Afortunadamente, pude evitar una soluci√≥n dif√≠cil basada en inteligencia artificial utilizando las t√©cnicas de visi√≥n artificial de la "vieja escuela".  La primera t√©cnica es <strong>la sustracci√≥n de fondo</strong> , que trata de aislar todas las partes cambiadas de la imagen.  En mi caso, lo √∫nico que se mueve en el campo de visi√≥n de la c√°mara son los detalles de LEGO.  (Por supuesto, la cinta tambi√©n se mueve, pero como tiene un color uniforme, parece estacionaria para la c√°mara).  Separe estos detalles de LEGO del fondo, y la mitad del problema est√° resuelto. <br><br>  Para que funcione la sustracci√≥n del fondo, los objetos en primer plano deben ser significativamente diferentes del fondo.  Los detalles de LEGO tienen una amplia gama de colores, as√≠ que tuve que elegir el color de fondo con mucho cuidado para que estuviera lo m√°s alejado posible de los colores de LEGO.  Es por eso que la cinta debajo de la c√°mara est√° hecha de papel: no solo debe ser muy uniforme, sino que tampoco puede consistir en LEGO, de lo contrario, ¬°ser√° del color de una de las partes que necesito reconocer!  Eleg√≠ el rosa p√°lido, pero cualquier otro color pastel, a diferencia de los colores habituales de LEGO, servir√°. <br><br>  La maravillosa biblioteca OpenCV ya tiene varios algoritmos para sustracci√≥n de fondo.  MOG2 Background Subtractor es el m√°s complejo de ellos y, al mismo tiempo, funciona incre√≠blemente r√°pido incluso en frambuesa pi.  Sin embargo, alimentar fotogramas de video directamente a MOG2 no funciona muy bien.  Las figuras de color gris claro y blanco est√°n demasiado cerca del brillo de un fondo p√°lido y se pierden en √©l.  Necesitaba encontrar una manera de separar m√°s claramente la cinta de las partes en ella, ordenando al sustractor de fondo que mirara m√°s de cerca el <i>color</i> y no el <i>brillo</i> .  Para hacer esto, fue suficiente para m√≠ aumentar la saturaci√≥n de las im√°genes antes de transferirlas a un sustractor de fondo.  Los resultados han mejorado significativamente. <br><br>  Despu√©s de restar el fondo, necesitaba usar operaciones morfol√≥gicas para eliminar el mayor ruido posible.  Para encontrar los contornos de las √°reas blancas, puede usar la funci√≥n findContours () de la biblioteca OpenCV.  Al aplicar varias heur√≠sticas para desviar los bucles que contienen ruido, puede convertir f√°cilmente estos bucles en cuadros delimitadores predefinidos. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/214/785/f54/214785f5468b2b84ae29f212a66d5abd.gif"></div><br><h2>  Rendimiento </h2><br>  Una red neuronal es una criatura voraz.  Para obtener los mejores resultados en la clasificaci√≥n, necesita im√°genes de la m√°xima resoluci√≥n y en la mayor cantidad posible.  Esto significa que necesito dispararlos a una velocidad de cuadro muy alta, manteniendo la calidad y resoluci√≥n de la imagen.  Tengo que exprimir al m√°ximo la c√°mara y la GPU Raspberry PI. <br><br>  Una <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">documentaci√≥n</a> muy detallada <a href="https://picamera.readthedocs.io/en/release-1.13/fov.html" rel="nofollow">para picamera</a> dice que el chip de la c√°mara V2 puede producir im√°genes de 1280x720 p√≠xeles de tama√±o con una frecuencia m√°xima de 90 cuadros por segundo.  Esta es una cantidad incre√≠ble de datos, y aunque la c√°mara puede generarlos, esto no significa que una computadora pueda manejarlos.  Si tuviera que procesar im√°genes RGB sin formato de 24 bits, tendr√≠a que transferir datos a una velocidad de aproximadamente 237 MB / s, que es demasiado para la pobre GPU de la computadora Pi y SDRAM.  Incluso cuando se usa compresi√≥n acelerada por GPU en JPEG, no se pueden lograr 90 fps. <br><br>  La Raspberry Pi es capaz de mostrar im√°genes YUV sin filtrar y sin procesar.  Aunque es m√°s dif√≠cil trabajar con √©l que con RGB, YUV en realidad tiene muchas propiedades convenientes.  El m√°s importante de ellos es que almacena solo 12 bits por p√≠xel (para RGB es de 24 bits). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d50/28a/686/d5028a6862f79b8caf4d6f895dd46d84.png"></div><br>  <i>Cada cuatro bytes de Y tienen un byte U y un byte V, es decir, 1.5 bytes por p√≠xel.</i> <br><br>  Esto significa que, en comparaci√≥n con los cuadros RGB, puedo procesar el <strong>doble de</strong> cuadros YUV, y esto sin contar el tiempo adicional que la GPU ahorra al convertir a imagen RGB. <br><br>  Sin embargo, este enfoque impone restricciones √∫nicas en el proceso de procesamiento.  La mayor√≠a de las operaciones con un cuadro de video de tama√±o completo consumir√°n una cantidad extremadamente grande de memoria y recursos de CPU.  Dentro de mis estrictos l√≠mites de tiempo, ni siquiera es posible decodificar un marco YUV de pantalla completa. <br><br>  Afortunadamente, no necesito procesar todo el marco.  Para el reconocimiento de objetos, los rect√°ngulos delimitadores no tienen que ser precisos, la precisi√≥n aproximada es suficiente, por lo que todo el proceso de reconocimiento de objetos se puede realizar con un marco mucho m√°s peque√±o.  La operaci√≥n de alejamiento no es necesaria para tener en cuenta todos los p√≠xeles de un cuadro de tama√±o completo, por lo que los cuadros se pueden reducir muy r√°pidamente y sin costo.  Luego, la escala de los rect√°ngulos delimitadores resultantes aumenta nuevamente y se usa para cortar objetos de un marco YUV de tama√±o completo.  Gracias a esto, no necesito decodificar o procesar el marco completo de alta resoluci√≥n. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fcd/d6a/9e4/fcdd6a9e466f0b87be4dbb349f19e402.png"></div><br>  Afortunadamente, gracias al m√©todo de almacenamiento de este formato YUV (ver arriba), es muy f√°cil implementar operaciones de recorte y zoom r√°pidas que funcionan directamente con el formato YUV.  Adem√°s, todo el proceso puede ser paralelo a cuatro n√∫cleos Pi sin ning√∫n problema.  Sin embargo, descubr√≠ que no todos los n√∫cleos se utilizan en todo su potencial, y esto nos dice que el ancho de banda de la memoria sigue siendo el cuello de botella.  Pero aun as√≠, logr√© alcanzar 70-80FPS en la pr√°ctica.  Un an√°lisis m√°s profundo del uso de la memoria podr√≠a ayudar a acelerar las cosas a√∫n m√°s. <br><br><hr><br>  Si quieres saber m√°s sobre el proyecto, lee mi art√≠culo anterior, <a rel="nofollow" href="https://towardsdatascience.com/how-i-created-over-100-000-labeled-lego-training-images-ec74191bb4ef">"C√≥mo cre√© m√°s de 100 mil im√°genes LEGO etiquetadas para el aprendizaje"</a> . <br><br>  Video del funcionamiento de toda la m√°quina de clasificaci√≥n: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/04JkdHEX3Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/480294/">https://habr.com/ru/post/480294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../480280/index.html">Estudio de seguridad TurboConf</a></li>
<li><a href="../480282/index.html">La historia de la startup de fitness Peloton: de una valoraci√≥n de $ 8 mil millones a publicidad fallida y pron√≥sticos de una ca√≠da del 85% en las existencias</a></li>
<li><a href="../480284/index.html">Mi (nano) experiencia con Yandex.Maps API o por qu√© necesito instrucciones</a></li>
<li><a href="../480288/index.html">¬øEs posible transmitir y recibir informaci√≥n sin restricciones sobre la distancia y la velocidad de la luz?</a></li>
<li><a href="../480290/index.html">Port√°til casero ZedRipper en diecis√©is Z80</a></li>
<li><a href="../480296/index.html">Desarrollo reactivo de Telegram bot</a></li>
<li><a href="../480300/index.html">En 2011, la cuesti√≥n de si Nginx pertenece a Igor Sysoev o Rambler</a></li>
<li><a href="../480302/index.html">Pioneros y pioneros. Impresora de construcci√≥n circular 3D: ¬øc√≥mo comenz√≥ todo?</a></li>
<li><a href="../480304/index.html">Inferencia de tipos en jscodeshift y TypeScript</a></li>
<li><a href="../480306/index.html">¬øPor qu√© vencer a la puerta cerrada?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>