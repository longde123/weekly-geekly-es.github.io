<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ¦… ğŸ¤ğŸ» ğŸ¥‘ Membangun sistem moderasi pesan otomatis ğŸ›· ğŸ›… ğŸ•µğŸ¾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sistem moderasi otomatis diimplementasikan dalam layanan web dan aplikasi di mana diperlukan untuk memproses sejumlah besar pesan pengguna. Sistem sem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Membangun sistem moderasi pesan otomatis</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/454628/"><img src="https://habrastorage.org/webt/xu/yp/u9/xuypu9acrj8o6qbkrhu53ue4kni.png" alt="gambar"><br>  Sistem moderasi otomatis diimplementasikan dalam layanan web dan aplikasi di mana diperlukan untuk memproses sejumlah besar pesan pengguna.  Sistem semacam itu dapat mengurangi biaya moderasi manual, mempercepatnya dan memproses semua pesan pengguna secara real-time.  Dalam artikel ini, kita akan berbicara tentang membangun sistem moderasi otomatis untuk memproses bahasa Inggris menggunakan algoritma pembelajaran mesin.  Kami akan membahas seluruh pekerjaan pipa mulai dari tugas-tugas penelitian dan pilihan algoritma ML untuk diluncurkan ke produksi.  Mari kita lihat di mana mencari sendiri dataset yang sudah jadi dan bagaimana cara mengumpulkan data untuk tugas itu sendiri. <br><a name="habracut"></a><br><br>  <i>Disiapkan dengan Ira Stepanyuk ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">id_step</a> ), Data Scientist di Poteha Labs</i> <br><br><h2>  Deskripsi tugas </h2><br>  Kami bekerja dengan obrolan aktif multi-pengguna, di mana pesan singkat dari puluhan pengguna dapat datang dalam satu obrolan setiap menit.  Tugasnya adalah untuk menyoroti pesan dan pesan beracun dengan komentar cabul dalam dialog dari obrolan tersebut.  Dari sudut pandang pembelajaran mesin, ini adalah tugas klasifikasi biner, di mana setiap pesan harus ditugaskan ke salah satu kelas. <br><br>  Untuk mengatasi masalah ini, pertama-tama, perlu untuk memahami apa pesan beracun itu dan apa yang sebenarnya membuatnya beracun.  Untuk melakukan ini, kami melihat sejumlah besar pesan khas pengguna di Internet.  Berikut adalah beberapa contoh yang telah kami bagi menjadi pesan beracun dan normal. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Toksik </th><th>  Normal </th></tr><tr><td>  Anda adalah homo sialan * ot </td><td>  buku ini sangat bodoh </td></tr><tr><td>  anak Anda sangat jelek (1) </td><td>  Pemenang menang, pecundang membuat alasan </td></tr><tr><td>  Orang kulit putih adalah pemilik kulit hitam (2) </td><td>  hitam seperti jiwaku (2) </td></tr></tbody></table></div><br>  Dapat dilihat bahwa pesan beracun sering mengandung kata-kata cabul, tapi tetap saja ini bukan prasyarat.  Pesan tersebut mungkin tidak mengandung kata-kata yang tidak pantas, tetapi menyinggung seseorang (contoh (1)).  Selain itu, terkadang pesan beracun dan normal berisi kata-kata yang sama yang digunakan dalam konteks yang berbeda - menyinggung atau tidak (contoh (2)).  Pesan seperti itu juga harus dapat dibedakan. <br>  Setelah mempelajari berbagai pesan, untuk sistem moderasi kami, kami menyebut pesan itu <b><i>beracun</i></b> yang berisi pernyataan yang cabul, ekspresi menghina atau kebencian terhadap seseorang. <br><br><h2>  Data </h2><br><h4>  Buka data </h4><br>  Salah satu dataset moderasi paling terkenal adalah dataset dari <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tantangan Klasifikasi Komentar</a> Kaggle <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Toxic</a> .  Bagian dari markup dalam dataset tidak benar: misalnya, pesan dengan kata-kata cabul dapat ditandai sebagai normal.  Karena itu, Anda tidak bisa hanya mengikuti kompetisi Kernel dan mendapatkan algoritma klasifikasi yang berfungsi dengan baik.  Anda perlu lebih banyak bekerja dengan data, melihat contoh mana yang tidak cukup, dan menambahkan data tambahan dengan contoh tersebut. <br><br>  Selain kompetisi, ada beberapa publikasi ilmiah dengan tautan ke dataset yang sesuai ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">misalnya</a> ), tetapi tidak semua dapat digunakan dalam proyek komersial.  Sebagian besar kumpulan data ini berisi pesan dari jejaring sosial Twitter, tempat Anda dapat menemukan banyak tweet beracun.  Selain itu, data dikumpulkan dari Twitter, karena tagar tertentu dapat digunakan untuk mencari dan menandai pesan pengguna yang beracun. <br><br><h4>  Data manual </h4><br>  Setelah kami mengumpulkan dataset dari sumber terbuka dan melatihnya model dasar, menjadi jelas bahwa data terbuka tidak cukup: kualitas model tidak memuaskan.  Selain membuka data untuk menyelesaikan masalah, pilihan pesan yang tidak terisi dari messenger game dengan sejumlah besar pesan beracun juga tersedia bagi kami. <br><br><img src="https://habrastorage.org/webt/eh/sp/5o/ehsp5oivhvjnfgxrnjqc7wszf9u.gif" alt="gambar"><br><br>  Untuk menggunakan data ini untuk tugas mereka, mereka harus diberi label entah bagaimana.  Pada saat itu, sudah ada classifier dasar terlatih, yang kami putuskan untuk digunakan untuk penandaan semi-otomatis.  Setelah menjalankan semua pesan melalui model, kami mendapatkan probabilitas toksisitas setiap pesan dan diurutkan dalam urutan menurun.  Pada awal daftar ini dikumpulkan pesan-pesan dengan kata-kata cabul dan menyinggung.  Pada akhirnya, sebaliknya, ada pesan pengguna normal.  Dengan demikian, sebagian besar data (dengan nilai probabilitas yang sangat besar dan sangat kecil) tidak dapat ditandai, tetapi segera ditugaskan ke kelas tertentu.  Tetap menandai pesan yang jatuh di tengah daftar, yang dilakukan secara manual. <br><br><h4>  Augmentasi Data </h4><br>  Seringkali dalam kumpulan data Anda dapat melihat pesan yang diubah di mana pengklasifikasi tersebut salah, dan orang tersebut benar memahami artinya. <br>  Ini karena pengguna menyesuaikan dan belajar menipu sistem moderasi sehingga algoritma membuat kesalahan pada pesan beracun dan artinya tetap jelas bagi orang tersebut.  Apa yang sedang dilakukan pengguna: <br><br><ul><li>  kesalahan ketik menghasilkan: <i>Anda adalah orang tolol, mengacaukan Anda</i> , </li><li>  ganti karakter alfabet dengan angka yang serupa dalam uraian: <i>n1gga, b0ll0cks</i> , </li><li>  masukkan spasi tambahan: <i>idiot</i> , </li><li>  hapus spasi di antara kata-kata: <i>dieyoustupid</i> . </li></ul><br><br>  Untuk melatih pengklasifikasi yang tahan terhadap penggantian seperti itu, Anda harus melakukan seperti yang dilakukan pengguna: menghasilkan perubahan yang sama dalam pesan dan menambahkannya ke pelatihan yang diatur ke data utama. <br>  Secara umum, perjuangan ini tidak terhindarkan: pengguna akan selalu berusaha menemukan kerentanan dan peretasan, dan moderator akan menerapkan algoritma baru. <br><br><h3>  Deskripsi subtugas </h3><br>  Kami dihadapkan dengan subtugas untuk menganalisis pesan dalam dua mode berbeda: <br><br><ul><li>  mode online - analisis pesan secara real-time, dengan kecepatan respons maksimum; </li><li>  mode offline - analisis log pesan dan alokasi dialog beracun. </li></ul><br>  Dalam mode online, kami memproses setiap pesan pengguna dan menjalankannya melalui model.  Jika pesan itu beracun, sembunyikan di antarmuka obrolan, dan jika itu normal, maka tampilkan.  Dalam mode ini, semua pesan harus diproses dengan sangat cepat: model harus memberikan respons secepatnya agar tidak mengganggu struktur dialog antar pengguna. <br>  Dalam mode offline, tidak ada batasan waktu untuk bekerja, dan karena itu saya ingin mengimplementasikan model dengan kualitas tertinggi. <br><br><h3>  Mode online.  Pencarian Kamus </h3><br>  Terlepas dari model mana yang dipilih berikutnya, kita harus menemukan dan memfilter pesan dengan kata-kata cabul.  Untuk menyelesaikan masalah ini, paling mudah untuk menyusun kamus kata-kata dan ekspresi yang tidak valid yang tidak dapat dilewati, dan mencari kata-kata tersebut di setiap pesan.  Pencarian harus cepat, sehingga algoritma pencarian substring naif untuk waktu itu tidak cocok.  Algoritma yang cocok untuk menemukan serangkaian kata dalam string adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">algoritma Aho-Korasik</a> .  Karena pendekatan ini, dimungkinkan untuk dengan cepat mengidentifikasi beberapa contoh beracun dan memblokir pesan sebelum dikirim ke algoritma utama.  Menggunakan algoritma ML akan memungkinkan Anda untuk "memahami arti" dari pesan dan meningkatkan kualitas klasifikasi. <br><br><h3>  Mode online.  Model pembelajaran mesin dasar </h3><br>  Untuk model dasar, kami memutuskan untuk menggunakan pendekatan standar untuk klasifikasi teks: Algoritma klasifikasi klasik TF-IDF +.  Lagi karena alasan kecepatan dan kinerja. <br><br>  TF-IDF adalah ukuran statistik yang memungkinkan Anda menentukan kata-kata paling penting untuk teks dalam tubuh menggunakan dua parameter: frekuensi kata dalam setiap dokumen dan jumlah dokumen yang mengandung kata tertentu (lebih terinci di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> ).  Setelah menghitung untuk setiap kata dalam pesan TF-IDF, kami mendapatkan representasi vektor dari pesan ini. <br>  TF-IDF dapat dihitung untuk kata-kata dalam teks, serta untuk kata-kata dan karakter n-gram.  Perpanjangan seperti itu akan bekerja lebih baik, karena ia akan mampu menangani frasa dan kata yang sering muncul yang tidak ada dalam perangkat pelatihan (out-of-vocabulary). <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TfidfVectorizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sparse vect_word = TfidfVectorizer(max_features=<span class="hljs-number"><span class="hljs-number">10000</span></span>, lowercase=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, analyzer=<span class="hljs-string"><span class="hljs-string">'word'</span></span>, min_df=<span class="hljs-number"><span class="hljs-number">8</span></span>, stop_words=stop_words, ngram_range=(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)) vect_char = TfidfVectorizer(max_features=<span class="hljs-number"><span class="hljs-number">30000</span></span>, lowercase=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, analyzer=<span class="hljs-string"><span class="hljs-string">'char'</span></span>, min_df=<span class="hljs-number"><span class="hljs-number">8</span></span>, ngram_range=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>)) x_vec_word = vect_word.fit_transform(x_train) x_vec_char = vect_char.fit_transform(x_train) x_vec = sparse.hstack([x_vec_word, x_vec_char])</code> </pre>  <i>Contoh menggunakan TF-IDF pada n-gram kata dan karakter</i> <br><br>  Setelah mengonversi pesan ke vektor, Anda dapat menggunakan metode klasik apa pun untuk klasifikasi: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">regresi logistik, SVM</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hutan acak, peningkatan</a> . <br><br>  Kami memutuskan untuk menggunakan regresi logistik dalam tugas kami, karena model ini memberikan peningkatan kecepatan dibandingkan dengan pengklasifikasi ML klasik lainnya dan memprediksi probabilitas kelas, yang memungkinkan Anda untuk secara fleksibel memilih ambang klasifikasi dalam produksi. <br><br>  Algoritma yang diperoleh dengan menggunakan TF-IDF dan regresi logistik dengan cepat berfungsi dan mendefinisikan pesan dengan baik dengan kata-kata dan ekspresi yang cabul, tetapi tidak selalu memahami artinya.  Misalnya, sering pesan dengan kata-kata ' <i>hitam</i> ' dan ' <i>feminizm</i> ' masuk dalam kelas beracun.  Saya ingin memperbaiki masalah ini dan belajar untuk lebih memahami arti pesan menggunakan versi classifier selanjutnya. <br><br><h3>  Mode offline </h3><br>  Untuk lebih memahami makna pesan, Anda dapat menggunakan algoritma jaringan saraf: <br><br><ul><li>  Embeddings (Word2Vec, FastText) </li><li>  Jaringan Saraf Tiruan (CNN, RNN, LSTM) </li><li>  Model pra-terlatih baru (ELMo, ULMFiT, BERT) </li></ul><br>  Kami akan membahas beberapa algoritma ini dan bagaimana mereka dapat digunakan secara lebih rinci. <br><br><h4>  Word2Vec dan FastText </h4><br>  Menanamkan model memungkinkan Anda untuk mendapatkan representasi vektor kata dari teks.  Ada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dua jenis Word2Vec</a> : Skip-gram dan CBOW (Continuous Bag of Words).  Dalam Lewati-gram, konteksnya diprediksi oleh kata, tetapi dalam CBOW, sebaliknya: kata tersebut diprediksi oleh konteks. <br><img src="https://habrastorage.org/webt/rc/kb/iv/rckbivfc1dvmna3bvccy6xoai_g.png" alt="gambar"><br>  Model semacam itu dilatih pada kumpulan besar teks dan memungkinkan Anda untuk mendapatkan representasi kata-kata vektor dari lapisan tersembunyi dari jaringan saraf yang terlatih.  Kerugian dari arsitektur ini adalah bahwa model belajar dari serangkaian kata-kata terbatas yang terkandung dalam corpus.  Ini berarti bahwa untuk semua kata yang tidak ada di badan teks pada tahap pelatihan, tidak akan ada embeddings.  Dan situasi ini sering terjadi ketika model pra-pelatihan digunakan untuk tugas mereka: untuk beberapa kata tidak akan ada embeddings, sehingga sejumlah besar informasi yang berguna akan hilang. <br><br>  Untuk mengatasi masalah dengan kata-kata yang tidak ada dalam kamus (OOV, out-of-vocabulary) ada model penyematan ditingkatkan - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FastText</a> .  Alih-alih menggunakan kata-kata tunggal untuk melatih jaringan saraf, FastText membagi kata-kata menjadi n-gram (subword) dan belajar darinya.  Untuk mendapatkan representasi vektor dari suatu kata, Anda perlu mendapatkan representasi vektor dari n-gram kata ini dan menambahkannya. <br><br>  Dengan demikian, model Word2Vec dan FastText yang telah dilatih sebelumnya dapat digunakan untuk mendapatkan vektor fitur dari pesan.  Karakteristik yang diperoleh dapat diklasifikasikan dengan menggunakan pengklasifikasi ML klasik atau jaringan saraf yang terhubung penuh. <br><br><img src="https://habrastorage.org/webt/jb/bp/ma/jbbpma-miqfsht7roadxuk2bap4.png" alt="gambar"><br>  <i>Contoh output dari kata "terdekat" dalam arti menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">FastText yang sudah dilatih sebelumnya</a></i> <br><br><h4>  CNN Classifier </h4><br>  Untuk pemrosesan dan klasifikasi teks dari algoritma jaringan saraf, jaringan berulang (LSTM, GRU) lebih sering digunakan, karena mereka bekerja dengan urutan yang baik.  Jaringan konvolusional (CNN) paling sering digunakan untuk pemrosesan gambar, tetapi mereka juga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dapat digunakan</a> dalam tugas klasifikasi teks.  Pertimbangkan bagaimana ini bisa dilakukan. <br>  Setiap pesan adalah sebuah matriks di mana pada setiap baris untuk token (kata) representasi vektornya ditulis.  Konvolusi diterapkan pada matriks semacam itu dengan cara tertentu: filter konvolusi â€œmeluncurâ€ ke seluruh baris matriks (vektor kata), tetapi ia menangkap beberapa kata sekaligus (biasanya 2-5 kata), sehingga memproses kata-kata tersebut dalam konteks kata-kata tetangga.  Detail tentang bagaimana hal ini terjadi dapat dilihat pada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">gambar</a> . <br><img src="https://habrastorage.org/webt/hx/yd/vf/hxydvfho2bzzmgyjedkkt9lz9gc.png" alt="gambar"><br>  Mengapa menggunakan jaringan convolutional untuk pengolah kata ketika Anda dapat menggunakan berulang?  Faktanya adalah konvolusi bekerja lebih cepat.  Menggunakannya untuk klasifikasi pesan, Anda dapat sangat menghemat waktu untuk pelatihan. <br><br><h4>  ELMo </h4><br>  ELMo (Embeddings from Language Models) adalah model penyematan berdasarkan model bahasa yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">baru saja diperkenalkan</a> .  Model penyematan baru berbeda dari model Word2Vec dan FastText.  Vektor kata ELMo memiliki kelebihan tertentu: <br><br><ul><li>  Penyajian setiap kata tergantung pada keseluruhan konteks tempat kata itu digunakan. </li><li>  Representasi didasarkan pada simbol, yang memungkinkan pembentukan representasi yang andal untuk kata-kata OOV (out-of-vocabulary). </li></ul><br><br>  ELMo dapat digunakan untuk berbagai tugas di NLP.  Misalnya, untuk tugas kami, vektor pesan yang diterima menggunakan ELMo dapat dikirim ke classifier ML klasik atau menggunakan jaringan yang konvolusional atau terhubung sepenuhnya. <br>  Embedded pra-terlatih ELMo cukup mudah digunakan untuk tugas Anda, contoh penggunaan dapat ditemukan di <a href="">sini</a> . <br><br><h3>  Fitur Implementasi </h3><br><h4>  API Labu </h4><br>  API prototipe ditulis dalam Flask, karena mudah digunakan. <br><br><h4>  Dua Gambar Docker </h4><br>  Untuk penyebaran, kami menggunakan dua gambar buruh pelabuhan: yang dasar, di mana semua dependensi diinstal, dan yang utama untuk meluncurkan aplikasi.  Ini sangat menghemat waktu perakitan, karena gambar pertama jarang dibangun kembali, dan ini menghemat waktu selama penyebaran.  Cukup banyak waktu dihabiskan untuk membangun dan mengunduh perpustakaan pembelajaran mesin, yang tidak perlu dilakukan setiap komitmen. <br><br><h4>  Pengujian </h4><br>  Keunikan penerapan sejumlah besar algoritma pembelajaran mesin adalah bahwa bahkan dengan metrik tinggi pada dataset validasi, kualitas nyata dari algoritma dalam produksi dapat rendah.  Oleh karena itu, untuk menguji operasi algoritma, seluruh tim menggunakan bot di Slack.  Ini sangat nyaman, karena setiap anggota tim dapat memeriksa respons apa yang diberikan algoritma untuk pesan tertentu.  Metode pengujian ini memungkinkan Anda untuk segera melihat bagaimana algoritma akan bekerja pada data langsung. <br>  Alternatif yang baik adalah meluncurkan solusi di situs publik seperti Yandex Toloka dan AWS Mechanical Turk. <br><br><h3>  Kesimpulan </h3><br>  Kami memeriksa beberapa pendekatan untuk memecahkan masalah moderasi pesan otomatis dan menjelaskan fitur-fitur dari implementasi kami. <br>  Pengamatan utama yang diperoleh selama bekerja: <br><br><ul><li>  Pencarian kamus dan algoritma pembelajaran mesin berdasarkan TF-IDF dan regresi logistik memungkinkan untuk mengklasifikasikan pesan dengan cepat, tetapi tidak selalu dengan benar. </li><li>  Algoritma jaringan saraf dan model embeddings pra-terlatih lebih baik mengatasi tugas ini dan dapat menentukan toksisitas dalam makna pesan. </li></ul><br><br>  Tentu saja, kami memasang demo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Poteha Toxic Comment Detection yang terbuka</a> di bot <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Facebook</a> .  Bantu kami membuat bot lebih baik! <br><br>  Saya akan dengan senang hati menjawab pertanyaan di komentar. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id454628/">https://habr.com/ru/post/id454628/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id454616/index.html">Penggunaan AI untuk meningkatkan efisiensi pekerja mental</a></li>
<li><a href="../id454618/index.html">Lubang Produktivitas: Bagaimana Slack Menyakiti Alur Kerja Kami</a></li>
<li><a href="../id454620/index.html">#NoDeployFriday: membantu atau membahayakan?</a></li>
<li><a href="../id454622/index.html">Kreisel EVEX 910e: model historis - kehidupan baru</a></li>
<li><a href="../id454626/index.html">DevOops kemarin dan hari ini</a></li>
<li><a href="../id454630/index.html">Situasi luar biasa: bagian 1 dari 4</a></li>
<li><a href="../id454634/index.html">Minggu Keamanan 23: Kerentanan notepad, sejuta sistem dengan RDP yang tidak ditatch</a></li>
<li><a href="../id454640/index.html">Melakukan debug layanan microser jarak jauh melalui SSH di bawah VPN dalam 4 putaran</a></li>
<li><a href="../id454642/index.html">"" Buat aplikasi untuk orang-orang "- ini tidak harus dituliskan di lutut": tentang pengembangan ponsel di CFT</a></li>
<li><a href="../id454644/index.html">Pelatihan Cisco 200-125 CCNA v3.0. Hari 8. Beralih pengaturan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>