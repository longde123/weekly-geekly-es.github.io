<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëè üîá üè™ Bagaimana Kami Meningkatkan Tensorflow Melayani Produktivitas hingga 70% üö¶ üñåÔ∏è üîè</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow telah menjadi platform standar untuk pembelajaran mesin (ML), populer baik di industri maupun dalam penelitian. Banyak perpustakaan, alat, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bagaimana Kami Meningkatkan Tensorflow Melayani Produktivitas hingga 70%</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/445928/"> Tensorflow telah menjadi platform standar untuk pembelajaran mesin (ML), populer baik di industri maupun dalam penelitian.  Banyak perpustakaan, alat, dan kerangka kerja gratis telah dibuat untuk melatih dan memelihara model ML.  Proyek Tensorflow Melayani membantu mempertahankan model ML dalam lingkungan produksi terdistribusi. <br><br>  Layanan Mux kami menggunakan Tensorflow Serving di beberapa bagian infrastruktur, kami telah membahas penggunaan Tensorflow Serving dalam mengkodekan judul video.  Hari ini kami akan fokus pada metode yang meningkatkan latensi dengan mengoptimalkan server perkiraan dan klien.  Perkiraan model biasanya adalah operasi "online" (di jalur kritis meminta aplikasi), oleh karena itu, tujuan utama optimasi adalah untuk memproses volume besar permintaan dengan keterlambatan serendah mungkin. <br><a name="habracut"></a><br><h1>  Apa yang dimaksud dengan Melayani Tensorflow? </h1><br>  Tensorflow Serving menyediakan arsitektur server yang fleksibel untuk menggunakan dan memelihara model ML.  Setelah model dilatih dan siap digunakan untuk peramalan, Penyajian Tensorflow mengharuskan mengekspornya ke format yang kompatibel (dapat ditayangkan). <br><br>  <i>Servable</i> adalah abstraksi pusat yang membungkus objek Tensorflow.  Sebagai contoh, suatu model dapat direpresentasikan sebagai satu atau lebih objek yang dapat Servable.  Dengan demikian, Servable adalah objek dasar yang digunakan klien untuk melakukan perhitungan.  Masalah ukuran yang dapat diservis: model yang lebih kecil membutuhkan lebih sedikit ruang, menggunakan lebih sedikit memori dan memuat lebih cepat.  Untuk mengunduh dan memelihara menggunakan Predict API, model harus dalam format SavedModel. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving menggabungkan komponen dasar untuk membuat server gRPC / HTTP yang melayani beberapa model ML (atau beberapa versi), menyediakan komponen pemantauan dan arsitektur khusus. <br><br><h1>  Tensorflow Melayani dengan Docker </h1><br>  Mari kita lihat metrik dasar latensi dalam memperkirakan kinerja dengan pengaturan Tensorflow Serving standar (tanpa optimasi CPU). <br><br>  Pertama, unduh gambar terbaru dari hub TensorFlow Docker: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  Dalam artikel ini, semua kontainer berjalan di host dengan empat core, 15 GB, Ubuntu 16.04. <br><br><h3>  Ekspor Model Tensorflow ke SavedModel </h3><br>  Ketika model dilatih menggunakan Tensorflow, output dapat disimpan sebagai titik kontrol variabel (file pada disk).  Output dilakukan secara langsung dengan mengembalikan titik kontrol model atau dalam format grafik beku beku (file biner). <br><br>  Untuk Melayani Tensorflow, grafik beku ini perlu diekspor ke format SavedModel.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Dokumentasi Tensorflow</a> berisi contoh mengekspor model yang terlatih ke format SavedModel. <br><br>  Tensorflow juga menyediakan banyak model <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">resmi dan penelitian</a> sebagai titik awal untuk eksperimen, penelitian, atau produksi. <br><br>  Sebagai contoh, kita akan menggunakan model <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">deep residual neural network (ResNet)</a> untuk mengklasifikasikan dataset ImageNet dari 1000 kelas.  Unduh model <code>ResNet-50 v2</code> yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">telah</a> <code>ResNet-50 v2</code> , khususnya pilihan Channels_last (NHWC) di <i>SavedModel</i> : sebagai aturan, ini berfungsi lebih baik pada CPU. <br><br>  Salin direktori model RestNet ke dalam struktur berikut: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving mengharapkan struktur direktori yang dipesan secara numerik untuk versi.  Dalam kasus kami, direktori <code>1/</code> sesuai dengan model versi 1, yang berisi arsitektur model <code>saved_model.pb</code> dengan snapshot bobot model (variabel). <br><br><h3>  Memuat dan memproses SavedModel </h3><br>  Perintah berikut memulai server model Melayani Tensorflow dalam wadah Docker.  Untuk memuat SavedModel, Anda harus memasang direktori model di direktori kontainer yang diharapkan. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  Memeriksa log kontainer menunjukkan bahwa ModelServer sudah aktif dan berjalan untuk menangani permintaan output untuk model <code>resnet</code> di titik akhir gRPC dan HTTP: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Memprakirakan Klien </h3><br>  Tensorflow Serving mendefinisikan skema API dalam format <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">buffer protokol</a> (protobufs).  Implementasi klien GRPC untuk API peramalan dikemas sebagai paket Python <code>tensorflow_serving.apis</code> .  Kita akan membutuhkan <code>tensorflow</code> paket Python lain untuk fungsi utilitas. <br><br>  Instal dependensi untuk membuat klien sederhana: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  Model <code>ResNet-50 v2</code> mengharapkan input tensor floating point dalam struktur data diformat kanal_last (NHWC).  Oleh karena itu, gambar input dibaca menggunakan opencv-python dan dimuat ke array numpy (tinggi √ó lebar √ó saluran) sebagai tipe data float32.  Skrip di bawah ini membuat rintisan klien prediksi dan memuat data JPEG ke dalam array numpy, mengubahnya menjadi tensor_proto untuk membuat permintaan perkiraan gRPC: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Setelah menerima input JPEG, klien yang bekerja akan menghasilkan hasil berikut: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  Tensor yang dihasilkan berisi perkiraan dalam bentuk nilai integer dan probabilitas tanda. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  Untuk satu permintaan, penundaan seperti itu tidak dapat diterima.  Tapi tidak ada yang mengejutkan: biner Tensorflow Serving secara default dirancang untuk rentang peralatan terluas untuk sebagian besar kasus penggunaan.  Anda mungkin memperhatikan baris-baris berikut dalam log dari wadah Penyajian Tensorflow standar: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  Ini menunjukkan biner TensorFlow Melayani yang berjalan pada platform CPU yang belum dioptimalkan. <br><br><h3>  Membangun biner yang dioptimalkan </h3><br>  Menurut <a href="">dokumentasi</a> Tensorflow, direkomendasikan untuk mengkompilasi Tensorflow dari sumber dengan semua optimisasi yang tersedia untuk CPU pada host di mana biner akan bekerja.  Saat perakitan, flag khusus memungkinkan aktivasi set instruksi CPU untuk platform tertentu: <br><br><div class="scrollable-table"><table><tbody><tr><th>  Set instruksi </th><th>  Bendera </th></tr><tr><td>  AVX </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  Semua didukung oleh prosesor </td><td>  --copt = -march = asli </td></tr></tbody></table></div><br>  Clone a Tensorflow Melayani versi tertentu.  Dalam kasus kami, ini adalah 1,13 (yang terakhir pada saat publikasi artikel ini): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  Gambar dev Tensorflow Melayani menggunakan alat Basel untuk membangun.  Kami mengkonfigurasinya untuk set instruksi CPU tertentu: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  Jika tidak ada cukup memori, batasi konsumsi memori selama proses build dengan flag <code>--local_resources=2048,.5,1.0</code> .  Untuk informasi tentang bendera, lihat bantuan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Tensorflow Serving dan Docker</a> , serta <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi Bazel</a> . <br><br>  Buat gambar yang berfungsi berdasarkan yang sudah ada: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  ModelServer dikonfigurasi menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">bendera TensorFlow</a> untuk mendukung konkurensi.  Opsi berikut mengonfigurasi dua kumpulan utas untuk operasi paralel: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  mengontrol jumlah maksimum utas untuk eksekusi paralel satu operasi; <br></li><li>  digunakan untuk memparalelkan operasi yang memiliki sub-operasi yang bersifat independen. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  mengontrol jumlah utas maksimum untuk pelaksanaan paralel operasi independen; <br></li><li>  Operasi Tensorflow Graph, yang independen satu sama lain dan, oleh karena itu, dapat dilakukan di utas yang berbeda. </li></ul><br>  Secara default, kedua parameter diatur ke <code>0</code> .  Ini berarti bahwa sistem itu sendiri memilih nomor yang sesuai, yang paling sering berarti satu utas per inti.  Namun, parameter dapat diubah secara manual untuk konkurensi multi-core. <br><br>  Kemudian jalankan wadah Penyajian dengan cara yang sama seperti yang sebelumnya, kali ini dengan gambar Docker dikompilasi dari sumber dan dengan bendera optimisasi Tensorflow untuk prosesor tertentu: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Log kontainer seharusnya tidak lagi menampilkan peringatan tentang CPU yang tidak ditentukan.  Tanpa mengubah kode pada permintaan perkiraan yang sama, penundaan berkurang sekitar 35,8%: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Meningkatkan kecepatan dalam perkiraan klien </h3><br>  Apakah masih mungkin untuk mempercepat?  Kami telah mengoptimalkan sisi server untuk CPU kami, tetapi penundaan lebih dari 1 detik tampaknya masih terlalu besar. <br><br>  Kebetulan memuat perpustakaan <code>tensorflow</code> dan <code>tensorflow</code> memberikan kontribusi yang signifikan terhadap keterlambatan tersebut.  Setiap panggilan yang tidak perlu ke <code>tf.contrib.util.make_tensor_proto</code> juga menambahkan sepersekian detik. <br><br>  Anda mungkin bertanya: "Bukankah kita perlu paket Python TensorFlow untuk benar-benar membuat permintaan prediksi ke server Tensorflow?"  Bahkan, tidak ada <i>kebutuhan</i> nyata untuk paket <code>tensorflow</code> dan <code>tensorflow</code> . <br><br>  Seperti disebutkan sebelumnya, API prediksi Tensorflow didefinisikan sebagai proto-buffer.  Oleh karena itu, dua dependensi eksternal dapat diganti dengan <code>tensorflow_serving</code> dan <code>tensorflow</code> sesuai - dan kemudian Anda tidak perlu menarik seluruh perpustakaan Tensorflow (berat) pada klien. <br><br>  Pertama, singkirkan <code>tensorflow_serving</code> dan <code>tensorflow_serving</code> dan tambahkan paket <code>grpcio-tools</code> . <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  Kloning <code>tensorflow/tensorflow</code> dan <code>tensorflow/serving</code> dan salin file protobuf berikut ke proyek klien: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Salin file-file protobuf ini ke direktori <code>protos/</code> dengan path asli dipertahankan: <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  Untuk kesederhanaan, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">prediction_service.proto</a> dapat disederhanakan untuk mengimplementasikan hanya Predict RPC agar tidak mengunduh dependensi bersarang dari RPC lain yang ditentukan dalam layanan.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Ini adalah</a> contoh dari <code>prediction_service.</code> disederhanakan. <br><br>  Buat implementasi gRPC Python menggunakan <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Sekarang seluruh modul <code>tensorflow_serving</code> dapat dihapus: <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... dan ganti dengan protobuffer yang dihasilkan dari <code>protos/tensorflow_serving/apis</code> : <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  Pustaka Tensorflow diimpor untuk menggunakan fungsi helper <code>make_tensor_proto</code> , yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">diperlukan untuk membungkus objek</a> python / numpy sebagai objek TensorProto. <br><br>  Dengan demikian, kita dapat mengganti dependensi dan fragmen kode berikut: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  mengimpor protobuffers dan membangun objek TensorProto: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  Skrip Python lengkap ada di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> .  Jalankan klien pemula yang diperbarui yang membuat permintaan prediksi untuk Penyajian Tensorflow yang dioptimalkan: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  Diagram berikut menunjukkan perkiraan waktu eksekusi dalam versi Tensorflow Serving yang dioptimalkan dibandingkan dengan standar, lebih dari 10 kali berjalan: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  Penundaan rata-rata menurun sekitar 3,38 kali. <br><br><h1>  Optimasi Bandwidth </h1><br>  Tensorflow Serving dapat dikonfigurasi untuk menangani sejumlah besar data.  Optimalisasi bandwidth biasanya dilakukan untuk pemrosesan batch "berdiri sendiri", di mana batas latensi yang ketat bukanlah persyaratan yang ketat. <br><br><h3>  Pemrosesan Gelombang Sisi Server </h3><br>  Seperti yang dinyatakan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dokumentasi</a> , pemrosesan batch sisi server didukung secara asli di Tensorflow Serving. <br><br>  Pertukaran antara latensi dan throughput ditentukan oleh parameter pemrosesan batch.  Mereka memungkinkan Anda untuk mencapai throughput maksimum yang mampu akselerator perangkat keras. <br><br>  Untuk mengaktifkan pengemasan, atur <code>--batching_parameters_file</code> <code>--enable_batching</code> dan <code>--batching_parameters_file</code> .  Parameter ditetapkan sesuai dengan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">SessionBundleConfig</a> .  Untuk sistem pada CPU, set <code>num_batch_threads</code> ke jumlah core yang tersedia.  Untuk GPU, lihat parameter yang sesuai di <a href="">sini</a> . <br><br>  Setelah mengisi seluruh paket di sisi server, permintaan penerbitan digabungkan menjadi satu permintaan besar (tensor), dan dikirim ke sesi Tensorflow dengan permintaan gabungan.  Dalam situasi ini, paralelisme CPU / GPU benar-benar terlibat. <br><br>  Beberapa kegunaan umum untuk pemrosesan batch Tensorflow: <br><br><ul><li>  Menggunakan permintaan klien asinkron untuk mengisi paket sisi server <br></li><li>  Pemrosesan batch yang lebih cepat dengan mentransfer komponen grafik model ke CPU / GPU <br></li><li>  Melayani permintaan dari beberapa model dari satu server <br></li><li>  Pemrosesan batch sangat disarankan untuk pemrosesan "offline" dari sejumlah besar permintaan </li></ul><br><h3>  Pemrosesan Batch Sisi Klien </h3><br>  Kelompok pemrosesan batch sisi klien beberapa permintaan masuk menjadi satu. <br><br>  Karena model ResNet sedang menunggu input dalam format NHWC (dimensi pertama adalah jumlah input), kami dapat menggabungkan beberapa gambar input menjadi satu permintaan RPC: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  Untuk paket gambar N, tensor output dalam respons akan berisi hasil prediksi untuk jumlah input yang sama.  Dalam kasus kami, N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Akselerasi perangkat keras </h1><br>  Beberapa kata tentang GPU. <br><br>  Proses pembelajaran secara alami menggunakan paralelisasi pada GPU, karena pembangunan jaringan saraf yang dalam membutuhkan komputasi besar untuk mencapai solusi optimal. <br><br>  Tetapi untuk hasil keluaran, paralelisasi tidak begitu jelas.  Seringkali Anda dapat mempercepat output dari jaringan saraf ke GPU, tetapi Anda perlu memilih dan menguji peralatan dengan cermat, dan melakukan analisis teknis dan ekonomi yang mendalam.  Paralelisasi perangkat keras lebih berharga untuk pemrosesan batch kesimpulan "otonom" (volume besar). <br><br>  Sebelum pindah ke GPU, pertimbangkan persyaratan bisnis dengan analisis biaya (moneter, operasional, teknis) yang cermat untuk keuntungan terbesar (mengurangi latensi, throughput tinggi). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id445928/">https://habr.com/ru/post/id445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id445918/index.html">20 tahun RollerCoaster Tycoon: wawancara dengan pencipta game</a></li>
<li><a href="../id445920/index.html">Langsung: cara mengekang pengembangan iOS di tim besar</a></li>
<li><a href="../id445922/index.html">Mengapa menonton siaran online jika Anda dapat membaca Habr</a></li>
<li><a href="../id445924/index.html">TREASURES: ketika jam tangan pintar menjadi aneh</a></li>
<li><a href="../id445926/index.html">Program Rahasia UFO AS juga telah meneliti lubang cacing dan dimensi ekstra.</a></li>
<li><a href="../id445932/index.html">Keamanan Aplikasi Klien: Kiat Praktis untuk Pengembang Front-End</a></li>
<li><a href="../id445936/index.html">Pengembangan elektronik. Tentang mikrokontroler di jari</a></li>
<li><a href="../id445940/index.html">AMA dengan Habr, v 7.0. Lemon, Donat, dan Berita</a></li>
<li><a href="../id445946/index.html">MWC: petunjuk penggunaan</a></li>
<li><a href="../id445948/index.html">Warisan dalam C ++: pemula, menengah, lanjutan</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>