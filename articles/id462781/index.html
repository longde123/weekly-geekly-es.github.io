<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ™ğŸ¿ ğŸ˜„ ğŸ• Analisis leksikal ekspresi reguler profesional ğŸ‘©ğŸ½â€ğŸ¤â€ğŸ‘¨ğŸ» ğŸ—½ ğŸ’”</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Penguraian teks selalu dimulai dengan analisis leksikal atau tokenizing. Ada cara mudah untuk menyelesaikan masalah ini untuk hampir semua bahasa meng...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Analisis leksikal ekspresi reguler profesional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/462781/"><p>  Penguraian teks selalu dimulai dengan analisis leksikal atau tokenizing.  Ada cara mudah untuk menyelesaikan masalah ini untuk hampir semua bahasa menggunakan ekspresi reguler.  Penggunaan lain dari regexp tua yang baik. </p><a name="habracut"></a><br><p>  Saya sering menghadapi tugas mengurai teks.  Untuk tugas-tugas sederhana, seperti menganalisis nilai yang dimasukkan pengguna, fungsi ekspresi reguler dasar sudah cukup.  Untuk tugas yang rumit dan berat seperti menulis kompiler atau analisis kode statis, Anda dapat menggunakan alat khusus (AntLR, JavaCC, Yacc).  Tetapi saya sering menemukan tugas-tugas tingkat menengah, ketika tidak ada cukup ekspresi reguler, tapi saya tidak ingin menarik alat-alat berat ke dalam proyek.  Selain itu, alat ini biasanya berfungsi pada tahap waktu kompilasi dan pada saat run-time tidak memungkinkan perubahan parameter analisis (misalnya, membentuk daftar kata kunci dari file atau tabel database). </p><br><p> Sebagai contoh, saya akan memberikan tugas yang muncul selama proses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">mempercepat query SQL</a> .  Kami menganalisis log kueri SQL kami dan ingin menemukan kueri "buruk" menurut aturan tertentu.  Misalnya, kueri tempat bidang yang sama diperiksa untuk serangkaian nilai menggunakan OR </p><br><pre><code class="sql hljs">name = 'John' OR name = 'Michael' OR name = 'Bob'</code> </pre> <br><p>  Kami ingin mengganti permintaan tersebut dengan </p><br><pre> <code class="sql hljs">name IN ('John', 'Michael', 'Bob')</code> </pre> <br><p>  Ekspresi reguler tidak bisa lagi mengatasinya, tetapi saya juga tidak ingin membuat parser SQL lengkap menggunakan AntLR.  Dimungkinkan untuk membagi teks permintaan menjadi token dan menggunakan kode sederhana untuk membuat penguraian tanpa alat khusus. </p><br><p>  Masalah ini dapat dipecahkan dengan menggunakan fungsionalitas dasar dari ekspresi reguler.  Mari kita coba untuk membagi query SQL menjadi token.  Kami akan melihat versi SQL yang disederhanakan agar tidak membebani teks dengan detail.  Untuk membuat lexer SQL lengkap, Anda harus menulis ekspresi reguler yang sedikit lebih kompleks. </p><br><p>  Berikut adalah serangkaian ekspresi untuk token bahasa SQL dasar: </p><br><pre> <code class="plaintext hljs">1. keyword : \b(?:select|from|where|group|by|order|or|and|not|exists|having|join|left|right|inner)\b 2. id : [A-Za-z][A-Za-z0-9]* 3. real_number : [0-9]+\.[0-9]* 4. number : [0-9]+ 5. string : '[^']*' 6. space : \s+ 7. comment : \-\-[^\n\r]* 8. operation : [+\-\*/.=\(\)]</code> </pre> <br><p>  Saya ingin memperhatikan ekspresi reguler untuk kata kunci </p><br><pre> <code class="plaintext hljs">keyword : \b(?:select|from|where|group|by|order|or|and|not|exists|having|join|left|right|inner)\b</code> </pre> <br><p>  Ini memiliki dua fitur. </p><br><ol><li>  Operator \ b digunakan di awal dan di akhir, misalnya, agar tidak memotong <strong>atau</strong> awalan dari <strong>organisasi</strong> kata, yang merupakan kata kunci dan yang beberapa mesin regex akan terpisah menjadi token terpisah tanpa menggunakan operator \ b. </li><li>  semua kata dikelompokkan oleh tanda kurung non-menangkap (? :) yang tidak menangkap pertandingan.  Ini akan digunakan di masa depan agar tidak melanggar pengindeksan ekspresi reguler parsial dalam ekspresi umum. </li></ol><br><p>  Sekarang Anda dapat menggabungkan semua ekspresi ini menjadi satu kesatuan, menggunakan pengelompokan dan operator <strong>|</strong> </p><br><pre> <code class="plaintext hljs">(\b(?:select|from|where|group|by|order|or|and|not|exists|having|join|left|right|inner)\b)|([A-Za-z][A-Za-z0-9]*)|([0-9]+\.[0-9]*)|([0-9]+)|('[^']*')|(\s+)|(\-\-[^\n\r]*)|([+\-\*/.=\(\)])</code> </pre> <br><p>  Sekarang Anda dapat mencoba menerapkan ekspresi ini ke ekspresi SQL, misalnya untuk itu </p><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>) <span class="hljs-comment"><span class="hljs-comment">-- count of 'Johns' FROM person WHERE name = 'John'</span></span></code> </pre> <br><p>  Berikut ini <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hasilnya</a> pada Regex Tester.  Dengan mengklik tautan, Anda dapat bermain-main dengan ekspresi dan hasil analisis.  Dapat dilihat bahwa, misalnya, <strong>SELECT</strong> berkorespondensi langsung dengan 1 grup, yang sesuai dengan jenis <strong>kata kunci</strong> . </p><br><p><img src="https://habrastorage.org/webt/2_/aq/mp/2_aqmpj7fpnwgnmgwlf-rtp9rr4.png" alt="gambar"></p><br><p>  Anda mungkin memperhatikan bahwa seluruh teks permintaan ternyata dibagi menjadi substring dan masing-masing sesuai dengan grup tertentu.  Dengan nomor grup, Anda dapat menghubungkannya dengan jenis token (token). </p><br><p>  Membuat algoritma yang diberikan menjadi program dalam bahasa pemrograman apa pun yang mendukung ekspresi reguler tidaklah sulit.  Ini adalah kelas kecil yang mengimplementasikan ini di Jawa. </p><br><div class="spoiler">  <b class="spoiler_title">RegexTokenizer.java (+ beberapa kelas lagi)</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> org.example; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.ArrayList; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.Enumeration; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.List; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.regex.Matcher; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.regex.Pattern; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.stream.Collectors; <span class="hljs-comment"><span class="hljs-comment">/** *    . *   ,         . */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">RegexTokenizer</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Enumeration</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Token</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// ,      private final String content; //         private final ITokenType[] tokenTypes; private final Matcher matcher; //    private int currentPosition = 0; /** * @param content    * @param tokenTypes         */ public RegexTokenizer(String content, ITokenType[] tokenTypes) { this.content = content; this.tokenTypes = tokenTypes; //       List&lt;String&gt; regexList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; tokenTypes.length; i++) { ITokenType tokenType = tokenTypes[i]; regexList.add("(?&lt;g" + i + "&gt;" + tokenType.getRegex() + ")"); } String regex = regexList.stream().collect(Collectors.joining("|")); Pattern pattern = Pattern.compile(regex, Pattern.CASE_INSENSITIVE); //    matcher = pattern.matcher(content); matcher.find(); } @Override public boolean hasMoreElements() { return currentPosition &lt; content.length(); } @Override public Token nextElement() { boolean found = currentPosition &gt; matcher.start() ? matcher.find() : true; int start = found ? matcher.start() : content.length(); int end = found ? matcher.end() : content.length(); if(found &amp;&amp; currentPosition == start) { currentPosition = end; //  -   for (int i = 0; i &lt; tokenTypes.length; i++) { String si = "g" + i; if (matcher.start(si) == start &amp;&amp; matcher.end(si) == end) { return createToken(content, tokenTypes[i], start, end); } } } throw new IllegalStateException("      " + currentPosition); } /** *  -  ,    ,     , *            (, ) * @param content     * @param tokenType   * @param start      * @param end      * @return  - */ protected Token createToken(String content, ITokenType tokenType, int start, int end) { return new Token(content.substring(start, end), tokenType, start); } /** *     SQL */ public enum SQLTokenType implements ITokenType { KEYWORD("\\b(?:select|from|where|group|by|order|or|and|not|exists|having|join|left|right|inner)\\b"), ID("[A-Za-z][A-Za-z0-9]*"), REAL_NUMBER("[0-9]+\\.[0-9]*"), NUMBER("[0-9]+"), STRING("'[^']*'"), SPACE("\\s+"), COMMENT("\\-\\-[^\\n\\r]*"), OPERATION("[+\\-\\*/.=\\(\\)]"); private final String regex; SQLTokenType(String regex) { this.regex = regex; } @Override public String getRegex() { return regex; } } public static void main(String[] args) { String s = "select count(id) -- count of 'Johns' \n" + "FROM person\n" + "where name = 'John'"; RegexTokenizer tokenizer = new RegexTokenizer(s, SQLTokenType.values()); while(tokenizer.hasMoreElements()) { Token token = tokenizer.nextElement(); System.out.println(token.getText() + " : " + token.getType()); } } } /** * -  () */ public class Token { //   private final String text; //   private final ITokenType type; //      private final int start; public Token(String text, ITokenType type, int start) { this.text = text; this.type = type; this.start = start; } public String getText() { return text; } public ITokenType getType() { return type; } public int getStart() { return start; } } /** *     */ public interface ITokenType { /** *       */ String getRegex(); }</span></span></code> </pre></div></div><br><p>  Di kelas ini, algoritma diimplementasikan menggunakan kelompok bernama, yang tidak ada di semua mesin.  Fitur ini memungkinkan Anda untuk mengakses grup bukan dengan indeks, tetapi dengan nama, yang sedikit lebih nyaman daripada mengakses dengan indeks. </p><br><p>  Pada I7 2.3GHz saya, kelas ini menunjukkan kecepatan analisis 5-20 MB per detik (tergantung pada kompleksitas ekspresi).  Algoritme dapat diparalelkan dengan menganalisis beberapa file sekaligus, meningkatkan kecepatan kerja secara keseluruhan. </p><br><p>  Saya menemukan beberapa algoritme yang serupa di jaringan, tetapi saya menemukan opsi yang tidak membentuk ekspresi reguler yang sama, tetapi secara konsisten menerapkan ekspresi reguler untuk setiap jenis token ke awal baris, kemudian membuang token yang ditemukan dari awal baris dan sekali lagi mencoba menerapkan semua regexps.  Ini bekerja sekitar 10-20 kali lebih lambat, membutuhkan lebih banyak memori dan algoritma lebih rumit.  Saya mencapai kecepatan kerja yang lebih besar hanya dengan menggunakan implementasi ekspresi reguler saya berdasarkan DFA ( <strong>deterministic</strong> finite state machine).  Dalam mesin regex, NKA biasanya digunakan - mesin keadaan terbatas <strong>nondeterministic</strong> ).  DFA 2-3 kali lebih cepat, tetapi ekspresi reguler untuk itu lebih sulit untuk ditulis karena terbatasnya operator. </p><br><p>  Dalam contoh saya untuk SQL, saya menyederhanakan ekspresi reguler sedikit dan tokenizer yang dihasilkan tidak dapat dianggap sebagai analisa leksikal penuh dari query SQL, tetapi tujuan artikel ini adalah untuk menunjukkan prinsip, dan tidak membuat tokenizer SQL nyata.  Saya menggunakan pendekatan ini dalam praktik saya dan membuat analisis leksikal penuh untuk SQL, Java, C, XML, HTML, JSON, Pascal, dan bahkan COBOL (saya harus mengotak-atiknya). </p><br><p>  Berikut adalah beberapa aturan sederhana untuk menulis ekspresi reguler untuk analisis leksikal. </p><br><ol><li>  Jika token yang sama dapat ditetapkan untuk jenis yang berbeda (misalnya, kata kunci apa pun dapat dikenali sebagai pengidentifikasi), maka jenis yang lebih sempit harus ditentukan di awal.  Kemudian ekspresi reguler untuk itu akan diterapkan terlebih dahulu dan itu akan menentukan jenis token.  Misalnya, dalam contoh saya, <strong>kata kunci</strong> didefinisikan sebelum <strong>id</strong> dan token <em>pilih</em> akan dikenali sebagai <strong>kata kunci</strong> , bukan <strong>id</strong> </li><li>  Tentukan token yang lebih panjang terlebih dahulu, lalu yang lebih pendek.  Misalnya, Anda pertama-tama perlu mendefinisikan <em>&lt;=</em> , <em>&gt; =</em> dan kemudian memisahkan <em>&gt;</em> , <em>&lt;</em> , <em>=</em> Dalam hal ini, teks <em>&lt;=</em> akan dikenali dengan benar sebagai token tunggal dari operator yang kurang dari atau sama, dan bukan dua token yang terpisah <em>&lt;</em> dan <em>=</em> </li><li>  Belajarlah untuk menggunakan <strong>lookahead</strong> dan <strong>lookbehind</strong> .  Sebagai contoh, karakter * dalam SQL memiliki arti operator perkalian dan indikasi semua bidang dalam tabel.  Menggunakan tampilan sederhana di belakang <strong>, Anda dapat</strong> memisahkan dua kasus ini, misalnya, di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sini</a> regexp <strong>(? &lt;=. \ S <em>| select \ s</em> ) *</strong> menemukan karakter * hanya dalam nilai "semua bidang tabel". </li><li>  Terkadang berguna untuk mendefinisikan ekspresi reguler untuk kesalahan yang terjadi dalam teks.  Misalnya, jika Anda melakukan penyorotan sintaks, Anda dapat mendefinisikan jenis token string yang belum selesai sebagai <code>'[^\n\r]*</code> .  Dalam proses pengeditan teks, pengguna mungkin tidak punya waktu untuk menutup tanda kutip dalam string, tetapi tokenizer Anda akan dapat mengenali situasi ini dengan benar dan menyorotnya dengan benar. </li></ol><br><p>  Dengan menggunakan aturan ini dan menerapkan algoritma ini, Anda dapat dengan cepat membagi teks menjadi token untuk hampir semua bahasa. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id462781/">https://habr.com/ru/post/id462781/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id462769/index.html">Penerapan pembelajaran mesin dan ilmu data dalam industri</a></li>
<li><a href="../id462771/index.html">Mereka tidak menyembunyikan apa pun</a></li>
<li><a href="../id462773/index.html">Cara bekerja dengan Google Trends: panduan lengkap untuk pemula</a></li>
<li><a href="../id462775/index.html">Homebridge Automation dengan Node-Red</a></li>
<li><a href="../id462777/index.html">Cara mengevaluasi kinerja server Linux: alat benchmarking terbuka</a></li>
<li><a href="../id462783/index.html">Matriks: 20 tahun kemudian</a></li>
<li><a href="../id462787/index.html">Bagaimana cara menjinakkan seorang junior?</a></li>
<li><a href="../id462789/index.html">Ke Jerman oleh pengembang tanpa sekolah menengah</a></li>
<li><a href="../id462793/index.html">Keset dengan gajah dan kuda. Metode siklik "Tahanan Kaukasus"</a></li>
<li><a href="../id462795/index.html">Skema deklaratif dan apa yang salah dengan Magento 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>