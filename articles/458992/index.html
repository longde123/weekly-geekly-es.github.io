<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚¨áÔ∏è üíÜüèæ üßïüèø Atenci√≥n para dummies e implementaci√≥n en Keras üïØÔ∏è üåû ‚ò¢Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sobre art√≠culos sobre inteligencia artificial en ruso 
 A pesar de que el mecanismo de Atenci√≥n se describe en la literatura inglesa, todav√≠a no he vi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Atenci√≥n para dummies e implementaci√≥n en Keras</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458992/"><h2>  Sobre art√≠culos sobre inteligencia artificial en ruso </h2><br>  A pesar de que el mecanismo de Atenci√≥n se describe en la literatura inglesa, todav√≠a no he visto una descripci√≥n decente de esta tecnolog√≠a en el sector de habla rusa.  Hay muchos art√≠culos sobre Inteligencia Artificial (IA) en nuestro idioma.  Sin embargo, los art√≠culos que se encontraron revelan solo los modelos de IA m√°s simples, por ejemplo, redes de convoluci√≥n, redes generativas.  Sin embargo, seg√∫n los √∫ltimos desarrollos de vanguardia en el campo de la IA, hay muy pocos art√≠culos en el sector de habla rusa. <br><br><a name="habracut"></a>  La falta de art√≠culos en ruso sobre los √∫ltimos desarrollos se convirti√≥ en un problema para m√≠ cuando entr√© en el tema, estudi√© el estado actual de las cosas en el campo de la IA.  S√© bien ingl√©s, le√≠ art√≠culos en ingl√©s sobre temas de IA.  Sin embargo, cuando surge un nuevo concepto o un nuevo principio de IA, su comprensi√≥n en un idioma extranjero es dolorosa y prolongada.  Saber ingl√©s, penetrar en un objeto no nativo en un objeto complejo todav√≠a vale mucho m√°s tiempo y esfuerzo.  Despu√©s de leer la descripci√≥n, te haces la pregunta: ¬øcu√°nto porcentaje entiendes?  Si hubiera un art√≠culo en ruso, lo entender√≠a al 100% despu√©s de la primera lectura.  Esto sucedi√≥ con las redes generativas, para lo cual existe una excelente serie de art√≠culos: despu√©s de leer todo se hizo claro.  Pero en el mundo de las redes, hay muchos enfoques que se describen solo en ingl√©s y que tuvieron que abordarse durante d√≠as. <br><br>  Peri√≥dicamente escribir√© art√≠culos en mi idioma nativo, aportando conocimiento a nuestro campo de idiomas.  Como sabes, la mejor manera de entender un tema es explic√°rselo a alguien.  Entonces, ¬øqui√©n m√°s sino yo deber√≠a comenzar una serie de art√≠culos sobre la IA arquitect√≥nica m√°s moderna, compleja y avanzada?  Al final del art√≠culo, comprender√© un enfoque 100%, y ser√° √∫til para alguien que lea y mejore su comprensi√≥n (por cierto, amo a Gesser, pero mejor ** Blanche de bruxelles **). <br><br>  Cuando entiendes el tema, hay 4 niveles de comprensi√≥n: <br><br><ol><li>  Usted comprende el principio y las entradas y salidas del Algoritmo / Nivel </li><li>  entiendes las salidas de reuni√≥n y, en t√©rminos generales, c√≥mo funciona </li><li>  comprende todo lo anterior, as√≠ como el dispositivo de cada nivel de red (por ejemplo, en el modelo VAE entendi√≥ el principio y tambi√©n entendi√≥ la esencia del truco de reparameterizaci√≥n) </li><li>  Entend√≠ todo, incluso cada nivel, tambi√©n entend√≠ por qu√© todo se aprende y, al mismo tiempo, puedo seleccionar hiperpar√°metros para mi tarea, en lugar de copiar y pegar soluciones listas para usar. </li></ol><br>  Para las nuevas arquitecturas, la transici√≥n del nivel 1 al nivel 4 a menudo es dif√≠cil: los autores enfatizan que est√°n describiendo m√°s de cerca varios detalles importantes superficialmente (¬ølos entendieron ellos mismos?).  O su cerebro no contiene ninguna construcci√≥n, por lo que incluso despu√©s de leer la descripci√≥n no descifr√≥ y no se convirti√≥ en habilidades.  Esto sucede si durante tus a√±os de estudiante dormiste en la misma clase de matan, despu√©s de una fiesta nocturna ÔÅä donde diste el tapete correcto.  aparato.  Y justo aqu√≠ necesitamos art√≠culos en nuestro idioma nativo que revelen los matices y sutilezas de cada operaci√≥n. <br><br><h2>  Atenci√≥n concepto y aplicaci√≥n </h2><br>  Lo anterior es un escenario de niveles de comprensi√≥n.  Para analizar la Atenci√≥n, comencemos en el nivel uno.  Antes de describir las entradas y salidas, analizaremos la esencia: en qu√© conceptos b√°sicos, comprensibles incluso para un ni√±o, se basa este concepto.  En el art√≠culo usaremos el t√©rmino en ingl√©s Atenci√≥n, porque en este formulario tambi√©n es una llamada a la funci√≥n de la biblioteca Keras (no se implementa directamente en √©l, se requiere un m√≥dulo adicional, pero m√°s sobre eso a continuaci√≥n).  Para leer m√°s, debe comprender las bibliotecas de Keras y Python, ya que se proporcionar√° el c√≥digo fuente. <br><br><img src="https://habrastorage.org/webt/lf/nv/-a/lfnv-ayy8tlpfgkiwcrgiinkme4.png" align="right">  Atenci√≥n se traduce del ingl√©s como "atenci√≥n".  Este t√©rmino describe correctamente la esencia del enfoque: si usted es automovilista y el general de polic√≠a de tr√°nsito se muestra en la foto, intuitivamente le da importancia, independientemente del contexto de la foto.  Es probable que eche un vistazo m√°s de cerca al general.  Tensa los ojos, mira las correas de los hombros con cuidado: cu√°ntas estrellas tiene all√≠ espec√≠ficamente.  Si el general no es muy alto, ign√≥ralo.  De lo contrario, consid√©relo como un factor clave en la toma de decisiones.  As√≠ es como funciona nuestro cerebro.  En la cultura rusa, hemos sido entrenados por generaciones para prestar atenci√≥n a los altos rangos, nuestro cerebro autom√°ticamente asigna alta prioridad a dichos objetos. <br><br>  La atenci√≥n es una manera de decirle a la red a qu√© debe prestar m√°s atenci√≥n, es decir, informar la probabilidad de un resultado particular dependiendo del estado de las neuronas y los datos de entrada.  La capa de Atenci√≥n implementada en Keras identifica los factores basados ‚Äã‚Äãen el conjunto de entrenamiento, cuya atenci√≥n reduce el error de red.  La identificaci√≥n de factores importantes se lleva a cabo a trav√©s del m√©todo de propagaci√≥n inversa de errores, de forma similar a c√≥mo se hace para las redes de convoluci√≥n. <br><br>  En el entrenamiento, Atenci√≥n demuestra su naturaleza probabil√≠stica.  El mecanismo en s√≠ mismo forma una matriz de escalas de importancia.  Si no hubi√©ramos entrenado la Atenci√≥n, podr√≠amos haber establecido la importancia, por ejemplo, emp√≠ricamente (lo general es m√°s importante que la insignia).  Pero cuando entrenamos una red en datos, la importancia se convierte en una funci√≥n de la probabilidad de un resultado particular, dependiendo de los datos recibidos en la entrada de la red.  Por ejemplo, si nos encontramos con un general que vive en la Rusia zarista, entonces la probabilidad de obtener guanteletes ser√≠a alta.  Habiendo comprobado esto, ser√≠a posible a trav√©s de varias reuniones personales, recolectando estad√≠sticas.  Despu√©s de eso, nuestro cerebro pondr√° el peso apropiado en el hecho de la reuni√≥n de este tema y colocar√° marcadores en las correas de los hombros y las rayas.  Cabe se√±alar que el marcador establecido no es probable: ahora la reuni√≥n del general conllevar√° consecuencias completamente diferentes para usted que entonces, adem√°s, el peso puede ser m√°s de uno.  Pero, el peso puede reducirse a probabilidad normaliz√°ndolo. <br><br>  La naturaleza probabil√≠stica del mecanismo de atenci√≥n en el aprendizaje se manifiesta en las tareas de traducci√≥n autom√°tica.  Por ejemplo, informemos a la red que al traducir del ruso al ingl√©s, la palabra Amor se traduce en el 90% de los casos como Amor, en el 9% de los casos como Sexo, en el 1% de los casos como no.  La red marca de inmediato muchas opciones, mostrando la mejor calidad de capacitaci√≥n.  Al traducir, le decimos a la red: "cuando traduzca la palabra amor, preste especial atenci√≥n a la palabra inglesa Amor, tambi√©n vea si todav√≠a puede ser Sexo". <br><br>  El enfoque de Atenci√≥n se aplica para trabajar con texto, as√≠ como con series de sonido y tiempo.  Para el procesamiento de texto, las redes neuronales recurrentes (RNN, LSTM, GRU) son ampliamente utilizadas.  La atenci√≥n puede complementarlos o reemplazarlos, moviendo la red a arquitecturas m√°s simples y r√°pidas. <br><br>  Uno de los usos m√°s famosos de Attention es usarlo para abandonar la red de recurrencia y pasar a un modelo totalmente conectado.  Las redes recurrentes tienen una serie de deficiencias: la incapacidad de proporcionar capacitaci√≥n en la GPU, el reciclaje r√°pido.  Usando el mecanismo de Atenci√≥n, podemos construir una red capaz de aprender secuencias sobre la base de una red totalmente conectada, entrenarla en la GPU, usar droput. <br><br>  La atenci√≥n se usa ampliamente para mejorar el rendimiento de las redes de recurrencia, por ejemplo, en el campo de la traducci√≥n de un idioma a otro.  Cuando se utiliza el enfoque de codificaci√≥n / decodificaci√≥n, que a menudo se usa en la IA moderna (por ejemplo, codificadores autom√°ticos variacionales).  Cuando se agrega una capa de Atenci√≥n entre el codificador y el decodificador, el resultado de la operaci√≥n de la red mejora notablemente. <br><br>  En este art√≠culo, no cito arquitecturas de red espec√≠ficas usando Atenci√≥n; esto ser√° objeto de un trabajo separado.  Una lista de todos los posibles usos de la atenci√≥n es digna de un art√≠culo separado. <br><br><h2>  Implementando la atenci√≥n en Keras fuera de la caja </h2><br>  Cuando entiendes qu√© tipo de enfoque, es muy √∫til aprender el principio b√°sico.  Pero a menudo, la comprensi√≥n completa solo se obtiene al observar una implementaci√≥n t√©cnica.  Si ve los flujos de datos que componen la funci√≥n de la operaci√≥n, queda claro qu√© se calcula exactamente.  Pero primero debe ejecutarlo y escribir "Atenci√≥n hola palabra". <br><br>  Actualmente, la atenci√≥n no est√° implementada en Keras.  Pero ya hay implementaciones de terceros, como <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">atenci√≥n-keras,</a> que se pueden instalar con github.  Entonces su c√≥digo ser√° extremadamente simple: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> attention_keras.layers.attention <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AttentionLayer attn_layer = AttentionLayer(name=<span class="hljs-string"><span class="hljs-string">'attention_layer'</span></span>) attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])</code> </pre> <br>  Esta implementaci√≥n admite la funci√≥n de visualizaci√≥n de la escala de Atenci√≥n.  Despu√©s de haber entrenado la Atenci√≥n, puede obtener una se√±alizaci√≥n matricial que, seg√∫n la red, es especialmente importante sobre este tipo (imagen de github de la p√°gina de la biblioteca de atenci√≥n-keras). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ju/hr/yj/juhryjvb8eedahp2h77q07p_pwm.png"></div><br>  B√°sicamente, no necesita nada m√°s: incluya este c√≥digo en su red como uno de los niveles y disfrute aprendiendo su red.  Cualquier red, cualquier algoritmo est√° dise√±ado en las primeras etapas a nivel conceptual (como la base de datos, por cierto), despu√©s de lo cual la implementaci√≥n se especifica en una representaci√≥n l√≥gica y f√≠sica antes de la implementaci√≥n.  Este m√©todo de dise√±o a√∫n no se ha desarrollado para redes neuronales (oh s√≠, este ser√° el tema de mi pr√≥ximo art√≠culo).  ¬øNo entiendes c√≥mo funcionan las capas de convoluci√≥n en el interior?  El principio se describe, los usas. <br><br><h2>  Implementaci√≥n de Keras de Atenci√≥n baja </h2><br>  Para finalmente entender el tema, a continuaci√≥n analizaremos en detalle la implementaci√≥n de Atenci√≥n bajo el cap√≥.  El concepto es bueno, pero ¬øc√≥mo funciona exactamente y por qu√© el resultado se obtiene exactamente como se indica? <br><br>  La implementaci√≥n m√°s simple del mecanismo de Atenci√≥n en Keras toma solo 3 l√≠neas: <br><br><pre> <code class="python hljs">inputs = Input(shape=(input_dims,)) attention_probs = Dense(input_dims, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_probs'</span></span>)(inputs) attention_mul = merge([inputs, attention_probs], output_shape=<span class="hljs-number"><span class="hljs-number">32</span></span>, name=<span class="hljs-string"><span class="hljs-string">'attention_mul'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'mul'</span></span></code> </pre><br>  En este caso, la capa de entrada se declara en la primera l√≠nea, luego viene una capa completamente conectada con la funci√≥n de activaci√≥n softmax con el n√∫mero de neuronas igual al n√∫mero de elementos en la primera capa.  La tercera capa multiplica el resultado de la capa completamente conectada por el elemento de datos de entrada por elemento. <br><br>  A continuaci√≥n se muestra toda la clase de Atenci√≥n, que implementa un mecanismo de auto atenci√≥n un poco m√°s complejo, que se puede utilizar como un nivel completo en el modelo; la clase hereda la clase de capa Keras. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Attention class Attention(Layer): def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs): self.supports_masking = True self.init = initializers.get('glorot_uniform') self.W_regularizer = regularizers.get(W_regularizer) self.b_regularizer = regularizers.get(b_regularizer) self.W_constraint = constraints.get(W_constraint) self.b_constraint = constraints.get(b_constraint) self.bias = bias self.step_dim = step_dim self.features_dim = 0 super(Attention, self).__init__(**kwargs) def build(self, input_shape): assert len(input_shape) == 3 self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint) self.features_dim = input_shape[-1] if self.bias: self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint) else: self.b = None self.built = True def compute_mask(self, input, input_mask=None): return None def call(self, x, mask=None): features_dim = self.features_dim step_dim = self.step_dim eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim)) if self.bias: eij += self.b eij = K.tanh(eij) a = K.exp(eij) if mask is not None: a *= K.cast(mask, K.floatx()) a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx()) a = K.expand_dims(a) weighted_input = x * a return K.sum(weighted_input, axis=1) def compute_output_shape(self, input_shape): return input_shape[0], self.features_dim</span></span></code> </pre><br>  Aqu√≠ vemos m√°s o menos lo mismo que se implement√≥ anteriormente a trav√©s de una capa Keras totalmente conectada, solo realizada a trav√©s de una l√≥gica m√°s profunda en un nivel inferior.  Se crea un nivel param√©trico (self.W) en la funci√≥n, que luego se multiplica escalar (K.dot) por el vector de entrada.  La l√≥gica cableada en esta variante es un poco m√°s complicada: desplazamiento (si se revela el par√°metro de sesgo), tangente hiperb√≥lica, exposici√≥n, m√°scara (si se especifica), la normalizaci√≥n se aplica al vector de entrada multiplicado por uno mismo. W, entonces el vector de entrada se vuelve a ponderar por El resultado obtenido.  No tengo una descripci√≥n de la l√≥gica establecida en este ejemplo; reproduzco las operaciones de lectura del c√≥digo.  Por cierto, escriba los comentarios si reconoce alg√∫n tipo de funci√≥n matem√°tica de alto nivel en esta l√≥gica. <br><br>  La clase tiene un par√°metro "sesgo", es decir  sesgo  Si el par√°metro est√° activado, luego de aplicar la capa Densa, el vector final se agregar√° al vector de los par√°metros de la capa "self.b", lo que har√° posible no solo determinar los "pesos" para nuestra funci√≥n de atenci√≥n, sino tambi√©n cambiar el nivel de atenci√≥n en un n√∫mero.  Ejemplo de vida: tenemos miedo a los fantasmas, pero nunca los hemos conocido.  Por lo tanto, hacemos una correcci√≥n por miedo -100 puntos.  Es decir, solo si el miedo se sale de la escala por 100 puntos, tomaremos decisiones sobre la protecci√≥n contra los fantasmas, llamando a una agencia de caza fantasmas, comprando dispositivos repelentes, etc. <br><br><h2>  Conclusi√≥n </h2><br>  El mecanismo de atenci√≥n tiene variaciones.  La opci√≥n de Atenci√≥n m√°s simple implementada en la clase anterior se llama Auto-Atenci√≥n.  La auto atenci√≥n es un mecanismo dise√±ado para procesar datos secuenciales, teniendo en cuenta el contexto de cada marca de tiempo.  Se usa con mayor frecuencia para trabajar con informaci√≥n textual.  La implementaci√≥n de la auto-atenci√≥n se puede sacar de la caja importando la biblioteca keras-self-atenci√≥n.  Hay otras variaciones de atenci√≥n.  Al estudiar materiales en ingl√©s, fue posible contar m√°s de 5 variaciones. <br><br>  Al escribir incluso este art√≠culo relativamente corto, estudi√© m√°s de 10 art√≠culos en ingl√©s.  Por supuesto, no pude descargar todos los datos de todos estos art√≠culos en 5 p√°ginas, solo hice un apret√≥n para crear una "gu√≠a para tontos".  Para comprender todos los matices del mecanismo de Atenci√≥n, necesita un libro de p√°ginas 150-200.  Realmente espero haber podido revelar la esencia b√°sica de este mecanismo para que aquellos que reci√©n comienzan a entender el aprendizaje autom√°tico entiendan c√≥mo funciona todo esto. <br><br><h2>  Fuentes </h2><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Mecanismo de atenci√≥n en redes neuronales con Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Atenci√≥n en redes profundas con Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Secuencia a secuencia basada en la atenci√≥n en Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Clasificaci√≥n de texto usando el mecanismo de atenci√≥n en Keras</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Atenci√≥n generalizada: redes neuronales convolucionales 2D para la predicci√≥n de secuencia a secuencia</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øC√≥mo implementar la capa de atenci√≥n en Keras?</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Atencion</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Atencion</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Traducci√≥n autom√°tica neuronal con atenci√≥n</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458992/">https://habr.com/ru/post/458992/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458982/index.html">Recetas Nginx: conversi√≥n de HTML y URL a PDF y PS</a></li>
<li><a href="../458984/index.html">C√≥mo crear la primera aplicaci√≥n para operar en el intercambio: 3 pasos iniciales</a></li>
<li><a href="../458986/index.html">Recetas PostgreSQL: conversi√≥n de HTML y URL a PDF y PS</a></li>
<li><a href="../458988/index.html">Texturizado, o lo que necesitas saber para convertirte en un Artista de Surface. Parte 4. Modelos, normales y barrido</a></li>
<li><a href="../458990/index.html">Deja de celoso con comentarios en el c√≥digo</a></li>
<li><a href="../458994/index.html">Raspberry Pi + CentOS = Punto de acceso Wi-Fi (o Raspberry Router en un Red Hat)</a></li>
<li><a href="../458996/index.html">User Inyerface: c√≥mo no atormentar al usuario</a></li>
<li><a href="../459000/index.html">C√≥mo intent√© mejorar Halo 2, pero casi lo arruin√©</a></li>
<li><a href="../459002/index.html">C√≥mo configurar HTTPS - SSL Configuration Generator ayudar√°</a></li>
<li><a href="../459004/index.html">Algoritmo criptogr√°fico Grasshopper: casi el complejo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>