<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñêüèæ üöÆ üçí La seguridad de los algoritmos de aprendizaje autom√°tico. Protecci√≥n y prueba de modelos con Python üßëüèª‚Äçü§ù‚Äçüßëüèª üçö üç∂</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En el art√≠culo anterior, hablamos sobre un problema de aprendizaje autom√°tico como ejemplos adversarios y algunos tipos de ataques que permiten que se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>La seguridad de los algoritmos de aprendizaje autom√°tico. Protecci√≥n y prueba de modelos con Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dsec/blog/438644/"><p><img src="https://habrastorage.org/webt/wo/o_/u2/woo_u2i8ll_fqrqvt3o-typrlue.jpeg" alt="imagen"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">En el art√≠culo anterior,</a> hablamos sobre un problema de aprendizaje autom√°tico como ejemplos adversarios y algunos tipos de ataques que permiten que se generen.  Este art√≠culo se centrar√° en algoritmos de protecci√≥n contra este tipo de efecto y recomendaciones para probar modelos. </p><a name="habracut"></a><br><h2 id="zaschita">  Protecci√≥n </h2><br><p>  En primer lugar, expliquemos de inmediato un punto: es imposible defenderse completamente de tal efecto, y esto es bastante natural.  De hecho, si resolvi√©ramos por completo el problema de los ejemplos adversarios, resolver√≠amos simult√°neamente el problema de construir un hiperplano ideal, que, por supuesto, no se puede hacer sin un conjunto de datos general. </p><br><p>  Hay dos etapas para defender un modelo de aprendizaje autom√°tico: </p><br><p>  <strong>Aprendizaje</strong> : ense√±amos a nuestro algoritmo a responder correctamente a ejemplos adversarios. </p><br><p>  <strong>Operaci√≥n</strong> : estamos tratando de detectar un ejemplo de confrontaci√≥n durante la fase de operaci√≥n del modelo. </p><br><p>  Vale la pena decir de inmediato que puede trabajar con los m√©todos de protecci√≥n presentados en este art√≠culo utilizando la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Caja de herramientas de robustez adversa de</a> IBM. </p><br><h3 id="adversarial-training">  Entrenamiento Adversarial </h3><br><p><img src="https://habrastorage.org/webt/4w/t_/lm/4wt_lmm-cbcdye9rabryki0jj70.png" alt="imagen"></p><br><p> Si le pregunta a una persona que acaba de familiarizarse con el problema Adversarial con ejemplos, la pregunta: ‚Äú¬øC√≥mo protegerse de este efecto?‚Äù, Entonces, por supuesto, 9 de cada 10 personas dir√°n: ‚ÄúAgreguemos los objetos generados al conjunto de entrenamiento‚Äù.  Este enfoque fue propuesto inmediatamente en el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Propiedades intrigantes de las redes neuronales</a> en 2013.  Fue en este art√≠culo que este problema se describi√≥ por primera vez y el ataque L-BFGS, que permite recibir ejemplos adversos. </p><br><p>  Este m√©todo es muy simple.  Generamos ejemplos Adversarial usando varios tipos de ataques y los agregamos al conjunto de entrenamiento en cada iteraci√≥n, aumentando as√≠ la "resistencia" del modelo Adversarial a los ejemplos. </p><br><p>  La desventaja de este m√©todo es bastante obvia: en cada iteraci√≥n de entrenamiento, para cada ejemplo, podemos generar una gran cantidad de ejemplos, respectivamente, y el tiempo para modelar el entrenamiento aumenta muchas veces. </p><br><p>  Puede aplicar este m√©todo utilizando la biblioteca ART-IBM de la siguiente manera. </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.adversarial_trainer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AdversarialTrainer trainer = AdversarialTrainer(model, attacks) trainer.fit(x_train, y_train)</code> </pre> <br><h3 id="gaussian-data-augmentation">  Aumento de datos gaussiano </h3><br><p><img src="https://habrastorage.org/webt/jf/9d/ko/jf9dkoia9fom1rtqkvgwe-7aulo.png" alt="imagen"></p><br><p>  El siguiente m√©todo, descrito en el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Eficaces defensas contra ataques adversos</a> , utiliza una l√≥gica similar: tambi√©n sugiere agregar objetos adicionales al conjunto de entrenamiento, pero a diferencia del Entrenamiento Adversario, estos objetos no son ejemplos adversos, sino objetos de conjunto de entrenamiento ligeramente ruidosos (Gaussian se usa como ruido ruido, de ah√≠ el nombre del m√©todo).  Y, de hecho, esto parece muy l√≥gico, porque el principal problema de los modelos es precisamente su baja inmunidad al ruido. </p><br><p>  Este m√©todo muestra resultados similares al Entrenamiento Adversario, al tiempo que dedica mucho menos tiempo a generar objetos para el entrenamiento. </p><br><p>  Puede aplicar este m√©todo utilizando la clase GaussianAugmentation en ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.gaussian_augmentation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianAugmentation GDA = GaussianAugmentation() new_x = GDA(x_train)</code> </pre> <br><h3 id="label-smoothing">  Suavizado de etiquetas </h3><br><p>  El m√©todo de suavizado de etiquetas es muy sencillo de implementar, pero tiene un gran significado probabil√≠stico.  No entraremos en detalles sobre la interpretaci√≥n probabil√≠stica de este m√©todo; puede encontrarlo en el art√≠culo original <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Repensar la arquitectura de inicio para la visi√≥n por computadora</a> .  Pero, para decirlo brevemente, Label Smoothing es un tipo adicional de regularizaci√≥n del modelo en el problema de clasificaci√≥n, lo que lo hace m√°s resistente al ruido. </p><br><p>  De hecho, este m√©todo suaviza las etiquetas de clase.  Haci√©ndolos, digamos, no 1, sino 0.9.  Por lo tanto, los modelos de entrenamiento son multados por una "confianza" mucho mayor en la etiqueta de un objeto en particular. </p><br><p>  La aplicaci√≥n de este m√©todo en Python se puede ver a continuaci√≥n. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.label_smoothing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelSmoothing LS = LabelSmoothing() new_x, new_y = LS(train_x, train_y)</code> </pre> <br><h3 id="bounded-relu">  Acotado relu </h3><br><p><img src="https://habrastorage.org/webt/dw/mw/sz/dwmwszowk1t9l6byacxcscvmvh4.png" alt="imagen"></p><br><p>  Cuando hablamos de ataques, muchos pudieron notar que algunos ataques (JSMA, OnePixel) dependen de qu√© tan fuerte es el gradiente en un punto u otro en la imagen de entrada.  El m√©todo simple y "barato" (en t√©rminos de costos computacionales y de tiempo) de Bounded ReLU est√° tratando de resolver este problema. </p><br><p>  La esencia del m√©todo es la siguiente.  Reemplacemos la funci√≥n de activaci√≥n de ReLU en una red neuronal con la misma, que est√° limitada no solo desde abajo, sino tambi√©n desde arriba, suavizando as√≠ los mapas de gradiente, y en puntos espec√≠ficos no ser√° posible obtener una salpicadura, lo que no le permitir√° enga√±ar al algoritmo cambiando un p√≠xel de la imagen. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"></script></p><br>  \ begin {ecation *} f (x) = <br>  \ begin {cases} <br>  0, x &lt;0 <br>  \\ <br>  x, 0 \ leq x \ leq t <br>  \\ <br>  t, x&gt; t <br>  \ end {casos} <br>  \ end {ecuaci√≥n *} <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"></script></p><br><p>  Este m√©todo tambi√©n se ha descrito en el art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Defensas eficientes contra ataques adversos</a> </p><br><h3 id="postroenie-ansambley-modeley">  Conjuntos de modelos de construcci√≥n </h3><br><p><img src="https://habrastorage.org/webt/cq/2i/pg/cq2ipgeru_vavdrnk-usoydtlxw.png" alt="imagen"><br>  No es dif√≠cil enga√±ar a un modelo entrenado.  Enga√±ar a dos modelos al mismo tiempo con un objeto es a√∫n m√°s dif√≠cil.  ¬øY si hay N de tales modelos?  En esto se basa el m√©todo de conjunto de modelos.  Simplemente construimos N modelos diferentes y agregamos su salida en una sola respuesta.  Si los modelos tambi√©n est√°n representados por diferentes algoritmos, entonces es extremadamente dif√≠cil enga√±ar a dicho sistema, ¬°pero es extremadamente dif√≠cil! </p><br><p>  Es bastante natural que la implementaci√≥n de conjuntos de modelos sea un enfoque puramente arquitect√≥nico, que plantea muchas preguntas (¬øQu√© modelos b√°sicos tomar? ¬øC√≥mo agregar los resultados de los modelos b√°sicos? ¬øExiste una relaci√≥n entre los modelos? Y as√≠ sucesivamente).  Por esta raz√≥n, este enfoque no est√° implementado en ART-IBM </p><br><h3 id="feature-squeezing">  Exprimir caracter√≠sticas </h3><br><p><img src="https://habrastorage.org/webt/sn/wy/wp/snwywpuqqae7pun4njrlmluseeg.png" alt="imagen"><br>  Este m√©todo, descrito en la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">compresi√≥n de caracter√≠sticas: detecci√≥n de ejemplos adversos en redes neuronales profundas</a> , funciona durante la fase operativa del modelo.  Le permite detectar ejemplos adversos. </p><br><p>  La idea detr√°s de este m√©todo es la siguiente: si entrena n modelos con los mismos datos, pero con diferentes relaciones de compresi√≥n, los resultados de su trabajo seguir√°n siendo similares.  Al mismo tiempo, el ejemplo Adversarial, que funciona en la red de origen, probablemente fallar√° en redes adicionales.  Por lo tanto, considerando la diferencia por pares entre las salidas de la red neuronal inicial y las adicionales, eligiendo el m√°ximo de ellas y compar√°ndolas con un umbral preseleccionado, podemos afirmar que el objeto de entrada es Adversarial o absolutamente v√°lido. </p><br><p>  El siguiente es un m√©todo para obtener objetos comprimidos utilizando ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.feature_squeezing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FeatureSqueezing FS = FeatureSqueezing() new_x = FS(train_x)</code> </pre> <br><p>  Terminaremos con los m√©todos de protecci√≥n.  Pero ser√≠a un error no comprender un punto importante.  Si un atacante no tiene acceso a la entrada y salida del modelo, no comprender√° c√≥mo se procesan los datos sin procesar dentro de su sistema antes de ingresar el modelo.  Entonces, y solo entonces, todos sus ataques se reducir√°n a la clasificaci√≥n aleatoria de los valores de entrada, lo que, naturalmente, es poco probable que conduzca al resultado deseado. </p><br><h2 id="testirovanie">  Prueba </h2><br><p>  Ahora hablemos de probar algoritmos para contrarrestar los ejemplos adversos.  Aqu√≠, antes que nada, es necesario entender c√≥mo vamos a probar nuestro modelo.  Si suponemos que de alguna manera un atacante puede obtener acceso completo a todo el modelo, entonces es necesario probar nuestro modelo utilizando los m√©todos de ataque de WhiteBox. <br><img src="https://habrastorage.org/webt/vj/pm/-w/vjpm-wuwle8c5sngov5ksw5iahq.png" alt="imagen"></p><br><p>  En otro caso, suponemos que un atacante nunca tendr√° acceso a los "interiores" de nuestro modelo, pero podr√°, aunque sea indirectamente, influir en los datos de entrada y ver el resultado del modelo.  Entonces deber√≠as aplicar los m√©todos de los ataques BlackBox. <br><img src="https://habrastorage.org/webt/xc/h9/wo/xch9wo0pweqhlf33pzgrgdiihqm.png" alt="imagen"></p><br><p>  El algoritmo de prueba general se puede describir con el siguiente ejemplo: </p><br><p><img src="https://habrastorage.org/webt/1d/p_/lx/1dp_lxdocm0zkd2ssmbg_fmnvba.jpeg" alt="imagen"></p><br><p>  Que haya una red neuronal entrenada escrita en TensorFlow (TF NN).  Afirmamos que nuestra red puede caer en manos de un atacante al penetrar en el sistema donde se encuentra el modelo.  En este caso, necesitamos llevar a cabo ataques WhiteBox.  Para hacer esto, definimos un grupo de ataque y marcos (FoolBox - FB, CleverHans - CH, Caja de herramientas de robustez adversaria - ART) que permiten implementar estos ataques.  Luego, contando cu√°ntos ataques tuvieron √©xito, calculamos la tasa de √©xito (SR).  Si SR nos conviene, terminamos las pruebas; de lo contrario, utilizamos uno de los m√©todos de protecci√≥n, por ejemplo, implementado en ART-IBM.  Por otra parte, llevamos a cabo ataques y consideramos SR.  Hacemos esta operaci√≥n c√≠clicamente, hasta que SR nos convenga. </p><br><h2 id="vyvody">  Conclusiones </h2><br><p>  Me gustar√≠a terminar aqu√≠ con informaci√≥n general sobre ataques, defensas y pruebas de modelos de aprendizaje autom√°tico.  Resumiendo los dos art√≠culos, podemos concluir lo siguiente: </p><br><ol><li>  No creas en el aprendizaje autom√°tico como una especie de milagro que puede resolver todos tus problemas. </li><li>  Cuando aplique algoritmos de aprendizaje autom√°tico en sus tareas, piense en cu√°n resistente es este algoritmo a una amenaza como los ejemplos adversos. </li><li>  Puede proteger el algoritmo tanto del lado del aprendizaje autom√°tico como del lado del sistema en el que se opera este modelo. </li><li>  Pruebe sus modelos, especialmente en casos donde el resultado del modelo afecta directamente la decisi√≥n </li><li>  Las bibliotecas como FoolBox, CleverHans, ART-IBM proporcionan una interfaz conveniente para atacar y defender modelos de aprendizaje autom√°tico. </li></ol><br><p>  Tambi√©n en este art√≠culo me gustar√≠a resumir el trabajo con las bibliotecas FoolBox, CleverHans y ART-IBM: </p><br><p>  FoolBox es una biblioteca simple y comprensible para atacar redes neuronales, que admite muchos marcos diferentes. </p><br><p>  CleverHans es una biblioteca que le permite realizar ataques cambiando muchos par√°metros del ataque, un poco m√°s complicado que FoolBox, admite menos marcos. </p><br><p>  ART-IBM es la √∫nica biblioteca de lo anterior que le permite trabajar con m√©todos de seguridad, hasta ahora solo es compatible con TensorFlow y Keras, pero se est√° desarrollando m√°s r√°pido que otros. </p><br><p>  Aqu√≠ vale la pena decir que hay otra <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">biblioteca</a> para trabajar con ejemplos adversarios de Baidu, pero, desafortunadamente, es adecuada solo para personas que hablan chino. </p><br><p>  En el pr√≥ximo art√≠culo sobre este tema, analizaremos parte de la tarea que se propuso resolver durante ZeroNights HackQuest 2018 enga√±ando a una red neuronal t√≠pica utilizando la biblioteca FoolBox. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/438644/">https://habr.com/ru/post/438644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../438634/index.html">En las oficinas, hace demasiado calor o demasiado fr√≠o: ¬øhay una mejor manera de ajustar la temperatura?</a></li>
<li><a href="../438636/index.html">Incorporaci√≥n defectuosa de funciones en Go</a></li>
<li><a href="../438638/index.html">Analizamos el protocolo de mensajes de buscapersonas POCSAG, parte 2</a></li>
<li><a href="../438640/index.html">Moneda electr√≥nica abierta de alta velocidad</a></li>
<li><a href="../438642/index.html">Los fundamentos de la programaci√≥n reactiva con RxJS</a></li>
<li><a href="../438646/index.html">Acerca de la creaci√≥n de im√°genes est√©reo econ√≥micas en los dedos (estereograma, anaglifo, estereoscopio)</a></li>
<li><a href="../438648/index.html">Comparaci√≥n de sistemas de BI (Tableau, Power BI, Oracle, Qlik)</a></li>
<li><a href="../438650/index.html">Cohete 9M729. Algunas palabras sobre el "infractor" del Tratado INF</a></li>
<li><a href="../438652/index.html">Portabelizaci√≥n IDA</a></li>
<li><a href="../438654/index.html">OpenSceneGraph: Integraci√≥n con Qt Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>