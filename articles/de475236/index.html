<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍧 🧑🏽 👨🏽‍🚒 Ignoriere niemals wieder das Verstärkungstraining. 🕴🏾 👩‍👦‍👦 ◼️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Ich präsentiere Ihnen die Übersetzung des Artikels „Ignorieren Sie nie wieder das Reinforcement Learning“ von Dr. Michel Kana. 

 Mit einem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ignoriere niemals wieder das Verstärkungstraining.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Hallo habr  Ich präsentiere Ihnen die Übersetzung des Artikels „Ignorieren Sie nie wieder das Reinforcement Learning“ von Dr. Michel Kana. <br><br>  Mit einem Lehrer lernen und ohne Lehrer lernen ist nicht alles.  Das weiß jeder.  Beginnen Sie mit OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="Bild"></div><br>  <i>Wirst du den Schachweltmeister, Backgammon oder Go besiegen?</i> <br><br>  Es gibt eine Möglichkeit, wie Sie dies tun können - Verstärkungstraining. <br><a name="habracut"></a><br><h2>  Was ist Bestärkungslernen? </h2><br>  Verstärktes Lernen ist das Lernen, in einer Umgebung konsistente Entscheidungen zu treffen und dabei die maximale Belohnung zu erhalten, die für jede Aktion gewährt wird. <br><br>  In ihm ist kein Lehrer, nur ein Zeichen der Belohnung durch die Umwelt.  Zeit ist wichtig, und Aktionen wirken sich auf nachfolgende Daten aus.  Solche Bedingungen führen zu Schwierigkeiten beim Lernen mit oder ohne Lehrer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="Bild"></div><br>  Im folgenden Beispiel versucht die Maus, so viel Nahrung wie möglich zu finden, und vermeidet nach Möglichkeit Stromschläge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="Bild"></div><br>  Eine Maus kann mutig sein und eine Entladung bekommen, um an einen Ort mit viel Käse zu gelangen.  Es wird besser sein, als nur still zu stehen und nichts zu empfangen. <br><br>  Die Maus möchte nicht in jeder Situation die besten Entscheidungen treffen.  Dies würde einen hohen geistigen Aufwand von ihr erfordern, und es wäre nicht universell. <br><br>  Das Training mit Verstärkungen bietet einige magische Methoden, mit denen unsere Maus lernen kann, wie man Stromschläge vermeidet und so viel Nahrung wie möglich erhält. <br><br>  Die Maus ist ein Agent.  Ein Labyrinth mit Wänden, Käse und Elektroschocker ist <b>die Umwelt</b> .  Die Maus kann sich nach links, rechts, oben und unten bewegen - das sind <b>Aktionen</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="Bild"></div><br>  Die Maus will Käse, keinen Stromschlag.  Käse ist eine <b>Belohnung</b> .  Die Maus kann die Umgebung inspizieren - das sind <b>Beobachtungen</b> . <br><br><h2>  Eisverstärkungstraining </h2><br>  Lassen wir die Maus im Labyrinth und gehen weiter zum Eis.  „Der Winter ist gekommen.  Sie und Ihre Freunde haben Frisbee in den Park geworfen, als Sie plötzlich Frisbee in die Mitte des Sees geworfen haben.  Grundsätzlich war das Wasser im See gefroren, aber es gab ein paar Löcher, in denen das Eis geschmolzen war.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="Bild"></div><br>  „Wenn du auf eines der Löcher trittst, fällst du ins Eiswasser.  Außerdem gibt es auf der Welt einen riesigen Mangel an Frisbee, daher ist es unbedingt erforderlich, dass Sie den See umrunden und eine Auffahrt finden. “( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> ) <br><br><h2>  Wie fühlst du dich in einer ähnlichen Situation? </h2><br>  Dies ist eine Herausforderung für das verstärkte Lernen.  Der Agent kontrolliert die Bewegungen des Charakters in der Gitterwelt.  Einige Gitterplättchen sind passierbar, während andere dazu führen, dass der Charakter ins Wasser fällt.  Der Agent erhält eine Belohnung dafür, dass er einen passablen Weg zum Ziel gefunden hat. <br><br>  Wir können eine solche Umgebung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI Gym</a> simulieren - einem Toolkit zum Entwickeln und Vergleichen von Lernalgorithmen mit Verstärkungen.  Es bietet Zugriff auf eine standardisierte Reihe von Umgebungen, wie in unserem Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Frozen Lake</a> .  Dies ist ein Textmedium, das mit ein paar Codezeilen erstellt werden kann. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Jetzt brauchen wir eine Struktur, die es uns ermöglicht, die Probleme des Lernens systematisch mit Verstärkung anzugehen. <br><br><h2>  Markov Entscheidungsprozess </h2><br>  In unserem Beispiel steuert der Agent die Bewegung des Charakters in der Gitterwelt. Diese Umgebung wird als vollständig beobachtbare Umgebung bezeichnet. <br><br>  Da die zukünftige Kachel nicht von früheren Kacheln abhängig ist, wird die aktuelle Kachel berücksichtigt <br>  (Wir haben es mit einer Folge von Zufallszuständen zu tun, das heißt mit der <b>Markov-Eigenschaft.</b> ) Deshalb haben wir es mit dem sogenannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markov-Prozess zu</a> tun. <br><br>  Der aktuelle Status fasst alles zusammen, was für die Entscheidung über den nächsten Schritt erforderlich ist, und nichts, woran man sich erinnern muss. <br><br>  In jeder nächsten Zelle (d. H. Einer Situation) wählt der Agent mit einer gewissen Wahrscheinlichkeit die Aktion aus, die zur nächsten Zelle führt, d. H. Die Situation, und die Umgebung antwortet dem Agenten mit Beobachtung und Belohnung. <br><br>  Wir addieren die Belohnungsfunktion und den Rabattkoeffizienten zum Markov-Prozess und erhalten den sogenannten <i>Markov-Belohnungsprozess</i> .  Durch Hinzufügen einer Reihe von Aktionen erhalten wir <i>den Markov-Entscheidungsprozess</i> ( <b>MDP</b> ).  Die Komponenten von MDP werden nachstehend ausführlicher beschrieben. <br><br><h2>  Zustand </h2><br>  Ein Zustand ist ein Teil der Umgebung, eine numerische Darstellung dessen, was der Agent zu einem bestimmten Zeitpunkt in der Umgebung beobachtet, des Zustands des Seenetzes.  S ist der Startpunkt, G ist das Ziel, F ist das feste Eis, auf dem der Agent stehen kann, und H ist das Loch, in das der Agent fallen wird, wenn er darauf tritt.  Wir haben 16 Zustände in einer 4 x 4-Grid-Umgebung oder 64 Zustände in einer 8 x 8-Umgebung. Im Folgenden werden wir ein Beispiel für eine 4 x 4-Umgebung mit OpenAI Gym zeichnen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Aktionen </h2><br>  Der Agent verfügt über 4 mögliche Aktionen, die in der Umgebung als 0, 1, 2, 3 für links, rechts, unten bzw. oben dargestellt werden. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="Bild"></div><br><h2>  Zustandsübergangsmodell </h2><br>  Das Statusübergangsmodell beschreibt, wie sich der Status der Umgebung ändert, wenn ein Agent basierend auf seinem aktuellen Status Maßnahmen ergreift. <br><br>  Das Modell wird normalerweise durch die Übergangswahrscheinlichkeit beschrieben, die als quadratische Übergangsmatrix der Größe N x N ausgedrückt wird, wobei N die Anzahl der Zustände unseres Modells ist.  Die folgende Abbildung zeigt ein Beispiel für eine solche Matrix für Wetterbedingungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="Bild"></div><br>  In der Gefrorenen See-Umgebung gehen wir davon aus, dass der See nicht rutschig ist.  Wenn wir richtig gehen, dann gehen wir definitiv richtig.  Daher sind alle Wahrscheinlichkeiten gleich. <br><br>  "Links" verschiebt die Zelle von Agent 1 nach links oder belässt sie an derselben Position, wenn sich der Agent am linken Rand befindet. <br><br>  "Rechts" verschiebt es 1 Zelle nach rechts oder belässt es an derselben Position, wenn sich der Agent am rechten Rand befindet. <br><br>  "Auf" verschiebt die Zelle von Agent 1 nach oben, oder der Agent bleibt an derselben Stelle, wenn er sich an der oberen Grenze befindet. <br><br>  "Nach unten" verschiebt die Zelle von Agent 1 nach unten oder bleibt an derselben Stelle, wenn sie sich an der unteren Grenze befindet. <br><br><h2>  Vergütung </h2><br>  In jedem Zustand F erhält der Agent 0 Belohnungen, in Zustand H erhält er -1, da der Agent in diesem Zustand stirbt.  Und wenn der Agent das Ziel erreicht, erhält er eine Belohnung von +1. <br><br>  Aufgrund der Tatsache, dass beide Modelle, das Übergangsmodell und das Belohnungsmodell, deterministische Funktionen sind, macht dies die Umgebung deterministisch.  \ <br><br><h2>  Rabatt </h2><br>  Der Rabatt ist ein optionaler Parameter, der die Wichtigkeit zukünftiger Belohnungen steuert.  Sie wird im Bereich von 0 bis 1 gemessen. Mit diesem Parameter soll verhindert werden, dass die Gesamtbelohnung unendlich wird. <br><br>  Der Rabatt modelliert auch das Verhalten des Agenten, wenn der Agent die sofortige Belohnung der Belohnung vorzieht, die möglicherweise in der Zukunft erhalten wird. <br><br><h2>  Wert </h2><br>  Der Wert des Vermögens ist das erwartete langfristige Einkommen mit einem Abschlag für das Vermögen. <br><br><h2>  Richtlinie (π) </h2><br>  Die Strategie, mit der der Agent die nächste Aktion auswählt, wird als Richtlinie bezeichnet.  Unter allen verfügbaren Richtlinien ist die optimale die, die die Höhe der während der Episode erhaltenen oder erwarteten Vergütung maximiert. <br><br><h2>  Folge </h2><br>  Die Episode beginnt, wenn der Agent in der Startzelle erscheint, und endet, wenn der Agent entweder in das Loch fällt oder die Zielzelle erreicht. <br><br><h2>  Lassen Sie uns alles visualisieren </h2><br>  Nachdem wir alle am Markov-Entscheidungsprozess beteiligten Konzepte überprüft haben, können wir nun mit OpenAI Gym mehrere zufällige Aktionen in einer 16x16-Umgebung modellieren.  Der Agent wählt jedes Mal eine zufällige Aktion aus und führt sie aus.  Das System berechnet die Belohnung und zeigt den neuen Zustand der Umgebung an. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="Bild"></div><br><h2>  Fazit </h2><br>  In diesem Artikel haben wir kurz die grundlegenden Konzepte des verstärkenden Lernens besprochen.  Unser Beispiel enthält eine Einführung in das OpenAI Gym-Toolkit, mit dem Sie problemlos mit vorgefertigten Umgebungen experimentieren können. <br><br>  Im nächsten Teil stellen wir vor, wie Richtlinien entworfen und implementiert werden, die es dem Agenten ermöglichen, eine Reihe von Aktionen durchzuführen, um das Ziel zu erreichen und eine Auszeichnung zu erhalten, z. B. einen Weltmeister zu besiegen. <br><br>  Vielen Dank für Ihre Aufmerksamkeit. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475236/">https://habr.com/ru/post/de475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475212/index.html">Suchen Sie mit dem kostenlosen Dienstprogramm dbForge Search nach Daten und Objekten in der MS SQL Server-Datenbank</a></li>
<li><a href="../de475214/index.html">Wenn ein Unternehmen stirbt: Wie überlebe ich den Bankrott?</a></li>
<li><a href="../de475218/index.html">Kryptografische Protokolle: Definitionen, Datensätze, Eigenschaften, Klassifizierung, Angriffe</a></li>
<li><a href="../de475226/index.html">Praktikum bei der Haxe Foundation</a></li>
<li><a href="../de475228/index.html">Payroll Gabel. Du bist ein Programmierer für Mama</a></li>
<li><a href="../de475238/index.html">Blade Runner Timeline - November 2019. Hat sich die Prognose erfüllt?</a></li>
<li><a href="../de475240/index.html">Verwendung strenger Module in großen Python-Projekten: Instagram-Erfahrung. Teil 1</a></li>
<li><a href="../de475242/index.html">Verwendung strenger Module in großen Python-Projekten: Instagram-Erfahrung. Teil 2</a></li>
<li><a href="../de475244/index.html">Erwartete neue JavaScript-Funktionen, die Sie kennen sollten</a></li>
<li><a href="../de475246/index.html">Asynchrone Python-Programmierung: Ein kurzer Überblick</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>