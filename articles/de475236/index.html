<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçß üßëüèΩ üë®üèΩ‚Äçüöí Ignoriere niemals wieder das Verst√§rkungstraining. üï¥üèæ üë©‚Äçüë¶‚Äçüë¶ ‚óºÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo habr Ich pr√§sentiere Ihnen die √úbersetzung des Artikels ‚ÄûIgnorieren Sie nie wieder das Reinforcement Learning‚Äú von Dr. Michel Kana. 

 Mit einem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ignoriere niemals wieder das Verst√§rkungstraining.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/475236/">  Hallo habr  Ich pr√§sentiere Ihnen die √úbersetzung des Artikels ‚ÄûIgnorieren Sie nie wieder das Reinforcement Learning‚Äú von Dr. Michel Kana. <br><br>  Mit einem Lehrer lernen und ohne Lehrer lernen ist nicht alles.  Das wei√ü jeder.  Beginnen Sie mit OpenAI Gym. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0cf/f0a/00c/0cff0a00cef85b7f0555b5e22b22fefe.png" alt="Bild"></div><br>  <i>Wirst du den Schachweltmeister, Backgammon oder Go besiegen?</i> <br><br>  Es gibt eine M√∂glichkeit, wie Sie dies tun k√∂nnen - Verst√§rkungstraining. <br><a name="habracut"></a><br><h2>  Was ist Best√§rkungslernen? </h2><br>  Verst√§rktes Lernen ist das Lernen, in einer Umgebung konsistente Entscheidungen zu treffen und dabei die maximale Belohnung zu erhalten, die f√ºr jede Aktion gew√§hrt wird. <br><br>  In ihm ist kein Lehrer, nur ein Zeichen der Belohnung durch die Umwelt.  Zeit ist wichtig, und Aktionen wirken sich auf nachfolgende Daten aus.  Solche Bedingungen f√ºhren zu Schwierigkeiten beim Lernen mit oder ohne Lehrer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af2/58a/992/af258a9921e6a4e1721d08806b2ce44d.png" alt="Bild"></div><br>  Im folgenden Beispiel versucht die Maus, so viel Nahrung wie m√∂glich zu finden, und vermeidet nach M√∂glichkeit Stromschl√§ge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/763/5aa/e3b7635aa2e3f97e799c999b28777327.png" alt="Bild"></div><br>  Eine Maus kann mutig sein und eine Entladung bekommen, um an einen Ort mit viel K√§se zu gelangen.  Es wird besser sein, als nur still zu stehen und nichts zu empfangen. <br><br>  Die Maus m√∂chte nicht in jeder Situation die besten Entscheidungen treffen.  Dies w√ºrde einen hohen geistigen Aufwand von ihr erfordern, und es w√§re nicht universell. <br><br>  Das Training mit Verst√§rkungen bietet einige magische Methoden, mit denen unsere Maus lernen kann, wie man Stromschl√§ge vermeidet und so viel Nahrung wie m√∂glich erh√§lt. <br><br>  Die Maus ist ein Agent.  Ein Labyrinth mit W√§nden, K√§se und Elektroschocker ist <b>die Umwelt</b> .  Die Maus kann sich nach links, rechts, oben und unten bewegen - das sind <b>Aktionen</b> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/19d/c21/c31/19dc21c318011ce705e222053fda2f5e.png" alt="Bild"></div><br>  Die Maus will K√§se, keinen Stromschlag.  K√§se ist eine <b>Belohnung</b> .  Die Maus kann die Umgebung inspizieren - das sind <b>Beobachtungen</b> . <br><br><h2>  Eisverst√§rkungstraining </h2><br>  Lassen wir die Maus im Labyrinth und gehen weiter zum Eis.  ‚ÄûDer Winter ist gekommen.  Sie und Ihre Freunde haben Frisbee in den Park geworfen, als Sie pl√∂tzlich Frisbee in die Mitte des Sees geworfen haben.  Grunds√§tzlich war das Wasser im See gefroren, aber es gab ein paar L√∂cher, in denen das Eis geschmolzen war.  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> ) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/367/406/d24/367406d24be00f3c02fb7df9a7f9e773.png" alt="Bild"></div><br>  ‚ÄûWenn du auf eines der L√∂cher trittst, f√§llst du ins Eiswasser.  Au√üerdem gibt es auf der Welt einen riesigen Mangel an Frisbee, daher ist es unbedingt erforderlich, dass Sie den See umrunden und eine Auffahrt finden. ‚Äú( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a> ) <br><br><h2>  Wie f√ºhlst du dich in einer √§hnlichen Situation? </h2><br>  Dies ist eine Herausforderung f√ºr das verst√§rkte Lernen.  Der Agent kontrolliert die Bewegungen des Charakters in der Gitterwelt.  Einige Gitterpl√§ttchen sind passierbar, w√§hrend andere dazu f√ºhren, dass der Charakter ins Wasser f√§llt.  Der Agent erh√§lt eine Belohnung daf√ºr, dass er einen passablen Weg zum Ziel gefunden hat. <br><br>  Wir k√∂nnen eine solche Umgebung mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenAI Gym</a> simulieren - einem Toolkit zum Entwickeln und Vergleichen von Lernalgorithmen mit Verst√§rkungen.  Es bietet Zugriff auf eine standardisierte Reihe von Umgebungen, wie in unserem Beispiel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Frozen Lake</a> .  Dies ist ein Textmedium, das mit ein paar Codezeilen erstellt werden kann. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gym.envs.registration <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> register <span class="hljs-comment"><span class="hljs-comment"># load 4x4 environment if 'FrozenLakeNotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLakeNotSlippery-v0'] register(id='FrozenLakeNotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '4x4', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 ) # load 16x16 environment if 'FrozenLake8x8NotSlippery-v0' in gym.envs.registry.env_specs: del gym.envs.registry.env_specs['FrozenLake8x8NotSlippery-v0'] register( id='FrozenLake8x8NotSlippery-v0', entry_point='gym.envs.toy_text:FrozenLakeEnv', kwargs={'map_name' : '8x8', 'is_slippery': False}, max_episode_steps=100, reward_threshold=0.8196 )</span></span></code> </pre> <br>  Jetzt brauchen wir eine Struktur, die es uns erm√∂glicht, die Probleme des Lernens systematisch mit Verst√§rkung anzugehen. <br><br><h2>  Markov Entscheidungsprozess </h2><br>  In unserem Beispiel steuert der Agent die Bewegung des Charakters in der Gitterwelt. Diese Umgebung wird als vollst√§ndig beobachtbare Umgebung bezeichnet. <br><br>  Da die zuk√ºnftige Kachel nicht von fr√ºheren Kacheln abh√§ngig ist, wird die aktuelle Kachel ber√ºcksichtigt <br>  (Wir haben es mit einer Folge von Zufallszust√§nden zu tun, das hei√üt mit der <b>Markov-Eigenschaft.</b> ) Deshalb haben wir es mit dem sogenannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markov-Prozess zu</a> tun. <br><br>  Der aktuelle Status fasst alles zusammen, was f√ºr die Entscheidung √ºber den n√§chsten Schritt erforderlich ist, und nichts, woran man sich erinnern muss. <br><br>  In jeder n√§chsten Zelle (d. H. Einer Situation) w√§hlt der Agent mit einer gewissen Wahrscheinlichkeit die Aktion aus, die zur n√§chsten Zelle f√ºhrt, d. H. Die Situation, und die Umgebung antwortet dem Agenten mit Beobachtung und Belohnung. <br><br>  Wir addieren die Belohnungsfunktion und den Rabattkoeffizienten zum Markov-Prozess und erhalten den sogenannten <i>Markov-Belohnungsprozess</i> .  Durch Hinzuf√ºgen einer Reihe von Aktionen erhalten wir <i>den Markov-Entscheidungsprozess</i> ( <b>MDP</b> ).  Die Komponenten von MDP werden nachstehend ausf√ºhrlicher beschrieben. <br><br><h2>  Zustand </h2><br>  Ein Zustand ist ein Teil der Umgebung, eine numerische Darstellung dessen, was der Agent zu einem bestimmten Zeitpunkt in der Umgebung beobachtet, des Zustands des Seenetzes.  S ist der Startpunkt, G ist das Ziel, F ist das feste Eis, auf dem der Agent stehen kann, und H ist das Loch, in das der Agent fallen wird, wenn er darauf tritt.  Wir haben 16 Zust√§nde in einer 4 x 4-Grid-Umgebung oder 64 Zust√§nde in einer 8 x 8-Umgebung. Im Folgenden werden wir ein Beispiel f√ºr eine 4 x 4-Umgebung mit OpenAI Gym zeichnen. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_states_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.observation_space) print() env.env.s=random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,env.observation_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>) env.render() view_states_frozen_lake()</code> </pre><br><h2>  Aktionen </h2><br>  Der Agent verf√ºgt √ºber 4 m√∂gliche Aktionen, die in der Umgebung als 0, 1, 2, 3 f√ºr links, rechts, unten bzw. oben dargestellt werden. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">view_actions_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> print(env.action_space) print(<span class="hljs-string"><span class="hljs-string">"Possible actions: [0..%a]"</span></span> % (env.action_space.n<span class="hljs-number"><span class="hljs-number">-1</span></span>)) view_actions_frozen_lake()</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ed/9cc/b7e/5ed9ccb7ed1ff6a1285f22657118356d.png" alt="Bild"></div><br><h2>  Zustands√ºbergangsmodell </h2><br>  Das Status√ºbergangsmodell beschreibt, wie sich der Status der Umgebung √§ndert, wenn ein Agent basierend auf seinem aktuellen Status Ma√ünahmen ergreift. <br><br>  Das Modell wird normalerweise durch die √úbergangswahrscheinlichkeit beschrieben, die als quadratische √úbergangsmatrix der Gr√∂√üe N x N ausgedr√ºckt wird, wobei N die Anzahl der Zust√§nde unseres Modells ist.  Die folgende Abbildung zeigt ein Beispiel f√ºr eine solche Matrix f√ºr Wetterbedingungen. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ed/6f5/f38/0ed6f5f3884231ab1e5b08bc65b8effb.png" alt="Bild"></div><br>  In der Gefrorenen See-Umgebung gehen wir davon aus, dass der See nicht rutschig ist.  Wenn wir richtig gehen, dann gehen wir definitiv richtig.  Daher sind alle Wahrscheinlichkeiten gleich. <br><br>  "Links" verschiebt die Zelle von Agent 1 nach links oder bel√§sst sie an derselben Position, wenn sich der Agent am linken Rand befindet. <br><br>  "Rechts" verschiebt es 1 Zelle nach rechts oder bel√§sst es an derselben Position, wenn sich der Agent am rechten Rand befindet. <br><br>  "Auf" verschiebt die Zelle von Agent 1 nach oben, oder der Agent bleibt an derselben Stelle, wenn er sich an der oberen Grenze befindet. <br><br>  "Nach unten" verschiebt die Zelle von Agent 1 nach unten oder bleibt an derselben Stelle, wenn sie sich an der unteren Grenze befindet. <br><br><h2>  Verg√ºtung </h2><br>  In jedem Zustand F erh√§lt der Agent 0 Belohnungen, in Zustand H erh√§lt er -1, da der Agent in diesem Zustand stirbt.  Und wenn der Agent das Ziel erreicht, erh√§lt er eine Belohnung von +1. <br><br>  Aufgrund der Tatsache, dass beide Modelle, das √úbergangsmodell und das Belohnungsmodell, deterministische Funktionen sind, macht dies die Umgebung deterministisch.  \ <br><br><h2>  Rabatt </h2><br>  Der Rabatt ist ein optionaler Parameter, der die Wichtigkeit zuk√ºnftiger Belohnungen steuert.  Sie wird im Bereich von 0 bis 1 gemessen. Mit diesem Parameter soll verhindert werden, dass die Gesamtbelohnung unendlich wird. <br><br>  Der Rabatt modelliert auch das Verhalten des Agenten, wenn der Agent die sofortige Belohnung der Belohnung vorzieht, die m√∂glicherweise in der Zukunft erhalten wird. <br><br><h2>  Wert </h2><br>  Der Wert des Verm√∂gens ist das erwartete langfristige Einkommen mit einem Abschlag f√ºr das Verm√∂gen. <br><br><h2>  Richtlinie (œÄ) </h2><br>  Die Strategie, mit der der Agent die n√§chste Aktion ausw√§hlt, wird als Richtlinie bezeichnet.  Unter allen verf√ºgbaren Richtlinien ist die optimale die, die die H√∂he der w√§hrend der Episode erhaltenen oder erwarteten Verg√ºtung maximiert. <br><br><h2>  Folge </h2><br>  Die Episode beginnt, wenn der Agent in der Startzelle erscheint, und endet, wenn der Agent entweder in das Loch f√§llt oder die Zielzelle erreicht. <br><br><h2>  Lassen Sie uns alles visualisieren </h2><br>  Nachdem wir alle am Markov-Entscheidungsprozess beteiligten Konzepte √ºberpr√ºft haben, k√∂nnen wir nun mit OpenAI Gym mehrere zuf√§llige Aktionen in einer 16x16-Umgebung modellieren.  Der Agent w√§hlt jedes Mal eine zuf√§llige Aktion aus und f√ºhrt sie aus.  Das System berechnet die Belohnung und zeigt den neuen Zustand der Umgebung an. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">simulate_frozen_lake</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(env = gym.make</span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-string">'FrozenLakeNotSlippery-v0'</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">)</span></span></span></span><span class="hljs-function"><span class="hljs-params">, nb_trials=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">10</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> rew_tot=<span class="hljs-number"><span class="hljs-number">0</span></span> obs= env.reset() env.render() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(nb_trials+<span class="hljs-number"><span class="hljs-number">1</span></span>): action = env.action_space.sample() <span class="hljs-comment"><span class="hljs-comment"># select a random action obs, rew, done, info = env.step(action) # perform the action rew_tot = rew_tot + rew # calculate the total reward env.render() # display the environment print("Reward: %r" % rew_tot) # print the total reward simulate_frozen_lake(env = gym.make('FrozenLake8x8NotSlippery-v0'))</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ff/108/b51/6ff108b516731cf1b5536455cc84830f.jpg" alt="Bild"></div><br><h2>  Fazit </h2><br>  In diesem Artikel haben wir kurz die grundlegenden Konzepte des verst√§rkenden Lernens besprochen.  Unser Beispiel enth√§lt eine Einf√ºhrung in das OpenAI Gym-Toolkit, mit dem Sie problemlos mit vorgefertigten Umgebungen experimentieren k√∂nnen. <br><br>  Im n√§chsten Teil stellen wir vor, wie Richtlinien entworfen und implementiert werden, die es dem Agenten erm√∂glichen, eine Reihe von Aktionen durchzuf√ºhren, um das Ziel zu erreichen und eine Auszeichnung zu erhalten, z. B. einen Weltmeister zu besiegen. <br><br>  Vielen Dank f√ºr Ihre Aufmerksamkeit. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de475236/">https://habr.com/ru/post/de475236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de475212/index.html">Suchen Sie mit dem kostenlosen Dienstprogramm dbForge Search nach Daten und Objekten in der MS SQL Server-Datenbank</a></li>
<li><a href="../de475214/index.html">Wenn ein Unternehmen stirbt: Wie √ºberlebe ich den Bankrott?</a></li>
<li><a href="../de475218/index.html">Kryptografische Protokolle: Definitionen, Datens√§tze, Eigenschaften, Klassifizierung, Angriffe</a></li>
<li><a href="../de475226/index.html">Praktikum bei der Haxe Foundation</a></li>
<li><a href="../de475228/index.html">Payroll Gabel. Du bist ein Programmierer f√ºr Mama</a></li>
<li><a href="../de475238/index.html">Blade Runner Timeline - November 2019. Hat sich die Prognose erf√ºllt?</a></li>
<li><a href="../de475240/index.html">Verwendung strenger Module in gro√üen Python-Projekten: Instagram-Erfahrung. Teil 1</a></li>
<li><a href="../de475242/index.html">Verwendung strenger Module in gro√üen Python-Projekten: Instagram-Erfahrung. Teil 2</a></li>
<li><a href="../de475244/index.html">Erwartete neue JavaScript-Funktionen, die Sie kennen sollten</a></li>
<li><a href="../de475246/index.html">Asynchrone Python-Programmierung: Ein kurzer √úberblick</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>