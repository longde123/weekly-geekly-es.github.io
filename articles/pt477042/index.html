<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üÜë üë©‚Äçüöí üîå Otimiza√ß√£o da estrat√©gia de blackjack de Monte Carlo üèáüèΩ üì≠ üßëüèº‚Äçü§ù‚Äçüßëüèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A tradu√ß√£o do artigo foi preparada especificamente para os alunos do curso Machine Learning . 



 O treinamento refor√ßado conquistou o mundo da Intel...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Otimiza√ß√£o da estrat√©gia de blackjack de Monte Carlo</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/477042/">  <i>A tradu√ß√£o do artigo foi preparada especificamente para os alunos do curso <a href="https://otus.pw/Zkti/">Machine Learning</a> .</i> <br><hr><br><img src="https://habrastorage.org/webt/dj/mh/7h/djmh7hubq1mecnsb-gilezo8qyi.png"><br><br>  O treinamento refor√ßado conquistou o mundo da Intelig√™ncia Artificial.  A partir do AlphaGo e <a href="https://www.space.com/alphastar-artificial-intelligence-starcraft-2-grandmaster.html">AlphaStar</a> , um n√∫mero crescente de atividades que antes eram dominadas por seres humanos agora s√£o conquistadas por agentes de IA com base no treinamento de refor√ßo.  Em resumo, essas conquistas dependem da otimiza√ß√£o das a√ß√µes do agente em um ambiente espec√≠fico para obter a recompensa m√°xima.  Nos √∫ltimos artigos do <a href="https://medium.com/gradientcrescent">GradientCrescent,</a> analisamos v√°rios aspectos fundamentais do aprendizado refor√ßado, desde o b√°sico de sistemas de <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-the-k-bandit-problem-illustrat-940eea430296">bandidos</a> e abordagens baseadas em <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-automating-pong-in-using-a-policy-model-an-implementation-b71f64c158ff">pol√≠ticas</a> at√© a otimiza√ß√£o do comportamento baseado em recompensa em <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-markov-decision-processes-policies-value-functions-94f7389e1e82">ambientes Markov</a> .  Todas essas abordagens exigiram conhecimento completo de nosso ambiente.  <a href="https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-gridworld-with-dynamic-programming-9b98a6f20310">A programa√ß√£o din√¢mica</a> , por exemplo, exige que tenhamos uma distribui√ß√£o de probabilidade completa de todas as transi√ß√µes de estado poss√≠veis.  Entretanto, na realidade, descobrimos que a maioria dos sistemas n√£o pode ser totalmente interpretada e que as distribui√ß√µes de probabilidade n√£o podem ser obtidas explicitamente devido √† complexidade, incerteza inerente ou limita√ß√µes nas capacidades computacionais.  Como analogia, considere a tarefa do meteorologista - o n√∫mero de fatores envolvidos na previs√£o do tempo pode ser t√£o grande que √© imposs√≠vel calcular com precis√£o a probabilidade. <a name="habracut"></a><br><br>  Para tais casos, m√©todos de ensino como Monte Carlo s√£o a solu√ß√£o.  O termo Monte Carlo √© comumente usado para descrever qualquer abordagem para estimativa de amostragem aleat√≥ria.  Em outras palavras, n√£o prevemos conhecimento sobre o meio ambiente, mas aprendemos com a experi√™ncia passando por sequ√™ncias exemplares de estados, a√ß√µes e recompensas obtidas como resultado da intera√ß√£o com o meio ambiente.  Esses m√©todos funcionam observando diretamente as recompensas retornadas pelo modelo durante a opera√ß√£o normal para julgar o valor m√©dio de suas condi√ß√µes.  √â interessante notar que, mesmo sem nenhum conhecimento da din√¢mica do ambiente (que deve ser considerada a distribui√ß√£o de probabilidade das transi√ß√µes de estado), ainda podemos obter um comportamento ideal para maximizar as recompensas. <br><br>  Como exemplo, considere o resultado de um lan√ßamento de 12 dados.  Considerando esses lan√ßamentos como um √∫nico estado, podemos calcular a m√©dia desses resultados para nos aproximarmos do verdadeiro resultado previsto.  Quanto maior a amostra, mais precisamente nos aproximamos do resultado esperado real. <br><br><img src="https://habrastorage.org/webt/xr/85/d6/xr85d6ugn6dbszaqh5b-nbq-usq.png"><br>  <i>A quantidade m√©dia esperada em 12 dados para 60 tiros √© 41,57</i> <br><br>  Esse tipo de avalia√ß√£o baseada em amostragem pode parecer familiar para o leitor, uma vez que essa amostragem tamb√©m √© realizada para sistemas k-bandit.  Em vez de comparar bandidos diferentes, os m√©todos de Monte Carlo s√£o usados ‚Äã‚Äãpara comparar pol√≠ticas diferentes em ambientes Markov, determinando o valor do estado conforme uma determinada pol√≠tica √© seguida at√© que o trabalho seja conclu√≠do. <br><br><h3>  Estimativa de Monte Carlo do valor do estado </h3><br>  No contexto do aprendizado por refor√ßo, os m√©todos de Monte Carlo s√£o uma maneira de avaliar a signific√¢ncia do estado de um modelo, calculando a m√©dia dos resultados da amostra.  Devido √† necessidade de um estado terminal, os m√©todos de Monte Carlo s√£o inerentemente aplic√°veis ‚Äã‚Äãa ambientes epis√≥dicos.  Devido a essa limita√ß√£o, os m√©todos de Monte Carlo s√£o geralmente considerados "aut√¥nomos", nos quais todas as atualiza√ß√µes s√£o realizadas ap√≥s atingir o estado terminal.  Uma analogia simples para encontrar uma sa√≠da de um labirinto pode ser dada - uma abordagem aut√¥noma for√ßaria o agente a chegar ao fim antes de usar a experi√™ncia intermedi√°ria adquirida, a fim de tentar reduzir o tempo necess√°rio para atravessar o labirinto.  Por outro lado, com a abordagem on-line, o agente muda constantemente seu comportamento j√° durante a passagem do labirinto, talvez ele perceba que os corredores verdes levam a becos sem sa√≠da e decidem evit√°-los, por exemplo.  Discutiremos abordagens on-line em um dos seguintes artigos. <br><br>  O m√©todo Monte Carlo pode ser formulado da seguinte maneira: <br><br><img src="https://habrastorage.org/webt/u4/kd/o_/u4kdo_uc3dj64rihmdjhnyj1r7y.png"><br><br>  Para entender melhor como o m√©todo de Monte Carlo funciona, considere o diagrama de transi√ß√£o de estado abaixo.  A recompensa para cada transi√ß√£o de estado √© exibida em preto, um fator de desconto de 0,5 √© aplicado a ela.  Vamos deixar de lado o valor real do estado e focar no c√°lculo dos resultados de um arremesso. <br><br><img src="https://habrastorage.org/webt/oc/he/n9/ochen91kvgbix5pgltzijpbozgg.png"><br>  <i>Diagrama de transi√ß√£o de estado.</i>  <i>O n√∫mero do status √© mostrado em vermelho, o resultado √© preto.</i> <br>  Dado que o estado do terminal retorna um resultado igual a 0, vamos calcular o resultado de cada estado, come√ßando com o estado do terminal (G5).  Observe que definimos o fator de desconto como 0,5, o que levar√° a uma pondera√ß√£o nos estados posteriores. <br><br><img src="https://habrastorage.org/webt/uw/y6/qq/uwy6qq_amgfxpsyj2_3gpij498o.png"><br><br>  Ou mais geralmente: <br><br><img src="https://habrastorage.org/webt/k7/i4/ua/k7i4ua-g8vaph8w62-rfpc3r8sk.png"><br><br>  Para evitar o armazenamento de todos os resultados da lista, podemos executar o procedimento de atualiza√ß√£o gradual do valor do estado no m√©todo Monte Carlo, usando uma equa√ß√£o que tem algumas semelhan√ßas com a descida tradicional do gradiente: <br><br><img src="https://habrastorage.org/webt/lv/tm/nk/lvtmnki_ff4csblmytygnnxjn6u.png"><br>  <i>Procedimento de atualiza√ß√£o incremental de Monte Carlo.</i>  <i>S √© o estado, V √© o seu valor, G √© o seu resultado e A √© o par√¢metro do valor da etapa.</i> <br><br>  Como parte do treinamento de refor√ßo, os m√©todos de Monte Carlo podem at√© ser classificados como Primeira visita ou Toda visita.  Em resumo, a diferen√ßa entre os dois √© quantas vezes um estado pode ser visitado em uma passagem antes da atualiza√ß√£o de Monte Carlo.  O m√©todo de primeira visita de Monte Carlo estima o valor de todos os estados como o valor m√©dio dos resultados ap√≥s visitas √∫nicas a cada estado antes da conclus√£o, enquanto o m√©todo de cada visita de Monte Carlo calcula a m√©dia dos resultados ap√≥s n visitas at√© a conclus√£o.  Usaremos a primeira visita de Monte Carlo ao longo deste artigo devido √† sua relativa simplicidade. <br><br><h3>  Gerenciamento de pol√≠ticas de Monte Carlo </h3><br>  Se o modelo n√£o puder fornecer a pol√≠tica, Monte Carlo poder√° ser usado para avaliar os valores da a√ß√£o do estado.  Isso √© mais √∫til do que apenas o significado dos estados, j√° que a id√©ia do significado de cada a√ß√£o <i>(q)</i> em um determinado estado permite que o agente formule automaticamente uma pol√≠tica a partir de observa√ß√µes em um ambiente desconhecido. <br><br>  Mais formalmente, podemos usar Monte Carlo para estimar <i>q (s, a, pi)</i> , o resultado esperado ao iniciar a partir do estado s, a√ß√£o ae pol√≠tica subseq√ºente <i>Pi</i> .  Os m√©todos de Monte Carlo permanecem os mesmos, exceto que h√° uma dimens√£o adicional de a√ß√µes tomadas para um determinado estado.  Acredita-se que um par de a√ß√£o-estado <i>(s, a)</i> seja visitado durante a passagem se o estado <i>s for</i> visitado e a a√ß√£o <i>a</i> for realizada nele.  Da mesma forma, a avalia√ß√£o das a√ß√µes de valor pode ser realizada usando as abordagens ‚ÄúPrimeira visita‚Äù e ‚ÄúToda visita‚Äù. <br><br>  Como na programa√ß√£o din√¢mica, podemos usar uma pol√≠tica de itera√ß√£o generalizada (GPI) para formar uma pol√≠tica a partir da observa√ß√£o dos valores de a√ß√£o do estado. <br><br><img src="https://habrastorage.org/webt/wi/pm/p5/wipmp58itbgocsyruulw0ccliay.png"><br><br>  Alternando as etapas de avalia√ß√£o e aprimoramento de pol√≠ticas, e incluindo pesquisas para garantir que todas as a√ß√µes poss√≠veis sejam visitadas, podemos alcan√ßar a pol√≠tica ideal para cada condi√ß√£o.  Para o GPI de Monte Carlo, essa rota√ß√£o geralmente √© feita ap√≥s o final de cada passe. <br><br><img src="https://habrastorage.org/webt/nf/b4/nw/nfb4nwijndtfex4ytwhdn2ujcyw.png"><br>  <i>GPI de Monte Carlo</i> <br><br><h3>  Estrat√©gia de Blackjack </h3><br>  Para entender melhor como o m√©todo de Monte Carlo funciona na pr√°tica na tarefa de avaliar v√°rios valores de estado, vamos fazer uma demonstra√ß√£o passo a passo do jogo de blackjack.  Para come√ßar, vamos determinar as regras e condi√ß√µes do nosso jogo: <br><br><ul><li>  Vamos jogar apenas contra o dealer, n√£o haver√° outros jogadores.  Isso nos permitir√° considerar as m√£os do revendedor como parte do ambiente. </li><li>  O valor dos cart√µes com n√∫meros iguais aos valores nominais.  O valor dos cart√µes de figuras: Valete, Rei e Rainha √© 10. O valor do √°s pode ser 1 ou 11, dependendo da escolha do jogador. </li><li>  Ambos os lados recebem dois cart√µes.  Duas cartas de jogador est√£o viradas para cima, uma das cartas do dealer tamb√©m est√° virada para cima. </li><li>  O objetivo do jogo √© que a quantidade de cartas na m√£o seja &lt;= 21.  Um valor maior que 21 √© um fracasso, se ambos os lados t√™m um valor de 21, o jogo √© empatado. </li><li>  Depois que o jogador v√™ suas cartas e a primeira carta do dealer, o jogador pode escolher uma nova carta ("ainda") ou n√£o ("suficiente") at√© que ele esteja satisfeito com a soma dos valores das cartas em sua m√£o. </li><li>  Em seguida, o croupier mostra sua segunda carta - se o valor resultante for menor que 17, ele √© obrigado a pegar cartas at√© atingir 17 pontos, ap√≥s o que n√£o aceita mais a carta. </li></ul><br>  Vamos ver como o m√©todo Monte Carlo funciona com essas regras. <br><br><h4>  Rodada 1. </h4><br>  Voc√™ ganha um total de 19. Mas voc√™ tenta pegar a sorte pela cauda, ‚Äã‚Äãarriscar, conseguir 3 e ficar sem dinheiro.  Quando voc√™ faliu, o dealer tinha apenas uma carta aberta com uma soma de 10. Isso pode ser representado da seguinte maneira: <br><br><img src="https://habrastorage.org/webt/4o/di/sl/4odisljnoapgkqblzsixkr2a4_4.png"><br><br>  Se ficarmos sem dinheiro, nossa recompensa para a rodada √© -1.  Vamos definir esse valor como o resultado de retorno do pen√∫ltimo estado, usando o seguinte formato [Valor do agente, valor do revendedor, √°s?]: <br><br><img src="https://habrastorage.org/webt/2q/er/g4/2qerg4mooln9blj9sfqo1ndzxbo.png"><br><br>  Bem, agora estamos sem sorte.  Vamos para outra rodada. <br><br><h4>  Rodada 2. </h4><br>  Voc√™ digita um total de 19. Desta vez, voc√™ decide parar.  O revendedor disca 13, pega um cart√£o e fica sem dinheiro.  O pen√∫ltimo estado pode ser descrito da seguinte maneira. <br><br><img src="https://habrastorage.org/webt/al/th/dv/althdvyqwkhwqd5cj2vvzcjbiam.png"><br><br>  Vamos descrever as condi√ß√µes e recompensas que recebemos nesta rodada: <br><br><img src="https://habrastorage.org/webt/rm/mv/n8/rmmvn88mg7za9gyrtvrvohywmwc.png"><br><br>  Com o final da passagem, agora podemos atualizar os valores de todos os nossos estados nesta rodada usando os resultados calculados.  Tomando um fator de desconto de 1, simplesmente distribu√≠mos nossa nova recompensa de m√£o, como foi feito nas transi√ß√µes de estado anteriores.  Como o estado <i>V (19, 10, no)</i> retornou anteriormente -1, calculamos o valor de retorno esperado e o atribu√≠mos ao nosso estado: <br><br><img src="https://habrastorage.org/webt/u0/um/q9/u0umq9dpg6qw420ecz44cqfp9mk.png"><br>  <i>Valores finais do estado para demonstra√ß√£o usando o blackjack como exemplo</i> . <br><br><h3>  Implementa√ß√£o </h3><br>  Vamos escrever um jogo de blackjack usando o m√©todo Monte Carlo da primeira visita para descobrir todos os valores de estado poss√≠veis (ou v√°rias combina√ß√µes dispon√≠veis) no jogo usando Python.  Nossa abordagem ser√° baseada na <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">abordagem de Sudharsan et.</a>  <a href="http://aigradients.com/2019/07/03/blackjack-with-monte-carlo-prediction/">al.</a>  .  Como de costume, voc√™ pode encontrar todo o c√≥digo do artigo em <a href="https://github.com/EXJUSTICE/GradientCrescent)">nosso GitHub</a> . <br><br>  Para simplificar a implementa√ß√£o, usaremos o gin√°sio da OpenAI.  Pense no ambiente como uma interface para iniciar o blackjack com uma quantidade m√≠nima de c√≥digo; isso nos permitir√° focar na implementa√ß√£o do aprendizado refor√ßado.  Convenientemente, todas as informa√ß√µes coletadas sobre os estados, a√ß√µes e recompensas s√£o armazenadas nas vari√°veis ‚Äã‚Äãde <i>"observa√ß√£o"</i> , que s√£o acumuladas durante as sess√µes de jogo atuais. <br><br>  Vamos come√ßar importando todas as bibliotecas necess√°rias para obter e coletar nossos resultados. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> functools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> partial %matplotlib inline plt.style.use(<span class="hljs-string"><span class="hljs-string">'ggplot'</span></span>)</code> </pre> <br>  Em seguida, vamos inicializar nosso ambiente de <i>academia</i> e definir uma pol√≠tica que coordene as a√ß√µes de nosso agente.  De fato, continuaremos a receber o cart√£o at√© que o valor na m√£o atinja 19 ou mais, ap√≥s o que paramos. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Observation here encompassess all data about state that we need, as well as reactions to it env = gym.make('Blackjack-v0') #Define a policy where we hit until we reach 19. # actions here are 0-stand, 1-hit def sample_policy(observation): score, dealer_score, usable_ace = observation return 0 if score &gt;= 19 else 1</span></span></code> </pre> <br>  Vamos definir um m√©todo para gerar dados de aprova√ß√£o usando nossa pol√≠tica.  Armazenaremos informa√ß√µes sobre o status, as a√ß√µes executadas e a remunera√ß√£o pela a√ß√£o. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_episode</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># we initialize the list for storing states, actions, and rewards states, actions, rewards = [], [], [] # Initialize the gym environment observation = env.reset() while True: # append the states to the states list states.append(observation) # now, we select an action using our sample_policy function and append the action to actions list action = sample_policy(observation) actions.append(action) # We perform the action in the environment according to our sample_policy, move to the next state observation, reward, done, info = env.step(action) rewards.append(reward) # Break if the state is a terminal state (ie done) if done: break return states, actions, rewards</span></span></code> </pre> <br>  Por fim, vamos definir a primeira visita da fun√ß√£o de previs√£o de Monte Carlo.  Primeiro, inicializamos um dicion√°rio vazio para armazenar os valores atuais do estado e um dicion√°rio que armazena o n√∫mero de registros para cada estado em diferentes passagens. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">first_visit_mc_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(policy, env, n_episodes)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, we initialize the empty value table as a dictionary for storing the values of each state value_table = defaultdict(float) N = defaultdict(int)</span></span></code> </pre> <br>  Para cada passagem, chamamos nosso m√©todo <i>generate_episode</i> para obter informa√ß√µes sobre os valores do estado e as recompensas recebidas ap√≥s a ocorr√™ncia do estado.  Tamb√©m inicializamos a vari√°vel para armazenar nossos resultados incrementais.  Em seguida, obtemos a recompensa e o valor atual do estado para cada estado visitado durante o passe e aumentamos nossos retornos vari√°veis ‚Äã‚Äãpelo valor da recompensa por etapa. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_episodes): <span class="hljs-comment"><span class="hljs-comment"># Next, we generate the epsiode and store the states and rewards states, _, rewards = generate_episode(policy, env) returns = 0 # Then for each step, we store the rewards to a variable R and states to S, and we calculate for t in range(len(states) ‚Äî 1, -1, -1): R = rewards[t] S = states[t] returns += R # Now to perform first visit MC, we check if the episode is visited for the first time, if yes, #This is the standard Monte Carlo Incremental equation. # NewEstimate = OldEstimate+StepSize(Target-OldEstimate) if S not in states[:t]: N[S] += 1 value_table[S] += (returns ‚Äî value_table[S]) / N[S] return value_table</span></span></code> </pre> <br>  Deixe-me lembr√°-lo que, como estamos implementando a primeira visita de Monte Carlo, visitamos um estado de uma s√≥ vez.  Portanto, fazemos uma verifica√ß√£o condicional no dicion√°rio de estado para ver se o estado foi visitado.  Se essa condi√ß√£o for atendida, poderemos calcular o novo valor usando o procedimento definido anteriormente para atualizar os valores de estado usando o m√©todo Monte Carlo e aumentar o n√∫mero de observa√ß√µes para esse estado em 1. Em seguida, repetimos o processo para a pr√≥xima passagem para obter o resultado m√©dio. . <br><br>  Vamos executar o que obtivemos e ver os resultados! <br><br><pre> <code class="python hljs">value = first_visit_mc_prediction(sample_policy, env, n_episodes=<span class="hljs-number"><span class="hljs-number">500000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): print(value.popitem())</code> </pre> <br><img src="https://habrastorage.org/webt/xc/ow/al/xcowalpr-34tgxtf0gq8zlfllgg.png"><br>  <i>Conclus√£o de uma amostra mostrando os valores de estado de v√°rias combina√ß√µes nas m√£os no blackjack.</i> <br><br>  Podemos continuar fazendo observa√ß√µes de Monte Carlo para 5000 passes e construindo uma distribui√ß√£o de valores de estado que descrevem os valores de qualquer combina√ß√£o nas m√£os do jogador e do dealer. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_blackjack</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(V, ax1, ax2)</span></span></span><span class="hljs-function">:</span></span> player_sum = np.arange(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">21</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) dealer_show = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) usable_ace = np.array([<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>]) state_values = np.zeros((len(player_sum), len(dealer_show), len(usable_ace))) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, player <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(player_sum): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, dealer <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(dealer_show): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, ace <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(usable_ace): state_values[i, j, k] = V[player, dealer, ace] X, Y = np.meshgrid(player_sum, dealer_show) ax1.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">0</span></span>]) ax2.plot_wireframe(X, Y, state_values[:, :, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ax <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ax1, ax2: ax.set_zlim(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) ax.set_ylabel(<span class="hljs-string"><span class="hljs-string">'player sum'</span></span>) ax.set_xlabel(<span class="hljs-string"><span class="hljs-string">'dealer sum'</span></span>) ax.set_zlabel(<span class="hljs-string"><span class="hljs-string">'state-value'</span></span>) fig, axes = pyplot.subplots(nrows=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>),subplot_kw={<span class="hljs-string"><span class="hljs-string">'projection'</span></span>: <span class="hljs-string"><span class="hljs-string">'3d'</span></span>}) axes[<span class="hljs-number"><span class="hljs-number">0</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/o usable ace'</span></span>) axes[<span class="hljs-number"><span class="hljs-number">1</span></span>].set_title(<span class="hljs-string"><span class="hljs-string">'state-value distribution w/ usable ace'</span></span>) plot_blackjack(value, axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], axes[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/db/7t/he/db7thebzqawmtv_jemoprsaeqvg.png"><br>  <i>Visualiza√ß√£o dos valores de estado de v√°rias combina√ß√µes no blackjack.</i> <br><br>  Ent√£o, vamos resumir o que aprendemos. <br><br><ul><li>  Os m√©todos de aprendizado baseados em amostragem nos permitem avaliar valores de estado e estado de a√ß√£o sem qualquer din√¢mica de transi√ß√£o, simplesmente por amostragem. </li><li>  As abordagens de Monte Carlo s√£o baseadas em amostragem aleat√≥ria do modelo, observando as recompensas retornadas pelo modelo e coletando informa√ß√µes durante a opera√ß√£o normal para determinar o valor m√©dio de seus estados. </li><li>  Usando m√©todos de Monte Carlo, √© poss√≠vel uma pol√≠tica de itera√ß√£o generalizada. </li><li>  O valor de todas as combina√ß√µes poss√≠veis nas m√£os do jogador e do dealer no blackjack pode ser estimado usando v√°rias simula√ß√µes de Monte Carlo, abrindo caminho para estrat√©gias otimizadas. </li></ul><br>  Isso conclui a introdu√ß√£o ao m√©todo Monte Carlo.  Em nosso pr√≥ximo artigo, seguiremos para m√©todos de ensino da forma Aprendizado das diferen√ßas temporais. <br><br><h3>  Fontes: </h3><br>  Sutton et.  Aprendizagem por Refor√ßo <br>  White et.  Fundamentos da Aprendizagem por Refor√ßo, Universidade de Alberta <br>  Silva et.  Aprendizagem por Refor√ßo, UCL <br>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Platt et.</a>  <a href="http://www.ccs.neu.edu/home/rplatt/cs7180_fall2018/slides/monte_carlo.pdf">Al, Northeaster University</a> <br><br>  S√≥ isso.  Vejo voc√™ no <a href="https://otus.pw/Zkti/">curso</a> ! </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt477042/">https://habr.com/ru/post/pt477042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt477022/index.html">Como podemos ajud√°-lo? Como voc√™ pode nos ajudar?</a></li>
<li><a href="../pt477026/index.html">S√©timo Hackathon Anual da JetBrains</a></li>
<li><a href="../pt477032/index.html">Do blockchain ao DAG: livrar-se dos intermedi√°rios</a></li>
<li><a href="../pt477038/index.html">A melhor linguagem de programa√ß√£o para iniciantes</a></li>
<li><a href="../pt477040/index.html">Gr√°fico Gartner 2019: sobre o que s√£o todas essas chav√µes?</a></li>
<li><a href="../pt477044/index.html">Automa√ß√£o de testes de ponta a ponta de um sistema de informa√ß√£o integrado. Parte 2. T√©cnico</a></li>
<li><a href="../pt477046/index.html">.Net Meetup em Raiffeisenbank 28/11 + Transmiss√£o</a></li>
<li><a href="../pt477048/index.html">Por que uma empresa com uma capitaliza√ß√£o de US $ 55 bilh√µes pensou em deixar a bolsa</a></li>
<li><a href="../pt477050/index.html">Sexta-feira negra de 2019 para vigil√¢ncia por v√≠deo e nuvens.</a></li>
<li><a href="../pt477052/index.html">Reator, WebFlux, Kotlin Coroutines ou Assincronia com um exemplo simples</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>