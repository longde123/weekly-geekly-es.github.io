<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ûó üßî üñêüèº Das Buch ‚ÄûEffective Spark. Skalierung und Optimierung " üõ©Ô∏è üí¥ üà≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Beitrag werden wir uns mit dem Zugriff auf die Spark-API aus verschiedenen Programmiersprachen in der JVM sowie einigen Leistungsproblemen b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Das Buch ‚ÄûEffective Spark. Skalierung und Optimierung "</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/414525/"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/webt/g1/uu/lu/g1uulu2edgzcixecswin9lfylnc.jpeg" align="left" alt="Bild"></a>  In diesem Beitrag werden wir uns mit dem Zugriff auf die Spark-API aus verschiedenen Programmiersprachen in der JVM sowie einigen Leistungsproblemen befassen, die √ºber die Scala-Sprache hinausgehen.  Selbst wenn Sie au√üerhalb der JVM arbeiten, kann dieser Abschnitt hilfreich sein, da Nicht-JVM-Sprachen h√§ufig von der Java-API und nicht von der Scala-API abh√§ngen. <br><br>  Das Arbeiten in anderen Programmiersprachen bedeutet nicht immer, dass Sie √ºber die JVM hinausgehen m√ºssen, und das Arbeiten in der JVM bietet viele Vorteile in Bezug auf die Leistung - haupts√§chlich aufgrund der Tatsache, dass Sie keine Daten kopieren m√ºssen.  Obwohl es nicht erforderlich ist, spezielle Bindungsbibliotheken oder Adapter zu verwenden, um von au√üerhalb der Scala-Sprache auf Spark zuzugreifen, kann es schwierig sein, Scala-Code aus anderen Programmiersprachen aufzurufen.  Das Spark-Framework unterst√ºtzt die Verwendung von Java 8 in Lambda-Ausdr√ºcken, und diejenigen, die √§ltere Versionen des JDK verwenden, haben die M√∂glichkeit, die entsprechende Schnittstelle aus dem Paket org.apache.spark.api.java.function zu implementieren.  Selbst in F√§llen, in denen Sie keine Daten kopieren m√ºssen, kann die Arbeit in einer anderen Programmiersprache kleine, aber wichtige Leistungsnuancen aufweisen. <br><a name="habracut"></a><br>  Die Schwierigkeiten beim Zugriff auf verschiedene Scala-APIs sind besonders ausgepr√§gt, wenn Funktionen mit Klassen-Tags aufgerufen werden oder wenn Eigenschaften verwendet werden, die mithilfe impliziter Typkonvertierungen bereitgestellt werden (z. B. alle Funktionen von RDD-Sets, die sich auf die Double- und Tuple-Klassen beziehen).  F√ºr Mechanismen, die von impliziten Typkonvertierungen abh√§ngen, werden h√§ufig √§quivalente konkrete Klassen zusammen mit expliziten Konvertierungen f√ºr diese bereitgestellt.  Dummy-Klassen-Tags (z. B. AnyRef) k√∂nnen an Funktionen √ºbergeben werden, die von Klassen-Tags abh√§ngen (Adapter tun dies h√§ufig automatisch.  Die Verwendung bestimmter Klassen anstelle impliziter Typkonvertierungen f√ºhrt normalerweise nicht zu zus√§tzlichem Overhead. Dummy-Klassen-Tags k√∂nnen jedoch einige Compiler-Optimierungen einschr√§nken. <br><br>  Die Java-API unterscheidet sich in ihren Eigenschaften nicht allzu sehr von der Scala-API. Nur gelegentlich fehlen einige Funktionen oder Entwickler-APIs.  Andere JVM-Programmiersprachen wie die Clojure-Sprache mit DSL <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Flambo</a> und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sparkling-</a> Bibliothek werden mithilfe verschiedener Java-APIs unterst√ºtzt, anstatt die Scala-API direkt aufzurufen.  Da die meisten Sprachbindungen, auch Nicht-JVM-Sprachen wie Python und R, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Java-</a> API durchlaufen, ist es hilfreich, damit umzugehen. <br><br>  Die Java-APIs sind den Scala-APIs sehr √§hnlich, obwohl sie unabh√§ngig von Klassen-Tags und impliziten Konvertierungen sind.  Das Fehlen des letzteren bedeutet, dass Sie anstelle der automatischen Konvertierung der RDD-S√§tze von Tupel- oder Doppelobjekten in spezielle Klassen mit zus√§tzlichen Funktionen Funktionen der expliziten Typkonvertierung verwenden m√ºssen (z. B. mapToDouble oder mapToPair).  Die angegebenen Funktionen sind nur f√ºr Java-RDD-S√§tze definiert.  Zum Gl√ºck aus Kompatibilit√§tsgr√ºnden sind diese speziellen Typen nur Adapter f√ºr Scala RDD-Sets.  Dar√ºber hinaus geben diese Sonderfunktionen verschiedene Datentypen zur√ºck, z. B. JavaDoubleRDD und JavaPairRDD, mit Funktionen, die durch implizite Scala-Sprachtransformationen bereitgestellt werden. <br><br>  Wenden wir uns noch einmal dem kanonischen Beispiel der Wortz√§hlung mit der Java-API zu (Beispiel 7.1).  Da das Aufrufen der Scala-API von Java aus manchmal eine entmutigende Aufgabe sein kann, sind fast alle Spark-Java-Framework-APIs in der Scala-Sprache mit versteckten Klassen-Tags und impliziten Konvertierungen implementiert.  Aus diesem Grund sind Java-Adapter eine sehr d√ºnne Schicht, die im Durchschnitt nur aus wenigen Codezeilen besteht, und das Umschreiben ist praktisch m√ºhelos. <br><br>  Beispiel 7.1  Wortz√§hlung (Java) <br><br><pre><code class="hljs actionscript"><span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> scala.Tuple2;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaRDD;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaPairRDD </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.spark.api.java.JavaSparkContext;</span></span>  <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.regex.Pattern;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.util.Arrays;</span></span>  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">WordCount</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> Pattern pattern = Pattern.compile(<span class="hljs-string"><span class="hljs-string">" "</span></span>);  <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">void</span></span> main(String[] args) throws Exception { JavaSparkContext jsc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JavaSparkContext(); JavaRDD&lt;String&gt; lines = jsc.textFile(args[<span class="hljs-number"><span class="hljs-number">0</span></span>]); JavaRDD&lt;String&gt; words = lines.flatMap(e -&gt; Arrays.asList(                                           pattern.split(e)).iterator()); JavaPairRDD&lt;String, Integer&gt; wordsIntial = words.mapToPair(  e -&gt; <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Tuple2&lt;String, Integer&gt;(e, <span class="hljs-number"><span class="hljs-number">1</span></span>));   } }</code> </pre> <br>  Manchmal m√ºssen Sie m√∂glicherweise Java-RDDs in Scala-RDDs konvertieren oder umgekehrt.  Dies wird am h√§ufigsten f√ºr Bibliotheken ben√∂tigt, die Eingaben ben√∂tigen oder Scala-RDD-S√§tze zur√ºckgeben. Manchmal sind die grundlegenden Spark-Eigenschaften jedoch noch nicht in der Java-API verf√ºgbar.  Das Konvertieren von Java-RDDs in Scala-RDDs ist der einfachste Weg, diese neuen Funktionen zu verwenden. <br><br>  Wenn Sie den Java-RDD-Satz in die Scala-Bibliothek √ºbertragen m√ºssen, die einen regul√§ren RDD-Spark an der Eingabe erwartet, k√∂nnen Sie mit der Methode rdd () auf die zugrunde liegende RDD-Scala zugreifen.  Meistens reicht dies aus, um die endg√ºltige RDD in eine beliebige gew√ºnschte Scala-Bibliothek zu √ºbertragen.  Zu den bemerkenswerten Ausnahmen z√§hlen Scala-Bibliotheken, die in ihrer Arbeit auf impliziten Konvertierungen von Arten von Inhaltss√§tzen oder Klassen-Tag-Informationen beruhen.  In diesem Fall ist der einfachste Weg, auf implizite Konvertierungen zuzugreifen, das Schreiben eines kleinen Adapters in Scala.  Wenn Scala-Shells nicht verwendet werden k√∂nnen, k√∂nnen Sie die entsprechende Funktion der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">JavaConverters-</a> Klasse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufrufen</a> und ein Dummy-Klassen-Tag bilden. <br><br>  Um ein Dummy-Klassen-Tag zu erstellen, k√∂nnen Sie die Methode scala.reflect.ClassTag $ .MODULE $ .AnyRef () verwenden oder die echte Methode mit scala.reflect.ClassTag $ .MODULE $ .apply (CLASS) abrufen, wie in den Beispielen 7.2 und 7.3 gezeigt. <br><br>  F√ºr die Konvertierung von Scala RDD nach RDD Java sind Klassen-Tag-Informationen h√§ufig wichtiger als die meisten Spark-Bibliotheken.  Der Grund daf√ºr ist, dass verschiedene JavaRDD-Klassen zwar √∂ffentlich zug√§ngliche Konstruktoren bereitstellen, die Scala RDD als Argumente verwenden, diese jedoch aus Scala-Code aufgerufen werden sollen und daher Informationen zum Klassen-Tag ben√∂tigen. <br><br>  Dummy-Klassen-Tags werden am h√§ufigsten in generischem Code oder Vorlagencode verwendet, bei dem die genauen Typen zum Zeitpunkt der Kompilierung unbekannt sind.  Solche Tags sind oft genug, obwohl die M√∂glichkeit besteht, dass einige Nuancen auf der Seite des Scala-Codes verloren gehen.  In sehr seltenen F√§llen erfordert der Scala-Code genaue Informationen zu Klassen-Tags.  In diesem Fall m√ºssen Sie ein echtes Tag verwenden.  In den meisten F√§llen ist dies nicht sehr aufw√§ndig und verbessert die Leistung. Versuchen Sie daher, solche Tags nach M√∂glichkeit zu verwenden. <br><br>  Beispiel 7.2.  Kompatibilit√§t von Java / Scala RDD mit einem Dummy-Klassen-Tag <br><br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static JavaPairRDD wrapPairRDDFakeCt( RDD&lt;Tuple2&lt;String, <span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt;&gt; RDD) { //       AnyRef ‚Äî   //        , //        , //        //    ClassTag&lt;<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>&gt; fake = ClassTag$.MODULE$.AnyRef(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> JavaPairRDD(rdd, fake, fake); }</code> </pre> <br>  Beispiel 7.3.  Sicherstellen der Java / Scala RDD-Kompatibilit√§t <br><br><pre> <code class="hljs ruby">public static JavaPairRDD wrapPairRDD( RDD&lt;Tuple2&lt;String, Object<span class="hljs-meta"><span class="hljs-meta">&gt;&gt; </span></span>RDD) { <span class="hljs-regexp"><span class="hljs-regexp">//</span></span>    ClassTag&lt;String&gt; strCt = ClassTag$.MODULE$.apply(String.class); ClassTag&lt;Long&gt; longCt = ClassTag$.MODULE$.apply(scala.Long.class); return new JavaPairRDD(rdd, strCt, longCt); }</code> </pre> <br>  Sowohl die Spark SQL- als auch die ML-Pipeline-APIs wurden in Java und Scala gr√∂√ütenteils konsistent gemacht.  Es gibt jedoch Java-spezifische Hilfsfunktionen, und die ihnen entsprechenden Scala-Funktionen sind nicht einfach aufzurufen.  Hier sind ihre Beispiele: verschiedene numerische Funktionen wie Plus, Minus usw. f√ºr die Column-Klasse.  Es ist schwierig, ihre √ºberladenen Entsprechungen aus der Scala-Sprache (+, -) zu bezeichnen.  Anstatt JavaDataFrame und JavaSQLContext zu verwenden, werden Java-erforderliche Methoden in SQLContext und regul√§ren DataFrame-Sets verf√ºgbar gemacht.  Dies kann Sie verwirren, da einige der in der Java-Dokumentation genannten Methoden nicht aus Java-Code verwendet werden k√∂nnen. In solchen F√§llen werden jedoch Funktionen mit demselben Namen zum Aufrufen aus Java bereitgestellt. <br><br>  Benutzerdefinierte Funktionen (UDFs) in der Java-Sprache und in den meisten anderen Sprachen au√üer Scala erfordern die Angabe des Typs des von der Funktion zur√ºckgegebenen Werts, da dieser nicht logisch abgeleitet werden kann, √§hnlich wie er in der Scala-Sprache ausgef√ºhrt wird (Beispiel 7.4). . <br><br>  Beispiel 7.4.  UDF-Beispiel f√ºr Java <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">sqlContext</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.udf</span></span>() <span class="hljs-selector-class"><span class="hljs-selector-class">.register</span></span>("<span class="hljs-selector-tag"><span class="hljs-selector-tag">strlen</span></span>", (<span class="hljs-selector-tag"><span class="hljs-selector-tag">String</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span>) <span class="hljs-selector-tag"><span class="hljs-selector-tag">-</span></span>&gt; <span class="hljs-selector-tag"><span class="hljs-selector-tag">s</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.length</span></span>(), <span class="hljs-selector-tag"><span class="hljs-selector-tag">DataTypes</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.StringType</span></span>);</code> </pre> <br>  Obwohl die f√ºr die Scala- und Java-APIs erforderlichen Typen unterschiedlich sind, erfordert das Umschlie√üen von Java-Sammlungstypen kein zus√§tzliches Kopieren.  Bei Iteratoren wird die f√ºr den Adapter erforderliche Typkonvertierung beim Zugriff auf die Elemente verz√∂gert ausgef√ºhrt, sodass das Spark-Framework bei Bedarf Daten sichern kann (wie im Abschnitt "Durchf√ºhren von Iterator-Iterator-Transformationen mit der Funktion mapPartitions" auf Seite 121 erl√§utert).  Dies ist sehr wichtig, da f√ºr viele einfache Vorg√§nge die Kosten f√ºr das Kopieren von Daten h√∂her sein k√∂nnen als die Kosten f√ºr die Berechnung selbst. <br><br><h3>  Jenseits von Scala und JVM </h3><br>  Wenn Sie sich nicht auf die JVM beschr√§nken, steigt die Anzahl der f√ºr die Arbeit verf√ºgbaren Programmiersprachen dramatisch an.  Bei der aktuellen Spark-Architektur kann das Arbeiten au√üerhalb der JVM - insbesondere auf Arbeitsknoten - zu erheblichen Kostensteigerungen f√ºhren, da Daten in Arbeitsknoten zwischen der JVM und dem Zielsprachencode kopiert werden.  Bei komplexen Vorg√§ngen ist der Anteil der Kosten f√ºr das Kopieren von Daten relativ gering, bei einfachen Vorg√§ngen kann dies jedoch leicht zu einer Verdoppelung der gesamten Rechenkosten f√ºhren. <br><br>  Die erste Nicht-JVM-Programmiersprache, die direkt au√üerhalb von Spark unterst√ºtzt wird, ist Python. Die API und die Schnittstelle sind das Modell, auf dem Implementierungen f√ºr andere Nicht-JVM-Programmiersprachen basieren. <br><br><h3>  Wie PySpark funktioniert </h3><br>  PySpark stellt eine Verbindung zu JVM Spark her, indem eine Mischung aus Kan√§len f√ºr Worker und Py4J, eine spezialisierte Bibliothek, die Python / Java-Interaktion bietet, f√ºr den Treiber verwendet werden.  Darunter verbirgt die einfache Architektur auf den ersten Blick viele komplexe Nuancen, dank derer PySpark funktioniert, wie in Abb.  7.1.  Eines der Hauptprobleme: Selbst wenn die Daten von einem Python-Worker in die JVM kopiert werden, k√∂nnen sie von einer virtuellen Maschine nicht einfach analysiert werden.  Sowohl der Python- als auch der Java-Mitarbeiter m√ºssen besondere Anstrengungen unternehmen, um sicherzustellen, dass die JVM √ºber gen√ºgend Informationen f√ºr Vorg√§nge wie die Partitionierung verf√ºgt. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/10/ez/wf/10ezwfv-1jvl1gxwsansnexwvj4.png" alt="Bild"></div><br><h3>  PySpark RDD Kits </h3><br>  Die Kosten f√ºr Ressourcen zum √úbertragen von Daten zur und von der JVM sowie zum Ausf√ºhren des Python-Executors sind erheblich.  Sie k√∂nnen viele Leistungsprobleme mit den PySpark RDD Suite-APIs mithilfe der DataFrame / Dataset-APIs vermeiden, da die Daten so lange wie m√∂glich in der JVM verbleiben. <br><br>  Das Kopieren von Daten von der JVM nach Python erfolgt mithilfe von Sockets und serialisierten Bytes.  Eine allgemeinere Version f√ºr die Interaktion mit Programmen in anderen Sprachen ist √ºber die PipedRDD-Schnittstelle verf√ºgbar, deren Anwendung im Unterabschnitt ‚ÄûUsing Pipe‚Äú gezeigt wird. <br><br>  Die Organisation von Kan√§len f√ºr den Datenaustausch (in zwei Richtungen) f√ºr jede Transformation w√§re zu teuer.  Infolgedessen organisiert PySpark (wenn m√∂glich) die Python-Transformationspipeline im Python-Interpreter und verkettet die Filteroperation und anschlie√üend die Zuordnung auf dem Python-Objektiterator mithilfe der speziellen PipelinedRDD-Klasse.  Selbst wenn Sie Daten mischen m√ºssen und PySpark keine Konvertierungen in der virtuellen Maschine eines einzelnen Mitarbeiters verketten kann, k√∂nnen Sie den Python-Interpreter wiederverwenden, sodass sich die Kosten f√ºr das Starten des Interpreters nicht weiter verlangsamen. <br><br>  Dies ist nur ein Teil des Puzzles.  Normale PipedRDDs arbeiten mit dem String-Typ, der aufgrund des Fehlens eines nat√ºrlichen Schl√ºssels nicht so einfach zu mischen ist.  In PySpark und in seinem Image und seiner √Ñhnlichkeit in Bibliotheken, die an viele andere Programmiersprachen gebunden sind, wird ein spezieller Typ von PairwiseRDD verwendet, bei dem der Schl√ºssel eine lange Ganzzahl ist und dessen Deserialisierung durch Benutzercode in der Scala-Sprache durchgef√ºhrt wird, der zum Parsen von Python-Werten vorgesehen ist.  Die Kosten f√ºr diese Deserialisierung sind nicht zu hoch, aber es zeigt, dass Scala im Spark-Framework die Ergebnisse von Python-Code grunds√§tzlich als "undurchsichtige" Byte-Arrays betrachtet. <br><br>  Bei aller Einfachheit funktioniert dieser Integrationsansatz √ºberraschend gut, und die meisten Operationen an Scala-RDD-Sets sind in Python verf√ºgbar.  An einigen der schwierigsten Stellen im Code wird auf Bibliotheken zugegriffen, z. B. auf MLlib, und es werden Daten aus verschiedenen Quellen geladen / gespeichert. <br><br>  Das Arbeiten mit verschiedenen Datenformaten ist ebenfalls mit Einschr√§nkungen verbunden, da ein wesentlicher Teil des Codes zum Laden / Speichern von Daten aus dem Spark-Framework auf den Hadoop-Java-Schnittstellen basiert.  Dies bedeutet, dass alle geladenen Daten zuerst in die JVM geladen und erst dann nach Python verschoben werden. <br><br>  F√ºr die Interaktion mit MLlib werden normalerweise zwei Ans√§tze verwendet: Entweder verwendet PySpark einen speziellen Datentyp mit Scala-Typkonvertierungen, oder der Algorithmus wird in Python erneut implementiert.  Diese Probleme k√∂nnen mit dem Spark ML-Paket vermieden werden, das die DataFrame / Dataset-Schnittstelle verwendet, die normalerweise Daten in der JVM speichert. <br><br><h3>  PySpark DataFrame- und Dataset-Kits </h3><br>  Die DataFrame- und Dataset-Sets weisen mit den Python-RDD-Set-APIs nicht viele Leistungsprobleme auf, da sie Daten so lange wie m√∂glich in der JVM speichern.  Der gleiche Leistungstest, den wir durchgef√ºhrt haben, um die √úberlegenheit von DataFrame-Sets gegen√ºber RDD-Sets zu veranschaulichen (siehe Abbildung 3.1), zeigt signifikante Unterschiede bei der Ausf√ºhrung in Python (Abbildung 7.2). <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/d9/mk/tl/d9mktl7qhe3hg8z2e9lnyanlgde.png" alt="Bild"></div><br>  Bei vielen Vorg√§ngen mit DataFrame- und Dataset-S√§tzen m√ºssen Sie m√∂glicherweise √ºberhaupt keine Daten aus der JVM verschieben, obwohl f√ºr die Verwendung verschiedener UDF-, UDAF- und Python-Lambda-Ausdr√ºcke nat√ºrlich einige der Daten in die JVM verschoben werden m√ºssen.  Dies f√ºhrt zu dem folgenden vereinfachten Schema f√ºr viele Operationen, das wie das in Fig. 1 gezeigte aussieht.  7.3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4e/3q/el/4e3qel6hamrvb5ipzycqh9sftcg.png" alt="Bild"></div><br><h3>  Zugriff auf zugrunde liegende Java-Objekte und gemischten Code in Scala </h3><br>  Eine wichtige Konsequenz der PySpark-Architektur ist, dass viele der Spark Python-Framework-Klassen tats√§chlich Adapter sind, um Aufrufe aus Python-Code in eine verst√§ndliche JVM-Form zu √ºbersetzen. <br><br>  Wenn Sie mit Scala / Java-Entwicklern zusammenarbeiten und mit deren Code interagieren m√∂chten, gibt es im Voraus keine Adapter f√ºr den Zugriff auf Ihren Code. Sie k√∂nnen jedoch Ihre Java / Scala-UDF registrieren und sie aus Python-Code verwenden.  Ab Spark 2.1 kann dies mit der registerJavaFunction-Methode des sqlContext-Objekts erfolgen. <br><br>  Manchmal verf√ºgen diese Adapter nicht √ºber alle erforderlichen Mechanismen, und da Python keinen starken Schutz gegen das Aufrufen privater Methoden bietet, k√∂nnen Sie sich sofort an die JVM wenden.  Mit derselben Technik k√∂nnen Sie auf Ihren eigenen Code in der JVM zugreifen und die Ergebnisse mit geringem Aufwand wieder in Python-Objekte konvertieren. <br><br>  Im Unterabschnitt "Gro√üe Abfragepl√§ne und iterative Algorithmen" auf S. 22.  Wir haben festgestellt, wie wichtig es ist, die JVM-Version der DataFrame- und RDD-Sets zu verwenden, um den Abfrageplan zu reduzieren.  Dies ist eine Problemumgehung, da der SQL-Optimierer aufgrund der Platzierung des RDD-Satzes in der Mitte die M√∂glichkeit verliert, √ºber den Moment hinaus zu schauen, in dem die Daten in RDD angezeigt werden, wenn die Abfragepl√§ne f√ºr die Verarbeitung durch den Spark SQL-Optimierer zu gro√ü werden.  Dasselbe kann mit Hilfe √∂ffentlicher Python-APIs erreicht werden. Gleichzeitig gehen jedoch viele Vorteile von DataFrame-Sets verloren, da alle Daten √ºber die Arbeitsknoten von Python hin und her gehen m√ºssen.  Stattdessen k√∂nnen Sie das Ursprungsdiagramm reduzieren, indem Sie weiterhin Daten in der JVM speichern (wie in Beispiel 7.5 gezeigt). <br><br>  Beispiel 7.5  Trimmen eines gro√üen Abfrageplans f√ºr einen DataFrame mit Python <br><br><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cutLineage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    DataFrame ‚Äî     .. :              &gt;&gt;&gt; df = RDD.toDF() &gt;&gt;&gt; cutDf = cutLineage(df) &gt;&gt;&gt; cutDf.count() 3 """</span></span> jRDD = df._jdf.toJavaRDD() jSchema = df._jdf.schema() jRDD.cache() sqlCtx = df.sql_ctx <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx newJavaDF = javaSqlCtx.createDataFrame(jRDD, jSchema) newDF = DataFrame(newJavaDF, sqlCtx) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> newDF</code> </pre> <br>  Im Allgemeinen wird gem√§√ü der Konvention die Syntax _j [abgek√ºrzter_Name] verwendet, um auf die internen Java-Versionen der meisten Python-Objekte zuzugreifen.  So verf√ºgt das SparkContext-Objekt beispielsweise √ºber _jsc, mit dem Sie das interne SparkContext-Java-Objekt abrufen k√∂nnen.  Dies ist nur im Treiberprogramm m√∂glich. Wenn Sie also PySpark-Objekte an Arbeitsknoten senden, k√∂nnen Sie nicht auf die interne Java-Komponente zugreifen und der gr√∂√üte Teil der API funktioniert nicht. <br><br>  Um auf die Spark-Klasse in der JVM zuzugreifen, die keinen Python-Adapter hat, k√∂nnen Sie das Py4J-Gateway auf dem Treiber verwenden.  Das SparkContext-Objekt enth√§lt einen Link zum Gateway in der Eigenschaft _gateway.  Die Syntax sc._gateway.jvm. [Full_class_name_in_JVM] erm√∂glicht den Zugriff auf jedes Java-Objekt. <br><br>  Eine √§hnliche Technik funktioniert f√ºr Ihre eigenen Scala-Klassen, wenn sie gem√§√ü dem Klassenpfad angeordnet sind.  Sie k√∂nnen dem Klassenpfad JAR-Dateien hinzuf√ºgen, indem Sie den Befehl spark-submit mit dem Parameter --jars verwenden oder die Konfigurationseigenschaften spark.driver.extraClassPath festlegen.  Beispiel 7.6, das zur Erzeugung von Reis beitrug.  7.2 wurde absichtlich entwickelt, um Daten f√ºr Leistungstests unter Verwendung des vorhandenen Scala-Codes zu generieren. <br><br>  Beispiel 7.6  Aufrufen von Nicht-Spark-JVM-Klassen mit Py4J <br><br><pre> <code class="hljs pgsql">sc = sqlCtx._sc #  <span class="hljs-keyword"><span class="hljs-keyword">SQL</span></span> Context,   <span class="hljs-number"><span class="hljs-number">2.1</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>   , #  <span class="hljs-number"><span class="hljs-number">2.0</span></span>, ‚Äî  ,   :p try: try: javaSqlCtx = sqlCtx._jsqlContext <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._ssql_ctx <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: javaSqlCtx = sqlCtx._jwrapped jsc = sc._jsc scalasc = jsc.sc() gateway = sc._gateway #  java-,   RDD JVM- # <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> (<span class="hljs-type"><span class="hljs-type">Int</span></span>, <span class="hljs-type"><span class="hljs-type">Double</span></span>).   RDD  Python   #  RDD  Java (   <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>),   # ,      . #   Java-RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span> ‚Äî     #    DataFrame,     #    RDD  <span class="hljs-keyword"><span class="hljs-keyword">Row</span></span>. java_rdd = (gateway.jvm.com.highperformancespark.examples. tools.GenerateScalingData. generateMiniScaleRows(scalasc, <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span>, numCols)) #     <span class="hljs-type"><span class="hljs-type">JSON</span></span>     . #  Python-     Java-. schema = StructType([ StructField("zip", IntegerType()), StructField("fuzzyness", DoubleType())]) #   <span class="hljs-number"><span class="hljs-number">2.1</span></span> /  <span class="hljs-number"><span class="hljs-number">2.1</span></span> try: jschema = javaSqlCtx.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: jschema = sqlCtx._jsparkSession.parseDataType(<span class="hljs-keyword"><span class="hljs-keyword">schema</span></span>.json()) #  RDD (Java)  DataFrame (Java) java_dataframe = javaSqlCtx.createDataFrame(java_rdd, jschema) #  DataFrame (Java)  DataFrame (Python) python_dataframe = DataFrame(java_dataframe, sqlCtx) #  DataFrame (Python)   RDD pairRDD = python_dataframe.rdd.map(lambda <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>: (<span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-keyword"><span class="hljs-keyword">row</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (python_dataframe, pairRDD)</code> </pre> <br><br>  Obwohl viele Python-Klassen lediglich Adapter von Java-Objekten sind, k√∂nnen nicht alle Java-Objekte in Python-Objekte eingeschlossen und dann in Spark verwendet werden.  Beispielsweise werden Objekte in PySpark-RDD-Sets als serialisierte Zeichenfolgen dargestellt, die nur in Python-Code einfach analysiert werden k√∂nnen.  Gl√ºcklicherweise sind DataFrame-Objekte zwischen verschiedenen Programmiersprachen standardisiert. Wenn Sie also Ihre Daten in DataFrame-Sets konvertieren k√∂nnen, k√∂nnen Sie sie in Python-Objekte einbinden und sie entweder direkt als Python DataFrame verwenden oder einen Python DataFrame in eine RDD davon konvertieren gleiche Sprache. <br><br>  ¬ªWeitere Informationen zum Buch finden Sie auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Website des Herausgebers</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalt</a> <br>  ¬ª <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auszug</a> <br><br>  20% Rabatt auf Gutscheine f√ºr Sprayers - <b>Spark</b> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de414525/">https://habr.com/ru/post/de414525/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de414513/index.html">27 gro√üartige Open Source-Webentwicklungstools</a></li>
<li><a href="../de414515/index.html">Lektion zur Optimierung von Webanwendungsservern</a></li>
<li><a href="../de414517/index.html">Oxford-Wissenschaftler: Die Wahrscheinlichkeit, dass wir im vorhersehbaren Teil des Universums allein sind, ist viel h√∂her als Null</a></li>
<li><a href="../de414519/index.html">Wie kann man 15 Minuten Scrum-Meetings in ein volles Haus verwandeln?</a></li>
<li><a href="../de414523/index.html">Vergleich der Quadcopter DJI Mavic Pro und Mavic Air</a></li>
<li><a href="../de414527/index.html">Was uns auf Highload ++ Sibirien erwartet, au√üer auf bemalten B√§ren</a></li>
<li><a href="../de414531/index.html">Niemand wei√ü, was ab dem 1. Juli mit Online-Eink√§ufen passieren wird</a></li>
<li><a href="../de414535/index.html">Manchester: der Geburtsort von Mutlosigkeit, Post-Punk und zwei ber√ºhmten Fu√üballclubs</a></li>
<li><a href="../de414537/index.html">Wie wir eines der besten AR-Spiele der Welt ohne bezahlten Verkehr gemacht haben</a></li>
<li><a href="../de414539/index.html">Cyberkriminelle stehlen zunehmend personenbezogene Daten von Russen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>