<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§≤ üõÄüèø üçΩÔ∏è So erkennen Sie Bilder und Texte auf Ihrem Telefon mit ML Kit üê∂ üï£ ü§¥üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor zwei Jahren sagte Sundar Pichai, der Leiter von Google, dass das Unternehmen von Mobile-First zu AI-First wird und sich auf maschinelles Lernen ko...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So erkennen Sie Bilder und Texte auf Ihrem Telefon mit ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Vor zwei Jahren sagte Sundar Pichai, der Leiter von Google, dass das Unternehmen von Mobile-First zu AI-First wird und sich auf maschinelles Lernen konzentriert.  Ein Jahr sp√§ter wurde das Machine Learning Kit ver√∂ffentlicht - eine Reihe von Tools, mit denen Sie ML unter iOS und Android effektiv nutzen k√∂nnen. </p><br><p>  In den USA wird viel √ºber das ML-Kit gesprochen, aber auf Russisch gibt es fast keine Informationen.  Und da wir es f√ºr einige Aufgaben in Yandex.Money verwenden, habe ich beschlossen, meine Erfahrungen zu teilen und anhand von Beispielen zu zeigen, wie man es verwendet, um interessante Dinge zu tun. </p><br><p>  Mein Name ist Yura. Letztes Jahr habe ich im Yandex.Money-Team an einer mobilen Geldb√∂rse gearbeitet.  Wir werden √ºber maschinelles Lernen in Mobilger√§ten sprechen. </p><a name="habracut"></a><br><hr><br><p>  Hinweis  Redaktion: Dieser Beitrag ist eine Nacherz√§hlung des Berichts von Yuri Chechetkin ‚ÄûVom Handy zuerst zur KI zuerst‚Äú aus der Yandex.Money-Metapher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  Was ist das ML Kit? </h2><br><p>  Dies ist das mobile SDK von Google, mit dem maschinelles Lernen auf Android- und iOS-Ger√§ten problemlos verwendet werden kann.  Es ist nicht notwendig, ein Experte f√ºr ML oder k√ºnstliche Intelligenz zu sein, da Sie in wenigen Codezeilen sehr komplexe Dinge implementieren k√∂nnen.  Dar√ºber hinaus ist es nicht erforderlich zu wissen, wie neuronale Netze oder Modelloptimierungen funktionieren. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  Was kann das ML Kit? </h2><br><p>  Die Grundfunktionen sind recht breit.  Sie k√∂nnen beispielsweise Text und Gesichter erkennen, Objekte suchen und verfolgen, Beschriftungen f√ºr Bilder und Ihre eigenen Klassifizierungsmodelle erstellen, Barcodes und QR-Tags scannen. </p><br><p>  Wir haben die QR-Code-Erkennung bereits in der Yandex.Money-Anwendung verwendet. </p><br><p>  Es gibt auch ein ML-Kit </p><br><ol><li>  Wahrzeichenerkennung; </li><li>  Definition der Sprache, in der der Text geschrieben ist; </li><li>  √úbersetzung von Texten auf dem Ger√§t; </li><li>  Schnelle Antwort auf einen Brief oder eine Nachricht. </li></ol><br><p>  Neben einer Vielzahl von Methoden, die sofort einsatzbereit sind, werden benutzerdefinierte Modelle unterst√ºtzt, die praktisch endlose M√∂glichkeiten bieten. Sie k√∂nnen beispielsweise Schwarzwei√üfotos einf√§rben und farbig machen. </p><br><p>  Es ist wichtig, dass Sie hierf√ºr keine Dienste, APIs oder Backends verwenden m√ºssen.  Alles kann direkt auf dem Ger√§t ausgef√ºhrt werden, sodass wir keinen Benutzerverkehr laden, keine Fehler im Zusammenhang mit dem Netzwerk erhalten und keine F√§lle bearbeiten m√ºssen, z. B. mangelndes Internet, Verbindungsverlust usw.  Dar√ºber hinaus arbeitet es auf dem Ger√§t viel schneller als √ºber ein Netzwerk. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Texterkennung </h2><br><p>  <strong>Aufgabe: Bei einem Foto muss der Text in einem Rechteck eingekreist sein.</strong> </p><br><p>  Wir beginnen mit der Abh√§ngigkeit in Gradle.  Es reicht aus, eine Abh√§ngigkeit zu verbinden, und wir sind bereit zu arbeiten. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Es lohnt sich, Metadaten anzugeben, die besagen, dass das Modell beim Herunterladen der Anwendung vom Play Market auf das Ger√§t heruntergeladen wird.  Wenn Sie dies nicht tun und ohne Modell auf die API zugreifen, wird eine Fehlermeldung angezeigt, und das Modell muss im Hintergrund heruntergeladen werden.  Wenn Sie mehrere Modelle verwenden m√ºssen, ist es ratsam, diese durch Kommas getrennt anzugeben.  In unserem Beispiel verwenden wir das OCR-Modell, und der Name des Restes finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Nach der Projektkonfiguration m√ºssen Sie die Eingabewerte festlegen.  ML Kit funktioniert mit dem FirebaseVisionImage-Typ. Wir haben f√ºnf Methoden, deren Signatur ich unten geschrieben habe.  Sie konvertieren die √ºblichen Android- und Java-Typen in ML Kit-Typen, mit denen bequem gearbeitet werden kann. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Achten Sie auf die letzten beiden - sie arbeiten mit einem Array von Bytes und einem Bytepuffer, und wir m√ºssen Metadaten angeben, damit ML Kit versteht, wie man mit all dem umgeht.  Metadaten beschreiben tats√§chlich das Format, in diesem Fall die Breite und H√∂he, das Standardformat, IMAGE_FORMAT_NV21 und die Drehung. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Wenn die Eingabedaten erfasst werden, erstellen Sie einen Detektor, der den Text erkennt. </p><br><p>  Es gibt zwei Arten von Detektoren: Auf dem Ger√§t und in der Cloud werden sie buchst√§blich in einer Zeile erstellt.  Es ist erw√§hnenswert, dass der Detektor am Ger√§t nur mit Englisch funktioniert.  Der Cloud-Detektor unterst√ºtzt mehr als 20 Sprachen, die in der speziellen setLanguageHints-Methode angegeben werden m√ºssen. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  Die Anzahl der unterst√ºtzten Sprachen betr√§gt mehr als 20, sie befinden sich alle auf der offiziellen Website.  In unserem Beispiel nur Englisch und Russisch. </p><br><p>  Nachdem Sie eine Eingabe und einen Detektor eingegeben haben, rufen Sie einfach die processImage-Methode f√ºr diesen Detektor auf.  Wir erhalten das Ergebnis in Form einer Aufgabe, an die wir zwei R√ºckrufe h√§ngen - f√ºr Erfolg und f√ºr Fehler.  Bei der Standardausnahme tritt ein Fehler auf, und der FirebaseVisionText-Typ wird von onSuccessListener erfolgreich ausgef√ºhrt. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  Wie arbeite ich mit dem Typ FirebaseVisionText? </h2><br><p>  Es besteht aus Textbl√∂cken (TextBlock), die wiederum aus Linien (Linie) und Linien von Elementen (Element) bestehen.  Sie sind ineinander verschachtelt. </p><br><p>  Dar√ºber hinaus verf√ºgt jede dieser Klassen √ºber f√ºnf Methoden, die unterschiedliche Daten zum Objekt zur√ºckgeben.  Ein Rechteck ist der Bereich, in dem sich der Text befindet. Vertrauen ist die Genauigkeit des erkannten Textes. Eckpunkte sind die Eckpunkte im Uhrzeigersinn, beginnend mit der oberen linken Ecke, den erkannten Sprachen und dem Text selbst. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  Wof√ºr ist das? </h2><br><p>  Wir k√∂nnen sowohl den gesamten Text im Bild als auch die einzelnen Abs√§tze, Teile, Zeilen oder nur W√∂rter erkennen.  Als Beispiel k√∂nnen wir wiederholen, in jeder Phase einen Text nehmen, die R√§nder dieses Textes nehmen und zeichnen.  Sehr bequem. </p><br><p>  Wir planen, dieses Tool in unserer Anwendung zum Erkennen von Bankkarten zu verwenden, deren Etiketten nicht dem Standard entsprechen.  Nicht alle Kartenerkennungsbibliotheken funktionieren gut, und f√ºr benutzerdefinierte Karten w√§re das ML-Kit sehr n√ºtzlich.  Da wenig Text vorhanden ist, ist die Verarbeitung auf diese Weise sehr einfach. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Erkennung von Objekten auf dem Foto </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Am Beispiel des folgenden Tools m√∂chte ich zeigen, dass das Funktionsprinzip ungef√§hr gleich ist.  In diesem Fall Erkennung dessen, was auf dem Objekt dargestellt ist.  Wir erstellen auch zwei Detektoren, einen auf dem Ger√§t, den anderen in der Cloud. Wir k√∂nnen die Mindestgenauigkeit als Parameter angeben.  Der Standardwert ist 0,5, angegeben 0,7 und einsatzbereit.  Wir erhalten das Ergebnis auch in Form von FirebaseImageLabel. Dies ist eine Liste von Labels, von denen jedes eine ID, eine Beschreibung und eine Genauigkeit enth√§lt. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold versteckt das Gl√ºck </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Sie k√∂nnen versuchen zu verstehen, wie gut Harold den Schmerz verbirgt und ob er gleichzeitig gl√ºcklich ist.  Wir verwenden ein Gesichtserkennungswerkzeug, das neben der Erkennung von Gesichtsz√ºgen auch erkennen kann, wie gl√ºcklich eine Person ist.  Wie sich herausstellte, ist Harold zu 93% gl√ºcklich.  Oder er versteckt den Schmerz sehr gut. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  Von leicht zu leicht, aber etwas komplizierter.  Kundenspezifische Modelle. </h2><br><p>  <strong>Aufgabe: Klassifizierung dessen, was auf dem Foto abgebildet ist.</strong> </p><br><p>  Ich machte ein Foto vom Laptop und erkannte das Modem, den Desktop-Computer und die Tastatur.  Klingt nach der Wahrheit.  Es gibt tausend Klassifikatoren, von denen er drei nimmt, die dieses Foto am besten beschreiben. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  Wenn Sie mit benutzerdefinierten Modellen arbeiten, k√∂nnen wir auch mit ihnen sowohl auf dem Ger√§t als auch √ºber die Cloud arbeiten. </p><br><p>  Wenn wir √ºber die Cloud arbeiten, m√ºssen Sie zur Firebase-Konsole, zur Registerkarte ML Kit und zum benutzerdefinierten Tippen gehen, wo wir unser Modell auf TensorFlow Lite hochladen k√∂nnen, da ML Kit mit Modellen mit dieser Aufl√∂sung funktioniert.  Wenn wir es auf einem Ger√§t verwenden, k√∂nnen wir das Modell einfach als Asset in einen beliebigen Teil des Projekts einf√ºgen. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  Wir weisen auf die Abh√§ngigkeit vom Dolmetscher hin, der mit benutzerdefinierten Modellen arbeiten kann, und vergessen nicht die Erlaubnis, mit dem Internet zu arbeiten. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  F√ºr die Modelle auf dem Ger√§t m√ºssen Sie in Gradle angeben, dass das Modell nicht komprimiert werden soll, da es verzerrt werden kann. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Wenn wir alles in unserer Umgebung konfiguriert haben, m√ºssen wir spezielle Bedingungen festlegen, zu denen beispielsweise die Verwendung von Wi-Fi geh√∂rt, die ebenfalls aufgeladen werden m√ºssen und bei Android N Ger√§te im Leerlauf verf√ºgbar sind. Diese Bedingungen zeigen an, dass das Telefon aufgeladen wird oder sich im Standby-Modus befindet. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Wenn wir ein Remote-Modell erstellen, legen wir die Initialisierungs- und Aktualisierungsbedingungen sowie das Flag fest, ob unser Modell aktualisiert werden soll.  Der Modellname sollte mit dem in der Firebase-Konsole angegebenen √ºbereinstimmen.  Wenn wir das Remote-Modell erstellt haben, m√ºssen wir es im Firebase Model Manager registrieren. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Wir f√ºhren die gleichen Schritte f√ºr das lokale Modell aus, geben dessen Namen und den Pfad zum Modell an und registrieren es im Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Danach m√ºssen Sie solche Optionen erstellen, in denen wir die Namen unserer Modelle angeben, das Remote-Modell installieren, das lokale Modell installieren und einen Interpreter mit diesen Optionen erstellen.  Wir k√∂nnen entweder ein entferntes Modell oder nur ein lokales Modell angeben, und der Interpreter wird selbst verstehen, mit welchem ‚Äã‚ÄãModell er arbeiten soll. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  ML Kit wei√ü nichts √ºber das Format der Eingabe- und Ausgabedaten von benutzerdefinierten Modellen, daher m√ºssen Sie diese angeben. </p><br><p>  Eingabedaten sind ein mehrdimensionales Array, wobei 1 die Anzahl der Bilder, 224 x 224 die Aufl√∂sung und 3 ein dreikanaliges RGB-Bild ist.  Nun, der Datentyp ist Bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Die Ausgabewerte sind 1000 Klassifikatoren.  Wir legen das Format der Eingabe- und Ausgabewerte in Bytes mit den angegebenen mehrdimensionalen Arrays fest.  Neben Bytes sind auch float, long, int verf√ºgbar. </p><br><p>  Jetzt setzen wir die Eingabewerte.  Wir nehmen Bitmap, komprimieren es auf 224 mal 224, konvertieren es in ByteBuffer und erstellen Eingabewerte mit FirebaseModelInput mit einem speziellen Builder. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  Und jetzt, wenn es einen Interpreter gibt, das Format der Eingabe- und Ausgabewerte und die Eingabewerte selbst, k√∂nnen wir die Anforderung mit der Ausf√ºhrungsmethode ausf√ºhren.  Wir √ºbertragen alle oben genannten Elemente als Parameter und erhalten als Ergebnis FirebaseModelOutput, das ein Generikum des von uns angegebenen Typs enth√§lt.  In diesem Fall war es ein Byte-Array, nach dem wir mit der Verarbeitung beginnen k√∂nnen.  Dies sind genau die tausend Klassifikatoren, die wir angefordert haben, und wir zeigen zum Beispiel die Top 3 an, die am besten geeignet sind. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Eint√§gige Implementierung </h2><br><p>  Alles ist recht einfach zu implementieren und die Erkennung von Objekten mit integrierten Werkzeugen kann in nur einem Tag realisiert werden.  Das Tool ist f√ºr iOS und Android verf√ºgbar. Au√üerdem k√∂nnen Sie f√ºr beide Plattformen dasselbe TensorFlow-Modell verwenden. </p><br><p>  Dar√ºber hinaus stehen unz√§hlige Methoden zur Verf√ºgung, die viele F√§lle abdecken k√∂nnen.  Die meisten APIs sind auf dem Ger√§t verf√ºgbar, dh die Erkennung funktioniert auch ohne Internet. </p><br><p>  Und vor allem - Unterst√ºtzung f√ºr benutzerdefinierte Modelle, die f√ºr jede Aufgabe nach Belieben verwendet werden k√∂nnen. </p><br><h2 id="poleznye-ssylki">  N√ºtzliche Links </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ML Kit Dokumentation</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github ML Kit Demo-Projekt</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Maschinelles Lernen f√ºr Handys mit Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Maschinelles Lernen SDK f√ºr mobile Entwickler (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Kreditkartenscanners mit dem Firebase ML Kit (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461867/">https://habr.com/ru/post/de461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461851/index.html">Blockchain und Elektrizit√§t</a></li>
<li><a href="../de461855/index.html">IT-Geh√§lter im ersten Halbjahr 2019: Laut Gehaltsrechner ‚ÄûMy Circle‚Äú</a></li>
<li><a href="../de461859/index.html">Sie wissen nichts √ºber Lebensmitteltechnologie</a></li>
<li><a href="../de461861/index.html">Office 365 Cloud-Sicherheit: Check Point CloudGuard SaaS-Test</a></li>
<li><a href="../de461865/index.html">Videokurs ‚ÄûEinf√ºhrung in das Umkehren von Grund auf mit IDA PRO. Kapitel 1</a></li>
<li><a href="../de461871/index.html">101 Tipps, um ein guter Programmierer (und Mensch) zu werden</a></li>
<li><a href="../de461873/index.html">ViewPager 2 - neue Funktionalit√§t im alten Wrapper</a></li>
<li><a href="../de461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../de461877/index.html">Java vs Kotlin f√ºr Android: Meinungen von Entwicklern</a></li>
<li><a href="../de461879/index.html">Das Buch "Linux in Aktion"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>