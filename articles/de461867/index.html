<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤲 🛀🏿 🍽️ So erkennen Sie Bilder und Texte auf Ihrem Telefon mit ML Kit 🐶 🕣 🤴🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor zwei Jahren sagte Sundar Pichai, der Leiter von Google, dass das Unternehmen von Mobile-First zu AI-First wird und sich auf maschinelles Lernen ko...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>So erkennen Sie Bilder und Texte auf Ihrem Telefon mit ML Kit</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yamoney/blog/461867/"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Vor zwei Jahren sagte Sundar Pichai, der Leiter von Google, dass das Unternehmen von Mobile-First zu AI-First wird und sich auf maschinelles Lernen konzentriert.  Ein Jahr später wurde das Machine Learning Kit veröffentlicht - eine Reihe von Tools, mit denen Sie ML unter iOS und Android effektiv nutzen können. </p><br><p>  In den USA wird viel über das ML-Kit gesprochen, aber auf Russisch gibt es fast keine Informationen.  Und da wir es für einige Aufgaben in Yandex.Money verwenden, habe ich beschlossen, meine Erfahrungen zu teilen und anhand von Beispielen zu zeigen, wie man es verwendet, um interessante Dinge zu tun. </p><br><p>  Mein Name ist Yura. Letztes Jahr habe ich im Yandex.Money-Team an einer mobilen Geldbörse gearbeitet.  Wir werden über maschinelles Lernen in Mobilgeräten sprechen. </p><a name="habracut"></a><br><hr><br><p>  Hinweis  Redaktion: Dieser Beitrag ist eine Nacherzählung des Berichts von Yuri Chechetkin „Vom Handy zuerst zur KI zuerst“ aus der Yandex.Money-Metapher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  Was ist das ML Kit? </h2><br><p>  Dies ist das mobile SDK von Google, mit dem maschinelles Lernen auf Android- und iOS-Geräten problemlos verwendet werden kann.  Es ist nicht notwendig, ein Experte für ML oder künstliche Intelligenz zu sein, da Sie in wenigen Codezeilen sehr komplexe Dinge implementieren können.  Darüber hinaus ist es nicht erforderlich zu wissen, wie neuronale Netze oder Modelloptimierungen funktionieren. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  Was kann das ML Kit? </h2><br><p>  Die Grundfunktionen sind recht breit.  Sie können beispielsweise Text und Gesichter erkennen, Objekte suchen und verfolgen, Beschriftungen für Bilder und Ihre eigenen Klassifizierungsmodelle erstellen, Barcodes und QR-Tags scannen. </p><br><p>  Wir haben die QR-Code-Erkennung bereits in der Yandex.Money-Anwendung verwendet. </p><br><p>  Es gibt auch ein ML-Kit </p><br><ol><li>  Wahrzeichenerkennung; </li><li>  Definition der Sprache, in der der Text geschrieben ist; </li><li>  Übersetzung von Texten auf dem Gerät; </li><li>  Schnelle Antwort auf einen Brief oder eine Nachricht. </li></ol><br><p>  Neben einer Vielzahl von Methoden, die sofort einsatzbereit sind, werden benutzerdefinierte Modelle unterstützt, die praktisch endlose Möglichkeiten bieten. Sie können beispielsweise Schwarzweißfotos einfärben und farbig machen. </p><br><p>  Es ist wichtig, dass Sie hierfür keine Dienste, APIs oder Backends verwenden müssen.  Alles kann direkt auf dem Gerät ausgeführt werden, sodass wir keinen Benutzerverkehr laden, keine Fehler im Zusammenhang mit dem Netzwerk erhalten und keine Fälle bearbeiten müssen, z. B. mangelndes Internet, Verbindungsverlust usw.  Darüber hinaus arbeitet es auf dem Gerät viel schneller als über ein Netzwerk. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="1"></p><br><h2 id="raspoznavanie-teksta">  Texterkennung </h2><br><p>  <strong>Aufgabe: Bei einem Foto muss der Text in einem Rechteck eingekreist sein.</strong> </p><br><p>  Wir beginnen mit der Abhängigkeit in Gradle.  Es reicht aus, eine Abhängigkeit zu verbinden, und wir sind bereit zu arbeiten. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  Es lohnt sich, Metadaten anzugeben, die besagen, dass das Modell beim Herunterladen der Anwendung vom Play Market auf das Gerät heruntergeladen wird.  Wenn Sie dies nicht tun und ohne Modell auf die API zugreifen, wird eine Fehlermeldung angezeigt, und das Modell muss im Hintergrund heruntergeladen werden.  Wenn Sie mehrere Modelle verwenden müssen, ist es ratsam, diese durch Kommas getrennt anzugeben.  In unserem Beispiel verwenden wir das OCR-Modell, und der Name des Restes finden Sie in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  Nach der Projektkonfiguration müssen Sie die Eingabewerte festlegen.  ML Kit funktioniert mit dem FirebaseVisionImage-Typ. Wir haben fünf Methoden, deren Signatur ich unten geschrieben habe.  Sie konvertieren die üblichen Android- und Java-Typen in ML Kit-Typen, mit denen bequem gearbeitet werden kann. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Achten Sie auf die letzten beiden - sie arbeiten mit einem Array von Bytes und einem Bytepuffer, und wir müssen Metadaten angeben, damit ML Kit versteht, wie man mit all dem umgeht.  Metadaten beschreiben tatsächlich das Format, in diesem Fall die Breite und Höhe, das Standardformat, IMAGE_FORMAT_NV21 und die Drehung. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  Wenn die Eingabedaten erfasst werden, erstellen Sie einen Detektor, der den Text erkennt. </p><br><p>  Es gibt zwei Arten von Detektoren: Auf dem Gerät und in der Cloud werden sie buchstäblich in einer Zeile erstellt.  Es ist erwähnenswert, dass der Detektor am Gerät nur mit Englisch funktioniert.  Der Cloud-Detektor unterstützt mehr als 20 Sprachen, die in der speziellen setLanguageHints-Methode angegeben werden müssen. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  Die Anzahl der unterstützten Sprachen beträgt mehr als 20, sie befinden sich alle auf der offiziellen Website.  In unserem Beispiel nur Englisch und Russisch. </p><br><p>  Nachdem Sie eine Eingabe und einen Detektor eingegeben haben, rufen Sie einfach die processImage-Methode für diesen Detektor auf.  Wir erhalten das Ergebnis in Form einer Aufgabe, an die wir zwei Rückrufe hängen - für Erfolg und für Fehler.  Bei der Standardausnahme tritt ein Fehler auf, und der FirebaseVisionText-Typ wird von onSuccessListener erfolgreich ausgeführt. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  Wie arbeite ich mit dem Typ FirebaseVisionText? </h2><br><p>  Es besteht aus Textblöcken (TextBlock), die wiederum aus Linien (Linie) und Linien von Elementen (Element) bestehen.  Sie sind ineinander verschachtelt. </p><br><p>  Darüber hinaus verfügt jede dieser Klassen über fünf Methoden, die unterschiedliche Daten zum Objekt zurückgeben.  Ein Rechteck ist der Bereich, in dem sich der Text befindet. Vertrauen ist die Genauigkeit des erkannten Textes. Eckpunkte sind die Eckpunkte im Uhrzeigersinn, beginnend mit der oberen linken Ecke, den erkannten Sprachen und dem Text selbst. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  Wofür ist das? </h2><br><p>  Wir können sowohl den gesamten Text im Bild als auch die einzelnen Absätze, Teile, Zeilen oder nur Wörter erkennen.  Als Beispiel können wir wiederholen, in jeder Phase einen Text nehmen, die Ränder dieses Textes nehmen und zeichnen.  Sehr bequem. </p><br><p>  Wir planen, dieses Tool in unserer Anwendung zum Erkennen von Bankkarten zu verwenden, deren Etiketten nicht dem Standard entsprechen.  Nicht alle Kartenerkennungsbibliotheken funktionieren gut, und für benutzerdefinierte Karten wäre das ML-Kit sehr nützlich.  Da wenig Text vorhanden ist, ist die Verarbeitung auf diese Weise sehr einfach. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Erkennung von Objekten auf dem Foto </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Am Beispiel des folgenden Tools möchte ich zeigen, dass das Funktionsprinzip ungefähr gleich ist.  In diesem Fall Erkennung dessen, was auf dem Objekt dargestellt ist.  Wir erstellen auch zwei Detektoren, einen auf dem Gerät, den anderen in der Cloud. Wir können die Mindestgenauigkeit als Parameter angeben.  Der Standardwert ist 0,5, angegeben 0,7 und einsatzbereit.  Wir erhalten das Ergebnis auch in Form von FirebaseImageLabel. Dies ist eine Liste von Labels, von denen jedes eine ID, eine Beschreibung und eine Genauigkeit enthält. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold versteckt das Glück </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  Sie können versuchen zu verstehen, wie gut Harold den Schmerz verbirgt und ob er gleichzeitig glücklich ist.  Wir verwenden ein Gesichtserkennungswerkzeug, das neben der Erkennung von Gesichtszügen auch erkennen kann, wie glücklich eine Person ist.  Wie sich herausstellte, ist Harold zu 93% glücklich.  Oder er versteckt den Schmerz sehr gut. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="4"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  Von leicht zu leicht, aber etwas komplizierter.  Kundenspezifische Modelle. </h2><br><p>  <strong>Aufgabe: Klassifizierung dessen, was auf dem Foto abgebildet ist.</strong> </p><br><p>  Ich machte ein Foto vom Laptop und erkannte das Modem, den Desktop-Computer und die Tastatur.  Klingt nach der Wahrheit.  Es gibt tausend Klassifikatoren, von denen er drei nimmt, die dieses Foto am besten beschreiben. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  Wenn Sie mit benutzerdefinierten Modellen arbeiten, können wir auch mit ihnen sowohl auf dem Gerät als auch über die Cloud arbeiten. </p><br><p>  Wenn wir über die Cloud arbeiten, müssen Sie zur Firebase-Konsole, zur Registerkarte ML Kit und zum benutzerdefinierten Tippen gehen, wo wir unser Modell auf TensorFlow Lite hochladen können, da ML Kit mit Modellen mit dieser Auflösung funktioniert.  Wenn wir es auf einem Gerät verwenden, können wir das Modell einfach als Asset in einen beliebigen Teil des Projekts einfügen. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  Wir weisen auf die Abhängigkeit vom Dolmetscher hin, der mit benutzerdefinierten Modellen arbeiten kann, und vergessen nicht die Erlaubnis, mit dem Internet zu arbeiten. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  Für die Modelle auf dem Gerät müssen Sie in Gradle angeben, dass das Modell nicht komprimiert werden soll, da es verzerrt werden kann. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  Wenn wir alles in unserer Umgebung konfiguriert haben, müssen wir spezielle Bedingungen festlegen, zu denen beispielsweise die Verwendung von Wi-Fi gehört, die ebenfalls aufgeladen werden müssen und bei Android N Geräte im Leerlauf verfügbar sind. Diese Bedingungen zeigen an, dass das Telefon aufgeladen wird oder sich im Standby-Modus befindet. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  Wenn wir ein Remote-Modell erstellen, legen wir die Initialisierungs- und Aktualisierungsbedingungen sowie das Flag fest, ob unser Modell aktualisiert werden soll.  Der Modellname sollte mit dem in der Firebase-Konsole angegebenen übereinstimmen.  Wenn wir das Remote-Modell erstellt haben, müssen wir es im Firebase Model Manager registrieren. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  Wir führen die gleichen Schritte für das lokale Modell aus, geben dessen Namen und den Pfad zum Modell an und registrieren es im Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  Danach müssen Sie solche Optionen erstellen, in denen wir die Namen unserer Modelle angeben, das Remote-Modell installieren, das lokale Modell installieren und einen Interpreter mit diesen Optionen erstellen.  Wir können entweder ein entferntes Modell oder nur ein lokales Modell angeben, und der Interpreter wird selbst verstehen, mit welchem ​​Modell er arbeiten soll. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  ML Kit weiß nichts über das Format der Eingabe- und Ausgabedaten von benutzerdefinierten Modellen, daher müssen Sie diese angeben. </p><br><p>  Eingabedaten sind ein mehrdimensionales Array, wobei 1 die Anzahl der Bilder, 224 x 224 die Auflösung und 3 ein dreikanaliges RGB-Bild ist.  Nun, der Datentyp ist Bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  Die Ausgabewerte sind 1000 Klassifikatoren.  Wir legen das Format der Eingabe- und Ausgabewerte in Bytes mit den angegebenen mehrdimensionalen Arrays fest.  Neben Bytes sind auch float, long, int verfügbar. </p><br><p>  Jetzt setzen wir die Eingabewerte.  Wir nehmen Bitmap, komprimieren es auf 224 mal 224, konvertieren es in ByteBuffer und erstellen Eingabewerte mit FirebaseModelInput mit einem speziellen Builder. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  Und jetzt, wenn es einen Interpreter gibt, das Format der Eingabe- und Ausgabewerte und die Eingabewerte selbst, können wir die Anforderung mit der Ausführungsmethode ausführen.  Wir übertragen alle oben genannten Elemente als Parameter und erhalten als Ergebnis FirebaseModelOutput, das ein Generikum des von uns angegebenen Typs enthält.  In diesem Fall war es ein Byte-Array, nach dem wir mit der Verarbeitung beginnen können.  Dies sind genau die tausend Klassifikatoren, die wir angefordert haben, und wir zeigen zum Beispiel die Top 3 an, die am besten geeignet sind. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  Eintägige Implementierung </h2><br><p>  Alles ist recht einfach zu implementieren und die Erkennung von Objekten mit integrierten Werkzeugen kann in nur einem Tag realisiert werden.  Das Tool ist für iOS und Android verfügbar. Außerdem können Sie für beide Plattformen dasselbe TensorFlow-Modell verwenden. </p><br><p>  Darüber hinaus stehen unzählige Methoden zur Verfügung, die viele Fälle abdecken können.  Die meisten APIs sind auf dem Gerät verfügbar, dh die Erkennung funktioniert auch ohne Internet. </p><br><p>  Und vor allem - Unterstützung für benutzerdefinierte Modelle, die für jede Aufgabe nach Belieben verwendet werden können. </p><br><h2 id="poleznye-ssylki">  Nützliche Links </h2><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ML Kit Dokumentation</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Github ML Kit Demo-Projekt</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Maschinelles Lernen für Handys mit Firebase (Google I / O'19)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Maschinelles Lernen SDK für mobile Entwickler (Google I / O'18)</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen eines Kreditkartenscanners mit dem Firebase ML Kit (Medium.com)</a> </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461867/">https://habr.com/ru/post/de461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461851/index.html">Blockchain und Elektrizität</a></li>
<li><a href="../de461855/index.html">IT-Gehälter im ersten Halbjahr 2019: Laut Gehaltsrechner „My Circle“</a></li>
<li><a href="../de461859/index.html">Sie wissen nichts über Lebensmitteltechnologie</a></li>
<li><a href="../de461861/index.html">Office 365 Cloud-Sicherheit: Check Point CloudGuard SaaS-Test</a></li>
<li><a href="../de461865/index.html">Videokurs „Einführung in das Umkehren von Grund auf mit IDA PRO. Kapitel 1</a></li>
<li><a href="../de461871/index.html">101 Tipps, um ein guter Programmierer (und Mensch) zu werden</a></li>
<li><a href="../de461873/index.html">ViewPager 2 - neue Funktionalität im alten Wrapper</a></li>
<li><a href="../de461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../de461877/index.html">Java vs Kotlin für Android: Meinungen von Entwicklern</a></li>
<li><a href="../de461879/index.html">Das Buch "Linux in Aktion"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>