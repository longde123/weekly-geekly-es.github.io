<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👩🏽‍🤝‍👩🏼 👩🏽‍🎓 🥙 Reconnaissance d'objets et d'émotions humaines à l'aide du kit Firebase ML 🎅🏼 🍃 🏁</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Si vous avez suivi Google I / O (ou du moins regardé Keynotes), vous avez peut-être remarqué l'annonce d'un nouveau produit dans le cadre de la plate-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconnaissance d'objets et d'émotions humaines à l'aide du kit Firebase ML</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420085/"><p><img src="https://habrastorage.org/webt/6n/o3/xk/6no3xkgvsfzp6mlhr5ctmsuuy0c.jpeg" alt="Reconnaissance d'objets et d'émotions humaines à l'aide du kit Firebase ML"></p><br><p>  Si vous avez suivi Google I / O (ou du moins regardé Keynotes), vous avez peut-être remarqué l'annonce d'un nouveau produit dans le cadre de la plate-forme Firebase appelée ML Kit. </p><br><p>  ML Kit fournit une API avec laquelle vous pouvez ajouter de puissantes fonctions d'apprentissage automatique aux applications (Android et iOS), que vous soyez un développeur expérimenté en apprentissage automatique ou simplement un débutant dans ce domaine. </p><a name="habracut"></a><br><p>  Bien que ce produit n'ait pas retenu l'attention lors de la conférence (merci, Google Duplex), il existe certainement de nombreuses façons utiles de l'utiliser dans le développement Android. </p><br><p>  Jouons donc avec lui et créons une petite application qui ressemblera (presque) à Google Lens! </p><br><p>  Voici quelques captures d'écran de l'application.  Sur eux, vous pouvez voir une tentative d'identification des objets dans l'image. </p><br><p><img src="https://habrastorage.org/webt/wr/fu/wx/wrfuwxigtg5o73nuhkdjl5spvt4.png" alt="Reconnaissance des écouteurs 1"></p><br><p><img src="https://habrastorage.org/webt/kf/be/cl/kfbeclb_ibimfqxu9kmkwgzalxg.png" alt="Reconnaissance des écouteurs 2"></p><br><p>  Assez précis, hein? <br>  Vous pouvez également utiliser cette API pour définir des émotions humaines telles que le bonheur, la tristesse, la colère, etc., ce qui est encore plus cool. </p><br><h3 id="hvatit-boltovni-pokazhite-mne-kod">  Arrête de parler, montre moi le code !!!!! </h3><br><p>  Il y a 5 API dans le kit ML: </p><br><ol><li>  Reconnaissance de texte (nous avons déjà publié <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un article</a> sur l'application utilisant cette fonctionnalité) </li><li>  Détection des visages ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un</a> tel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">article</a> est sur notre blog) </li><li>  Numérisation de codes à barres </li><li>  Identification des objets dans l'image (celle que nous allons utiliser) </li><li>  Reconnaissance des caractères </li></ol><br><p>  Dans cet article, nous utiliserons l'API d'identification d'objet sur l'image.  En utilisant cette API, nous obtenons une liste d'objets qui ont été reconnus dans l'image: personnes, choses, lieux, activités, etc. </p><br><p>  De plus, il existe 2 types de cette API.  Le premier est l' <strong>API intégrée à votre appareil</strong> qui fonctionne, ce qui est logique, sur l'appareil lui-même.  Il est gratuit et peut reconnaître plus de 400 objets différents dans les images. </p><br><p>  La seconde est l' <strong>API cloud</strong> , qui s'exécute sur Google Cloud et reconnaît plus de 10 000 objets différents.  Il est payé, mais les 1000 premières demandes par mois sont gratuites. </p><br><p>  Dans cet article, nous examinerons le premier type d'API, comme  c'est gratuit (mais le principe du payant est similaire à gratuit). </p><br><p>  Commençons. </p><br><ul><li> <strong>Connectez Firebase à votre projet et ajoutez la</strong> <code>firebase-ml-vision</code> <br>  Comment connecter Firebase, vous pouvez le voir dans un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bon tutoriel</a> de Google.  Vous devez également ajouter les dépendances appropriées pour utiliser cette API: </li></ul><br><pre> <code class="hljs delphi"><span class="hljs-keyword"><span class="hljs-keyword">implementation</span></span> <span class="hljs-string"><span class="hljs-string">'com.google.firebase:firebase-ml-vision:15.0.0'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">implementation</span></span> <span class="hljs-string"><span class="hljs-string">'com.google.firebase:firebase-ml-vision-image-label-model:15.0.0'</span></span></code> </pre> <br><ul><li><p>  <strong>Intégrer la fonction appareil photo dans l'application</strong> <br>  L'API Vision a besoin d'une image pour recevoir des données, alors créez une application qui vous permet de télécharger des images de la galerie ou créez une application qui utilise une caméra pour prendre une image et l'analyser instantanément. <br>  Si vous ne souhaitez pas utiliser l'API de caméra standard, vous pouvez simplement utiliser la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bibliothèque pratique et simple</a> pour cette fonctionnalité. </p><br></li><li><p>  <strong>Utiliser un bitmap pour accéder à l'API Vision</strong> <br>  La bibliothèque mentionnée ci-dessus fournit directement l'image bitmap, qui peut être utilisée pour accéder à l'API. </p><br></li></ul><br><pre> <code class="hljs pgsql">fab_take_photo.setOnClickListener { // cameraView <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> a custom <span class="hljs-keyword"><span class="hljs-keyword">View</span></span> which provides camera preview cameraView.captureImage { cameraKitImage -&gt; // <span class="hljs-keyword"><span class="hljs-keyword">Get</span></span> the Bitmap <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> the captured shot <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> use it <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> make the API <span class="hljs-keyword"><span class="hljs-keyword">call</span></span> getLabelsFromDevice(cameraKitImage.bitmap) } } private fun getLabelsFromDevice(bitmap: Bitmap) { val image : FirebaseVisionImage = FirebaseVisionImage.fromBitmap(bitmap) val detector : FirebaseVisionLabelDetector = FirebaseVision.getInstance().visionLabelDetector detector.detectInImage(image) .addOnSuccessListener { // Task completed successfully <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(firebaseVision : FirebaseVisionLabel <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> it){ // Logging through the list <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> labels returned <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> the API <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Log</span></span> them <span class="hljs-keyword"><span class="hljs-keyword">Log</span></span>.d(TAG,"Item Name ${firebaseVision.confidence}") <span class="hljs-keyword"><span class="hljs-keyword">Log</span></span>.d(TAG,"Confidence ${firebaseVision.confidence}") } } .addOnFailureListener { // Task failed <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> an <span class="hljs-keyword"><span class="hljs-keyword">exception</span></span> Toast.makeText(baseContext,"Sorry, something went wrong!",Toast.LENGTH_SHORT).<span class="hljs-keyword"><span class="hljs-keyword">show</span></span>() } }</code> </pre> <br><p>  Dans l'extrait de code ci-dessus, nous créons d'abord un <code>FirebaseVisionImage</code> partir du bitmap. </p><br><p>  Ensuite, nous créons une instance de <code>FirebaseVisionLabelDetector</code> qui passe par <code>FirebaseVisionImage</code> et trouve les <code>FirebaseVisionLabels</code> (objets) spécifiques qu'il reconnaît dans l'image fournie. </p><br><p>  Enfin, nous passons l'image à la méthode <code>detectInImage()</code> et laissons le détecteur analyser l'image. </p><br><p>  Nous pouvons configurer les auditeurs pour gérer une analyse réussie et pour une échec.  Là, nous aurons accès à la liste des objets identifiés dans l'image et à l'exception qui s'est produite, respectivement. </p><br><p>  Pour chaque objet reconnu, vous pouvez obtenir son <strong>nom, sa précision de reconnaissance et son identifiant d'entité</strong> . </p><br><p>  Comme mentionné précédemment, cette API peut également être utilisée pour définir les émotions humaines dans une image, ce qui peut être vu dans les captures d'écran ci-dessous: </p><br><p><img src="https://habrastorage.org/webt/fu/jr/qq/fujrqqrmmpd-gx9puoir9z8_0ni.png" alt="Émotions humaines"></p><br><p><img src="https://habrastorage.org/webt/i8/rg/ss/i8rgss-9xmow9qblquay2o2kgpw.png" alt="Reconnaissance des émotions 1"></p><br><p><img src="https://habrastorage.org/webt/sc/j9/ah/scj9ahvdfphvfa4qouhqsuwbqb0.png" alt="Reconnaissance des émotions 2"></p><br><p>  Le code de l' <strong>API cloud est</strong> très similaire au code que nous avons écrit pour <strong>l'API</strong> de <strong>l'appareil</strong> .  Seuls le <strong>type de détecteur</strong> ( <code>FirebaseVisionCloudLabelDetector</code> vs <code>FirebaseVisionLabelDetector</code> ) et le <strong>type d'objets identifiés</strong> ( <code>FirebaseVisionCloudLabel</code> vs <code>FirebaseVisionLabels</code> ) sont <code>FirebaseVisionLabels</code> : </p><br><pre> <code class="hljs kotlin"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getLabelsFromDevice</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span> { ... <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> detector : FirebaseVisionCloudLabelDetector = FirebaseVision.getInstance().visionCloudLabelDetector detector.detectInImage(image) .addOnSuccessListener { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(firebaseVision : FirebaseVisionCloudLabel <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> it){ ... } } .addOnFailureListener { ... } }</code> </pre> <br><p>  Outre les modifications du code, vous devez également configurer la facturation (paiement) pour votre projet et activer l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API Google Vision</a> dans votre Google Cloud Console. </p><br><p>  Veuillez noter que l'API vous permet d'effectuer seulement <strong>1000 requêtes gratuites</strong> par mois, vous n'avez donc pas besoin de payer si vous voulez simplement jouer avec. </p><br><p>  L'application présentée dans les captures d'écran se trouve sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">référentiel GitHub</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420085/">https://habr.com/ru/post/fr420085/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420075/index.html">Cette ingénieuse carte explique comment tout se combine en physique.</a></li>
<li><a href="../fr420077/index.html">Nouvelles opportunités de trading en ligne</a></li>
<li><a href="../fr420079/index.html">Création d'herbe interactive dans Unreal Engine</a></li>
<li><a href="../fr420081/index.html">Installez Archlinux avec le chiffrement complet du système et LVM sur LUKS</a></li>
<li><a href="../fr420083/index.html">Action idéale pour Google Assistant - 8 leçons du Hackathon de Moscou</a></li>
<li><a href="../fr420087/index.html">Il s'agit d'un article avec des rapports et des vidéos sur MS SQL Server.</a></li>
<li><a href="../fr420089/index.html">Robomobiles: peloton et saucisses bavaroises</a></li>
<li><a href="../fr420091/index.html">Commentaires dans le code comme moyen d'expression de soi</a></li>
<li><a href="../fr420093/index.html">GitHub a dévoilé son code d'équilibrage de charge - Comment fonctionne leur solution</a></li>
<li><a href="../fr420095/index.html">Cryptographie asymétrique avec une clé secrète à usage unique: description de l'idée et application possible</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>