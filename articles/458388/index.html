<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äç‚öïÔ∏è üë©üèΩ‚Äçüè≠ üÜí Deep (Learning + Random) Forest y an√°lisis de art√≠culos ü¶Å üè§ üë®üèª‚Äçüè´</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuamos hablando sobre la conferencia sobre estad√≠sticas y aprendizaje autom√°tico AISTATS 2019. En esta publicaci√≥n analizaremos art√≠culos sobre m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Deep (Learning + Random) Forest y an√°lisis de art√≠culos</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ru_mts/blog/458388/"><p>  Continuamos hablando sobre la conferencia sobre estad√≠sticas y aprendizaje autom√°tico AISTATS 2019. En esta publicaci√≥n analizaremos art√≠culos sobre modelos profundos de conjuntos de √°rboles, mezclaremos la regularizaci√≥n para obtener datos muy escasos y una aproximaci√≥n eficiente de la validaci√≥n cruzada. </p><br><p><img src="https://habrastorage.org/webt/n-/uv/xj/n-uvxjud1se0puoearqtlott2de.jpeg"></p><a name="habracut"></a><br><h2 id="algoritm-glubokiy-les-an-exploration-to-non-nn-deep-models-based-on-non-differentiable-modules">  Algoritmo de bosque profundo: una exploraci√≥n de modelos profundos no NN basados ‚Äã‚Äãen m√≥dulos no diferenciables </h2><br><p>  Zhi-Hua Zhou (Universidad de Nanjing) <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Presentaci√≥n</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo</a> <br>  Implementaciones - abajo </p><br><p>  Un profesor de China habl√≥ sobre el conjunto de √°rboles, que los autores llaman la primera capacitaci√≥n profunda sobre m√≥dulos no diferenciables.  Esto puede parecer una declaraci√≥n demasiado fuerte, pero este profesor y su √≠ndice H 95 son oradores invitados, este hecho nos permite tomar la declaraci√≥n m√°s en serio.  La teor√≠a b√°sica de Deep Forest se ha desarrollado durante mucho tiempo, el art√≠culo original ya es 2017 (casi 200 citas), pero los autores escriben bibliotecas y cada a√±o mejoran el algoritmo en velocidad.  Y ahora, al parecer, han llegado al punto en que esta hermosa teor√≠a finalmente se puede poner en pr√°ctica. </p><br><p>  <em>Vista general de la arquitectura de Deep Forest</em> <br><img src="https://habrastorage.org/webt/4k/7q/1q/4k7q1qlzs5rixw4itr5luctdpuq.jpeg"></p><br><p>  <strong>Antecedentes</strong> </p><br><p>  Los modelos profundos, que ahora se entienden como redes neuronales profundas, se utilizan para capturar dependencias de datos complejas.  Adem√°s, result√≥ que aumentar el n√∫mero de capas es m√°s eficiente que aumentar el n√∫mero de unidades en cada capa.  Pero las redes neuronales tienen sus inconvenientes: </p><br><ul><li>  Se necesitan muchos datos para no volver a entrenar, </li><li>  Se necesitan muchos recursos inform√°ticos para aprender en un tiempo razonable, </li><li>  Demasiados hiperpar√°metros que son dif√≠ciles de configurar de manera √≥ptima </li></ul><br><p>  Adem√°s, los elementos de las redes neuronales profundas son m√≥dulos diferenciables que no son necesariamente efectivos para cada tarea.  A pesar de la complejidad de las redes neuronales, los algoritmos conceptualmente simples, como un bosque aleatorio, a menudo funcionan mejor o no mucho peor.  Pero para tales algoritmos, debe dise√±ar caracter√≠sticas manualmente, lo que tambi√©n es dif√≠cil de hacer de manera √≥ptima. </p><br><p>  Los investigadores ya han notado que los conjuntos en Kaggle: son "muy perfectos", e inspirados por las palabras de Scholl y Hinton de que la diferenciaci√≥n es el lado m√°s d√©bil de Deep Learning, decidieron crear un conjunto de √°rboles con propiedades DL. </p><br><p>  <em>Diapositiva "C√≥mo hacer un buen conjunto"</em> <br><img src="https://habrastorage.org/webt/8w/cb/z9/8wcbz9ml-7qidb5ii4-meqcinec.jpeg"></p><br><p>  La arquitectura se dedujo de las propiedades de los conjuntos: los elementos de los conjuntos no deber√≠an ser muy pobres en calidad y diferir. </p><br><p>  GcForest consta de dos fases: Cascade Forest y Multi-Grained Scanning.  Adem√°s, para que la cascada no se vuelva a entrenar, consta de 2 tipos de √°rboles, uno de los cuales es un √°rbol absolutamente aleatorio que se puede utilizar en datos no asignados.  El n√∫mero de capas se determina dentro del algoritmo de validaci√≥n cruzada. <br><img src="https://habrastorage.org/webt/qv/co/-b/qvco-br5vregwj-rrxn3bxnyfeq.jpeg"></p><br><p>  <em>Dos tipos de arboles</em> <br><img src="https://habrastorage.org/webt/mc/kp/ia/mckpiaiavjyh9hawcxhvtbusego.jpeg"></p><br><p>  <strong>Resultados</strong> </p><br><p>  Adem√°s de los resultados en conjuntos de datos est√°ndar, los autores trataron de usar gcForest en las transacciones del sistema de pago chino para buscar fraudes y obtuvieron F1 y AUC mucho m√°s altos que los de LR y DNN.  Estos resultados solo est√°n en la presentaci√≥n, pero el c√≥digo que se ejecuta en algunos conjuntos de datos est√°ndar est√° en Git. </p><br><p><img src="https://habrastorage.org/webt/y3/kf/gy/y3kfgytp_qawqyskwmvrrumzdna.jpeg"></p><br><p>  <em>Resultados de sustituci√≥n de algoritmo.</em>  <em>mdDF es √≥ptimo Margen Distribution Deep Forest, una variante de gcForest</em> </p><br><p><img src="https://habrastorage.org/webt/e1/oh/wq/e1ohwqrilda60nmdnosaa_ye4yk.jpeg"></p><br><p>  Pros: </p><br><ul><li>  Pocos hiperpar√°metros, el n√∫mero de capas se ajusta autom√°ticamente dentro del algoritmo. </li><li>  La configuraci√≥n predeterminada se elige para que funcione bien en muchas tareas. </li><li>  Complejidad adaptativa del modelo, en datos peque√±os: un modelo peque√±o </li><li>  No es necesario configurar funciones </li><li>  Funciona en calidad comparable a las redes neuronales profundas, y a veces mejor </li></ul><br><p>  Contras: </p><br><ul><li>  No acelerado en GPU </li><li>  En las fotos pierde DNNs </li></ul><br><p>  Las redes neuronales tienen un problema de atenuaci√≥n de gradiente, mientras que el bosque profundo tiene un problema de "desaparici√≥n de la diversidad".  Como se trata de un conjunto, cuantos m√°s elementos "diferentes" y "buenos" se utilicen, mayor ser√° la calidad.  El problema es que los autores ya han probado casi todos los enfoques cl√°sicos (muestreo, aleatorizaci√≥n).  Mientras no aparezca una nueva investigaci√≥n b√°sica sobre el tema de las "diferencias", ser√° dif√≠cil mejorar la calidad de los bosques profundos.  Pero ahora es posible mejorar la velocidad de la inform√°tica. </p><br><p>  <strong>Reproducibilidad de resultados</strong> </p><br><p>  XGBoost me intrig√≥ con los datos tabulares, y quer√≠a reproducir el resultado.  Tom√© el conjunto de datos de Adultos y apliqu√© GcForestCS (una versi√≥n ligeramente acelerada de GcForest) con par√°metros de los autores del art√≠culo y XGBoost con par√°metros predeterminados.  En el ejemplo que ten√≠an los autores, las caracter√≠sticas categ√≥ricas ya estaban preprocesadas de alguna manera, pero no se indic√≥ c√≥mo.  Como resultado, utilic√© CatBoostEncoder y otra m√©trica: ROC AUC.  Los resultados fueron estad√≠sticamente diferentes: XGBoost gan√≥.  El tiempo de funcionamiento de XGBoost es insignificante, mientras que gcForestCS tiene 20 minutos de cada validaci√≥n cruzada en 5 veces.  Por otro lado, los autores probaron el algoritmo en diferentes conjuntos de datos y ajustaron los par√°metros para este conjunto de datos a su preprocesamiento de caracter√≠sticas. </p><br><p>  El c√≥digo se puede encontrar <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </p><br><p>  <strong>Implementaciones</strong> </p><br><p>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">El c√≥digo oficial de los autores del art√≠culo.</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Modificaci√≥n mejorada oficial, m√°s r√°pido, pero sin documentaci√≥n</a> <br>  ‚Üí La <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">implementaci√≥n es m√°s simple</a> </p><br><h2 id="pclasso-the-lasso-meets-principal-components-regression">  PcLasso: el lazo se encuentra con la regresi√≥n de componentes principales </h2><br><p>  J. Kenneth Tay, Jerome Friedman, Robert Tibshirani (Universidad de Stanford) </p><br><p>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Presentaci√≥n</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ejemplo de uso</a> </p><br><p>  A principios de 2019, J. Kenneth Tay, Jerome Friedman y Robert Tibshirani de la Universidad de Stanford propusieron un nuevo m√©todo de ense√±anza con un maestro, especialmente adecuado para datos escasos. </p><br><p>  Los autores del art√≠culo resolvieron el problema de analizar datos sobre estudios de expresi√≥n g√©nica, que se describen en Zeng &amp; Breesy (2016).  El objetivo es el estado mutacional del gen p53, que regula la expresi√≥n g√©nica en respuesta a diversas se√±ales de estr√©s celular.  El objetivo del estudio es identificar predictores que se correlacionan con el estado mutacional de p53.  Los datos constan de 50 filas, 17 de las cuales se clasifican como normales y las 33 restantes tienen mutaciones en el gen p53.  Seg√∫n el an√°lisis de Subramanian et al.  (2005) 308 conjuntos de genes que est√°n entre 15 y 500 se incluyen en este an√°lisis.  Estos kits de genes contienen un total de 4,301 genes y est√°n disponibles en el paquete grpregOverlap R.  Al expandir datos para procesar grupos superpuestos, se generan 13,237 columnas.  Los autores del art√≠culo utilizaron el m√©todo pcLasso, que ayud√≥ a mejorar los resultados del modelo. </p><br><p>  <em>En la imagen vemos un aumento en AUC cuando se usa "pcLasso"</em> <br><img src="https://habrastorage.org/webt/ok/p6/mg/okp6mgex-l9p49vcz5gedg8xa5o.jpeg"></p><br><p>  <strong>La esencia del m√©todo.</strong> </p><br><p>  M√©todo combina <img src="https://tex.s2cms.ru/svg/l_1" alt="l_1">  -regularizaci√≥n con <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  , que reduce el vector de coeficientes a los componentes principales de la matriz de caracter√≠sticas.  Llamaron al m√©todo propuesto "componentes centrales del lazo" ("pcLasso" disponible en R).  El m√©todo puede ser especialmente poderoso si las variables se agrupan previamente (el usuario elige qu√© y c√≥mo agrupar).  En este caso, pcLasso comprime cada grupo y obtiene la soluci√≥n en la direcci√≥n de los componentes principales de este grupo.  En el proceso de resoluci√≥n, tambi√©n se realiza la selecci√≥n de grupos significativos entre los disponibles. </p><br><p>  Presentamos la matriz diagonal de la descomposici√≥n singular de una matriz centrada de caracter√≠sticas. <img src="https://tex.s2cms.ru/svg/X" alt="X">  como sigue: </p><br><p>  Representamos nuestra descomposici√≥n singular de la matriz centrada X (SVD) como <img src="https://tex.s2cms.ru/svg/X%3DUDV%5ET" alt="X = UDV ^ T">  donde <img src="https://tex.s2cms.ru/svg/D" alt="D">  Es una matriz diagonal que consiste en valores singulares.  En esta forma <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  - La regularizaci√≥n puede ser representada: <br><img src="https://tex.s2cms.ru/svg/%5Cbeta%5ET%20VZV%5ET%20%5Cbeta" alt="\ beta ^ T VZV ^ T \ beta">  donde <img src="https://tex.s2cms.ru/svg/Z" alt="Z">  - matriz diagonal que contiene la funci√≥n de cuadrados de valores singulares: <img src="https://tex.s2cms.ru/svg/Z_%7B11%7D%3Df_1%20(d_1%5E2%2Cd_2%5E2%2C%E2%80%A6%2Cd_m%5E2%20)%2C%E2%80%A6%2CZ_%7B22%7D%3Df_2%20(d_1%5E2%2Cd_2%5E2%2C%E2%80%A6%2Cd_m%5E2%20)" alt="Z_ {11} = f_1 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2), ..., Z_ {22} = f_2 (d_1 ^ 2, d_2 ^ 2, ..., d_m ^ 2)">  . </p><br><p>  En general, en <img src="https://tex.s2cms.ru/svg/l_2" alt="l_2">  -regularizaci√≥n <img src="https://tex.s2cms.ru/svg/Z_%7Bjj%7D%3D1" alt="Z_ {jj} = 1">  para todos <img src="https://tex.s2cms.ru/svg/j" alt="j">  eso corresponde <img src="https://tex.s2cms.ru/svg/%5Cbeta%5ET%20%5Cbeta" alt="\ beta ^ T \ beta">  .  Sugieren minimizar la siguiente funcionalidad: </p><br><p><img src="https://habrastorage.org/webt/6l/fj/lv/6lfjlv9m-zy8qfhcvrcqcymuuxa.jpeg"></p><br><p>  Aqui <img src="https://tex.s2cms.ru/svg/D" alt="D">  - matriz de diferencias de elementos diagonales <img src="https://tex.s2cms.ru/svg/d_1%5E2-d_1%5E2%2Cd_1%5E2-d_2%5E2%2C%E2%80%A6%2Cd_1%5E2-d_m%5E2" alt="d_1 ^ 2-d_1 ^ 2, d_1 ^ 2-d_2 ^ 2, ..., d_1 ^ 2-d_m ^ 2">  .  En otras palabras, controlamos el vector <img src="https://tex.s2cms.ru/svg/%5Cbeta%20" alt="\ beta">  usando hiperpar√°metro tambi√©n <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  . <br>  Transformando esta expresi√≥n, obtenemos la soluci√≥n: </p><br><p><img src="https://habrastorage.org/webt/vf/qs/6b/vfqs6b8fnqo3bmlacyr5j4fpigs.jpeg"></p><br><p>  Pero la "caracter√≠stica" principal del m√©todo, por supuesto, es la capacidad de agrupar datos y, sobre la base de estos grupos, resaltar los componentes principales del grupo.  Luego reescribimos nuestra soluci√≥n en la forma: </p><br><p><img src="https://habrastorage.org/webt/9l/ij/wc/9lijwc3_kvwtvxalszzyt4zq4l4.jpeg"></p><br><p>  Aqui <img src="https://tex.s2cms.ru/svg/%5Cbeta_k" alt="\ beta_k">  - subvector vectorial <img src="https://tex.s2cms.ru/svg/%5Cbeta" alt="\ beta">  correspondiente al grupo k, <img src="https://tex.s2cms.ru/svg/d_k%3D(d_%7Bk1%7D%2C%E2%80%A6%2Cd_%7Bkmk%7D)" alt="d_k = (d_ {k1}, ..., d_ {kmk})">  - valores singulares <img src="https://tex.s2cms.ru/svg/X_k" alt="X_k">  arreglado en orden descendente, y <img src="https://tex.s2cms.ru/svg/D_%7Bd_%7Bk1%7D%5E2-d_%7Bkj%7D%5E2%7D" alt="D_ {d_ {k1} ^ 2-d_ {kj} ^ 2}">  - matriz diagonal <img src="https://tex.s2cms.ru/svg/d_%7Bk1%7D%5E2-d_%7Bkj%7D%5E2%2C%20j%3D1%2C2%2C%E2%80%A6%2Cm_k" alt="d_ {k1} ^ 2-d_ {kj} ^ 2, j = 1,2, ..., m_k"></p><br><p>  Algunas notas sobre la soluci√≥n del objetivo funcional: </p><br><ol><li><p>  La funci√≥n objetivo es convexa, y el componente no liso es separable.  Por lo tanto, se puede optimizar de manera efectiva utilizando el descenso de gradiente. <br>  El enfoque es comprometer m√∫ltiples valores <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  (incluido cero, respectivamente, obteniendo el est√°ndar <img src="https://tex.s2cms.ru/svg/l_1" alt="l_1">  -regularizaci√≥n), y luego optimizar: <img src="https://habrastorage.org/webt/uz/bf/eo/uzbfeori9kupj8b05x46dcj6iri.jpeg">  recogiendo <img src="https://tex.s2cms.ru/svg/%5Clambda" alt="\ lambda">  .  En consecuencia, los par√°metros <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  y <img src="https://tex.s2cms.ru/svg/%5Clambda" alt="\ lambda">  son seleccionados para validaci√≥n cruzada. </p><br></li><li><p>  Par√°metro <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  Dif√≠cil de interpretar.  En el software (paquete pcLasso), el usuario mismo establece el valor de este par√°metro, que pertenece al intervalo [0,1], donde 1 corresponde a <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  = 0 (lazo). </p><br></li></ol><br><p>  En la pr√°ctica, variando los valores <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  = 0.25, 0.5, 0.75, 0.9, 0.95 y 1, puede cubrir una amplia gama de modelos. </p><br><p>  <em>El algoritmo en s√≠ es el siguiente</em> <br><img src="https://habrastorage.org/webt/l-/3x/si/l-3xsipork2hzeh7ws1bg1hz86o.jpeg"></p><br><p>  Este algoritmo ya est√° escrito en R, si lo desea, ya puede usarlo.  La biblioteca se llama 'pcLasso'. </p><br><h2>  Una navaja suiza Infinitesimal del ej√©rcito </h2><br><p>  Ryan Giordano (UC Berkeley);  William Stephenson (MIT);  Runjing Liu (UC Berkeley); <br>  Michael Jordan (UC Berkeley);  Tamara Broderick (MIT) </p><br><p>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Art√≠culo</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">C√≥digo</a> </p><br><p>  La calidad de los algoritmos de aprendizaje autom√°tico a menudo se mide mediante validaci√≥n cruzada m√∫ltiple (validaci√≥n cruzada o bootstrap).  Estos m√©todos son potentes, pero lentos en grandes conjuntos de datos. </p><br><p>  En este trabajo, los colegas usan una aproximaci√≥n lineal de pesos, produciendo resultados que funcionan m√°s r√°pido.  Esta aproximaci√≥n lineal se conoce en la literatura estad√≠stica como "navaja infinitesimal".  Se utiliza principalmente como una herramienta te√≥rica para probar resultados asint√≥ticos.  Los resultados del art√≠culo son aplicables independientemente de si los pesos y los datos son estoc√°sticos o deterministas.  Como consecuencia, esta aproximaci√≥n estima secuencialmente la verdadera validaci√≥n cruzada para cualquier k fija. </p><br><p>  <em>Presentaci√≥n del Premio de papel al autor del art√≠culo.</em> <br><img src="https://habrastorage.org/webt/3n/1a/-k/3n1a-kygdmkbs0drfjdg38b9z5g.jpeg"></p><br><p>  <strong>La esencia del m√©todo.</strong> </p><br><p>  Considere el problema de estimar un par√°metro desconocido <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%5Cin%20%5COmega_%7B%5Ctheta%7D%20%5Csubset%20R%5E%7BD%7D" alt="\ theta \ in \ Omega _ {\ theta} \ subset R ^ {D}">  donde <img src="https://tex.s2cms.ru/svg/%5COmega_%7B%5Ctheta%7D%20" alt="\ Omega _ {\ theta}">  Es compacto y el tama√±o de nuestro conjunto de datos es <img src="https://tex.s2cms.ru/svg/N" alt="N">  .  Nuestro an√°lisis se llevar√° a cabo en un conjunto de datos fijo.  Define nuestra calificaci√≥n <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%5Cin%20%5COmega_%7B%5Ctheta%7D%20" alt="\ theta \ in \ Omega _ {\ theta}">  como sigue: </p><br><ol><li>  Para cada <img src="https://tex.s2cms.ru/svg/n%3D1%2C2%E2%80%A6%2CN" alt="n = 1,2 ..., N">  establecer <img src="https://tex.s2cms.ru/svg/g_n" alt="g_n">  ( <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  ) Es una funci√≥n de <img src="https://tex.s2cms.ru/svg/%5COmega_%7B%5Ctheta%7D%20%5Csubset%20R%5E%7BD%7D" alt="\ Omega _ {\ theta} \ subconjunto R ^ {D}"></li><li><img src="https://tex.s2cms.ru/svg/%5Comega_n%20" alt="\ omega_n">  Es un n√∫mero real y <img src="https://tex.s2cms.ru/svg/%5Comega" alt="\ omega">  Es un vector que consiste en <img src="https://tex.s2cms.ru/svg/%5Comega_n" alt="\ omega_n"></li></ol><br><p>  Entonces <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D" alt="\ hat {\ theta}">  se puede representar como: </p><br><p><img src="https://habrastorage.org/webt/zh/ce/yi/zhceyifw80rl6neeoaedkd7x-20.jpeg"></p><br><p>  Al resolver este problema de optimizaci√≥n mediante el m√©todo de gradiente, asumimos que las funciones son diferenciables y podemos calcular el Hessian.  El principal problema que resolvemos es el costo computacional asociado con la evaluaci√≥n. <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20%CC%82(%5Comega)" alt="\ hat {\ theta} ÃÇ (\ omega)">  para todos <img src="https://tex.s2cms.ru/svg/%5Comega%E2%88%88W" alt="\ omega‚ààW">  .  La principal contribuci√≥n de los autores del art√≠culo consiste en calcular la estimaci√≥n. <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1%3D%5Chat%7B%5Ctheta%7D_1%20(1_%7B%5Comega%7D)" alt="\ hat {\ theta} _1 = \ hat {\ theta} _1 (1 _ {\ omega})">  donde <img src="https://tex.s2cms.ru/svg/1_%5Comega%3D(1%2C1%2C%E2%80%A6%2C1)" alt="1_ \ omega = (1,1, ..., 1)">  .  En otras palabras, nuestra optimizaci√≥n depender√° solo de derivados <img src="https://tex.s2cms.ru/svg/g_n%20(%5Ctheta)" alt="g_n (\ theta)">  que suponemos que existen y son de Hesse: </p><br><p><img src="https://habrastorage.org/webt/tb/zb/t2/tbzbt2u2q_bdqidfjxfl9ywqesc.jpeg"></p><br><p>  A continuaci√≥n, definimos una ecuaci√≥n con un punto fijo y su derivada: <br><img src="https://habrastorage.org/webt/hj/8f/x3/hj8fx3broftye-ssmq4mwpkvaui.jpeg"></p><br><p>  Aqu√≠ vale la pena prestar atenci√≥n a que <img src="https://tex.s2cms.ru/svg/G(%5Ctheta%20%CC%82(%5Comega)%2Cw)%3D0" alt="G (\ theta ÃÇ (\ omega), w) = 0">  desde <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20(%5Comega)" alt="\ hat {\ theta} (\ omega)">  - soluci√≥n para <img src="https://tex.s2cms.ru/svg/%5Cfrac%7B%201%20%7D%7B%20N%20%7D%20%5Csum_%7Bn%3D1%7D%5E%7BN%7D%20%5Comega_n%20g_n%20(%5Ctheta)%3D0" alt="\ frac {1} {N} \ sum_ {n = 1} ^ {N} \ omega_n g_n (\ theta) = 0">  .  Tambi√©n definimos: <img src="https://tex.s2cms.ru/svg/H_1%3DH(%5Chat%7B%5Ctheta%7D_1%2C1_%5Comega)" alt="H_1 = H (\ hat {\ theta} _1,1_ \ omega)">  , y la matriz de pesos como: <img src="https://tex.s2cms.ru/svg/%5CDelta%5Comega%3D%20%5Comega-1_%5Comega%20%5Cin%20R%5E%7Bn%7D" alt="\ Delta \ omega = \ omega-1_ \ omega \ en R ^ {n}">  .  En el caso cuando <img src="https://tex.s2cms.ru/svg/H_1" alt="H_1">  tiene una matriz inversa, podemos usar el teorema de la funci√≥n impl√≠cita y la 'regla de la cadena': </p><br><p><img src="https://habrastorage.org/webt/c7/iv/x2/c7ivx2dadeupxwlo2hzgmxslrxe.jpeg"></p><br><p>  Esta derivada nos permite formar una aproximaci√≥n lineal. <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D%20%CC%82(%5Comega)" alt="\ hat {\ theta} ÃÇ (\ omega)">  a trav√©s de <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1" alt="\ hat {\ theta} _1">  que se ve as√≠: </p><br><p><img src="https://habrastorage.org/webt/lc/rc/5h/lcrc5hirn1act9jl8lp2jakjpsk.jpeg"></p><br><p>  Desde <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_%7BIJ%7D" alt="\ hat {\ theta} _ {IJ}">  depende solo de <img src="https://tex.s2cms.ru/svg/%5Chat%7B%5Ctheta%7D_1" alt="\ hat {\ theta} _1">  y <img src="https://tex.s2cms.ru/svg/%5CDelta%20%5Comega" alt="\ Delta \ omega">  , y no de soluciones para otros valores <img src="https://tex.s2cms.ru/svg/%5Comega" alt="\ omega">  , en consecuencia, no es necesario volver a calcular y encontrar nuevos valores de œâ.  En cambio, uno necesita resolver el SLE (sistema de ecuaciones lineales). </p><br><p>  <strong>Resultados</strong> </p><br><p>  En la pr√°ctica, esto reduce significativamente el tiempo en comparaci√≥n con la validaci√≥n cruzada: <br><img src="https://habrastorage.org/webt/sw/dr/6-/swdr6-j8t7pqcdwf_96705qs1tg.jpeg"></p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/458388/">https://habr.com/ru/post/458388/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458374/index.html">TJBOT como una ilustraci√≥n de los servicios de IBM Watson</a></li>
<li><a href="../458376/index.html">No es otro lenguaje de programaci√≥n. Parte 1: l√≥gica de dominio</a></li>
<li><a href="../458378/index.html">Usando Avocode para el dise√±o del sitio. Revisi√≥n para principiantes. Bonificaci√≥n: registre un per√≠odo de prueba de 30 d√≠as</a></li>
<li><a href="../458382/index.html">¬øPor qu√© estamos ense√±ando esto?</a></li>
<li><a href="../458384/index.html">HP 3D Structured Light Scanner Pro S3 Revisi√≥n y prueba</a></li>
<li><a href="../458390/index.html">Ceph - de "en la rodilla" a "producci√≥n" parte 2</a></li>
<li><a href="../458394/index.html">Asegurar protocolos inal√°mbricos utilizando LoRaWAN como ejemplo</a></li>
<li><a href="../458396/index.html">C√≥mo hice que el desarrollo en Vue.js sea conveniente con la representaci√≥n del lado del servidor</a></li>
<li><a href="../458398/index.html">Higiene del trabajo a distancia o los beneficios de la telepat√≠a.</a></li>
<li><a href="../458400/index.html">Arquitectura e implementaci√≥n de microservicios, paso a paso, parte 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>