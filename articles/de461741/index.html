<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>✖️ ⌨️ 🖐🏾 Hierarchisches Clustering kategorialer Daten in R. 🐌 👩🏽‍🎤 🚐</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die Übersetzung wurde für Studenten des Kurses „Applied Analytics on R“ erstellt . 




 Dies war mein erster Versuch, Clients basierend auf realen Da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hierarchisches Clustering kategorialer Daten in R.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/461741/">  <i>Die Übersetzung wurde für Studenten des Kurses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„Applied Analytics on R“ erstellt</a> .</i> <br><br><img src="https://habrastorage.org/webt/wq/q0/sp/wqq0sphqihtnsg1f8eor15ffkgi.png"><br><hr><br><br>  Dies war mein erster Versuch, Clients basierend auf realen Daten zu gruppieren, und es gab mir wertvolle Erfahrungen.  Es gibt viele Artikel im Internet über das Clustering mit numerischen Variablen, aber es war nicht so einfach, Lösungen für kategoriale Daten zu finden, was etwas schwieriger ist.  Clustering-Methoden für kategoriale Daten befinden sich noch in der Entwicklung, und in einem anderen Beitrag werde ich einen anderen ausprobieren. <br><a name="habracut"></a><br>  Auf der anderen Seite denken viele Menschen, dass das Clustering kategorialer Daten möglicherweise keine aussagekräftigen Ergebnisse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">liefert</a> - und dies ist teilweise richtig (siehe die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ausgezeichnete Diskussion zu CrossValidated</a> ).  Irgendwann dachte ich: „Was mache ich?  Sie können einfach in Kohorten unterteilt werden. “  Eine Kohortenanalyse ist jedoch auch nicht immer ratsam, insbesondere bei einer signifikanten Anzahl von kategorialen Variablen mit einer großen Anzahl von Ebenen: Sie können problemlos mit 5-7 Kohorten umgehen, aber wenn Sie 22 Variablen haben und jede 5 Ebenen hat (z. B. eine Kundenumfrage mit diskreten Schätzungen 1) , 2, 3, 4 und 5), und Sie müssen verstehen, mit welchen charakteristischen Gruppen von Kunden Sie es zu tun haben - Sie erhalten 22x5 Kohorten.  Niemand möchte sich mit einer solchen Aufgabe beschäftigen.  Und hier könnte Clustering helfen.  In diesem Beitrag werde ich darüber sprechen, was ich selbst wissen möchte, sobald ich mit dem Clustering begonnen habe. <br><br>  Der Clustering-Prozess selbst besteht aus drei Schritten: <br><br><ol><li>  Der Aufbau einer Matrix der Unähnlichkeit ist zweifellos die wichtigste Entscheidung beim Clustering.  Alle nachfolgenden Schritte basieren auf der von Ihnen erstellten Unähnlichkeitsmatrix. </li><li>  Die Wahl der Clustering-Methode. </li><li>  Cluster-Evaluierung. </li></ol><br>  Dieser Beitrag ist eine Art Einführung, die die Grundprinzipien des Clustering und seiner Implementierung in der Umgebung R beschreibt. <br><br><h2>  Unähnlichkeitsmatrix </h2><br>  Die Basis für das Clustering wird die Unähnlichkeitsmatrix sein, die mathematisch beschreibt, wie unterschiedlich die Punkte im Datensatz voneinander entfernt sind.  Sie können die Punkte, die am nächsten beieinander liegen, weiter in Gruppen zusammenfassen oder die am weitesten voneinander entfernten Punkte voneinander trennen - dies ist die Hauptidee der Clusterbildung. <br><br>  In diesem Stadium sind Unterschiede zwischen Datentypen wichtig, da die Unähnlichkeitsmatrix auf den Abständen zwischen einzelnen Datenpunkten basiert.  Die Abstände zwischen den Punkten numerischer Daten sind leicht vorstellbar (ein bekanntes Beispiel sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">euklidische Abstände</a> ), aber bei kategorialen Daten (Faktoren in R) ist nicht alles so offensichtlich. <br><br>  Um in diesem Fall eine Unähnlichkeitsmatrix zu erstellen, sollte der sogenannte Gover-Abstand verwendet werden.  Ich werde nicht auf den mathematischen Teil dieses Konzepts eingehen, sondern nur Links bereitstellen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">da</a> .  Aus diesem <code>metric = c("gower")</code> bevorzuge ich die Verwendung von <code>daisy()</code> mit der <code>metric = c("gower")</code> aus dem <code>cluster</code> . <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#-----   -----# #    ,       ,     ,   ,    library(dplyr) #     set.seed(40) #     #    ;   data.frame()     #    ,   200   1  200 id.s &lt;- c(1:200) %&gt;% factor() budget.s &lt;- sample(c("small", "med", "large"), 200, replace = T) %&gt;% factor(levels=c("small", "med", "large"), ordered = TRUE) origins.s &lt;- sample(c("x", "y", "z"), 200, replace = T, prob = c(0.7, 0.15, 0.15)) area.s &lt;- sample(c("area1", "area2", "area3", "area4"), 200, replace = T, prob = c(0.3, 0.1, 0.5, 0.2)) source.s &lt;- sample(c("facebook", "email", "link", "app"), 200, replace = T, prob = c(0.1,0.2, 0.3, 0.4)) ##   —      dow.s &lt;- sample(c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), 200, replace = T, prob = c(0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2)) %&gt;% factor(levels=c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), ordered = TRUE) #  dish.s &lt;- sample(c("delicious", "the one you don't like", "pizza"), 200, replace = T) #   data.frame()      synthetic.customers &lt;- data.frame(id.s, budget.s, origins.s, area.s, source.s, dow.s, dish.s) #-----   -----# library(cluster) #       #   : daisy(), diana(), clusplot() gower.dist &lt;- daisy(synthetic.customers[ ,2:7], metric = c("gower")) # class(gower.dist) ## , </span></span></code> </pre> <br>  Die Unähnlichkeitsmatrix ist fertig.  Für 200 Beobachtungen wird es schnell erstellt, erfordert jedoch möglicherweise einen sehr großen Rechenaufwand, wenn Sie mit einem großen Datensatz arbeiten. <br><br>  In der Praxis ist es sehr wahrscheinlich, dass Sie zuerst den Datensatz bereinigen, die erforderlichen Transformationen aus den Zeilen in Faktoren durchführen und die fehlenden Werte verfolgen müssen.  In meinem Fall enthielt der Datensatz auch Zeilen mit fehlenden Werten, die jedes Mal wunderschön gruppiert wurden. Es schien also ein Schatz zu sein - bis ich mir die Werte ansah (leider!). <br><br><h2>  Clustering-Algorithmen </h2><br>  Möglicherweise wissen Sie bereits, dass Clustering <i>k-means und hierarchisch ist</i> .  In diesem Beitrag konzentriere ich mich auf die zweite Methode, da sie flexibler ist und verschiedene Ansätze ermöglicht: Sie können entweder einen <i>agglomerativen</i> (von unten nach oben) oder einen <i>abteilungsweisen</i> (von oben nach unten) Clustering-Algorithmus wählen. <br><br><img src="https://habrastorage.org/webt/nl/vp/u4/nlvpu4e8ykoh_nd_el_4i6plh8q.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Programmierhandbuch für UC Business Analytics R.</a></i> <br><br>  Agglomeratives Clustering beginnt mit <code>n</code> Clustern, wobei <code>n</code> die Anzahl der Beobachtungen ist: Es wird angenommen, dass jeder von ihnen ein separater Cluster ist.  Dann versucht der Algorithmus, die ähnlichsten Datenpunkte untereinander zu finden und zu gruppieren - so beginnt die Clusterbildung. <br><br>  Divisional Clustering wird in umgekehrter Weise durchgeführt - es wird zunächst angenommen, dass alle n Datenpunkte, die wir haben, ein großer Cluster sind, und dann werden die am wenigsten ähnlichen in separate Gruppen unterteilt. <br><br>  Bei der Entscheidung, welche dieser Methoden ausgewählt werden soll, ist es immer sinnvoll, alle Optionen auszuprobieren. Im Allgemeinen <i>eignet sich agglomeratives Clustering jedoch besser zur Identifizierung kleiner Cluster und wird von den meisten Computerprogrammen verwendet, und Divisionsclustering ist besser zur Identifizierung großer Cluster geeignet</i> . <br><br>  Bevor ich mich für eine Methode entscheide, schaue ich mir lieber Dendrogramme an - eine grafische Darstellung von Clustering.  Wie Sie später sehen werden, sind einige Dendrogramme gut ausbalanciert, während andere sehr chaotisch sind. <br><br>  # Die Haupteingabe für den folgenden Code ist die Unähnlichkeit (Distanzmatrix) <br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#             #            —         —    #------------  ------------# divisive.clust &lt;- diana(as.matrix(gower.dist), diss = TRUE, keep.diss = TRUE) plot(divisive.clust, main = "Divisive")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/54/mp/m1/54mpm19v8jkkpmj6usehxlgr5qk.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#------------   ------------# #      #         —     ,      #    (complete linkages) aggl.clust.c &lt;- hclust(gower.dist, method = "complete") plot(aggl.clust.c, main = "Agglomerative, complete linkages")</span></span></code> </pre> <br><h2>  Bewertung der Clusterqualität </h2><br>  In diesem Stadium muss zwischen verschiedenen Clustering-Algorithmen und einer unterschiedlichen Anzahl von Clustern gewählt werden.  Sie können verschiedene Bewertungsmethoden anwenden, ohne zu vergessen, sich vom <b>gesunden Menschenverstand</b> leiten zu lassen.  Ich habe diese Wörter fett und kursiv hervorgehoben, da die Aussagekraft der Auswahl <b>sehr wichtig ist</b> - die Anzahl der Cluster und die Methode zur Aufteilung von Daten in Gruppen sollten aus praktischer Sicht praktisch sein.  Die Anzahl der Kombinationen von Werten kategorialer Variablen ist endlich (da sie diskret sind), aber keine darauf basierende Aufschlüsselung ist sinnvoll.  Möglicherweise möchten Sie auch nicht sehr wenige Cluster haben - in diesem Fall sind sie zu verallgemeinert.  Am Ende hängt alles von Ihrem Ziel und den Aufgaben der Analyse ab. <br><br>  Im Allgemeinen möchten Sie beim Erstellen von Clustern klar definierte Gruppen von Datenpunkten erhalten, damit der Abstand zwischen solchen Punkten innerhalb des Clusters ( <i>oder die Kompaktheit</i> ) minimal und der Abstand zwischen Gruppen ( <i>Trennbarkeit</i> ) maximal möglich ist.  Dies ist intuitiv leicht zu verstehen: Der Abstand zwischen Punkten ist ein Maß für ihre Unähnlichkeit, die auf der Grundlage der Unähnlichkeitsmatrix erhalten wird.  Daher basiert die Bewertung der Qualität der Clusterbildung auf der Bewertung der Kompaktheit und Trennbarkeit. <br><br>  Als nächstes werde ich zwei Ansätze demonstrieren und zeigen, dass einer von ihnen bedeutungslose Ergebnisse liefern kann. <br><br><ul><li>  <i>Ellbogenmethode</i> : Beginnen Sie damit, wenn der wichtigste Faktor für Ihre Analyse die Kompaktheit der Cluster ist, d. H. Die Ähnlichkeit innerhalb der Gruppen. </li><li>  <i>Bewertungsmethode für Silhouetten</i> : Das als Maß für die Datenkonsistenz verwendete <i>Silhouettendiagramm</i> zeigt, wie nahe die einzelnen Punkte innerhalb eines Clusters an den Punkten in benachbarten Clustern liegen. </li></ul><br>  In der Praxis führen diese beiden Methoden häufig zu unterschiedlichen Ergebnissen, was zu Verwirrung führen kann. Die maximale Kompaktheit und die klarste Trennung werden mit einer unterschiedlichen Anzahl von Clustern erreicht, sodass der gesunde Menschenverstand und das Verständnis dessen, was Ihre Daten wirklich bedeuten, eine wichtige Rolle spielen bei der endgültigen Entscheidung. <br><br>  Es gibt auch eine Reihe von Metriken, die Sie analysieren können.  Ich werde sie direkt zum Code hinzufügen. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,        #      ,     ,   —   #     ,      ,         ,   ,     library(fpc) cstats.table &lt;- function(dist, tree, k) { clust.assess &lt;- c("cluster.number","n","within.cluster.ss","average.within","average.between", "wb.ratio","dunn2","avg.silwidth") clust.size &lt;- c("cluster.size") stats.names &lt;- c() row.clust &lt;- c() output.stats &lt;- matrix(ncol = k, nrow = length(clust.assess)) cluster.sizes &lt;- matrix(ncol = k, nrow = k) for(i in c(1:k)){ row.clust[i] &lt;- paste("Cluster-", i, " size") } for(i in c(2:k)){ stats.names[i] &lt;- paste("Test", i-1) for(j in seq_along(clust.assess)){ output.stats[j, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.assess])[j] } for(d in 1:k) { cluster.sizes[d, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.size])[d] dim(cluster.sizes[d, i]) &lt;- c(length(cluster.sizes[i]), 1) cluster.sizes[d, i] } } output.stats.df &lt;- data.frame(output.stats) cluster.sizes &lt;- data.frame(cluster.sizes) cluster.sizes[is.na(cluster.sizes)] &lt;- 0 rows.all &lt;- c(clust.assess, row.clust) # rownames(output.stats.df) &lt;- clust.assess output &lt;- rbind(output.stats.df, cluster.sizes)[ ,-1] colnames(output) &lt;- stats.names[2:k] rownames(output) &lt;- rows.all is.num &lt;- sapply(output, is.numeric) output[is.num] &lt;- lapply(output[is.num], round, 2) output } #     :      7 #     ,            stats.df.divisive &lt;- cstats.table(gower.dist, divisive.clust, 7) stats.df.divisive</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/r-/g_/ou/r-g_oukwyorhqnsls_cbg4c8spw.png"><br><br>  Der Mittelwert innerhalb des Indikators, der den durchschnittlichen Abstand zwischen Beobachtungen innerhalb von Clustern darstellt, nimmt ebenso ab wie innerhalb von Cluster.ss (die Summe der Quadrate der Abstände zwischen Beobachtungen in einem Cluster).  Die durchschnittliche Breite der Silhouette (durchschnittliche Silberbreite) variiert nicht so eindeutig, es ist jedoch immer noch eine umgekehrte Beziehung zu erkennen. <br>  Beachten Sie, wie unverhältnismäßig die Clustergrößen sind.  Ich würde mich nicht beeilen, mit einer unvergleichlichen Anzahl von Beobachtungen innerhalb von Clustern zu arbeiten.  Einer der Gründe ist, dass der Datensatz möglicherweise unausgewogen ist und einige Beobachtungsgruppen alle anderen in der Analyse überwiegen - dies ist nicht gut und führt höchstwahrscheinlich zu Fehlern. <br><br> <code>stats.df.aggl &lt;-cstats.table(gower.dist, aggl.clust.c, 7) #      </code> <br> <br> <code>stats.df.aggl</code> <br> <br><img src="https://habrastorage.org/webt/a_/-u/aa/a_-uaa_nff99nuyobulroyk_hka.png"><br><br>  Beachten Sie, wie viel besser die Anzahl der Beobachtungen pro Gruppe durch agglomeratives hierarchisches Clustering auf der Grundlage der vollständigen Kommunikationsmethode ausgeglichen wird. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># ---------    ---------# #   «»       #    ,     7  library(ggplot2) #  #   ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/kw/kz/xy/kwkzxyuuzwhe0yst9kteg9inias.png"><br><br>  Also haben wir ein Diagramm des "Ellbogens" erstellt.  Es zeigt, wie die Summe der quadratischen Abstände zwischen den Beobachtungen (wir verwenden sie als Maß für die Nähe der Beobachtungen - je kleiner sie sind, desto näher sind die Messungen innerhalb des Clusters) für eine unterschiedliche Anzahl von Clustern variieren.  Idealerweise sollten wir an der Stelle, an der eine weitere Clusterbildung nur eine geringfügige Abnahme der Quadratsumme (SS) ergibt, eine deutliche „Ellbogenbiegung“ sehen.  Für die folgende Grafik würde ich bei ungefähr 7 anhalten. Obwohl in diesem Fall einer der Cluster nur aus zwei Beobachtungen besteht.  Mal sehen, was beim agglomerativen Clustering passiert. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#       ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/y0/ck/q-/y0ckq-zxtzg0fbjr9gcq1jgorvq.png"><br><br>  Agglomerativer „Ellbogen“ ähnelt dem Teilungsbogen, aber die Grafik sieht glatter aus - Biegungen sind nicht so ausgeprägt.  Wie beim Divisionsclustering würde ich mich auf 7 Cluster konzentrieren. Wenn ich jedoch zwischen diesen beiden Methoden wähle, mag ich die Clustergrößen, die durch die agglomerative Methode erhalten werden, mehr - es ist besser, dass sie miteinander vergleichbar sind. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/u9/nj/nf/u9njnfcjqbxbzlfgpl5sxqailra.png"><br><br>  Wenn Sie die Silhouette-Schätzmethode verwenden, sollten Sie den Betrag auswählen, der den maximalen Silhouette-Koeffizienten ergibt, da Sie Cluster benötigen, die weit genug voneinander entfernt sind, um als getrennt betrachtet zu werden. <br><br>  Der Silhouette-Koeffizient kann zwischen –1 und 1 liegen, wobei 1 einer guten Konsistenz innerhalb der Cluster entspricht und –1 nicht sehr gut. <br>  Im Fall des obigen Diagramms würden Sie 9 statt 5 Cluster auswählen. <br><br>  Zum Vergleich: Im „einfachen“ Fall ähnelt das Silhouettendiagramm dem folgenden.  Nicht ganz wie bei uns, aber fast. <br><br><img src="https://habrastorage.org/webt/18/yw/uj/18ywujz8uh4q5hhnhtxlzrgs1nm.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Sailors</a></i> <br><br><pre> <code class="sql hljs">ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</code> </pre> <br><img src="https://habrastorage.org/webt/vk/f1/fl/vkf1fln-v-nedwuh6rzbjkxz2pg.png"><br><br>  Das Diagramm der Silhouette-Breite zeigt uns: Je mehr Sie den Datensatz teilen, desto klarer werden die Cluster.  Am Ende erreichen Sie jedoch einzelne Punkte, die Sie nicht benötigen.  Dies ist jedoch genau das, was Sie sehen werden, wenn Sie beginnen, die Anzahl der Cluster <i>k</i> zu erhöhen.  Zum Beispiel habe ich für <code>k=30</code> das folgende Diagramm erhalten: <br><br><img src="https://habrastorage.org/webt/sz/nq/sy/sznqsykdros9uf8clfabfg8yb94.png"><br><br>  Zusammenfassend: Je mehr Sie den Datensatz teilen, desto besser sind die Cluster, aber wir können keine einzelnen Punkte erreichen (in der obigen Tabelle haben wir beispielsweise 30 Cluster ausgewählt und wir haben nur 200 Datenpunkte). <br><br>  Agglomeratives Clustering scheint mir in unserem Fall also viel ausgewogener zu sein: Clustergrößen sind mehr oder weniger vergleichbar (sehen Sie sich nur einen Cluster mit nur zwei Beobachtungen an, wenn Sie nach der Divisionsmethode dividieren!), Und ich würde bei 7 Clustern aufhören, die mit dieser Methode erhalten wurden.  Mal sehen, wie sie aussehen und woraus sie bestehen. <br><br>  Der Datensatz besteht aus 6 Variablen, die in 2D oder 3D visualisiert werden müssen, sodass Sie hart arbeiten müssen!  Die Art der kategorialen Daten unterliegt auch einigen Einschränkungen, sodass vorgefertigte Lösungen möglicherweise nicht funktionieren.  Ich muss: a) sehen, wie die Beobachtungen in Cluster unterteilt sind, b) verstehen, wie die Beobachtungen kategorisiert werden.  Daher habe ich a) ein Farbdendrogramm, b) eine Wärmekarte der Anzahl der Beobachtungen pro Variable in jedem Cluster erstellt. <br><br><pre> <code class="sql hljs">library("ggplot2") library("reshape2") library("purrr") library("dplyr") <span class="hljs-comment"><span class="hljs-comment">#    library("dendextend") dendro &lt;- as.dendrogram(aggl.clust.c) dendro.col &lt;- dendro %&gt;% set("branches_k_color", k = 7, value = c("darkslategray", "darkslategray4", "darkslategray3", "gold3", "darkcyan", "cyan3", "gold3")) %&gt;% set("branches_lwd", 0.6) %&gt;% set("labels_colors", value = c("darkslategray")) %&gt;% set("labels_cex", 0.5) ggd1 &lt;- as.ggdend(dendro.col) ggplot(ggd1, theme = theme_minimal()) + labs(x = "Num. observations", y = "Height", title = "Dendrogram, k = 7")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/iy/hf/jx/iyhfjxt9q7vztvwbaazmqlzzno0.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#     ( ) ggplot(ggd1, labels = T) + scale_y_reverse(expand = c(0.2, 0)) + coord_polar(theta="x")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/if/4g/yv/if4gyv42vtgecjd9n-b_0bb91rs.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  —   #    —       #    ,      clust.num &lt;- cutree(aggl.clust.c, k = 7) synthetic.customers.cl &lt;- cbind(synthetic.customers, clust.num) cust.long &lt;- melt(data.frame(lapply(synthetic.customers.cl, as.character), stringsAsFactors=FALSE), id = c("id.s", "clust.num"), factorsAsStrings=T) cust.long.q &lt;- cust.long %&gt;% group_by(clust.num, variable, value) %&gt;% mutate(count = n_distinct(id.s)) %&gt;% distinct(clust.num, variable, value, count) # heatmap.c ,      — ,   ,     heatmap.c &lt;- ggplot(cust.long.q, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat","sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = count))+ scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") #            cust.long.p &lt;- cust.long.q %&gt;% group_by(clust.num, variable) %&gt;% mutate(perc = count / sum(count)) %&gt;% arrange(clust.num) heatmap.p &lt;- ggplot(cust.long.p, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat", "sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = perc), alpha = 0.85)+ labs(title = "Distribution of characteristics across clusters", x = "Cluster number", y = NULL) + geom_hline(yintercept = 3.5) + geom_hline(yintercept = 10.5) + geom_hline(yintercept = 13.5) + geom_hline(yintercept = 17.5) + geom_hline(yintercept = 21.5) + scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") heatmap.p</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/c5/gg/y5/c5ggy5vih07qcfi4h26mgcvkfgy.png"><br><br>  Die Wärmekarte zeigt grafisch, wie viele Beobachtungen für jede Faktorstufe für die Anfangsfaktoren (die Variablen, mit denen wir begonnen haben) gemacht werden.  Die dunkelblaue Farbe entspricht einer relativ großen Anzahl von Beobachtungen innerhalb des Clusters.  Diese Wärmekarte zeigt auch, dass für den Wochentag (Sonne, Samstag, Montag) und die Korbgröße (groß, mittel, klein) die Anzahl der Kunden in jeder Zelle nahezu gleich ist - dies kann bedeuten, dass diese Kategorien für die Analyse nicht bestimmend sind, und Vielleicht müssen sie nicht berücksichtigt werden. <br><br><h2>  Fazit </h2><br>  In diesem Artikel haben wir die Unähnlichkeitsmatrix berechnet, die agglomerativen und Divisionsmethoden der hierarchischen Clusterbildung getestet und uns mit den Ellbogen- und Silhouettenmethoden zur Bewertung der Qualität von Clustern vertraut gemacht. <br><br>  Divisions- und agglomeratives hierarchisches Clustering ist ein guter Anfang, um das Thema zu untersuchen, aber hören Sie hier nicht auf, wenn Sie die Clusteranalyse wirklich beherrschen möchten.  Es gibt viele andere Methoden und Techniken.  Der Hauptunterschied zum Clustering numerischer Daten besteht in der Berechnung der Unähnlichkeitsmatrix.  Bei der Beurteilung der Clusterqualität liefern nicht alle Standardmethoden zuverlässige und aussagekräftige Ergebnisse - die Silhouette-Methode ist höchstwahrscheinlich nicht geeignet. <br><br>  Und schließlich, da einige Zeit vergangen ist, seit ich dieses Beispiel gemacht habe, sehe ich jetzt eine Reihe von Mängeln in meinem Ansatz und freue mich über jedes Feedback.  Eines der wesentlichen Probleme meiner Analyse hing nicht mit dem Clustering als solchem ​​zusammen - <i>mein Datensatz war</i> in vielerlei Hinsicht <i>unausgewogen</i> , und dieser Moment blieb unberücksichtigt.  Dies hatte spürbare Auswirkungen auf das Clustering: 70% der Kunden gehörten einer Ebene des Faktors „Staatsbürgerschaft“ an, und diese Gruppe dominierte die meisten der erhaltenen Cluster, sodass es schwierig war, die Unterschiede innerhalb anderer Ebenen des Faktors zu berechnen.  Das nächste Mal werde ich versuchen, den Datensatz auszugleichen und die Clustering-Ergebnisse zu vergleichen.  Aber mehr dazu in einem anderen Beitrag. <br><br>  Wenn Sie meinen Code klonen möchten, finden Sie hier den Link zu github: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/khunreus/cluster-categorical</a> <br>  Ich hoffe dir hat dieser Artikel gefallen! <br><br><h3>  <i>Quellen, die mir geholfen haben:</i> </h3><br>  Hierarchisches Clustering-Handbuch (Datenaufbereitung, Clustering, Visualisierung) - Dieser Blog ist interessant für diejenigen, die sich für Business Analytics in der R-Umgebung interessieren: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://uc-r.github.io/hc_clustering</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https: // uc-r. github.io/kmeans_clustering</a> <br><br>  Clustering: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/</a> <br><br>    (   k-): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/</a> <br><br>    denextend,        : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#the-set-function</a> <br><br>    ,   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/</a> <br><br>     : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://jcoliver.github.io/learn-r/008-ggplot-dendrograms-and-heatmaps.html</a> <br><br>       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025633/</a> (  GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/khunreus/EnsCat</a> ). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461741/">https://habr.com/ru/post/de461741/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461731/index.html">DPKI: Beseitigung der Nachteile einer zentralisierten PKI durch Blockchain</a></li>
<li><a href="../de461733/index.html">Englisch lernen: 9 amerikanische Redewendungen</a></li>
<li><a href="../de461735/index.html">FFmpeg DXVA2 Hardware-Dekodierungspraxis</a></li>
<li><a href="../de461737/index.html">Wir sammeln die Umgebung für modernes TDD auf JavaScript + VS-Code</a></li>
<li><a href="../de461739/index.html">Backend United 4: Okroshka. Vorfälle</a></li>
<li><a href="../de461743/index.html">Sicherheitswoche 31: VLC-Sicherheitslücke und defektes Telefon</a></li>
<li><a href="../de461745/index.html">DeviceLock DLP: Preise des russischen Schwarzmarktes für das Durchbrechen personenbezogener Daten (plus eine Antwort auf die Antwort der Tinkoff Bank)</a></li>
<li><a href="../de461747/index.html">Wie wir ML in einer Anwendung mit fast 50 Millionen Benutzern implementiert haben. Sberbank Erfahrung</a></li>
<li><a href="../de461749/index.html">Schönheit im Auge des Betrachters</a></li>
<li><a href="../de461751/index.html">Designerbeitrag zur Entwicklung mobiler Apps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>