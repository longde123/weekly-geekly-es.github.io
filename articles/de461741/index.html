<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚úñÔ∏è ‚å®Ô∏è üñêüèæ Hierarchisches Clustering kategorialer Daten in R. üêå üë©üèΩ‚Äçüé§ üöê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die √úbersetzung wurde f√ºr Studenten des Kurses ‚ÄûApplied Analytics on R‚Äú erstellt . 




 Dies war mein erster Versuch, Clients basierend auf realen Da...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hierarchisches Clustering kategorialer Daten in R.</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/otus/blog/461741/">  <i>Die √úbersetzung wurde f√ºr Studenten des Kurses <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚ÄûApplied Analytics on R‚Äú erstellt</a> .</i> <br><br><img src="https://habrastorage.org/webt/wq/q0/sp/wqq0sphqihtnsg1f8eor15ffkgi.png"><br><hr><br><br>  Dies war mein erster Versuch, Clients basierend auf realen Daten zu gruppieren, und es gab mir wertvolle Erfahrungen.  Es gibt viele Artikel im Internet √ºber das Clustering mit numerischen Variablen, aber es war nicht so einfach, L√∂sungen f√ºr kategoriale Daten zu finden, was etwas schwieriger ist.  Clustering-Methoden f√ºr kategoriale Daten befinden sich noch in der Entwicklung, und in einem anderen Beitrag werde ich einen anderen ausprobieren. <br><a name="habracut"></a><br>  Auf der anderen Seite denken viele Menschen, dass das Clustering kategorialer Daten m√∂glicherweise keine aussagekr√§ftigen Ergebnisse <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">liefert</a> - und dies ist teilweise richtig (siehe die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ausgezeichnete Diskussion zu CrossValidated</a> ).  Irgendwann dachte ich: ‚ÄûWas mache ich?  Sie k√∂nnen einfach in Kohorten unterteilt werden. ‚Äú  Eine Kohortenanalyse ist jedoch auch nicht immer ratsam, insbesondere bei einer signifikanten Anzahl von kategorialen Variablen mit einer gro√üen Anzahl von Ebenen: Sie k√∂nnen problemlos mit 5-7 Kohorten umgehen, aber wenn Sie 22 Variablen haben und jede 5 Ebenen hat (z. B. eine Kundenumfrage mit diskreten Sch√§tzungen 1) , 2, 3, 4 und 5), und Sie m√ºssen verstehen, mit welchen charakteristischen Gruppen von Kunden Sie es zu tun haben - Sie erhalten 22x5 Kohorten.  Niemand m√∂chte sich mit einer solchen Aufgabe besch√§ftigen.  Und hier k√∂nnte Clustering helfen.  In diesem Beitrag werde ich dar√ºber sprechen, was ich selbst wissen m√∂chte, sobald ich mit dem Clustering begonnen habe. <br><br>  Der Clustering-Prozess selbst besteht aus drei Schritten: <br><br><ol><li>  Der Aufbau einer Matrix der Un√§hnlichkeit ist zweifellos die wichtigste Entscheidung beim Clustering.  Alle nachfolgenden Schritte basieren auf der von Ihnen erstellten Un√§hnlichkeitsmatrix. </li><li>  Die Wahl der Clustering-Methode. </li><li>  Cluster-Evaluierung. </li></ol><br>  Dieser Beitrag ist eine Art Einf√ºhrung, die die Grundprinzipien des Clustering und seiner Implementierung in der Umgebung R beschreibt. <br><br><h2>  Un√§hnlichkeitsmatrix </h2><br>  Die Basis f√ºr das Clustering wird die Un√§hnlichkeitsmatrix sein, die mathematisch beschreibt, wie unterschiedlich die Punkte im Datensatz voneinander entfernt sind.  Sie k√∂nnen die Punkte, die am n√§chsten beieinander liegen, weiter in Gruppen zusammenfassen oder die am weitesten voneinander entfernten Punkte voneinander trennen - dies ist die Hauptidee der Clusterbildung. <br><br>  In diesem Stadium sind Unterschiede zwischen Datentypen wichtig, da die Un√§hnlichkeitsmatrix auf den Abst√§nden zwischen einzelnen Datenpunkten basiert.  Die Abst√§nde zwischen den Punkten numerischer Daten sind leicht vorstellbar (ein bekanntes Beispiel sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">euklidische Abst√§nde</a> ), aber bei kategorialen Daten (Faktoren in R) ist nicht alles so offensichtlich. <br><br>  Um in diesem Fall eine Un√§hnlichkeitsmatrix zu erstellen, sollte der sogenannte Gover-Abstand verwendet werden.  Ich werde nicht auf den mathematischen Teil dieses Konzepts eingehen, sondern nur Links bereitstellen: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">da</a> .  Aus diesem <code>metric = c("gower")</code> bevorzuge ich die Verwendung von <code>daisy()</code> mit der <code>metric = c("gower")</code> aus dem <code>cluster</code> . <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#-----   -----# #    ,       ,     ,   ,    library(dplyr) #     set.seed(40) #     #    ;   data.frame()     #    ,   200   1  200 id.s &lt;- c(1:200) %&gt;% factor() budget.s &lt;- sample(c("small", "med", "large"), 200, replace = T) %&gt;% factor(levels=c("small", "med", "large"), ordered = TRUE) origins.s &lt;- sample(c("x", "y", "z"), 200, replace = T, prob = c(0.7, 0.15, 0.15)) area.s &lt;- sample(c("area1", "area2", "area3", "area4"), 200, replace = T, prob = c(0.3, 0.1, 0.5, 0.2)) source.s &lt;- sample(c("facebook", "email", "link", "app"), 200, replace = T, prob = c(0.1,0.2, 0.3, 0.4)) ##   ‚Äî      dow.s &lt;- sample(c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), 200, replace = T, prob = c(0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2)) %&gt;% factor(levels=c("mon", "tue", "wed", "thu", "fri", "sat", "sun"), ordered = TRUE) #  dish.s &lt;- sample(c("delicious", "the one you don't like", "pizza"), 200, replace = T) #   data.frame()      synthetic.customers &lt;- data.frame(id.s, budget.s, origins.s, area.s, source.s, dow.s, dish.s) #-----   -----# library(cluster) #       #   : daisy(), diana(), clusplot() gower.dist &lt;- daisy(synthetic.customers[ ,2:7], metric = c("gower")) # class(gower.dist) ## , </span></span></code> </pre> <br>  Die Un√§hnlichkeitsmatrix ist fertig.  F√ºr 200 Beobachtungen wird es schnell erstellt, erfordert jedoch m√∂glicherweise einen sehr gro√üen Rechenaufwand, wenn Sie mit einem gro√üen Datensatz arbeiten. <br><br>  In der Praxis ist es sehr wahrscheinlich, dass Sie zuerst den Datensatz bereinigen, die erforderlichen Transformationen aus den Zeilen in Faktoren durchf√ºhren und die fehlenden Werte verfolgen m√ºssen.  In meinem Fall enthielt der Datensatz auch Zeilen mit fehlenden Werten, die jedes Mal wundersch√∂n gruppiert wurden. Es schien also ein Schatz zu sein - bis ich mir die Werte ansah (leider!). <br><br><h2>  Clustering-Algorithmen </h2><br>  M√∂glicherweise wissen Sie bereits, dass Clustering <i>k-means und hierarchisch ist</i> .  In diesem Beitrag konzentriere ich mich auf die zweite Methode, da sie flexibler ist und verschiedene Ans√§tze erm√∂glicht: Sie k√∂nnen entweder einen <i>agglomerativen</i> (von unten nach oben) oder einen <i>abteilungsweisen</i> (von oben nach unten) Clustering-Algorithmus w√§hlen. <br><br><img src="https://habrastorage.org/webt/nl/vp/u4/nlvpu4e8ykoh_nd_el_4i6plh8q.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Programmierhandbuch f√ºr UC Business Analytics R.</a></i> <br><br>  Agglomeratives Clustering beginnt mit <code>n</code> Clustern, wobei <code>n</code> die Anzahl der Beobachtungen ist: Es wird angenommen, dass jeder von ihnen ein separater Cluster ist.  Dann versucht der Algorithmus, die √§hnlichsten Datenpunkte untereinander zu finden und zu gruppieren - so beginnt die Clusterbildung. <br><br>  Divisional Clustering wird in umgekehrter Weise durchgef√ºhrt - es wird zun√§chst angenommen, dass alle n Datenpunkte, die wir haben, ein gro√üer Cluster sind, und dann werden die am wenigsten √§hnlichen in separate Gruppen unterteilt. <br><br>  Bei der Entscheidung, welche dieser Methoden ausgew√§hlt werden soll, ist es immer sinnvoll, alle Optionen auszuprobieren. Im Allgemeinen <i>eignet sich agglomeratives Clustering jedoch besser zur Identifizierung kleiner Cluster und wird von den meisten Computerprogrammen verwendet, und Divisionsclustering ist besser zur Identifizierung gro√üer Cluster geeignet</i> . <br><br>  Bevor ich mich f√ºr eine Methode entscheide, schaue ich mir lieber Dendrogramme an - eine grafische Darstellung von Clustering.  Wie Sie sp√§ter sehen werden, sind einige Dendrogramme gut ausbalanciert, w√§hrend andere sehr chaotisch sind. <br><br>  # Die Haupteingabe f√ºr den folgenden Code ist die Un√§hnlichkeit (Distanzmatrix) <br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#             #            ‚Äî         ‚Äî    #------------  ------------# divisive.clust &lt;- diana(as.matrix(gower.dist), diss = TRUE, keep.diss = TRUE) plot(divisive.clust, main = "Divisive")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/54/mp/m1/54mpm19v8jkkpmj6usehxlgr5qk.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#------------   ------------# #      #         ‚Äî     ,      #    (complete linkages) aggl.clust.c &lt;- hclust(gower.dist, method = "complete") plot(aggl.clust.c, main = "Agglomerative, complete linkages")</span></span></code> </pre> <br><h2>  Bewertung der Clusterqualit√§t </h2><br>  In diesem Stadium muss zwischen verschiedenen Clustering-Algorithmen und einer unterschiedlichen Anzahl von Clustern gew√§hlt werden.  Sie k√∂nnen verschiedene Bewertungsmethoden anwenden, ohne zu vergessen, sich vom <b>gesunden Menschenverstand</b> leiten zu lassen.  Ich habe diese W√∂rter fett und kursiv hervorgehoben, da die Aussagekraft der Auswahl <b>sehr wichtig ist</b> - die Anzahl der Cluster und die Methode zur Aufteilung von Daten in Gruppen sollten aus praktischer Sicht praktisch sein.  Die Anzahl der Kombinationen von Werten kategorialer Variablen ist endlich (da sie diskret sind), aber keine darauf basierende Aufschl√ºsselung ist sinnvoll.  M√∂glicherweise m√∂chten Sie auch nicht sehr wenige Cluster haben - in diesem Fall sind sie zu verallgemeinert.  Am Ende h√§ngt alles von Ihrem Ziel und den Aufgaben der Analyse ab. <br><br>  Im Allgemeinen m√∂chten Sie beim Erstellen von Clustern klar definierte Gruppen von Datenpunkten erhalten, damit der Abstand zwischen solchen Punkten innerhalb des Clusters ( <i>oder die Kompaktheit</i> ) minimal und der Abstand zwischen Gruppen ( <i>Trennbarkeit</i> ) maximal m√∂glich ist.  Dies ist intuitiv leicht zu verstehen: Der Abstand zwischen Punkten ist ein Ma√ü f√ºr ihre Un√§hnlichkeit, die auf der Grundlage der Un√§hnlichkeitsmatrix erhalten wird.  Daher basiert die Bewertung der Qualit√§t der Clusterbildung auf der Bewertung der Kompaktheit und Trennbarkeit. <br><br>  Als n√§chstes werde ich zwei Ans√§tze demonstrieren und zeigen, dass einer von ihnen bedeutungslose Ergebnisse liefern kann. <br><br><ul><li>  <i>Ellbogenmethode</i> : Beginnen Sie damit, wenn der wichtigste Faktor f√ºr Ihre Analyse die Kompaktheit der Cluster ist, d. H. Die √Ñhnlichkeit innerhalb der Gruppen. </li><li>  <i>Bewertungsmethode f√ºr Silhouetten</i> : Das als Ma√ü f√ºr die Datenkonsistenz verwendete <i>Silhouettendiagramm</i> zeigt, wie nahe die einzelnen Punkte innerhalb eines Clusters an den Punkten in benachbarten Clustern liegen. </li></ul><br>  In der Praxis f√ºhren diese beiden Methoden h√§ufig zu unterschiedlichen Ergebnissen, was zu Verwirrung f√ºhren kann. Die maximale Kompaktheit und die klarste Trennung werden mit einer unterschiedlichen Anzahl von Clustern erreicht, sodass der gesunde Menschenverstand und das Verst√§ndnis dessen, was Ihre Daten wirklich bedeuten, eine wichtige Rolle spielen bei der endg√ºltigen Entscheidung. <br><br>  Es gibt auch eine Reihe von Metriken, die Sie analysieren k√∂nnen.  Ich werde sie direkt zum Code hinzuf√ºgen. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#      ,        #      ,     ,   ‚Äî   #     ,      ,         ,   ,     library(fpc) cstats.table &lt;- function(dist, tree, k) { clust.assess &lt;- c("cluster.number","n","within.cluster.ss","average.within","average.between", "wb.ratio","dunn2","avg.silwidth") clust.size &lt;- c("cluster.size") stats.names &lt;- c() row.clust &lt;- c() output.stats &lt;- matrix(ncol = k, nrow = length(clust.assess)) cluster.sizes &lt;- matrix(ncol = k, nrow = k) for(i in c(1:k)){ row.clust[i] &lt;- paste("Cluster-", i, " size") } for(i in c(2:k)){ stats.names[i] &lt;- paste("Test", i-1) for(j in seq_along(clust.assess)){ output.stats[j, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.assess])[j] } for(d in 1:k) { cluster.sizes[d, i] &lt;- unlist(cluster.stats(d = dist, clustering = cutree(tree, k = i))[clust.size])[d] dim(cluster.sizes[d, i]) &lt;- c(length(cluster.sizes[i]), 1) cluster.sizes[d, i] } } output.stats.df &lt;- data.frame(output.stats) cluster.sizes &lt;- data.frame(cluster.sizes) cluster.sizes[is.na(cluster.sizes)] &lt;- 0 rows.all &lt;- c(clust.assess, row.clust) # rownames(output.stats.df) &lt;- clust.assess output &lt;- rbind(output.stats.df, cluster.sizes)[ ,-1] colnames(output) &lt;- stats.names[2:k] rownames(output) &lt;- rows.all is.num &lt;- sapply(output, is.numeric) output[is.num] &lt;- lapply(output[is.num], round, 2) output } #     :      7 #     ,            stats.df.divisive &lt;- cstats.table(gower.dist, divisive.clust, 7) stats.df.divisive</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/r-/g_/ou/r-g_oukwyorhqnsls_cbg4c8spw.png"><br><br>  Der Mittelwert innerhalb des Indikators, der den durchschnittlichen Abstand zwischen Beobachtungen innerhalb von Clustern darstellt, nimmt ebenso ab wie innerhalb von Cluster.ss (die Summe der Quadrate der Abst√§nde zwischen Beobachtungen in einem Cluster).  Die durchschnittliche Breite der Silhouette (durchschnittliche Silberbreite) variiert nicht so eindeutig, es ist jedoch immer noch eine umgekehrte Beziehung zu erkennen. <br>  Beachten Sie, wie unverh√§ltnism√§√üig die Clustergr√∂√üen sind.  Ich w√ºrde mich nicht beeilen, mit einer unvergleichlichen Anzahl von Beobachtungen innerhalb von Clustern zu arbeiten.  Einer der Gr√ºnde ist, dass der Datensatz m√∂glicherweise unausgewogen ist und einige Beobachtungsgruppen alle anderen in der Analyse √ºberwiegen - dies ist nicht gut und f√ºhrt h√∂chstwahrscheinlich zu Fehlern. <br><br> <code>stats.df.aggl &lt;-cstats.table(gower.dist, aggl.clust.c, 7) #      </code> <br> <br> <code>stats.df.aggl</code> <br> <br><img src="https://habrastorage.org/webt/a_/-u/aa/a_-uaa_nff99nuyobulroyk_hka.png"><br><br>  Beachten Sie, wie viel besser die Anzahl der Beobachtungen pro Gruppe durch agglomeratives hierarchisches Clustering auf der Grundlage der vollst√§ndigen Kommunikationsmethode ausgeglichen wird. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># ---------    ---------# #   ¬´¬ª       #    ,     7  library(ggplot2) #  #   ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/kw/kz/xy/kwkzxyuuzwhe0yst9kteg9inias.png"><br><br>  Also haben wir ein Diagramm des "Ellbogens" erstellt.  Es zeigt, wie die Summe der quadratischen Abst√§nde zwischen den Beobachtungen (wir verwenden sie als Ma√ü f√ºr die N√§he der Beobachtungen - je kleiner sie sind, desto n√§her sind die Messungen innerhalb des Clusters) f√ºr eine unterschiedliche Anzahl von Clustern variieren.  Idealerweise sollten wir an der Stelle, an der eine weitere Clusterbildung nur eine geringf√ºgige Abnahme der Quadratsumme (SS) ergibt, eine deutliche ‚ÄûEllbogenbiegung‚Äú sehen.  F√ºr die folgende Grafik w√ºrde ich bei ungef√§hr 7 anhalten. Obwohl in diesem Fall einer der Cluster nur aus zwei Beobachtungen besteht.  Mal sehen, was beim agglomerativen Clustering passiert. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#       ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=within.cluster.ss)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Within clusters sum of squares (SS)") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/y0/ck/q-/y0ckq-zxtzg0fbjr9gcq1jgorvq.png"><br><br>  Agglomerativer ‚ÄûEllbogen‚Äú √§hnelt dem Teilungsbogen, aber die Grafik sieht glatter aus - Biegungen sind nicht so ausgepr√§gt.  Wie beim Divisionsclustering w√ºrde ich mich auf 7 Cluster konzentrieren. Wenn ich jedoch zwischen diesen beiden Methoden w√§hle, mag ich die Clustergr√∂√üen, die durch die agglomerative Methode erhalten werden, mehr - es ist besser, dass sie miteinander vergleichbar sind. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  ggplot(data = data.frame(t(cstats.table(gower.dist, divisive.clust, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Divisive clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/u9/nj/nf/u9njnfcjqbxbzlfgpl5sxqailra.png"><br><br>  Wenn Sie die Silhouette-Sch√§tzmethode verwenden, sollten Sie den Betrag ausw√§hlen, der den maximalen Silhouette-Koeffizienten ergibt, da Sie Cluster ben√∂tigen, die weit genug voneinander entfernt sind, um als getrennt betrachtet zu werden. <br><br>  Der Silhouette-Koeffizient kann zwischen ‚Äì1 und 1 liegen, wobei 1 einer guten Konsistenz innerhalb der Cluster entspricht und ‚Äì1 nicht sehr gut. <br>  Im Fall des obigen Diagramms w√ºrden Sie 9 statt 5 Cluster ausw√§hlen. <br><br>  Zum Vergleich: Im ‚Äûeinfachen‚Äú Fall √§hnelt das Silhouettendiagramm dem folgenden.  Nicht ganz wie bei uns, aber fast. <br><br><img src="https://habrastorage.org/webt/18/yw/uj/18ywujz8uh4q5hhnhtxlzrgs1nm.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Sailors</a></i> <br><br><pre> <code class="sql hljs">ggplot(data = data.frame(t(cstats.table(gower.dist, aggl.clust.c, 15))), aes(x=cluster.number, y=avg.silwidth)) + geom_point()+ geom_line()+ ggtitle("Agglomerative clustering") + labs(x = "Num.of clusters", y = "Average silhouette width") + theme(plot.title = element_text(hjust = 0.5))</code> </pre> <br><img src="https://habrastorage.org/webt/vk/f1/fl/vkf1fln-v-nedwuh6rzbjkxz2pg.png"><br><br>  Das Diagramm der Silhouette-Breite zeigt uns: Je mehr Sie den Datensatz teilen, desto klarer werden die Cluster.  Am Ende erreichen Sie jedoch einzelne Punkte, die Sie nicht ben√∂tigen.  Dies ist jedoch genau das, was Sie sehen werden, wenn Sie beginnen, die Anzahl der Cluster <i>k</i> zu erh√∂hen.  Zum Beispiel habe ich f√ºr <code>k=30</code> das folgende Diagramm erhalten: <br><br><img src="https://habrastorage.org/webt/sz/nq/sy/sznqsykdros9uf8clfabfg8yb94.png"><br><br>  Zusammenfassend: Je mehr Sie den Datensatz teilen, desto besser sind die Cluster, aber wir k√∂nnen keine einzelnen Punkte erreichen (in der obigen Tabelle haben wir beispielsweise 30 Cluster ausgew√§hlt und wir haben nur 200 Datenpunkte). <br><br>  Agglomeratives Clustering scheint mir in unserem Fall also viel ausgewogener zu sein: Clustergr√∂√üen sind mehr oder weniger vergleichbar (sehen Sie sich nur einen Cluster mit nur zwei Beobachtungen an, wenn Sie nach der Divisionsmethode dividieren!), Und ich w√ºrde bei 7 Clustern aufh√∂ren, die mit dieser Methode erhalten wurden.  Mal sehen, wie sie aussehen und woraus sie bestehen. <br><br>  Der Datensatz besteht aus 6 Variablen, die in 2D oder 3D visualisiert werden m√ºssen, sodass Sie hart arbeiten m√ºssen!  Die Art der kategorialen Daten unterliegt auch einigen Einschr√§nkungen, sodass vorgefertigte L√∂sungen m√∂glicherweise nicht funktionieren.  Ich muss: a) sehen, wie die Beobachtungen in Cluster unterteilt sind, b) verstehen, wie die Beobachtungen kategorisiert werden.  Daher habe ich a) ein Farbdendrogramm, b) eine W√§rmekarte der Anzahl der Beobachtungen pro Variable in jedem Cluster erstellt. <br><br><pre> <code class="sql hljs">library("ggplot2") library("reshape2") library("purrr") library("dplyr") <span class="hljs-comment"><span class="hljs-comment">#    library("dendextend") dendro &lt;- as.dendrogram(aggl.clust.c) dendro.col &lt;- dendro %&gt;% set("branches_k_color", k = 7, value = c("darkslategray", "darkslategray4", "darkslategray3", "gold3", "darkcyan", "cyan3", "gold3")) %&gt;% set("branches_lwd", 0.6) %&gt;% set("labels_colors", value = c("darkslategray")) %&gt;% set("labels_cex", 0.5) ggd1 &lt;- as.ggdend(dendro.col) ggplot(ggd1, theme = theme_minimal()) + labs(x = "Num. observations", y = "Height", title = "Dendrogram, k = 7")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/iy/hf/jx/iyhfjxt9q7vztvwbaazmqlzzno0.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#     ( ) ggplot(ggd1, labels = T) + scale_y_reverse(expand = c(0.2, 0)) + coord_polar(theta="x")</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/if/4g/yv/if4gyv42vtgecjd9n-b_0bb91rs.png"><br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">#  ‚Äî   #    ‚Äî       #    ,      clust.num &lt;- cutree(aggl.clust.c, k = 7) synthetic.customers.cl &lt;- cbind(synthetic.customers, clust.num) cust.long &lt;- melt(data.frame(lapply(synthetic.customers.cl, as.character), stringsAsFactors=FALSE), id = c("id.s", "clust.num"), factorsAsStrings=T) cust.long.q &lt;- cust.long %&gt;% group_by(clust.num, variable, value) %&gt;% mutate(count = n_distinct(id.s)) %&gt;% distinct(clust.num, variable, value, count) # heatmap.c ,      ‚Äî ,   ,     heatmap.c &lt;- ggplot(cust.long.q, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat","sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = count))+ scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") #            cust.long.p &lt;- cust.long.q %&gt;% group_by(clust.num, variable) %&gt;% mutate(perc = count / sum(count)) %&gt;% arrange(clust.num) heatmap.p &lt;- ggplot(cust.long.p, aes(x = clust.num, y = factor(value, levels = c("x","y","z", "mon", "tue", "wed", "thu", "fri","sat", "sun", "delicious", "the one you don't like", "pizza", "facebook", "email", "link", "app", "area1", "area2", "area3", "area4", "small", "med", "large"), ordered = T))) + geom_tile(aes(fill = perc), alpha = 0.85)+ labs(title = "Distribution of characteristics across clusters", x = "Cluster number", y = NULL) + geom_hline(yintercept = 3.5) + geom_hline(yintercept = 10.5) + geom_hline(yintercept = 13.5) + geom_hline(yintercept = 17.5) + geom_hline(yintercept = 21.5) + scale_fill_gradient2(low = "darkslategray1", mid = "yellow", high = "turquoise4") heatmap.p</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/c5/gg/y5/c5ggy5vih07qcfi4h26mgcvkfgy.png"><br><br>  Die W√§rmekarte zeigt grafisch, wie viele Beobachtungen f√ºr jede Faktorstufe f√ºr die Anfangsfaktoren (die Variablen, mit denen wir begonnen haben) gemacht werden.  Die dunkelblaue Farbe entspricht einer relativ gro√üen Anzahl von Beobachtungen innerhalb des Clusters.  Diese W√§rmekarte zeigt auch, dass f√ºr den Wochentag (Sonne, Samstag, Montag) und die Korbgr√∂√üe (gro√ü, mittel, klein) die Anzahl der Kunden in jeder Zelle nahezu gleich ist - dies kann bedeuten, dass diese Kategorien f√ºr die Analyse nicht bestimmend sind, und Vielleicht m√ºssen sie nicht ber√ºcksichtigt werden. <br><br><h2>  Fazit </h2><br>  In diesem Artikel haben wir die Un√§hnlichkeitsmatrix berechnet, die agglomerativen und Divisionsmethoden der hierarchischen Clusterbildung getestet und uns mit den Ellbogen- und Silhouettenmethoden zur Bewertung der Qualit√§t von Clustern vertraut gemacht. <br><br>  Divisions- und agglomeratives hierarchisches Clustering ist ein guter Anfang, um das Thema zu untersuchen, aber h√∂ren Sie hier nicht auf, wenn Sie die Clusteranalyse wirklich beherrschen m√∂chten.  Es gibt viele andere Methoden und Techniken.  Der Hauptunterschied zum Clustering numerischer Daten besteht in der Berechnung der Un√§hnlichkeitsmatrix.  Bei der Beurteilung der Clusterqualit√§t liefern nicht alle Standardmethoden zuverl√§ssige und aussagekr√§ftige Ergebnisse - die Silhouette-Methode ist h√∂chstwahrscheinlich nicht geeignet. <br><br>  Und schlie√ülich, da einige Zeit vergangen ist, seit ich dieses Beispiel gemacht habe, sehe ich jetzt eine Reihe von M√§ngeln in meinem Ansatz und freue mich √ºber jedes Feedback.  Eines der wesentlichen Probleme meiner Analyse hing nicht mit dem Clustering als solchem ‚Äã‚Äãzusammen - <i>mein Datensatz war</i> in vielerlei Hinsicht <i>unausgewogen</i> , und dieser Moment blieb unber√ºcksichtigt.  Dies hatte sp√ºrbare Auswirkungen auf das Clustering: 70% der Kunden geh√∂rten einer Ebene des Faktors ‚ÄûStaatsb√ºrgerschaft‚Äú an, und diese Gruppe dominierte die meisten der erhaltenen Cluster, sodass es schwierig war, die Unterschiede innerhalb anderer Ebenen des Faktors zu berechnen.  Das n√§chste Mal werde ich versuchen, den Datensatz auszugleichen und die Clustering-Ergebnisse zu vergleichen.  Aber mehr dazu in einem anderen Beitrag. <br><br>  Wenn Sie meinen Code klonen m√∂chten, finden Sie hier den Link zu github: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/khunreus/cluster-categorical</a> <br>  Ich hoffe dir hat dieser Artikel gefallen! <br><br><h3>  <i>Quellen, die mir geholfen haben:</i> </h3><br>  Hierarchisches Clustering-Handbuch (Datenaufbereitung, Clustering, Visualisierung) - Dieser Blog ist interessant f√ºr diejenigen, die sich f√ºr Business Analytics in der R-Umgebung interessieren: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://uc-r.github.io/hc_clustering</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https: // uc-r. github.io/kmeans_clustering</a> <br><br>  Clustering: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://www.sthda.com/english/articles/29-cluster-validation-essentials/97-cluster-validation-statistics-must-know-methods/</a> <br><br>    (   k-): <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/</a> <br><br>    denextend,        : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#the-set-function</a> <br><br>    ,   : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.r-statistics.com/2010/06/clustergram-visualization-and-diagnostics-for-cluster-analysis-r-code/</a> <br><br>     : <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://jcoliver.github.io/learn-r/008-ggplot-dendrograms-and-heatmaps.html</a> <br><br>       ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025633/</a> (  GitHub: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://github.com/khunreus/EnsCat</a> ). </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de461741/">https://habr.com/ru/post/de461741/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de461731/index.html">DPKI: Beseitigung der Nachteile einer zentralisierten PKI durch Blockchain</a></li>
<li><a href="../de461733/index.html">Englisch lernen: 9 amerikanische Redewendungen</a></li>
<li><a href="../de461735/index.html">FFmpeg DXVA2 Hardware-Dekodierungspraxis</a></li>
<li><a href="../de461737/index.html">Wir sammeln die Umgebung f√ºr modernes TDD auf JavaScript + VS-Code</a></li>
<li><a href="../de461739/index.html">Backend United 4: Okroshka. Vorf√§lle</a></li>
<li><a href="../de461743/index.html">Sicherheitswoche 31: VLC-Sicherheitsl√ºcke und defektes Telefon</a></li>
<li><a href="../de461745/index.html">DeviceLock DLP: Preise des russischen Schwarzmarktes f√ºr das Durchbrechen personenbezogener Daten (plus eine Antwort auf die Antwort der Tinkoff Bank)</a></li>
<li><a href="../de461747/index.html">Wie wir ML in einer Anwendung mit fast 50 Millionen Benutzern implementiert haben. Sberbank Erfahrung</a></li>
<li><a href="../de461749/index.html">Sch√∂nheit im Auge des Betrachters</a></li>
<li><a href="../de461751/index.html">Designerbeitrag zur Entwicklung mobiler Apps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>