<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÄ üöß ü•ï Erstellen Sie eine Failover-L√∂sung, die auf der Oracle RAC- und AccelStor Shared-Nothing-Architektur basiert üéõÔ∏è üé¶ üòû</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eine betr√§chtliche Anzahl von Unternehmensanwendungen und Virtualisierungssystemen verf√ºgt √ºber eigene Mechanismen zum Erstellen fehlertoleranter L√∂su...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen Sie eine Failover-L√∂sung, die auf der Oracle RAC- und AccelStor Shared-Nothing-Architektur basiert</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/accelstor/blog/448538/"><p>  Eine betr√§chtliche Anzahl von Unternehmensanwendungen und Virtualisierungssystemen verf√ºgt √ºber eigene Mechanismen zum Erstellen fehlertoleranter L√∂sungen.  Insbesondere ist Oracle RAC (Oracle Real Application Cluster) ein Cluster aus zwei oder mehr Oracle-Datenbankservern, die zusammenarbeiten, um die Last auszugleichen und Fehlertoleranz auf Server- / Anwendungsebene bereitzustellen.  Um in diesem Modus zu arbeiten, ist ein gemeinsamer Speicher erforderlich, dessen Rolle normalerweise der Speicher ist. </p><br><p>  Wie wir bereits in einem unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> besprochen haben, weisen Speichersysteme trotz des Vorhandenseins doppelter Komponenten (einschlie√ülich Steuerungen) immer noch Fehlerquellen auf - haupts√§chlich in Form eines einzelnen Datensatzes.  Um eine Oracle-L√∂sung mit erh√∂hten Zuverl√§ssigkeitsanforderungen zu erstellen, muss das Schema ‚ÄûN Server - ein Speicher‚Äú daher kompliziert sein. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/l3/lh/1t/l3lh1tmhqobqcqow6yxo-g3j8lc.png"></div><br><a name="habracut"></a><br><p>  Zun√§chst m√ºssen Sie nat√ºrlich entscheiden, gegen welche Risiken wir uns versichern m√∂chten.  In diesem Artikel wird der Schutz vor Bedrohungen wie dem Eintreffen eines Meteoriten nicht ber√ºcksichtigt.  Der Aufbau einer geografisch verteilten Disaster Recovery-L√∂sung bleibt daher ein Thema f√ºr einen der folgenden Artikel.  Hier sehen wir uns die sogenannte Cross-Rack-Disaster-Recovery-L√∂sung an, bei der der Schutz auf der Ebene von Serverschr√§nken aufgebaut wird.  Die Schr√§nke selbst k√∂nnen sich im selben Raum oder in verschiedenen R√§umen befinden, normalerweise jedoch im selben Geb√§ude. </p><br><p>  Diese Schr√§nke sollten alle erforderlichen Ger√§te und Software enthalten, die den Betrieb von Oracle-Datenbanken unabh√§ngig vom Status des ‚ÄûNachbarn‚Äú gew√§hrleisten.  Mit anderen Worten, mit der Cross-Rack-Disaster-Recovery-L√∂sung eliminieren wir die Ausfallrisiken: </p><br><ul><li>  Oracle-Anwendungsserver </li><li>  Speichersysteme </li><li>  Vermittlungssysteme </li><li>  Vollst√§ndiger Ausfall aller Ger√§te im Schrank: <br><ul><li>  Stromausfall </li><li>  Ausfall des K√ºhlsystems </li><li>  Externe Faktoren (Mensch, Natur usw.) </li></ul></li></ul><br><p>  Die Duplizierung von Oracle-Servern impliziert das Prinzip von Oracle RAC und wird √ºber die Anwendung implementiert.  Das Duplizieren von Schaltwerkzeugen ist ebenfalls kein Problem.  Aber mit der Verdoppelung des Speichersystems ist nicht alles so einfach. </p><br><p>  Am einfachsten ist es, Daten vom Prim√§rspeicher in die Sicherung zu replizieren.  Abh√§ngig von den Speicherfunktionen synchron oder asynchron.  Die asynchrone Replikation wirft sofort die Frage auf, ob die Datenkonsistenz mit Oracle sichergestellt werden kann.  Aber selbst wenn eine Software in die Anwendung integriert ist, m√ºssen Administratoren im Falle eines Unfalls auf dem Hauptspeichersystem manuell eingreifen, um den Cluster auf den Sicherungsspeicher umzustellen. </p><br><p>  Eine komplexere Option sind Software- und / oder Hardware-Virtualisierer von Speichersystemen, die Probleme mit der Konsistenz und manuellen Eingriffen beseitigen.  Die Komplexit√§t der Bereitstellung und der anschlie√üenden Verwaltung sowie die sehr unanst√§ndigen Kosten solcher L√∂sungen machen vielen jedoch Angst. </p><br><p>  Nur f√ºr Szenarien wie die Cross-Rack-Notfallwiederherstellung ist das All Flash AccelStor NeoSapphire ‚Ñ¢ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">H710-Array</a> mit der Shared-Nothing-Architektur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">perfekt</a> .  Dieses Modell ist ein Zwei-Einzel-Speichersystem, das seine eigene FlexiRemap¬Æ-Technologie f√ºr die Arbeit mit Flash-Laufwerken verwendet.  Dank des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">FlexiRemap¬Æ</a> NeoSapphire ‚Ñ¢ H710 k√∂nnen bis zu 600K IOPS @ 4K Random Write und 1M + IOPS @ 4K Random Read bereitgestellt werden, was mit klassischem RAID-basiertem Speicher nicht m√∂glich ist. </p><br><p>  Das Hauptmerkmal des NeoSapphire ‚Ñ¢ H710 ist jedoch die Ausf√ºhrung von zwei Knoten als separate Geh√§use, von denen jedes eine eigene Kopie der Daten hat.  Die Knotensynchronisation erfolgt √ºber die externe InfiniBand-Schnittstelle.  Dank dieser Architektur k√∂nnen Knoten √ºber Entfernungen von bis zu 100 m auf verschiedene Standorte verteilt werden, wodurch die Cross-Rack-Disaster-Recovery-L√∂sung bereitgestellt wird.  Beide Knoten arbeiten vollst√§ndig im synchronen Modus.  Auf der Hostseite sieht das H710 wie ein gew√∂hnlicher Dual-Controller-Speicher aus.  Daher m√ºssen keine zus√§tzlichen Software- und Hardwareoptionen und besonders komplexen Einstellungen vorgenommen werden. </p><br><p>  Wenn Sie alle oben beschriebenen Cross-Rack-Disaster-Recovery-L√∂sungen vergleichen, hebt sich die AccelStor-Version von den anderen ab: </p><br><table><tbody><tr><th></th><th>  AccelStor NeoSapphire ‚Ñ¢ Shared Nothing-Architektur </th><th>  Software- oder Hardware-Virtualisierer des Speichers </th><th>  Replikationsl√∂sung </th></tr><tr><td colspan="4">  <b>Verf√ºgbarkeit</b> </td></tr><tr><td>  Serverfehler </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td></tr><tr><td>  Switch-Fehler </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td></tr><tr><td>  Speicherfehler </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <font color="green"><b>Ausfallzeiten</b></font> </td></tr><tr><td>  Ausfall des gesamten Schranks </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <b>Keine Ausfallzeiten</b> </td><td>  <font color="green"><b>Ausfallzeiten</b></font> </td></tr><tr><td colspan="4">  <b>Kosten und Komplexit√§t</b> </td></tr><tr><td>  L√∂sungskosten </td><td>  Niedrig * </td><td>  <font color="green">Hoch</font> </td><td>  <font color="green">Hoch</font> </td></tr><tr><td>  Bereitstellungsschwierigkeiten </td><td>  Niedrig </td><td>  <font color="green">Hoch</font> </td><td>  <font color="green">Hoch</font> </td></tr></tbody></table><br><p>  <i>* AccelStor NeoSapphire ‚Ñ¢ ist immer noch ein All-Flash-Array, das per Definition keine ‚Äû3 Kopeken‚Äú kostet, zumal es eine doppelte Kapazit√§tsreserve hat.</i>  <i>Wenn man jedoch die Gesamtkosten der darauf basierenden L√∂sung mit √§hnlichen Kosten anderer Anbieter vergleicht, k√∂nnen die Kosten als niedrig angesehen werden.</i> </p><br><p>  Die Topologie zum Verbinden von Anwendungsservern und allen Flash-Array-Knoten sieht folgenderma√üen aus: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/5i/gb/cw/5igbcwdvm92bpabsylae4iysdra.png"></div><br><p>  Bei der Planung der Topologie wird au√üerdem dringend empfohlen, Verwaltungsschalter und Serververbindungen zu duplizieren. </p><br><p>  Im Folgenden werden wir √ºber die Verbindung √ºber Fibre Channel sprechen.  Bei Verwendung von iSCSI ist alles gleich, angepasst an die verwendeten Switch-Typen und leicht unterschiedliche Array-Einstellungen. </p><br><h3>  Vorarbeiten am Array </h3><br><div class="spoiler">  <b class="spoiler_title">Gebrauchte Hardware und Software</b> <div class="spoiler_text"><p>  <b>Server- und Switch-Spezifikationen</b> </p><br><table><tbody><tr><th>  Komponenten </th><th>  Beschreibung </th></tr><tr><td>  Oracle Database 11g-Server </td><td>  Zwei </td></tr><tr><td>  Server-Betriebssystem </td><td>  Oracle Linux </td></tr><tr><td>  Oracle-Datenbankversion </td><td>  11 g (RAC) </td></tr><tr><td>  Prozessoren pro Server </td><td>  Zwei Intel¬Æ Xeon¬Æ CPU E5-2667 v2 mit 16 Kernen bei 3,30 GHz </td></tr><tr><td>  Physischer Speicher pro Server </td><td>  128 GB </td></tr><tr><td>  FC-Netzwerk </td><td>  16 Gbit / s FC mit Multipathing </td></tr><tr><td>  FC HBA </td><td>  Emulex Lpe-16002B </td></tr><tr><td>  Spezielle √∂ffentliche 1-GbE-Ports f√ºr die Clusterverwaltung </td><td>  Intel Ethernet Adapter rj45 </td></tr><tr><td>  16 Gbit / s FC-Schalter </td><td>  Brokat 6505 </td></tr><tr><td>  Spezielle private 10-GbE-Ports f√ºr die Datensynchronisation </td><td>  Intel X520 </td></tr></tbody></table><br><p>  <b>AccelStor NeoSapphhire ‚Ñ¢ Alle Flash-Array-Spezifikationen</b> </p><br><table><tbody><tr><th>  Komponenten </th><th>  Beschreibung </th></tr><tr><td>  Speichersystem </td><td>  NeoSapphire ‚Ñ¢ Hochverf√ºgbarkeitsmodell: H710 </td></tr><tr><td>  Bildversion </td><td>  4.0.1 </td></tr><tr><td>  Gesamtzahl der Laufwerke </td><td>  48 </td></tr><tr><td>  Laufwerksgr√∂√üe </td><td>  1,92 TB </td></tr><tr><td>  Laufwerkstyp </td><td>  SSD </td></tr><tr><td>  FC-Zielports </td><td>  16x 16-Gbit-Ports (8 pro Knoten) </td></tr><tr><td>  Verwaltungsports </td><td>  Das 1-GbE-Ethernet-Kabel, das √ºber einen Ethernet-Switch mit Hosts verbunden ist </td></tr><tr><td>  Herzschlag-Port </td><td>  Das 1-GbE-Ethernet-Kabel, das zwei Speicherknoten verbindet </td></tr><tr><td>  Datensynchronisationsport </td><td>  InfiniBand-Kabel mit 56 Gbit / s </td></tr></tbody></table><br></div></div><br><p>  Bevor Sie ein Array verwenden, m√ºssen Sie es initialisieren.  Standardm√§√üig ist die Steueradresse beider Knoten identisch (192.168.1.1).  Sie m√ºssen nacheinander eine Verbindung zu ihnen herstellen, neue (bereits unterschiedliche) Verwaltungsadressen festlegen und die Zeitsynchronisierung konfigurieren. Anschlie√üend k√∂nnen die Verwaltungsports mit einem einzelnen Netzwerk verbunden werden.  Danach werden die Knoten zu einem HA-Paar zusammengefasst, indem Interlink-Verbindungen Subnetze zugewiesen werden. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/c3/kw/sp/c3kwspwqm38yl7ennakzykubswq.jpeg"></div><br><p>  Nach Abschluss der Initialisierung k√∂nnen Sie das Array von jedem Knoten aus steuern. </p><br><p>  Erstellen Sie als N√§chstes die erforderlichen Volumes und ver√∂ffentlichen Sie sie f√ºr Anwendungsserver. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/6h/4s/gf/6h4sgfinagrbbngoououu-lg1au.png"></div><br><p>  Es wird dringend empfohlen, mehrere Volumes f√ºr Oracle ASM zu erstellen, da dies die Anzahl der Ziele f√ºr die Server erh√∂ht, was letztendlich die Gesamtleistung verbessert (mehr zu Warteschlangen in einem anderen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Artikel</a> ). </p><br><div class="spoiler">  <b class="spoiler_title">Testkonfiguration</b> <div class="spoiler_text"><table><tbody><tr><th>  Name des Speichervolumens </th><th>  Volumengr√∂√üe </th></tr><tr><td>  Daten01 </td><td>  200 GB </td></tr><tr><td>  Daten02 </td><td>  200 GB </td></tr><tr><td>  Daten03 </td><td>  200 GB </td></tr><tr><td>  Daten04 </td><td>  200 GB </td></tr><tr><td>  Data05 </td><td>  200 GB </td></tr><tr><td>  Daten06 </td><td>  200 GB </td></tr><tr><td>  Daten07 </td><td>  200 GB </td></tr><tr><td>  Daten08 </td><td>  200 GB </td></tr><tr><td>  Daten09 </td><td>  200 GB </td></tr><tr><td>  Daten10 </td><td>  200 GB </td></tr><tr><td>  Grid01 </td><td>  1 GB </td></tr><tr><td>  Grid02 </td><td>  1 GB </td></tr><tr><td>  Grid03 </td><td>  1 GB </td></tr><tr><td>  Grid04 </td><td>  1 GB </td></tr><tr><td>  Grid05 </td><td>  1 GB </td></tr><tr><td>  Grid06 </td><td>  1 GB </td></tr><tr><td>  Redo01 </td><td>  100 GB </td></tr><tr><td>  Redo02 </td><td>  100 GB </td></tr><tr><td>  Redo03 </td><td>  100 GB </td></tr><tr><td>  Redo04 </td><td>  100 GB </td></tr><tr><td>  Redo05 </td><td>  100 GB </td></tr><tr><td>  Redo06 </td><td>  100 GB </td></tr><tr><td>  Redo07 </td><td>  100 GB </td></tr><tr><td>  Redo08 </td><td>  100 GB </td></tr><tr><td>  Redo09 </td><td>  100 GB </td></tr><tr><td>  Redo10 </td><td>  100 GB </td></tr></tbody></table><br></div></div><br><h3>  Einige Erkl√§rungen zu den Betriebsarten des Arrays und den Prozessen, die in Notsituationen auftreten </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/5m/_k/uu/5m_kuurlb-2fy52d8p1tzufcyic.png"></div><br><p>  Jeder Knotendatensatz verf√ºgt √ºber einen Parameter "Versionsnummer".  Nach der anf√§nglichen Initialisierung ist sie gleich und gleich 1. Wenn die Versionsnummer aus irgendeinem Grund unterschiedlich ist, werden immer Daten von der √§lteren Version zur j√ºngeren Version synchronisiert, wonach die Nummer f√ºr die j√ºngere Version ausgerichtet wird, d. H.  Dies bedeutet, dass die Kopien identisch sind.  Gr√ºnde, warum Versionen variieren k√∂nnen: </p><br><ul><li>  Geplanter Neustart eines der Knoten </li><li>  Ein Unfall an einem der Knoten aufgrund eines pl√∂tzlichen Herunterfahrens (Stromversorgung, √úberhitzung usw.). </li><li>  Unterbrochene InfiniBand-Verbindung mit Unf√§higkeit zur Synchronisierung </li><li>  Absturz auf einem der Knoten aufgrund von Datenbesch√§digung.  Es ist bereits die Erstellung einer neuen HA-Gruppe und die vollst√§ndige Synchronisierung des Datensatzes erforderlich. </li></ul><br><p>  In jedem Fall erh√∂ht der online verbleibende Knoten seine Versionsnummer um eins, sodass er nach erneuter Verbindung mit dem Paar seinen Datensatz synchronisiert. </p><br><p>  Wenn die Verbindung √ºber die Ethernet-Verbindung unterbrochen wird, wechselt Heartbeat vor√ºbergehend zu InfiniBand und kehrt nach Wiederherstellung innerhalb von 10 Sekunden zur√ºck. </p><br><h3>  Host-Konfiguration </h3><br><p>  Um Fehlertoleranz sicherzustellen und die Leistung zu erh√∂hen, m√ºssen Sie die MPIO-Unterst√ºtzung f√ºr das Array aktivieren.  F√ºgen Sie dazu der Datei /etc/multipath.conf Zeilen hinzu und laden Sie den Multipath-Dienst neu </p><br><div class="spoiler">  <b class="spoiler_title">Versteckter Text</b> <div class="spoiler_text">  Ger√§te { <br>  Ger√§t { <br>  Anbieter "AStor" <br>  path_grouping_policy "group_by_prio" <br>  path_selector "Warteschlangenl√§nge 0" <br>  path_checker "tur" <br>  Eigenschaften "0" <br>  hardware_handler "0" <br>  prio "const" <br>  sofortiges Failback <br>  fast_io_fail_tmo 5 <br>  dev_loss_tmo 60 <br>  user_friendly_names ja <br>  detect_prio ja <br>  rr_min_io_rq 1 <br>  no_path_retry 0 <br>  }} <br>  }} <br><br></div></div><br><p>  Damit ASM √ºber ASMLib mit MPIO arbeiten kann, m√ºssen Sie die Datei / etc / sysconfig / oracleasm √§ndern und anschlie√üend /etc/init.d/oracleasm scandisks ausf√ºhren </p><br><div class="spoiler">  <b class="spoiler_title">Versteckter Text</b> <div class="spoiler_text"><p>  # ORACLEASM_SCANORDER: √úbereinstimmende Muster, um das Scannen der Festplatte zu bestellen <br>  ORACLEASM_SCANORDER = "dm" </p><br><br><p>  # ORACLEASM_SCANEXCLUDE: √úbereinstimmende Muster, um Festplatten vom Scan auszuschlie√üen <br>  ORACLEASM_SCANEXCLUDE = "sd" </p><br><p></p><h4>  Hinweis </h4><br><p>  <i>Wenn Sie ASMLib nicht verwenden m√∂chten, k√∂nnen Sie die UDEV-Regeln verwenden, die die Grundlage f√ºr ASMLib bilden.</i> </p><br><p>  <i>Ab Version 12.1.0.2 Oracle Database steht die Option zur Installation als Teil der ASMFD-Software zur Verf√ºgung.</i> </p><br></div></div><br><p>  Stellen Sie sicher, dass die f√ºr Oracle ASM erstellten Datentr√§ger an der Gr√∂√üe des Blocks ausgerichtet sind, mit dem das Array physisch arbeitet (4 KB).  Andernfalls k√∂nnen Leistungsprobleme auftreten.  Daher m√ºssen Sie Volumes mit den entsprechenden Parametern erstellen: </p><br><p>  <i>parted / dev / mapper / Ger√§tename mklabel gpt mkpart primary 2048s 100% Align-Check Optimal 1</i> </p><br><h3>  Verteilung von Datenbanken auf erstellten Volumes f√ºr unsere Testkonfiguration </h3><br><table><tbody><tr><th>  Name des Speichervolumens </th><th>  Volumengr√∂√üe </th><th>  Zuordnung von Volume-LUNs </th><th>  ASM Volume Device Detail </th><th>  Gr√∂√üe der Zuordnungseinheit </th></tr><tr><td>  Daten01 </td><td>  200 GB </td><td rowspan="26">  Ordnen Sie alle Speichervolumes dem Speichersystem aller Datenports zu </td><td rowspan="10">  Redundanz: normal <br>  Name: DGDATA <br>  Zweck: Datendateien <br></td><td rowspan="10">  4 MB </td></tr><tr><td>  Daten02 </td><td>  200 GB </td></tr><tr><td>  Daten03 </td><td>  200 GB </td></tr><tr><td>  Daten04 </td><td>  200 GB </td></tr><tr><td>  Data05 </td><td>  200 GB </td></tr><tr><td>  Daten06 </td><td>  200 GB </td></tr><tr><td>  Daten07 </td><td>  200 GB </td></tr><tr><td>  Daten08 </td><td>  200 GB </td></tr><tr><td>  Daten09 </td><td>  200 GB </td></tr><tr><td>  Daten10 </td><td>  200 GB </td></tr><tr><td>  <font color="#248dee">Grid01</font> </td><td>  <font color="#248dee">1 GB</font> </td><td rowspan="3">  <font color="#248dee">Redundanz: normal</font> <font color="#248dee"><br></font>  <font color="#248dee">Name: DGGRID1</font> <font color="#248dee"><br></font>  <font color="#248dee">Zweck: Raster: CRS und Abstimmung</font> <br></td><td rowspan="3">  <font color="#248dee">4 MB</font> </td></tr><tr><td>  <font color="#248dee">Grid02</font> </td><td>  <font color="#248dee">1 GB</font> </td></tr><tr><td>  <font color="#248dee">Grid03</font> </td><td>  <font color="#248dee">1 GB</font> </td></tr><tr><td>  Grid04 </td><td>  1 GB </td><td rowspan="3">  Redundanz: normal <br>  Name: DGGRID2 <br>  Zweck: Raster: CRS und Abstimmung <br></td><td rowspan="3">  4 MB </td></tr><tr><td>  Grid05 </td><td>  1 GB </td></tr><tr><td>  Grid06 </td><td>  1 GB </td></tr><tr><td>  <font color="#248dee">Redo01</font> </td><td>  <font color="#248dee">100 GB</font> </td><td rowspan="5">  <font color="#248dee">Redundanz: normal</font> <font color="#248dee"><br></font>  <font color="#248dee">Name: DGREDO1</font> <font color="#248dee"><br></font>  <font color="#248dee">Zweck: Protokoll von Thread 1 wiederholen</font> <font color="#248dee"><br></font> <br></td><td rowspan="5">  <font color="#248dee">4 MB</font> </td></tr><tr><td>  <font color="#248dee">Redo02</font> </td><td>  <font color="#248dee">100 GB</font> </td></tr><tr><td>  <font color="#248dee">Redo03</font> </td><td>  <font color="#248dee">100 GB</font> </td></tr><tr><td>  <font color="#248dee">Redo04</font> </td><td>  <font color="#248dee">100 GB</font> </td></tr><tr><td>  <font color="#248dee">Redo05</font> </td><td>  <font color="#248dee">100 GB</font> </td></tr><tr><td>  Redo06 </td><td>  100 GB </td><td rowspan="5">  Redundanz: normal <br>  Name: DGREDO2 <br>  Zweck: Protokoll von Thread 2 wiederholen <br><br></td><td rowspan="5">  4 MB </td></tr><tr><td>  Redo07 </td><td>  100 GB </td></tr><tr><td>  Redo08 </td><td>  100 GB </td></tr><tr><td>  Redo09 </td><td>  100 GB </td></tr><tr><td>  Redo10 </td><td>  100 GB </td></tr></tbody></table><br><div class="spoiler">  <b class="spoiler_title">Datenbankeinstellungen</b> <div class="spoiler_text"><ul><li>  Blockgr√∂√üe = 8K </li><li>  Swap Space = 16 GB </li><li>  AMM deaktivieren (Automatic Memory Management) </li><li>  Deaktivieren Sie transparente gro√üe Seiten </li></ul><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Andere Einstellungen</b> <div class="spoiler_text"><p>  <u># vi /etc/sysctl.conf</u> <br>  ‚úì fs.aio-max-nr = 1048576 <br>  ‚úì fs.file-max = 6815744 <br>  ‚úì kernel.shmmax 103079215104 <br>  ‚úì kernel.shmall 31457280 <br>  ‚úì kernel.shmmn 4096 <br>  ‚úì kernel.sem = 250 32000 100 128 <br>  ‚úì net.ipv4.ip_local_port_range = 9000 65500 <br>  ‚úì net.core.rmem_default = 262144 <br>  ‚úì net.core.rmem_max = 4194304 <br>  ‚úì net.core.wmem_default = 262144 <br>  ‚úì net.core.wmem_max = 1048586 <br>  ‚úì vm.swappiness = 10 <br>  ‚úì vm.min_free_kbytes = 524288 # Legen Sie dies nicht fest, wenn Sie Linux x86 verwenden <br>  ‚úì vm.vfs_cache_pressure = 200 <br>  ‚úì vm.nr_hugepages = 57000 </p><br><br><p>  <u># vi /etc/security/limits.conf</u> <br>  ‚úì Gitter soft nproc 2047 <br>  ‚úì Gitter hart nproc 16384 <br>  ‚úì Raster Soft Nofile 1024 <br>  ‚úì Raster-Hard-Nofile 65536 <br>  ‚úì Gitter-Softstack 10240 <br>  ‚úì Grid Hard Stack 32768 <br>  ‚úì oracle soft nproc 2047 <br>  ‚úì oracle hard nproc 16384 <br>  ‚úì Orakel Soft Nofile 1024 <br>  ‚úì Oracle Hard Nofile 65536 <br>  ‚úì Orakel-Softstack 10240 <br>  ‚úì Oracle Hard Stack 32768 <br>  ‚úì Soft Memlock 120795954 <br>  ‚úì Hard Memlock 120795954 <br></p><br><p>  <u>sqlplus "/ as sysdba"</u> <br>  System-Set-Prozesse √§ndern = 2000 scope = spfile; <br>  alter system set open_cursors = 2000 scope = spfile; <br>  System √§ndern set session_cached_cursors = 300 scope = spfile; <br>  alter system set db_files = 8192 scope = spfile; <br></p><br><br></div></div><br><h3>  Fehlertoleranztest </h3><br><p>  Zu Demonstrationszwecken wurde HammerDB verwendet, um die OLTP-Last zu emulieren.  HammerDB-Konfiguration: </p><br><table><tbody><tr><td>  <b>Anzahl der Lager</b> </td><td>  256 </td></tr><tr><td>  Gesamtzahl der Transaktionen pro Benutzer </td><td>  1000000000000 </td></tr><tr><td>  Virtuelle Benutzer </td><td>  256 </td></tr></tbody></table><br><p>  Als Ergebnis wurde der 2.1M TPM-Indikator erhalten, der weit von der Leistungsgrenze des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">H710-</a> Arrays entfernt ist, aber die ‚ÄûObergrenze‚Äú f√ºr die aktuelle Hardwarekonfiguration von Servern (haupts√§chlich aufgrund von Prozessoren) und deren Anzahl darstellt.  Der Zweck dieses Tests besteht weiterhin darin, die Fehlertoleranz der gesamten L√∂sung zu demonstrieren und nicht die maximale Leistung zu erzielen.  Deshalb werden wir einfach auf dieser Zahl aufbauen. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3o/ds/id/3odsid2wr0ynssl4zuu8idq8dfq.png"></div><br><h3>  Test auf Ausfall eines der Knoten </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ar/tf/og/artfogfilgwzy2vtxlsyvkp98vu.png"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/zk/iv/xt/zkivxtazysrew4hhzjan-vujnma.png"></div><br><p>  Die Hosts haben einige der Pfade zum Speicher verloren und die verbleibenden Pfade mit dem zweiten Knoten weiter bearbeitet.  Die Leistung ging aufgrund der Umstrukturierung der Pfade f√ºr einige Sekunden zur√ºck und normalisierte sich dann wieder.  Es ist keine Dienstunterbrechung aufgetreten. </p><br><h3>  Schrankausfalltest mit allen Ger√§ten </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/mq/my/2b/mqmy2bzrge24zhnj217spj2rts4.png"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/eq/nh/-8/eqnh-8pwbhyzqgdleh1tsiafkr0.png"></div><br><p>  In diesem Fall sank die Leistung aufgrund der Umstrukturierung der Pfade ebenfalls f√ºr einige Sekunden und kehrte dann auf die H√§lfte des Werts des urspr√ºnglichen Indikators zur√ºck.  Das Ergebnis wurde aufgrund des Ausschlusses vom Betrieb eines Anwendungsservers vom Original halbiert.  Eine Dienstunterbrechung ist ebenfalls nicht aufgetreten. </p><br><blockquote>  Wenn Sie eine fehlertolerante Cross-Rack-Disaster-Recovery-L√∂sung f√ºr Oracle zu angemessenen Kosten und mit geringem Aufwand f√ºr Bereitstellung / Verwaltung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">implementieren m√ºssen</a> , ist die Zusammenarbeit mit Oracle RAC und der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AccelStor Shared-Nothing-</a> Architektur eine der besten Optionen.  Anstelle von Oracle RAC kann es auch eine andere Software geben, die Clustering bereitstellt, z. B. dasselbe DBMS oder dieselben Virtualisierungssysteme.  Das Prinzip der Konstruktion der L√∂sung bleibt gleich.  Und das Endergebnis ist Null f√ºr RTO und RPO. </blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de448538/">https://habr.com/ru/post/de448538/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de448528/index.html">Kostenloser Wireguard VPN Service unter AWS</a></li>
<li><a href="../de448530/index.html">Wie Megaphone auf Mobilabonnements geschlafen hat</a></li>
<li><a href="../de448532/index.html">Weltraum-Rechenzentrum. Das Experiment zusammenfassen</a></li>
<li><a href="../de448534/index.html">Warum brauchen wir industrielle Schalter mit verbesserter EMV?</a></li>
<li><a href="../de448536/index.html">Transparenz - Das Allheilmittel der Butcherts</a></li>
<li><a href="../de448540/index.html">VMware NSX f√ºr die Kleinsten. Teil 5. Konfigurieren des Load Balancers</a></li>
<li><a href="../de448544/index.html">B√ºndel aus Stahl. Wie entstehen sie?</a></li>
<li><a href="../de448546/index.html">UITableView automatische Kopf- und Fu√üzeilengr√∂√üen mit AutoLayout</a></li>
<li><a href="../de448548/index.html">Konstruktion in der Kunst: von Brueghel bis Vasya Lozhkin</a></li>
<li><a href="../de448550/index.html">Berichtwettbewerb bei #PAYMENTSECURITY 2019 er√∂ffnet</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>