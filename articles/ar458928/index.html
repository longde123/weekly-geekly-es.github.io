<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👏🏽 Ⓜ️ 😪 XLNet مقابل بيرت 🕠 👨🏻‍🎓 👨🏻‍🏭</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="في نهاية شهر يونيو ، قام فريق من جامعة كارنيجي ميلون بعرض XLNet لدينا ، حيث قام على الفور بوضع المنشور والكود والنموذج النهائي ( XLNet-Large ، Cased :...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>XLNet مقابل بيرت</h1><div class="post__body post__body_full" style=";text-align:right;direction:rtl"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/ods/blog/458928/" style=";text-align:right;direction:rtl"><img src="https://habrastorage.org/webt/py/g0/es/pyg0es7u25w7xb0cc8z49aczcls.png"><br><br>  في نهاية شهر يونيو ، قام فريق من جامعة كارنيجي ميلون بعرض XLNet لدينا ، حيث قام على الفور بوضع <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">المنشور</a> <a href="">والكود</a> والنموذج النهائي ( <a href="">XLNet-Large</a> ، <a href="">Cased</a> : 24 طبقة ، 1024 مخفية ، 16 رأسًا).  هذا نموذج تم تدريبه مسبقًا لحل مختلف مشكلات معالجة اللغة الطبيعية. <br><br>  في المنشور ، أشاروا على الفور إلى مقارنة <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">طرازهم</a> مع <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">بيرت</a> جوجل.  يكتبون أن XLNet متفوقة على BERT في عدد كبير من المهام.  ويظهر النتائج في 18 مهمة متطورة. <br><a name="habracut"></a><br><h2 style=";text-align:right;direction:rtl">  بيرت ، XLNet والمحولات </h2><br>  واحدة من الاتجاهات الحديثة في التعلم العميق هو نقل التعلم.  نقوم بتدريب النماذج لحل المشكلات البسيطة على كمية هائلة من البيانات ، ثم نستخدم هذه النماذج المدربة مسبقًا ، ولكن لحل المشكلات الأخرى الأكثر تحديدًا.  BERT و XLNet هما مجرد شبكات مدربة مسبقًا يمكن استخدامها لحل مشكلات معالجة اللغة الطبيعية. <br><br>  تعمل هذه النماذج على تطوير فكرة <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">المحولات</a> - النهج السائد حاليًا في بناء نماذج للتعامل مع المتواليات.  مفصل للغاية ومع أمثلة من التعليمات البرمجية على المحولات وآلية الاهتمام مكتوب في <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">المحول المشروح</a> . <br><br>  إذا نظرت إلى <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">لوحة المتصدرين المعيارية لتقييم اللغة العامة (GLUE)</a> ، فيمكنك من خلال ذلك مشاهدة العديد من النماذج المعتمدة على المحولات.  بما في ذلك كلا النموذجين التي تظهر نتائج أفضل من البشر.  يمكننا القول أنه مع المحولات ، نشهد ثورة صغيرة في معالجة اللغة الطبيعية. <br><br><h2 style=";text-align:right;direction:rtl">  عيوب بيرت </h2><br>  بيرت هو التشفير التلقائي (التشفير التلقائي ، AE).  إنه يخفي ويفسد بعض الكلمات في التسلسل ويحاول استعادة التسلسل الأصلي للكلمات من السياق. <br><br>  هذا يؤدي إلى عيوب النموذج: <br><br><ul style=";text-align:right;direction:rtl"><li style=";text-align:right;direction:rtl">  يتم التنبؤ بكل كلمة مخفية بشكل فردي.  نفقد المعلومات حول العلاقات المحتملة بين الكلمات المقنعة.  تقدم المقالة مثالًا يسمى "نيويورك".  إذا حاولنا التنبؤ بهذه الكلمات بشكل مستقل في السياق ، فلن نأخذ في الاعتبار العلاقة بينهما. </li><li style=";text-align:right;direction:rtl">  عدم الاتساق بين مراحل تدريب نموذج بيرت واستخدام نموذج بيرت المدرَّب مسبقًا.  عندما نقوم بتدريب النموذج - قمنا بإخفاء الكلمات ([MASK]) ، عندما نستخدم النموذج المدربين مسبقًا ، فإننا لا نوفر بالفعل هذه الرموز المميزة للمدخلات. </li></ul><br>  ومع ذلك ، على الرغم من هذه المشكلات ، أظهر BERT أحدث النتائج في العديد من مهام معالجة اللغة الطبيعية. <br><br><h2 style=";text-align:right;direction:rtl">  ميزات XLNet </h2><br>  XLNet عبارة عن نمذجة لغة الانحدار التلقائي ، AR LM.  إنها تحاول التنبؤ بالرمز التالي من تسلسل الرموز السابقة.  في نماذج الانحدار التلقائي الكلاسيكية ، يتم أخذ هذا التسلسل السياقي بشكل مستقل عن اتجاهين للسلسلة الأصلية. <br><br>  يعمم XLNet هذه الطريقة ويشكل السياق من أماكن مختلفة في تسلسل المصدر.  كيف يفعل ذلك.  يأخذ كل (من الناحية النظرية) التباديل الممكنة من التسلسل الأصلي ويتوقع كل رمزية في التسلسل من سابقاتها. <br><br>  فيما يلي مثال من المقالة حول كيفية توقع الرمز المميز x3 من التباديل المختلفة للتسلسل الأصلي. <br><br><img src="https://habrastorage.org/webt/yq/mb/fa/yqmbfas9mcnfkciq6pmew_-4hh8.png"><br><br>  علاوة على ذلك ، السياق ليس كيس كلمات.  يتم أيضًا تقديم معلومات حول الترتيب الأولي للرموز إلى النموذج. <br><br>  إذا رسمنا أوجه تشابه مع BERT ، اتضح أننا لا نخفي الرموز المميزة مقدمًا ، بل نستخدم مجموعات مختلفة من الرموز المخفية في التباينات المختلفة.  في الوقت نفسه ، تختفي المشكلة الثانية لـ BERT - عدم وجود الرموز المخفية عند استخدام النموذج المدربين مسبقًا.  في حالة XLNet ، تسلسل كامل ، دون أقنعة ، هو بالفعل إدخال. <br><br>  من أين يأتي XL في الاسم.  XL - لأن XLNet يستخدم آلية Attention والأفكار من طراز Transformer-XL.  على الرغم من أن اللغات الشريرة تدعي أن XL يلمح إلى مقدار الموارد اللازمة لتدريب الشبكة. <br><br><img src="https://habrastorage.org/webt/hs/fb/u-/hsfbu-ufj-9e-me1agkauoa389c.png"><br><br>  وحول الموارد.  على Twitter ، قاموا بنشر <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">حساب</a> تكلفة تدريب الشبكة باستخدام المعلمات من المقال.  اتضح 245000 دولار.  صحيح ، ثم جاء مهندس من Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=ar&amp;u=">وصحح</a> أن المقالة تذكر 512 من رقائق TPU ، أربعة منها على الجهاز.  وهذا هو ، التكلفة بالفعل 62440 دولار ، أو حتى 32720 دولار ، بالنظر إلى 512 النوى ، والتي ذكرت أيضا في المقال. <br><br><h2 style=";text-align:right;direction:rtl">  XLNet مقابل بيرت </h2><br>  حتى الآن ، تم وضع نموذج واحد مُدرَّب مسبقًا للغة الإنجليزية للمقالة (XLNet-Large، Cased).  لكن المقالة تذكر أيضًا التجارب على نماذج أصغر.  وفي العديد من المهام ، تُظهر نماذج XLNet نتائج أفضل مقارنةً بنماذج BERT المشابهة. <br><br><img src="https://habrastorage.org/webt/ac/p_/th/acp_thyxqwgcuyhvfvkkixhwj_y.png"><br><br>  اجتذب ظهور BERT وخاصة النماذج المدربة مسبقًا الكثير من اهتمام الباحثين وأدى إلى عدد كبير من الأعمال ذات الصلة.  الآن هنا هو XLNet.  من المثير للاهتمام معرفة ما إذا كان لبعض الوقت يصبح المعيار الفعلي في البرمجة اللغوية العصبية ، أو العكس ، سيحفز الباحثين في البحث عن بنى وأساليب جديدة لمعالجة اللغة الطبيعية. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/ar458928/">https://habr.com/ru/post/ar458928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../ar458918/index.html">ما يمكن أن تتعلمه من تصميم الألعاب غير العادية</a></li>
<li><a href="../ar458920/index.html">مؤتمر لمحبي DevOps</a></li>
<li><a href="../ar458922/index.html">كيفية الانتقال من ESXi إلى KVM / LXD ولا تفقد عقلك</a></li>
<li><a href="../ar458924/index.html">الحوادث تساعدك على التعلم</a></li>
<li><a href="../ar458926/index.html">المأساة لا تأتي وحدها</a></li>
<li><a href="../ar458930/index.html">كيف وصل طلاب بيرم إلى نهائيات بطولة تحليل البيانات الدولية 2019 لكأس البيانات</a></li>
<li><a href="../ar458932/index.html">يوتا - أو كيف يمكنك معرفة كل شيء</a></li>
<li><a href="../ar458934/index.html">نشر التطبيقات على مجموعات Kubernetes متعددة مع Helm</a></li>
<li><a href="../ar458936/index.html">"من السهل الإجابة بدلاً من الصمت" - مقابلة كبيرة مع والد ذاكرة المعاملات ، موريس هيرليشي</a></li>
<li><a href="../ar458938/index.html">تم تجميع C ++ 20 ، وتم بدء تشغيل C ++ 23. نتائج الاجتماع في كولونيا</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>