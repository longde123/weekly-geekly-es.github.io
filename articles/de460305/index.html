<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëàüèø ü§Ωüèª üíß AERODISK Motor: Katastrophal. Teil 2. Metrocluster üàµ üë©üèø‚Äçüè´ üï¥üèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Leser von Habr! In einem fr√ºheren Artikel haben wir √ºber ein einfaches Tool zur Katastrophenvertr√§glichkeit in AERODISK ENGINE-Speichersystemen ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AERODISK Motor: Katastrophal. Teil 2. Metrocluster</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/aerodisk/blog/460305/"><p><img src="https://habrastorage.org/webt/ft/el/xz/ftelxzp4fmlci1fn9endwmjffew.jpeg"></p><br><p>  Hallo Leser von Habr!  In einem fr√ºheren Artikel haben wir √ºber ein einfaches Tool zur Katastrophenvertr√§glichkeit in AERODISK ENGINE-Speichersystemen gesprochen - √ºber die Replikation.  In diesem Artikel werden wir uns mit einem komplexeren und interessanteren Thema befassen - dem U-Bahn-Cluster, dh einem Mittel zum automatisierten Katastrophenschutz f√ºr zwei Rechenzentren, mit dem Rechenzentren im Aktiv-Aktiv-Modus arbeiten k√∂nnen.  Wir werden es erz√§hlen, zeigen, brechen und reparieren. </p><a name="habracut"></a><br><h2 id="kak-obychno-v-nachale-teoriya">  Wie √ºblich zu Beginn der Theorie </h2><br><p>  Ein U-Bahn-Cluster ist ein Cluster, der √ºber mehrere Standorte innerhalb einer Stadt oder eines Bezirks verteilt ist.  Das Wort "Cluster" weist uns klar darauf hin, dass der Komplex automatisiert ist, dh das Umschalten von Clusterknoten bei Fehlern erfolgt automatisch. </p><br><p>  Hier liegt der Hauptunterschied zwischen dem Metro-Cluster und der normalen Replikation.  Automatisierung von Operationen.  Das hei√üt, bei bestimmten Vorf√§llen (Ausfall des Rechenzentrums, defekte Kan√§le usw.) f√ºhrt das Speichersystem unabh√§ngig die erforderlichen Aktionen aus, um die Datenverf√ºgbarkeit aufrechtzuerhalten.  Bei Verwendung regul√§rer Replikate werden diese Aktionen vom Administrator ganz oder teilweise manuell ausgef√ºhrt. </p><br><h3 id="dlya-chego-eto-nuzhno">  Wof√ºr ist das? </h3><br><p> Das Hauptziel, das Kunden mit der einen oder anderen Implementierung des Metro-Clusters verfolgen, ist die Minimierung des RTO (Recovery Time Objective).  Minimieren Sie also die Wiederherstellungszeit von IT-Services nach einem Ausfall.  Wenn Sie die normale Replikation verwenden, ist die Wiederherstellungszeit immer l√§nger als die Wiederherstellungszeit mit dem Metro-Cluster.  Warum?  Sehr einfach.  Der Administrator muss am Arbeitsplatz sein und die Replikation von Hand wechseln. Der Metro-Cluster f√ºhrt dies automatisch durch. </p><br><p>  Wenn Sie keinen dedizierten Administrator im Dienst haben, der nicht schl√§ft, isst, raucht oder krank wird und 24 Stunden am Tag den Speicherstatus √ºberpr√ºft, kann nicht garantiert werden, dass der Administrator w√§hrend eines Fehlers f√ºr manuelles Umschalten verf√ºgbar ist. </p><br><p>  Dementsprechend RTO in Abwesenheit eines U-Bahn-Clusters oder <del>  unsterbliche Administratorstufe 99 </del>  Der Dienst des Administrators im Dienst entspricht der Summe der Umschaltzeit aller Systeme und der maximalen Zeitspanne, nach der der Administrator garantiert mit Speichersystemen und verwandten Systemen arbeitet. </p><br><p>  Wir kommen daher zu dem offensichtlichen Schluss, dass der Metro-Cluster verwendet werden sollte, wenn die RTO-Anforderung Minuten und nicht Stunden oder Tage betr√§gt, dh wenn im schlimmsten Fall ein Ausfall des Rechenzentrums erforderlich ist, muss die IT-Abteilung dem Unternehmen Zeit geben, um den Zugriff auf die IT wiederherzustellen -Dienstleistungen innerhalb von Minuten oder sogar Sekunden. </p><br><h3 id="kak-eto-rabotaet">  Wie funktioniert es </h3><br><p>  Auf der unteren Ebene verwendet der Metro-Cluster den Mechanismus der synchronen Datenreplikation, den wir in einem fr√ºheren Artikel beschrieben haben (siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link</a> ).  Da die Replikation synchron ist, sind die Anforderungen daf√ºr angemessen, oder besser: </p><br><ul><li>  Glasfaser als Physik, 10 Gigabit Ethernet (oder h√∂her); </li><li>  Die Entfernung zwischen den Rechenzentren betr√§gt nicht mehr als 40 Kilometer. </li><li>  Optische Kanalverz√∂gerung zwischen Rechenzentren (zwischen Speichersystemen) bis zu 5 Millisekunden (optimal 2). </li></ul><br><p>  Alle diese Anforderungen sind beratender Natur, dh der Metro-Cluster funktioniert auch dann, wenn diese Anforderungen nicht erf√ºllt werden. Es muss jedoch verstanden werden, dass die Folgen der Nichteinhaltung dieser Anforderungen gleich der Verlangsamung beider Speichersysteme im Metro-Cluster sind. </p><br><p>  Ein synchrones Replikat wird also verwendet, um Daten zwischen Speichersystemen zu √ºbertragen, und wie Replikate automatisch umgeschaltet werden und vor allem, wie Split-Brain vermieden werden kann.  Hierzu wird auf der obigen Ebene eine zus√§tzliche Entit√§t verwendet - der Arbiter. </p><br><h3 id="kak-rabotaet-arbitr-i-v-chem-ego-zadacha">  Wie arbeitet der Schiedsrichter und was ist seine Aufgabe? </h3><br><p>  Der Arbiter ist eine kleine virtuelle Maschine oder ein Hardware-Cluster, der auf der dritten Plattform (z. B. im B√ºro) ausgef√ºhrt werden muss und √ºber ICMP und SSH Zugriff auf den Speicher bietet.  Nach dem Start sollte der Arbiter die IP festlegen und dann auf der Speicherseite seine Adresse sowie die Adressen der Fernbedienungen angeben, die am Metro-Cluster teilnehmen.  Danach ist der Schiedsrichter bereit zu arbeiten. </p><br><p>  Der Arbiter √ºberwacht st√§ndig alle Speichersysteme im Metro-Cluster. Wenn ein Speichersystem nicht verf√ºgbar ist, entscheidet er, nachdem er best√§tigt hat, dass es von einem anderen Clustermitglied (einem der "Live" -Speichersysteme) nicht verf√ºgbar ist, mit dem Verfahren zum Wechseln der Replikationsregeln und der Zuordnung. </p><br><p>  Ein sehr wichtiger Punkt.  Der Arbiter muss sich immer an einem anderen Standort befinden als dem, an dem sich der Speicher befindet, dh weder im Rechenzentrum 1, in dem sich der Speicher 1 befindet, noch im Rechenzentrum 2, in dem der Speicher 2 installiert ist. </p><br><p>  Warum?  Denn nur so kann ein Schiedsrichter mit Hilfe eines der √ºberlebenden Speichersysteme den Sturz eines der beiden Standorte, an denen die Speichersysteme installiert sind, eindeutig und genau bestimmen.  Alle anderen M√∂glichkeiten, einen Schiedsrichter zu platzieren, k√∂nnen zu einem gespaltenen Gehirn f√ºhren. </p><br><h3 id="teper-pogruzimsya-v-detali-raboty-arbitra">  Tauchen Sie nun in die Details des Schiedsrichters ein </h3><br><p>  Der Arbiter f√ºhrt mehrere Dienste aus, die st√§ndig von allen Speichercontrollern abgefragt werden.  Wenn sich das Ergebnis der Umfrage vom vorherigen unterscheidet (verf√ºgbar / nicht zug√§nglich), wird es in einer kleinen Datenbank aufgezeichnet, die auch als Schiedsrichter fungiert. </p><br><p>  <strong>Betrachten Sie die Logik des Schiedsrichters genauer.</strong> </p><br><p>  <u>Schritt 1. Feststellung der Unzug√§nglichkeit.</u>  Ein Ereignissignal √ºber den Ausfall des Speichersystems ist das Fehlen eines Pings von beiden Controllern desselben Speichersystems f√ºr 5 Sekunden. </p><br><p>  <u>Schritt 2. Starten Sie den Schaltvorgang.</u>  Nachdem der Schiedsrichter verstanden hat, dass eines der Speichersysteme nicht verf√ºgbar ist, sendet er eine Anfrage an das "lebende" Speichersystem, um sicherzustellen, dass das "tote" Speichersystem wirklich gestorben ist. </p><br><p>  Nach Erhalt eines solchen Befehls vom Schiedsrichter pr√ºft das zweite (Live-) Speichersystem zus√§tzlich die Verf√ºgbarkeit des heruntergefallenen ersten Speichersystems und sendet, falls dies nicht der Fall ist, dem Schiedsrichter eine Best√§tigung seiner Vermutungen.  Speicher ist wirklich nicht verf√ºgbar. </p><br><p>  Nach Erhalt einer solchen Best√§tigung startet der Arbiter die Remote-Prozedur zum Umschalten der Replikation und zum Erh√∂hen der Zuordnung f√ºr die Replikate, die auf dem gel√∂schten Speichersystem aktiv (prim√§r) waren, und sendet einen Befehl an das zweite Speichersystem, um diese Replikate von sekund√§r zu prim√§r zu erstellen und die Zuordnung zu erh√∂hen.  Nun, das zweite Speichersystem f√ºhrt diese Prozeduren aus, wonach es von sich aus Zugriff auf die verlorenen LUNs gew√§hrt. </p><br><p>  Warum ben√∂tige ich eine zus√§tzliche √úberpr√ºfung?  F√ºr das Quorum.  Das hei√üt, der gr√∂√üte Teil der ungeraden (3) Gesamtzahl der Clustermitglieder sollte den Fall eines der Clusterknoten best√§tigen.  Nur dann ist diese Entscheidung genau richtig.  Dies ist notwendig, um ein fehlerhaftes Schalten und dementsprechend ein Split-Brain zu vermeiden. </p><br><p>  Schritt 2 dauert ungef√§hr 5 bis 10 Sekunden. Unter Ber√ºcksichtigung der Zeit, die erforderlich ist, um die Unzug√§nglichkeit festzustellen (5 Sekunden), stehen LUNs mit gel√∂schtem Speicher innerhalb von 10 bis 15 Sekunden nach dem Unfall automatisch f√ºr die Arbeit mit Live-Speicher zur Verf√ºgung. </p><br><p>  Es ist klar, dass Sie, um ein Trennen der Hosts zu vermeiden, auch auf die korrekte Einstellung der Zeit√ºberschreitungen auf den Hosts achten m√ºssen.  Das empfohlene Zeitlimit betr√§gt mindestens 30 Sekunden.  Dadurch kann der Host w√§hrend eines Last√ºbergangs w√§hrend eines Last√ºbergangs nicht vom Speichersystem getrennt werden, und es kann garantiert werden, dass die Eingabe / Ausgabe nicht unterbrochen wird. </p><br><blockquote>  Nur eine Sekunde, es stellt sich heraus, wenn mit dem Metro-Cluster alles in Ordnung ist, warum ben√∂tigen Sie eine regelm√§√üige Replikation? </blockquote><p>  In der Tat ist nicht alles so einfach. </p><br><h3 id="rassmotrim-plyusy-i-minusy-metroklastera">  Ber√ºcksichtigen Sie die Vor- und Nachteile des U-Bahn-Clusters </h3><br><p>  Wir haben also festgestellt, dass die offensichtlichen Vorteile des Metro-Clusters gegen√ºber der herk√∂mmlichen Replikation folgende sind: </p><br><ul><li>  Vollst√§ndige Automatisierung mit minimaler Wiederherstellungszeit im Katastrophenfall; </li><li>  Und alle :-). </li></ul><br><p>  Und jetzt Aufmerksamkeit, Nachteile: </p><br><ul><li>  Die Kosten der Entscheidung.  Obwohl f√ºr den Metro-Cluster in Aerodisk-Systemen keine zus√§tzliche Lizenz erforderlich ist (es wird dieselbe Lizenz wie f√ºr das Replikat verwendet), sind die Kosten der L√∂sung immer noch h√∂her als bei Verwendung der synchronen Replikation.  Es m√ºssen alle Anforderungen f√ºr das synchrone Replikat sowie die Anforderungen f√ºr den Metro-Cluster in Bezug auf zus√§tzliches Switching und zus√§tzlichen Standort implementiert werden (siehe Planung des Metro-Clusters). </li><li>  Die Komplexit√§t der Entscheidung.  Der Metro-Cluster ist viel komplexer als ein regul√§res Replikat und erfordert viel mehr Aufmerksamkeit und Arbeit f√ºr Planung, Konfiguration und Dokumentation. </li></ul><br><p>  Zusammenfassend.  <strong>Metro Cluster ist nat√ºrlich eine sehr technologische und gute L√∂sung, wenn Sie RTO wirklich in Sekunden oder Minuten bereitstellen m√ºssen.</strong>  Aber wenn es keine solche Aufgabe gibt und RTO in Stunden f√ºr das Gesch√§ft in Ordnung ist, macht es keinen Sinn, Spatzen aus der Kanone zu schie√üen.  Die √ºbliche Replikation von Arbeitern ist ausreichend, da dem U-Bahn-Cluster zus√§tzliche Kosten entstehen und die IT-Infrastruktur kompliziert wird. </p><br><h2 id="planirovanie-metroklastera">  Metro Cluster Planung </h2><br><p>  Dieser Abschnitt erhebt keinen Anspruch auf eine umfassende Anleitung f√ºr den Entwurf des U-Bahn-Clusters, sondern zeigt nur die Hauptrichtungen auf, die ausgearbeitet werden sollten, wenn Sie sich f√ºr den Aufbau eines solchen Systems entscheiden.  Stellen Sie daher bei der tats√§chlichen Implementierung des U-Bahn-Clusters sicher, dass Sie den Hersteller von Speichersystemen (dh uns) und andere verwandte Systeme f√ºr Konsultationen einbeziehen. </p><br><h3 id="ploschadki">  Plattformen </h3><br><p>  Wie oben angegeben, sind f√ºr einen U-Bahn-Cluster mindestens drei Standorte erforderlich.  Zwei Rechenzentren, in denen Speichersysteme und verwandte Systeme funktionieren, sowie eine dritte Plattform, auf der der Schiedsrichter arbeiten wird. </p><br><p>  Die empfohlene Entfernung zwischen den Rechenzentren betr√§gt nicht mehr als 40 Kilometer.  Gr√∂√üere Entfernungen verursachen h√∂chstwahrscheinlich zus√§tzliche Verz√∂gerungen, die im Fall eines U-Bahn-Clusters h√∂chst unerw√ºnscht sind.  Es sei daran erinnert, dass Verz√∂gerungen bis zu 5 Millisekunden betragen sollten, obwohl es w√ºnschenswert ist, 2 zu erf√ºllen. </p><br><p>  Es wird auch empfohlen, Verz√∂gerungen w√§hrend des Planungsprozesses zu √ºberpr√ºfen.  Jeder mehr oder weniger erwachsene Anbieter, der Glasfaser zwischen den Rechenzentren bereitstellt, kann eine Qualit√§tspr√ºfung ziemlich schnell organisieren. </p><br><p>  F√ºr Verz√∂gerungen vor dem Arbiter (dh zwischen der dritten Plattform und den ersten beiden) betr√§gt der empfohlene Verz√∂gerungsschwellenwert bis zu 200 Millisekunden, dh eine regul√§re Unternehmens-VPN-Verbindung √ºber das Internet ist geeignet. </p><br><h3 id="kommutaciya-i-set">  Switching und Networking </h3><br><p>  Im Gegensatz zu einem Replikationsschema, bei dem es ausreicht, Speichersysteme von verschiedenen Standorten aus miteinander zu verbinden, erfordert ein Schema mit einem Metro-Cluster die Verbindung von Hosts mit beiden Speichersystemen an verschiedenen Standorten.  Um den Unterschied klarer zu machen, sind beide Schemata unten aufgef√ºhrt. </p><br><p><img src="https://habrastorage.org/webt/qq/tg/x-/qqtgx-ze1zjj9fhbb7drzz6zabw.png"></p><br><p><img src="https://habrastorage.org/webt/l3/vs/es/l3vsesenlgm7lalbws0gdqhysuy.png"></p><br><p>  Wie Sie dem Diagramm entnehmen k√∂nnen, betrachten die Hosts auf Site 1 sowohl SHD1 als auch SHD 2. Im Gegenteil, die Hosts von Plattform 2 betrachten SHD 2 und SHD1.  Das hei√üt, jeder Host sieht beide Speichersysteme.  Dies ist eine Voraussetzung f√ºr den Betrieb des Metro-Clusters. </p><br><p>  Nat√ºrlich muss nicht jeder Host mit einem optischen Kabel in ein anderes Rechenzentrum gezogen werden, es reichen keine Anschl√ºsse und Schn√ºrsenkel aus.  Alle diese Verbindungen m√ºssen √ºber Ethernet 10G + - oder FibreChannel 8G + -Switches hergestellt werden (FC nur zum Verbinden von Hosts und Speicher f√ºr E / A, der Replikationskanal ist derzeit nur √ºber IP verf√ºgbar (Ethernet 10G +). </p><br><p>  Nun ein paar Worte zur Netzwerktopologie.  Ein wichtiger Punkt ist die korrekte Konfiguration der Subnetze.  Sie m√ºssen sofort mehrere Subnetze f√ºr die folgenden Verkehrstypen identifizieren: </p><br><ul><li>  Das Subnetz f√ºr die Replikation, √ºber das Daten zwischen den Speichersystemen synchronisiert werden.  Es kann mehrere geben, in diesem Fall spielt es keine Rolle, alles h√§ngt von der aktuellen (bereits implementierten) Netzwerktopologie ab.  Wenn es zwei davon gibt, sollte nat√ºrlich das Routing zwischen ihnen konfiguriert werden. </li><li>  Speichersubnetze, √ºber die Hosts auf Speicherressourcen zugreifen (sofern es sich um iSCSI handelt).  In jedem Rechenzentrum sollte ein solches Subnetz vorhanden sein. </li><li>  Steuern Sie Subnetze, dh drei routingf√§hige Subnetze an drei Standorten, von denen aus die Speicherverwaltung durchgef√ºhrt wird, und es gibt auch einen Arbiter. </li></ul><br><p>  Subnetze f√ºr den Zugriff auf Hostressourcen werden hier nicht ber√ºcksichtigt, da sie stark aufgabenabh√§ngig sind. </p><br><p>  Das Aufteilen von unterschiedlichem Datenverkehr in verschiedene Subnetze ist √§u√üerst wichtig (es ist besonders wichtig, das Replikat von E / A zu trennen). Wenn Sie den gesamten Datenverkehr in ein ‚Äûdickes‚Äú Subnetz mischen, kann dieser Datenverkehr nicht verwaltet werden und kann unter den Bedingungen von zwei Rechenzentren immer noch unterschiedliche Daten verursachen Optionen f√ºr Netzwerkkollisionen.  Wir werden im Rahmen dieses Artikels nicht viel auf dieses Problem eingehen, da Sie √ºber die Planung eines Netzwerks zwischen Rechenzentren auf den Ressourcen der Hersteller von Netzwerkger√§ten lesen k√∂nnen, wo es ausf√ºhrlich beschrieben wird. </p><br><h3 id="konfiguraciya-arbitra">  Schiedsrichter-Konfiguration </h3><br><p>  Der Arbiter muss √ºber die ICMP- und SSH-Protokolle Zugriff auf alle Speicherverwaltungsschnittstellen gew√§hren.  Sie sollten auch die Fehlertoleranz des Arbiters ber√ºcksichtigen.  Es gibt eine Nuance. </p><br><p>  Die Fehlertoleranz des Arbiters ist sehr w√ºnschenswert, aber optional.  Und was passiert, wenn der Schiedsrichter zur falschen Zeit abst√ºrzt? </p><br><ul><li>  Der Betrieb des U-Bahn-Clusters im Normalmodus √§ndert sich nicht, weil  arbtir hat keinerlei Auswirkungen auf den Betrieb des U-Bahn-Clusters im normalen Modus (seine Aufgabe besteht darin, die Last zwischen den Rechenzentren rechtzeitig umzuschalten). </li><li>  Wenn der Schiedsrichter aus dem einen oder anderen Grund f√§llt und den Unfall im Rechenzentrum "aufweckt", erfolgt keine Umschaltung, da niemand die erforderlichen Befehle zum Umschalten und Organisieren eines Quorums erteilt.  In diesem Fall wird der U-Bahn-Cluster zu einem regul√§ren Replikationsschema, das w√§hrend einer Katastrophe von Hand umgeschaltet werden muss, was sich auf RTO auswirkt. </li></ul><br><p>  Was folgt daraus?  Wenn Sie wirklich eine minimale RTO sicherstellen m√ºssen, m√ºssen Sie die Fehlertoleranz des Arbiters sicherstellen.  Hierf√ºr gibt es zwei M√∂glichkeiten: </p><br><ul><li>  F√ºhren Sie eine virtuelle Maschine mit einem Arbiter auf einem Failover-Hypervisor aus, da alle erwachsenen Hypervisoren ein Failover unterst√ºtzen. </li><li>  Wenn am dritten Standort (in einem bedingten B√ºro) <del>  Faulheit, einen normalen Cluster zu setzen </del>  Da es keinen vorhandenen Hypervizor-Cluster gibt, haben wir eine Hardwareversion des Arbiters bereitgestellt, die in einer 2U-Box hergestellt wird, in der zwei normale x-86-Server funktionieren und die einen lokalen Fehler √ºberleben k√∂nnen. </li></ul><br><p>  Wir empfehlen dringend, dass der Schiedsrichter fehlertolerant ist, obwohl der Metro-Cluster ihn im normalen Modus nicht ben√∂tigt.  Sowohl Theorie als auch Praxis zeigen jedoch, dass es besser ist, auf Nummer sicher zu gehen, wenn Sie eine wirklich zuverl√§ssige, katastrophensichere Infrastruktur aufbauen.  Es ist besser, sich und Ihr Unternehmen vor dem "Gesetz der Gemeinheit" zu sch√ºtzen, dh vor dem Ausfall sowohl des Schiedsrichters als auch eines der Standorte, an denen sich das Speichersystem befindet. </p><br><h3 id="arhitektura-resheniya">  L√∂sungsarchitektur </h3><br><p>  Unter Ber√ºcksichtigung der oben genannten Anforderungen erhalten wir die folgende allgemeine L√∂sungsarchitektur. </p><br><p><img src="https://habrastorage.org/webt/di/wt/oq/diwtoqu2jdf7ik-nsigr4ypx37s.png"></p><br><p>  LUNs sollten gleichm√§√üig auf zwei Standorte verteilt sein, um starke √úberlastungen zu vermeiden.  Gleichzeitig muss bei der Dimensionierung in beiden Rechenzentren nicht nur ein doppeltes Volume (das zum gleichzeitigen Speichern von Daten auf zwei Speichersystemen erforderlich ist), sondern auch eine doppelte Leistung in IOPS und MB / s festgelegt werden, um eine Verschlechterung der Anwendungen bei Ausfall eines der Rechenzentren zu verhindern. ov. </p><br><p>  Unabh√§ngig davon stellen wir fest, dass bei einem ordnungsgem√§√üen Ansatz zur Gr√∂√üenbestimmung (dh vorausgesetzt, wir haben die richtigen Obergrenzen f√ºr IOPS und MB / s sowie die erforderlichen CPU- und RAM-Ressourcen angegeben) bei einem Ausfall eines der Speichersysteme im U-Bahn-Cluster kein schwerwiegender Leistungsabfall auftritt tempor√§re Arbeit an einem Speichersystem. </p><br><p>  Dies liegt an der Tatsache, dass unter den Bedingungen von zwei Standorten gleichzeitig die synchrone Replikation die H√§lfte der Schreibleistung ‚Äûverschlingt‚Äú, da jede Transaktion auf zwei Speichersysteme geschrieben werden muss (√§hnlich wie bei RAID-1/10).  Wenn also eines der Speichersysteme ausf√§llt, verschwindet der Effekt der vor√ºbergehenden Replikation (bis das ausgefallene Speichersystem steigt) und die Schreibleistung wird um das Doppelte erh√∂ht.  Nachdem die LUNs eines ausgefallenen Speichersystems auf einem funktionierenden Speichersystem neu gestartet wurden, verschwindet diese zweifache Erh√∂hung aufgrund der Belastung durch die LUNs eines anderen Speichersystems und wir kehren zu dem Leistungsniveau zur√ºck, das wir vor dem ‚ÄûL√∂schen‚Äú hatten. aber nur im Rahmen einer Plattform. </p><br><p>  Mit Hilfe einer kompetenten Dimensionierung k√∂nnen Bedingungen bereitgestellt werden, unter denen Benutzer den Ausfall eines gesamten Speichersystems √ºberhaupt nicht sp√ºren.  Aber auch dies erfordert eine sehr sorgf√§ltige Dimensionierung, f√ºr die Sie uns √ºbrigens kostenlos kontaktieren k√∂nnen :-). </p><br><h2 id="nastroyka-metroklastera">  Metro Cluster Setup </h2><br><p>  Das Einrichten eines Metro-Clusters √§hnelt dem Einrichten einer regul√§ren Replikation, die wir in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">vorherigen Artikel beschrieben haben</a> .  Daher konzentrieren wir uns nur auf die Unterschiede.  Wir haben einen Stand basierend auf der oben beschriebenen Architektur im Labor nur in der Minimalversion eingerichtet: zwei √ºber 10G-Ethernet miteinander verbundene Speichersysteme, zwei 10G-Switches und ein Host, der die 10 Speicherports √ºber die Switches auf beiden Speichersystemen betrachtet.  Der Arbiter wird in einer virtuellen Maschine ausgef√ºhrt. </p><br><p><img src="https://habrastorage.org/webt/gy/-v/ci/gy-vci08eyyimjl6lwa2mciot8k.png"></p><br><p>  W√§hlen Sie beim Einrichten von virtuellen IPs (VIPs) f√ºr ein Replikat den VIP-Typ f√ºr den Metro-Cluster aus. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ym/aw/89/ymaw89gofhplngagm5nvveg4oe0.png"></a> </p><br><p>  Wir haben zwei Replikationsverbindungen f√ºr zwei LUNs erstellt und diese auf zwei Speichersysteme verteilt: LUN TEST Primary f√ºr SHD1 (METRO-Verbindung), LUN TEST2 Primary f√ºr SHD2 (METRO2-Verbindung). </p><br><p><img src="https://habrastorage.org/webt/rz/7w/6e/rz7w6esdq4bued37xobjg1obcha.jpeg"></p><br><p>  F√ºr sie haben wir zwei identische Ziele eingerichtet (in unserem Fall iSCSI, aber FC wird auch unterst√ºtzt, die Logik der Einstellung ist dieselbe). </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/d8/r-/dw/d8r-dw3gglptcowecdjabpewdve.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/lf/ys/_6/lfys_6aohva42tp89ddqcjhakes.jpeg"></p><br><p>  F√ºr Replikationsverbindungen haben sie Zuordnungen auf jedem Speichersystem vorgenommen. </p><br><p>  SHD1: </p><br><p><img src="https://habrastorage.org/webt/fw/qe/5q/fwqe5qqhcir_4rlbj3usmea1hgg.jpeg"></p><br><p>  SHD2: </p><br><p><img src="https://habrastorage.org/webt/rn/68/ap/rn68apxwkixmyh3dpv70vo-9pk8.jpeg"></p><br><p>  Multipath konfiguriert und dem Host pr√§sentiert. </p><br><p><img src="https://habrastorage.org/webt/gx/ii/wj/gxiiwjndb_fzeywc5hrxdfbqnrk.jpeg"></p><br><p><img src="https://habrastorage.org/webt/c6/df/5j/c6df5jjxnggdqk8wnjfbebb1rty.jpeg"></p><br><h3 id="nastraivaem-arbitra">  Konfigurieren Sie den Arbiter </h3><br><p>  Sie m√ºssen mit dem Schiedsrichter selbst nichts Besonderes tun, sondern ihn nur auf der dritten Plattform aktivieren, ihm eine IP-Adresse festlegen und den Zugriff √ºber ICMP und SSH konfigurieren.  Die Konfiguration selbst erfolgt √ºber die Speichersysteme selbst.  In diesem Fall reicht es aus, den Arbiter einmal auf einem der Speichercontroller im Metro-Cluster zu konfigurieren. Diese Einstellungen werden automatisch an alle Controller verteilt. </p><br><p>  Im Abschnitt Remote-Replikation &gt;&gt; Metrocluster (auf einem beliebigen Controller) &gt;&gt; Schaltfl√§che Konfigurieren. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/sy/h0/x-/syh0x-gywrz2yu8w9mk0us5mwcg.jpeg"></a> </p><br><p>  Wir stellen die IP des Arbiters sowie die Steuerschnittstellen der beiden Controller des Remote-Speichersystems vor. </p><br><p><img src="https://habrastorage.org/webt/rs/u2/dt/rsu2dt3tt18k1it_qvmdsdnxldy.jpeg"></p><br><p>  Danach m√ºssen Sie alle Dienste aktivieren (die Schaltfl√§che "Alles neu starten").  Im Falle einer zuk√ºnftigen Neukonfiguration m√ºssen die Dienste neu gestartet werden, damit die Einstellungen wirksam werden. </p><br><p><img src="https://habrastorage.org/webt/kj/sb/7u/kjsb7u9rrzlk6cttl7amr_ebxma.jpeg"></p><br><p>  √úberpr√ºfen Sie, ob alle Dienste ausgef√ºhrt werden. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/oh/mw/xl/ohmwxl5jry0dufgfamey0ddxris.jpeg"></a> </p><br><p>  <strong>Damit ist die Einrichtung des U-Bahn-Clusters abgeschlossen.</strong> </p><br><h2 id="krash-test">  Crashtest </h2><br><p>  Der Crashtest ist in unserem Fall recht einfach und schnell, da die Replikationsfunktionalit√§t (Switching, Konsistenz usw.) in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">fr√ºheren Artikel</a> ber√ºcksichtigt wurde.  Um die Zuverl√§ssigkeit eines U-Bahn-Clusters zu testen, reicht es daher aus, die Automatisierung der Unfallerkennung, des Umschaltens und das Fehlen von Aufzeichnungsverlusten (E / A-Stopps) zu √ºberpr√ºfen. </p><br><p>  Zu diesem Zweck emulieren wir den vollst√§ndigen Ausfall eines der Speichersysteme, indem wir beide Controller physisch herunterfahren und eine gro√üe Datei vorl√§ufig in die LUN kopieren, die auf dem anderen Speichersystem aktiviert werden muss. </p><br><p><img src="https://habrastorage.org/webt/f-/lb/fo/f-lbfonzy8n4iawaihumetqtkmo.png"></p><br><p>  Deaktivieren Sie einen Speicher.  Auf dem zweiten Speichersystem werden in den Protokollen Warnungen und Meldungen angezeigt, dass die Verbindung zum benachbarten System unterbrochen wurde.  Wenn Sie Warnungen f√ºr die SMTP- oder SNMP-√úberwachung konfiguriert haben, erh√§lt der Administrator die entsprechenden Warnungen. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ae/jo/xv/aejoxv6piwk4fvl-nc1leklbxce.jpeg"></a> </p><br><p>  Genau 10 Sekunden sp√§ter (in beiden Screenshots zu sehen) wurde der METRO-Replikationslink (derjenige, der auf dem abgest√ºrzten Speichersystem prim√§r war) automatisch auf dem laufenden Speichersystem prim√§r.  Unter Verwendung der vorhandenen Zuordnung blieb LUN TEST f√ºr den Host verf√ºgbar, die Aufzeichnung sackte etwas ab (innerhalb der versprochenen 10 Prozent), brach jedoch nicht ab. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/v_/gj/d9/v_gjd9sgoi5hwrgjdkwxwtdomeu.jpeg"></a> </p><br><p><img src="https://habrastorage.org/webt/ih/lq/rg/ihlqrgv7dq-4fv6dlnhair6nib4.jpeg"></p><br><p>  <strong>Test erfolgreich abgeschlossen.</strong> </p><br><h2 id="podvodim-itog">  Zusammenfassend </h2><br><p>  Die derzeitige Implementierung des Metroclusters in den Speichersystemen der AERODISK Engine N-Serie erm√∂glicht die vollst√§ndige L√∂sung von Problemen, bei denen Ausfallzeiten von IT-Services vermieden oder minimiert werden m√ºssen und deren Betrieb rund um die Uhr und 365 Tage die Woche mit minimalem Arbeitsaufwand sichergestellt werden muss. </p><br><p>  , ,    ,      ‚Ä¶       ,      ,    .      ,         ,        ,         . </p><br><p> ,   . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de460305/">https://habr.com/ru/post/de460305/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de460287/index.html">Runet News Visualisierung</a></li>
<li><a href="../de460291/index.html">Probleme der Stapelverarbeitung von Anfragen und deren L√∂sungen (Teil 1)</a></li>
<li><a href="../de460295/index.html">Was bedeutet unsicher in Rust?</a></li>
<li><a href="../de460297/index.html">WeakRef - Vorschlag zur Erg√§nzung des ECMAScript-Standards</a></li>
<li><a href="../de460301/index.html">Hochleistungs-LED-Lampen der neuen Generation</a></li>
<li><a href="../de460307/index.html">Modellierungserfahrung vom Computer Vision Mail.ru-Team</a></li>
<li><a href="../de460311/index.html">Zeit f√ºr eine neue Geldtheorie</a></li>
<li><a href="../de460313/index.html">Haben verschiedene Hits etwas gemeinsam?</a></li>
<li><a href="../de460319/index.html">Jagd nach Weltrauminspektoren</a></li>
<li><a href="../de460321/index.html">Galerie der besten ML- und Data Science-Notizb√ºcher</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>