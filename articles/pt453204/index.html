<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👇🏼 🙌 📡 Contêineres, microsserviços e malhas de serviço 👨🏿‍💼 🚇 ⛔️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Existem vários artigos sobre a malha de serviço na Internet, e aqui está outro. Viva! Mas porque? Então, o que quero expressar minha opinião é que ser...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Contêineres, microsserviços e malhas de serviço</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/453204/">  Existem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">vários</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigos</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sobre a</a> malha de serviço <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na</a> Internet, e aqui está outro.  Viva!  Mas porque?  Então, o que quero expressar minha opinião é que seria melhor que malhas de serviço aparecessem há 10 anos, antes do surgimento de plataformas de contêineres como Docker e Kubernetes.  Não afirmo que meu ponto de vista seja melhor ou pior que outros, mas como as malhas de serviço são animais bastante complexos, a multiplicidade de pontos de vista ajudará a melhor entendê-los. <br><br>  Falarei sobre a plataforma dotCloud, que foi construída em mais de cem microsserviços e suportou milhares de aplicativos em contêineres.  Explicarei os problemas que encontramos durante seu desenvolvimento e lançamento e como as malhas de serviço podem ajudar (ou não). <br><a name="habracut"></a><br><h1>  História do dotCloud </h1><br>  Eu já escrevi sobre a história do dotCloud e a escolha da arquitetura para esta plataforma, mas falei um pouco sobre o nível da rede.  Se você não quiser ler o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo anterior</a> sobre o dotCloud, aqui está um breve resumo: é uma plataforma PaaS como serviço que permite aos clientes iniciar uma ampla gama de aplicativos (Java, PHP, Python ...), com suporte para uma ampla variedade de serviços de dados (MongoDB, MySQL, Redis ...) e um fluxo de trabalho como o Heroku: você carrega seu código na plataforma, cria imagens de contêineres e as implementa. <br><br>  Vou lhe dizer como o tráfego foi direcionado para a plataforma dotCloud.  Não porque foi especialmente legal (embora o sistema funcionou bem por seu tempo!), Mas principalmente porque, com a ajuda de ferramentas modernas, esse design pode ser facilmente implementado em pouco tempo por uma equipe modesta, se eles precisam de uma maneira de direcionar o tráfego entre vários microsserviços ou um monte de aplicativos.  Assim, você pode comparar as opções: o que acontece se você desenvolver tudo sozinho ou usar a malha de serviço existente.  Escolha padrão: faça você mesmo ou compre. <br><br><h1>  Roteamento de tráfego para aplicativos hospedados </h1><br>  Os aplicativos DotCloud podem fornecer pontos de extremidade HTTP e TCP. <br><br>  <b>Os pontos de extremidade HTTP são</b> adicionados dinamicamente à configuração de cluster do balanceador de carga <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Hipache</a> .  Isso é semelhante ao que os recursos do Kubernetes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ingress</a> e um balanceador de carga como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Traefik fazem hoje</a> . <br><br>  Os clientes se conectam aos pontos de extremidade HTTP por meio de seus respectivos domínios, desde que o nome do domínio aponte para os balanceadores de carga do dotCloud.  Nada de especial. <br><br>  <b>Os pontos de extremidade TCP</b> são associados a um número de porta, que é passado para todos os contêineres dessa pilha por meio de variáveis ​​de ambiente. <br><br>  Os clientes podem se conectar aos pontos de extremidade do TCP usando o nome do host apropriado (algo como gateway-X.dotcloud.com) e o número da porta. <br><br>  Esse nome de host é resolvido para um cluster de servidores "nats" (não relacionados ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NATS</a> ) que encaminham as conexões TCP de entrada para o contêiner correto (ou, no caso de serviços com balanceamento de carga, para os contêineres corretos). <br><br>  Se você estiver familiarizado com o Kubernetes, isso provavelmente lembrará os serviços do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">NodePort</a> . <br><br>  Não havia serviços <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ClusterIP</a> equivalentes na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">plataforma dotCloud</a> : por simplicidade, o acesso aos serviços era o mesmo, tanto de dentro como de fora da plataforma. <br><br>  Tudo foi organizado de maneira simples: as implementações iniciais das redes de roteamento HTTP e TCP, provavelmente apenas algumas centenas de linhas de Python.  Algoritmos simples (eu diria ingênuos) que foram finalizados com o crescimento da plataforma e o advento de requisitos adicionais. <br><br>  Não foi necessária uma refatoração extensiva do código existente.  Em particular, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aplicativos de 12 fatores</a> podem usar diretamente o endereço obtido por meio de variáveis ​​de ambiente. <br><br><h1>  Como isso difere de uma malha de serviço moderna? </h1><br>  <b>Visibilidade</b> limitada.  Geralmente, não tínhamos métricas para a grade de roteamento TCP.  Quanto ao roteamento HTTP, versões posteriores têm métricas HTTP detalhadas com códigos de erro e tempos de resposta, mas as malhas de serviço modernas vão ainda mais longe, fornecendo integração com sistemas de coleta de métricas como o Prometheus, por exemplo. <br><br>  A visibilidade é importante não apenas do ponto de vista operacional (para ajudar a solucionar problemas), mas também quando novos recursos são lançados.  Trata-se de uma <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">implantação azul esverdeada</a> segura e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">implantação de canários</a> . <br><br>  <b>A eficiência de roteamento</b> também <b>é</b> limitada.  Na grade de roteamento dotCloud, todo o tráfego precisava passar por um cluster de nós de roteamento dedicados.  Isso significou um possível cruzamento de várias fronteiras do AZ (zonas de acessibilidade) e um aumento significativo no atraso.  Lembro-me de como corrigi problemas com o código que fazia mais de cem consultas SQL por página e, para cada consulta, abriu uma nova conexão com o servidor SQL.  Quando iniciada localmente, a página é carregada instantaneamente, mas no dotCloud, o carregamento leva alguns segundos, porque leva dezenas de milissegundos para cada conexão TCP (e consulta SQL subsequente).  Nesse caso em particular, conexões persistentes resolveram o problema. <br><br>  As malhas de serviço modernas se saem melhor com esses problemas.  Primeiro, eles verificam se as conexões são roteadas <i>na origem</i> .  O fluxo lógico é o mesmo: <code> →  → </code> , mas agora a malha funciona localmente e não em nós remotos, portanto, a conexão <code> → </code> é local e muito rápida (microssegundos em vez de milissegundos). <br><br>  As malhas de serviço modernas também implementam algoritmos de balanceamento de carga mais inteligentes.  Ao controlar o desempenho dos back-ends, eles podem enviar mais tráfego para back-ends mais rápidos, o que leva a um aumento no desempenho geral. <br><br>  <b>A segurança também</b> é melhor.  A grade de roteamento do dotCloud funcionou completamente no EC2 Classic e não criptografou o tráfego (supondo que se alguém conseguisse detectar o tráfego de rede do EC2, você já terá grandes problemas).  As malhas de serviço modernas protegem de forma transparente todo o nosso tráfego, por exemplo, com autenticação TLS mútua e criptografia subsequente. <br><br><h1>  Roteamento de tráfego para serviços de plataforma </h1><br>  Ok, discutimos o tráfego entre aplicativos, mas e a plataforma dotCloud? <br><br>  A plataforma em si consistia em cerca de cem microsserviços responsáveis ​​por várias funções.  Alguns receberam pedidos de outros, e outros eram trabalhadores em segundo plano que se conectaram a outros serviços, mas não aceitaram conexões.  De qualquer forma, cada serviço deve conhecer os pontos de extremidade dos endereços aos quais é necessário se conectar. <br><br>  Muitos serviços de alto nível podem usar a grade de roteamento descrita acima.  De fato, muitos dos mais de centenas de microsserviços dotCloud foram implantados como aplicativos regulares na própria plataforma dotCloud.  Mas um pequeno número de serviços de baixo nível (em particular, que implementam essa grade de roteamento) precisava de algo mais simples, com menos dependências (já que não podiam depender de si mesmos para trabalhar - um bom e velho problema de galinha e ovo). <br><br>  Esses serviços importantes e de baixo nível foram implantados executando contêineres diretamente em vários nós principais.  Ao mesmo tempo, serviços de plataforma padrão não estavam envolvidos: o vinculador, o planejador e o corredor.  Se você deseja comparar com as modernas plataformas de contêineres, é como iniciar um plano de controle com a <code>docker run</code> diretamente nos nós, em vez de delegar a tarefa Kubernetes.  Isso é bastante semelhante ao conceito de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">módulos estáticos (lares)</a> que o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">kubeadm</a> ou o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">bootkube usa</a> ao carregar um cluster independente. <br><br>  Esses serviços foram expostos de maneira simples e grosseira: seus nomes e endereços foram listados no arquivo YAML;  e cada cliente teve que tirar uma cópia desse arquivo YAML para implantação. <br><br>  Por um lado, é extremamente confiável, porque não requer suporte a um armazenamento de chave / valor externo como o Zookeeper (não se esqueça, naquele momento o etcd ou o Consul ainda não existiam).  Por outro lado, isso dificultava a movimentação de serviços.  Sempre que em movimento, todos os clientes devem ter recebido um arquivo YAML atualizado (e potencialmente reinicializados).  Não é muito conveniente! <br><br>  Posteriormente, começamos a introduzir um novo esquema, em que cada cliente se conectava a um servidor proxy local.  Em vez do endereço e da porta, basta que ele saiba apenas o número da porta do serviço e conecte-se através do <code>localhost</code> .  O servidor proxy local processa essa conexão e a encaminha para o servidor real.  Agora, ao mover o back-end para outra máquina ou escalar em vez de atualizar todos os clientes, é necessário atualizar apenas todos esses proxies locais;  e uma reinicialização não é mais necessária. <br><br>  (Também foi planejado encapsular o tráfego nas conexões TLS e colocar outro servidor proxy no lado receptor, além de verificar os certificados TLS sem a participação do serviço receptor, configurado para aceitar conexões apenas no <code>localhost</code> . Mais sobre isso mais adiante). <br><br>  Isso é muito semelhante ao <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SmartStack</a> do Airbnb, mas a diferença significativa é que o SmartStack é implementado e implantado na produção, enquanto o sistema interno de roteamento dotCloud foi colocado em uma caixa quando o dotCloud se transformou no Docker. <br><br>  Pessoalmente, considero o SmartStack um dos antecessores de sistemas como Istio, Linkerd e Consul Connect, porque todos seguem o mesmo padrão: <br><br><ul><li>  Executando proxies em cada nó. <br></li><li>  Clientes se conectam ao proxy. <br></li><li>  O plano de gerenciamento atualiza a configuração do proxy ao alterar back-end. <br></li><li>  ... Lucro! </li></ul><br><h1>  Implementação moderna de uma malha de serviço </h1><br>  Se precisarmos implementar uma grade semelhante hoje, podemos usar princípios semelhantes.  Por exemplo, configure a zona DNS interna mapeando nomes de serviço para endereços em <code>127.0.0.0/8</code> .  Em seguida, execute o HAProxy em cada nó do cluster, aceitando conexões com cada endereço de serviço ( <code>127.0.0.0/8</code> nesta sub-rede) e redirecionando / equilibrando a carga para os back-end correspondentes.  A configuração do HAProxy pode ser controlada pelo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">confd</a> , permitindo armazenar informações de back-end no etcd ou Consul e enviar automaticamente a configuração atualizada ao HAProxy quando necessário. <br><br>  É assim que o Istio funciona!  Mas com algumas diferenças: <br><br><ul><li>  Usa o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Envoy Proxy em</a> vez do HAProxy. <br></li><li>  Salva a configuração de back-end via API do Kubernetes, em vez do etcd ou Consul. <br></li><li>  Os serviços são endereços alocados na sub-rede interna (endereços do Kubernetes ClusterIP) em vez de 127.0.0.0/8. <br></li><li>  Possui um componente opcional (Citadel) para adicionar autenticação TLS mútua entre o cliente e os servidores. <br></li><li>  Suporta novos recursos, como quebra de circuito, rastreamento distribuído, implantação de canários, etc. </li></ul><br>  Vamos dar uma olhada rápida em algumas das diferenças. <br><br><h3>  Proxy enviado </h3><br>  O Enftoy Proxy foi escrito por Lyft [concorrente da Uber no mercado de táxis - aprox.  trans.].  É muito semelhante a outros proxies de várias maneiras (por exemplo, HAProxy, Nginx, Traefik ...), mas Lyft escreveu o seu próprio porque precisava de funções que não estão em outros proxies, e parecia mais razoável criar um novo do que expandir o existente. <br><br>  O enviado pode ser usado sozinho.  Se eu tiver um serviço específico que deve se conectar a outros serviços, posso configurá-lo para conectar-se ao Envoy e, em seguida, configurar e reconfigurar dinamicamente o Envoy com a localização de outros serviços, enquanto recebo muitos recursos adicionais excelentes, por exemplo, visibilidade.  Em vez de uma biblioteca-cliente personalizada ou incorporação de rastreamento de chamadas no código, direcionamos o tráfego para o Envoy e ele coleta métricas para nós. <br><br>  Mas o Envoy também pode trabalhar como um plano de dados para uma malha de serviço.  Isso significa que, para essa malha de serviço, o Envoy agora está configurado <i>pelo</i> plano de controle. <br><br><h3>  Plano de controle </h3><br>  No plano de gerenciamento, o Istio conta com a API Kubernetes.  <i>Isso não é muito diferente do uso de confd</i> , que depende do etcd ou Consul para exibir um conjunto de chaves em um data warehouse.  O Istio, por meio da API do Kubernetes, exibe o conjunto de recursos do Kubernetes. <br><br>  <i>Entre o caso</i> : eu pessoalmente achei <a href="">útil</a> essa <a href="">descrição da API do Kubernetes</a> , que diz: <br><br><blockquote>  O servidor de API do Kubernetes é um "servidor burro" que oferece armazenamento, versão, validação, atualização e semântica de recursos da API. </blockquote><br>  O Istio foi projetado para trabalhar com o Kubernetes;  e se você quiser usá-lo fora do Kubernetes, precisará executar uma instância do servidor da API do Kubernetes (e serviço auxiliar etcd). <br><br><h3>  Endereços de Serviço </h3><br>  O Istio conta com os endereços ClusterIP que o Kubernetes aloca, para que os serviços do Istio obtenham um endereço interno (não no intervalo <code>127.0.0.0/8</code> ). <br><br>  O tráfego para o endereço ClusterIP de um serviço específico no cluster Kubernetes sem o Istio é interceptado pelo kube-proxy e enviado à parte do servidor desse proxy.  Se você estiver interessado em detalhes técnicos, o kube-proxy definirá as regras do iptables (ou balanceadores de carga IPVS, dependendo de como você a configura) para reescrever os endereços IP de destino das conexões que vão para o endereço ClusterIP. <br><br>  Depois de instalar o Istio no cluster Kubernetes, nada muda até que seja explicitamente ativado para o consumidor em questão ou mesmo para todo o espaço para nome, introduzindo o contêiner <code>sidecar</code> em lareiras personalizadas.  Esse contêiner iniciará uma instância do Envoy e definirá uma série de regras do iptables para interceptar o tráfego para outros serviços e redirecionar esse tráfego para o Envoy. <br><br>  Quando integrado ao DNS do Kubernetes, isso significa que nosso código pode se conectar pelo nome do serviço e tudo "simplesmente funciona".  Em outras palavras, nosso código emite solicitações como <code>http://api/v1/users/4242</code> , depois a <code>api</code> resolve a solicitação para <code>10.97.105.48</code> , as regras do iptables interceptam conexões de 10.97.105.48 e as redirecionam para o proxy local do Envoy, e esse proxy local direciona solicitação para a API de back-end real.  Fuh! <br><br><h3>  Coisinhas extras </h3><br>  O Istio também fornece criptografia e autenticação de ponta a ponta através do mTLS (TLS mútuo).  O componente chamado <i>Citadel</i> é responsável por isso. <br><br>  Também existe um componente do <i>Mixer</i> que a Envoy pode solicitar para <i>cada</i> solicitação, a fim de tomar uma decisão especial sobre essa solicitação, dependendo de vários fatores, como cabeçalhos, carregamento de back-end, etc. ... (não se preocupe: existem várias maneiras de garantir que o Mixer funcione e até se travar, o Enviado continuará funcionando normalmente como proxy). <br><br>  E, é claro, mencionamos visibilidade: o Envoy coleta um grande número de métricas, fornecendo rastreamento distribuído.  Na arquitetura dos microsserviços, se uma solicitação de API precisar passar pelos microsserviços A, B, C e D, quando você efetuar login no sistema, o rastreio distribuído adicionará um identificador exclusivo à solicitação e salvará esse identificador através de subconsultas em todos esses microsserviços, permitindo registrar todas as chamadas relacionadas, seus atrasos etc. <br><br><h1>  Desenvolver ou comprar </h1><br>  Istio tem uma reputação de ser um sistema complexo.  Por outro lado, a construção de uma grade de roteamento, que descrevi no início deste post, é relativamente simples usando as ferramentas existentes.  Então, faz sentido criar sua própria malha de serviço? <br><br>  Se tivermos necessidades modestas (você não precisa de visibilidade, um disjuntor e outras sutilezas), então pensamos em desenvolver sua própria ferramenta.  Mas se usarmos o Kubernetes, pode até não ser necessário, porque o Kubernetes já fornece ferramentas básicas para descoberta de serviços e balanceamento de carga. <br><br>  Porém, se tivermos requisitos avançados, "comprar" uma malha de serviço parece ser uma opção muito melhor.  (Isso nem sempre é uma "compra", porque o Istio vem com código-fonte aberto, mas ainda precisamos investir tempo de engenharia para entender seu trabalho, implantá-lo e gerenciá-lo). <br><br><h1>  O que escolher: Istio, Linkerd ou Consul Connect? </h1><br>  Até agora, falamos apenas do Istio, mas essa não é a única malha de serviço.  Uma alternativa popular é o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Linkerd</a> , e também o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Consul Connect</a> . <br><br>  O que escolher? <br><br>  Honestamente, eu não sei.  No momento, não me considero competente o suficiente para responder a essa pergunta.  Existem alguns <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigos</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">interessantes</a> comparando essas ferramentas e até <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">referências</a> . <br><br>  Uma abordagem promissora é usar uma ferramenta como o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">SuperGloo</a> .  Ele implementa uma camada de abstração para simplificar e unificar as APIs fornecidas pelas malhas de serviço.  Em vez de estudar APIs específicas (e, na minha opinião, relativamente complexas) de várias malhas de serviço, podemos usar construções SuperGloo mais simples - e alternar facilmente de uma para outra, como se tivéssemos um formato de configuração intermediário que descreve interfaces HTTP e back-end capazes de gerar a configuração real para Nginx, HAProxy, Traefik, Apache ... <br><br>  Aceitei um pouco o Istio e o SuperGloo e, no próximo artigo, quero mostrar como adicionar o Istio ou o Linkerd a um cluster existente usando o SuperGloo, e quanto o último suportará seu trabalho, ou seja, permite alternar de uma malha de serviço para outra sem reescrever as configurações. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt453204/">https://habr.com/ru/post/pt453204/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt453190/index.html">ANPR usando RoR & React Native</a></li>
<li><a href="../pt453192/index.html">Sincronização e assincronia de processos</a></li>
<li><a href="../pt453194/index.html">Resolvemos o problema do Best Reverser com PHDays 9</a></li>
<li><a href="../pt453196/index.html">Pesquisa da Forrester: uma comparação dos dez principais fornecedores de análise de composição de software</a></li>
<li><a href="../pt453200/index.html">Discussão: o projeto OpenROAD pretende resolver a tarefa de automatizar o design dos processadores</a></li>
<li><a href="../pt453206/index.html">Entrevista com Kelsey Moody: Como construir uma empresa e acabar com patologias relacionadas à idade</a></li>
<li><a href="../pt453212/index.html">Consumer Reports: O mais recente piloto automático da Tesla está longe de ser perfeito</a></li>
<li><a href="../pt453214/index.html">Como e por que manter a forma se você é uma pessoa de TI em um site remoto</a></li>
<li><a href="../pt453216/index.html">Sistemas de monitoramento de tráfego em redes VoIP. Parte Dois - Princípios da Organização</a></li>
<li><a href="../pt453218/index.html">A principal coisa com o YaC 2019: cem drones nas estradas, Yandex.Module, food, smart home</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>