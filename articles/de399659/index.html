<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🐡 👨🏾‍🚒 ⏰ Maschinengerücht. SoundNet neuronales Netzwerk trainiert, um Objekte durch Ton zu erkennen 🤴🏿 🍄 ⚙️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Links: Ein Versuch, die Szene und Objekte nur am Ton zu erkennen. Rechts: eine echte Tonquelle.
 
 In jüngster Zeit haben neuronale Netze erhebliche F...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Maschinengerücht. SoundNet neuronales Netzwerk trainiert, um Objekte durch Ton zu erkennen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/399659/"><img src="https://habrastorage.org/files/ddf/3fb/5b2/ddf3fb5b2c1f4db590d386cf633c3503.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Links: Ein Versuch, die Szene und Objekte nur am Ton zu erkennen. Rechts: eine echte Tonquelle.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In jüngster Zeit haben neuronale Netze erhebliche Fortschritte bei der Erkennung von Objekten und Szenen in Videos erzielt. Solche Erfolge werden durch Training an massiven Datensätzen mit markierten Objekten ermöglicht (siehe z. B. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">„Erlernen tiefer Funktionen für die Szenenerkennung mithilfe der Ortsdatenbank“</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . NIPS, 2014). Durch Betrachten von Fotos oder Videos kann der Computer die Szene fast genau bestimmen, indem er aus </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">401 Szenen</font></a><font style="vertical-align: inherit;"> eine geeignete Beschreibung auswählt</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zum Beispiel eine überfüllte Küche, eine stilvolle Küche, ein Schlafzimmer für Teenager usw. Auf dem Gebiet des Verständnisses haben die Klänge des neuronalen Netzwerks jedoch noch keinen solchen Fortschritt gezeigt. Spezialisten des Informatik- und Künstlichen Intelligenzlabors (CSAIL) des Massachusetts Institute of Technology haben diesen Mangel durch die Entwicklung des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoundNet-Systems für</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> maschinelles Lernen </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">behoben</font></a><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Tat ist es genauso wichtig, eine Szene per Ton zu lokalisieren wie eine Szene per Video. Am Ende kann das Bild von der Kamera oft verschwommen sein oder nicht genügend Informationen liefern. Wenn das Mikrofon funktioniert, kann der Roboter bereits herausfinden, wo es sich befindet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aus wissenschaftlicher Sicht ist das Training von neuronalen SoundNet-Netzen eine banale Aufgabe. CSAIL-Mitarbeiter verwendeten die natürliche Synchronisationsmethode zwischen Bildverarbeitung und maschinellem Hören und lehrten das neuronale Netzwerk, die Klangdarstellung eines Objekts automatisch aus nicht zugewiesenem Videomaterial zu extrahieren. Für das Training verwendeten wir ungefähr 2 Millionen Flickr-Videos (26 TB Daten) sowie eine Datenbank mit kommentierten Sounds - 50 Kategorien und ungefähr 2000 Samples. </font></font><br>
<br>
<img src="https://habrastorage.org/files/148/b51/575/148b5157503a4b499feabc69aa563d2a.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoundNet-Architektur für neuronale Netze</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl das Training des neuronalen Netzwerks unter visueller Beobachtung stattfand, liefert das System im Offline-Modus ein hervorragendes Ergebnis, indem mindestens drei akustische Standardszenen klassifiziert werden, nach denen die Entwickler dies überprüft haben. Darüber hinaus ergab ein Test des neuronalen Netzwerks, dass sie unabhängig voneinander lernte, die für einige Szenen charakteristischen Geräusche zu erkennen, und die Entwickler stellten ihre Beispiele nicht zur spezifischen Erkennung dieser Objekte zur Verfügung. Anhand des nicht markierten Videomaterials erfuhr das neuronale Netzwerk selbst, welche Szene dem Klang einer jubelnden Menge (dies ist ein Stadion) und eines Vogel-Twitter (dies ist ein Rasen oder ein Park) entspricht. Gleichzeitig mit der Szene erkennt das neuronale Netzwerk ein bestimmtes Objekt, das die Schallquelle ist.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Video zeigt einige Beispiele zum Erkennen von Objekten anhand von Ton. Zuerst werden die Töne und das Erkennungsergebnis angezeigt und das Bild selbst wird unscharf - Sie können also versuchen, sich selbst zu überprüfen. Werden Sie in der Lage sein, den Ort der Aktion und das Vorhandensein bestimmter Objekte nur durch Schall so genau zu verstehen wie das neuronale Netzwerk? Was bedeutet zum Beispiel höchstwahrscheinlich das Lied "Happy Birthday To You!", Das von mehreren Personen gleichzeitig gesungen wird? Die richtige Antwort: Das Objekt </font></font><font color="white"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">brennt Kerzen</font></font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die Szene ist ein </font></font><font color="white"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Restaurant, ein Café, eine Bar</font></font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yJCjVvIY4dU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Die Bildverarbeitung hat begonnen, so gut zu funktionieren, dass wir diese Technologie auf andere Bereiche übertragen können", </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sagte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Carl Vondrick, Student am Massachusetts Institute of Technology für Elektrotechnik und Informatik, einer der Autoren der wissenschaftlichen Arbeit. - Wir haben die natürliche Beziehung zwischen Computer Vision und Sound genutzt. Aufgrund der Vielzahl unbeschrifteter Videomaterialien war es möglich, einen großen Maßstab zu erreichen, sodass das neuronale Netzwerk gelernt hat, Ton zu verstehen. “</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SoundNet-Tests wurden an zwei Standarddatenbanken für Tonaufnahmen durchgeführt und zeigten eine um 13-15% höhere Genauigkeit der Objekterkennung als die besten dieser Programme. In einem Datensatz mit 10 verschiedenen Klangkategorien klassifiziert SoundNet Klänge mit einer Genauigkeit von 92% und in einem Datensatz mit 50 Kategorien mit einer Genauigkeit von 74%. Zum Vergleich zeigen Personen mit denselben Datensätzen eine Erkennungsgenauigkeit von durchschnittlich 96% und 81%.</font></font><br>
<br>
<img src="https://habrastorage.org/files/b91/d3c/4fb/b91d3c4fb4f24d13b133a6706f528744.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sogar Menschen können manchmal nicht genau bestimmen, was sie hören. Versuchen Sie, ein solches Experiment selbst durchzuführen. Lassen Sie einen Kollegen ein beliebiges Video von YouTube starten - und Sie versuchen, nicht auf den Monitor zu schauen, um zu sagen, was passiert, woher die Geräusche kommen und was auf dem Bildschirm angezeigt wird. Weit davon entfernt, immer zu erraten. Die Aufgabe für künstliche Intelligenz ist also nicht einfach, aber SoundNet hat es ganz gut geschafft. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Zukunft könnten solche Computerprogramme praktische Anwendung finden. Beispielsweise erkennt Ihr Mobiltelefon automatisch, dass Sie einen öffentlichen Ort betreten haben - ein Kino oder ein Theater - und schaltet die Ruftonlautstärke automatisch stumm. Wenn der Film gestartet wurde und sich das Publikum beruhigt hat, schaltet das Telefon den Ton automatisch aus und den Vibrationsalarm ein.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Orientierung nach Gelände durch Geräusche hilft bei Steuerungsprogrammen für autonome Roboter und andere Maschinen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Sicherheitssystemen und Smart Homes kann das System auf bestimmte Geräusche automatisch auf bestimmte Geräusche reagieren. </font><font style="vertical-align: inherit;">Zum Beispiel das Geräusch eines zerbrochenen Fensters. </font><font style="vertical-align: inherit;">In den „Smart Cities“ der Zukunft wird die Erkennung von Straßenlärm helfen, die Ursachen zu verstehen und mit Schallverschmutzung umzugehen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wissenschaftliche Artikel </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">veröffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 27. Oktober 2016 in den offenen Zugang zu arXiv.org (arXiv: 1610,09001, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pdf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de399659/">https://habr.com/ru/post/de399659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de399649/index.html">Ein Durchbruch bei der Energiespeicherung oder ein anderer Fall, in dem ein Wissenschaftler einen Journalisten „missbraucht“ hat?</a></li>
<li><a href="../de399651/index.html">Kalte Fusion: Experimente erzeugen Energie, die nicht sein sollte</a></li>
<li><a href="../de399653/index.html">Megahertz wird nicht gefangen, Kerne wachsen nicht. Was ist mit dem technischen Fortschritt im PC passiert?</a></li>
<li><a href="../de399655/index.html">Erschwingliche 3D-CNC-Fräsmaschinen von 250.000 bis 1.000.000 Rubel</a></li>
<li><a href="../de399657/index.html">Путь чайника в астрофото. Часть 3 — Туманность Ориона (M42)</a></li>
<li><a href="../de399663/index.html">Fragen Sie Ethan Nr. 110: Wie sah der Himmel aus, als sich die Erde gerade bildete?</a></li>
<li><a href="../de399665/index.html">Wenn auch nur für Brot</a></li>
<li><a href="../de399667/index.html">Das neuronale Netz sagt 1 Sekunde der Zukunft in der Fotografie voraus</a></li>
<li><a href="../de399669/index.html">Ein Schritt zur Seite: Warum die MacBook Pro Touchbar die Entwicklung von Touch-Interfaces nicht unterstützt</a></li>
<li><a href="../de399671/index.html">Atommüll Laserschneiden Roboter Schlange</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>