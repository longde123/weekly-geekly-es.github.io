<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üê° üë®üèæ‚Äçüöí ‚è∞ Maschinenger√ºcht. SoundNet neuronales Netzwerk trainiert, um Objekte durch Ton zu erkennen ü§¥üèø üçÑ ‚öôÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Links: Ein Versuch, die Szene und Objekte nur am Ton zu erkennen. Rechts: eine echte Tonquelle.
 
 In j√ºngster Zeit haben neuronale Netze erhebliche F...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Maschinenger√ºcht. SoundNet neuronales Netzwerk trainiert, um Objekte durch Ton zu erkennen</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/399659/"><img src="https://habrastorage.org/files/ddf/3fb/5b2/ddf3fb5b2c1f4db590d386cf633c3503.jpg"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Links: Ein Versuch, die Szene und Objekte nur am Ton zu erkennen. Rechts: eine echte Tonquelle.</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 
In j√ºngster Zeit haben neuronale Netze erhebliche Fortschritte bei der Erkennung von Objekten und Szenen in Videos erzielt. Solche Erfolge werden durch Training an massiven Datens√§tzen mit markierten Objekten erm√∂glicht (siehe z. B. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄûErlernen tiefer Funktionen f√ºr die Szenenerkennung mithilfe der Ortsdatenbank‚Äú</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . NIPS, 2014). Durch Betrachten von Fotos oder Videos kann der Computer die Szene fast genau bestimmen, indem er aus </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">401 Szenen</font></a><font style="vertical-align: inherit;"> eine geeignete Beschreibung ausw√§hlt</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zum Beispiel eine √ºberf√ºllte K√ºche, eine stilvolle K√ºche, ein Schlafzimmer f√ºr Teenager usw. Auf dem Gebiet des Verst√§ndnisses haben die Kl√§nge des neuronalen Netzwerks jedoch noch keinen solchen Fortschritt gezeigt. Spezialisten des Informatik- und K√ºnstlichen Intelligenzlabors (CSAIL) des Massachusetts Institute of Technology haben diesen Mangel durch die Entwicklung des </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoundNet-Systems f√ºr</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> maschinelles Lernen </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">behoben</font></a><font style="vertical-align: inherit;"> .</font></font><br>
<a name="habracut"></a><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In der Tat ist es genauso wichtig, eine Szene per Ton zu lokalisieren wie eine Szene per Video. Am Ende kann das Bild von der Kamera oft verschwommen sein oder nicht gen√ºgend Informationen liefern. Wenn das Mikrofon funktioniert, kann der Roboter bereits herausfinden, wo es sich befindet. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Aus wissenschaftlicher Sicht ist das Training von neuronalen SoundNet-Netzen eine banale Aufgabe. CSAIL-Mitarbeiter verwendeten die nat√ºrliche Synchronisationsmethode zwischen Bildverarbeitung und maschinellem H√∂ren und lehrten das neuronale Netzwerk, die Klangdarstellung eines Objekts automatisch aus nicht zugewiesenem Videomaterial zu extrahieren. F√ºr das Training verwendeten wir ungef√§hr 2 Millionen Flickr-Videos (26 TB Daten) sowie eine Datenbank mit kommentierten Sounds - 50 Kategorien und ungef√§hr 2000 Samples. </font></font><br>
<br>
<img src="https://habrastorage.org/files/148/b51/575/148b5157503a4b499feabc69aa563d2a.png"><br>
<i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SoundNet-Architektur f√ºr neuronale Netze</font></font></i><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Obwohl das Training des neuronalen Netzwerks unter visueller Beobachtung stattfand, liefert das System im Offline-Modus ein hervorragendes Ergebnis, indem mindestens drei akustische Standardszenen klassifiziert werden, nach denen die Entwickler dies √ºberpr√ºft haben. Dar√ºber hinaus ergab ein Test des neuronalen Netzwerks, dass sie unabh√§ngig voneinander lernte, die f√ºr einige Szenen charakteristischen Ger√§usche zu erkennen, und die Entwickler stellten ihre Beispiele nicht zur spezifischen Erkennung dieser Objekte zur Verf√ºgung. Anhand des nicht markierten Videomaterials erfuhr das neuronale Netzwerk selbst, welche Szene dem Klang einer jubelnden Menge (dies ist ein Stadion) und eines Vogel-Twitter (dies ist ein Rasen oder ein Park) entspricht. Gleichzeitig mit der Szene erkennt das neuronale Netzwerk ein bestimmtes Objekt, das die Schallquelle ist.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Das Video zeigt einige Beispiele zum Erkennen von Objekten anhand von Ton. Zuerst werden die T√∂ne und das Erkennungsergebnis angezeigt und das Bild selbst wird unscharf - Sie k√∂nnen also versuchen, sich selbst zu √ºberpr√ºfen. Werden Sie in der Lage sein, den Ort der Aktion und das Vorhandensein bestimmter Objekte nur durch Schall so genau zu verstehen wie das neuronale Netzwerk? Was bedeutet zum Beispiel h√∂chstwahrscheinlich das Lied "Happy Birthday To You!", Das von mehreren Personen gleichzeitig gesungen wird? Die richtige Antwort: Das Objekt </font></font><font color="white"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">brennt Kerzen</font></font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , die Szene ist ein </font></font><font color="white"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Restaurant, ein Caf√©, eine Bar</font></font></font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br>
<br>
<iframe width="560" height="315" src="https://www.youtube.com/embed/yJCjVvIY4dU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
"Die Bildverarbeitung hat begonnen, so gut zu funktionieren, dass wir diese Technologie auf andere Bereiche √ºbertragen k√∂nnen", </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sagte</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Carl Vondrick, Student am Massachusetts Institute of Technology f√ºr Elektrotechnik und Informatik, einer der Autoren der wissenschaftlichen Arbeit. - Wir haben die nat√ºrliche Beziehung zwischen Computer Vision und Sound genutzt. Aufgrund der Vielzahl unbeschrifteter Videomaterialien war es m√∂glich, einen gro√üen Ma√üstab zu erreichen, sodass das neuronale Netzwerk gelernt hat, Ton zu verstehen. ‚Äú</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
SoundNet-Tests wurden an zwei Standarddatenbanken f√ºr Tonaufnahmen durchgef√ºhrt und zeigten eine um 13-15% h√∂here Genauigkeit der Objekterkennung als die besten dieser Programme. In einem Datensatz mit 10 verschiedenen Klangkategorien klassifiziert SoundNet Kl√§nge mit einer Genauigkeit von 92% und in einem Datensatz mit 50 Kategorien mit einer Genauigkeit von 74%. Zum Vergleich zeigen Personen mit denselben Datens√§tzen eine Erkennungsgenauigkeit von durchschnittlich 96% und 81%.</font></font><br>
<br>
<img src="https://habrastorage.org/files/b91/d3c/4fb/b91d3c4fb4f24d13b133a6706f528744.png"><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Sogar Menschen k√∂nnen manchmal nicht genau bestimmen, was sie h√∂ren. Versuchen Sie, ein solches Experiment selbst durchzuf√ºhren. Lassen Sie einen Kollegen ein beliebiges Video von YouTube starten - und Sie versuchen, nicht auf den Monitor zu schauen, um zu sagen, was passiert, woher die Ger√§usche kommen und was auf dem Bildschirm angezeigt wird. Weit davon entfernt, immer zu erraten. Die Aufgabe f√ºr k√ºnstliche Intelligenz ist also nicht einfach, aber SoundNet hat es ganz gut geschafft. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Zukunft k√∂nnten solche Computerprogramme praktische Anwendung finden. Beispielsweise erkennt Ihr Mobiltelefon automatisch, dass Sie einen √∂ffentlichen Ort betreten haben - ein Kino oder ein Theater - und schaltet die Ruftonlautst√§rke automatisch stumm. Wenn der Film gestartet wurde und sich das Publikum beruhigt hat, schaltet das Telefon den Ton automatisch aus und den Vibrationsalarm ein.</font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Die Orientierung nach Gel√§nde durch Ger√§usche hilft bei Steuerungsprogrammen f√ºr autonome Roboter und andere Maschinen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
In Sicherheitssystemen und Smart Homes kann das System auf bestimmte Ger√§usche automatisch auf bestimmte Ger√§usche reagieren. </font><font style="vertical-align: inherit;">Zum Beispiel das Ger√§usch eines zerbrochenen Fensters. </font><font style="vertical-align: inherit;">In den ‚ÄûSmart Cities‚Äú der Zukunft wird die Erkennung von Stra√üenl√§rm helfen, die Ursachen zu verstehen und mit Schallverschmutzung umzugehen. </font></font><br>
<br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
Wissenschaftliche Artikel </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ver√∂ffentlicht</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 27. Oktober 2016 in den offenen Zugang zu arXiv.org (arXiv: 1610,09001, </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=aue&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pdf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ).</font></font></div>
      
    </div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de399659/">https://habr.com/ru/post/de399659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de399649/index.html">Ein Durchbruch bei der Energiespeicherung oder ein anderer Fall, in dem ein Wissenschaftler einen Journalisten ‚Äûmissbraucht‚Äú hat?</a></li>
<li><a href="../de399651/index.html">Kalte Fusion: Experimente erzeugen Energie, die nicht sein sollte</a></li>
<li><a href="../de399653/index.html">Megahertz wird nicht gefangen, Kerne wachsen nicht. Was ist mit dem technischen Fortschritt im PC passiert?</a></li>
<li><a href="../de399655/index.html">Erschwingliche 3D-CNC-Fr√§smaschinen von 250.000 bis 1.000.000 Rubel</a></li>
<li><a href="../de399657/index.html">–ü—É—Ç—å —á–∞–π–Ω–∏–∫–∞ –≤ –∞—Å—Ç—Ä–æ—Ñ–æ—Ç–æ. –ß–∞—Å—Ç—å 3 ‚Äî –¢—É–º–∞–Ω–Ω–æ—Å—Ç—å –û—Ä–∏–æ–Ω–∞ (M42)</a></li>
<li><a href="../de399663/index.html">Fragen Sie Ethan Nr. 110: Wie sah der Himmel aus, als sich die Erde gerade bildete?</a></li>
<li><a href="../de399665/index.html">Wenn auch nur f√ºr Brot</a></li>
<li><a href="../de399667/index.html">Das neuronale Netz sagt 1 Sekunde der Zukunft in der Fotografie voraus</a></li>
<li><a href="../de399669/index.html">Ein Schritt zur Seite: Warum die MacBook Pro Touchbar die Entwicklung von Touch-Interfaces nicht unterst√ºtzt</a></li>
<li><a href="../de399671/index.html">Atomm√ºll Laserschneiden Roboter Schlange</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>