<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🎹 🔫 👋🏾 Les navigateurs coupent le son dans votre application WebRTC. Arrête quoi? 🎞️ 💿 🐝</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="La technologie WebRTC (appels vocaux et vidéo) est bonne car elle est intégrée directement sur le Web, ce qui, bien sûr, est parfait pour WebRTC. Cepe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Les navigateurs coupent le son dans votre application WebRTC. Arrête quoi?</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/417085/">  La technologie WebRTC (appels vocaux et vidéo) est bonne car elle est intégrée directement sur le Web, ce qui, bien sûr, est parfait pour WebRTC.  Cependant, le Web est parfois très compliqué lorsque WebRTC doit fonctionner à l'encontre des exigences générales d'utilisation des navigateurs.  Le dernier exemple est la lecture automatique (ci-après dénommée «lecture automatique») de l'audio / vidéo, lorsque de nombreux utilisateurs perdent soudainement le son.  L'ancien auteur de webrtcHacks - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Dag-Inge Aas</a> - a personnellement rencontré ce problème.  Voici ses réflexions: à quoi s'attendre des navigateurs en termes de lecture automatique, les derniers changements dans Chrome 66+, ainsi que quelques conseils sur la façon de vivre avec ces restrictions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lt/r4/ar/ltr4arvdfpa3fefwdabnlzrkd5a.jpeg"></div><br>  <i><font color="lightgray">Les navigateurs ne veulent pas entendre le mal, donc les politiciens en lecture automatique coupent le son dans tous les médias.</font></i>  <i><font color="lightgray">Cela peut être un problème pour les applications WebRTC.</font></i> <br><a name="habracut"></a><br>  Si vous lisez ceci, il est très probable que vous ayez rencontré un étrange bug dans les navigateurs Safari 11+ et Chrome 66+.  Nous parlons de sons d'interface qui sont soudainement devenus inaudibles (par exemple, un signal d'appel entrant), ou d'un visualiseur audio inactif, ou de la façon dont les interlocuteurs ne sont pas entendus. <br><br>  Pour le moment, le bogue a affecté presque tous les lecteurs WebRTC populaires.  C'est drôle, mais il semble que Meet de Google et Chromebox pour les réunions soient également affectés. <br><br>  La racine de tous les maux: les changements dans les politiques de lecture automatique.  Dans cet article, je parlerai des innovations, de leur relation avec WebRTC et de la manière de les gérer dans vos applications. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/q7/fj/jaq7fj2bekoaoywfm4avujbbxrm.png"></div><br>  <font color="lightgray">Erreur de l'année: erreur non interceptée: le contenu audio n'a pas été autorisé à démarrer.</font>  <font color="lightgray">Il doit être repris (ou créé) après un geste de l'utilisateur sur la page.</font>  <font color="lightgray"><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">https://goo.gl/7K7WLu</a></font> <br><br><h3>  Changements </h3><br>  Tout a commencé en 2007, lorsque l'iPhone et iOS sont apparus.  Si vous avez travaillé avec Safari pour iOS dans le passé, vous avez peut-être remarqué que Safari a besoin de la participation de l'utilisateur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pour lire les éléments &lt;audio&gt; et &lt;video&gt; avec du son</a> .  Plus tard, cette exigence a été légèrement assouplie lorsque iOS 10 a permis aux éléments vidéo de jouer automatiquement, mais sans son.  Cela est devenu un problème pour WebRTC, car  l'élément &lt;video&gt; doit «voir» et «entendre» le flux multimédia.  Dans le contexte de WebRTC, laisser la vidéo s'exécuter automatiquement sans son est inutile, car pendant un appel vidéo, vous devez entendre les interlocuteurs par défaut, et ne pas appuyer sur "play" pour ce faire.  Quoi qu'il en soit, peu de développeurs WebRTC étaient impliqués dans Safari pour iOS, car la plateforme ne prenait en charge WebRTC <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">que récemment</a> .  Avant iOS 11. <br><br>  J'ai rencontré un bug pour la première fois lorsque j'ai testé la dernière implémentation (à l'époque) des appels vidéo pour iOS.  À ma grande surprise, il a cessé de fonctionner, alors que je n'étais pas seul.  L' <b>utilisateur de</b> Github, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">kylemcdonald, a</a> <b>signalé un</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bogue getUserMedia</a> sur iOS.  Solution?  Ajoutez une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nouvelle propriété</a> playsinline à l' <b>élément</b> vidéo, ce qui permet à la vidéo de jouer avec le son.  Malheureusement, les développeurs n'ont pas mentionné WebRTC dans le post d'origine sur la modification de la lecture automatique dans Safari, mais ils ont tout de même <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">écrit sur WebRTC</a> séparément, avant la sortie.  L'article dit ce qui suit sur <b>MediaStreams</b> et la lecture audio: <br><br><ul><li>  Les médias utilisant <b>MediaStream</b> seront <b>lus</b> automatiquement si la page Web utilise déjà une caméra / un microphone; </li><li>  Les médias utilisant <b>MediaStream</b> seront automatiquement lus si la page Web lit déjà du son.  Pour que la lecture du son commence, l'implication de l'utilisateur est toujours nécessaire. </li></ul><br>  Donc, ce document ne mentionne pas <b>playsinline</b> , mais si vous combinez les deux annonces, vous pouvez comprendre comment faire fonctionner l'application WebRTC dans Safari sur iOS. <br><br><h3>  Pourquoi la lecture automatique est-elle limitée? </h3><br>  Au départ, ils voulaient protéger les utilisateurs des coûts de trafic inutiles.  En 2007, le transfert de données était coûteux (et le reste dans une grande partie du monde) et seuls quelques sites ont été adaptés au mobile.  De plus, le son de lecture automatique a été et reste la chose la plus ennuyeuse sur le Web.  Par conséquent, pour lire (et télécharger) la vidéo, l'action de l'utilisateur était requise;  cela garantissait que l'utilisateur était au courant de ce qui se passait. <br><br>  Puis vint le GIF.  Les gifs ne sont que des animations à l'intérieur de &lt;img&gt;, ils ne nécessitaient donc pas la "permission" de l'utilisateur.  Cependant, ils <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">peuvent être difficiles</a> et donc douloureux pour les utilisateurs d'Internet mobile.  La vidéo épargne le trafic, mais nécessite le consentement du téléchargement - les pages Web ont donc continué à utiliser les GIF.  Tout a changé dans iOS 10 lorsque Safari a activé la lecture automatique avec le son désactivé.  Depuis lors, l'optimisation de la charge est une question de vidéo autorisée et de départ de gifs de trois minutes dans l'oubli. <br><br><h3>  Limitation de la lecture automatique dans les navigateurs de bureau </h3><br>  Si vous cherchez comment arrêter le son de la lecture automatique, vous trouverez de nombreuses façons.  Récemment, les agences de presse ont constaté que lorsqu'elles utilisent un son TRÈS FORT après le chargement de la page, les utilisateurs passent plus de temps sur le site et cliquent sur la publicité.  Bien sûr, cela vaut la peine de le faire, mais ils le font toujours.  Par conséquent, les navigateurs de bureau ont suivi l'exemple de Safari et ont interdit le son de lecture automatique - en particulier Chrome, qui a déployé de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">nouvelles politiques de lecture automatique</a> dans la version 66. <br><br>  Cependant, Chrome s'est soudainement tourné vers le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Media Engagement Index</a> (MEI) d'origine. <br><br><h3>  L'indice d'engagement des médias (MEI) </h3><br>  MEI est la façon dont Chrome mesure la volonté d'un utilisateur d'autoriser la lecture automatique sur une page;  cet index dépend du comportement précédent sur la page.  Vous pouvez voir à quoi cela ressemble ici: <b>chrome: // media-engagement /</b> .  MEI est considéré séparément pour chaque profil utilisateur et fonctionne en mode navigation privée (pour cette raison, il est très difficile pour les développeurs de tester des pages avec zéro MEI avant de les lancer en production).  Quelqu'un devine-t-il ce qui va se passer ensuite? <br><br> <a href=""><img src="https://habrastorage.org/webt/in/sq/yb/insqybsyqdl9cv2whta_qa_e_c0.png"></a> <br>  <font color="lightgray">Capture d'écran de la page interne de Chrome: // media-engagement / (source: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">developers.google.com/web/updates/2017/09/autoplay-policy-changes</a> )</font> <br><br><h3>  Non seulement &lt;audio&gt; et &lt;video&gt; </h3><br>  Il s'est avéré que les nouvelles politiques affectaient non seulement les balises &lt;audio&gt; et &lt;video&gt;.  Un modèle UX populaire pour WebRTC consiste à montrer à l'utilisateur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le niveau de volume du microphone</a> .  Pour ce faire, le son est <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">analysé via un AudioContext</a> , qui prend un <b>MediaStream</b> et émet son signal sous la forme d'un histogramme.  Dans ce cas, le son n'est pas <b>lu</b> , mais l'analyse audio est toujours bloquée en raison de l' <b>AudioContext</b> , qui, en théorie, vous permet de produire du son. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_1/3c/ia/_13cialfrfe4n1sh_i4-deetnnc.png"></div><br>  <font color="lightgray">Exemple de test de microphone</font> <br><br>  Le problème a été signalé pour la première fois dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">traqueur de bogues Webkit en décembre</a> , et six jours plus tard, un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">correctif est arrivé dans Webkit</a> .  Correction?  Ne bloquez pas <b>AudioContext</b> si la page reçoit déjà du son et de la vidéo. <br><br>  Alors pourquoi lisez-vous toujours cet article?  Il s'est avéré que <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chrome a fait la même erreur que Safari</a> .  Bien que cela ait affecté de nombreux services WebRTC, Google est silencieux à ce sujet.  Il y a eu de nombreuses tentatives pour les amener <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">à faire une déclaration publique</a> sur l'impact de la lecture automatique sur WebRTC, mais cela ne s'est pas encore produit. <br><br><h3>  Les valeurs MEI interfèrent avec les tests </h3><br>  Comment en sommes-nous arrivés là?  Bien sûr, les développeurs ont dû tester <b>AudioContext</b> avant même les modifications de Chrome 66 qui affectaient chaque utilisateur.  Et voici l'IEDM;  vous comprenez, une interaction fréquente avec la page vous donne un MEI plus élevé, respectivement, les développeurs sont moins susceptibles de rencontrer un bug, car l'audio est depuis longtemps autorisé à jouer et à analyser.  Et oui, le mode navigation privée n'aide pas, car MEI continue d'y être compté.  Seul le lancement de Chrome avec un nouveau compte détectera un bogue - un fait que même <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les ingénieurs expérimentés de Google QA</a> peuvent facilement oublier. <br><br><h3>  Que doivent faire les fabricants de navigateurs? </h3><br>  Les changements dans les fonctionnalités de base sont difficiles à mettre en œuvre correctement.  Chrome a publié de nombreux changements de stratégie de lecture automatique, mais aucun d'entre eux n'était lié à WebRTC ou <b>MediaStreams</b> .  Apparemment, les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">API d'autorisations</a> oubliées <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">n'ont</a> pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">été</a> mises à jour, les développeurs ne peuvent donc pas tester la demande d'actions personnalisées.  Alternativement, vous pouvez autoriser <b>AudioContext</b> à émettre du son si la page fonctionne déjà avec une caméra / un microphone (comme Safari), mais cela ressemble plus à un hack qu'à une solution.  Et cela n'aidera pas dans d'autres cas d'analyse sonore lorsque <b>getUserMedia</b> n'est pas utilisé. <br><br>  Une solution concrète renforcée pour les fabricants de navigateurs consiste à autoriser <b>les autorisations des médias à</b> influencer MEI.  Si l'utilisateur a donné un accès constant à sa caméra et son microphone, il est logique de supposer que la page Web peut être suffisamment fiable pour produire du son sans actions supplémentaires et indépendamment du fait qu'elle fonctionne avec la caméra / le microphone ou non.  Au final, l'utilisateur estime que vous ne partagerez pas sa caméra et son microphone avec des millions de personnes à leur insu.  Dans ce cas, la lecture des sons d'interface est le moindre des problèmes. <br><br><h3>  Comment aider votre application </h3><br>  Heureusement, il existe quelques astuces pour vous aider.  Voici une liste de ce que nous avons ajouté à Confrere lorsque nous avons rencontré un problème dans Safari pour iOS. <br><br><h4>  Jeux en ligne ajoutés </h4><br>  Pour renvoyer le son à la vidéo, ajoutez l'attribut <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">playsinline</a> à l'élément vidéo.  Cet attribut est bien documenté, fonctionne dans Safari et Chrome et n'a aucun effet secondaire dans les autres navigateurs. <br><br><h4>  Actions utilisateur </h4><br>  Pour «guérir» un visualiseur audio, ajoutez simplement une action personnalisée.  Nous avons eu de la chance car nous avons pu ajouter (et ajouté) à notre écran de test de nombreuses étapes, impliquant la participation explicite de l'utilisateur.  Vous avez peut-être moins de chance.  Jusqu'à ce que Google se charge de le réparer, il n'y a pas d'autre moyen que d'impliquer l'utilisateur. <br><br><h4>  Impossible de corriger les sons d'interface </h4><br>  Ce n'est actuellement pas possible.  Certaines personnes essaient de créer un <b>AudioContext</b> qui est réutilisé dans l'application et tous les sons le traversent, mais je n'ai pas testé cette méthode.  Safari est un peu plus simple: si vous travaillez déjà avec un appareil photo / microphone, vous pouvez jouer les sons des messages et appels entrants.  Mais je ne pense pas que vous souhaitiez utiliser constamment la caméra / le microphone juste pour alerter profondément l'utilisateur d'un appel entrant. <br><br>  Comme vous pouvez le voir, il existe déjà des moyens de résoudre le problème jusqu'à ce qu'une solution à long terme apparaisse.  Oui, et n'oubliez pas <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">de vous abonner au bug</a> pour recevoir les mises à jour. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417085/">https://habr.com/ru/post/fr417085/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr417073/index.html">Uber Mobile Developer Day</a></li>
<li><a href="../fr417075/index.html">API de peinture CSS</a></li>
<li><a href="../fr417079/index.html">Gestionnaire de packages pour Kubernetes - Helm: Past, Present, Future</a></li>
<li><a href="../fr417081/index.html">Nord, volonté, espoir, pays sans frontières, ou comment les projets sont réalisés dans des conditions sibériennes sévères</a></li>
<li><a href="../fr417083/index.html">Fortes charges de la Coupe du monde 2018</a></li>
<li><a href="../fr417087/index.html">HPE Digitize 2018: événement et diffusion en direct</a></li>
<li><a href="../fr417089/index.html">Ordinateur quantique: un photon pour tout gouverner</a></li>
<li><a href="../fr417091/index.html">Créez un shader d'eau de dessin animé pour le Web. 3e partie</a></li>
<li><a href="../fr417093/index.html">Interrupteurs tactiles avec Modbus: pourquoi sont-ils nécessaires et comment les appliquer dans un appartement intelligent</a></li>
<li><a href="../fr417097/index.html">Métaprogrammation JavaScript</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>