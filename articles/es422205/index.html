<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë® ü§∏üèæ üíó Richard Hamming: Cap√≠tulo 13. Teor√≠a de la informaci√≥n üòú üë®üèº üë©üèø‚Äçüé§</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lo hicimos! 

 "El objetivo de este curso es prepararte para tu futuro t√©cnico". 
 Hola Habr ¬øRecuerdas el incre√≠ble art√≠culo "T√∫ y tu trabajo" (+219,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Richard Hamming: Cap√≠tulo 13. Teor√≠a de la informaci√≥n</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422205/">  Lo hicimos! <br><br><blockquote>  "El objetivo de este curso es prepararte para tu futuro t√©cnico". </blockquote><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/6ff/9ea/d676ff9eadd2a38b0948de76bbf27fd4.jpg" alt="imagen" align="right">  Hola Habr  ¬øRecuerdas el incre√≠ble art√≠culo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"T√∫ y tu trabajo"</a> (+219, 2588 marcado, 429k lecturas)? <br><br>  Entonces, Hamming (s√≠, s√≠, los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√≥digos de Hamming que se</a> autoverifican y corrigen a s√≠ mismos) tiene un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">libro</a> completo escrito basado en sus conferencias.  Lo estamos traduciendo, porque el hombre est√° hablando de negocios. <br><br>  Este libro no es solo sobre TI, es un libro sobre el estilo de pensamiento de personas incre√≠blemente geniales.  <i>‚ÄúEsto no es solo una carga de pensamiento positivo;</i>  <i>describe condiciones que aumentan las posibilidades de hacer un gran trabajo ".</i> <br><br>  <i>Gracias por la traducci√≥n a Andrei Pakhomov.</i> <br><br>  La teor√≠a de la informaci√≥n fue desarrollada por C.E. Shannon a fines de la d√©cada de 1940.  La gerencia de Bell Labs insisti√≥ en que lo llamara "teor√≠a de la comunicaci√≥n", porque  Este es un nombre mucho m√°s preciso.  Por razones obvias, el nombre "Teor√≠a de la informaci√≥n" tiene un impacto significativamente mayor en el p√∫blico, por lo que Shannon lo eligi√≥, y es lo que sabemos hasta el d√≠a de hoy.  El nombre en s√≠ sugiere que la teor√≠a se ocupa de la informaci√≥n, lo que la hace importante, ya que estamos penetrando m√°s profundamente en la era de la informaci√≥n.  En este cap√≠tulo, abordar√© algunas conclusiones b√°sicas de esta teor√≠a, dar√© evidencia no estricta, sino intuitiva de algunas de las disposiciones separadas de esta teor√≠a, para que comprenda cu√°l es la "Teor√≠a de la informaci√≥n", d√≥nde puede aplicarla y d√≥nde no . <br><a name="habracut"></a><br>  En primer lugar, ¬øqu√© es "informaci√≥n"?  Shannon identifica la informaci√≥n con incertidumbre.  Eligi√≥ el logaritmo negativo de la probabilidad de un evento como una medida cuantitativa de la informaci√≥n que recibe cuando un evento ocurre con probabilidad p.  Por ejemplo, si te digo que el clima en Los √Ångeles es brumoso, entonces p est√° cerca de 1, que en general no nos da mucha informaci√≥n.  Pero si digo que llueve en Monterrey en junio, entonces habr√° incertidumbre en este mensaje y contendr√° m√°s informaci√≥n.  Un evento confiable no contiene ninguna informaci√≥n, ya que log 1 = 0. <br><br>  Deteng√°monos en esto con m√°s detalle.  Shannon cre√≠a que una medida cuantitativa de informaci√≥n deber√≠a ser una funci√≥n continua de la probabilidad de un evento p, y para eventos independientes deber√≠a ser aditiva: la cantidad de informaci√≥n obtenida como resultado de dos eventos independientes deber√≠a ser igual a la cantidad de informaci√≥n obtenida como resultado de un evento conjunto.  Por ejemplo, el resultado de una tirada de dados y monedas generalmente se considera como eventos independientes.  Traduzcamos lo anterior al lenguaje de las matem√°ticas.  Si I (p) es la cantidad de informaci√≥n contenida en el evento con probabilidad p, entonces para un evento conjunto que consta de dos eventos independientes x con probabilidad p <sub>1</sub> e y con probabilidad p <sub>2</sub> obtenemos <br><br><img src="https://habrastorage.org/webt/yi/hc/2d/yihc2dbw1d5_rlv2rptbyqs1mww.jpeg" alt="imagen"><br>  <i>(eventos independientes x e y)</i> <br><br>  Esta es la ecuaci√≥n funcional de Cauchy, verdadera para todos los p <sub>1</sub> y p2.  Para resolver esta ecuaci√≥n funcional, suponga que <br><br>  p <sub>1</sub> = p <sub>2</sub> = p, <br><br>  da <br><br><img src="https://habrastorage.org/webt/dk/rj/gf/dkrjgfn-xucnz-twcyhq3l6xefw.jpeg" alt="imagen"><br><br>  Si p <sub>1</sub> = p <sup>2</sup> y p <sub>2</sub> = p, entonces <br><br><img src="https://habrastorage.org/webt/ux/-s/cj/ux-scjh2wymlpjj-hpnsnzrgsly.jpeg" alt="imagen"><br><br>  etc.  Extendiendo este proceso usando el m√©todo est√°ndar para exponenciales para todos los n√∫meros racionales m / n, lo siguiente es cierto <br><br><img src="https://habrastorage.org/webt/gt/_t/ro/gt_trop25xscgj5m3c1-k0nxiwe.jpeg" alt="imagen"><br><br>  De la supuesta continuidad de la medida de informaci√≥n, se deduce que la funci√≥n logar√≠tmica es la √∫nica soluci√≥n continua a la ecuaci√≥n funcional de Cauchy. <br><br>  En la teor√≠a de la informaci√≥n, es habitual tomar la base del logaritmo de 2, por lo que la elecci√≥n binaria contiene exactamente 1 bit de informaci√≥n.  Por lo tanto, la informaci√≥n se mide por la f√≥rmula <br><br><img src="https://habrastorage.org/webt/wx/ix/ow/wxixowgi6tookkpcslvwgmu6bpg.jpeg" alt="imagen"><br><br>  Hagamos una pausa y veamos qu√© sucedi√≥ arriba.  En primer lugar, no le dimos una definici√≥n al concepto de "informaci√≥n", simplemente definimos una f√≥rmula para su medida cuantitativa. <br><br>  En segundo lugar, esta medida depende de la incertidumbre y, aunque es lo suficientemente adecuada para m√°quinas, por ejemplo, sistemas telef√≥nicos, radio, televisi√≥n, computadoras, etc., no refleja una actitud humana normal hacia la informaci√≥n. <br><br>  En tercer lugar, esta es una medida relativa, depende del estado actual de su conocimiento.  Si observa el flujo de "n√∫meros aleatorios" del generador de n√∫meros aleatorios, asume que cada n√∫mero siguiente es indefinido, pero si conoce la f√≥rmula para calcular "n√∫meros aleatorios", se conocer√° el siguiente n√∫mero y, en consecuencia, no contendr√° informaci√≥n <br><br>  Por lo tanto, la definici√≥n dada por Shannon para informaci√≥n es en muchos casos adecuada para m√°quinas, pero no parece corresponder a la comprensi√≥n humana de la palabra.  Por esta raz√≥n, la "Teor√≠a de la informaci√≥n" debe llamarse la "Teor√≠a de la comunicaci√≥n".  Sin embargo, es demasiado tarde para cambiar las definiciones (gracias a las cuales la teor√≠a ha ganado su popularidad inicial, y que todav√≠a hacen que la gente piense que esta teor√≠a trata con la "informaci√≥n"), entonces tenemos que soportarlas, pero hay que Comprenda claramente hasta qu√© punto la definici√≥n de informaci√≥n dada por Shannon est√° lejos de su sentido com√∫n.  La informaci√≥n de Shannon trata con algo completamente diferente, a saber, la incertidumbre. <br><br>  Esto es lo que necesita pensar cuando ofrece alguna terminolog√≠a.  ¬øQu√© tan consistente es la definici√≥n propuesta, por ejemplo, la definici√≥n dada por Shannon, con su idea original, y qu√© tan diferente es?  Casi no existe un t√©rmino que refleje con precisi√≥n su visi√≥n anterior del concepto, pero al final, es la terminolog√≠a utilizada la que refleja el significado del concepto, por lo que formalizar algo a trav√©s de definiciones claras siempre hace ruido. <br><br>  Considere un sistema cuyo alfabeto consiste en s√≠mbolos q con probabilidades pi.  En este caso, la <i>cantidad promedio de informaci√≥n</i> en el sistema (su valor esperado) es: <br><br><img src="https://habrastorage.org/webt/wj/ss/83/wjss83z5fnynnyurbcftgokak6e.jpeg" alt="imagen"><br><br>  Esto se llama entrop√≠a del sistema de distribuci√≥n de probabilidad {pi}.  Usamos el t√©rmino "entrop√≠a" porque la misma forma matem√°tica surge en termodin√°mica y mec√°nica estad√≠stica.  Es por eso que el t√©rmino "entrop√≠a" crea a su alrededor un aura de importancia que, en √∫ltima instancia, no est√° justificada.  ¬°La misma forma matem√°tica de notaci√≥n no implica la misma interpretaci√≥n de los caracteres! <br><br>  La entrop√≠a de la distribuci√≥n de probabilidad juega un papel importante en la teor√≠a de la codificaci√≥n.  La desigualdad de Gibbs para dos distribuciones de probabilidad diferentes pi y qi es una de las consecuencias importantes de esta teor√≠a.  Entonces tenemos que demostrar que <br><br><img src="https://habrastorage.org/webt/jx/1v/c-/jx1vc-ac5t4kzyxhpplqfb20bmu.jpeg" alt="imagen"><br><br>  La prueba se basa en un gr√°fico obvio, la fig.  13.I, que muestra que <br><br><img src="https://habrastorage.org/webt/gn/_o/i1/gn_oi1vqrmafw6k5v0g043jpz30.jpeg" alt="imagen"><br><br>  y la igualdad se logra solo para x = 1. Aplicamos la desigualdad a cada sumando de la suma del lado izquierdo: <br><br><img src="https://habrastorage.org/webt/rg/wl/5o/rgwl5owcdte29pj1ycvixfgdogo.jpeg" alt="imagen"><br><br>  Si el alfabeto del sistema de comunicaci√≥n consta de q caracteres, entonces tomando la probabilidad de transmisi√≥n de cada car√°cter qi = 1 / q y sustituyendo q, obtenemos de la desigualdad de Gibbs <br><br><img src="https://habrastorage.org/webt/qo/dz/gv/qodzgviyhmwnovomxokegxao54e.jpeg" alt="imagen"><br><br><img src="https://habrastorage.org/webt/8n/2m/y9/8n2my9t_5vd7vktnvdezxi0p1q4.jpeg" alt="imagen"><br><br>  <i>Figura 13.I</i> <br><br>  Esto sugiere que si la probabilidad de transmitir todos los caracteres q es igual e igual a - 1 / q, entonces la entrop√≠a m√°xima es ln q, de lo contrario, la desigualdad se mantiene. <br><br>  En el caso de un c√≥digo decodificado de forma √∫nica, tenemos la desigualdad de Kraft <br><br><img src="https://habrastorage.org/webt/0-/8k/5l/0-8k5lsd16wlmzgwmmzvxgvlbr4.jpeg" alt="imagen"><br><br>  Ahora si definimos pseudo-probabilidades <br><br><img src="https://habrastorage.org/webt/s9/qc/fv/s9qcfv1ywfj7_zkfvwm4jna8cqw.jpeg" alt="imagen"><br><br>  donde por supuesto <img src="https://habrastorage.org/webt/is/_u/vw/is_uvwjifihdmybm68w_ayyfk-q.jpeg" alt="imagen">  = 1, que se deduce de la desigualdad de Gibbs, <br><br><img src="https://habrastorage.org/webt/my/hy/s0/myhys0cvxd6kgzu7f8r10vtnlym.jpeg" alt="imagen"><br><br>  y aplicar un poco de √°lgebra (recuerde que K ‚â§ 1, por lo que podemos omitir el t√©rmino logar√≠tmico y posiblemente fortalecer la desigualdad m√°s adelante), obtenemos <br><br><img src="https://habrastorage.org/webt/v0/i4/yo/v0i4yoxhwj8grndqupxwbo4zppw.jpeg" alt="imagen"><br><br>  donde L es la longitud promedio del c√≥digo. <br><br>  Por lo tanto, la entrop√≠a es el l√≠mite m√≠nimo para cualquier c√≥digo de caracteres con una palabra de c√≥digo promedio L. Este es el teorema de Shannon para un canal sin interferencia. <br><br>  Ahora consideramos el teorema principal sobre las limitaciones de los sistemas de comunicaci√≥n en los que la informaci√≥n se transmite en forma de un flujo de bits independientes y est√° presente el ruido.  Se supone que la probabilidad de la transmisi√≥n correcta de un bit es P&gt; 1/2, y la probabilidad de que el valor del bit se invierta durante la transmisi√≥n (se produce un error) es Q = 1 - P. Por conveniencia, suponemos que los errores son independientes y la probabilidad de error es la misma para cada env√≠o bits, es decir, hay "ruido blanco" en el canal de comunicaci√≥n. <br><br>  La forma en que tenemos una larga secuencia de n bits codificados en un solo mensaje es una extensi√≥n n-dimensional de un c√≥digo de un bit.  Determinaremos el valor de n m√°s tarde.  Considere un mensaje que consiste en n-bits como un punto en el espacio n-dimensional.  Dado que tenemos un espacio n-dimensional, y por simplicidad asumimos que cada mensaje tiene la misma probabilidad de ocurrencia, hay M mensajes posibles (M tambi√©n se determinar√° m√°s adelante), por lo tanto, la probabilidad de cualquier mensaje enviado es igual <br><br><img src="https://habrastorage.org/webt/vs/gj/e5/vsgje5adnr3222jgs5qqlii1hga.jpeg" alt="imagen"><br><br><img src="https://habrastorage.org/webt/s8/mu/br/s8mubr4blaju8evwtbsgqxjvqye.jpeg" alt="imagen"><br>  (remitente) <br>  <i>Gr√°fico 13.II</i> <br><br>  A continuaci√≥n, considere la idea del ancho de banda del canal.  Sin entrar en detalles, la capacidad del canal se define como la cantidad m√°xima de informaci√≥n que se puede transmitir de manera confiable a trav√©s del canal de comunicaci√≥n, teniendo en cuenta el uso de la codificaci√≥n m√°s eficiente.  No hay argumento de que se pueda transmitir m√°s informaci√≥n a trav√©s del canal de comunicaci√≥n que su capacidad.  Esto se puede probar para un canal sim√©trico binario (que usamos en nuestro caso).  La capacidad del canal para el env√≠o a nivel de bits se establece como <br><br><img src="https://habrastorage.org/webt/pi/ek/76/piek76b-zyqfhs7k2qxzhleky9y.jpeg" alt="imagen"><br><br>  donde, como antes, P es la probabilidad de que no haya ning√∫n error en ning√∫n bit enviado.  Al enviar n bits independientes, la capacidad del canal se determina como <br><br><img src="https://habrastorage.org/webt/nv/7x/67/nv7x67ybwwcisk8pxntmligj3ky.jpeg" alt="imagen"><br><br>  Si estamos cerca del ancho de banda del canal, deber√≠amos enviar casi tal volumen de informaci√≥n para cada uno de los caracteres ai, i = 1, ..., M. Dado que la probabilidad de aparici√≥n de cada car√°cter ai es 1 / M, obtenemos <br><br><img src="https://habrastorage.org/webt/ne/mf/om/nemfom_mjugxea6infrm7asm4la.jpeg" alt="imagen"><br><br>  cuando enviamos cualquiera de los mensajes M equiprobables ai, tenemos <br><br><img src="https://habrastorage.org/webt/mt/c-/si/mtc-siba4teyqt0flsyzqs3-eja.jpeg" alt="imagen"><br><br>  Al enviar n bits, esperamos que ocurran errores nQ.  En la pr√°ctica, para un mensaje que consiste en n bits, tendremos aproximadamente nQ errores en el mensaje recibido.  Para n grande, variaci√≥n relativa (variaci√≥n = ancho de distribuci√≥n,) <br>  la distribuci√≥n del n√∫mero de errores ser√° m√°s estrecha al aumentar n. <br><br>  Entonces, desde el lado del transmisor, tomo el mensaje ai para enviar y dibujo una esfera a su alrededor con un radio <br><br><img src="https://habrastorage.org/webt/de/j8/h9/dej8h9wxzm0ktckfoksyksxdcm0.jpeg" alt="imagen"><br><br>  que es ligeramente mayor en una cantidad igual a e2 que el n√∫mero esperado de errores Q, (Figura 13.II).  Si n es lo suficientemente grande, entonces hay una probabilidad arbitrariamente peque√±a de la aparici√≥n del punto de mensaje bj en el lado del receptor, que va m√°s all√° de esta esfera.  Dibujaremos la situaci√≥n, tal como la veo desde el punto de vista del transmisor: tenemos cualquier radio desde el mensaje transmitido ai hasta el mensaje recibido bj con una probabilidad de error igual (o casi igual) a la distribuci√≥n normal, alcanzando un m√°ximo en nQ.  Para cualquier e2 dado, hay n tan grande que la probabilidad de que el punto resultante bj, que va m√°s all√° de mi esfera, sea tan peque√±a como desee. <br><br>  Ahora considere la misma situaci√≥n de su parte (Fig. 13.III).  En el lado del receptor hay una esfera S (r) del mismo radio r alrededor del punto recibido bj en el espacio n-dimensional, de modo que si el mensaje recibido bj est√° dentro de mi esfera, entonces el mensaje ai que envi√© est√° dentro de su esfera. <br><br>  ¬øC√≥mo puede ocurrir un error?  Se puede producir un error en los casos descritos en la tabla a continuaci√≥n: <br><br><img src="https://habrastorage.org/webt/ap/jj/ce/apjjcepa80evnhjxgc_swmf62z4.jpeg" alt="imagen"><br><br>  <i>Figura 13.III</i> <br><br><img src="https://habrastorage.org/webt/qt/m3/v8/qtm3v8viaw9cw291jxn11t9ya0i.jpeg" alt="imagen"><br><br>  Aqu√≠ vemos que si en la esfera construida alrededor del punto recibido hay al menos un punto m√°s correspondiente a un posible mensaje no codificado enviado, entonces se produjo un error durante la transmisi√≥n, ya que no puede determinar cu√°l de estos mensajes se transmiti√≥.  El mensaje enviado no contiene errores solo si el punto correspondiente est√° en la esfera, y no hay otros puntos posibles en este c√≥digo que est√©n en la misma esfera. <br><br>  Tenemos una ecuaci√≥n matem√°tica para la probabilidad de un error Re si se envi√≥ ai <br><br><img src="https://habrastorage.org/webt/uu/fr/fo/uufrfozpmxs0asmf4sjlinmk9rk.jpeg" alt="imagen"><br><br>  Podemos descartar el primer factor en el segundo t√©rmino, tom√°ndolo como 1. Por lo tanto, obtenemos la desigualdad <br><br><img src="https://habrastorage.org/webt/oh/qt/_9/ohqt_97ig6afppntwqmz7ifd-fs.jpeg" alt="imagen"><br><br>  Obviamente <br><br><img src="https://habrastorage.org/webt/ac/hx/ty/achxtyvmg-aq2yirdswhnsm4olk.jpeg" alt="imagen"><br><br>  por lo tanto <br><br><img src="https://habrastorage.org/webt/ym/27/5p/ym275poqvhr2e8jyuajtui9a4ky.jpeg" alt="imagen"><br><br>  volver a aplicar al √∫ltimo miembro a la derecha <br><br><img src="https://habrastorage.org/webt/hd/4v/9g/hd4v9gnlnbiyjcjv-grtzvsslsg.jpeg" alt="imagen"><br><br>  Si n se toma lo suficientemente grande, el primer t√©rmino se puede tomar arbitrariamente peque√±o, digamos, menos de cierto n√∫mero d.  Por lo tanto tenemos <br><br><img src="https://habrastorage.org/webt/lt/ey/hf/lteyhffjmrrzy8pjblxrsigpkzu.jpeg" alt="imagen"><br><br>  Ahora veamos c√≥mo puede crear un c√≥digo de reemplazo simple para codificar mensajes M que consisten en n bits.  Al no tener idea de c√≥mo construir el c√≥digo (los c√≥digos de correcci√≥n de errores a√∫n no se han inventado), Shannon eligi√≥ la codificaci√≥n aleatoria.  Lanza una moneda por cada uno de los n bits en el mensaje y repite el proceso para M mensajes.  Todo lo que necesita hacer es lanzar monedas nM, por lo que es posible <br><br><img src="https://habrastorage.org/webt/ii/ke/ss/iikessacb5q-xgdhr5btks6otss.jpeg" alt="imagen"><br><br>  diccionarios de c√≥digo que tienen la misma probabilidad de ¬ΩnM.  Por supuesto, el proceso aleatorio de crear un libro de c√≥digos significa que hay una probabilidad de duplicados, as√≠ como puntos de c√≥digo, que estar√°n cerca uno del otro y, por lo tanto, ser√°n una fuente de posibles errores.  Es necesario demostrar que si esto no sucede con una probabilidad mayor que cualquier nivel de error peque√±o seleccionado, entonces el n dado es lo suficientemente grande. <br>  ¬°El punto decisivo es que Shannon promedi√≥ todos los libros de c√≥digos posibles para encontrar el error promedio!  Usaremos el s√≠mbolo Av [.] Para denotar el promedio sobre el conjunto de todos los posibles diccionarios de c√≥digo aleatorio.  Promediar sobre la constante d, por supuesto, da una constante, ya que para promediar cada t√©rmino coincide con cualquier otro t√©rmino en la suma <br><br><img src="https://habrastorage.org/webt/-j/gh/_z/-jgh_zkjbnuzjdj39bz03euss_g.jpeg" alt="imagen"><br><br>  que se puede aumentar (M - 1 va a M) <br><br><img src="https://habrastorage.org/webt/ma/3u/wz/ma3uwzaee8iaohanrz5qsgvspnq.jpeg" alt="imagen"><br><br>  Para cualquier mensaje en particular, al promediar todos los libros de c√≥digos, la codificaci√≥n recorre todos los valores posibles, por lo que la probabilidad promedio de que un punto est√© en una esfera es la relaci√≥n del volumen de la esfera con el volumen total del espacio.  El alcance de la esfera. <br><br><img src="https://habrastorage.org/webt/ry/pu/qb/rypuqbtnwvzsd4ei3_-6sxsot3c.jpeg" alt="imagen"><br><br>  donde s = Q + e2 &lt;1/2 y ns debe ser un n√∫mero entero. <br><br>  El √∫ltimo t√©rmino a la derecha es el m√°s grande en esta suma.  Primero, estimamos su valor mediante la f√≥rmula de Stirling para factoriales.  Luego observamos el coeficiente de reducci√≥n del t√©rmino frente a √©l, tenga en cuenta que este coeficiente aumenta cuando se mueve hacia la izquierda y, por lo tanto, podemos: (1) limitar el valor de la suma a la suma de la progresi√≥n geom√©trica con este coeficiente inicial, (2) expandir la progresi√≥n geom√©trica de ns miembros a n√∫mero infinito de t√©rminos, (3) calcule la suma de la progresi√≥n geom√©trica infinita (√°lgebra est√°ndar, nada significativo) y finalmente obtenga el valor l√≠mite (para un n suficientemente grande): <br><br><img src="https://habrastorage.org/webt/6f/jo/lr/6fjolr9tmeljs-eglmaxup0yrli.jpeg" alt="imagen"><br><br>  Observe c√≥mo apareci√≥ la entrop√≠a H (s) en la identidad binomial.  Tenga en cuenta que la expansi√≥n en la serie de Taylor H (s) = H (Q + e2) proporciona una estimaci√≥n obtenida teniendo en cuenta solo la primera derivada e ignorando todas las dem√°s.  Ahora recojamos la expresi√≥n final: <br><br><img src="https://habrastorage.org/webt/9h/-u/wd/9h-uwd1ukrxttavrdy91ifezgpw.jpeg" alt="imagen"><br><br>  donde <br><br><img src="https://habrastorage.org/webt/bm/h4/bb/bmh4bbgfnl2h9g0-omxft9b8lbe.jpeg" alt="imagen"><br><br>  Todo lo que tenemos que hacer es elegir e2 para que e3 &lt;e1, y luego el √∫ltimo t√©rmino ser√° arbitrariamente peque√±o, para n suficientemente grande.  Por lo tanto, el error de PE promedio se puede obtener arbitrariamente peque√±o con una capacidad de canal arbitrariamente cercana a C. <br>  Si el promedio de todos los c√≥digos tiene un error suficientemente peque√±o, entonces al menos un c√≥digo debe ser adecuado, por lo tanto, existe al menos un sistema de codificaci√≥n adecuado.  Este es un resultado importante obtenido por Shannon: "Teorema de Shannon para un canal con interferencia", aunque debe tenerse en cuenta que lo demostr√≥ para un caso mucho m√°s general que para un simple canal sim√©trico binario que utilic√©.  Para el caso general, los c√°lculos matem√°ticos son mucho m√°s complicados, pero las ideas no son tan diferentes, tan a menudo, usando el ejemplo de un caso especial, uno puede revelar el verdadero significado del teorema. <br><br>  Critiquemos el resultado.  Repetimos repetidamente: "Para n suficientemente grande".  ¬øPero qu√© tan grande es n?  Muy, muy grande, si realmente desea estar simult√°neamente cerca del ancho de banda del canal y estar seguro de la transferencia de datos correcta.  Tan grande que, de hecho, tendr√° que esperar mucho tiempo para acumular un mensaje de tantos bits para codificarlo m√°s tarde.  Al mismo tiempo, el tama√±o del diccionario de c√≥digo aleatorio ser√° enorme (despu√©s de todo, dicho diccionario no puede representarse en una forma m√°s corta que la lista completa de todos los Mn bits, mientras que n y M son muy grandes). <br><br>  Los c√≥digos de correcci√≥n de errores evitan esperar un mensaje muy largo, con su posterior codificaci√≥n y decodificaci√≥n a trav√©s de libros de c√≥digos muy grandes, porque evitan los libros de c√≥digos per se y utilizan en su lugar c√°lculos convencionales.  En una teor√≠a simple, tales c√≥digos, como regla, pierden su capacidad de acercarse a la capacidad del canal y al mismo tiempo mantienen una tasa de error bastante baja, pero cuando el c√≥digo corrige una gran cantidad de errores, muestran buenos resultados.  En otras palabras, si est√° colocando alg√∫n tipo de capacidad de canal para la correcci√≥n de errores, entonces debe usar la opci√≥n de correcci√≥n de errores la mayor parte del tiempo, es decir, se debe corregir una gran cantidad de errores en cada mensaje enviado, de lo contrario perder√° esta capacidad por nada. <br><br>  ¬°Adem√°s, el teorema demostrado anteriormente todav√≠a no tiene sentido!  Muestra que los sistemas de transmisi√≥n eficientes deber√≠an utilizar esquemas de codificaci√≥n sofisticados para cadenas de bits muy largas.  Un ejemplo son los sat√©lites que han volado fuera del planeta exterior;  A medida que se alejan de la Tierra y del Sol, se ven obligados a corregir cada vez m√°s errores en el bloque de datos: algunos sat√©lites usan paneles solares, que dan alrededor de 5 vatios, otros usan fuentes de energ√≠a at√≥mica que dan aproximadamente la misma potencia.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La d√©bil potencia de la fuente de alimentaci√≥n, el peque√±o tama√±o de las placas transmisoras y el tama√±o limitado de las placas receptoras en la Tierra, la gran distancia que debe recorrer la se√±al; todo esto requiere el uso de c√≥digos con un alto nivel de correcci√≥n de errores para construir un sistema de comunicaci√≥n efectivo.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Regresamos al espacio n-dimensional que usamos en la prueba anterior. Al discutirlo, demostramos que casi todo el volumen de la esfera se concentra cerca de la superficie externa, por lo tanto, casi con certeza la se√±al enviada se ubicar√° en la superficie de la esfera construida alrededor de la se√±al recibida, incluso con un radio relativamente peque√±o de dicha esfera. Por lo tanto, no es sorprendente que la se√±al recibida despu√©s de la correcci√≥n de un n√∫mero arbitrariamente grande de errores, nQ, se encuentre arbitrariamente cerca de la se√±al sin errores. La capacidad del canal de comunicaci√≥n, que examinamos anteriormente, es la clave para comprender este fen√≥meno. Tenga en cuenta que las esferas construidas para los c√≥digos de correcci√≥n de errores de Hamming no se superponen. Un gran n√∫mero de mediciones pr√°cticamente ortogonales en el espacio n-dimensional muestran¬øPor qu√© podemos colocar esferas M en un espacio con una ligera superposici√≥n? Si permite una superposici√≥n peque√±a, arbitrariamente peque√±a, que puede conducir a un peque√±o n√∫mero de errores durante la decodificaci√≥n, puede obtener una disposici√≥n densa de esferas en el espacio. Hamming garantiz√≥ un cierto nivel de correcci√≥n de errores, Shannon, una baja probabilidad de error, pero al mismo tiempo mantiene el ancho de banda real arbitrariamente cerca de la capacidad del canal de comunicaci√≥n, lo que los c√≥digos de Hamming no pueden hacer.pero al mismo tiempo, la preservaci√≥n del rendimiento real es arbitrariamente cercana a la capacidad del canal de comunicaci√≥n, lo que los c√≥digos de Hamming no pueden hacer.pero al mismo tiempo, la preservaci√≥n del rendimiento real es arbitrariamente cercana a la capacidad del canal de comunicaci√≥n, lo que los c√≥digos de Hamming no pueden hacer.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La teor√≠a de la informaci√≥n no habla sobre c√≥mo dise√±ar un sistema efectivo, pero indica la direcci√≥n del movimiento hacia sistemas de comunicaci√≥n efectivos. Esta es una herramienta valiosa para construir sistemas de comunicaci√≥n entre m√°quinas, pero, como se se√±al√≥ anteriormente, no tiene mucho que ver con la forma en que las personas intercambian informaci√≥n entre s√≠. El grado en que la herencia biol√≥gica es similar a los sistemas de comunicaci√≥n t√©cnica es simplemente desconocido, por lo que actualmente no est√° claro c√≥mo se aplica la teor√≠a de la informaci√≥n a los genes. No tenemos m√°s remedio que intentarlo, y si el √©xito nos muestra la naturaleza maquinal de este fen√≥meno, el fracaso se√±alar√° otros aspectos significativos de la naturaleza de la informaci√≥n.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No nos distraigamos mucho. Hemos visto que todas las definiciones iniciales, en mayor o menor medida, deben expresar la esencia de nuestras creencias iniciales, pero se caracterizan por un cierto grado de distorsi√≥n y, por lo tanto, no son aplicables. Se acepta tradicionalmente que, en √∫ltima instancia, la definici√≥n que usamos realmente define la esencia; pero, solo nos dice c√≥mo procesar las cosas y de ninguna manera tiene sentido. El enfoque postulativo, tan aclamado en los c√≠rculos matem√°ticos, deja mucho que desear en la pr√°ctica.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ahora veremos un ejemplo de pruebas de coeficiente intelectual, donde la definici√≥n es tan c√≠clica como desee y, como resultado, lo enga√±a. Se crea una prueba que se supone que mide la inteligencia. Despu√©s de eso, se revisa para que sea lo m√°s consistente posible, y luego se publica y calibra de una manera simple para que la "inteligencia" medida se distribuya normalmente (por supuesto, a lo largo de la curva de calibraci√≥n). Todas las definiciones deben verificarse de manera cruzada, no solo cuando se proponen por primera vez, sino mucho m√°s tarde, cuando se usan en las conclusiones formuladas. ¬øEn qu√© medida los l√≠mites de definici√≥n son adecuados para la tarea en cuesti√≥n? ¬øCon qu√© frecuencia las definiciones dadas en las mismas condiciones comienzan a aplicarse en condiciones bastante diferentes? ¬°Esto es bastante com√∫n!En las humanidades que inevitablemente encontrar√°s en tu vida, esto sucede con m√°s frecuencia.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Por lo tanto, uno de los objetivos de esta presentaci√≥n de la teor√≠a de la informaci√≥n, adem√°s de demostrar su utilidad, era advertirle sobre este peligro o demostrar c√≥mo usarlo para obtener el resultado deseado. Se ha notado durante mucho tiempo que las definiciones iniciales determinan lo que se encuentra al final, en mucho mayor medida de lo que parece. Las definiciones iniciales requieren que prestes mucha atenci√≥n no solo en cualquier situaci√≥n nueva, sino tambi√©n en √°reas con las que has estado trabajando durante mucho tiempo. Esto le permitir√° comprender hasta qu√© punto los resultados obtenidos son una tautolog√≠a y no algo √∫til.</font></font><br><br>      ,       .   ,   ,     ,    !      ,   . <br><br> <i> ...</i> <br><br> <i>    ,     ‚Äî       magisterludi2016@yandex.ru</i> <br><br> ,         ‚Äî <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬´The Dream Machine:   ¬ª</a> ) <br><br> <b> </b> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> ,     </a> . ( <i>  10 ,  20  </i> ) <br><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text"> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pr√≥logo</font></font></a> <br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Introducci√≥n al arte de hacer ciencia e ingenier√≠a: aprender a aprender (28 de marzo de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Traducci√≥n: Cap√≠tulo 1</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Fundamentos de la revoluci√≥n digital (discreta)" (30 de marzo de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 2. Fundamentos de la revoluci√≥n digital (discreta)</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Historia de las computadoras: hardware" (31 de marzo de 1995) </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;">Cap√≠tulo 3.</font></a><font style="vertical-align: inherit;"> Historia de las computadoras: hardware</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Historia de las computadoras - Software" (4 de abril de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 4. Historia de las computadoras - Software</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Historia de las computadoras: aplicaciones (6 de abril de 1995) </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cap√≠tulo 5. Historia de las computadoras: aplicaci√≥n pr√°ctica</font></font></a> </li><li> ¬´Artificial Intelligence ‚Äî Part I¬ª (April 7, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 6.   ‚Äî 1</a> </li><li> ¬´Artificial Intelligence ‚Äî Part II¬ª (April 11, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 7.   ‚Äî II</a> </li><li> ¬´Artificial Intelligence III¬ª (April 13, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 8.  -III</a> </li><li> ¬´n-Dimensional Space¬ª (April 14, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 9. N- </a> </li><li> ¬´Coding Theory ‚Äî The Representation of Information, Part I¬ª (April 18, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 10.   ‚Äî I</a> </li><li> ¬´Coding Theory ‚Äî The Representation of Information, Part II¬ª (April 20, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 11.   ‚Äî II</a> </li><li> ¬´Error-Correcting Codes¬ª (April 21, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 12.    </a> </li><li> ¬´Information Theory¬ª (April 25, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 13.  </a> </li><li> ¬´Digital Filters, Part I¬ª (April 27, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 14.   ‚Äî 1</a> </li><li> ¬´Digital Filters, Part II¬ª (April 28, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 15.   ‚Äî 2</a> </li><li> ¬´Digital Filters, Part III¬ª (May 2, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 16.   ‚Äî 3</a> </li><li> ¬´Digital Filters, Part IV¬ª (May 4, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u="> 17.   ‚Äî IV</a> </li><li>  "Simulaci√≥n, Parte I" (5 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 18. Modelado - I</a> </li><li>  "Simulaci√≥n, Parte II" (9 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 19. Modelado - II</a> </li><li>  "Simulaci√≥n, Parte III" (11 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 20. Modelado - III</a> </li><li>  Fibra √≥ptica (12 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 21. Fibra √≥ptica</a> </li><li>  Instrucci√≥n asistida por computadora (16 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 22. Aprendizaje</a> asistido por computadora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">(CAI)</a> </li><li>  Matem√°ticas (18 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 23. Matem√°ticas</a> </li><li>  Mec√°nica cu√°ntica (19 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 24. Mec√°nica cu√°ntica</a> </li><li>  Creatividad (23 de mayo de 1995).  Traducci√≥n: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 25. Creatividad</a> </li><li>  "Expertos" (25 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 26. Expertos</a> </li><li>  ‚ÄúDatos no confiables‚Äù (26 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 27. Datos no v√°lidos</a> </li><li>  Ingenier√≠a de sistemas (30 de mayo de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 28. Ingenier√≠a de sistemas</a> </li><li>  "Obtiene lo que mide" (1 de junio de 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cap√≠tulo 29.</a> Obtiene lo que mide </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">"C√≥mo sabemos lo que sabemos"</a> (2 de junio de 1995) se <i>traduce en segmentos de 10 minutos</i> </li><li>  Hamming, "Usted y su investigaci√≥n" (6 de junio de 1995).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Traducci√≥n: usted y su trabajo</a> </li></ol><br>     ,     ‚Äî       magisterludi2016@yandex.ru <br><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es422205/">https://habr.com/ru/post/es422205/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es422195/index.html">El uso de ACS en miner√≠a</a></li>
<li><a href="../es422197/index.html">Decimos una palabra sobre el relevo</a></li>
<li><a href="../es422199/index.html">Semana de la seguridad 33: ¬øpor qui√©n oscila el monitor?</a></li>
<li><a href="../es422201/index.html">China, d√©jame descartar?</a></li>
<li><a href="../es422203/index.html">DIY clicker</a></li>
<li><a href="../es422207/index.html">Monstruos despu√©s de las vacaciones: AMD Threadripper 2990WX 32-Core y 2950X 16-Core (parte 4)</a></li>
<li><a href="../es422209/index.html">Monstruos despu√©s de las vacaciones: AMD Threadripper 2990WX 32-Core y 2950X 16-Core (parte 5)</a></li>
<li><a href="../es422211/index.html">Hermosa estructura de componentes en la nube de Microsoft Azure</a></li>
<li><a href="../es422213/index.html">Un d√≠a sin JavaScript: ¬øqu√© podr√≠a salir mal?</a></li>
<li><a href="../es422217/index.html">No es realmente serio sobre el alojamiento o c√≥mo verificar la adecuaci√≥n del host</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>