<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÜ üëßüèª üë®‚Äçüë®‚Äçüëß Siete mitos en la investigaci√≥n del aprendizaje autom√°tico üè∫ üìÄ üêù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Para aquellos que son demasiado flojos para leer todo: se sugiere una refutaci√≥n de siete mitos populares, que en el campo de la investigaci√≥n del apr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Siete mitos en la investigaci√≥n del aprendizaje autom√°tico</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444172/">  Para aquellos que son demasiado flojos para leer todo: se sugiere una refutaci√≥n de siete mitos populares, que en el campo de la investigaci√≥n del aprendizaje autom√°tico a menudo se considera cierto, a partir de febrero de 2019. Este art√≠culo est√° disponible en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">el sitio web de ArXiv en formato pdf</a> [en ingl√©s]. <br><br>  Mito 1: TensorFlow es una biblioteca tensorial. <br>  Mito 2: Las bases de datos de im√°genes reflejan fotos reales encontradas en la naturaleza. <br>  Mito 3: Los investigadores de MO no usan kits de prueba para realizar pruebas. <br>  Mito 4: el entrenamiento de redes neuronales utiliza todos los datos de entrada. <br>  Mito 5: Se requiere la normalizaci√≥n de lotes para entrenar redes residuales muy profundas. <br>  Mito 6: Las redes con atenci√≥n son mejores que la convoluci√≥n. <br>  Mito 7: Los mapas de significancia son una forma confiable de interpretar redes neuronales. <br><br>  Y ahora para los detalles. <br><a name="habracut"></a><br><h2>  Mito 1: TensorFlow es una biblioteca tensorial </h2><br>  De hecho, esta es una biblioteca para trabajar con matrices, y esta diferencia es muy significativa. <br><br>  En el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">c√°lculo de derivados de orden superior de expresiones de matriz y tensoriales.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Laue y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los</a> autores de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">NeurIPS 2018</a> demuestran que su biblioteca de diferenciaci√≥n autom√°tica, basada en c√°lculo de tensor real, tiene √°rboles de expresi√≥n mucho m√°s compactos.  El hecho es que el c√°lculo del tensor utiliza la notaci√≥n de √≠ndice, que le permite trabajar igualmente con los modos directo e inverso. <br><br>  La numeraci√≥n matricial oculta los √≠ndices por conveniencia de la notaci√≥n, por lo que los √°rboles de expresi√≥n de diferenciaci√≥n autom√°tica a menudo se vuelven demasiado complejos. <br><br>  Considere la multiplicaci√≥n matricial C = AB.  Tenemos <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>B</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="26.011ex" height="2.178ex" viewBox="0 -780.1 11199 937.7" role="img" focusable="false" style="vertical-align: -0.366ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-43" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-3D" x="2658" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-64" x="3965" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6F" x="4488" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="4974" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="5335" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-42" x="6086" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-2B" x="7067" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="8068" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-64" x="9069" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6F" x="9592" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="10078" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-42" x="10439" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>B</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ dot {C} = \ dot {A} B + A \ dot {B} </script>  para modo directo y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>A</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>C</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="28.27ex" height="2.78ex" viewBox="0 -935.7 12171.6 1197.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-3D" x="1028" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-62" x="2334" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-61" x="2764" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-72" x="3293" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-43" x="3745" y="0"></use><g transform="translate(4505,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-42" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-54" x="1074" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-2C" x="5863" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-42" x="6308" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-3D" x="7345" y="0"></use><g transform="translate(8401,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-54" x="1061" y="513"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-62" x="10000" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-61" x="10430" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-72" x="10959" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-43" x="11411" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow><msup><mi>B</mi><mi>T</mi></msup><mo>,</mo><mi>B</mi><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>C</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-2"> A = \ bar {C} B ^ T, B = A ^ T \ bar {C} </script>  por lo contrario  Para realizar correctamente la multiplicaci√≥n, debe observar estrictamente el orden y el uso de la separaci√≥n sil√°bica.  Desde el punto de vista de la grabaci√≥n, esto parece confuso para una persona involucrada en MO, pero desde el punto de vista de los c√°lculos, esta es una carga adicional para el programa. <br><br>  Otro ejemplo, menos trivial: c = det (A).  Tenemos <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy=&quot;false&quot;>(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.951ex" height="2.66ex" viewBox="0 -832 10312.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-64" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6F" x="773" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="1259" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-63" x="1620" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-3D" x="2331" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="3388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-72" x="3749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-28" x="4201" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-69" x="4590" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6E" x="4936" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-76" x="5536" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-28" x="6022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="6411" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-29" x="7162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-64" x="7801" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6F" x="8325" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-74" x="8810" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="9172" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-29" x="9922" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>d</mi><mi>o</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ dot {c} = tr (inv (A) \ dot {A}) </script>  para modo directo y <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>A</mi></mrow><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy=&quot;false&quot;>(</mo><mi>A</mi><msup><mo stretchy=&quot;false&quot;>)</mo><mi>T</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.835ex" height="2.901ex" viewBox="0 -935.7 9831.7 1249" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-62" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-61" x="679" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-72" x="1209" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="1660" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-3D" x="2688" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-62" x="3995" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-61" x="4424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-72" x="4954" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-63" x="5405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-63" x="5839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-69" x="6272" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-6E" x="6618" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-76" x="7218" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-28" x="7704" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-41" x="8093" y="0"></use><g transform="translate(8844,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMAIN-29" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://habr.com/ru/post/444172/&amp;usg=ALkJrhhYKBvTGy-OsdW3iNpfDnJAhZgWvw#MJMATHI-54" x="550" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>A</mi></mrow><mo>=</mo><mtext>&nbsp;</mtext><mi>b</mi><mi>a</mi><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>c</mi></mrow><mi>c</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo stretchy="false">(</mo><mi>A</mi><msup><mo stretchy="false">)</mo><mi>T</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ bar {A} = \ bar {c} cinv (A) ^ T </script>  por lo contrario  En este caso, obviamente es imposible usar el √°rbol de expresi√≥n para ambos modos, dado que consisten en operadores diferentes. <br><br>  En general, la forma en que TensorFlow y otras bibliotecas (por ejemplo, Mathematica, Maple, Sage, SimPy, ADOL-C, TAPENADE, TensorFlow, Theano, PyTorch, HIPS autograd) implementaron la diferenciaci√≥n autom√°tica, lo que lleva al hecho de que para directo e inverso Los √°rboles de expresi√≥n diferentes e ineficaces se construyen en el modo.  La numeraci√≥n del tensor evita estos problemas debido a la conmutatividad de la multiplicaci√≥n debido a la notaci√≥n de √≠ndice.  Para detalles sobre c√≥mo funciona esto, vea el art√≠culo cient√≠fico. <br><br>  Los autores probaron su m√©todo realizando una diferenciaci√≥n autom√°tica del r√©gimen inverso, tambi√©n conocido como propagaci√≥n inversa, en tres tareas diferentes, y midieron el tiempo que tom√≥ calcular las Hesse. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/923/263/331/923263331347f83e3bac0fe7a9477e8b.png"><br><br>  En el primer problema, se optimiz√≥ la funci√≥n cuadr√°tica x <sup>T</sup> Ax.  En el segundo, se calcul√≥ la regresi√≥n log√≠stica, en el tercero - factorizaci√≥n matricial. <br><br>  En la CPU, su m√©todo result√≥ ser dos √≥rdenes de magnitud m√°s r√°pido que las bibliotecas populares como TensorFlow, Theano, PyTorch y HIPS autograd. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/184/395/7dd/1843957dde52a4722da309adcdd6ea92.png"><br><br>  En la GPU, observaron una aceleraci√≥n a√∫n mayor, de hasta tres √≥rdenes de magnitud. <br><br>  <b>Las consecuencias:</b> <br><br>  Calcular derivados para funciones de segundo o mayor orden utilizando las bibliotecas actuales de aprendizaje profundo es demasiado costoso desde un punto de vista computacional.  Esto incluye el c√°lculo de los tensores generales de cuarto orden como los Hessianos (por ejemplo, en MAML y la optimizaci√≥n de segundo orden de Newton).  Afortunadamente, las f√≥rmulas cuadr√°ticas son raras en el aprendizaje profundo.  Sin embargo, a menudo se encuentran en el aprendizaje autom√°tico "cl√°sico": <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">SVM</a> , m√©todo de m√≠nimos cuadrados, LASSO, procesos gaussianos, etc. <br><br><h2>  Mito 2: las bases de datos de im√°genes reflejan fotos del mundo real </h2><br>  A muchas personas les gusta pensar que las redes neuronales han aprendido a reconocer objetos mejor que las personas.  Esto no es asi.  Pueden estar por delante de las personas en las bases de im√°genes seleccionadas, por ejemplo, ImageNet, pero en el caso del reconocimiento de objetos de fotos reales de la vida cotidiana, definitivamente no podr√°n adelantar a un adulto com√∫n.  Esto se debe a que la selecci√≥n de im√°genes en los conjuntos de datos actuales no coincide con la selecci√≥n de todas las im√°genes posibles que se encuentran naturalmente en la realidad. <br><br>  En un trabajo bastante antiguo, Un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">vistazo imparcial al sesgo del conjunto de datos.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Torralba y Efros.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">CVPR 2011.</a> , Los autores propusieron estudiar las distorsiones asociadas con un conjunto de im√°genes en doce bases de datos populares, descubriendo si es posible entrenar al clasificador para determinar el conjunto de datos del que se tom√≥ esta imagen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/400/31a/f1a/40031af1a6a5bef3c9314ecb2373926a.png"><br><br>  Las posibilidades de adivinar accidentalmente el conjunto de datos correcto son 1/12 ‚âà 8%, mientras que los propios cient√≠ficos hicieron frente a la tarea con una tasa de √©xito de&gt; 75%. <br><br>  Entrenaron SVM en un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">histograma de gradiente direccional</a> (HOG) y descubrieron que el clasificador complet√≥ la tarea en el 39% de los casos, lo que excede significativamente los golpes aleatorios.  Si repetimos este experimento hoy, con las redes neuronales m√°s avanzadas, seguramente ver√≠amos un aumento en la precisi√≥n del clasificador. <br><br>  Si las bases de datos de im√°genes muestran correctamente las im√°genes verdaderas del mundo real, no tendr√≠amos que poder determinar de qu√© conjunto de datos proviene una imagen en particular. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/879/7f7/4ad/8797f74ad14a0a4e28959ff0c67faca8.png"><br><br>  Sin embargo, hay rasgos en los datos que hacen que cada conjunto de im√°genes sea diferente de las dem√°s.  ImageNet tiene muchos autos de carrera que es poco probable que describan el auto promedio "te√≥rico" en su conjunto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/614/416/401/6144164018d5608bc5c2dc55e6e981f6.png"><br><br>  Los autores tambi√©n determinaron el valor de cada conjunto de datos midiendo qu√© tan bien funciona un clasificador entrenado en un conjunto con im√°genes de otros conjuntos.  De acuerdo con esta m√©trica, las bases de datos LabelMe e ImageNet resultaron ser las menos sesgadas, habiendo recibido una calificaci√≥n de 0.58 utilizando el m√©todo de "canasta de divisas".  Todos los valores resultaron ser inferiores a la unidad, lo que significa que la capacitaci√≥n en un conjunto de datos diferente siempre conduce a un bajo rendimiento.  En un mundo ideal sin conjuntos sesgados, algunos n√∫meros deber√≠an haber excedido uno. <br><br>  Los autores concluyeron pesimistamente: <br><blockquote>  Entonces, ¬øcu√°l es el valor de los conjuntos de datos existentes para algoritmos de entrenamiento dise√±ados para el mundo real?  La respuesta resultante se puede describir como "mejor que nada pero no mucho". </blockquote><br><br><h2>  Mito 3: los investigadores de MO no usan kits de prueba para las pruebas </h2><br>  En el libro de texto sobre aprendizaje autom√°tico, se nos ense√±a a dividir el conjunto de datos en capacitaci√≥n, evaluaci√≥n y verificaci√≥n.  La efectividad del modelo, entrenado en el conjunto de entrenamiento y evaluado en la evaluaci√≥n, ayuda a la persona involucrada en el MO a ajustar el modelo para maximizar la eficiencia en su uso real.  No es necesario tocar el conjunto de prueba hasta que la persona termine de ajustarse para proporcionar una evaluaci√≥n imparcial de la efectividad real del modelo en el mundo real.  Si una persona hace trampa usando un conjunto de pruebas en las etapas de capacitaci√≥n o evaluaci√≥n, el modelo corre el riesgo de adaptarse demasiado a un conjunto de datos en particular. <br><br>  En el mundo hipercompetitivo de la investigaci√≥n de MO, los nuevos algoritmos y modelos a menudo se juzgan por la efectividad de su trabajo con los datos de verificaci√≥n.  Por lo tanto, no tiene sentido que los investigadores escriban o publiquen documentos que describan m√©todos que funcionan mal con los conjuntos de datos de prueba.  Y esto, en esencia, significa que la comunidad de la Regi√≥n de Mosc√∫ en su conjunto utiliza un conjunto de pruebas para la evaluaci√≥n. <br><br>  ¬øCu√°les son las consecuencias de esta estafa? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d4/f64/749/2d4f647491071cd0b77eb199ef563275.png"><br><br>  Autores de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">¬øLos clasificadores CIFAR-10 se generalizan a CIFAR-10?</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Recht y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ArXiv 2018</a> investig√≥ este problema creando un nuevo conjunto de pruebas para CIFAR-10.  Para hacer esto, hicieron una selecci√≥n de im√°genes de Tiny Images. <br><br>  Eligieron CIFAR-10 porque es uno de los conjuntos de datos m√°s utilizados en el MO, el segundo conjunto m√°s popular en NeurIPS 2017 (despu√©s de MNIST).  El proceso de creaci√≥n de un conjunto de datos para CIFAR-10 tambi√©n est√° bien descrito y es transparente, en la gran base de datos de Tiny Images hay muchas etiquetas detalladas, por lo que puede reproducir un nuevo conjunto de prueba, minimizando el cambio de distribuci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/71a/cd9/091/71acd909148795f73b49819b39946a5a.png"><br><br>  Descubrieron que una gran cantidad de modelos diferentes de redes neuronales en el nuevo conjunto de prueba mostraron una ca√≠da significativa en la precisi√≥n (4% - 15%).  Sin embargo, el rango de rendimiento relativo de cada modelo se mantuvo bastante estable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6b/5aa/cfa/b6b5aacfab0c2628ea2ffe28babae533.png"><br><br>  En general, los modelos con mejor rendimiento mostraron una menor ca√≠da de precisi√≥n en comparaci√≥n con los de peor rendimiento.  Esto es bueno porque se deduce que la p√©rdida de generalizaci√≥n del modelo debido al enga√±o, al menos en el caso de CIFAR-10, disminuye a medida que la comunidad inventa m√©todos y modelos de MO mejorados. <br><br><h2>  Mito 4: el entrenamiento de la red neuronal utiliza todas las entradas </h2><br>  En general, se acepta que los <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">datos son un nuevo petr√≥leo</a> y que cuantos m√°s datos tengamos, mejor podremos capacitar modelos de aprendizaje profundo que ahora son ineficientes en la muestra y est√°n sobrepatrizados. <br><br>  En <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">un estudio emp√≠rico de ejemplos de olvido durante el aprendizaje de redes neuronales profundas.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Toneva y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los</a> autores de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICLR 2019</a> demuestran una redundancia significativa en varios conjuntos comunes de im√°genes peque√±as.  Sorprendentemente, el 30% de los datos de CIFAR-10 simplemente se pueden eliminar sin cambiar la precisi√≥n del cheque en una cantidad significativa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/418/39b/03a/41839b03afd94771e940b60e2d7f905d.png"><br>  <i>Historias de olvido de (izquierda a derecha) MNIST, permutedMNIST y CIFAR-10.</i> <br><br>  El olvido ocurre cuando una red neuronal clasifica incorrectamente una imagen en el tiempo t + 1, mientras que en el tiempo t pudo clasificar correctamente una imagen.  El flujo de tiempo se mide mediante actualizaciones SGD.  Para rastrear el olvido, los autores lanzaron su red neuronal en un peque√±o conjunto de datos despu√©s de cada actualizaci√≥n de SGD, y no en todos los ejemplos disponibles en la base de datos.  Los ejemplos que no est√°n sujetos al olvido se llaman ejemplos inolvidables. <br><br>  Descubrieron que 91.7% MNIST, 75.3% permutedMNIST, 31.3% CIFAR-10 y 7.62% CIFAR-100 son ejemplos inolvidables.  Esto es intuitivamente comprensible, ya que aumentar la diversidad y la complejidad del conjunto de datos deber√≠a hacer que la red neuronal olvide m√°s ejemplos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdb/16b/ff9/fdb16bff98521d9d764588a679e51a59.png"><br><br>  Los ejemplos olvidables parecen exhibir caracter√≠sticas m√°s raras y extra√±as en comparaci√≥n con las inolvidables.  Los autores los comparan con los vectores de soporte en SVM, ya que parecen dibujar el contorno de los l√≠mites de decisi√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/c74/969/a75c74969991b75f621d340952a5cde3.png"><br><br>  Ejemplos inolvidables, a su vez, codifican principalmente informaci√≥n redundante.  Si clasificamos los ejemplos por el grado de inolvidabilidad, podemos comprimir el conjunto de datos eliminando los m√°s inolvidables. <br><br>  El 30% de los datos de CIFAR-10 se pueden eliminar sin afectar la precisi√≥n de las verificaciones, y la eliminaci√≥n del 35% de los datos conduce a una ligera ca√≠da en la precisi√≥n de las verificaciones en un 0.2%.  Si selecciona el 30% de los datos al azar, eliminarlos conducir√° a una p√©rdida significativa en la precisi√≥n de la verificaci√≥n del 1%. <br><br>  Del mismo modo, el 8% de los datos se pueden eliminar del CIFAR-100 sin disminuir la precisi√≥n de la validaci√≥n. <br><br>  Estos resultados muestran que existe una redundancia significativa en los datos para el entrenamiento de redes neuronales, similar al entrenamiento SVM, donde los vectores no compatibles pueden eliminarse sin afectar la decisi√≥n del modelo. <br><br>  <b>Las consecuencias:</b> <br><br>  Si podemos determinar cu√°l de los datos es inolvidable antes de comenzar el entrenamiento, entonces podemos ahorrar espacio elimin√°ndolos y tiempo sin usarlos al entrenar una red neuronal. <br><br><h2>  Mito 5: Se requiere la normalizaci√≥n de lotes para entrenar redes residuales muy profundas. </h2><br>  Durante mucho tiempo se crey√≥ que "entrenar una red neuronal profunda para la optimizaci√≥n directa solo para un prop√≥sito controlado (por ejemplo, la probabilidad logar√≠tmica de una clasificaci√≥n correcta) usando el descenso de gradiente, comenzando con par√°metros aleatorios, no funciona bien". <br><br>  El mont√≥n de m√©todos ingeniosos de inicializaci√≥n aleatoria, funciones de activaci√≥n, t√©cnicas de optimizaci√≥n y otras innovaciones, como las conexiones residuales, que han aparecido desde entonces, facilitaron el entrenamiento de redes neuronales profundas utilizando el m√©todo de descenso de gradiente. <br><br>  Pero se produjo un avance real despu√©s de la introducci√≥n de la normalizaci√≥n por lotes (y otras t√©cnicas de normalizaci√≥n secuencial), limitando el tama√±o de las activaciones para cada capa de la red a fin de eliminar el problema de la desaparici√≥n y los gradientes explosivos. <br><br>  En un trabajo reciente, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Arreglo de inicializaci√≥n: aprendizaje residual sin normalizaci√≥n.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Zhang y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICLR 2019</a> ha demostrado que es posible entrenar una red con 10,000 capas usando SGD puro sin aplicar ninguna normalizaci√≥n. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d69/a29/cbb/d69a29cbb7bfce9750c9aa3ee6d7d659.png"><br><br>  Los autores compararon el entrenamiento residual de la red neuronal para diferentes profundidades en CIFAR-10 y descubrieron que si bien los m√©todos de inicializaci√≥n est√°ndar no funcionaban para 100 capas, los m√©todos de reparaci√≥n y normalizaci√≥n por lotes tuvieron √©xito con 10,000 capas. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc9/23f/26a/bc923f26a4c39fb67663383716bc10d2.png" alt="imagen"><br><br>  Llevaron a cabo un an√°lisis te√≥rico y mostraron que "la normalizaci√≥n del gradiente de ciertas capas est√° limitada por el n√∫mero que aumenta infinitamente desde una red profunda", que es un problema de gradientes explosivos.  Para evitar esto, se usa Foxup, cuya idea clave es escalar los pesos en m capas para cada una de las ramas residuales L por el n√∫mero de veces dependiendo de my L. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/07f/a8f/39a/07fa8f39ad814861fd0cba6c936a2e18.png"><br><br>  La reparaci√≥n ayud√≥ a entrenar una red residual profunda con 110 capas en el CIFAR-10 con una alta velocidad de aprendizaje comparable al comportamiento de una red de arquitectura similar entrenada usando la normalizaci√≥n por lotes. <br><br>  Los autores mostraron resultados de prueba similares utilizando Fixup en la red sin ninguna normalizaci√≥n, trabajando con la base de datos ImageNet y con traducciones del ingl√©s al alem√°n. <br><br><h2>  Mito 6: Las redes con atenci√≥n son mejores que las convolucionales. </h2><br>  La idea de que los mecanismos de "atenci√≥n" son superiores a las redes neuronales convolucionales est√° ganando popularidad en la comunidad de investigadores de MO.  En el trabajo de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/(">Vaswani y sus colegas</a> , se observ√≥ que "el costo computacional de las convoluciones desmontables es igual a la combinaci√≥n de una capa de auto atenci√≥n y una capa de retroalimentaci√≥n puntual". <br><br>  Incluso las redes competitivas generativas avanzadas muestran la ventaja de la auto-atenci√≥n sobre la convoluci√≥n est√°ndar al modelar dependencias de largo alcance. <br><br>  Los contribuyentes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">prestan menos atenci√≥n con convoluciones ligeras y din√°micas.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Wu y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ICLR 2019</a> arroja dudas sobre la eficiencia param√©trica y la efectividad de la auto atenci√≥n al modelar dependencias de largo alcance, y ofrece nuevas opciones de convoluci√≥n, parcialmente inspiradas en la auto atenci√≥n, m√°s efectivas en t√©rminos de par√°metros. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b40/f9a/a15/b40f9aa151eb4bcc3ebf224483ff8028.png"><br><br>  Las circunvoluciones "ligeras" son separables en profundidad, softmax normalizadas en dimensi√≥n de tiempo, separadas por peso en dimensi√≥n de canal y reutilizan los mismos pesos en cada paso de tiempo (como redes neuronales recurrentes).  Las convoluciones din√°micas son ligeras que usan pesos diferentes en cada paso de tiempo. <br><br>  Tales trucos hacen que las convoluciones ligeras y din√°micas sean varias veces m√°s efectivas que las convoluciones indivisibles est√°ndar. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/87d/d51/da0/87dd51da07637c42c4e4c2811b08b241.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6a/469/ae5/c6a469ae5432cf31761e7663449497fd.png"><br><br>  Los autores muestran que estas nuevas convoluciones corresponden o exceden las redes autoabsorbentes en la traducci√≥n autom√°tica, el modelado del lenguaje, los problemas de suma abstracta, utilizando los mismos o menos par√°metros. <br><br><h2>  Mito 7: Tarjetas de importancia: una forma confiable de interpretar redes neuronales </h2><br>  Aunque existe la opini√≥n de que las redes neuronales son cajas negras, ha habido muchos intentos de interpretarlas.  Los m√°s populares son los mapas de significancia u otros m√©todos similares que asignan evaluaciones de importancia a caracter√≠sticas o ejemplos de capacitaci√≥n. <br><br>  Es tentador poder concluir que una imagen dada ha sido clasificada de cierta manera debido a ciertas partes de la imagen que son significativas para la red neuronal.  Para calcular los mapas de significancia, existen varios m√©todos que a menudo utilizan la activaci√≥n de redes neuronales en una imagen determinada y los gradientes que pasan por la red. <br><br>  En la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">interpretaci√≥n de las redes neuronales es fr√°gil.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Ghorbani y col.</a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Los</a> autores de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">AAAI 2019</a> muestran que pueden introducir un cambio escurridizo en la imagen, que, sin embargo, distorsionar√° su mapa de importancia. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/077/b89/e92/077b89e923e2936769b2f99662171a94.png"><br><br>  La red neuronal determina la mariposa monarca no por el patr√≥n en sus alas, sino por la presencia de hojas verdes sin importancia en el fondo de la foto. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d2/314/9fa/1d23149fa91caab32dc2ea16a8593379.png"><br><br>  Las im√°genes multidimensionales a menudo est√°n m√°s cerca de los l√≠mites de decisi√≥n establecidos por las redes neuronales profundas, de ah√≠ su sensibilidad a los ataques adversos.  Y si los ataques competitivos mueven las im√°genes m√°s all√° de los l√≠mites de la soluci√≥n, los ataques interpretativos competitivos los desplazan a lo largo del l√≠mite de la soluci√≥n sin abandonar el territorio de la misma soluci√≥n. <br><br>  El m√©todo b√°sico desarrollado por los autores es una modificaci√≥n del m√©todo Goodfello de marcado r√°pido de gradiente, que fue uno de los primeros m√©todos exitosos de ataques competitivos.  Se puede suponer que otros ataques m√°s nuevos y m√°s complejos tambi√©n se pueden usar para ataques a la interpretaci√≥n de redes neuronales. <br><br>  <b>Las consecuencias:</b> <br><br>  Debido a la creciente difusi√≥n del aprendizaje profundo en √°reas cr√≠ticas de aplicaci√≥n como la imagen m√©dica, es importante abordar cuidadosamente la interpretaci√≥n de las decisiones tomadas por las redes neuronales.  Por ejemplo, aunque ser√≠a genial si la red neuronal convolucional pudiera reconocer la mancha en la imagen de MRI como un tumor maligno, no se debe confiar en estos resultados si se basan en m√©todos de interpretaci√≥n poco confiables. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/444172/">https://habr.com/ru/post/444172/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444162/index.html">Revisi√≥n del tel√©fono Snom D120 IP</a></li>
<li><a href="../444164/index.html">Descripci√≥n general del sistema de advertencia Snom PA1</a></li>
<li><a href="../444166/index.html">Pavel Finkelstein sobre Kotlin en producci√≥n en jug.msk.ru</a></li>
<li><a href="../444168/index.html">C√≥mo transferir Windows 10 con licencia a otra computadora</a></li>
<li><a href="../444170/index.html">Publique aplicaciones de iOS en App Store con GitLab y fastlane</a></li>
<li><a href="../444174/index.html">GeekBrains invita a los principiantes a un juego educativo</a></li>
<li><a href="../444176/index.html">Cifrados elementales en lenguaje sencillo</a></li>
<li><a href="../444178/index.html">9 consejos para crear juegos independientes de un solo desarrollador</a></li>
<li><a href="../444182/index.html">Ir condiciones y sus rarezas</a></li>
<li><a href="../444184/index.html">Sobre las perspectivas de los centros de datos premontados</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>