<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ô†Ô∏è üíª üßìüèæ Comment les services de base de donn√©es g√©r√©s sont-ils organis√©s dans Yandex? üôÖüèª ü§∂üèª üë©üèø‚Äç‚öïÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lorsque vous faites confiance √† quelqu'un, la chose la plus pr√©cieuse que vous avez - les donn√©es de votre application ou service - vous voulez imagin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Comment les services de base de donn√©es g√©r√©s sont-ils organis√©s dans Yandex?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/477860/">  Lorsque vous faites confiance √† quelqu'un, la chose la plus pr√©cieuse que vous avez - les donn√©es de votre application ou service - vous voulez imaginer comment cette personne va g√©rer votre plus grande valeur. <br><br>  Je m'appelle Vladimir Borodin, je suis le chef de la plateforme de donn√©es Yandex.Cloud.  Aujourd'hui, je veux vous dire comment tout est organis√© et fonctionne au sein des services de bases de donn√©es g√©r√©es Yandex, pourquoi tout est fait comme √ßa et quels sont les avantages - du point de vue des utilisateurs - de nos solutions.  Et bien s√ªr, vous d√©couvrirez certainement ce que nous pr√©voyons de finaliser dans un proche avenir afin que le service devienne meilleur et plus pratique pour tous ceux qui en ont besoin. <br><br>  Eh bien, allons-y! <br><br><img src="https://habrastorage.org/webt/xw/qq/id/xwqqid27hmguyyigukvb2z3o8da.png" alt="image"><br><a name="habracut"></a><br>  Bases de donn√©es g√©r√©es (bases de donn√©es g√©r√©es Yandex) est l'un des services Yandex.Cloud les plus populaires.  Plus pr√©cis√©ment, il s'agit de tout un groupe de services, qui est d√©sormais le deuxi√®me derri√®re les machines virtuelles Yandex Compute Cloud en popularit√©. <br><br>  Les bases de donn√©es g√©r√©es Yandex permettent d'obtenir rapidement une base de donn√©es fonctionnelle et assument de telles t√¢ches: <br><br><ul><li>  √âvolutivit√© - de la capacit√© √©l√©mentaire d'ajouter des ressources informatiques ou de l'espace disque √† une augmentation du nombre de r√©pliques et de fragments. </li><li>  Installez les mises √† jour, mineures et majeures. </li><li>  Sauvegarde et restauration. </li><li>  Offrir une tol√©rance aux pannes. </li><li>  Suivi </li><li>  Fournir des outils de configuration et de gestion pratiques. </li></ul><br><h2>  Organisation des services de base de donn√©es g√©r√©s: vue de dessus </h2><br>  Le service comprend deux parties principales: le plan de contr√¥le et le plan de donn√©es.  Control Plane est, tout simplement, une API de gestion de base de donn√©es qui vous permet de cr√©er, modifier ou supprimer des bases de donn√©es.  Le plan de donn√©es est le niveau de stockage direct des donn√©es. <br><br><img src="https://habrastorage.org/webt/vo/4c/o-/vo4co-q4zc3r6xwe0is2pl9hrri.png" alt="image"><br><br>  Les utilisateurs du service disposent en effet de deux points d'entr√©e: <br><br><ul><li>  Dans le plan de contr√¥le.  En fait, il existe de nombreuses entr√©es - la console Web, l'utilitaire CLI et l'API de passerelle qui fournit l'API publique (gRPC et REST).  Mais tous vont finalement √† ce que nous appelons l'API interne, et donc nous consid√©rerons ce point d'entr√©e dans le plan de contr√¥le.  En fait, c'est le point de d√©part du domaine de responsabilit√© du service Bases de donn√©es g√©r√©es (MDB). </li><li> Dans le plan de donn√©es.  Il s'agit d'une connexion directe √† une base de donn√©es en cours d'ex√©cution via des protocoles d'acc√®s au SGBD.  Si c'est, par exemple, PostgreSQL, alors ce sera <a href="https://www.postgresql.org/docs/current/libpq.html">l'interface libpq</a> . </li></ul><br><img src="https://habrastorage.org/webt/cr/l5/pq/crl5pqrbr2mqqlun6ivgwlvn0u8.png" alt="image"><br>  Ci-dessous, nous d√©crirons plus en d√©tail tout ce qui se passe dans le plan de donn√©es, et nous analyserons chacun des composants du plan de contr√¥le. <br><br><h2>  Plan de donn√©es </h2><br>  Avant d'examiner les composants du plan de contr√¥le, examinons ce qui se passe dans le plan de donn√©es. <br><br><h3>  √Ä l'int√©rieur d'une machine virtuelle </h3><br>  MDB ex√©cute des bases de donn√©es dans les m√™mes machines virtuelles que celles fournies dans <a href="https://cloud.yandex.ru/services/compute">Yandex Compute Cloud</a> . <br><br><img src="https://habrastorage.org/webt/-w/gm/qj/-wgmqjlxi30m4jz7dgp4sct86cw.png" alt="image"><br><br>  Tout d'abord, un moteur de base de donn√©es, par exemple PostgreSQL, y est d√©ploy√©.  En parall√®le, diff√©rents programmes auxiliaires peuvent √™tre lanc√©s.  Pour PostgreSQL, ce sera <a href="https://github.com/yandex/odyssey">Odyssey</a> , l'extracteur de connexion √† la base de donn√©es. <br><br>  Toujours √† l'int√©rieur de la machine virtuelle, un certain ensemble de services standard est lanc√©, le sien pour chaque SGBD: <br><br><ul><li>  Service de cr√©ation de sauvegardes.  Pour PostgreSQL, il s'agit d'un outil <a href="https://github.com/wal-g/wal-g">WAL-G</a> open source <a href="https://github.com/wal-g/wal-g">.</a>  Il cr√©e des sauvegardes et les stocke dans <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> . </li><li>  Salt Minion est un composant du syst√®me <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> pour les op√©rations et la gestion de la configuration.  Plus d'informations √† ce sujet sont fournies ci-dessous dans la description de l'infrastructure de d√©ploiement. </li><li>  M√©triques MDB, qui est responsable de la transmission des mesures de base de donn√©es √† <a href="https://cloud.yandex.ru/services/monitoring">Yandex Monitoring</a> et √† notre microservice pour surveiller l'√©tat des clusters et h√¥tes MDB Health. </li><li>  Le client Push, qui envoie des journaux SGBD et des journaux de facturation au service Logbroker, est une solution sp√©ciale pour la collecte et la livraison de donn√©es. </li><li>  MDB cron - notre v√©lo, qui diff√®re du cron habituel par la capacit√© d'effectuer des t√¢ches p√©riodiques avec une pr√©cision d'une seconde. </li></ul><br><h3>  Topologie du r√©seau </h3><br><img src="https://habrastorage.org/webt/x8/_6/ge/x8_6ge0uluxqju22unryyrxxhxw.png" alt="image"><br><br>  Chaque h√¥te du plan de donn√©es poss√®de deux interfaces r√©seau: <br><br><ul><li>  L'un d'eux se connecte au r√©seau de l'utilisateur.  En g√©n√©ral, il est n√©cessaire de g√©rer la charge du produit.  Gr√¢ce √† elle, la r√©plication est √† la poursuite. </li><li>  Le second reste dans l'un de nos r√©seaux g√©r√©s via lesquels les h√¥tes acc√®dent au Control Plane. </li></ul><br>  Oui, les h√¥tes de diff√©rents clients sont coinc√©s dans un tel r√©seau g√©r√©, mais ce n'est pas effrayant, car sur l'interface g√©r√©e (presque) rien n'√©coute, les connexions r√©seau sortantes dans Control Plane ne sont ouvertes que depuis celui-ci.  Presque personne, car il existe des ports ouverts (par exemple, SSH), mais ils sont ferm√©s par un pare-feu local qui autorise uniquement les connexions √† partir d'h√¥tes sp√©cifiques.  Par cons√©quent, si un attaquant acc√®de √† une machine virtuelle avec une base de donn√©es, il ne peut pas acc√©der aux bases de donn√©es d'autres personnes. <br><br><h3>  S√©curit√© du plan de donn√©es </h3><br>  Puisque nous parlons de s√©curit√©, il faut dire que nous avons initialement con√ßu le service en fonction de l'attaquant se rootant sur la machine virtuelle du cluster. <br><br>  En fin de compte, nous avons mis beaucoup d'efforts pour faire ce qui suit: <br><br><ul><li>  Pare-feu local et grand; </li><li>  Cryptage de toutes les connexions et sauvegardes; </li><li>  Tous avec authentification et autorisation; </li><li>  AppArmor </li><li>  IDS auto-√©crit. </li></ul><br>  Consid√©rez maintenant les composants du plan de contr√¥le. <br><br><h2>  Avion de contr√¥le </h2><br><h3>  API interne </h3><br>  L'API interne est le premier point d'entr√©e dans le plan de contr√¥le.  Voyons comment tout fonctionne ici. <br><br><img src="https://habrastorage.org/webt/vb/ru/j7/vbruj7qxxu2tmlaplki2dt_74cy.png" alt="image"><br><br>  Supposons que l'API interne re√ßoive une demande de cr√©ation d'un cluster de base de donn√©es. <br><br>  Tout d'abord, l'API interne acc√®de au service d'acc√®s au service cloud, qui est charg√© de v√©rifier l'authentification et l'autorisation de l'utilisateur.  Si l'utilisateur r√©ussit la v√©rification, l'API interne v√©rifie la validit√© de la demande elle-m√™me.  Par exemple, une demande de cr√©ation d'un cluster sans sp√©cifier son nom ou avec un nom d√©j√† pris √©chouera au test. <br><blockquote>  Et l'API interne peut envoyer des demandes √† l'API d'autres services.  Si vous souhaitez cr√©er un cluster dans un certain r√©seau A et un h√¥te sp√©cifique dans un sous-r√©seau B sp√©cifique, l'API interne doit s'assurer que vous avez des droits √† la fois sur le r√©seau A et le sous-r√©seau sp√©cifi√© B. En m√™me temps, il v√©rifiera que le sous-r√©seau B appartient au r√©seau A Cela n√©cessite un acc√®s √† l'API d'infrastructure. </blockquote><br>  Si la demande est valide, les informations sur le cluster cr√©√© seront enregistr√©es dans la m√©tabase.  Nous l'appelons MetaDB, il est d√©ploy√© sur PostgreSQL.  MetaDB a une table avec une file d'attente d'op√©rations.  L'API interne enregistre des informations sur l'op√©ration et d√©finit la t√¢che de mani√®re transactionnelle.  Apr√®s cela, des informations sur l'op√©ration sont renvoy√©es √† l'utilisateur. <br><br>  En g√©n√©ral, pour traiter la plupart des demandes de l'API interne, il suffit d'utiliser MetaDB et l'API des services associ√©s.  Mais il existe deux autres composants vers lesquels l'API interne r√©pond pour r√©pondre √† certaines requ√™tes: LogsDB, o√π se trouvent les journaux du cluster d'utilisateurs, et MDB Health.  √Ä propos de chacun d'eux sera d√©crit plus en d√©tail ci-dessous. <br><br><h3>  Ouvrier </h3><br>  Les travailleurs sont simplement un ensemble de processus qui interrogent la file d'attente des op√©rations dans MetaDB, les r√©cup√®rent et les ex√©cutent. <br><br><img src="https://habrastorage.org/webt/ez/u7/q8/ezu7q86g82hjjt2nqrdssqhxpn0.png" alt="image"><br><br>  Que fait exactement un travailleur lors de la cr√©ation d'un cluster?  Il se tourne d'abord vers l'API d'infrastructure pour cr√©er des machines virtuelles √† partir de nos images (ils ont d√©j√† tous les packages n√©cessaires install√©s et la plupart des choses sont configur√©es, les images sont mises √† jour une fois par jour).  Lorsque les machines virtuelles sont cr√©√©es et que le r√©seau y d√©colle, le travailleur se tourne vers l'infrastructure de d√©ploiement (nous vous en dirons plus √† ce sujet plus tard) pour d√©ployer ce dont l'utilisateur a besoin sur les machines virtuelles. <br><br>  De plus, le travailleur acc√®de √† d'autres services Cloud.  Par exemple, dans <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> pour cr√©er un compartiment dans lequel les sauvegardes de cluster seront enregistr√©es.  Au service de <a href="https://cloud.yandex.ru/services/monitoring">surveillance Yandex</a> , qui collectera et visualisera les m√©triques de la base de donn√©es.  Le travailleur doit y cr√©er des m√©ta-informations de cluster.  √Ä l'API DNS, si l'utilisateur souhaite attribuer des adresses IP publiques aux h√¥tes du cluster. <br><br>  En g√©n√©ral, le travailleur travaille tr√®s simplement.  Il re√ßoit la t√¢che de la file d'attente de la m√©tabase et acc√®de au service souhait√©.  Apr√®s avoir termin√© chaque √©tape, le travailleur stocke des informations sur la progression de l'op√©ration dans la m√©tabase.  En cas d'√©chec, la t√¢che red√©marre simplement et s'ex√©cute √† l'endroit o√π elle s'√©tait arr√™t√©e.  Mais m√™me le red√©marrer depuis le d√©but n'est pas un probl√®me, car presque tous les types de t√¢ches pour les travailleurs sont √©crits de mani√®re idempotante.  En effet, le travailleur peut effectuer l'une ou l'autre √©tape de l'op√©ration, mais il n'y a aucune information √† ce sujet dans MetaDB. <br><br><h3>  D√©ployer l'infrastructure </h3><br>  Tout en bas se trouve <a href="https://docs.saltstack.com/en/getstarted/">SaltStack</a> , un syst√®me de gestion de configuration open source assez courant √©crit en Python.  Le syst√®me est tr√®s <a href="https://docs.saltstack.com/en/latest/ref/index.html">extensible</a> , pour lequel nous l'aimons. <br><br>  Les principaux composants de Salt sont Salt Master, qui stocke des informations sur ce qui doit √™tre appliqu√© et o√π, et Salt Minion - un agent install√© sur chaque h√¥te, interagit avec le ma√Ætre et peut appliquer directement le sel du Salt Master sur l'h√¥te.  Pour les besoins de cet article, nous avons suffisamment de connaissances et vous pouvez en lire plus dans la <a href="https://docs.saltstack.com/en/getstarted/overview.html">documentation SaltStack</a> . <br><br>  Un ma√Ætre du sel n'est pas tol√©rant aux pannes et ne s'√©tend pas √† des milliers de serviteurs, plusieurs ma√Ætres sont n√©cessaires.  Interagir avec cela directement √† partir du travailleur n'est pas pratique, et nous avons √©crit nos liaisons sur Salt, que nous appelons le cadre de d√©ploiement. <br><br><img src="https://habrastorage.org/webt/ir/x6/8o/irx68o0wo493tgj6o8xh0xj0x3o.png" alt="image"><br><br>  Pour le travailleur, le seul point d'entr√©e est l'API Deploy, qui impl√©mente des m√©thodes telles que ¬´Appliquer l'√©tat entier ou ses √©l√©ments individuels √† de tels serviteurs¬ª et ¬´Dire le statut de tel ou tel d√©ploiement¬ª.  L'API de d√©ploiement stocke des informations sur tous les d√©ploiements et ses √©tapes sp√©cifiques dans DeployDB, o√π nous utilisons √©galement PostgreSQL.  Des informations sur tous les serviteurs et ma√Ætres et sur l'appartenance du premier au second y sont √©galement stock√©es. <br><br>  Deux composants suppl√©mentaires sont install√©s sur les ma√Ætres de sel: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/develop/ref/netapi/all/salt.netapi.rest_cherrypy.html">API Salt REST</a> , avec laquelle l'API Deploy interagit pour lancer des d√©ploiements.  L'API REST va au ma√Ætre du sel local, et il communique d√©j√† avec les sbires en utilisant ZeroMQ. </li><li>  L'essentiel est qu'il va √† l'API Deploy et re√ßoive les cl√©s publiques de tous les serviteurs qui doivent √™tre connect√©s √† ce salt-master.  Sans cl√© publique sur le ma√Ætre, le serviteur ne peut tout simplement pas se connecter au ma√Ætre. </li></ul><br>  En plus du s√©baste, deux composants sont √©galement install√©s dans le plan de donn√©es: <br><br><ul><li>  <a href="https://docs.saltstack.com/en/latest/ref/returners/">Returner</a> - un module (l'une des parties extensibles de salt), qui apporte le r√©sultat du d√©ploiement non seulement au salt-master, mais aussi dans l'API Deploy.  L'API de d√©ploiement lance le d√©ploiement en acc√©dant √† l'API REST sur l'assistant et re√ßoit le r√©sultat via le retourneur du serviteur. </li><li>  Master pinger, qui interroge p√©riodiquement l'API Deploy √† laquelle les serviteurs principaux doivent √™tre connect√©s.  Si l'API Deploy renvoie une nouvelle adresse d'assistant (par exemple, car l'ancienne est morte ou surcharg√©e), pinger reconfigure le serviteur. </li></ul><br>  Un autre endroit o√π nous utilisons l'extensibilit√© SaltStack est <a href="https://docs.saltstack.com/en/latest/ref/pillar/all/index.html">ext_pillar</a> - la possibilit√© d'obtenir un <a href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html">pilier</a> de quelque part √† l'ext√©rieur (certaines informations statiques, par exemple, la configuration de PostgreSQL, les utilisateurs, les bases de donn√©es, les extensions, etc.).  Nous allons √† l'API interne de notre module pour obtenir des param√®tres sp√©cifiques au cluster, car ils sont stock√©s dans MetaDB. <br><br>  S√©par√©ment, nous notons que le pilier contient √©galement des informations confidentielles (mots de passe utilisateur, certificats TLS, cl√©s GPG pour le chiffrement des sauvegardes), et donc, premi√®rement, toutes les interactions entre tous les composants sont chiffr√©es (pas dans nos bases de donn√©es venir sans TLS, HTTPS partout, le serviteur et le ma√Ætre chiffrent √©galement tout le trafic).  Et deuxi√®mement, tous ces secrets sont crypt√©s dans MetaDB, et nous utilisons la s√©paration des secrets - sur les machines API internes, il y a une cl√© publique qui crypte tous les secrets avant d'√™tre stock√©s dans MetaDB, et la partie priv√©e est sur des ma√Ætres de sel et seuls ils peuvent obtenir secrets ouverts pour le transfert en tant que pilier √† un serviteur (√† nouveau via un canal crypt√©). <br><br><h3>  MDB Health </h3><br>  Lorsque vous travaillez avec des bases de donn√©es, il est utile de conna√Ætre leur statut.  Pour cela, nous avons le microservice MDB Health.  Il re√ßoit des informations sur l'√©tat de l'h√¥te du composant interne des MDB de la machine virtuelle MDB et les stocke dans sa propre base de donn√©es (dans ce cas, Redis).  Et lorsqu'une demande concernant le statut d'un cluster particulier arrive dans l'API interne, l'API interne utilise les donn√©es de MetaDB et MDB Health. <br><br><img width="500" src="https://habrastorage.org/webt/ee/su/qu/eesuquvflpn5k_frgbcqlakkfkc.png" alt="image"><br><br>  Les informations sur tous les h√¥tes sont trait√©es et pr√©sent√©es sous une forme compr√©hensible dans l'API.  Outre l'√©tat des h√¥tes et des clusters pour certains SGBD, MDB Health indique √©galement si un h√¥te particulier est un ma√Ætre ou une r√©plique. <br><br><h3>  DNS MDB </h3><br>  Le microservice DNS MDB est n√©cessaire pour g√©rer les enregistrements CNAME.  Si le pilote de connexion √† la base de donn√©es ne permet pas de transf√©rer plusieurs h√¥tes dans la cha√Æne de connexion, vous pouvez vous connecter √† un <a href="https://cloud.yandex.ru/docs/managed-mysql/operations/connect">CNAME</a> sp√©cial, qui indique toujours le ma√Ætre actuel dans le cluster.  Si le ma√Ætre change, le CNAME change. <br><br><img width="500" src="https://habrastorage.org/webt/2y/g2/sd/2yg2sd8z2cgvuy9spzxo00w3eoy.png" alt="image"><br><br>  Comment √ßa se passe?  Comme nous l'avons dit ci-dessus, √† l'int√©rieur de la machine virtuelle, il y a un cron MDB, qui envoie p√©riodiquement une pulsation du contenu suivant au DNS MDB: "Dans ce cluster, l'enregistrement CNAME doit pointer vers moi."  MDB DNS accepte ces messages de toutes les machines virtuelles et d√©cide de modifier les enregistrements CNAME.  Si n√©cessaire, il modifie l'enregistrement via l'API DNS. <br><br>  Pourquoi avons-nous cr√©√© un service distinct pour cela?  Parce que l'API DNS n'a le contr√¥le d'acc√®s qu'au niveau de la zone.  Un attaquant potentiel, acc√©dant √† une machine virtuelle distincte, pourrait modifier les enregistrements CNAME des autres utilisateurs.  MDB DNS exclut ce sc√©nario car il v√©rifie l'autorisation. <br><br><h3>  Livraison et affichage des journaux de base de donn√©es </h3><br>  Lorsque la base de donn√©es de la machine virtuelle √©crit dans le journal, le composant client push sp√©cial lit cet enregistrement et envoie la ligne qui vient d'appara√Ætre √† Logbroker ( <a href="https://habr.com/ru/company/yandex/blog/239823/">ils l'ont d√©j√† √©crit</a> sur Habr√©).  L'interaction du client push avec LogBroker est construite avec une s√©mantique exacte: nous l'enverrons certainement et nous en assurerons strictement une fois. <br><br>  Un pool distinct de machines - LogConsumers - prend les journaux de la file d'attente LogBroker et les stocke dans la base de donn√©es LogsDB.  Le SGBD ClickHouse est utilis√© pour la base de donn√©es des journaux. <br><br><img width="500" src="https://habrastorage.org/webt/w-/zc/mr/w-zcmrza7z9pg6gacszmpm0kojo.png" alt="image"><br><br>  Lorsqu'une demande est envoy√©e √† l'API interne pour afficher les journaux pendant un intervalle de temps sp√©cifique pour un cluster particulier, l'API interne v√©rifie l'autorisation et envoie la demande √† LogsDB.  Ainsi, la boucle de remise des journaux est compl√®tement ind√©pendante de la boucle d'affichage des journaux. <br><br><h3>  Facturation </h3><br>  Le sch√©ma de facturation est construit de mani√®re similaire.  √Ä l'int√©rieur de la machine virtuelle, il y a un composant qui v√©rifie avec une certaine p√©riodicit√© que tout est en ordre avec la base de donn√©es.  Si tout va bien, vous pouvez effectuer la facturation pour cet intervalle de temps d√®s le dernier lancement.  Dans ce cas, un enregistrement est effectu√© dans le journal de facturation, puis le client push envoie l'enregistrement √† LogBroker.  Les donn√©es de Logbroker sont transf√©r√©es vers le syst√®me de facturation et les calculs y sont effectu√©s.  Il s'agit d'un sch√©ma de facturation pour l'ex√©cution de clusters. <br><br>  Si le cluster est d√©sactiv√©, l'utilisation des ressources informatiques cesse d'√™tre factur√©e, cependant, l'espace disque est factur√©.  Dans ce cas, la facturation √† partir de la machine virtuelle est impossible et le deuxi√®me circuit est impliqu√© - le circuit de facturation hors ligne.  Il existe un pool distinct de machines qui ratissent la liste des clusters d'arr√™t de MetaDB et √©crivent un journal au m√™me format dans Logbroker. <br><br>  La facturation hors ligne peut √©galement √™tre utilis√©e pour la facturation et les clusters inclus, mais nous facturerons les h√¥tes, m√™me s'ils sont en cours d'ex√©cution, mais ils ne fonctionnent pas.  Par exemple, lorsque vous ajoutez un h√¥te √† un cluster, il se d√©ploie √† partir de la sauvegarde et est rattrap√© par la r√©plication.  Il est faux de facturer l'utilisateur pour cela, car l'h√¥te est inactif pendant cette p√©riode. <br><br><img width="500" src="https://habrastorage.org/webt/fa/gx/hw/fagxhwoaqe0fb5q60chfthefkv8.png" alt="image"><br><br><h3>  Sauvegarde </h3><br>  Le sch√©ma de sauvegarde peut diff√©rer l√©g√®rement pour diff√©rents SGBD, mais le principe g√©n√©ral est toujours le m√™me. <br><br>  Chaque moteur de base de donn√©es utilise son propre outil de sauvegarde.  Pour PostgreSQL et MySQL, il s'agit de <a href="https://github.com/wal-g/wal-g">WAL-G</a> .  Il cr√©e des sauvegardes, les compresse, les chiffre et les place dans <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> .  Dans le m√™me temps, chaque cluster est plac√© dans un compartiment s√©par√© (d'une part, pour l'isolement, et d'autre part, pour faciliter la sauvegarde de l'espace pour les sauvegardes) et est chiffr√© avec sa propre cl√© de chiffrement. <br><br><img width="500" src="https://habrastorage.org/webt/mf/dh/w0/mfdhw0bgyo59pytnhaqkaqghbou.png" alt="image"><br><br>  Voici comment fonctionnent le plan de contr√¥le et le plan de donn√©es.  De tout cela, le service de base de donn√©es g√©r√©e Yandex.Cloud est form√©. <br><br><h2>  Pourquoi tout est arrang√© de cette fa√ßon </h2><br>  Bien s√ªr, au niveau mondial, quelque chose pourrait √™tre mis en ≈ìuvre selon des sch√©mas plus simples.  Mais nous avions nos propres raisons de ne pas suivre la voie de la moindre r√©sistance. <br><br>  Tout d'abord, nous voulions avoir un plan de contr√¥le commun pour tous les types de SGBD.  Peu importe celui que vous choisissez, au final, votre demande parvient √† la m√™me API interne et tous les composants qui en font partie sont √©galement communs √† tous les SGBD.  Cela rend notre vie un peu plus compliqu√©e en termes de technologie.  D'un autre c√¥t√©, il est beaucoup plus facile d'introduire de nouvelles fonctionnalit√©s et capacit√©s qui affectent tous les SGBD.  Cela se fait une fois, pas six. <br><br>  Deuxi√®me moment important pour nous - nous voulions garantir autant que possible l'ind√©pendance du plan de donn√©es par rapport au plan de contr√¥le.  Et aujourd'hui, m√™me si Control Plane est compl√®tement indisponible, toutes les bases de donn√©es continueront de fonctionner.  Le service assurera leur fiabilit√© et leur disponibilit√©. <br><br>  Troisi√®mement, le d√©veloppement de presque tous les services est toujours un compromis.  D'une mani√®re g√©n√©rale, grosso modo, la vitesse de publication des versions et la fiabilit√© suppl√©mentaire sont quelque part plus importantes.  En m√™me temps, personne ne peut d√©sormais se permettre de faire une ou deux sorties par an, c'est √©vident.  Si vous regardez Control Plane, nous nous concentrons ici sur la vitesse de d√©veloppement, sur l'introduction rapide de nouvelles fonctionnalit√©s, en d√©ployant des mises √† jour plusieurs fois par semaine.  Et Data Plane est responsable de la s√©curit√© de vos bases de donn√©es, de la tol√©rance aux pannes, voici donc un cycle de version compl√®tement diff√©rent, mesur√© en semaines.  Et cette flexibilit√© en termes de d√©veloppement nous assure √©galement leur ind√©pendance mutuelle. <br><br>  Autre exemple: les services de base de donn√©es g√©n√©ralement g√©r√©s ne fournissent aux utilisateurs que des lecteurs r√©seau.  Yandex.Cloud propose √©galement des disques locaux.  La raison est simple: leur vitesse est beaucoup plus √©lev√©e.  Avec les lecteurs r√©seau, par exemple, il est plus facile de faire √©voluer la machine virtuelle de haut en bas.  Il est plus facile de faire des sauvegardes sous forme d'instantan√©s de stockage r√©seau.  Mais de nombreux utilisateurs ont besoin d'une vitesse √©lev√©e, nous augmentons donc les outils de sauvegarde. <br><br><h2>  Plans futurs </h2><br>  Et quelques mots sur les plans d'am√©lioration du service √† moyen terme.  Il s'agit de plans qui affectent l'ensemble des bases de donn√©es g√©r√©es Yandex dans leur ensemble, plut√¥t que les SGBD individuels. <br><br>  Tout d'abord, nous voulons donner plus de flexibilit√© dans le r√©glage de la fr√©quence de cr√©ation de sauvegarde.  Il existe des sc√©narios o√π il est n√©cessaire que pendant la journ√©e les sauvegardes soient effectu√©es toutes les quelques heures, pendant la semaine - une fois par jour, pendant le mois - une fois par semaine, pendant l'ann√©e - une fois par mois.  Pour ce faire, nous d√©veloppons un composant distinct entre l'API interne et <a href="https://cloud.yandex.ru/services/storage">Yandex Object Storage</a> . <br><br>  Un autre point important, important pour les utilisateurs et pour nous, est la vitesse des op√©rations.  Nous avons r√©cemment apport√© des modifications majeures √† l'infrastructure de d√©ploiement et r√©duit le temps d'ex√©cution de presque toutes les op√©rations √† quelques secondes.  Les op√©rations de cr√©ation d'un cluster et d'ajout d'un h√¥te au cluster n'√©taient pas couvertes.  Le temps d'ex√©cution de la deuxi√®me op√©ration d√©pend de la quantit√© de donn√©es.  Mais nous allons acc√©l√©rer le premier dans un avenir proche, car les utilisateurs souhaitent souvent cr√©er et supprimer des clusters dans leurs pipelines CI / CD. <br><br>  Notre liste de cas importants comprend l'ajout de la fonction d'augmentation automatique de la taille du disque.  Maintenant, cela se fait manuellement, ce qui n'est pas tr√®s pratique et pas tr√®s bon. <br><br>  Enfin, nous proposons aux utilisateurs un grand nombre de graphiques montrant ce qui se passe avec la base de donn√©es.  Nous donnons acc√®s aux journaux.  Dans le m√™me temps, nous constatons que les donn√©es sont parfois insuffisantes.  Besoin d'autres graphiques, d'autres tranches.  Ici, nous pr√©voyons √©galement des am√©liorations. <br><br>  Notre histoire sur le service de base de donn√©es g√©r√©e s'est av√©r√©e longue et probablement assez fastidieuse.  Mieux que tous les mots et descriptions, seulement une vraie pratique.  Par cons√©quent, si vous le souhaitez, vous pouvez √©valuer ind√©pendamment les capacit√©s de nos services: <br><br><ul><li>  <a href="https://cloud.yandex.ru/services/managed-postgresql">Service g√©r√© Yandex pour PostgreSQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mysql">Service g√©r√© Yandex pour MySQL</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-mongodb">Service g√©r√© Yandex pour MongoDB</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-clickhouse">Service g√©r√© Yandex pour ClickHouse</a> </li><li>  <a href="https://cloud.yandex.ru/services/managed-redis">Service g√©r√© Yandex pour Redis</a> </li><li>  <a href="https://cloud.yandex.ru/services/data-proc">Yandex Data Proc</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr477860/">https://habr.com/ru/post/fr477860/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr477850/index.html">React Native - une solution miracle pour tous les probl√®mes? Comment nous avons choisi un outil multiplateforme pour Profi.ru</a></li>
<li><a href="../fr477852/index.html">Hypocrisie non toxique</a></li>
<li><a href="../fr477854/index.html">Que se passe-t-il lors de la connexion √† l'int√©rieur et √† l'ext√©rieur d'un tunnel VPN</a></li>
<li><a href="../fr477856/index.html">Acc√©l√©rateurs flash PCI-E de 800 Go √† 6,4 To: de l'aube √† la vie sur un PC / serveur classique</a></li>
<li><a href="../fr477858/index.html">Travail hors table: quels projets sont vraiment apparus apr√®s la pr√©-acc√©l√©ration?</a></li>
<li><a href="../fr477864/index.html">TabPy pour travailler avec des donn√©es dans ClickHouse de Tableau</a></li>
<li><a href="../fr477870/index.html">Tableau de bord Grafana pour syst√®me de bi√®re BeerTender</a></li>
<li><a href="../fr477874/index.html">CDN dynamique pour le streaming WebRTC avec faible latence et transcodage</a></li>
<li><a href="../fr477878/index.html">Comment les jeux comp√©titifs vous aident √† mieux travailler</a></li>
<li><a href="../fr477882/index.html">40 canaux et chats pour les personnes int√©ress√©es par DevOps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>