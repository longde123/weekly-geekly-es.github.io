<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò• ü§• üôèüèæ Meta-clustering con minimizaci√≥n de errores, y por qu√© creo que el cerebro funciona de esta manera ‚ùå üòª üèê</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hola a todos! Quiero compartir con ustedes mi idea del aprendizaje autom√°tico. 

 Los grandes avances en el aprendizaje autom√°tico son impresionantes....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Meta-clustering con minimizaci√≥n de errores, y por qu√© creo que el cerebro funciona de esta manera</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/427407/">  Hola a todos!  Quiero compartir con ustedes mi idea del aprendizaje autom√°tico. <br><br>  Los grandes avances en el aprendizaje autom√°tico son impresionantes.  Las redes convolucionales y los LSTM son geniales.  Pero casi todas las tecnolog√≠as modernas se basan en la propagaci√≥n inversa del error.  Basado en este m√©todo, es poco probable que pueda construir una m√°quina de pensar.  Las redes neuronales est√°n formadas por algo as√≠ como un cerebro congelado, entrenado de una vez por todas, incapaz de <strike>cambiar el</strike> pensamiento. <br><br>  Pens√©, ¬øpor qu√© no tratar de crear algo como un cerebro vivo?  Una especie de reingenier√≠a.  Dado que en todos los animales, a pesar de las diferencias de inteligencia, el cerebro consta de aproximadamente las mismas neuronas, algunos principios b√°sicos deber√≠an estar en el centro de su trabajo. <br><a name="habracut"></a><br><h2>  Lo que no s√© sobre las neuronas. </h2><br>  Hay varias preguntas para las cuales no he encontrado respuestas inequ√≠vocas en la literatura popular; <br><br><ul><li>  Obviamente, una neurona de alguna manera responde a los neurotransmisores, pero ¬øc√≥mo exactamente?  La simple suposici√≥n de que cuanto mayor es el neurotransmisor, con mayor frecuencia se adhiere, obviamente no resiste las cr√≠ticas.  Si esto fuera as√≠, la activaci√≥n de una neurona desencadenar√≠a la activaci√≥n de varios vecinos, los de la siguiente, y en poco tiempo esta avalancha capturar√≠a todo el cerebro.  Pero en realidad esto no sucede, al mismo tiempo, solo una peque√±a parte de las neuronas trabaja en el cerebro.  Por qu√© </li><li>  Las neuronas son obviamente unidades de memoria, pero ¬øc√≥mo almacenan la informaci√≥n?  La parte central de la neurona no es nada especial: el n√∫cleo de las mitocondrias y similares.  Axon no puede influir en el pico, porque la informaci√≥n va solo en una direcci√≥n, desde el n√∫cleo.  Entonces, lo √∫nico que queda son las dendritas.  Pero, ¬øc√≥mo se almacena la informaci√≥n en ellos?  En forma anal√≥gica o digital? </li><li>  Obviamente, las neuronas de alguna manera est√°n aprendiendo.  ¬øPero c√≥mo exactamente?  Suponga que las dendritas crecen en lugares donde hab√≠a muchos neurotransmisores justo antes de la espiga.  Pero si esto es as√≠, la neurona activada crecer√° un poco y la pr√≥xima vez que aparezca un neurotransmisor, ser√° el m√°s grueso entre los vecinos, absorber√° la mayor parte del neurotransmisor y funcionar√° nuevamente.  Y de nuevo un poco crecer.  ¬øY as√≠ hasta el infinito, hasta que estrangula a todos sus vecinos?  ¬øHay algo mal aqu√≠? </li><li>  Si una neurona crece, entonces las vecinas deber√≠an disminuir, la cabeza no es de goma.  Algo deber√≠a hacer que la neurona se seque.  Que? </li></ul><br><h2>  Solo agrupamiento </h2><br>  La respuesta plausible a todas estas preguntas me parece que el cerebro funciona como muchos grupos simples.  ¬øEs posible ejecutar dicho algoritmo en un grupo de neuronas?  Por ejemplo, el m√©todo K-means.  Simplemente necesito simplificarlo un poco.  En el algoritmo cl√°sico, los centros se calculan iterativamente como el promedio de todos los ejemplos considerados, pero cambiaremos el centro inmediatamente despu√©s de cada ejemplo. <br><br>  Veamos qu√© necesitamos para implementar el algoritmo de agrupamiento. <br><br><ul><li>  Los centros de grupos, por supuesto, son las dendritas de las neuronas de nuestro grupo.  ¬øPero c√≥mo recordar la informaci√≥n?  Suponga que la celda unitaria para almacenar informaci√≥n en la dendrita es el volumen de la rama de dendrita en la regi√≥n de sinapsis.  Cuanto m√°s gruesa es la rama, respectivamente, su volumen es mayor, mayor se guarda el valor.  Por lo tanto, cada dendrita puede memorizar varias cantidades anal√≥gicas. </li><li>  Comparadores para calcular la proximidad de un ejemplo.  Es mas complicado.  Supongamos que despu√©s de que se hayan enviado los datos (los axones expulsaron un neurotransmisor), cada neurona funcionar√° m√°s r√°pido, mientras m√°s datos almacenados (el centro del grupo) sean similares al ejemplo dado (el n√∫mero de neurotransmisores).  Tenga en cuenta que la tasa de respuesta de una neurona no se ve afectada por la cantidad absoluta del neurotransmisor, sino por la proximidad de la cantidad del neurotransmisor al valor almacenado en las dendritas.  Supongamos que si el neurotransmisor es peque√±o, entonces la dendrita no da una orden para aumentar.  No sucede nada y si hay muchos neurotransmisores, el pico de la rama dendr√≠tica ocurre antes que en otras ramas dendr√≠ticas y no llega al n√∫cleo.  Pero si el neurotransmisor es el correcto, entonces todas las ramas dendr√≠ticas dar√°n un mini pico aproximadamente al mismo tiempo, y esta onda se convertir√° en un pico de una neurona que ir√° a lo largo del ax√≥n. </li><li>  Un comparador de entradas m√∫ltiples le permite comparar resultados y elegir el mejor.  Supongamos que las neuronas cercanas tienen un efecto inhibitorio sobre todos sus vecinos.  Entonces, en un cierto grupo de neuronas, solo una puede estar activa en cualquier momento.  El que funcion√≥ primero.  Como las neuronas del grupo est√°n cerca, tienen el mismo acceso a todos los axones que vienen a este grupo.  Por lo tanto, la neurona en la que la informaci√≥n almacenada est√° m√°s cerca del ejemplo en cuesti√≥n funcionar√° en el grupo. </li><li>  El mecanismo de desplazamiento del centro hacia el ejemplo.  Bueno, todo es simple.  Despu√©s del pico neuronal, todas las dendritas de esta neurona cambian su volumen.  Donde la concentraci√≥n del neurotransmisor era demasiado alta, las ramas crecen.  Donde era insuficiente, las ramitas se reducen.  Donde la concentraci√≥n es correcta, el volumen no cambia.  Los vol√∫menes de ramitas var√≠an un poco.  Pero de inmediato.  El siguiente pico es el pr√≥ximo cambio. </li></ul><br>  Verifiquemos el algoritmo resultante en la pr√°ctica.  Dibuj√© algunas l√≠neas en Python.  Esto es lo que sucede con dos dimensiones de n√∫meros aleatorios: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a4/5b2/a12/2a45b2a122a9536cf95e3768af65d4f2.gif"></div><br>  Y aqu√≠ est√° MNIST: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ceb/6a5/61a/ceb6a561a711c640904a74d1443fa706.gif"></div><br>  A primera vista, parece que todo lo anterior no ha cambiado nada.  Bueno, ten√≠amos algunos datos en la entrada, de alguna manera los transformamos, obtuvimos otros datos. <br><br>  Pero realmente hay una diferencia.  Si antes de la conversi√≥n ten√≠amos un mont√≥n de par√°metros anal√≥gicos, despu√©s de la conversi√≥n solo ten√≠amos un par√°metro, al mismo tiempo codificado por un c√≥digo unitario.  Cada neurona del grupo puede asociarse con una acci√≥n espec√≠fica. <br><br>  Perm√≠tanme dar un ejemplo: supongamos que solo hay dos neuronas en un grupo de agrupaci√≥n.  Ll√°malos "sabroso" y "miedo".  Para permitir que el cerebro tome una decisi√≥n, solo es necesario conectar la neurona "EAT" a la primera y "RUN" a la segunda.  Para esto necesitamos un maestro.  Pero ahora no se trata de eso, ense√±ar con un maestro es un tema para otro art√≠culo. <br><br>  Si aumenta el n√∫mero de grupos, la precisi√≥n aumentar√° gradualmente.  Un caso extremo es el n√∫mero de cl√∫steres igual al n√∫mero de ejemplos.  Pero hay un problema, la cantidad de neuronas en el cerebro es limitada.  Uno debe comprometerse constantemente, ya sea la precisi√≥n o el tama√±o del cerebro. <br><br><h2>  Meta agrupamiento </h2><br>  Supongamos que no tenemos un grupo de agrupaci√≥n, sino dos.  En este caso, se aplican los mismos valores a las entradas.  Obviamente, obtienes el mismo resultado. <br><br>  Hagamos un peque√±o error al azar.  Deje que, a veces, cada cl√∫ster seleccione no el centro m√°s cercano del cl√∫ster, sino cu√°l.  Entonces los valores comenzar√°n a diferir, con el tiempo la diferencia se acumular√°. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/844/742/6ce/8447426cea375229e23546b4be16ee1b.gif"></div><br>  Y ahora, calculemos el error de cada clusterer.  El error es la diferencia entre el ejemplo de entrada y el centro del cl√∫ster seleccionado.  Si un cl√∫ster seleccion√≥ el valor m√°s cercano y el otro al azar, entonces el segundo tendr√° un error mayor. <br><br>  Adelante, agregue una m√°scara a la entrada de cada clusterer.  Una m√°scara es un conjunto de coeficientes para cada entrada.  No cero o uno, como se usa com√∫nmente en las m√°scaras, sino alg√∫n n√∫mero real de cero a uno. <br><br>  Antes de dar un ejemplo a la entrada del clusterer, multiplicaremos este ejemplo por una m√°scara.  Por ejemplo, si se usa una m√°scara para una imagen, si para alg√∫n p√≠xel la m√°scara es igual a uno, entonces es como si fuera completamente transparente.  Y si la m√°scara es cero, entonces este p√≠xel siempre es negro.  Y si la m√°scara es 1/2, entonces el p√≠xel est√° medio oscurecido. <br><br>  Y ahora, la acci√≥n principal, reduciremos el valor de la m√°scara en proporci√≥n al error de agrupamiento.  Es decir, si el error es grande, entonces disminuiremos el valor m√°s fuertemente, y si es cero, no lo reduciremos en absoluto. <br><br>  Para que los valores de las m√°scaras no se restablezcan gradualmente a cero, los normalizaremos.  Es decir, la suma de los valores de m√°scara para cada par√°metro de entrada siempre es igual a uno.  Si se quita algo en una m√°scara, se agrega a otra. <br><br>  Tratemos de ver qu√© sucede con el ejemplo de MNIST.  Vemos que las m√°scaras dividen gradualmente los p√≠xeles en dos partes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/39c/204/a45/39c204a45731b8408b14cb9621cd91b0.gif"></div><br>  Las m√°scaras resultantes se muestran en el lado derecho de la imagen.  Al final del proceso, el cl√∫ster superior considera el inferior derecho y el cl√∫ster inferior el resto de los ejemplos.  Curiosamente, si reiniciamos el proceso, obtendremos otra separaci√≥n.  Pero al mismo tiempo, los grupos de par√°metros se obtienen no al azar, sino de tal manera que se reduce el error de predicci√≥n.  Los cl√∫steres parecen probar cada p√≠xel en su m√°scara, y al mismo tiempo, el p√≠xel recoge el cl√∫ster al que el p√≠xel se adapta mejor. <br><br>  Intentemos ingresar d√≠gitos dobles, no superpuestos entre s√≠, sino ubicados uno al lado del otro, de esta manera (este es un ejemplo, no dos): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9ee/675/145/9ee6751458d904e22d9d286e704084eb.jpg" alt="imagen"></div><br>  Ahora vemos que cada vez, la separaci√≥n ocurre igual.  Es decir, si hay una sola opci√≥n, claramente la mejor opci√≥n para separar las m√°scaras, se seleccionar√°. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/48b/55c/487/48b55c4877b5938ed22f5473bb4fb033.gif"></div><br>  Solo una cosa ser√° aleatoria, ya sea que la primera m√°scara seleccione el d√≠gito izquierdo o el derecho. <br><br>  Llamo a las m√°scaras resultantes meta-clusters.  Y el proceso de formar m√°scaras por meta-clustering.  ¬øPor qu√© meta?  Porque la agrupaci√≥n no es de ejemplos de entrada, sino de las entradas mismas. <br><br>  Un ejemplo es m√°s complicado.  Intentemos dividir 25 par√°metros en 5 meta clusters. <br><br>  Para hacer esto, tomamos cinco grupos de cinco par√°metros codificados por un c√≥digo unitario. <br><br>  Es decir, en cada grupo hay una y solo una unidad en un lugar aleatorio.  Siempre hay cinco unidades en cada ejemplo servido. <br><br>  En las im√°genes a continuaci√≥n, cada columna es un par√°metro de entrada y cada fila es una m√°scara de meta-cluster.  Los grupos en s√≠ no se muestran. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95e/a47/42b/95ea4742b10e16dac89d9a27dc02822a.gif"></div><br>  100 par√°metros y 10 meta clusters: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/55a/632/b55/55a632b5503d6b4c439c8905d1bdf46b.gif"></div><br>  Funciona!  En algunos lugares, incluso se parece un poco a una imagen de una matriz de la pel√≠cula del mismo nombre. <br><br>  El uso de meta-clustering puede reducir dr√°sticamente la cantidad de clusters. <br><br>  Por ejemplo, tome diez grupos de diez par√°metros, cada grupo tiene una unidad. <br><br>  Si tenemos un cl√∫ster (sin meta-cl√∫steres), entonces necesitamos 10 <sup>10</sup> = 10000000000 cl√∫steres para obtener un error cero. <br><br>  Y si tenemos diez grupos, entonces solo necesitamos 10 * 10 = 100 grupos.  Esto es similar al sistema de n√∫meros decimales, no necesita encontrar una notaci√≥n para todos los n√∫meros posibles, puede hacerlo con diez d√≠gitos. <br><br>  La agrupaci√≥n meta est√° muy bien paralelizada.  Los c√°lculos m√°s caros (comparando el ejemplo con el centro del grupo) se pueden realizar de forma independiente para cada grupo.  Tenga en cuenta, no para la agrupaci√≥n, sino para la agrupaci√≥n. <br><br><h2>  ¬øC√≥mo funciona en el cerebro? </h2><br>  Antes de eso, solo hablaba de dendritas, pero las neuronas tienen axones.  Y ellos tambi√©n estudian.  Por lo tanto, es muy probable que los axones sean las m√°scaras de los grupos de meta. <br><br>  Agregamos una funci√≥n m√°s a la descripci√≥n de la operaci√≥n dendr√≠tica anterior. <br><br>  Suponga que si se produce un pico neuronal, todas las dendritas de alguna manera emiten en la sinapsis alg√∫n tipo de sustancia que muestra la concentraci√≥n del neurotransmisor en la dendrita.  No del ax√≥n a la dendrita, sino de regreso.  La concentraci√≥n de esta sustancia depende del error de comparaci√≥n.  Supongamos que cuanto menor es el error, mayor es la cantidad de sustancia emitida.  Bueno, el ax√≥n reacciona a la cantidad de esta sustancia y crece.  Y si la sustancia es peque√±a, lo que significa un gran error, entonces el ax√≥n se reduce gradualmente. <br><br>  Y si cambia los axones desde el nacimiento del cerebro, con el tiempo, ir√°n solo a aquellos grupos de neuronas donde se necesitan sus adherencias a estos axones (no conducen a grandes errores). <br><br>  Ejemplo: recordemos rostros humanos.  Deje que cada cara se represente con una imagen de megap√≠xeles.  Entonces, para cada cara, necesitas una neurona con un mill√≥n de dendritas, lo que no es realista.  Ahora, divida todos los p√≠xeles en meta racimos, como ojos, nariz, orejas, etc.  Solo diez de esos meta clusters.  Deje que haya diez grupos, diez opciones de nariz, diez opciones de oreja, etc. para cada meta-grupo.  Ahora, para recordar la cara, una neurona con diez dendritas es suficiente.  Esto reduce la memoria (y el volumen del cerebro) en cinco √≥rdenes de magnitud. <br><br><h2>  Conclusi√≥n </h2><br>  Y ahora, si asumimos que el cerebro consiste en meta-racimos, podemos tratar de considerar desde este punto de vista algunos conceptos inherentes al cerebro vivo: <br><br>  Los cl√∫steres deben recibir capacitaci√≥n constante, de lo contrario, los datos nuevos no se procesar√°n correctamente.  Para entrenar grupos en el cerebro, se necesita una muestra equilibrada.  Perm√≠tanme explicar si el invierno es ahora, entonces el cerebro aprender√° solo de los ejemplos de invierno, y los grupos resultantes gradualmente se volver√°n relevantes solo para el invierno, y en el verano todo ser√° malo para este cerebro.  ¬øQu√© hacer al respecto?  Es necesario presentar peri√≥dicamente a todos los grupos no solo nuevos, sino tambi√©n ejemplos importantes antiguos (recuerdos tanto de invierno como de verano).  Y para que estos sentimientos no interfieran con los sentimientos actuales, debe desactivarlos temporalmente.  En animales, esto se llama un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">sue√±o</a> . <br><br>  Imagina, el cerebro ve algo peque√±o, GRIS, que corre.  Despu√©s del meta-agrupamiento, tenemos tres neuronas activas en tres meta-agrupamientos.  Y gracias a la memoria, el cerebro sabe que est√° delicioso.  Entonces, el cerebro ve algo peque√±o, AZUL que corre.  Pero el cerebro no sabe si es sabroso o aterrador.  Es suficiente deshabilitar temporalmente el meta cl√∫ster donde se encuentran los colores, y solo permanecer√° el peque√±o que se ejecuta.  Y el cerebro sabe que est√° delicioso.  Esto se llama <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">una analog√≠a</a> . <br><br>  Supongamos que el cerebro recuerda algo y luego cambia el grupo de neuronas activas de un grupo a otro, mientras que en los meta-grupos restantes hay una memoria real.  Y ahora, el cerebro ya ha introducido algo que nunca antes se hab√≠a visto.  Y esto ya es una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">imaginaci√≥n</a> . <br><br>  Gracias por su atenci√≥n, el c√≥digo est√° <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">aqu√≠</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es427407/">https://habr.com/ru/post/es427407/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es427397/index.html">Desarrollo de un conjunto de datos ac√∫sticos para entrenar una red neuronal.</a></li>
<li><a href="../es427399/index.html">Trabajar con datos al construir una API basada en GraphQL</a></li>
<li><a href="../es427401/index.html">Shaders de disoluci√≥n y exploraci√≥n mundial</a></li>
<li><a href="../es427403/index.html">ReportingObserver API: una mirada al c√≥digo de las p√°ginas web desde una nueva perspectiva</a></li>
<li><a href="../es427405/index.html">ES2018 - finalmente promete m√©todo</a></li>
<li><a href="../es427409/index.html">El libro "El √°gil brillante. Gesti√≥n de proyectos flexible con Agile, Scrum y Kanban ¬ª</a></li>
<li><a href="../es427413/index.html">Luchando por los recursos, parte 4: genial</a></li>
<li><a href="../es427415/index.html">Usamos Node.js para trabajar con archivos grandes y conjuntos de datos sin procesar.</a></li>
<li><a href="../es427417/index.html">Con humor sobre los disquetes de 8 pulgadas (en los a√±os 70 solo hab√≠a tales)</a></li>
<li><a href="../es427419/index.html">¬øQu√© hacer cuando el procesador no tiene nada que hacer?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>