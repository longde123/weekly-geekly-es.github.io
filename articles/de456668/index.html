<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏽‍🤝‍👨🏻 🧙 🤥 Microsoft ML Spark: Eine Spark-Erweiterung, die SparkML humaner und LightGBM als Bonus macht 👾 👑 🏠</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Viele, die mit Spark ML gearbeitet haben, wissen, dass einige der Dinge, die sie dort getan haben, "nicht ganz erfolgreich" sind. 
 oder gar nicht gem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Microsoft ML Spark: Eine Spark-Erweiterung, die SparkML humaner und LightGBM als Bonus macht</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raiffeisenbank/blog/456668/"><p>  Viele, die mit Spark ML gearbeitet haben, wissen, dass einige der Dinge, die sie dort getan haben, "nicht ganz erfolgreich" sind. <br>  oder gar nicht gemacht.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Position der Spark-Entwickler ist,</a> dass SparkML die Basisplattform ist und alle Erweiterungen separate Pakete sein müssen.  Dies ist jedoch nicht immer praktisch, da Data Scientist und Analysten mit vertrauten Tools (Jupter, Zeppelin) arbeiten möchten, bei denen das meiste benötigt wird.  Sie möchten keine 500-Megabyte-JAR-Dateien mit Maven-Assembly sammeln oder Abhängigkeiten in ihre Hände herunterladen und zu den Spark-Startparametern hinzufügen.  Eine genauere Arbeit mit Build-Systemen für JVM-Projekte erfordert möglicherweise viele zusätzliche Anstrengungen von Analysten und DataScientists, die an Jupyter / Zeppelin gewöhnt sind.  Es ist eindeutig eine schlechte Idee, DevOps und Clusteradministratoren zu bitten, eine Reihe von Paketen auf Rechenknoten zu platzieren.  Jeder, der selbst Erweiterungen für SparkML geschrieben hat, weiß, wie viele versteckte Schwierigkeiten es mit wichtigen Klassen und Methoden gibt (die aus irgendeinem Grund privat sind [ml]), Einschränkungen bei den Arten der gespeicherten Parameter usw. </p><br><p>  Und mit der MMLSpark-Bibliothek scheint das Leben jetzt etwas einfacher zu sein, und der Schwellenwert für den Einstieg in skalierbares maschinelles Lernen mit SparkML und Scala ist etwas niedriger. </p><a name="habracut"></a><br><h2 id="vvedenie">  Einführung </h2><br><p>  Aufgrund einer Reihe von Schwierigkeiten sowie einer geringen Anzahl vorgefertigter Methoden und Lösungen in SparkML schreiben viele Unternehmen ihre Erweiterungen für Spark.  Ein Beispiel ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> , das bei Odnoklassniki entwickelt wird und nach einer schnellen Einschätzung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalte</a> von GitHub sehr vielversprechend aussieht.  Leider sind die meisten dieser Lösungen entweder geschlossen oder offen, aber sie können nicht über Maven / sbt und die API-Dokumentation installiert werden, was die Arbeit mit ihnen sehr schwierig macht. </p><br><p>  Heute schauen wir uns die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MMLSpark-</a> Bibliothek an. </p><br><p>  Wir werden wie üblich das Beispiel der Aufgabe betrachten, Passagiere der Titanic zu klassifizieren.  Ziel ist es, möglichst viele Funktionen der MMLSpark-Bibliothek anzuzeigen, nicht <del>  Schalte SOTA auf ImageNet aus </del>  zeige cooles maschinelles Lernen.  Also wird die Titanic reichen. </p><br><p><img src="https://habrastorage.org/webt/rg/bt/sf/rgbtsf7j0ovmfa5lpjkfpfapzti.jpeg"></p><br><p>  Die Bibliothek selbst verfügt über eine native API für Scala ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> ), eine Python-API ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> ) und wird nach einigen Stellen im GitHub-Repository bald eine API für R haben. </p><br><p>  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gute Beispiel-Laptops im</a> GitHub-Projekt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(PySpark + Jupyter)</a> , aber wir werden den anderen Weg gehen.  Wie Dmitry Bugaychenko <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schrieb</a> , haben Sie bei der Entwicklung für Spark allen Grund, Scala dafür zu verwenden. Darüber hinaus können Sie mit Scala Ihren eigenen Transformer und Estimator viel effizienter und flexibler definieren, um sie in die SparkML-Pipeline einzubetten, aber wie langsam numpy funktioniert / pandas Code in UDF (auf ausführbare Dateien von der JVM aufgerufen) wurde bereits viel geschrieben. </p><br><h2 id="kratko-ob-ustanovke">  Installationsbeschreibung </h2><br><p>  Der gesamte Laptop ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> verfügbar.  Für die Arbeit mit der Titanic reicht das lokal auf einem Laptop mit Standardeinstellungen ausgeführte Zeppelin Docker-Image für die Augen aus.  Docker finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> .  Die MMLSpark-Bibliothek befindet sich nicht in Maven Central, sondern in Spark-Paketen. Um sie zu Zeppelin hinzuzufügen, müssen Sie den folgenden Block am Anfang des Laptops ausführen: </p><br><pre><code class="plaintext hljs">%spark.dep z.addRepo("bintray.com").url("http://dl.bintray.com/spark-packages/maven/") z.load("Azure:mmlspark:0.17")</code> </pre> <br><p>  Es ist erwähnenswert, dass die Bibliothek eine hervorragende Abwärtskompatibilität aufweist: Im Gegensatz zum XGBoost4j-spark, für den mindestens Spark 2.3+ erforderlich ist, wurde dieses Element in Spark 2.2.1 integriert, das mit dem Zeppelin Docker-Image geliefert wurde, und es gab keine Schwierigkeiten Ich habe es nicht bemerkt. </p><br><p>  <strong>Hinweis: Der</strong> größte Teil der MMLSpark-Bibliothek ist der Inferenz von Gittern in einem Cluster gewidmet, für die CNTK vorhanden ist (das laut Dokumentation vorgefertigte cntk-Modelle lesen sollte) und einem riesigen OpenCV-Block.  Wir werden uns auf alltäglichere Aufgaben konzentrieren und versuchen, den Fall zu „simulieren“, wenn wir riesige Arrays tabellarischer Daten haben, die in HDFS in Form von CSV, Tabellen oder in einem anderen Format vorliegen.  Wir müssen sie also vorverarbeiten und ein Modell erstellen, während diese Daten nicht in den Speicher einer Maschine passen.  Daher führen wir alle Aktionen im Cluster aus. </p><br><h2 id="chtenie-i-razvedochnyy-analiz">  Lesen und Intelligenzanalyse </h2><br><p>  Im Allgemeinen ist Spark + Zeppelin überhaupt nicht schlecht und kann die EDA-Aufgabe bewältigen, aber wir werden versuchen, ihre Fähigkeiten zu erweitern.  Zuerst importieren wir die Klassen, die wir brauchen: </p><br><ul><li>  Alles von spark.sql.types, um ein Schema zu deklarieren und die Daten korrekt zu lesen </li><li>  Alles von spark.sql.functions, um auf Spalten zuzugreifen und integrierte Funktionen zu verwenden </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">com.microsoft.ml.spark.SummarizeData</a> , das als Analogon zu pandas.DataFrame.describe bezeichnet werden kann </li></ul><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">SummarizeData</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.functions._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.types._</code> </pre> <br><p>  Wir lesen unsere Datei: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> titanicSchema = <span class="hljs-type"><span class="hljs-type">StructType</span></span>( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Passanger"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Ticket"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Fare"</span></span>, <span class="hljs-type"><span class="hljs-type">FloatType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">Nil</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = spark .read .schema(titanicSchema) .option(<span class="hljs-string"><span class="hljs-string">"header"</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) .csv(<span class="hljs-string"><span class="hljs-string">"/mountV/titanic/train.csv"</span></span>)</code> </pre> <br><p>  Und jetzt schauen wir uns die Daten selbst sowie ihre Größe an: </p><br><pre> <code class="scala hljs">println(<span class="hljs-string"><span class="hljs-string">s"Train shape is: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.count}</span></span></span><span class="hljs-string"> x </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.columns.length}</span></span></span><span class="hljs-string">"</span></span>) train.limit(<span class="hljs-number"><span class="hljs-number">5</span></span>).createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"trainHead"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> Es ist wirklich nicht erforderlich, createOrReplaceTempView zu verwenden, wenn Sie nur .show (5) schreiben können.  Aber show hat ein Problem: Wenn die Daten "breit" sind, "schwebt" die Textdarstellung der Platte, und es wird überhaupt nichts klar. </p><br><p>  Holen Sie sich die Größe unserer Daten: <code>Train shape is: 891 x 12</code> <br>  Und jetzt können wir uns in der SQL-Zelle die ersten 5 Zeilen ansehen: </p><br><pre> <code class="sql hljs">%sql <span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> trainHead</code> </pre> <br><p><img src="https://habrastorage.org/webt/qe/jc/wj/qejcwjvi65jp6xucnorcdfdmxo4.png"></p><br><p>  Nun, sehen wir uns die Zusammenfassung auf unserem Tisch an: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SummarizeData</span></span>() .setBasic(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCounts(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setPercentiles(<span class="hljs-literal"><span class="hljs-literal">false</span></span>) .setSample(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setErrorThreshold(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) .transform(train) .createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"summary"</span></span>)</code> </pre> <br><p>  Die SummarizeData-Klasse bietet gegenüber dem einfachen Dataset.describe mehrere Vorteile, da Sie die Anzahl fehlender und eindeutiger Werte zählen und die Genauigkeit der Berechnung von Quantilen festlegen können.  Dies kann für wirklich große Datenmengen von entscheidender Bedeutung sein. <br><img src="https://habrastorage.org/webt/91/g4/8c/91g48c0oaqiuz8pjgklo38jaiis.png"></p><br><div class="spoiler">  <b class="spoiler_title">Einige persönliche Gedanken</b> <div class="spoiler_text"><p>  Im Allgemeinen schien es mir persönlich, dass Odnoklassniki in PravdaML eine bessere Implementierung des SummarizeData-Analogons hatte.  Microsoft ist den einfachen Weg <code>org.apache.spark.sql.functions</code> und verwendet <code>org.apache.spark.sql.functions</code> . Es ist nur so, dass alles bequem in einer einzigen Klasse zusammengefasst ist.  Für Odnoklassniki wird dies über den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>VectorStatCollector</code></a> implementiert, der beim Aufrufen etwas komplexeren Code erfordert (Sie müssen zuerst alle Features in einen Vektor <code>VectorAssembler</code> ) und möglicherweise zusätzliche Operationen erfordert (z. B. weigert sich <code>VectorAssembler</code> normalerweise, <code>VectorAssembler</code> zu <code>DecimalType</code> ).  Aufgrund meiner Erfahrungen mit Spark gehe ich jedoch davon aus, dass SummarizeData aus MMLSpark mit Fehlern wie <code>StackOverflow</code> in <code>org.apache.spark.sql.catalyst</code> wenn <strong>wirklich viele</strong> Spalten vorhanden sind und das Berechnungsdiagramm zum Zeitpunkt des Starts nicht klein ist ( Obwohl speziell für solche Fans von "Extrem" in Spark 2.4 die Möglichkeit hinzugefügt wurde, den <code>Catalyst</code> Graph Optimizer zu reduzieren.  Nun, es scheint, dass mit einer <strong>wirklich großen</strong> Anzahl von Spalten die Version von Microsoft langsamer sein wird.  Dies muss aber natürlich separat geprüft werden. </p></div></div><br><h2 id="ochistka-dannyh">  Datenbereinigung </h2><br><p>  In der Titanic ist alles wie gewohnt - in einer Reihe von Zeichenfolgenspalten fehlen Werte.  Und eine Art Überhöhung in den Daten (es scheint, dass diese bestimmte Version der Daten nicht sehr spezifisch ist) - 25 Zeilen von den fehlenden Werten entfernt.  Beheben Sie zunächst Folgendes: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trainFiltered = train.filter(!(isnan(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>)) || isnull(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>))))</code> </pre> <br><h3 id="obrabotka-strokovyh-dannyh">  String-Datenverarbeitung </h3><br><p>  Soweit ich mich erinnere, waren die Attribute, die aus den Feldern <code>Name</code> und <code>Cabin</code> stammen, die besten, die in der Titanic hervorgebracht wurden.  Sie können sie viel liefern, aber wir beschränken uns auf einige wenige, um keine Beispiele für fast denselben Code zu nennen. </p><br><p>  Normalerweise ist es zweckmäßig, für solche Dinge reguläre Ausdrücke zu verwenden. <br>  Aber wir wollen in diesem Fall: </p><br><ul><li>  alles wurde verteilt, die Daten wurden am selben Ort verarbeitet wie sie waren; </li><li>  Alles wurde als SpakrML Transformer- oder Spark ML Estimator-Klassen konzipiert, damit es später in Pipeline zusammengestellt werden kann. </li></ul><br><p>  <strong>Hinweis: Die</strong> Pipeline garantiert uns erstens, dass wir immer die gleichen Transformationen sowohl auf den Zug als auch auf den Test anwenden, und ermöglicht es uns, den Fehler des "Blicks in die Zukunft" bei der Kreuzvalidierung zu erkennen.  Darüber hinaus erhalten wir einfache Funktionen zum Speichern, Laden und Vorhersagen mithilfe unserer Pipeline. </p><br><p>  SparkML hat eine „fast universelle“ Klasse für solche Aufgaben - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SQLTranformer</a> , aber das Schreiben in SQL ist eindeutig schlechter als das Schreiben in Scala, schon allein deshalb, weil es möglich ist, Syntax oder typische Fehler beim Kompilieren und Hervorheben der Syntax in Idea zu erkennen.  Und hier hilft uns MMLSpark, wo ein wirklich universeller <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UDFTransformer implementiert wird</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span></code> </pre> <br><p>  Zunächst werden wir unsere Transformationsfunktion erstellen, die bis zum Limit sehr einfach ist. Unser Ziel ist es nun, den Prozess der Erstellung von UDFTransformer aufzuzeigen.  Im Prinzip kann jeder anhand solcher einfachen Beispiele jeder Komplexitätsebene Logik hinzufügen. </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> miss = <span class="hljs-string"><span class="hljs-string">".*miss\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mr = <span class="hljs-string"><span class="hljs-string">".*mr\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mrs = <span class="hljs-string"><span class="hljs-string">".*mrs\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> master = <span class="hljs-string"><span class="hljs-string">".*master.*"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convertNames</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Option</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input).map(x =&gt; { x.toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> miss() =&gt; <span class="hljs-string"><span class="hljs-string">"Miss"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mr() =&gt; <span class="hljs-string"><span class="hljs-string">"Mr"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mrs() =&gt; <span class="hljs-string"><span class="hljs-string">"Mrs"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> master() =&gt; <span class="hljs-string"><span class="hljs-string">"Master"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-string"><span class="hljs-string">"Unknown"</span></span> } }) }</code> </pre> <br><p>  (Sie können sofort sehen, wie bequem Scala ist, mit fehlenden Werten zu arbeiten, die übrigens nicht nur <code>null</code> , sondern auch <code>Double.NaN</code> , aber es gibt <del>  so ein Witz </del>  so selten wie Auslassungen in <code>BooleanType</code> Variablen usw.) </p><br><p>  <code>UserDefinedFunction</code> nun unsere <code>UserDefinedFunction</code> und erstellen <code>UserDefinedFunction</code> sofort einen darauf basierenden <code>Transformer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformUDF = udf(convertNames _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setUDF(nameTransformUDF) .setInputCol(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"NameType"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> In einem Zeppelin-Laptop ist alles gleich, aber wenn alles später im Produktionscode zusammenkommt, ist es wichtig, dass sich alle UDFs in Klassen oder Objekten befinden, die <code>extends Serializable</code> .  Die offensichtliche Sache, die Sie manchmal vergessen und dann für eine lange Zeit vertiefen können, ist, was falsch ist, wenn Sie die langen Stapelspuren von Spark-Fehlern lesen. </p><br><p>  Jetzt haben wir noch das <code>Cabin</code> .  Schauen wir es uns genauer an: <br><img src="https://habrastorage.org/webt/1m/sn/n2/1msnn2zqeyerzpysj2ma3bwzuda.png"></p><br><p>  Wir sehen, dass viele Werte fehlen, Buchstaben, Zahlen, verschiedene Kombinationen usw.  Nehmen wir die Anzahl der Kabinen (wenn mehr als eine) sowie die Anzahl - sie haben wahrscheinlich eine Art Logik, zum Beispiel, wenn die Nummerierung von einem Ende des Schiffes stammt, hatten die Kabinen am Bug weniger Chancen.  Wir werden auch Funktionen erstellen und dann auf diesen <code>UDFTransformer</code> basieren: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getCabinsCount</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>).length <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numPattern = <span class="hljs-string"><span class="hljs-string">"([az])([0-9]+)"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNumbersFromCabin</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; { x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>)(<span class="hljs-number"><span class="hljs-number">0</span></span>).toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> numPattern(sym, num) =&gt; <span class="hljs-type"><span class="hljs-type">Integer</span></span>.parseInt(num) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-2</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountUDF = udf(getCabinsCount _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinUDF = udf(getNumbersFromCabin _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>) .setUDF(cabinsCountUDF) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>) .setUDF(numbersFromCabinUDF)</code> </pre> <br><p>  Beginnen wir nun mit den fehlenden Werten, nämlich dem Alter.  Lassen Sie uns zunächst die Visualisierungsfunktionen von Zeppelin nutzen: </p><br><p><img src="https://habrastorage.org/webt/h-/cj/l6/h-cjl6tzbakgmqxc1w-gt_kgwk0.png"></p><br><p>  Und sehen Sie, wie fehlende Werte alles verderben.  Es wäre logisch, sie durch eine Mitte (oder einen Median) zu ersetzen, aber unser Ziel ist es, alle Funktionen der MMLSpark-Bibliothek zu berücksichtigen.  Daher werden wir unseren eigenen <code>Estimator</code> schreiben, der die Gruppen- / Durchschnittswerte der Trainingsstichprobe berücksichtigt und durch die entsprechenden Lücken ersetzt. </p><br><p>  Wir werden brauchen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.{<span class="hljs-type"><span class="hljs-type">Dataset</span></span>, <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.{<span class="hljs-type"><span class="hljs-type">Estimator</span></span>, <span class="hljs-type"><span class="hljs-type">Model</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.param.{<span class="hljs-type"><span class="hljs-type">Param</span></span>, <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">Identifiable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">DefaultParamsWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">HasInputCol</span></span>, <span class="hljs-type"><span class="hljs-type">HasOutputCol</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorReadable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">Wrappable</span></span></code> </pre> <br><p>  Achten wir auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>ConstructorWritable</code></a> , das das Leben erheblich vereinfacht.  Wenn unser <code>Model</code> ein "trainiertes" Modell ist, das von der Methode <code>fit(),</code> wird und das vollständig vom Konstruktor bestimmt wird (und dies ist wahrscheinlich in 99% der Fälle der Fall), können wir die Serialisierung überhaupt nicht mit unseren Händen schreiben.  Dies vereinfacht und beschleunigt die Entwicklung erheblich, beseitigt Fehler und senkt auch die Einstiegsschwelle für DataScientist und Analysten, die normalerweise keine professionellen Programmierer sind. </p><br><p>  Definieren Sie unsere <code>Estimator</code> Klasse.  In der Tat ist das Wichtigste hier die <code>fit</code> , der Rest sind technische Punkte: </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerEstimator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">override val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Estimator</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasInputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasOutputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Wrappable</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">DefaultParamsWritable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">this</span></span></span></span>() = <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>(<span class="hljs-type"><span class="hljs-type">Identifiable</span></span>.randomUID(<span class="hljs-string"><span class="hljs-string">"GroupImputer"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> groupCol: <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>]( <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>, <span class="hljs-string"><span class="hljs-string">"groupCol"</span></span>, <span class="hljs-string"><span class="hljs-string">"Groupping column"</span></span> ) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">setGroupCol</span></span></span></span>(v: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.set(groupCol, v) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getGroupCol</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">String</span></span> = $(groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> meanDF = dataset .toDF .groupBy($(groupCol)) .agg(mean(col($(inputCol))).alias(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .select(col($(groupCol)), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>( uid, meanDF, getInputCol, getOutputCol, getGroupCol ) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>( $(outputCol), schema.filter(x =&gt; x.name == $(inputCol))(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType ) ) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">Estimator</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> to = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.uid) copyValues(to, extra).asInstanceOf[<span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>] } }</code> </pre> <br><p>  <strong>Hinweis:</strong> Ich habe defaultCopy nicht verwendet, da ich beim Aufruf aus irgendeinem Grund geschworen habe, keinen Konstruktor zu haben. \ &lt;Init&gt; (java.lang.String), obwohl dies anscheinend nicht hätte passieren dürfen.  In jedem Fall ist die Implementierung der <code>copy</code> einfach. </p><br><p>  Jetzt müssen Sie <code>Model</code> implementieren - eine Klasse, die das trainierte Modell beschreibt und die <code>transform</code> implementiert.  Wir werden es basierend auf der in <code>org.apache.spark.sql.functions</code> <code>coalesce</code> Funktion <code>org.apache.spark.sql.functions</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"> val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val meanDF: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">DataFrame</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val inputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val outputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val groupCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params"> </span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Model</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorWritable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> ttag: <span class="hljs-type"><span class="hljs-type">TypeTag</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = typeTag[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">objectsToSave</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">List</span></span>[<span class="hljs-type"><span class="hljs-type">Any</span></span>] = <span class="hljs-type"><span class="hljs-type">List</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transform</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { dataset .toDF .join(meanDF, <span class="hljs-type"><span class="hljs-type">Seq</span></span>(groupCol), <span class="hljs-string"><span class="hljs-string">"left"</span></span>) .withColumn( outputCol, coalesce(col(inputCol), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .cast(<span class="hljs-type"><span class="hljs-type">IntegerType</span></span>)) .drop(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span><span class="hljs-function"> </span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(outputCol, schema.filter(x =&gt; x.name == inputCol)(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType) ) }</code> </pre> <br><p>  Das letzte Objekt, das wir deklarieren müssen, ist ein <code>Reader</code> , den wir mithilfe der MMLSpark <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ConstructorReadable-</a> Klasse implementieren: </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorReadable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">]</span></span></code> </pre> <br><h2 id="sozdanie-pipeline">  Pipeline-Erstellung </h2><br><p>  In Pipeline möchte ich sowohl die üblichen SparkML-Klassen als auch die unglaublich praktische Sache von MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MultiColumnAdapter zeigen</a> , mit der Sie SparkML-Transformatoren auf viele Spalten gleichzeitig anwenden können (als Referenz nehmen beispielsweise StringIndexer und OneHotEncoder genau eine Spalte zur Eingabe, wodurch sie umgedreht werden Anzeige bei Schmerzen): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.{<span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>, <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.<span class="hljs-type"><span class="hljs-type">Pipeline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>, <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>}</code> </pre> <br><p>  Zunächst erklären wir, welche Spalten wir haben: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-string"><span class="hljs-string">"NameType"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>, <span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>)</code> </pre> <br><p>  Erstellen Sie nun einen String-Encoder: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> stringEncoder = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>() .setBaseStage(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>().setHandleInvalid(<span class="hljs-string"><span class="hljs-string">"keep"</span></span>)) .setInputCols(catCols) .setOutputCols(catCols.map(x =&gt; x + <span class="hljs-string"><span class="hljs-string">"_freqEncoded"</span></span>))</code> </pre> <br><p>  <strong>Hinweis:</strong> Im Gegensatz zu Scikit-Learn in SparkML arbeitet <code>StringIndexer</code> nach dem Prinzip des Frequenzcodierers und kann verwendet werden, um eine Ordnungsbeziehung anzugeben (d. H. Kategorie 0 &lt;Kategorie 1, und dies ist sinnvoll). Dieser Ansatz eignet sich häufig gut für entscheidende Bäume. </p><br><p>  <code>Imputer</code> unseren <code>Imputer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> missingImputer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>) .setGroupCol(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>)</code> </pre> <br><p>  Und <code>VectorAssembler</code> , da SparkML-Klassifizierer mit <code>VectorType</code> komfortabler arbeiten: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> assembler = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>() .setInputCols(stringEncoder.getOutputCols ++ numCols) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)</code> </pre> <br><p>  Jetzt werden wir die mit MMLSpark - LightGBM gelieferte Gradientenverstärkung verwenden, die zusammen mit XGBoost und CatBoost in den "Big Three" der besten Implementierungen dieses Algorithmus enthalten ist.  Es funktioniert um ein Vielfaches schneller, besser und stabiler als die GBM-Implementierung von SparkML (auch wenn der JVM-Port noch in der aktiven Entwicklung ist): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catColIndices = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> lgbClf = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>() .setFeaturesCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>) .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setProbabilityCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setPredictionCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setRawPredictionCol(<span class="hljs-string"><span class="hljs-string">"rawPrediction"</span></span>) .setIsUnbalance(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCategoricalSlotIndexes(catColIndices) .setObjective(<span class="hljs-string"><span class="hljs-string">"binary"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> LightGBM unterstützt das Arbeiten mit kategorialen Variablen (fast wie Catboost). Daher haben wir im Voraus angegeben, wo sich die Kategorieattribute in unserem Vektor befinden, und er selbst wird herausfinden, was mit ihnen zu tun ist und wie sie zu codieren sind. </p><br><div class="spoiler">  <b class="spoiler_title">Weitere Informationen zu LightGBM-Funktionen für Spark</b> <div class="spoiler_text"><ul><li>  Auf Knoten, auf denen RadHat LightGBM ausgeführt wird, stürzt jede Version außer der neuesten ab, da ihm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> <code>glibc</code> Version <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht gefällt</a> .  Dies wurde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kürzlich</a> behoben. Bei der Installation über Maven ruft MMLSpark bei der Installation über Maven die vorletzte Version von LightGBM ab, sodass Sie die Abhängigkeit der neuesten Version von RadHat mit Ihren Händen hinzufügen müssen. </li><li>  LightGBM erstellt in seiner Arbeit einen Socket auf dem Treiber für die Kommunikation mit Führungskräften, und zwar unter Verwendung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>new java.net.ServerSocket(0)</code></a> . Daher wird ein zufälliger Port von den kurzlebigen Ports des Betriebssystems verwendet.  Wenn sich der Bereich der kurzlebigen Ports von dem Bereich der von der Firewall geöffneten Ports unterscheidet, dann <del>  kann viel brennen </del>  Sie können einen interessanten Effekt erzielen, wenn LightGBM manchmal funktioniert (wenn ich einen guten Port gewählt habe) und manchmal nicht.  Und dort wird es Fehler wie <code>ConnectionTimeOut</code> , die beispielsweise auch auf die Option hinweisen, wenn GC an Führungskräften hängt oder so etwas.  Wiederholen Sie meine Fehler im Allgemeinen nicht. </li></ul></div></div><br><p>  Nun, endlich erklären Sie unsere Pipeline: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipeline = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>() .setStages( <span class="hljs-type"><span class="hljs-type">Array</span></span>( missingImputer, nameTransformer, cabinsCountTransformer, numbersFromCabinTransformer, stringEncoder, assembler, lgbClf ) )</code> </pre> <br><h2 id="obuchenie">  Schulung </h2><br><p>  Wir werden unser Trainingsset in einen Zug und einen Test aufteilen und unsere Pipeline überprüfen.  Hier ist es nur möglich, den Komfort der Pipeline zu bewerten, da sie völlig unabhängig von der Partition ist und uns garantiert, dass wir dieselben Transformationen zum Trainieren und Testen anwenden und alle Transformationsparameter im Zug „gelernt“ werden: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">Array</span></span>(trainDF, testDF) = trainFiltered.randomSplit(<span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>)) println(<span class="hljs-string"><span class="hljs-string">s"Train rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${trainDF.count}</span></span></span><span class="hljs-string">\nTest rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${testDF.count}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Train rows: 708 // Test rows: 158 val predictions = pipeline .fit(trainDF) .transform(testDF)</span></span></code> </pre> <br><p>  Zur bequemen Berechnung von Metriken verwenden wir eine andere Klasse aus MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ComputeModelStatistics</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.metrics.<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> modelEvaluator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span>() .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setScoresCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setScoredLabelsCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setEvaluationMetric(<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span>.<span class="hljs-type"><span class="hljs-type">ClassificationMetrics</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/we/rc/ba/wercbac58qpt0hsygugtqqkhc3u.png"></p><br><p>  Nicht schlecht, da wir die Standardeinstellungen nicht geändert haben. </p><br><h2 id="podbor-giperparametrov">  Auswahl von Hyperparametern </h2><br><p>  Um Hyperparameter in MMLSpark auszuwählen, gibt es eine separate coole Sache, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>TuneHyperparameters</code></a> , die eine zufällige Suche im Raster implementiert.  Leider wird <code>Pipeline</code> noch nicht unterstützt, sodass wir den üblichen SparkML <code>CrossValidator</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.tuning.{<span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>, <span class="hljs-type"><span class="hljs-type">CrossValidator</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.evaluation.<span class="hljs-type"><span class="hljs-type">BinaryClassificationEvaluator</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> paramSpace = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>() .addGrid(lgbClf.maxDepth, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) .addGrid(lgbClf.learningRate, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) .addGrid(lgbClf.numIterations, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>)) .build println(<span class="hljs-string"><span class="hljs-string">s"Size of ParamsGrid: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${paramSpace.size}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Size of ParamsGrid: 8 val crossValidator = new CrossValidator() .setEstimator(pipeline) .setEstimatorParamMaps(paramSpace) .setNumFolds(3) .setSeed(42L) .setEvaluator( new BinaryClassificationEvaluator() .setMetricName("areaUnderROC") .setLabelCol("Survived") .setRawPredictionCol("rawPrediction") ) val bestModel = crossValidator .fit(trainFiltered)</span></span></code> </pre> <br><p>  Leider habe ich keinen bequemen Weg gefunden, wie Sie die Ergebnisse zusammen mit den Parametern sehen können, mit denen sie erhalten wurden.  Daher ist es notwendig, "monströse" Designs zu verwenden: </p><br><pre> <code class="scala hljs">crossValidator .getEstimatorParamMaps .zip(bestModel.avgMetrics) .foreach(x =&gt; { println( <span class="hljs-string"><span class="hljs-string">"\n"</span></span> + x._1 .toSeq .foldLeft(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringBuilder</span></span>())( (a, b) =&gt; a .append(<span class="hljs-string"><span class="hljs-string">s"\n\t</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.param.name}</span></span></span><span class="hljs-string"> : </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.value}</span></span></span><span class="hljs-string">"</span></span>)) .toString + <span class="hljs-string"><span class="hljs-string">s"\n\tMetric: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${x._2}</span></span></span><span class="hljs-string">"</span></span> ) })</code> </pre> <br><p>  Was uns so etwas gibt: <br><img src="https://habrastorage.org/webt/xl/li/o7/xllio7tbr67gq_wqolrqwzkzswu.png"></p><br><p>  Wir haben das beste Ergebnis erzielt, indem wir die Lerngeschwindigkeit verringert und die Tiefe der Bäume erhöht haben.  Auf dieser Basis wäre es möglich, den Suchraum anzupassen und ein noch optimaleres Ergebnis zu erzielen, aber wir haben einfach kein solches Ziel. </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  Während MMLSpark Version 0.17 hat und immer noch separate Fehler enthält.  Von allen Spark-Erweiterungen, die ich gesehen habe, verfügt MMLSpark meiner Meinung nach über die umfassendste Dokumentation und den verständlichsten Installations- und Implementierungsprozess.  Microsoft hat es noch nicht wirklich beworben, es gab nur einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bericht über die Databricks</a> , aber dort ging es mehr um DeepLearning und nicht um solche routinemäßigen Dinge, über die ich geschrieben habe. </p><br><p>  Persönlich hat diese Bibliothek bei unseren Aufgaben sehr geholfen, sodass ich ein wenig weniger durch den Dschungel der Spark-Quellen kommen und nicht mit Reflect auf private [ml] -Methoden zugreifen konnte, und ein Kollege fand die Bibliothek fast zufällig.  Gleichzeitig wird aufgrund der Tatsache, dass sich die Bibliothek in der aktiven Entwicklung befindet, die Quelldateistruktur <del>  voller Brei </del>  etwas verwirrend.  Nun, da es keine speziellen Beispiele oder andere Dokumentationen gibt (außer für nacktes Scaladoc), mussten wir zuerst ständig in den Quellcode kriechen. </p><br><p>  Daher hoffe ich wirklich, dass dieses Mini-Tutorial (trotz all seiner Offensichtlichkeit und Einfachheit) für jemanden nützlich ist und dabei hilft, viel Zeit und Mühe zu sparen! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456668/">https://habr.com/ru/post/de456668/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456656/index.html">Lektionen zu SDL 2: Lektion 4 - Dehnen von PNG</a></li>
<li><a href="../de456658/index.html">Wachstum: wie wir Fähigkeiten in einem Team bewerten</a></li>
<li><a href="../de456662/index.html">So sparen Sie mit einer testgetriebenen Entwicklung Geld für einen Therapeuten</a></li>
<li><a href="../de456664/index.html">Wen Sie verklagen müssen, wenn ein Roboter Ihr Geld verliert</a></li>
<li><a href="../de456666/index.html">WebTotem oder wie wir das Internet sicherer machen wollen</a></li>
<li><a href="../de456670/index.html">Über die Spionageauthentifizierungsmethode</a></li>
<li><a href="../de456672/index.html">Nginx-Rezepte: Asynchrone Benachrichtigungen von PostgreSQL an den Websocket</a></li>
<li><a href="../de456674/index.html">Neue Werbemöglichkeiten auf Facebook, von denen Sie nichts wussten</a></li>
<li><a href="../de456676/index.html">Anmelden in einer verteilten PHP-Anwendung</a></li>
<li><a href="../de456678/index.html">Der elektronische Zustand der Zukunft. Teil 4</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>