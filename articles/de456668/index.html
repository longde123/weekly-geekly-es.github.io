<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèΩ‚Äçü§ù‚Äçüë®üèª üßô ü§• Microsoft ML Spark: Eine Spark-Erweiterung, die SparkML humaner und LightGBM als Bonus macht üëæ üëë üè†</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Viele, die mit Spark ML gearbeitet haben, wissen, dass einige der Dinge, die sie dort getan haben, "nicht ganz erfolgreich" sind. 
 oder gar nicht gem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Microsoft ML Spark: Eine Spark-Erweiterung, die SparkML humaner und LightGBM als Bonus macht</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/raiffeisenbank/blog/456668/"><p>  Viele, die mit Spark ML gearbeitet haben, wissen, dass einige der Dinge, die sie dort getan haben, "nicht ganz erfolgreich" sind. <br>  oder gar nicht gemacht.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die Position der Spark-Entwickler ist,</a> dass SparkML die Basisplattform ist und alle Erweiterungen separate Pakete sein m√ºssen.  Dies ist jedoch nicht immer praktisch, da Data Scientist und Analysten mit vertrauten Tools (Jupter, Zeppelin) arbeiten m√∂chten, bei denen das meiste ben√∂tigt wird.  Sie m√∂chten keine 500-Megabyte-JAR-Dateien mit Maven-Assembly sammeln oder Abh√§ngigkeiten in ihre H√§nde herunterladen und zu den Spark-Startparametern hinzuf√ºgen.  Eine genauere Arbeit mit Build-Systemen f√ºr JVM-Projekte erfordert m√∂glicherweise viele zus√§tzliche Anstrengungen von Analysten und DataScientists, die an Jupyter / Zeppelin gew√∂hnt sind.  Es ist eindeutig eine schlechte Idee, DevOps und Clusteradministratoren zu bitten, eine Reihe von Paketen auf Rechenknoten zu platzieren.  Jeder, der selbst Erweiterungen f√ºr SparkML geschrieben hat, wei√ü, wie viele versteckte Schwierigkeiten es mit wichtigen Klassen und Methoden gibt (die aus irgendeinem Grund privat sind [ml]), Einschr√§nkungen bei den Arten der gespeicherten Parameter usw. </p><br><p>  Und mit der MMLSpark-Bibliothek scheint das Leben jetzt etwas einfacher zu sein, und der Schwellenwert f√ºr den Einstieg in skalierbares maschinelles Lernen mit SparkML und Scala ist etwas niedriger. </p><a name="habracut"></a><br><h2 id="vvedenie">  Einf√ºhrung </h2><br><p>  Aufgrund einer Reihe von Schwierigkeiten sowie einer geringen Anzahl vorgefertigter Methoden und L√∂sungen in SparkML schreiben viele Unternehmen ihre Erweiterungen f√ºr Spark.  Ein Beispiel ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PravdaML</a> , das bei Odnoklassniki entwickelt wird und nach einer schnellen Einsch√§tzung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Inhalte</a> von GitHub sehr vielversprechend aussieht.  Leider sind die meisten dieser L√∂sungen entweder geschlossen oder offen, aber sie k√∂nnen nicht √ºber Maven / sbt und die API-Dokumentation installiert werden, was die Arbeit mit ihnen sehr schwierig macht. </p><br><p>  Heute schauen wir uns die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MMLSpark-</a> Bibliothek an. </p><br><p>  Wir werden wie √ºblich das Beispiel der Aufgabe betrachten, Passagiere der Titanic zu klassifizieren.  Ziel ist es, m√∂glichst viele Funktionen der MMLSpark-Bibliothek anzuzeigen, nicht <del>  Schalte SOTA auf ImageNet aus </del>  zeige cooles maschinelles Lernen.  Also wird die Titanic reichen. </p><br><p><img src="https://habrastorage.org/webt/rg/bt/sf/rgbtsf7j0ovmfa5lpjkfpfapzti.jpeg"></p><br><p>  Die Bibliothek selbst verf√ºgt √ºber eine native API f√ºr Scala ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> ), eine Python-API ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation</a> ) und wird nach einigen Stellen im GitHub-Repository bald eine API f√ºr R haben. </p><br><p>  Es gibt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gute Beispiel-Laptops im</a> GitHub-Projekt <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">(PySpark + Jupyter)</a> , aber wir werden den anderen Weg gehen.  Wie Dmitry Bugaychenko <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schrieb</a> , haben Sie bei der Entwicklung f√ºr Spark allen Grund, Scala daf√ºr zu verwenden. Dar√ºber hinaus k√∂nnen Sie mit Scala Ihren eigenen Transformer und Estimator viel effizienter und flexibler definieren, um sie in die SparkML-Pipeline einzubetten, aber wie langsam numpy funktioniert / pandas Code in UDF (auf ausf√ºhrbare Dateien von der JVM aufgerufen) wurde bereits viel geschrieben. </p><br><h2 id="kratko-ob-ustanovke">  Installationsbeschreibung </h2><br><p>  Der gesamte Laptop ist <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> verf√ºgbar.  F√ºr die Arbeit mit der Titanic reicht das lokal auf einem Laptop mit Standardeinstellungen ausgef√ºhrte Zeppelin Docker-Image f√ºr die Augen aus.  Docker finden Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> .  Die MMLSpark-Bibliothek befindet sich nicht in Maven Central, sondern in Spark-Paketen. Um sie zu Zeppelin hinzuzuf√ºgen, m√ºssen Sie den folgenden Block am Anfang des Laptops ausf√ºhren: </p><br><pre><code class="plaintext hljs">%spark.dep z.addRepo("bintray.com").url("http://dl.bintray.com/spark-packages/maven/") z.load("Azure:mmlspark:0.17")</code> </pre> <br><p>  Es ist erw√§hnenswert, dass die Bibliothek eine hervorragende Abw√§rtskompatibilit√§t aufweist: Im Gegensatz zum XGBoost4j-spark, f√ºr den mindestens Spark 2.3+ erforderlich ist, wurde dieses Element in Spark 2.2.1 integriert, das mit dem Zeppelin Docker-Image geliefert wurde, und es gab keine Schwierigkeiten Ich habe es nicht bemerkt. </p><br><p>  <strong>Hinweis: Der</strong> gr√∂√üte Teil der MMLSpark-Bibliothek ist der Inferenz von Gittern in einem Cluster gewidmet, f√ºr die CNTK vorhanden ist (das laut Dokumentation vorgefertigte cntk-Modelle lesen sollte) und einem riesigen OpenCV-Block.  Wir werden uns auf allt√§glichere Aufgaben konzentrieren und versuchen, den Fall zu ‚Äûsimulieren‚Äú, wenn wir riesige Arrays tabellarischer Daten haben, die in HDFS in Form von CSV, Tabellen oder in einem anderen Format vorliegen.  Wir m√ºssen sie also vorverarbeiten und ein Modell erstellen, w√§hrend diese Daten nicht in den Speicher einer Maschine passen.  Daher f√ºhren wir alle Aktionen im Cluster aus. </p><br><h2 id="chtenie-i-razvedochnyy-analiz">  Lesen und Intelligenzanalyse </h2><br><p>  Im Allgemeinen ist Spark + Zeppelin √ºberhaupt nicht schlecht und kann die EDA-Aufgabe bew√§ltigen, aber wir werden versuchen, ihre F√§higkeiten zu erweitern.  Zuerst importieren wir die Klassen, die wir brauchen: </p><br><ul><li>  Alles von spark.sql.types, um ein Schema zu deklarieren und die Daten korrekt zu lesen </li><li>  Alles von spark.sql.functions, um auf Spalten zuzugreifen und integrierte Funktionen zu verwenden </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">com.microsoft.ml.spark.SummarizeData</a> , das als Analogon zu pandas.DataFrame.describe bezeichnet werden kann </li></ul><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">SummarizeData</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.functions._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.types._</code> </pre> <br><p>  Wir lesen unsere Datei: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> titanicSchema = <span class="hljs-type"><span class="hljs-type">StructType</span></span>( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Passanger"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-type"><span class="hljs-type">ShortType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Ticket"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Fare"</span></span>, <span class="hljs-type"><span class="hljs-type">FloatType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">StructField</span></span>(<span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-type"><span class="hljs-type">StringType</span></span>) :: <span class="hljs-type"><span class="hljs-type">Nil</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> train = spark .read .schema(titanicSchema) .option(<span class="hljs-string"><span class="hljs-string">"header"</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) .csv(<span class="hljs-string"><span class="hljs-string">"/mountV/titanic/train.csv"</span></span>)</code> </pre> <br><p>  Und jetzt schauen wir uns die Daten selbst sowie ihre Gr√∂√üe an: </p><br><pre> <code class="scala hljs">println(<span class="hljs-string"><span class="hljs-string">s"Train shape is: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.count}</span></span></span><span class="hljs-string"> x </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${train.columns.length}</span></span></span><span class="hljs-string">"</span></span>) train.limit(<span class="hljs-number"><span class="hljs-number">5</span></span>).createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"trainHead"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> Es ist wirklich nicht erforderlich, createOrReplaceTempView zu verwenden, wenn Sie nur .show (5) schreiben k√∂nnen.  Aber show hat ein Problem: Wenn die Daten "breit" sind, "schwebt" die Textdarstellung der Platte, und es wird √ºberhaupt nichts klar. </p><br><p>  Holen Sie sich die Gr√∂√üe unserer Daten: <code>Train shape is: 891 x 12</code> <br>  Und jetzt k√∂nnen wir uns in der SQL-Zelle die ersten 5 Zeilen ansehen: </p><br><pre> <code class="sql hljs">%sql <span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> trainHead</code> </pre> <br><p><img src="https://habrastorage.org/webt/qe/jc/wj/qejcwjvi65jp6xucnorcdfdmxo4.png"></p><br><p>  Nun, sehen wir uns die Zusammenfassung auf unserem Tisch an: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SummarizeData</span></span>() .setBasic(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCounts(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setPercentiles(<span class="hljs-literal"><span class="hljs-literal">false</span></span>) .setSample(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setErrorThreshold(<span class="hljs-number"><span class="hljs-number">0.25</span></span>) .transform(train) .createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"summary"</span></span>)</code> </pre> <br><p>  Die SummarizeData-Klasse bietet gegen√ºber dem einfachen Dataset.describe mehrere Vorteile, da Sie die Anzahl fehlender und eindeutiger Werte z√§hlen und die Genauigkeit der Berechnung von Quantilen festlegen k√∂nnen.  Dies kann f√ºr wirklich gro√üe Datenmengen von entscheidender Bedeutung sein. <br><img src="https://habrastorage.org/webt/91/g4/8c/91g48c0oaqiuz8pjgklo38jaiis.png"></p><br><div class="spoiler">  <b class="spoiler_title">Einige pers√∂nliche Gedanken</b> <div class="spoiler_text"><p>  Im Allgemeinen schien es mir pers√∂nlich, dass Odnoklassniki in PravdaML eine bessere Implementierung des SummarizeData-Analogons hatte.  Microsoft ist den einfachen Weg <code>org.apache.spark.sql.functions</code> und verwendet <code>org.apache.spark.sql.functions</code> . Es ist nur so, dass alles bequem in einer einzigen Klasse zusammengefasst ist.  F√ºr Odnoklassniki wird dies √ºber den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>VectorStatCollector</code></a> implementiert, der beim Aufrufen etwas komplexeren Code erfordert (Sie m√ºssen zuerst alle Features in einen Vektor <code>VectorAssembler</code> ) und m√∂glicherweise zus√§tzliche Operationen erfordert (z. B. weigert sich <code>VectorAssembler</code> normalerweise, <code>VectorAssembler</code> zu <code>DecimalType</code> ).  Aufgrund meiner Erfahrungen mit Spark gehe ich jedoch davon aus, dass SummarizeData aus MMLSpark mit Fehlern wie <code>StackOverflow</code> in <code>org.apache.spark.sql.catalyst</code> wenn <strong>wirklich viele</strong> Spalten vorhanden sind und das Berechnungsdiagramm zum Zeitpunkt des Starts nicht klein ist ( Obwohl speziell f√ºr solche Fans von "Extrem" in Spark 2.4 die M√∂glichkeit hinzugef√ºgt wurde, den <code>Catalyst</code> Graph Optimizer zu reduzieren.  Nun, es scheint, dass mit einer <strong>wirklich gro√üen</strong> Anzahl von Spalten die Version von Microsoft langsamer sein wird.  Dies muss aber nat√ºrlich separat gepr√ºft werden. </p></div></div><br><h2 id="ochistka-dannyh">  Datenbereinigung </h2><br><p>  In der Titanic ist alles wie gewohnt - in einer Reihe von Zeichenfolgenspalten fehlen Werte.  Und eine Art √úberh√∂hung in den Daten (es scheint, dass diese bestimmte Version der Daten nicht sehr spezifisch ist) - 25 Zeilen von den fehlenden Werten entfernt.  Beheben Sie zun√§chst Folgendes: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trainFiltered = train.filter(!(isnan(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>)) || isnull(col(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>))))</code> </pre> <br><h3 id="obrabotka-strokovyh-dannyh">  String-Datenverarbeitung </h3><br><p>  Soweit ich mich erinnere, waren die Attribute, die aus den Feldern <code>Name</code> und <code>Cabin</code> stammen, die besten, die in der Titanic hervorgebracht wurden.  Sie k√∂nnen sie viel liefern, aber wir beschr√§nken uns auf einige wenige, um keine Beispiele f√ºr fast denselben Code zu nennen. </p><br><p>  Normalerweise ist es zweckm√§√üig, f√ºr solche Dinge regul√§re Ausdr√ºcke zu verwenden. <br>  Aber wir wollen in diesem Fall: </p><br><ul><li>  alles wurde verteilt, die Daten wurden am selben Ort verarbeitet wie sie waren; </li><li>  Alles wurde als SpakrML Transformer- oder Spark ML Estimator-Klassen konzipiert, damit es sp√§ter in Pipeline zusammengestellt werden kann. </li></ul><br><p>  <strong>Hinweis: Die</strong> Pipeline garantiert uns erstens, dass wir immer die gleichen Transformationen sowohl auf den Zug als auch auf den Test anwenden, und erm√∂glicht es uns, den Fehler des "Blicks in die Zukunft" bei der Kreuzvalidierung zu erkennen.  Dar√ºber hinaus erhalten wir einfache Funktionen zum Speichern, Laden und Vorhersagen mithilfe unserer Pipeline. </p><br><p>  SparkML hat eine ‚Äûfast universelle‚Äú Klasse f√ºr solche Aufgaben - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SQLTranformer</a> , aber das Schreiben in SQL ist eindeutig schlechter als das Schreiben in Scala, schon allein deshalb, weil es m√∂glich ist, Syntax oder typische Fehler beim Kompilieren und Hervorheben der Syntax in Idea zu erkennen.  Und hier hilft uns MMLSpark, wo ein wirklich universeller <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">UDFTransformer implementiert wird</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span></code> </pre> <br><p>  Zun√§chst werden wir unsere Transformationsfunktion erstellen, die bis zum Limit sehr einfach ist. Unser Ziel ist es nun, den Prozess der Erstellung von UDFTransformer aufzuzeigen.  Im Prinzip kann jeder anhand solcher einfachen Beispiele jeder Komplexit√§tsebene Logik hinzuf√ºgen. </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> miss = <span class="hljs-string"><span class="hljs-string">".*miss\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mr = <span class="hljs-string"><span class="hljs-string">".*mr\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> mrs = <span class="hljs-string"><span class="hljs-string">".*mrs\\..*"</span></span>.r <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> master = <span class="hljs-string"><span class="hljs-string">".*master.*"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">convertNames</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Option</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input).map(x =&gt; { x.toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> miss() =&gt; <span class="hljs-string"><span class="hljs-string">"Miss"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mr() =&gt; <span class="hljs-string"><span class="hljs-string">"Mr"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> mrs() =&gt; <span class="hljs-string"><span class="hljs-string">"Mrs"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> master() =&gt; <span class="hljs-string"><span class="hljs-string">"Master"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-string"><span class="hljs-string">"Unknown"</span></span> } }) }</code> </pre> <br><p>  (Sie k√∂nnen sofort sehen, wie bequem Scala ist, mit fehlenden Werten zu arbeiten, die √ºbrigens nicht nur <code>null</code> , sondern auch <code>Double.NaN</code> , aber es gibt <del>  so ein Witz </del>  so selten wie Auslassungen in <code>BooleanType</code> Variablen usw.) </p><br><p>  <code>UserDefinedFunction</code> nun unsere <code>UserDefinedFunction</code> und erstellen <code>UserDefinedFunction</code> sofort einen darauf basierenden <code>Transformer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformUDF = udf(convertNames _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> nameTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setUDF(nameTransformUDF) .setInputCol(<span class="hljs-string"><span class="hljs-string">"Name"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"NameType"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> In einem Zeppelin-Laptop ist alles gleich, aber wenn alles sp√§ter im Produktionscode zusammenkommt, ist es wichtig, dass sich alle UDFs in Klassen oder Objekten befinden, die <code>extends Serializable</code> .  Die offensichtliche Sache, die Sie manchmal vergessen und dann f√ºr eine lange Zeit vertiefen k√∂nnen, ist, was falsch ist, wenn Sie die langen Stapelspuren von Spark-Fehlern lesen. </p><br><p>  Jetzt haben wir noch das <code>Cabin</code> .  Schauen wir es uns genauer an: <br><img src="https://habrastorage.org/webt/1m/sn/n2/1msnn2zqeyerzpysj2ma3bwzuda.png"></p><br><p>  Wir sehen, dass viele Werte fehlen, Buchstaben, Zahlen, verschiedene Kombinationen usw.  Nehmen wir die Anzahl der Kabinen (wenn mehr als eine) sowie die Anzahl - sie haben wahrscheinlich eine Art Logik, zum Beispiel, wenn die Nummerierung von einem Ende des Schiffes stammt, hatten die Kabinen am Bug weniger Chancen.  Wir werden auch Funktionen erstellen und dann auf diesen <code>UDFTransformer</code> basieren: </p><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getCabinsCount</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>).length <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numPattern = <span class="hljs-string"><span class="hljs-string">"([az])([0-9]+)"</span></span>.r <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNumbersFromCabin</span></span></span></span>(input: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-type"><span class="hljs-type">Int</span></span> = { <span class="hljs-type"><span class="hljs-type">Option</span></span>(input) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">Some</span></span>(x) =&gt; { x.split(<span class="hljs-string"><span class="hljs-string">" "</span></span>)(<span class="hljs-number"><span class="hljs-number">0</span></span>).toLowerCase <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> numPattern(sym, num) =&gt; <span class="hljs-type"><span class="hljs-type">Integer</span></span>.parseInt(num) <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> _ =&gt; <span class="hljs-number"><span class="hljs-number">-1</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-type"><span class="hljs-type">None</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">-2</span></span> } } <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountUDF = udf(getCabinsCount _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinUDF = udf(getNumbersFromCabin _) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cabinsCountTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>) .setUDF(cabinsCountUDF) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numbersFromCabinTransformer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">UDFTransformer</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Cabin"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>) .setUDF(numbersFromCabinUDF)</code> </pre> <br><p>  Beginnen wir nun mit den fehlenden Werten, n√§mlich dem Alter.  Lassen Sie uns zun√§chst die Visualisierungsfunktionen von Zeppelin nutzen: </p><br><p><img src="https://habrastorage.org/webt/h-/cj/l6/h-cjl6tzbakgmqxc1w-gt_kgwk0.png"></p><br><p>  Und sehen Sie, wie fehlende Werte alles verderben.  Es w√§re logisch, sie durch eine Mitte (oder einen Median) zu ersetzen, aber unser Ziel ist es, alle Funktionen der MMLSpark-Bibliothek zu ber√ºcksichtigen.  Daher werden wir unseren eigenen <code>Estimator</code> schreiben, der die Gruppen- / Durchschnittswerte der Trainingsstichprobe ber√ºcksichtigt und durch die entsprechenden L√ºcken ersetzt. </p><br><p>  Wir werden brauchen: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql.{<span class="hljs-type"><span class="hljs-type">Dataset</span></span>, <span class="hljs-type"><span class="hljs-type">DataFrame</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.{<span class="hljs-type"><span class="hljs-type">Estimator</span></span>, <span class="hljs-type"><span class="hljs-type">Model</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.param.{<span class="hljs-type"><span class="hljs-type">Param</span></span>, <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">Identifiable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.util.<span class="hljs-type"><span class="hljs-type">DefaultParamsWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">HasInputCol</span></span>, <span class="hljs-type"><span class="hljs-type">HasOutputCol</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorWritable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ConstructorReadable</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">Wrappable</span></span></code> </pre> <br><p>  Achten wir auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>ConstructorWritable</code></a> , das das Leben erheblich vereinfacht.  Wenn unser <code>Model</code> ein "trainiertes" Modell ist, das von der Methode <code>fit(),</code> wird und das vollst√§ndig vom Konstruktor bestimmt wird (und dies ist wahrscheinlich in 99% der F√§lle der Fall), k√∂nnen wir die Serialisierung √ºberhaupt nicht mit unseren H√§nden schreiben.  Dies vereinfacht und beschleunigt die Entwicklung erheblich, beseitigt Fehler und senkt auch die Einstiegsschwelle f√ºr DataScientist und Analysten, die normalerweise keine professionellen Programmierer sind. </p><br><p>  Definieren Sie unsere <code>Estimator</code> Klasse.  In der Tat ist das Wichtigste hier die <code>fit</code> , der Rest sind technische Punkte: </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerEstimator</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">override val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Estimator</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasInputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">HasOutputCol</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Wrappable</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">DefaultParamsWritable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">this</span></span></span></span>() = <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>(<span class="hljs-type"><span class="hljs-type">Identifiable</span></span>.randomUID(<span class="hljs-string"><span class="hljs-string">"GroupImputer"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> groupCol: <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Param</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>]( <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>, <span class="hljs-string"><span class="hljs-string">"groupCol"</span></span>, <span class="hljs-string"><span class="hljs-string">"Groupping column"</span></span> ) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">setGroupCol</span></span></span></span>(v: <span class="hljs-type"><span class="hljs-type">String</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.set(groupCol, v) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getGroupCol</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">String</span></span> = $(groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> meanDF = dataset .toDF .groupBy($(groupCol)) .agg(mean(col($(inputCol))).alias(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .select(col($(groupCol)), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>( uid, meanDF, getInputCol, getOutputCol, getGroupCol ) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>( $(outputCol), schema.filter(x =&gt; x.name == $(inputCol))(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType ) ) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">Estimator</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> to = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.uid) copyValues(to, extra).asInstanceOf[<span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>] } }</code> </pre> <br><p>  <strong>Hinweis:</strong> Ich habe defaultCopy nicht verwendet, da ich beim Aufruf aus irgendeinem Grund geschworen habe, keinen Konstruktor zu haben. \ &lt;Init&gt; (java.lang.String), obwohl dies anscheinend nicht h√§tte passieren d√ºrfen.  In jedem Fall ist die Implementierung der <code>copy</code> einfach. </p><br><p>  Jetzt m√ºssen Sie <code>Model</code> implementieren - eine Klasse, die das trainierte Modell beschreibt und die <code>transform</code> implementiert.  Wir werden es basierend auf der in <code>org.apache.spark.sql.functions</code> <code>coalesce</code> Funktion <code>org.apache.spark.sql.functions</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"> val uid: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val meanDF: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">DataFrame</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val inputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val outputCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val groupCol: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">String</span></span></span></span><span class="hljs-class"><span class="hljs-params"> </span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Model</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">with</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorWritable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> ttag: <span class="hljs-type"><span class="hljs-type">TypeTag</span></span>[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] = typeTag[<span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">objectsToSave</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">List</span></span>[<span class="hljs-type"><span class="hljs-type">Any</span></span>] = <span class="hljs-type"><span class="hljs-type">List</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copy</span></span></span></span>(extra: <span class="hljs-type"><span class="hljs-type">ParamMap</span></span>): <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerModel</span></span>(uid, meanDF, inputCol, outputCol, groupCol) <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transform</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = { dataset .toDF .join(meanDF, <span class="hljs-type"><span class="hljs-type">Seq</span></span>(groupCol), <span class="hljs-string"><span class="hljs-string">"left"</span></span>) .withColumn( outputCol, coalesce(col(inputCol), col(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>)) .cast(<span class="hljs-type"><span class="hljs-type">IntegerType</span></span>)) .drop(<span class="hljs-string"><span class="hljs-string">"groupMean"</span></span>) } <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">transformSchema</span></span></span><span class="hljs-function"> </span></span>(schema: <span class="hljs-type"><span class="hljs-type">StructType</span></span>): <span class="hljs-type"><span class="hljs-type">StructType</span></span> = schema .add( <span class="hljs-type"><span class="hljs-type">StructField</span></span>(outputCol, schema.filter(x =&gt; x.name == inputCol)(<span class="hljs-number"><span class="hljs-number">0</span></span>).dataType) ) }</code> </pre> <br><p>  Das letzte Objekt, das wir deklarieren m√ºssen, ist ein <code>Reader</code> , den wir mithilfe der MMLSpark <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ConstructorReadable-</a> Klasse implementieren: </p><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GroupImputerModel</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ConstructorReadable</span></span></span><span class="hljs-class">[</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">GroupImputerModel</span></span></span><span class="hljs-class">]</span></span></code> </pre> <br><h2 id="sozdanie-pipeline">  Pipeline-Erstellung </h2><br><p>  In Pipeline m√∂chte ich sowohl die √ºblichen SparkML-Klassen als auch die unglaublich praktische Sache von MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MultiColumnAdapter zeigen</a> , mit der Sie SparkML-Transformatoren auf viele Spalten gleichzeitig anwenden k√∂nnen (als Referenz nehmen beispielsweise StringIndexer und OneHotEncoder genau eine Spalte zur Eingabe, wodurch sie umgedreht werden Anzeige bei Schmerzen): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.{<span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>, <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.<span class="hljs-type"><span class="hljs-type">Pipeline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.{<span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>, <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>}</code> </pre> <br><p>  Zun√§chst erkl√§ren wir, welche Spalten wir haben: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>, <span class="hljs-string"><span class="hljs-string">"Embarked"</span></span>, <span class="hljs-string"><span class="hljs-string">"NameType"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> numCols = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"PClass"</span></span>, <span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>, <span class="hljs-string"><span class="hljs-string">"SibSp"</span></span>, <span class="hljs-string"><span class="hljs-string">"Parch"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinCount"</span></span>, <span class="hljs-string"><span class="hljs-string">"CabinNumber"</span></span>)</code> </pre> <br><p>  Erstellen Sie nun einen String-Encoder: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> stringEncoder = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">MultiColumnAdapter</span></span>() .setBaseStage(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringIndexer</span></span>().setHandleInvalid(<span class="hljs-string"><span class="hljs-string">"keep"</span></span>)) .setInputCols(catCols) .setOutputCols(catCols.map(x =&gt; x + <span class="hljs-string"><span class="hljs-string">"_freqEncoded"</span></span>))</code> </pre> <br><p>  <strong>Hinweis:</strong> Im Gegensatz zu Scikit-Learn in SparkML arbeitet <code>StringIndexer</code> nach dem Prinzip des Frequenzcodierers und kann verwendet werden, um eine Ordnungsbeziehung anzugeben (d. H. Kategorie 0 &lt;Kategorie 1, und dies ist sinnvoll). Dieser Ansatz eignet sich h√§ufig gut f√ºr entscheidende B√§ume. </p><br><p>  <code>Imputer</code> unseren <code>Imputer</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> missingImputer = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">GroupImputerEstimator</span></span>() .setInputCol(<span class="hljs-string"><span class="hljs-string">"Age"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"AgeNoMissings"</span></span>) .setGroupCol(<span class="hljs-string"><span class="hljs-string">"Sex"</span></span>)</code> </pre> <br><p>  Und <code>VectorAssembler</code> , da SparkML-Klassifizierer mit <code>VectorType</code> komfortabler arbeiten: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> assembler = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>() .setInputCols(stringEncoder.getOutputCols ++ numCols) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>)</code> </pre> <br><p>  Jetzt werden wir die mit MMLSpark - LightGBM gelieferte Gradientenverst√§rkung verwenden, die zusammen mit XGBoost und CatBoost in den "Big Three" der besten Implementierungen dieses Algorithmus enthalten ist.  Es funktioniert um ein Vielfaches schneller, besser und stabiler als die GBM-Implementierung von SparkML (auch wenn der JVM-Port noch in der aktiven Entwicklung ist): </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> catColIndices = <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> lgbClf = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LightGBMClassifier</span></span>() .setFeaturesCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>) .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setProbabilityCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setPredictionCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setRawPredictionCol(<span class="hljs-string"><span class="hljs-string">"rawPrediction"</span></span>) .setIsUnbalance(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setCategoricalSlotIndexes(catColIndices) .setObjective(<span class="hljs-string"><span class="hljs-string">"binary"</span></span>)</code> </pre> <br><p>  <strong>Hinweis:</strong> LightGBM unterst√ºtzt das Arbeiten mit kategorialen Variablen (fast wie Catboost). Daher haben wir im Voraus angegeben, wo sich die Kategorieattribute in unserem Vektor befinden, und er selbst wird herausfinden, was mit ihnen zu tun ist und wie sie zu codieren sind. </p><br><div class="spoiler">  <b class="spoiler_title">Weitere Informationen zu LightGBM-Funktionen f√ºr Spark</b> <div class="spoiler_text"><ul><li>  Auf Knoten, auf denen RadHat LightGBM ausgef√ºhrt wird, st√ºrzt jede Version au√üer der neuesten ab, da ihm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">die</a> <code>glibc</code> Version <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nicht gef√§llt</a> .  Dies wurde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">k√ºrzlich</a> behoben. Bei der Installation √ºber Maven ruft MMLSpark bei der Installation √ºber Maven die vorletzte Version von LightGBM ab, sodass Sie die Abh√§ngigkeit der neuesten Version von RadHat mit Ihren H√§nden hinzuf√ºgen m√ºssen. </li><li>  LightGBM erstellt in seiner Arbeit einen Socket auf dem Treiber f√ºr die Kommunikation mit F√ºhrungskr√§ften, und zwar unter Verwendung des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>new java.net.ServerSocket(0)</code></a> . Daher wird ein zuf√§lliger Port von den kurzlebigen Ports des Betriebssystems verwendet.  Wenn sich der Bereich der kurzlebigen Ports von dem Bereich der von der Firewall ge√∂ffneten Ports unterscheidet, dann <del>  kann viel brennen </del>  Sie k√∂nnen einen interessanten Effekt erzielen, wenn LightGBM manchmal funktioniert (wenn ich einen guten Port gew√§hlt habe) und manchmal nicht.  Und dort wird es Fehler wie <code>ConnectionTimeOut</code> , die beispielsweise auch auf die Option hinweisen, wenn GC an F√ºhrungskr√§ften h√§ngt oder so etwas.  Wiederholen Sie meine Fehler im Allgemeinen nicht. </li></ul></div></div><br><p>  Nun, endlich erkl√§ren Sie unsere Pipeline: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipeline = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>() .setStages( <span class="hljs-type"><span class="hljs-type">Array</span></span>( missingImputer, nameTransformer, cabinsCountTransformer, numbersFromCabinTransformer, stringEncoder, assembler, lgbClf ) )</code> </pre> <br><h2 id="obuchenie">  Schulung </h2><br><p>  Wir werden unser Trainingsset in einen Zug und einen Test aufteilen und unsere Pipeline √ºberpr√ºfen.  Hier ist es nur m√∂glich, den Komfort der Pipeline zu bewerten, da sie v√∂llig unabh√§ngig von der Partition ist und uns garantiert, dass wir dieselben Transformationen zum Trainieren und Testen anwenden und alle Transformationsparameter im Zug ‚Äûgelernt‚Äú werden: </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">Array</span></span>(trainDF, testDF) = trainFiltered.randomSplit(<span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>)) println(<span class="hljs-string"><span class="hljs-string">s"Train rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${trainDF.count}</span></span></span><span class="hljs-string">\nTest rows: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${testDF.count}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Train rows: 708 // Test rows: 158 val predictions = pipeline .fit(trainDF) .transform(testDF)</span></span></code> </pre> <br><p>  Zur bequemen Berechnung von Metriken verwenden wir eine andere Klasse aus MMLSpark - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ComputeModelStatistics</a> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.<span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.microsoft.ml.spark.metrics.<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> modelEvaluator = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ComputeModelStatistics</span></span>() .setLabelCol(<span class="hljs-string"><span class="hljs-string">"Survived"</span></span>) .setScoresCol(<span class="hljs-string"><span class="hljs-string">"predictedProb"</span></span>) .setScoredLabelsCol(<span class="hljs-string"><span class="hljs-string">"predictedLabel"</span></span>) .setEvaluationMetric(<span class="hljs-type"><span class="hljs-type">MetricConstants</span></span>.<span class="hljs-type"><span class="hljs-type">ClassificationMetrics</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/we/rc/ba/wercbac58qpt0hsygugtqqkhc3u.png"></p><br><p>  Nicht schlecht, da wir die Standardeinstellungen nicht ge√§ndert haben. </p><br><h2 id="podbor-giperparametrov">  Auswahl von Hyperparametern </h2><br><p>  Um Hyperparameter in MMLSpark auszuw√§hlen, gibt es eine separate coole Sache, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>TuneHyperparameters</code></a> , die eine zuf√§llige Suche im Raster implementiert.  Leider wird <code>Pipeline</code> noch nicht unterst√ºtzt, sodass wir den √ºblichen SparkML <code>CrossValidator</code> : </p><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.tuning.{<span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>, <span class="hljs-type"><span class="hljs-type">CrossValidator</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.evaluation.<span class="hljs-type"><span class="hljs-type">BinaryClassificationEvaluator</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> paramSpace = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>() .addGrid(lgbClf.maxDepth, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>)) .addGrid(lgbClf.learningRate, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) .addGrid(lgbClf.numIterations, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">300</span></span>)) .build println(<span class="hljs-string"><span class="hljs-string">s"Size of ParamsGrid: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${paramSpace.size}</span></span></span><span class="hljs-string">"</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Size of ParamsGrid: 8 val crossValidator = new CrossValidator() .setEstimator(pipeline) .setEstimatorParamMaps(paramSpace) .setNumFolds(3) .setSeed(42L) .setEvaluator( new BinaryClassificationEvaluator() .setMetricName("areaUnderROC") .setLabelCol("Survived") .setRawPredictionCol("rawPrediction") ) val bestModel = crossValidator .fit(trainFiltered)</span></span></code> </pre> <br><p>  Leider habe ich keinen bequemen Weg gefunden, wie Sie die Ergebnisse zusammen mit den Parametern sehen k√∂nnen, mit denen sie erhalten wurden.  Daher ist es notwendig, "monstr√∂se" Designs zu verwenden: </p><br><pre> <code class="scala hljs">crossValidator .getEstimatorParamMaps .zip(bestModel.avgMetrics) .foreach(x =&gt; { println( <span class="hljs-string"><span class="hljs-string">"\n"</span></span> + x._1 .toSeq .foldLeft(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StringBuilder</span></span>())( (a, b) =&gt; a .append(<span class="hljs-string"><span class="hljs-string">s"\n\t</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.param.name}</span></span></span><span class="hljs-string"> : </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${b.value}</span></span></span><span class="hljs-string">"</span></span>)) .toString + <span class="hljs-string"><span class="hljs-string">s"\n\tMetric: </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${x._2}</span></span></span><span class="hljs-string">"</span></span> ) })</code> </pre> <br><p>  Was uns so etwas gibt: <br><img src="https://habrastorage.org/webt/xl/li/o7/xllio7tbr67gq_wqolrqwzkzswu.png"></p><br><p>  Wir haben das beste Ergebnis erzielt, indem wir die Lerngeschwindigkeit verringert und die Tiefe der B√§ume erh√∂ht haben.  Auf dieser Basis w√§re es m√∂glich, den Suchraum anzupassen und ein noch optimaleres Ergebnis zu erzielen, aber wir haben einfach kein solches Ziel. </p><br><h2 id="zaklyuchenie">  Fazit </h2><br><p>  W√§hrend MMLSpark Version 0.17 hat und immer noch separate Fehler enth√§lt.  Von allen Spark-Erweiterungen, die ich gesehen habe, verf√ºgt MMLSpark meiner Meinung nach √ºber die umfassendste Dokumentation und den verst√§ndlichsten Installations- und Implementierungsprozess.  Microsoft hat es noch nicht wirklich beworben, es gab nur einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bericht √ºber die Databricks</a> , aber dort ging es mehr um DeepLearning und nicht um solche routinem√§√üigen Dinge, √ºber die ich geschrieben habe. </p><br><p>  Pers√∂nlich hat diese Bibliothek bei unseren Aufgaben sehr geholfen, sodass ich ein wenig weniger durch den Dschungel der Spark-Quellen kommen und nicht mit Reflect auf private [ml] -Methoden zugreifen konnte, und ein Kollege fand die Bibliothek fast zuf√§llig.  Gleichzeitig wird aufgrund der Tatsache, dass sich die Bibliothek in der aktiven Entwicklung befindet, die Quelldateistruktur <del>  voller Brei </del>  etwas verwirrend.  Nun, da es keine speziellen Beispiele oder andere Dokumentationen gibt (au√üer f√ºr nacktes Scaladoc), mussten wir zuerst st√§ndig in den Quellcode kriechen. </p><br><p>  Daher hoffe ich wirklich, dass dieses Mini-Tutorial (trotz all seiner Offensichtlichkeit und Einfachheit) f√ºr jemanden n√ºtzlich ist und dabei hilft, viel Zeit und M√ºhe zu sparen! </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de456668/">https://habr.com/ru/post/de456668/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de456656/index.html">Lektionen zu SDL 2: Lektion 4 - Dehnen von PNG</a></li>
<li><a href="../de456658/index.html">Wachstum: wie wir F√§higkeiten in einem Team bewerten</a></li>
<li><a href="../de456662/index.html">So sparen Sie mit einer testgetriebenen Entwicklung Geld f√ºr einen Therapeuten</a></li>
<li><a href="../de456664/index.html">Wen Sie verklagen m√ºssen, wenn ein Roboter Ihr Geld verliert</a></li>
<li><a href="../de456666/index.html">WebTotem oder wie wir das Internet sicherer machen wollen</a></li>
<li><a href="../de456670/index.html">√úber die Spionageauthentifizierungsmethode</a></li>
<li><a href="../de456672/index.html">Nginx-Rezepte: Asynchrone Benachrichtigungen von PostgreSQL an den Websocket</a></li>
<li><a href="../de456674/index.html">Neue Werbem√∂glichkeiten auf Facebook, von denen Sie nichts wussten</a></li>
<li><a href="../de456676/index.html">Anmelden in einer verteilten PHP-Anwendung</a></li>
<li><a href="../de456678/index.html">Der elektronische Zustand der Zukunft. Teil 4</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>