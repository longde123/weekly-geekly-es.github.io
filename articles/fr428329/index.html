<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëß üåÖ üçΩÔ∏è Formation de renforcement: analyse des jeux vid√©o ü§òüèæ üï¢ üèÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Lors de la conf√©rence sur l'IA, Vladimir Ivanov vivanov879 , Sr. parlera de l'utilisation de l'apprentissage renforc√© Ing√©nieur Deep Learning chez Nvi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Formation de renforcement: analyse des jeux vid√©o</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/smileexpo/blog/428329/"><img align="left" src="https://habrastorage.org/webt/ls/va/9_/lsva9_rsrvkdmceogvse3sexdog.png"><br>  Lors de la conf√©rence sur l'IA, <b>Vladimir Ivanov <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">vivanov879</a> , Sr.</b> parlera de l'utilisation de l'apprentissage renforc√©  <b>Ing√©nieur Deep Learning chez Nvidia</b> .  L'expert est engag√© dans le machine learning au sein du d√©partement de test: ¬´J'analyse les donn√©es que nous collectons lors des tests de jeux vid√©o et de mat√©riel.  Pour cela, j'utilise l'apprentissage automatique et la vision par ordinateur.  L'essentiel du travail est l'analyse d'images, le nettoyage des donn√©es avant la formation, le balisage des donn√©es et la visualisation des solutions obtenues. ¬ª <br><br>  Dans l'article d'aujourd'hui, Vladimir explique pourquoi l'apprentissage renforc√© est utilis√© dans les voitures autonomes et explique comment un agent est form√© pour agir dans un environnement en mutation - √† l'aide d'exemples de jeux vid√©o. <br><br>  Au cours des derni√®res ann√©es, l'humanit√© a accumul√© une √©norme quantit√© de donn√©es.  Certains jeux de donn√©es sont partag√©s et pr√©sent√©s manuellement.  Par exemple, l'ensemble de donn√©es CIFAR, o√π chaque image est sign√©e, √† quelle classe elle appartient. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/16/ko/lp/16kolpeo1k3j1j9ppdh9vfjb6vc.jpeg"></div><br><br>  Il existe des ensembles de donn√©es o√π vous devez affecter une classe non seulement √† l'image dans son ensemble, mais √† chaque pixel de l'image.  Comme, par exemple, dans CityScapes. <br><br><img src="https://habrastorage.org/webt/eq/hz/b0/eqhzb0gt9v2demntxriobd5ewji.png"><br><br>  Ce qui unit ces t√¢ches, c'est qu'un r√©seau de neurones d'apprentissage n'a qu'√† se souvenir des sch√©mas des donn√©es.  Par cons√©quent, avec des quantit√©s de donn√©es suffisamment importantes, et dans le cas du CIFAR, il s'agit de 80 millions d'images, le r√©seau neuronal apprend √† se g√©n√©raliser.  En cons√©quence, elle g√®re bien la classification des images qu'elle n'avait jamais vues auparavant. <br><br>  Mais agissant dans le cadre de la technique p√©dagogique avec l'enseignant, qui travaille pour le marquage des images, il est impossible de r√©soudre des probl√®mes o√π l'on ne veut pas pr√©dire l'√©tiquette mais prendre des d√©cisions.  Comme, par exemple, dans le cas de la conduite autonome, o√π la t√¢che consiste √† atteindre de mani√®re s√ªre et fiable le point final de l'itin√©raire. <a name="habracut"></a><br><br>  Dans les probl√®mes de classification, nous avons utilis√© la technique d'enseignement avec l'enseignant - lorsque chaque image est affect√©e √† une classe sp√©cifique.  Mais que se passe-t-il si nous n'avons pas un tel balisage, mais qu'il existe un agent et un environnement dans lesquels il peut effectuer certaines actions?  Par exemple, que ce soit un jeu vid√©o, nous pouvons cliquer sur les fl√®ches de contr√¥le. <br><br><img src="https://habrastorage.org/webt/bn/-c/ad/bn-cadnljvu7relew1mfes6p0xw.jpeg"><br><br>  Ce type de probl√®me devrait √™tre r√©solu par une formation de renforcement.  Dans l'√©nonc√© g√©n√©ral du probl√®me, nous voulons apprendre √† effectuer la s√©quence correcte d'actions.  Il est fondamentalement important que l'agent ait la capacit√© d'ex√©cuter des actions encore et encore, explorant ainsi l'environnement dans lequel il se trouve.  Et au lieu de la bonne r√©ponse, que faire dans une situation particuli√®re, il re√ßoit une r√©compense pour une t√¢che correctement accomplie.  Par exemple, dans le cas d'un taxi autonome, le chauffeur recevra un bonus pour chaque trajet effectu√©. <br><br>  Revenons √† un exemple simple - un jeu vid√©o.  Prenez quelque chose de simple, comme le jeu de tennis de table Atari. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hg/ev/0b/hgev0bl9kpvpzlllfzyyroentow.jpeg"></div><br><br>  Nous contr√¥lerons la tablette √† gauche.  Nous jouerons contre le joueur informatique programm√© sur les r√®gles √† droite.  √âtant donn√© que nous travaillons avec une image et que les r√©seaux de neurones sont les plus efficaces pour extraire des informations √† partir d'images, appliquons une image √† l'entr√©e d'un r√©seau de neurones √† trois couches avec une taille de noyau 3x3.  A la sortie, elle devra choisir l'une des deux actions: d√©placer le plateau vers le haut ou vers le bas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/o_/5z/mwo_5zeixn2llke6ethglty43ao.png"></div><br><br>  Nous formons le r√©seau neuronal pour effectuer des actions qui m√®nent √† la victoire.  La technique d'entra√Ænement est la suivante.  Nous avons laiss√© le r√©seau neuronal jouer quelques parties de tennis de table.  Ensuite, nous commen√ßons √† trier les jeux jou√©s.  Dans ces jeux o√π elle a gagn√©, nous marquons les images √©tiquet√©es ¬´Up¬ª o√π elle a soulev√© la raquette et ¬´Down¬ª o√π elle l'a abaiss√©e.  Dans les matchs perdus, nous faisons le contraire.  Nous marquons ces photos o√π elle a abaiss√© la planche avec l'√©tiquette ¬´Up¬ª, et o√π elle l'a soulev√©e, ¬´Down¬ª.  Ainsi, nous r√©duisons le probl√®me √† l'approche que nous connaissons d√©j√† - la formation avec un enseignant.  Nous avons un ensemble d'images avec des √©tiquettes. <br><br><img src="https://habrastorage.org/webt/al/16/48/al1648vo1sfp72qj_8n2fpif3va.png"><br><br>  En utilisant cette technique de formation, en quelques heures, notre agent apprendra √† battre un joueur d'ordinateur programm√© selon les r√®gles. <br><br>  Que faire de la conduite autonome?  Le fait est que le tennis de table est un jeu tr√®s simple.  Et il peut produire des milliers d'images par seconde.  Dans notre r√©seau, il n'y a maintenant que 3 couches.  Par cons√©quent, le processus d'apprentissage est rapide comme l'√©clair.  Le jeu g√©n√®re une √©norme quantit√© de donn√©es et nous les traitons instantan√©ment.  Dans le cas de la conduite autonome, la collecte de donn√©es est beaucoup plus longue et plus co√ªteuse.  Les voitures sont ch√®res, et avec une voiture, nous ne recevrons que 60 images par seconde.  De plus, le prix de l'erreur augmente.  Dans un jeu vid√©o, on pouvait se permettre de jouer jeu apr√®s match au tout d√©but de l'entra√Ænement.  Mais nous ne pouvons pas nous permettre de g√¢cher la voiture. <br><br>  Dans ce cas, aidons le r√©seau neuronal au tout d√©but de l'entra√Ænement.  Nous fixons la cam√©ra sur la voiture, y mettons un conducteur exp√©riment√© et nous enregistrerons des photos de la cam√©ra.  Pour chaque photo, nous inscrivons l'angle de braquage de la voiture.  Nous formerons le r√©seau neuronal √† copier le comportement d'un conducteur exp√©riment√©.  Ainsi, nous avons de nouveau r√©duit la t√¢che √† l'enseignement d√©j√† connu avec un professeur. <br><br><img src="https://habrastorage.org/webt/pl/d0/oc/pld0oc75oafeojutvborl_upb8a.png"><br><br>  Avec un ensemble de donn√©es suffisamment grand et diversifi√©, qui comprendra diff√©rents paysages, saisons et conditions m√©t√©orologiques, le r√©seau de neurones apprendra √† contr√¥ler avec pr√©cision la voiture. <br><br>  Cependant, il y avait un probl√®me avec les donn√©es.  Ils sont tr√®s longs et co√ªteux √† collecter.  Utilisons un simulateur dans lequel toute la physique du mouvement de la voiture sera impl√©ment√©e - par exemple, DeepDrive.  Nous pouvons l'apprendre sans craindre de perdre une voiture. <br><br><img src="https://habrastorage.org/webt/fe/c0/4v/fec04vmzbamtd60xxv3ypctf2ua.jpeg"><br><br>  Dans ce simulateur, nous avons acc√®s √† tous les indicateurs de la voiture et du monde.  En outre, toutes les personnes, les voitures, leurs vitesses et leurs distances par rapport √† elles sont marqu√©es autour. <br><br><img src="https://habrastorage.org/webt/hq/37/1k/hq371k3xvab9kfcworznwlkelai.jpeg"><br><br>  Du point de vue de l'ing√©nieur, dans un tel simulateur, vous pouvez essayer en toute s√©curit√© de nouvelles techniques de formation.  Que doit faire un chercheur?  Par exemple, √©tudier diff√©rentes options de descente de gradient dans des probl√®mes d'apprentissage avec renforcement.  Pour tester une hypoth√®se simple, je ne veux pas tirer sur des moineaux √† partir d'un canon et ex√©cuter un agent dans un monde virtuel complexe, puis attendre des jours cons√©cutifs pour les r√©sultats de la simulation.  Dans ce cas, utilisons plus efficacement notre puissance de calcul.  Que les agents soient plus simples.  Prenons, par exemple, un mod√®le d'araign√©e √† quatre pattes.  Dans le simulateur Mujoco, cela ressemble √† ceci: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yp/ad/dg/ypaddgwogfekvvjemaf3vseohdi.png"></div><br><br>  Nous lui avons confi√© la t√¢che de courir √† la vitesse la plus √©lev√©e possible dans une direction donn√©e - par exemple, vers la droite.  Le nombre de param√®tres observ√©s pour une araign√©e est un vecteur √† 39 dimensions, qui enregistre la position et la vitesse de tous ses membres.  Contrairement au r√©seau neuronal pour le tennis de table, o√π il n'y avait qu'un seul neurone √† la sortie, il y en a huit √† la sortie (puisque l'araign√©e dans ce mod√®le a 8 articulations). <br><br>  Dans de tels mod√®les simples, diverses hypoth√®ses sur la technique d'enseignement peuvent √™tre test√©es.  Par exemple, comparons la vitesse d'apprentissage de l'ex√©cution, selon le type de r√©seau neuronal.  Que ce soit un r√©seau neuronal √† une seule couche, un r√©seau neuronal √† trois couches, un r√©seau convolutionnel et un r√©seau r√©current: <br><br><img src="https://habrastorage.org/webt/sq/zl/xu/sqzlxuj-k_nts13x7x5veh-56ay.png"><br><br>  La conclusion peut √™tre tir√©e comme suit: puisque le mod√®le d'araign√©e et la t√¢che sont assez simples, les r√©sultats de la formation sont approximativement les m√™mes pour diff√©rents mod√®les.  Un r√©seau √† trois couches est trop complexe et apprend donc moins bien. <br><br>  Malgr√© le fait que le simulateur fonctionne avec un mod√®le d'araign√©e simple, selon la t√¢che pos√©e √† l'araign√©e, la formation peut durer des jours.  Dans ce cas, animons plusieurs centaines d'araign√©es sur une surface en m√™me temps au lieu d'une et apprenons des donn√©es que nous recevrons de tout le monde.  Nous allons donc acc√©l√©rer la formation de plusieurs centaines de fois.  Voici un exemple du moteur Flex. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/p4/6l/4j/p46l4jyulhhspwa9e5z_4oe40oq.png"></div><br><br>  La seule chose qui a chang√© en termes d'optimisation du r√©seau neuronal est la collecte de donn√©es.  Lorsque nous n'avons ex√©cut√© qu'une seule araign√©e, nous avons re√ßu des donn√©es s√©quentiellement.  Une course apr√®s l'autre. <br><br><img src="https://habrastorage.org/webt/8t/cd/hr/8tcdhrcxqbx4mk3w99lacw2tfbu.png"><br><br>  Maintenant, il peut arriver que certaines araign√©es commencent √† peine la course, tandis que d'autres courent depuis longtemps. <br><br><img src="https://habrastorage.org/webt/6u/i6/jv/6ui6jv9y3zmbdy3cwfweuv1drt4.png"><br><br>  Nous en tiendrons compte lors de l'optimisation du r√©seau neuronal.  Sinon, tout reste le m√™me.  En cons√©quence, nous obtenons une acc√©l√©ration de l'entra√Ænement des centaines de fois, selon le nombre d'araign√©es qui sont simultan√©ment sur l'√©cran. <br><br>  Puisque nous avons un simulateur efficace, essayons de r√©soudre des probl√®mes plus complexes.  Par exemple, courir sur un terrain accident√©. <br><br><img src="https://habrastorage.org/webt/zo/sq/dn/zosqdnbygiz_2p-cvxl4hvtuvqw.png"><br><br>  √âtant donn√© que l'environnement dans ce cas est devenu plus agressif, modifions et compliquons les t√¢ches pendant la formation.  C‚Äôest difficile √† apprendre, mais facile au combat.  Par exemple, toutes les quelques minutes pour changer le terrain.  De plus, dirigons les agents externes vers l'agent.  Par exemple, jetons des balles sur lui et tournons le vent.  L'agent apprend alors √† courir m√™me sur des surfaces qu'il n'a jamais rencontr√©es.  Par exemple, montez des escaliers. <br><br><img src="https://habrastorage.org/webt/zc/-7/bq/zc-7bqzt5kcltfgrwfc9piw3mli.png"><br><br>  Puisque nous avons si bien appris √† courir dans les simulations, examinons les techniques d'entra√Ænement au renforcement dans les disciplines comp√©titives.  Par exemple, dans les jeux de tir.  La plate-forme VizDoom offre un monde dans lequel vous pouvez tirer, collecter des armes et reconstituer la sant√©.  Dans ce jeu, nous utiliserons √©galement un r√©seau de neurones.  Seulement maintenant, elle aura cinq sorties: quatre pour le mouvement et une pour le tir. <br><br>  Pour que la formation soit efficace, prenons-la progressivement.  Du simple au complexe.  √Ä l'entr√©e, le r√©seau neuronal re√ßoit une image, et avant de commencer √† faire quelque chose de conscient, il doit apprendre √† comprendre en quoi consiste le monde.  En √©tudiant dans des sc√©narios simples, elle apprendra √† comprendre quels objets habitent le monde et comment interagir avec eux.  Commen√ßons par le tiret: <br><br><img src="https://habrastorage.org/webt/99/cy/xm/99cyxm9xxkmd3wg9zryiguyqnoe.png"><br><br>  Ayant ma√Ætris√© ce sc√©nario, l'agent comprendra qu'il y a des ennemis, et ils devraient √™tre abattus, car vous obtenez des points pour eux.  Ensuite, nous le formerons dans un sc√©nario o√π la sant√© diminue constamment et vous devez la reconstituer. <br><br><img src="https://habrastorage.org/webt/06/gs/jr/06gsjr-tuvruizcpmy3da0gsyq4.jpeg"><br><br>  Ici, il apprendra qu'il a la sant√© et doit √™tre reconstitu√©, car en cas de d√©c√®s, l'agent re√ßoit une r√©compense n√©gative.  De plus, il apprendra que si vous vous rapprochez du sujet, vous pouvez le r√©cup√©rer.  Dans le premier sc√©nario, l'agent ne pouvait pas se d√©placer. <br><br>  Et dans le troisi√®me sc√©nario final, laissons-le tirer avec les bots programm√©s selon les r√®gles du jeu afin qu'il puisse affiner ses comp√©tences. <br><br><img src="https://habrastorage.org/webt/dn/pz/iq/dnpziqqditttkptyp7zflcdh4ag.png"><br><br>  Lors de la formation dans ce sc√©nario, la s√©lection correcte des r√©compenses que l'agent re√ßoit est tr√®s importante.  Par exemple, si vous donnez une r√©compense uniquement pour les rivaux vaincus, le signal sera tr√®s rare: s'il y a peu de joueurs autour, nous recevrons des points toutes les quelques minutes.  Par cons√©quent, utilisons la combinaison de r√©compenses pr√©c√©dentes.  L'agent recevra une r√©compense pour chaque action utile, qu'il s'agisse d'am√©liorer la sant√©, de s√©lectionner des cartouches ou de frapper un adversaire. <br><br><blockquote>  En cons√©quence, un agent form√© avec des r√©compenses bien choisies est plus fort que ses adversaires les plus exigeants en termes de calcul.  En 2016, un tel syst√®me a remport√© le concours VizDoom avec une marge de plus de la moiti√© des points marqu√©s √† la deuxi√®me place.  L'√©quipe finaliste a √©galement utilis√© un r√©seau de neurones, uniquement avec un grand nombre de couches et des informations suppl√©mentaires du moteur de jeu pendant l'entra√Ænement.  Par exemple, des informations sur la pr√©sence d‚Äôennemis dans le champ de vision de l‚Äôagent. </blockquote><br>  Nous avons examin√© des approches pour r√©soudre des probl√®mes, o√π il est important de prendre des d√©cisions.  Mais de nombreuses t√¢ches avec cette approche resteront non r√©solues.  Par exemple, le jeu de qu√™te Montezuma Revenge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s9/vp/_9/s9vp_9botnvsnfqcaxutbv-2pn0.jpeg"></div><br><br>  Ici, vous devez rechercher des cl√©s pour ouvrir les portes des chambres voisines.  Nous obtenons rarement les cl√©s et nous ouvrons les chambres encore moins souvent.  Il est √©galement important de ne pas √™tre distrait par des objets √©trangers.  Si vous entra√Ænez le syst√®me comme nous l'avons fait dans les t√¢ches pr√©c√©dentes et donnez des r√©compenses aux ennemis battus, il assommera simplement le cr√¢ne roulant encore et encore et n'examinera pas la carte.  Si vous √™tes int√©ress√©, je peux parler de la r√©solution de ces probl√®mes dans un article s√©par√©. <br><br>  <b>Vous pouvez √©couter le discours de Vladimir Ivanov √† la conf√©rence d'Amnesty International le 22 novembre</b> .  Un programme d√©taill√© et des billets sont disponibles sur le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">site officiel de l'</a> √©v√©nement. <br><br>  Lisez l'interview de Vladimir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr428329/">https://habr.com/ru/post/fr428329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr428313/index.html">Telegram sur MacOS [vraisemblablement] stocke √©galement localement la correspondance sous une forme accessible</a></li>
<li><a href="../fr428315/index.html">5 peurs des d√©veloppeurs que nous avons surmont√©es</a></li>
<li><a href="../fr428317/index.html">React hooks - gagner ou perdre?</a></li>
<li><a href="../fr428321/index.html">Analyse pr√©dictive des donn√©es - mod√©lisation et validation</a></li>
<li><a href="../fr428327/index.html">Que rechercher: R√®glement europ√©en sur l'identification √©lectronique eIDAS</a></li>
<li><a href="../fr428333/index.html">R√©sultats du Hackathon AI RAIF Hackathon 2018</a></li>
<li><a href="../fr428335/index.html">Mise √† jour du raccourci Siri</a></li>
<li><a href="../fr428337/index.html">JavaScript divertissant: sans accolades</a></li>
<li><a href="../fr428339/index.html">Ne l'automatisez pas: mauvais conseils commerciaux</a></li>
<li><a href="../fr428341/index.html">Technologie Qsan RAID EE</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>