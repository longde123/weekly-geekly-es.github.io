<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçß üßöüèΩ üêö Imers√£o em redes neurais convolucionais: transfer√™ncia de aprendizado ‚úäüèæ ‚è™ üë©üèª‚Äçüîß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="O curso completo em russo pode ser encontrado neste link . 
 O curso de ingl√™s original est√° dispon√≠vel neste link . 



 Conte√∫do 


1. Entrevista co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Imers√£o em redes neurais convolucionais: transfer√™ncia de aprendizado</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467967/"><p>  O curso completo em russo pode ser encontrado <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . <br>  O curso de ingl√™s original est√° dispon√≠vel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . </p><br><p><img src="https://habrastorage.org/webt/wu/ie/7c/wuie7cgpktklm4bytoweu7ki0oq.jpeg"></p><a name="habracut"></a><br><h1>  Conte√∫do </h1><br><ol><li>  Entrevista com Sebastian Trun </li><li>  1. Introdu√ß√£o </li><li>  Transferir modelo de aprendizagem </li><li>  MobileNet </li><li>  CoLab: Gatos contra c√£es com treinamento de transfer√™ncia </li><li>  Mergulhando em redes neurais convolucionais </li><li>  Parte pr√°tica: determina√ß√£o de cores com a transfer√™ncia de treinamento </li><li>  Sum√°rio </li></ol><br><h1>  Entrevista com Sebastian Trun </h1><br><p>  - Esta √© a li√ß√£o 6 e √© totalmente dedicada √† transfer√™ncia de aprendizado.  Transfer√™ncia de aprendizado √© o processo de usar um modelo existente com pouco refinamento para novas tarefas.  A transfer√™ncia de treinamento ajuda a reduzir o tempo de treinamento do modelo, aumentando a efici√™ncia do aprendizado desde o in√≠cio.  Sebastian, o que voc√™ acha da transfer√™ncia de treinamento?  Voc√™ j√° foi capaz de usar a metodologia de transfer√™ncia de ensino em seu trabalho e pesquisa? <br>  - Minha disserta√ß√£o foi dedicada apenas ao t√≥pico da transfer√™ncia de treinamento e foi chamada " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Explica√ß√£o com base na transfer√™ncia de treinamento</a> ".  Quando est√°vamos trabalhando em uma disserta√ß√£o, a id√©ia era que √© poss√≠vel ensinar a distinguir todos os outros objetos desse tipo em um objeto (conjunto de dados, entidade) em v√°rias varia√ß√µes e formatos.  No trabalho, utilizamos o algoritmo desenvolvido, que distinguia as principais caracter√≠sticas (atributos) do objeto e podia compar√°-las com outro objeto.  Bibliotecas como o Tensorflow j√° v√™m com modelos pr√©-treinados. <br>  - Sim, na Tensorflow, temos um conjunto completo de modelos pr√©-treinados que voc√™ pode usar para resolver problemas pr√°ticos.  Falaremos sobre conjuntos prontos um pouco mais tarde. <br>  - sim sim!  Se voc√™ pensar bem, as pessoas est√£o envolvidas na transfer√™ncia de treinamento o tempo todo ao longo de suas vidas. <br>  - Podemos dizer que, gra√ßas ao m√©todo de transfer√™ncia de treinamento, nossos novos alunos em algum momento n√£o precisar√£o saber algo sobre aprendizado de m√°quina porque ser√° suficiente para conectar um modelo j√° preparado e us√°-lo? <br>  - Programa√ß√£o √© escrever linha por linha, damos comandos ao computador.  Nosso objetivo √© garantir que todos no planeta possam e possam programar, fornecendo ao computador apenas exemplos de dados de entrada.  Concorde que, se voc√™ deseja ensinar um computador a distinguir gatos de c√£es, √© bastante dif√≠cil encontrar 100 mil imagens diferentes de gatos e 100 mil imagens diferentes de c√£es. Gra√ßas √† transfer√™ncia de treinamento, voc√™ pode resolver esse problema em v√°rias linhas. <br>  - Sim √© mesmo!  Obrigado pelas respostas e vamos finalmente prosseguir com o aprendizado. </p><br><h1>  1. Introdu√ß√£o </h1><br><p>  - Ol√° e bem vindo de volta! <br>  - Na √∫ltima vez, treinamos uma rede neural convolucional para classificar gatos e c√£es na imagem.  Nossa primeira rede neural foi treinada novamente, portanto seu resultado n√£o foi t√£o alto - cerca de 70% de precis√£o.  Depois disso, implementamos a extens√£o e a elimina√ß√£o de dados (desconex√£o arbitr√°ria de neur√¥nios), o que nos permitiu aumentar a precis√£o das previs√µes em at√© 80%. <br>  - Apesar de 80% parecer um excelente indicador, o erro de 20% ainda √© muito grande.  Certo?  O que podemos fazer para aumentar a precis√£o da classifica√ß√£o?  Nesta li√ß√£o, usaremos a t√©cnica de transfer√™ncia de conhecimento (transfer√™ncia do modelo de conhecimento), que nos permitir√° usar o modelo desenvolvido por especialistas e treinado em grandes conjuntos de dados.  Como veremos na pr√°tica, transferindo o modelo de conhecimento, podemos alcan√ßar 95% de precis√£o na classifica√ß√£o.  Vamos come√ßar! </p><br><h1>  Transfer√™ncia de modelo de aprendizagem </h1><br><p>  Em 2012, a rede neural AlexNet revolucionou o mundo do aprendizado de m√°quina e popularizou o uso de redes neurais convolucionais para classifica√ß√£o ao vencer o desafio de reconhecimento visual ImageNet em larga escala. </p><br><p><img src="https://habrastorage.org/webt/fs/xx/di/fsxxdicwkitfkxibie8kkjcyexu.png"></p><br><p>  Depois disso, a luta come√ßou a desenvolver redes neurais mais precisas e eficientes que poderiam superar o AlexNet nas tarefas de classifica√ß√£o de imagens do conjunto de dados do ImageNet. </p><br><p><img src="https://habrastorage.org/webt/oe/t0/ov/oet0ovye40p1cziih64hpmbhuus.png"></p><br><p>  Por v√°rios anos, redes neurais foram desenvolvidas para lidar melhor com a tarefa de classifica√ß√£o do que o AlexNet - Inception e ResNet. <br>  Concorda que seria √≥timo poder tirar proveito dessas redes neurais j√° treinadas em grandes conjuntos de dados da ImageNet e us√°-las em seu classificador para c√£es e gatos? </p><br><p>  Acontece que n√≥s podemos fazer isso!  A t√©cnica √© chamada de transfer√™ncia de aprendizado.  A id√©ia principal do m√©todo de transfer√™ncia do modelo de treinamento √© baseada no fato de que, ap√≥s treinar uma rede neural em um grande conjunto de dados, podemos aplicar o modelo obtido a um conjunto de dados que este modelo ainda n√£o encontrou.  √â por isso que a t√©cnica √© chamada de transfer√™ncia de aprendizado - transfer√™ncia do processo de aprendizado de um conjunto de dados para outro. </p><br><p>  Para aplicarmos a metodologia de transfer√™ncia do modelo de treinamento, precisamos alterar a √∫ltima camada de nossa rede neural convolucional: </p><br><p><img src="https://habrastorage.org/webt/3j/-g/g3/3j-gg3yxm9kvrtphiswnho2jgtc.png"></p><br><p>  Executamos essa opera√ß√£o porque cada conjunto de dados consiste em um n√∫mero diferente de classes de sa√≠da.  Por exemplo, os conjuntos de dados no ImageNet cont√™m 1000 classes de sa√≠da diferentes.  FashionMNIST cont√©m 10 classes.  Nosso conjunto de dados de classifica√ß√£o consiste em apenas 2 classes - gatos e c√£es. </p><br><p><img src="https://habrastorage.org/webt/5e/pm/ej/5epmejbamklkdfgzzw9v8rzb1ts.png"></p><br><p>  √â por isso que √© necess√°rio alterar a √∫ltima camada de nossa rede neural convolucional para que ela contenha o n√∫mero de sa√≠das que corresponderiam ao n√∫mero de classes no novo conjunto. </p><br><p><img src="https://habrastorage.org/webt/cg/mk/fz/cgmkfzxqmyqbdzsmvfdppjhsl9e.png"></p><br><p>  Tamb√©m precisamos garantir que n√£o alteremos o modelo pr√©-treinado durante o processo de treinamento.  A solu√ß√£o √© desativar as vari√°veis ‚Äã‚Äãdo modelo pr√©-treinado - simplesmente proibimos que o algoritmo atualize os valores da distribui√ß√£o direta e reversa para alter√°-los. <br>  Esse processo √© chamado de "congelamento do modelo". </p><br><p><img src="https://habrastorage.org/webt/8z/oz/0n/8zoz0nenaad4-lgezhhia25k_18.png"></p><br><p>  Ao ‚Äúcongelar‚Äù os par√¢metros do modelo pr√©-treinado, permitimos aprender apenas a √∫ltima camada da rede de classifica√ß√£o, os valores das vari√°veis ‚Äã‚Äãdo modelo pr√©-treinado permanecem inalterados. </p><br><p>  Outra vantagem indiscut√≠vel dos modelos pr√©-treinados √© que reduzimos o tempo de treinamento treinando apenas a √∫ltima camada com um n√∫mero significativamente menor de vari√°veis, e n√£o o modelo inteiro. </p><br><p>  Se n√£o congelarmos as vari√°veis ‚Äã‚Äãdo modelo pr√©-treinado, durante o processo de treinamento, os valores das vari√°veis ‚Äã‚Äãser√£o alterados no novo conjunto de dados.  Isso ocorre porque os valores das vari√°veis ‚Äã‚Äãna √∫ltima camada da classifica√ß√£o ser√£o preenchidos com valores aleat√≥rios.  Devido a valores aleat√≥rios na √∫ltima camada, nosso modelo cometer√° grandes erros na classifica√ß√£o, o que, por sua vez, acarretar√° fortes mudan√ßas nos pesos iniciais no modelo pr√©-treinado, o que √© extremamente indesej√°vel para n√≥s. </p><br><p><img src="https://habrastorage.org/webt/wp/uz/km/wpuzkmnan5rahfg3irexdsvrstm.png"></p><br><p>  √â por esse motivo que devemos sempre lembrar que, ao usar modelos existentes, os valores das vari√°veis ‚Äã‚Äãdevem ser "congelados" e a necessidade de treinar um modelo pr√©-treinado deve ser desativada. </p><br><p>  Agora que sabemos como a transfer√™ncia do modelo de treinamento funciona, basta escolher uma rede neural pr√©-treinada para uso em nosso pr√≥prio classificador!  Isso faremos na pr√≥xima parte. </p><br><h1>  MobileNet </h1><br><p>  Como mencionamos anteriormente, foram desenvolvidas redes neurais extremamente eficientes que apresentaram altos resultados nos conjuntos de dados ImageNet - AlexNet, Inception, Resonant.  Essas redes neurais s√£o redes muito profundas e cont√™m milhares e at√© milh√µes de par√¢metros.  Um grande n√∫mero de par√¢metros permite que a rede aprenda padr√µes mais complexos e, assim, obtenha maior precis√£o na classifica√ß√£o.  Um grande n√∫mero de par√¢metros de treinamento da rede neural afeta a velocidade do aprendizado, a quantidade de mem√≥ria necess√°ria para armazenar a rede e a complexidade dos c√°lculos. </p><br><p>  Nesta li√ß√£o, usaremos a moderna rede neural convolucional MobileNet.  MobileNet √© uma arquitetura de rede neural convolucional eficiente que reduz a quantidade de mem√≥ria usada para computa√ß√£o, mantendo alta precis√£o das previs√µes.  √â por isso que o MobileNet √© ideal para uso em dispositivos m√≥veis com uma quantidade limitada de mem√≥ria e recursos de computa√ß√£o. </p><br><p>  O MobileNet foi desenvolvido pelo Google e treinado no conjunto de dados ImageNet. </p><br><p>  Como o MobileNet foi treinado em 1.000 classes a partir do conjunto de dados ImageNet, o MobileNet possui 1.000 classes de sa√≠da, em vez das duas de que precisamos - um gato e um cachorro. </p><br><p><img src="https://habrastorage.org/webt/he/2f/ox/he2foxizd_xmt7rijxumteg-94k.png"></p><br><p>  Para concluir a transfer√™ncia do treinamento, pr√©-carregamos o vetor de recurso sem uma camada de classifica√ß√£o: </p><br><p><img src="https://habrastorage.org/webt/1b/o2/no/1bo2nop9cpz3ebag_jcyfrgje7w.png"></p><br><p>  No Tensorflow, um vetor de recurso carregado pode ser usado como uma camada Keras comum com dados de entrada de um determinado tamanho. </p><br><p>  Como o MobileNet foi treinado no conjunto de dados ImageNet, precisaremos trazer o tamanho dos dados de entrada para aqueles que foram usados ‚Äã‚Äãno processo de treinamento.  No nosso caso, o MobileNet foi treinado em imagens RGB de tamanho fixo de 224x224 px. </p><br><p>  O TensorFlow cont√©m um reposit√≥rio pr√©-treinado chamado TensorFlow Hub. </p><br><p><img src="https://habrastorage.org/webt/o5/we/9d/o5we9dvkdtaomahwj4nzomgquca.png"></p><br><p>  O TensorFlow Hub cont√©m alguns modelos pr√©-treinados nos quais a √∫ltima camada de classifica√ß√£o foi exclu√≠da da arquitetura da rede neural para posterior reutiliza√ß√£o. </p><br><p>  Voc√™ pode usar o TensorFlow Hub no c√≥digo em v√°rias linhas: </p><br><p><img src="https://habrastorage.org/webt/1h/ai/hg/1haihgg1llinsfv_wyhde4z0egy.png"></p><br><p>  Basta especificar a URL do vetor de recurso do modelo de treinamento desejado e, em seguida, incorporar o modelo em nosso classificador com a √∫ltima camada com o n√∫mero desejado de classes de sa√≠da.  √â a √∫ltima camada que ser√° submetida a treinamento e altera√ß√£o dos valores dos par√¢metros.  A compila√ß√£o e o treinamento do nosso novo modelo s√£o realizados da mesma maneira que fizemos antes: </p><br><p><img src="https://habrastorage.org/webt/uh/cr/5e/uhcr5esktfnxtxwxfgutvttxbxk.png"></p><br><p>  Vamos ver como isso realmente funciona e escrever o c√≥digo apropriado. </p><br><h1>  CoLab: Gatos contra c√£es com treinamento de transfer√™ncia </h1><br><p>  Link para o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CoLab em russo</a> e o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CoLab em ingl√™s</a> . </p><br><p>  O TensorFlow Hub √© um reposit√≥rio com modelos pr√©-treinados que podemos usar. </p><br><p>  A transfer√™ncia de aprendizado √© um processo no qual pegamos um modelo pr√©-treinado e o expandimos para executar uma tarefa espec√≠fica.  Ao mesmo tempo, deixamos intactas a parte do modelo pr√©-treinado que integramos na rede neural, mas apenas treinamos as √∫ltimas camadas de sa√≠da para obter o resultado desejado. </p><br><p>  Nesta parte pr√°tica, testaremos as duas op√ß√µes. </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Este link</a> permite explorar a lista inteira de modelos dispon√≠veis. </p><br><p>  <strong>Nesta parte da Colab</strong> </p><br><ol><li>  Usaremos o modelo do TensorFlow Hub para previs√µes; </li><li>  Usaremos o modelo TensorFlow Hub para o conjunto de dados de c√£es e gatos; </li><li>  Vamos transferir o treinamento usando o modelo do TensorFlow Hub. </li></ol><br><p> Antes de prosseguir com a implementa√ß√£o da parte pr√°tica atual, recomendamos redefinir o <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p>  <strong>Importa√ß√µes de bibliotecas</strong> </p><br><p>  Nesta parte pr√°tica, usaremos v√°rios recursos da biblioteca TensorFlow que ainda n√£o est√£o no release oficial.  √â por isso que instalaremos primeiro a vers√£o do TensorFlow e TensorFlow Hub para desenvolvedores. </p><br><p>  A instala√ß√£o da vers√£o dev do TensorFlow ativa automaticamente a vers√£o mais recente instalada.  Depois de terminarmos de lidar com esta parte pr√°tica, recomendamos restaurar as configura√ß√µes do TensorFlow e retornar √† vers√£o est√°vel atrav√©s do item de menu <code>Runtime -&gt; Reset all runtimes...</code>  A execu√ß√£o deste comando redefinir√° todas as configura√ß√µes do ambiente para as originais. </p><br><pre> <code class="python hljs">!pip install tf-nightly-gpu !pip install <span class="hljs-string"><span class="hljs-string">"tensorflow_hub==0.4.0"</span></span> !pip install -U tensorflow_datasets</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1) Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7) Collecting tf-estimator-nightly (from tf-nightly-gpu) Downloading https://files.pythonhosted.org/packages/ea/72/f092fc631ef2602fd0c296dcc4ef6ef638a6a773cb9fdc6757fecbfffd33/tf_estimator_nightly-1.14.0.dev2019092201-py2.py3-none-any.whl (450kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450kB 45.9MB/s Requirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.5) Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2) Requirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0) Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.0.1) Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.6) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tf-nightly-gpu) (2.8.0) Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (3.1.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (41.2.0) Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly&lt;1.16.0a0,&gt;=1.15.0a0-&gt;tf-nightly-gpu) (0.15.6) Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly-gpu Successfully installed tb-nightly-1.15.0a20190911 tf-estimator-nightly-1.14.0.dev2019092201 tf-nightly-gpu-1.15.0.dev20190821 Collecting tensorflow_hub==0.4.0 Downloading https://files.pythonhosted.org/packages/10/5c/6f3698513cf1cd730a5ea66aec665d213adf9de59b34f362f270e0bd126f/tensorflow_hub-0.4.0-py2.py3-none-any.whl (75kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 5.0MB/s Requirement already satisfied: protobuf&gt;=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (3.7.1) Requirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.16.5) Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.4.0) (1.12.0) Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.4.0-&gt;tensorflow_hub==0.4.0) (41.2.0) Installing collected packages: tensorflow-hub Found existing installation: tensorflow-hub 0.6.0 Uninstalling tensorflow-hub-0.6.0: Successfully uninstalled tensorflow-hub-0.6.0 Successfully installed tensorflow-hub-0.4.0 Collecting tensorflow_datasets Downloading https://files.pythonhosted.org/packages/6c/34/ff424223ed4331006aaa929efc8360b6459d427063dc59fc7b75d7e4bab3/tensorflow_datasets-1.2.0-py3-none-any.whl (2.3MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.3MB 4.9MB/s Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.16.0) Requirement already satisfied, skipping upgrade: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.11.2) Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.0) Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.16.5) Requirement already satisfied, skipping upgrade: requests&gt;=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.21.0) Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.28.1) Requirement already satisfied, skipping upgrade: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.1) Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (5.4.8) Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.2.1) Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.8.0) Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.14.0) Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.0) Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0) Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (19.1.0) Requirement already satisfied, skipping upgrade: idna&lt;2.9,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.8) Requirement already satisfied, skipping upgrade: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2019.6.16) Requirement already satisfied, skipping upgrade: chardet&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.0.4) Requirement already satisfied, skipping upgrade: urllib3&lt;1.25,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (1.24.3) Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf&gt;=3.6.1-&gt;tensorflow_datasets) (41.2.0) Requirement already satisfied, skipping upgrade: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.6.0) Installing collected packages: tensorflow-datasets Successfully installed tensorflow-datasets-1.2.0</code> </pre> <br><p>  J√° vimos e usamos algumas importa√ß√µes antes.  A partir do new - import <code>tensorflow_hub</code> , que instalamos e que usaremos nesta parte pr√°tica. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0. Please upgrade your code to TensorFlow 2.0: * https://www.tensorflow.org/beta/guide/migration_guide Or install the latest stable TensorFlow 1.X release: * `pip install -U "tensorflow==1.*"` Otherwise your code may be broken by the change.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p>  <strong>Parte 1: use o MobileNet do TensorFlow Hub para previs√µes</strong> </p><br><p>  Nesta parte do CoLab, vamos pegar um modelo pr√©-treinado, carreg√°-lo no Keras e test√°-lo. </p><br><p>  O modelo que usamos √© o MobileNet v2 (em vez do MobileNet, qualquer outro modelo de classificador de imagem compat√≠vel com tf2 com tfhub.dev pode ser usado). </p><br><p>  <strong>Classificador de download</strong> </p><br><p>  Fa√ßa o download do modelo MobileNet e crie um modelo Keras a partir dele.  O MobileNet na entrada espera receber uma imagem de 224x224 pixels de tamanho com 3 canais de cores (RGB). </p><br><pre> <code class="python hljs">CLASSIFIER_URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span></span> IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> model = tf.keras.Sequential([ hub.KerasLayer(CLASSIFIER_URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>)) ])</code> </pre> <br><p>  <strong>Execute o classificador em uma √∫nica imagem</strong> </p><br><p>  O MobileNet foi treinado no conjunto de dados ImageNet.  O ImageNet cont√©m 1000 classes de sa√≠da e uma dessas classes √© um uniforme militar.  Vamos encontrar a imagem na qual o uniforme militar estar√° localizado e que n√£o far√° parte do kit de treinamento do ImageNet para verificar a precis√£o da classifica√ß√£o. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PIL.Image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Image grace_hopper = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'image.jpg'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span></span>) grace_hopper = Image.open(grace_hopper).resize((IMAGE_RES, IMAGE_RES)) grace_hopper</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg 65536/61306 [================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/8g/gn/0u/8ggn0ur3pmnr_rxvezfdo4vrwcc.png"></p><br><pre> <code class="python hljs">grace_hopper = np.array(grace_hopper)/<span class="hljs-number"><span class="hljs-number">255.0</span></span> grace_hopper.shape</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">(224, 224, 3)</code> </pre> <br><p>  Lembre-se de que os modelos sempre recebem um conjunto (bloco) de imagens para processamento na entrada.  No c√≥digo abaixo, adicionamos uma nova dimens√£o - o tamanho do bloco. </p><br><pre> <code class="python hljs">result = model.predict(grace_hopper[np.newaxis, ...]) result.shape</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">(1, 1001)</code> </pre> <br><p>  O resultado da previs√£o foi um vetor com um tamanho de 1.001 elementos, em que cada valor representa a probabilidade de o objeto na imagem pertencer a uma determinada classe. </p><br><p>  A posi√ß√£o do valor m√°ximo de probabilidade pode ser encontrada usando a fun√ß√£o <code>argmax</code> .  No entanto, h√° uma pergunta que ainda n√£o respondemos - como podemos determinar a qual classe um elemento pertence com probabilidade m√°xima? </p><br><pre> <code class="python hljs">predicted_class = np.argmax(result[<span class="hljs-number"><span class="hljs-number">0</span></span>], axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">653</code> </pre> <br><p>  <strong>Decifrando previs√µes</strong> </p><br><p>  Para determinarmos a classe √† qual as previs√µes se relacionam, carregamos a lista de tags ImageNet e, pelo √≠ndice com fidelidade m√°xima, determinamos a classe √† qual a previs√£o se refere. </p><br><pre> <code class="python hljs">labels_path = tf.keras.utils.get_file(<span class="hljs-string"><span class="hljs-string">'ImageNetLabels.txt'</span></span>,<span class="hljs-string"><span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'</span></span>) imagenet_labels = np.array(open(labels_path).read().splitlines()) plt.imshow(grace_hopper) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) predicted_class_name = imagenet_labels[predicted_class] _ = plt.title(<span class="hljs-string"><span class="hljs-string">"Prediction: "</span></span> + predicted_class_name.title())</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt 16384/10484 [==============================================] - 0s 0us/step</code> </pre> <br><p><img src="https://habrastorage.org/webt/ai/1k/rt/ai1krt6mkpcafhb5jx8ozf6mq8s.png"></p><br><p>  Bingo!  Nosso modelo identificou corretamente o uniforme militar. </p><br><p>  <strong>Parte 2: use o modelo do TensorFlow Hub para um conjunto de dados de gatos e c√£es</strong> </p><br><p>  Agora vamos usar a vers√£o completa do modelo MobileNet e ver como ele lida com o conjunto de dados de c√£es e gatos. </p><br><p>  <strong>Conjunto de dados</strong> </p><br><p>  Podemos usar os conjuntos de dados TensorFlow para baixar um conjunto de dados de gatos e c√£es. </p><br><pre> <code class="python hljs">splits = tfds.Split.ALL.subsplit(weighted=(<span class="hljs-number"><span class="hljs-number">80</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) splits, info = tfds.load(<span class="hljs-string"><span class="hljs-string">'cats_vs_dogs'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split = splits) (train_examples, validation_examples) = splits num_examples = info.splits[<span class="hljs-string"><span class="hljs-string">'train'</span></span>].num_examples num_classes = info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset cats_vs_dogs (786.68 MiB) to /root/tensorflow_datasets/cats_vs_dogs/2.0.1... /usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning) WARNING:absl:1738 images were corrupted and were skipped Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/2.0.1. Subsequent calls will reuse this data.</code> </pre><br><p>  Nem todas as imagens em um conjunto de dados de gato e cachorro s√£o do mesmo tamanho. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example_image <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(train_examples.take(<span class="hljs-number"><span class="hljs-number">3</span></span>)): print(<span class="hljs-string"><span class="hljs-string">"Image {} shape: {}"</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example_image[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape))</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Image 1 shape: (500, 343, 3) Image 2 shape: (375, 500, 3) Image 3 shape: (375, 500, 3)</code> </pre> <br><p>  Portanto, as imagens do conjunto de dados obtidos requerem redu√ß√£o para um tamanho √∫nico, o que o modelo MobileNet espera na entrada - 224 x 224. </p><br><p>  A fun√ß√£o <code>.repeat()</code> e <code>steps_per_epoch</code> n√£o s√£o necess√°rias aqui, mas permitem economizar cerca de 15 segundos por itera√ß√£o de treinamento, porque  o buffer tempor√°rio deve ser inicializado apenas uma vez no in√≠cio do processo de aprendizado. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES)) / <span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = train_examples.shuffle(num_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  <strong>Execute o classificador em conjuntos de imagens</strong> </p><br><p>  Deixe-me lembr√°-lo de que, nesta etapa, ainda existe uma vers√£o completa da rede MobileNet pr√©-treinada, que cont√©m 1.000 classes de sa√≠da poss√≠veis.  O ImageNet cont√©m um grande n√∫mero de imagens de c√£es e gatos; portanto, vamos tentar inserir uma das imagens de teste do nosso conjunto de dados e ver qual previs√£o o modelo nos dar√°. </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches.take(<span class="hljs-number"><span class="hljs-number">1</span></span>))) image_batch = image_batch.numpy() label_batch = label_batch.numpy() result_batch = model.predict(image_batch) predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>)] predicted_class_names</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">array(['Persian cat', 'mink', 'Siamese cat', 'tabby', 'Bouvier des Flandres', 'dishwasher', 'Yorkshire terrier', 'tiger cat', 'tabby', 'Egyptian cat', 'Egyptian cat', 'tabby', 'dalmatian', 'Persian cat', 'Border collie', 'Newfoundland', 'tiger cat', 'Siamese cat', 'Persian cat', 'Egyptian cat', 'tabby', 'tiger cat', 'Labrador retriever', 'German shepherd', 'Eskimo dog', 'kelpie', 'mink', 'Norwegian elkhound', 'Labrador retriever', 'Egyptian cat', 'computer keyboard', 'boxer'], dtype='&lt;U30')</code> </pre> <br><p>  Os r√≥tulos s√£o semelhantes aos nomes de ra√ßas de c√£es e gatos.  Vamos agora exibir algumas imagens do nosso conjunto de dados de c√£es e gatos e colocar um r√≥tulo previsto em cada um deles. </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) plt.title(predicted_class_names[n]) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"ImageNet predictions"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/fc/af/w-/fcafw-kdtzotffq7k-nihlntuda.png"></p><br><p>  <strong>Parte 3: Implementar a transfer√™ncia de aprendizado com o hub TensorFlow</strong> </p><br><p>  Agora vamos usar o TensorFlow Hub para transferir a aprendizagem de um modelo para outro. </p><br><p>  No processo de transfer√™ncia de treinamento, reutilizamos um modelo pr√©-treinado alterando sua √∫ltima camada ou v√°rias camadas e, em seguida, iniciamos o processo de treinamento novamente em um novo conjunto de dados. </p><br><p>  No TensorFlow Hub, voc√™ pode encontrar n√£o apenas modelos pr√©-treinados completos (com a √∫ltima camada), mas tamb√©m modelos sem a √∫ltima camada de classifica√ß√£o.  Este √∫ltimo pode ser facilmente usado para transferir treinamento.  Continuaremos a usar o MobileNet v2 pelo simples motivo de que, nas partes subseq√ºentes de nosso curso, transferiremos esse modelo e o iniciaremos em um dispositivo m√≥vel usando o TensorFlow Lite. </p><br><p>  Tamb√©m continuaremos a usar o conjunto de dados de c√£es e gatos, para que tenhamos a oportunidade de comparar o desempenho desse modelo com os que implementamos do zero. </p><br><p>  Observe que chamamos o modelo parcial com o Hub TensorFlow (sem a √∫ltima camada de classifica√ß√£o) <code>feature_extractor</code> .  Esse nome √© explicado pelo fato de o modelo aceitar dados como entrada e transform√°-los em um conjunto finito de propriedades selecionadas (caracter√≠sticas).  Assim, nosso modelo identificou o conte√∫do da imagem, mas n√£o produziu a distribui√ß√£o final de probabilidade sobre as classes de sa√≠da.  O modelo extraiu um conjunto de propriedades da imagem. </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2'</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p>  Vamos executar um conjunto de imagens atrav√©s do <code>feature_extractor</code> e ver o formul√°rio resultante (formato de sa√≠da).  32 - o n√∫mero de imagens, 1280 - o n√∫mero de neur√¥nios na √∫ltima camada do modelo pr√©-treinado com o TensorFlow Hub. </p><br><pre> <code class="python hljs">feature_batch = feature_extractor(image_batch) print(feature_batch.shape)</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">(32, 1280)</code> </pre> <br><p>  Congelamos as vari√°veis ‚Äã‚Äãna camada de extra√ß√£o de propriedades para que apenas os valores das vari√°veis ‚Äã‚Äãda camada de classifica√ß√£o sejam alteradas durante o processo de treinamento. </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p>  <strong>Adicionar uma camada de classifica√ß√£o</strong> </p><br><p>  Agora envolva a camada do TensorFlow Hub no modelo <code>tf.keras.Sequential</code> e adicione uma camada de classifica√ß√£o. </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(<span class="hljs-number"><span class="hljs-number">2</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 2) 2562 ================================================================= Total params: 2,260,546 Trainable params: 2,562 Non-trainable params: 2,257,984 _________________________________________________________________</code> </pre> <br><p>  <strong>Modelo de trem</strong> </p><br><p>  Agora, treinamos o modelo resultante da maneira que fizemos antes de chamar a <code>compile</code> seguida pela <code>fit</code> ao treinamento. </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] ) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">Epoch 1/6 582/582 [==============================] - 77s 133ms/step - loss: 0.2381 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 582/582 [==============================] - 70s 120ms/step - loss: 0.1827 - acc: 0.9618 - val_loss: 0.1629 - val_acc: 0.9670 Epoch 3/6 582/582 [==============================] - 69s 119ms/step - loss: 0.1733 - acc: 0.9660 - val_loss: 0.1623 - val_acc: 0.9666 Epoch 4/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1677 - acc: 0.9676 - val_loss: 0.1627 - val_acc: 0.9677 Epoch 5/6 582/582 [==============================] - 68s 118ms/step - loss: 0.1636 - acc: 0.9689 - val_loss: 0.1634 - val_acc: 0.9675 Epoch 6/6 582/582 [==============================] - 69s 118ms/step - loss: 0.1604 - acc: 0.9701 - val_loss: 0.1643 - val_acc: 0.9668</code> </pre> <br><p>  Como voc√™ provavelmente notou, conseguimos atingir ~ 97% de precis√£o das previs√µes no conjunto de dados de valida√ß√£o.  Awesome!  A abordagem atual aumentou significativamente a precis√£o da classifica√ß√£o em compara√ß√£o com o primeiro modelo que treinamos e obtivemos uma precis√£o de classifica√ß√£o de ~ 87%.  O motivo √© que o MobileNet foi projetado por especialistas e cuidadosamente desenvolvido por um longo per√≠odo de tempo, e depois treinado em um conjunto de dados ImageNet incrivelmente grande. </p><br><p>  Voc√™ pode ver como criar seu pr√≥prio MobileNet em Keras <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">neste link</a> . </p><br><p>  Vamos criar gr√°ficos de altera√ß√µes nos valores de precis√£o e perda nos conjuntos de dados de treinamento e valida√ß√£o. </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'  '</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     '</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/o7/5c/7a/o75c7aieqvmudmyglkroiddrm90.png"></p><br><p>  O interessante aqui √© que os resultados no conjunto de dados de valida√ß√£o s√£o melhores do que os resultados no conjunto de dados de treinamento desde o in√≠cio at√© o final do processo de aprendizado. </p><br><p>  Uma raz√£o para esse comportamento √© que a precis√£o no conjunto de dados de valida√ß√£o √© medida no final da itera√ß√£o de treinamento, e a precis√£o no conjunto de dados de treinamento √© considerada como o valor m√©dio entre todas as itera√ß√µes de treinamento. </p><br><p>  A maior raz√£o para esse comportamento √© o uso da sub-rede MobileNet pr√©-treinada, que foi treinada anteriormente em um grande conjunto de dados de c√£es e gatos.  No processo de aprendizado, nossa rede continua expandindo o conjunto de dados de treinamento de entrada (o mesmo aumento), mas n√£o o conjunto de valida√ß√£o.  Isso significa que as imagens geradas no conjunto de dados de treinamento s√£o mais dif√≠ceis de classificar do que as imagens normais do conjunto de dados validado. </p><br><p>  <strong>Verificar resultados da previs√£o</strong> </p><br><p>  Para repetir o gr√°fico da se√ß√£o anterior, primeiro voc√™ precisa obter uma lista classificada de nomes de classe: </p><br><pre> <code class="python hljs">class_names = np.array(info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) class_names</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">array(['cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Passe o bloco com imagens pelo modelo e converta os √≠ndices resultantes em nomes de classe: </p><br><pre> <code class="python hljs">predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] predicted_class_names</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">array(['cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'dog', 'cat', 'cat', 'dog'], dtype='&lt;U3')</code> </pre> <br><p>  Vamos dar uma olhada nos r√≥tulos verdadeiros e previstos: </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">": "</span></span>, predicted_ids)</code> </pre> <br><p>  Conclus√£o: </p><br><pre> <code class="plaintext hljs">: [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1] : [0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1]</code> </pre> <br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace=<span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"  (: , : )"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/dc/ox/4w/dcox4wk3fek_1e2j9ltjmvai1ia.png"></p><br><h1>  Mergulhando em redes neurais convolucionais </h1><br><p>  Usando redes neurais convolucionais, conseguimos garantir que eles lidem bem com a tarefa de classificar imagens.  No entanto, no momento, mal podemos imaginar como eles realmente funcionam.  Se pud√©ssemos entender como o processo de aprendizagem ocorre, ent√£o, em princ√≠pio, poder√≠amos melhorar ainda mais o trabalho de classifica√ß√£o.  Uma maneira de entender como as redes neurais convolucionais funcionam √© visualizar as camadas e os resultados de seu trabalho.  √â altamente recomend√°vel que voc√™ estude os materiais <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">aqui</a> para entender melhor como visualizar os resultados das camadas convolucionais. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/50f/d7a/3eb/50fd7a3eb31650740807d84eb8ff1da2.gif" alt="imagem"></p><br><p>  O campo da vis√£o computacional viu a luz no fim do t√∫nel e fez progressos significativos desde o advento das redes neurais convolucionais.  A incr√≠vel velocidade com que as pesquisas s√£o realizadas nessa √°rea e as enormes s√©ries de imagens publicadas na Internet deram resultados incr√≠veis nos √∫ltimos anos.  A ascens√£o das redes neurais convolucionais come√ßou com a AlexNet em 2012, criada por Alex Krizhevsky, Ilya Sutskever e Jeffrey Hinton e ganhou o famoso Desafio de reconhecimento visual em grande escala ImageNet.  Desde ent√£o, n√£o havia d√∫vida no futuro brilhante usando redes neurais convolucionais, e o campo da vis√£o computacional e os resultados do trabalho nele apenas confirmaram esse fato.  Come√ßando com o reconhecimento do seu rosto em um telefone celular e terminando com o reconhecimento de objetos em carros aut√¥nomos, as redes neurais convolucionais j√° conseguiram mostrar e provar sua for√ßa e resolver muitos problemas do mundo real. </p><br><p>  Apesar do grande n√∫mero de grandes conjuntos de dados e modelos pr√©-treinados de redes neurais convolucionais, √†s vezes √© extremamente dif√≠cil entender como a rede funciona e para que exatamente essa rede √© treinada, especialmente para pessoas que n√£o t√™m conhecimento suficiente no campo de aprendizado de m√°quina.                 ,            , ,   Inception,   .                     .            ,    ,         ,         ,         . </p><br><p>       "   Python"  <br> Fran√ßois Chollet.   ,        .    Keras,     ,   " " TensorFlow, MXNET  Theano.   ,        ,            .           ,       . </p><br><p> <strong>  </strong> </p><br><p>            ,    ,           . </p><br><p>             (training accuracy)     .         ,          ,        , ,   Inception,                . </p><br><p>           ,       ,      .   Inception v3 (     ImageNet)     ,    Kaggle.         Inception,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">    </a> ,        <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Inception v3</a>      . </p><br><p>     10  ()     32 ,    2292293.           0.3195,     ‚Äî 0.6377.     <code>ImageDataGenerator</code>     ,      .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">GitHub </a> . </p><br><p> <strong>  </strong> </p><br><p>             ,    ""   ,      .               . </p><br><p> ,              Inception v3 ,        . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d4c/ac1/6f5/d4cac16f506f3d14ab4cca070f4d876b.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/222/102/eda/222102eda480f7108db0c9869ab46057.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca8/176/7c0/ca81767c0f5cae4748f1db6cee625e01.jpg" alt="imagem"></p><br><p>    ‚Äî     .             . </p><br><p>         ,              ()   .        (),       , ,  ,      .          ,      ,        ,     ,       . </p><br><p>   ReLU-    .    ,     <code>ReLU(z) = max(0, z)</code>     . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9e3/b87/e17/9e3b87e175577fe97da51fd1a2b50eac.png" alt="imagem"></p><br><p>          ,   ,   ,        ,      ,           ,   , ,   ..            ,            .     "" ()     ,            ,     ,             . </p><br><p> <strong>   </strong> </p><br><p>         ""        .               . </p><br><p>   ,     Inveption V3      : </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/00e/a03/d78/00ea03d78e161d7f6fff59ba1a133309.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/247/875/0da/2478750da1a4eb167f8dc1c9c55252d6.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/4a9/635/bd8/4a9635bd81a0d2e46435a05c39d3457a.jpg" alt="imagem"></p><br><p>              ,         . ,                   ,      ,           ,   ..          ,       ,                .            ,       ,  ,           "" ( ,      ). </p><br><p> <strong>    </strong> </p><br><p>       ,         , ,      .      ,                  . </p><br><p>     Class Activation Map (  ).      CAM       .       2D              ,                 . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/2f8/95e/f8b/2f895ef8b9086c9ea56745ce0f441ef9.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/6ff/392/e60/6ff392e60f007791bee52e439099759f.jpg" alt="imagem"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fe9/896/210/fe9896210055693195c21a96b74f3188.jpg" alt="imagem"></p><br><p>       ,     .    ,    ,        Mixed-  Inception V3-,        .        () ,           . </p><br><p>     ,          ,        .          <strong></strong> ,            ,        .       ,          .   ,                 ,        ,    ,        . </p><br><p>           ,      ""  -          .               .             . </p><br><p>    ,           ,                   . </p><br><h1>  :      </h1><br><p> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Colab  </a>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Colab  </a> . </p><br><p> <strong>TensorFlow Hub</strong> </p><br><p> TensorFlow Hub      ,      . </p><br><p>                  .     ,      ,   ,          . </p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a>      . </p><br><p>              <code>Runtime -&gt; Reset all runtimes...</code> </p><br><p> <strong></strong> </p><br><p>  ,    : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> absolute_import, division, print_function, unicode_literals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf tf.enable_eager_execution() <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_hub <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> hub <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow_datasets <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tfds <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layers</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">WARNING:tensorflow: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see: * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md * https://github.com/tensorflow/addons * https://github.com/tensorflow/io (for I/O related ops) If you depend on functionality not listed there, please file an issue.</code> </pre> <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> logging logger = tf.get_logger() logger.setLevel(logging.ERROR)</code> </pre> <br><p> <strong>      TensorFlow Datasets</strong> </p><br><p>            TensorFlow Datasets.   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> </a> ,       ‚Äî <code>tf_flowers</code> .        ,       .                <code>tfds.splits</code>   (70%)   (30%).        <code>tfds.load</code> .    <code>tfds.load</code> ,            ,      . </p><br><pre> <code class="python hljs">splits = tfds.Split.TRAIN.subsplit([<span class="hljs-number"><span class="hljs-number">70</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>]) (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Downloading and preparing dataset tf_flowers (218.21 MiB) to /root/tensorflow_datasets/tf_flowers/1.0.0... Dl Completed... 1/|/100% 1/1 [00:07&lt;00:00, 3.67s/ url] Dl Size... 218/|/100% 218/218 [00:07&lt;00:00, 30.69 MiB/s] Extraction completed... 1/|/100% 1/1 [00:07&lt;00:00, 7.05s/ file] Dataset tf_flowers downloaded and prepared to /root/tensorflow_datasets/tf_flowers/1.0.0. Subsequent calls will reuse this data.</code> </pre> <br><p> <strong>     </strong> </p><br><p> ,      ,    ()         ,      ,          ‚Äî   . </p><br><pre> <code class="python hljs">num_classes = dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].num_classes num_training_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> num_validation_examples = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> training_set: num_training_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> validation_set: num_validation_examples += <span class="hljs-number"><span class="hljs-number">1</span></span> print(<span class="hljs-string"><span class="hljs-string">'Total Number of Classes: {}'</span></span>.format(num_classes)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Training Images: {}'</span></span>.format(num_training_examples)) print(<span class="hljs-string"><span class="hljs-string">'Total Number of Validation Images: {} \n'</span></span>.format(num_validation_examples))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Total Number of Classes: 5 Total Number of Training Images: 2590 Total Number of Validation Images: 1080</code> </pre> <br><p>        ‚Äî  . </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, example <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(training_set.take(<span class="hljs-number"><span class="hljs-number">5</span></span>)): print(<span class="hljs-string"><span class="hljs-string">'Image {} shape: {} label: {}'</span></span>.format(i+<span class="hljs-number"><span class="hljs-number">1</span></span>, example[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape, example[<span class="hljs-number"><span class="hljs-number">1</span></span>]))</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Image 1 shape: (226, 240, 3) label: 0 Image 2 shape: (240, 145, 3) label: 2 Image 3 shape: (331, 500, 3) label: 2 Image 4 shape: (240, 320, 3) label: 0 Image 5 shape: (333, 500, 3) label: 1</code> </pre> <br><p> <strong>     </strong> </p><br><p>          ‚Äî ,   MobilNet v2     ‚Äî 224224     (grayscale).     <code>image</code> ()  <code>label</code> ()       . </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">224</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">format_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image, label)</span></span></span><span class="hljs-function">:</span></span> image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/<span class="hljs-number"><span class="hljs-number">255.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> image, label BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">32</span></span> train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p> <strong>    TensorFlow Hub</strong> </p><br><p>    TensorFlow Hub   . ,                      ,         . </p><br><p> <strong>   </strong> </p><br><p>      <code>feature_extractor</code>  MobileNet v2. ,      TensorFlow Hub (   )   .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="></a> .   <code>tf2-preview/mobilenet_v2/feature_vector</code> ,     URL       MobileNet v2 .   <code>feature_extractor</code>   <code>hub.KerasLayer</code>      <code>input_shape</code> . </p><br><pre> <code class="python hljs">URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>))</code> </pre> <br><p> <strong>   </strong> </p><br><p>                   ,   : </p><br><pre> <code class="python hljs">feature_extractor.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><p> <strong>  </strong> </p><br><p>               ,   .            .          . </p><br><pre> <code class="python hljs">model = tf.keras.Sequential([ feature_extractor, layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer (KerasLayer) (None, 1280) 2257984 _________________________________________________________________ dense (Dense) (None, 5) 6405 ================================================================= Total params: 2,264,389 Trainable params: 6,405 Non-trainable params: 2,257,984</code> </pre> <br><p> <strong> </strong> </p><br><p>            ,           . </p><br><pre> <code class="python hljs">model.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 17s 216ms/step - loss: 0.7765 - acc: 0.7170 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 12s 147ms/step - loss: 0.3806 - acc: 0.8757 - val_loss: 0.3485 - val_acc: 0.8833 Epoch 3/6 81/81 [==============================] - 12s 146ms/step - loss: 0.3011 - acc: 0.9031 - val_loss: 0.3190 - val_acc: 0.8907 Epoch 4/6 81/81 [==============================] - 12s 147ms/step - loss: 0.2527 - acc: 0.9205 - val_loss: 0.3031 - val_acc: 0.8917 Epoch 5/6 81/81 [==============================] - 12s 148ms/step - loss: 0.2177 - acc: 0.9371 - val_loss: 0.2933 - val_acc: 0.8972 Epoch 6/6 81/81 [==============================] - 12s 146ms/step - loss: 0.1905 - acc: 0.9456 - val_loss: 0.2870 - val_acc: 0.9000</code> </pre> <br><p>         ~90%  6  ,     !   ,    ,           ~76%  80  .        ,  MobilNet v2                . </p><br><p> <strong>         </strong> </p><br><p>               . </p><br><pre> <code class="python hljs">acc = history.history[<span class="hljs-string"><span class="hljs-string">'acc'</span></span>] val_acc = history.history[<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>] loss = history.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>] val_loss = history.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] epochs_range = range(EPOCHS) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>)) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(epochs_range, acc, label=<span class="hljs-string"><span class="hljs-string">'Training Accuracy'</span></span>) plt.plot(epochs_range, val_acc, label=<span class="hljs-string"><span class="hljs-string">'Validation Accuracy'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'lower right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Accuracy'</span></span>) plt.subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) plt.plot(epochs_range, loss, label=<span class="hljs-string"><span class="hljs-string">'Training Loss'</span></span>) plt.plot(epochs_range, val_loss, label=<span class="hljs-string"><span class="hljs-string">'Validation Loss'</span></span>) plt.legend(loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Training and Validation Loss'</span></span>) plt.show()</code> </pre> <br><p><img src="https://habrastorage.org/webt/ox/nb/hy/oxnbhyqanark0qrt1xmeqg3qv4a.png"></p><br><p>   ,   ,                    . </p><br><p>        ,           ,              . </p><br><p>        - MobileNet,           .              (  augmentation),    .                  . </p><br><p> <strong> </strong> </p><br><p>              NumPy.     ,      . </p><br><pre> <code class="python hljs">class_names = np.array(dataset_info.features[<span class="hljs-string"><span class="hljs-string">'label'</span></span>].names) print(class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['dandelion' 'daisy' 'tulips' 'sunflowers' 'roses']</code> </pre> <br><p> <strong>        </strong> </p><br><p>   <code>next()</code>   <code>image_batch</code> ( )   <code>label_batch</code> ( ).   <code>image_batch</code>  <code>label_batch</code>  NumPy     <code>.numpy()</code> .    <code>.predict()</code>      .        <code>np.argmax()</code>   .            . </p><br><pre> <code class="python hljs">image_batch, label_batch = next(iter(train_batches)) image_batch = image_batch.numpy() label_batch = label_batch.numpy() predicted_batch = model.predict(image_batch) predicted_batch = tf.squeeze(predicted_batch).numpy() predicted_ids = np.argmax(predicted_batch, axis=<span class="hljs-number"><span class="hljs-number">-1</span></span>) predicted_class_names = class_names[predicted_ids] print(predicted_class_names)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">['sunflowers' 'roses' 'tulips' 'tulips' 'daisy' 'dandelion' 'tulips' 'sunflowers' 'daisy' 'daisy' 'tulips' 'daisy' 'daisy' 'tulips' 'tulips' 'tulips' 'dandelion' 'dandelion' 'tulips' 'tulips' 'dandelion' 'roses' 'daisy' 'daisy' 'dandelion' 'roses' 'daisy' 'tulips' 'dandelion' 'dandelion' 'roses' 'dandelion']</code> </pre> <br><p> <strong>     </strong> </p><br><pre> <code class="python hljs">print(<span class="hljs-string"><span class="hljs-string">"Labels: "</span></span>, label_batch) print(<span class="hljs-string"><span class="hljs-string">"Predicted labels: "</span></span>, predicted_ids)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0] Predicted labels: [3 4 2 2 1 0 2 3 1 1 2 1 1 2 2 2 0 0 2 2 0 4 1 1 0 4 1 2 0 0 4 0]</code> </pre> <br><p> <strong>  </strong> </p><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">30</span></span>): plt.subplot(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,n+<span class="hljs-number"><span class="hljs-number">1</span></span>) plt.subplots_adjust(hspace = <span class="hljs-number"><span class="hljs-number">0.3</span></span>) plt.imshow(image_batch[n]) color = <span class="hljs-string"><span class="hljs-string">"blue"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> predicted_ids[n] == label_batch[n] <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">"red"</span></span> plt.title(predicted_class_names[n].title(), color=color) plt.axis(<span class="hljs-string"><span class="hljs-string">'off'</span></span>) _ = plt.suptitle(<span class="hljs-string"><span class="hljs-string">"Model predictions (blue: correct, red: incorrect)"</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/webt/e1/q-/rg/e1q-rgvnr6qns8-vvrcr8yzbexi.png"></p><br><p> <strong>     Inception-</strong> </p><br><p>    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> TensorFlow Hub</a>     <code>tf2-preview/inception_v3/feature_vector</code> .        Inception V3 .      ,      Inception V3     .  ,  Inception V3       299299 .   Inception V3    MobileNet V2. </p><br><pre> <code class="python hljs">IMAGE_RES = <span class="hljs-number"><span class="hljs-number">299</span></span> (training_set, validation_set), dataset_info = tfds.load(<span class="hljs-string"><span class="hljs-string">'tf_flowers'</span></span>, with_info=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, as_supervised=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, split=splits) train_batches = training_set.shuffle(num_training_examples//<span class="hljs-number"><span class="hljs-number">4</span></span>).map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) validation_batches = validation_set.map(format_image).batch(BATCH_SIZE).prefetch(<span class="hljs-number"><span class="hljs-number">1</span></span>) URL = <span class="hljs-string"><span class="hljs-string">"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4"</span></span> feature_extractor = hub.KerasLayer(URL, input_shape=(IMAGE_RES, IMAGE_RES, <span class="hljs-number"><span class="hljs-number">3</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) model_inception = tf.keras.Sequential([ feature_extractor, tf.keras.layers.Dense(num_classes, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model_inception.summary()</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Model: "sequential_1" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= keras_layer_1 (KerasLayer) (None, 2048) 21802784 _________________________________________________________________ dense_1 (Dense) (None, 5) 10245 ================================================================= Total params: 21,813,029 Trainable params: 10,245 Non-trainable params: 21,802,784</code> </pre> <br><pre> <code class="python hljs">model_inception.compile( optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss=<span class="hljs-string"><span class="hljs-string">'sparse_categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) EPOCHS = <span class="hljs-number"><span class="hljs-number">6</span></span> history = model_inception.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches)</code> </pre> <br><p> : </p><br><pre> <code class="plaintext hljs">Epoch 1/6 81/81 [==============================] - 44s 541ms/step - loss: 0.7594 - acc: 0.7309 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 Epoch 2/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3927 - acc: 0.8772 - val_loss: 0.3945 - val_acc: 0.8657 Epoch 3/6 81/81 [==============================] - 35s 434ms/step - loss: 0.3074 - acc: 0.9120 - val_loss: 0.3586 - val_acc: 0.8769 Epoch 4/6 81/81 [==============================] - 35s 434ms/step - loss: 0.2588 - acc: 0.9282 - val_loss: 0.3385 - val_acc: 0.8796 Epoch 5/6 81/81 [==============================] - 35s 436ms/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.3256 - val_acc: 0.8824 Epoch 6/6 81/81 [==============================] - 35s 435ms/step - loss: 0.1996 - acc: 0.9440 - val_loss: 0.3164 - val_acc: 0.8861</code> </pre> <br><h1>  Sum√°rio </h1><br><p>                   .          : </p><br><ul><li> <strong> :</strong> ,                .              . </li><li> <strong> :</strong>       . ""     ,       ,      . </li><li> <strong>MobileNet:</strong>       Google,                  . MobileNet              . </li></ul><br><p>              MobileNet      .                     .                   MobileNet    . </p><br><p> ‚Ä¶   call-to-action ‚Äî ,     share :) <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">YouTube</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Telegram</a> <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">VKontakte</a> <br>   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Ojok</a> . </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt467967/">https://habr.com/ru/post/pt467967/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt467953/index.html">Testando servidores virtuais baratos</a></li>
<li><a href="../pt467957/index.html">O que est√° por tr√°s da constante Feigenbaum</a></li>
<li><a href="../pt467959/index.html">Cosmologia e flutua√ß√µes qu√¢nticas no navegador</a></li>
<li><a href="../pt467961/index.html">Problemas e nuances ao desenvolver para SmartTV usando React.js</a></li>
<li><a href="../pt467965/index.html">Vivaldi 2.8 - Menu, por favor</a></li>
<li><a href="../pt467969/index.html">Apresenta√ß√µes modais de tela modal no iOS 13</a></li>
<li><a href="../pt467973/index.html">Nascimento da plataforma</a></li>
<li><a href="../pt467975/index.html">Huawei Dorado V6: Sichuan Heat</a></li>
<li><a href="../pt467977/index.html">Criando um aplicativo usando componentes com estilo no Vue.js</a></li>
<li><a href="../pt467979/index.html">Integra√ß√µes de publicidade: como funciona?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>