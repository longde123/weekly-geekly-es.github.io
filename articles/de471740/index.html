<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë©üèæ‚Äç‚öñÔ∏è üíõ ‚úäüèº Smart Cache Service basierend auf ZeroMQ und Tarantool ü•ì üìÄ üé™</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ruslan Aromatov, Hauptentwickler, ICD 



 Hallo Habr! Ich arbeite als Backend-Entwickler bei der Moscow Credit Bank und habe w√§hrend meiner Arbeit ei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Smart Cache Service basierend auf ZeroMQ und Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mkb/blog/471740/">  <b>Ruslan Aromatov, Hauptentwickler, ICD</b> <br><br><img src="https://habrastorage.org/webt/ld/1c/ck/ld1cckil16z47pv5vgyjgi7kwa8.png"><br><br>  Hallo Habr!  Ich arbeite als Backend-Entwickler bei der Moscow Credit Bank und habe w√§hrend meiner Arbeit einige Erfahrungen gesammelt, die ich gerne mit der Community teilen m√∂chte.  Heute werde ich Ihnen erz√§hlen, wie wir mit der mobilen Anwendung MKB Online unseren eigenen Cache-Service f√ºr die Frontserver unserer Kunden geschrieben haben.  Dieser Artikel kann f√ºr diejenigen n√ºtzlich sein, die am Service-Design beteiligt sind und mit der Microservice-Architektur, der Tarantool-In-Memory-Datenbank und der ZeroMQ-Bibliothek vertraut sind.  In dem Artikel wird es praktisch keine Beispiele f√ºr Code und Erkl√§rungen der Grundlagen geben, sondern nur eine Beschreibung der Logik der Dienste und ihrer Interaktion mit einem bestimmten Beispiel, das seit mehr als zwei Jahren an unserem Kampf arbeitet. <br><a name="habracut"></a><br><h4>  Wie alles begann </h4><br>  Vor ungef√§hr 6 Jahren war das Schema einfach.  Als Verm√§chtnis des Outsourcing-Unternehmens haben wir zwei Mobile-Banking-Clients f√ºr iOS und Android sowie einen Frontserver f√ºr sie.  Der Server selbst wurde in Java geschrieben, ging auf verschiedene Arten (haupts√§chlich Seife) in sein Backend und kommunizierte mit den Clients, indem er XML √ºber https √ºbertrug. <br><br>  Client-Anwendungen konnten sich irgendwie authentifizieren, eine Liste von Produkten anzeigen und ... sie schienen in der Lage zu sein, einige √úberweisungen und Zahlungen vorzunehmen, aber tats√§chlich machten sie es nicht sehr gut und nicht immer.  Daher war auf dem Frontserver weder eine gro√üe Anzahl von Benutzern noch eine ernsthafte Belastung zu verzeichnen (was jedoch nicht verhinderte, dass er etwa alle zwei Tage einmal herunterfiel). <br><br>  Es ist klar, dass wir (und zu dieser Zeit bestand unser Team aus vier Personen) als Verantwortliche f√ºr die mobile Bank nicht zu dieser Situation passten, und zun√§chst haben wir die aktuellen Anwendungen in Ordnung gebracht, aber der Frontserver erwies sich als sehr schlecht, also musste es so sein Schreiben Sie das Ganze schnell neu, ersetzen Sie gleichzeitig XML durch JSON und wechseln Sie zum <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WildFly-</a> Anwendungsserver.  √úber ein paar Jahre verteilt wird das Refactoring nicht auf einen separaten Beitrag zur√ºckgef√ºhrt, da alles haupts√§chlich getan wurde, um sicherzustellen, dass das System nur stabil funktioniert. <br><br>  Allm√§hlich entwickelten sich die Anwendungen und der Server stabiler und ihre Funktionalit√§t wurde st√§ndig erweitert, was sich auszahlt - es gab immer mehr Benutzer. <br><br>  Gleichzeitig traten Probleme wie Fehlertoleranz, Redundanz, Replikation und - be√§ngstigend zu denken - Hochlast auf. <br><br>  Eine schnelle L√∂sung f√ºr das Problem bestand darin, einen zweiten WildFly-Server hinzuzuf√ºgen, und die Anwendungen lernten, zwischen ihnen zu wechseln.  Das Problem der gleichzeitigen Arbeit mit Client-Sitzungen wurde durch das in WildFly integrierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Infinispan-</a> Modul gel√∂st. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/li/xt/wx/lixtwxkd09us1bfivvdxjv-dxrk.png" alt="Wie zuvor"></div><br>  Es schien, dass das Leben besser wurde ... <br><br><h4>  So kannst du nicht leben </h4><br>  Diese M√∂glichkeit, mit Sitzungen zu arbeiten, war jedoch nicht ohne Nachteile.  Ich werde diejenigen erw√§hnen, die nicht zu uns passten. <br><br><ol><li>  Sitzungsverlust.  Das wichtigste Minus.  Beispielsweise sendet eine Anwendung zwei Anforderungen an Server-1: Die erste Anforderung ist die Authentifizierung und die zweite ist eine Anforderung f√ºr eine Liste von Konten.  Die Authentifizierung ist erfolgreich. Auf Server-1 wird eine Sitzung erstellt.  Zu diesem Zeitpunkt wird die zweite Clientanforderung aufgrund einer schlechten Kommunikation pl√∂tzlich abgebrochen, und die Anwendung wechselt zu Server-2, wodurch die Weiterleitung der zweiten Anforderung erneut gesendet wird.  Bei einer bestimmten Arbeitslast hat Infinispan m√∂glicherweise keine Zeit, Daten zwischen Knoten zu synchronisieren.  Infolgedessen kann Server-2 die Client-Sitzung nicht √ºberpr√ºfen, sendet eine ver√§rgerte Antwort an den Client, der Client ist traurig und beendet seine Sitzung.  Der Benutzer muss sich erneut anmelden.  Traurig </li><li>  Ein Neustart des Servers kann auch zum Verlust von Sitzungen f√ºhren.  Zum Beispiel nach einem Update (und das passiert ziemlich oft).  Wenn Server-2 gestartet wird, kann es nicht funktionieren, bis die Daten mit Server-1 synchronisiert sind.  Es scheint, dass der Server gestartet wurde, aber tats√§chlich keine Anfragen annehmen sollte.  Dies ist unpraktisch. </li><li>  Dies ist ein integriertes WildFly-Modul, das verhindert, dass wir von diesem Anwendungsserver zu Microservices wechseln. </li></ol><br>  Von hier aus wurde eine Liste von dem, was wir m√∂chten, irgendwie von selbst erstellt. <br><br><ol><li>  Wir m√∂chten Client-Sitzungen so speichern, dass jeder Server (egal wie viele es sind) unmittelbar nach dem Start Zugriff auf sie hat. </li><li>  Wir m√∂chten alle Kundendaten zwischen Anfragen speichern (zum Beispiel Zahlungsparameter und all das). </li><li>  Wir m√∂chten im Allgemeinen beliebige Daten auf einem beliebigen Schl√ºssel speichern. </li><li>  Au√üerdem m√∂chten wir Kundendaten empfangen, bevor die Authentifizierung erfolgreich ist.  Zum Beispiel ist der Benutzer authentifiziert und alle seine Produkte sind genau dort, frisch und warm. </li><li>  Und wir wollen entsprechend der Last skalieren. </li><li>  F√ºhren Sie das Docker aus, schreiben Sie Protokolle auf einen einzelnen Stapel und z√§hlen Sie Metriken usw. </li><li>  Oh ja, und damit alles schnell geht. </li></ol><br><h4>  Mehl der Wahl </h4><br>  Bisher haben wir keine Microservice-Architektur implementiert, daher haben wir uns zun√§chst hingesetzt, um verschiedene Optionen zu lesen, anzusehen und auszuprobieren.  Es war sofort klar, dass wir ein schnelles Repository und eine Art Add-On dar√ºber ben√∂tigen, das sich mit Gesch√§ftslogik befasst und die Zugriffsschnittstelle zum Repository darstellt.  Dar√ºber hinaus w√§re es sch√∂n, einen schnellen Transport zwischen den Diensten zu gew√§hrleisten. <br><br>  Sie w√§hlten lange, stritten sich viel und experimentierten.  Ich werde jetzt nicht die Vor- und Nachteile aller Kandidaten beschreiben, dies gilt nicht f√ºr das Thema dieses Artikels. Ich sage nur, dass der Speicher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">tarantool sein wird</a> , wir werden unseren Service in Java schreiben und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ZeroMQ wird</a> als Transport funktionieren.  Ich werde nicht einmal argumentieren, dass die Auswahl sehr zweideutig ist, aber sie wurde weitgehend durch die Tatsache beeinflusst, dass wir keine unterschiedlichen gro√üen und schweren Frameworks (wegen ihres Gewichts und ihrer Langsamkeit), Boxed-L√∂sungen (wegen ihrer Vielseitigkeit und mangelnden Anpassung) m√∂gen, sondern gleichzeitig Wir lieben es, alle Teile unseres Systems so gut wie m√∂glich zu kontrollieren.  Um die Arbeit der Dienste zu steuern, haben wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Prometheus-</a> Metrik-Erfassungsserver mit seinen praktischen Agenten ausgew√§hlt, die in fast jeden Code integriert werden k√∂nnen.  Die Protokolle von all dem werden auf den ELK-Stapel gelegt. <br><br>  Nun, es scheint mir, dass es bereits zu viel Theorie gab. <br><br><h4>  Start und Ziel </h4><br>  Das Entwurfsergebnis war ungef√§hr ein solches Schema. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/he/kw/oh/hekwohpipjcqh4ufeoykog6f8cc.png" alt="Wie wollen wir"></div><br>  <b>Lagerung</b> <br><br>  Es sollte so dumm wie m√∂glich sein, nur Daten und deren aktuellen Status zu speichern, aber immer ohne Neustart zu funktionieren.  Entwickelt, um verschiedene Versionen von Frontservern zu bedienen.  Wir behalten alle Daten im Speicher, Wiederherstellung im Falle eines Neustarts durch .snap- und .xlog-Dateien. <br><br>  Tabelle (Speicherplatz) f√ºr Client-Sitzungen: <br><br><ul><li>  Sitzungs-ID </li><li>  Kunden-ID; </li><li>  Version (Service) </li><li>  Aktualisierungszeit (Zeitstempel); </li><li>  Lebenszeit (ttl); </li><li>  serialisierte Sitzungsdaten. </li></ul><br>  Hier ist alles einfach: Der Client wird authentifiziert, der Frontserver erstellt eine Sitzung und speichert sie im Speicher, wobei er sich an die Zeit erinnert.  Bei jeder Datenanforderung wird die Zeit aktualisiert, sodass die Sitzung am Leben bleibt.  Wenn sich herausstellt, dass die Daten auf Anfrage veraltet sind (oder √ºberhaupt keine vorhanden sind), geben wir einen speziellen R√ºckkehrcode zur√ºck, nach dem der Client seine Sitzung beendet. <br><br>  Einfache Cache-Tabelle (f√ºr alle Sitzungsdaten): <br><br><ul><li>  Schl√ºssel; </li><li>  Sitzungs-ID </li><li>  Art der gespeicherten Daten (beliebige Anzahl); </li><li>  Aktualisierungszeit (Zeitstempel); </li><li>  Lebenszeit (ttl); </li><li>  serialisierte Daten. </li></ul><br>  Tabelle der Kundendaten, die vor der Anmeldung aufgew√§rmt werden m√ºssen: <br><ul><li>  Kunden-ID; </li><li>  Sitzungs-ID </li><li>  Version (Service) </li><li>  Art der gespeicherten Daten (beliebige Anzahl); </li><li>  Aktualisierungszeit (Zeitstempel); </li><li>  Zustand; </li><li>  serialisierte Daten. </li></ul><br>  Ein wichtiges Feld ist hier der Zustand.  Tats√§chlich gibt es nur zwei davon - Leerlauf und Aktualisierung.  Sie werden von einem dar√ºber liegenden Dienst platziert, der f√ºr Clientdaten an das Backend geht, sodass eine andere Instanz dieses Dienstes nicht dieselbe (bereits nutzlose) Arbeit ausf√ºhrt und das Backend nicht l√§dt. <br><br>  Ger√§tetabelle: <br><br><ul><li>  Kunden-ID; </li><li>  Ger√§te-ID </li><li>  Aktualisierungszeit (Zeitstempel); </li></ul><br>  Die Ger√§tetabelle ist erforderlich, damit der Client bereits vor der Authentifizierung im System seine ID ermitteln und mit dem Empfang seiner Produkte beginnen kann (Aufw√§rmen des Caches).  Die Logik lautet wie folgt: Der erste Eingang ist immer kalt, da wir vor der Authentifizierung nicht wissen, welche Art von Client von einem unbekannten Ger√§t stammt (mobile Clients √ºbertragen bei allen Anforderungen immer Ger√§te-IDs).  Alle nachfolgenden Eintr√§ge von diesem Ger√§t werden von einem Aufw√§rmcache f√ºr den damit verbundenen Client begleitet. <br><br>  Die Arbeit mit Daten wird durch Serverprozeduren vom Java-Dienst isoliert.  Ja, ich musste Lua lernen, aber es dauerte nicht lange.  Neben der Datenverwaltung selbst sind Lua-Prozeduren auch f√ºr die R√ºckgabe aktueller Zust√§nde, die Indexauswahl, das Bereinigen veralteter Datens√§tze in Hintergrundprozessen (Fasern) und den Betrieb des integrierten Webservers verantwortlich, √ºber den der direkte Dienstzugriff auf Daten ausgef√ºhrt wird.  Hier ist es - die Sch√∂nheit, alles mit den H√§nden zu schreiben - die M√∂glichkeit der unbegrenzten Kontrolle.  Aber das Minus ist das gleiche - Sie m√ºssen alles selbst schreiben. <br><br>  Tarantool selbst arbeitet in einem Docker-Container. Alle erforderlichen Lua-Dateien werden dort in der Phase der Image-Assemblierung abgelegt.  Die gesamte Assembly durch Gradle-Skripte. <br><br>  Master-Slave-Replikation.  Auf dem anderen Host wird genau derselbe Container ausgef√ºhrt wie das Replikat des Hauptspeichers.  Es wird im Falle eines Notfallabsturzes des Masters ben√∂tigt - dann wechseln die Java-Dienste zum Slave und es wird zum Master.  F√ºr alle F√§lle gibt es einen dritten Sklaven.  Selbst ein vollst√§ndiger Datenverlust ist in unserem Fall traurig, aber nicht t√∂dlich.  Im schlimmsten Fall m√ºssen sich Benutzer anmelden und alle Daten abrufen, die erneut in den Cache gelangen. <br><br>  <b>Java-Dienst</b> <br><br>  Entwickelt als typischer zustandsloser Mikroservice.  Es hat keine Konfiguration, alle erforderlichen Parameter (und es gibt 6 davon) werden beim Erstellen des Docker-Containers durch Umgebungsvariablen √ºbergeben.  Es funktioniert mit dem Frontserver √ºber den ZeroMQ-Transport (org.zeromq.jzmq - die Java-Schnittstelle zur nativen libzmq.so.5.1.1, die wir selbst erstellt haben) unter Verwendung unseres eigenen Protokolls.  Es funktioniert mit einer Vogelspinne √ºber einen Java-Connector (org.tarantool.connector). <br><br>  Die Service-Initialisierung ist ganz einfach: <br><br><ul><li>  Wir starten einen Logger (log4j2); </li><li>  Aus den Umgebungsvariablen (wir befinden uns im Docker) lesen wir die f√ºr die Arbeit erforderlichen Parameter. </li><li>  Wir starten den Server f√ºr Metriken (Steg); </li><li>  Verbindung zur Vogelspinne herstellen (asynchron); </li><li>  Wir starten die erforderliche Anzahl von Thread-Handlern (Arbeitern); </li><li>  Wir starten einen Broker (zmq) - einen endlosen Nachrichtenverarbeitungszyklus. </li></ul><br>  Von alledem ist nur die Nachrichtenverarbeitungs-Engine interessant.  Unten sehen Sie ein Diagramm des Microservices. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/fd/ef/myfdef3ggy0oerhyfvec3iozvwc.png" alt="Message Broker-Logik"></div><br>  Beginnen wir mit dem Start des Brokers.  Unser Broker ist eine Reihe von zmq-Sockets vom Typ ROUTER, die Verbindungen von verschiedenen Clients akzeptieren und f√ºr den Versand von Nachrichten verantwortlich sind, die von diesen kommen. <br><br>  In unserem Fall haben wir einen Listening-Socket auf der externen Schnittstelle, der Nachrichten von Clients unter Verwendung des TCP-Protokolls empf√§ngt, und den anderen, der Nachrichten von Worker-Threads unter Verwendung des Inproc-Protokolls empf√§ngt (es ist viel schneller als TCP). <br><br><pre><code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** //   (   ,   ) ZContext zctx = new ZContext(); //    ZMQ.Socket clientServicePoint = zctx.createSocket(ZMQ.ROUTER); //    ZMQ.Socket workerServicePoint= zctx.createSocket(ZMQ.ROUTER); //     clientServicePoint.bind("tcp://*:" + Config.ZMQ_LISTEN_PORT); //     workerServicePoint.bind("inproc://worker-proc");</span></span></code> </pre> <br>  Nach dem Initialisieren der Sockets starten wir eine endlose Ereignisschleife. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *      */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> status;  <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> {   ZMQ.Poller poller = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ZMQ.Poller(<span class="hljs-number"><span class="hljs-number">2</span></span>);    poller.register(workerServicePoint, ZMQ.Poller.POLLIN);    poller.register(clientServicePoint, ZMQ.Poller.POLLIN);    <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> rc;    <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) {      <span class="hljs-comment"><span class="hljs-comment">//        rc = poller.poll(POLL_INTERVAL);      if (rc == -1) {        status = -1;        logger.errorInternal("Broker run error rc = -1");        break; //  -     }    //     ()    if (poller.pollin(0)) {       processBackendMessage(ZMsg.recvMsg(workerServicePoint));    }    //        if (poller.pollin(1)) {       processFrontendMessage(ZMsg.recvMsg(clientServicePoint));    }    processQueueForBackend(); }  } catch (Exception e) {    status = -1;  } finally {    clientServicePoint.close();    workerServicePoint.close();  }  return status; }</span></span></code> </pre><br>  Die Logik der Arbeit ist sehr einfach: Wir empfangen Nachrichten von verschiedenen Orten und machen etwas damit.  Wenn etwas kritisch mit uns zusammengebrochen ist, verlassen wir die Schleife, was zum Absturz des Prozesses f√ºhrt, der vom Docker-Daemon automatisch neu gestartet wird. <br><br>  Die Hauptidee ist, dass der Broker sich nicht mit Gesch√§ftslogik befasst, sondern nur den Nachrichtenkopf analysiert und Aufgaben an die Arbeitsthreads verteilt, die zuvor beim Start des Dienstes gestartet wurden.  Dabei hilft ihm eine einzelne Nachrichtenwarteschlange mit Priorisierung einer festen L√§nge. <br><br>  Lassen Sie uns den Algorithmus am Beispiel des obigen Schemas und Codes analysieren. <br><br>  Nach dem Start werden die Thread-Mitarbeiter, die sp√§ter als der Broker gestartet wurden, initialisiert und senden eine Bereitschaftsnachricht an den Broker.  Der Broker akzeptiert sie, analysiert sie und f√ºgt jeden Mitarbeiter der Liste hinzu. <br><br>  Auf dem Client-Socket tritt ein Ereignis auf - wir haben die Nachricht1 erhalten.  Der Broker ruft den Handler f√ºr eingehende Nachrichten auf, dessen Aufgabe ist: <br><br><ul><li>  Analyse des Nachrichtenkopfes; </li><li>  Platzieren einer Nachricht in einem Halterobjekt mit einer bestimmten Priorit√§t (basierend auf der Header-Analyse) und Lebensdauer; </li><li>  Platzieren des Inhabers in der Nachrichtenwarteschlange; </li><li>  Wenn die Warteschlange nicht voll ist, ist die Aufgabe des Handlers beendet. </li><li>  Wenn die Warteschlange voll ist, rufen wir die Methode auf, um eine Fehlermeldung an den Client zu senden. </li></ul><br>  In derselben Iteration der Schleife rufen wir den Message Queue Handler auf: <br><br><ul><li>  Wir fordern die aktuellste Nachricht aus der Warteschlange an (die Warteschlange entscheidet dies selbst basierend auf der Priorit√§t und Reihenfolge des Hinzuf√ºgens der Nachricht). </li><li>  √úberpr√ºfen Sie die Lebensdauer der Nachricht (wenn sie abgelaufen ist, rufen Sie die Methode auf, um eine Fehlermeldung an den Client zu senden). </li><li>  Wenn die Nachricht f√ºr die Verarbeitung relevant ist, versuchen Sie, den ersten freien Mitarbeiter arbeitsbereit zu machen. </li><li>  Wenn es keine gibt, stellen Sie die Nachricht wieder in die Warteschlange (genauer gesagt, l√∂schen Sie sie nicht von dort, sie bleibt dort h√§ngen, bis ihre Lebensdauer abl√§uft). </li><li>  Wenn wir einen Arbeiter zur Arbeit bereit haben, markieren wir ihn als besch√§ftigt und senden ihm eine Nachricht zur Bearbeitung. </li><li>  L√∂schen Sie die Nachricht aus der Warteschlange. </li></ul><br>  Wir machen das gleiche mit allen nachfolgenden Nachrichten.  Der Thread-Worker selbst ist wie ein Broker konzipiert - er hat denselben endlosen Nachrichtenverarbeitungszyklus.  Aber wir brauchen keine sofortige Verarbeitung mehr, es ist f√ºr lange Aufgaben ausgelegt. <br><br>  Nachdem der Mitarbeiter seine Aufgabe erledigt hat (z. B. zum Backend f√ºr die Produkte des Kunden oder in der Tarantel f√ºr die Sitzung), sendet er eine Nachricht an den Broker, die der Broker an den Client zur√ºcksendet.  Die Adresse des Kunden, an den die Antwort gesendet werden soll, wird ab dem Moment gespeichert, an dem die Nachricht vom Kunden im Inhaberobjekt eintrifft, das als Nachricht in einem etwas anderen Format an den Mitarbeiter gesendet wird und dann zur√ºckkehrt. <br><br>  Das Format der Nachrichten, die ich st√§ndig erw√§hne, ist unsere eigene Produktion.  ZeroMQ stellt uns standardm√§√üig die ZMsg-Klassen zur Verf√ºgung - die Nachricht selbst und den ZFrame - Teil dieser Nachricht, im Wesentlichen nur ein Array von Bytes, die ich bei Bedarf verwenden kann.  Unsere Nachricht besteht aus zwei Teilen (zwei ZFrames), von denen der erste ein bin√§rer Header und der zweite Daten sind (der Anforderungshauptteil beispielsweise in Form einer JSON-Zeichenfolge, die durch ein Array von Bytes dargestellt wird).  Der Nachrichtenkopf ist universell und wandert sowohl von Client zu Server als auch von Server zu Client. <br><br>  Tats√§chlich haben wir nicht das Konzept von "Anfrage" oder "Antwort", sondern nur Nachrichten.  Der Header enth√§lt: Protokollversion, Systemtyp (welches System angesprochen wird), Nachrichtentyp, Fehlercode auf Transportebene (wenn er nicht 0 ist, ist etwas in der Nachrichten√ºbertragungs-Engine passiert), Anforderungs-ID (Pass-Through-ID, die vom Client kommt - f√ºr die Ablaufverfolgung erforderlich), die Client-Sitzungs-ID (optional) sowie ein Zeichen f√ºr einen Fehler auf Datenebene (wenn beispielsweise die Backend-Antwort nicht analysiert werden konnte, setzen wir dieses Flag, damit der Parser auf der Clientseite die Antwort nicht deserialisiert, sondern Fehlerdaten empf√§ngt auf andere Weise). <br><br>  Dank eines einzigen Protokolls zwischen allen Microservices und einem solchen Header k√∂nnen wir die Komponenten unserer Services ganz einfach manipulieren.  Sie k√∂nnen den Broker beispielsweise in einen separaten Prozess umwandeln und ihn zu einem einzelnen Nachrichtenbroker auf der Ebene des gesamten Microservice-Systems machen.  Oder f√ºhren Sie beispielsweise Worker nicht in Form von Threads innerhalb des Prozesses aus, sondern als separate unabh√§ngige Prozesse.  Und w√§hrend sich der Code in ihnen nicht √§ndert.  Generell gibt es Raum f√ºr Kreativit√§t. <br><br><h4>  Ein bisschen √ºber Leistung und Ressourcen </h4><br>  Der Broker selbst ist schnell und die Gesamtbandbreite des Dienstes wird durch die Backend-Geschwindigkeit und die Anzahl der Worker begrenzt.  Praktischerweise wird die gesamte erforderliche Speichermenge sofort zu Beginn des Dienstes zugewiesen, und alle Threads werden sofort gestartet.  Die Warteschlangengr√∂√üe ist ebenfalls festgelegt.  Zur Laufzeit werden nur Nachrichten verarbeitet. <br><br>  Beispiel: Zus√§tzlich zum Haupt-Thread startet unser aktueller Cache-Kampfdienst weitere 100 Worker-Threads, und die Warteschlangengr√∂√üe ist auf dreitausend Nachrichten begrenzt.  Im normalen Betrieb verarbeitet jede Instanz bis zu 200 Nachrichten pro Sekunde und verbraucht etwa 250 MB Speicher und etwa 2-3% der CPU.  Bei Spitzenlasten springt es manchmal auf 7-8%.  Es funktioniert alles auf einer Art virtuellem Dual-Core-Xeon. <br><br>  Die regul√§re Arbeit des Dienstes impliziert die gleichzeitige Besch√§ftigung von 3-5 Arbeitern (von 100) mit der Anzahl der Nachrichten in der Warteschlange 0 (dh sie werden sofort verarbeitet).  Wenn sich das Backend verlangsamt, steigt die Anzahl der besch√§ftigten Mitarbeiter proportional zum Zeitpunkt seiner Reaktion.  In F√§llen, in denen ein Unfall auftritt und das Backend steigt, werden zuerst alle Mitarbeiter beendet. Danach beginnt die Nachrichtenwarteschlange zu verstopfen.  Wenn es vollst√§ndig verstopft ist, reagieren wir auf Kunden mit Verweigerungen der Verarbeitung.  Gleichzeitig verbrauchen wir keinen Speicher oder keine CPU-Ressourcen, geben keine stabilen Messdaten an und reagieren klar auf die Kunden, was gerade passiert. <br><br>  Der erste Screenshot zeigt den regul√§ren Betrieb des Dienstes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ji/ek/ap/jiekapcj1vylguqgijeheydw7nu.png" alt="Die regelm√§√üige Arbeit des Dienstes"></div><br>  Und beim zweiten ereignete sich ein Unfall - das Backend reagierte aus irgendeinem Grund nicht innerhalb von 30 Sekunden.  Es ist zu sehen, dass zun√§chst alle Arbeiter ausgegangen sind, woraufhin die Nachrichtenwarteschlange zu verstopfen begann. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4v/he/h8/4vheh8hrqsinriaelyow6bgp9wu.png" alt="Unfall"></div><br><h4>  Leistungstests </h4><br>  Die synthetischen Tests auf meiner Arbeitsmaschine (CentOS 7, Core i5, 16 GB RAM) zeigten Folgendes. <br><br>  Arbeiten Sie mit dem Repository (Schreiben in die Vogelspinne und sofortiges Lesen dieses Datensatzes mit einer Gr√∂√üe von 100 Byte - Simulieren der Arbeit mit der Sitzung) - 12000 U / min. <br><br>  Das gleiche, nur die Geschwindigkeit wurde nicht zwischen den Service-Tarantel-Punkten gemessen, sondern zwischen dem Kunden und dem Service.  Nat√ºrlich musste ich selbst einen Kunden f√ºr Stresstests schreiben.  Innerhalb einer Maschine konnten 7000 U / min erreicht werden.  In einem lokalen Netzwerk (und wir haben viele verschiedene virtuelle Maschinen, deren physische Verbindung unklar ist) variieren die Ergebnisse, aber bis zu 5000 U / min f√ºr eine Instanz sind durchaus m√∂glich.  Gott wei√ü, welche Art von Leistung, aber sie deckt mehr als zehnmal unsere Spitzenlasten ab.  Dies ist nur m√∂glich, wenn eine Instanz des Dienstes ausgef√ºhrt wird, wir jedoch mehrere davon haben und Sie jederzeit so viele ausf√ºhren k√∂nnen, wie Sie ben√∂tigen.  Wenn Dienste die Speichergeschwindigkeit blockieren, kann die Tarantel horizontal skaliert werden (Shard beispielsweise basierend auf der Client-ID). <br><br><h4>  Service Intelligence </h4><br>  Der aufmerksame Leser stellt wahrscheinlich bereits die Frage: Was ist die ‚ÄûSchlauheit‚Äú dieses Dienstes, die im Titel erw√§hnt wird?  Ich habe dies bereits beil√§ufig erw√§hnt, aber jetzt werde ich Ihnen mehr erz√§hlen. <br><br>  Eine der Hauptaufgaben des Dienstes bestand darin, die Zeit zu verk√ºrzen, die f√ºr die Ausgabe ihrer Produkte an Benutzer erforderlich ist (Listen mit Konten, Karten, Einzahlungen, Darlehen, Servicepaketen usw.), und gleichzeitig die Belastung des Backends (Verringerung der Anzahl von Anforderungen in gro√üen und schweren Oracle) aufgrund des Zwischenspeicherns in der Tarantel zu verringern. <br><br>  Und er hat es ganz gut gemacht.  Die Logik zum Aufw√§rmen des Client-Cache lautet wie folgt: <br><br><ul><li>  Der Benutzer startet die mobile Anwendung. </li><li>  Eine AppStart-Anforderung mit der Ger√§te-ID wird an den Frontserver gesendet. </li><li>  Der Frontserver sendet eine Nachricht mit dieser ID an den Cache-Dienst. </li><li>  Der Dienst sucht in der Ger√§tetabelle nach der Client-ID f√ºr dieses Ger√§t. </li><li>  Wenn es nicht da ist, passiert nichts (die Antwort wird nicht einmal gesendet, der Server wartet nicht darauf). </li><li>  Wenn sich die Client-ID befindet, erstellt der Worker eine Reihe von Nachrichten zum Empfangen von Listen von Benutzerprodukten, die sofort vom Broker verarbeitet und im normalen Modus an die Worker verteilt werden. </li><li>  Jeder Mitarbeiter sendet eine Anforderung f√ºr einen bestimmten Datentyp an den Benutzer, wobei der Status "Aktualisieren" in die Datenbank aufgenommen wird (dieser Status sch√ºtzt das Backend davor, dieselben Anforderungen zu wiederholen, wenn sie von anderen Instanzen des Dienstes stammen). </li><li>  Nach Erhalt der Daten werden diese in der Vogelspinne aufgezeichnet. </li><li>  Der Benutzer meldet sich beim System an, und die Anwendung sendet Anforderungen zum Empfangen ihrer Produkte, und der Server sendet diese Anforderungen in Form von Nachrichten an den Cache-Dienst. </li><li>  Wenn die Benutzerdaten bereits empfangen wurden, senden wir sie einfach aus dem Cache. </li><li>  Wenn die Daten gerade empfangen werden (Status "Aktualisieren"), wird im Worker ein Datenwartezyklus gestartet (entspricht dem Anforderungszeitlimit f√ºr das Backend). </li><li>  Sobald die Daten empfangen wurden (dh der Status dieses Datensatzes (Tupel) in der Tabelle lautet "Leerlauf"), gibt der Dienst diese an den Client weiter. </li><li>  Wenn die Daten nicht innerhalb eines bestimmten Zeitintervalls empfangen werden, wird ein Fehler an den Client zur√ºckgegeben. </li></ul><br>  In der Praxis konnten wir somit die durchschnittliche Zeit f√ºr den Empfang von Produkten f√ºr den Frontserver von 200 ms auf 20 ms, dh um das Zehnfache, und die Anzahl der Anforderungen an das Backend um das Vierfache reduzieren. <br><br><h4>  Die Probleme </h4><br>  Der Cache-Dienst arbeitet seit ungef√§hr zwei Jahren im Kampf und erf√ºllt derzeit unsere Anforderungen. <br><br>  Nat√ºrlich gibt es immer noch ungel√∂ste Probleme, manchmal treten Probleme auf.  Java-Dienste in der Schlacht sind noch nicht gefallen.  Die Vogelspinne fiel ein paar Mal auf SIGSEGV, aber es war eine alte Version, und nach dem Update kam es nicht wieder vor.  W√§hrend des Stresstests f√§llt die Replikation ab, auf dem Master ist ein Rohrbruch aufgetreten, wonach der Slave abgefallen ist, obwohl der Master weiter gearbeitet hat.  Es wurde durch Neustart des Sklaven entschieden. <br><br>  Es gab einmal einen Unfall im Rechenzentrum, und es stellte sich heraus, dass das Betriebssystem (CentOS 7) keine Festplatten mehr sah.  Das Dateisystem wurde schreibgesch√ºtzt.  Das √úberraschendste war, dass die Dienste weiterhin funktionierten, da wir alle Daten im Speicher behalten.  Die Tarantel konnte keine .xlog-Dateien schreiben, niemand hat etwas protokolliert, aber irgendwie hat alles funktioniert.  Der Neustartversuch war jedoch erfolglos - niemand konnte starten. <br><br>  Es gibt ein gro√ües ungel√∂stes Problem, und ich m√∂chte die Meinung der Community zu diesem Thema h√∂ren.  Wenn die Master-Tarantel abst√ºrzt, k√∂nnen Java-Dienste zu Slave wechseln, der weiterhin als Master arbeitet.  Dies geschieht jedoch nur, wenn der Master abst√ºrzt und nicht funktionieren kann. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e6/pu/wh/e6puwh26kmnngnrxkvyelgvvite.png" alt="Ungel√∂stes Problem"></div><br>  Angenommen, wir haben drei Instanzen eines Dienstes, die mit Daten auf einer Master-Tarantel arbeiten.  Die Dienste selbst fallen nicht aus, die Datenbankreplikation l√§uft, alles ist in Ordnung.  Aber pl√∂tzlich f√§llt ein Netzwerk zwischen Knoten 1 und Knoten 4 auseinander, in dem der Assistent arbeitet.  Service-1 entscheidet sich nach mehreren erfolglosen Versuchen, zur Sicherungsdatenbank zu wechseln, und beginnt dort mit dem Senden von Anforderungen. <br><br>  Unmittelbar danach akzeptiert der Tarantel-Slave Daten√§nderungsanforderungen, wodurch die Replikation vom Master auseinanderf√§llt und wir inkonsistente Daten erhalten.  Gleichzeitig arbeiten die Dienste 2 und 3 perfekt mit dem Master zusammen, und Dienst 1 kommuniziert gut mit dem ehemaligen Slave.  Es ist klar, dass wir in diesem Fall anfangen, Client-Sitzungen und andere Daten zu verlieren, obwohl alles von der technischen Seite aus funktioniert.  Wir haben ein solches potenzielles Problem noch nicht gel√∂st.  Gl√ºcklicherweise ist dies seit 2 Jahren nicht mehr geschehen, aber die Situation ist ziemlich real.  Jetzt kennt jeder Dienst die Nummer des Gesch√§fts, in das er geht, und wir haben eine Warnung f√ºr diese Metrik, die beim Wechsel vom Master zum Slave funktioniert.  Und Sie m√ºssen alles mit Ihren H√§nden reparieren.  Wie l√∂sen Sie solche Probleme? <br><br><h4>  Pl√§ne </h4><br>  Wir planen, an dem oben beschriebenen Problem zu arbeiten, indem wir die Anzahl der Mitarbeiter begrenzen, die gleichzeitig mit einer Art von Anfrage besch√§ftigt sind, den Dienst sicher (ohne aktuelle Anfragen zu verlieren) stoppen und weiter polieren. <br><br><h4>  Fazit </h4><br>  Das ist vielleicht alles, obwohl ich das Thema eher oberfl√§chlich durchgearbeitet habe, aber die allgemeine Logik der Arbeit sollte klar sein.  Daher bin ich, wenn m√∂glich, bereit, in den Kommentaren zu antworten.  Ich habe kurz beschrieben, wie ein kleines Hilfssubsystem der Frontserver der Bank f√ºr die Bedienung mobiler Clients funktioniert. <br><br>  Wenn das Thema f√ºr die Community von Interesse ist, kann ich Ihnen einige unserer L√∂sungen vorstellen, die zur Verbesserung der Qualit√§t des Kundenservice f√ºr die Bank beitragen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de471740/">https://habr.com/ru/post/de471740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de471724/index.html">Agile und Gehirnbed√ºrfnisse: Stressmanagement</a></li>
<li><a href="../de471726/index.html">Moderne Methode zur Messung der Impulsantwort und der nichtlinearen Verzerrung</a></li>
<li><a href="../de471728/index.html">Avalonia meine Vor- und Nachteile</a></li>
<li><a href="../de471736/index.html">Ber√ºhrungsloser Ethernet-Sensor</a></li>
<li><a href="../de471738/index.html">Eine kurze Geschichte dar√ºber, wie Convenience manchmal ins Knie schie√üt</a></li>
<li><a href="../de471742/index.html">Sberbank AI Reise. Wie wir einem neuronalen Netzwerk beigebracht haben, eine Pr√ºfung abzulegen</a></li>
<li><a href="../de471744/index.html">Tarantool Data Grid: Architektur und Funktionen</a></li>
<li><a href="../de471746/index.html">Vollst√§ndiges Handbuch zum Konfigurieren von HTTP-Headern f√ºr die Sicherheit</a></li>
<li><a href="../de471748/index.html">Apothekenoptimierung: Was wir mit Mathe gemacht haben</a></li>
<li><a href="../de471750/index.html">Privilegierte Zugriffsverwaltung als vorrangige Aufgabe in der Informationssicherheit (z. B. Fudo PAM)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>