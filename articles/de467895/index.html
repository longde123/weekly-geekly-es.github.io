<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßùüèº üíÖüèº üïü Welche Muster finden neuronale Netze? üëÜüèΩ üê¢ üë¶</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In diesem Beitrag m√∂chte ich √ºber die Muster sprechen, die neuronale Netze finden k√∂nnen. Viele Anf√§ngerhandb√ºcher konzentrieren sich auf die Technik ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Welche Muster finden neuronale Netze?</h1><div class="post__body post__body_full">
      <div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/467895/">  In diesem Beitrag m√∂chte ich √ºber die Muster sprechen, die neuronale Netze finden k√∂nnen.  Viele Anf√§ngerhandb√ºcher konzentrieren sich auf die Technik des Schreibens von Code f√ºr neuronale Netze, w√§hrend Fragen der ‚ÄûLogik‚Äú (was k√∂nnen neuronale Netze? Welche Architekturen eignen sich besser f√ºr welche Aufgaben und warum?) Oft am Rande bleiben.  Ich hoffe, mein Beitrag wird Anf√§ngern helfen, die F√§higkeiten neuronaler Netze besser zu verstehen.  Dazu werden wir versuchen zu sehen, wie sie mit einigen Modellaufgaben umgehen.  Beispielcode wird in Python mithilfe der Keras-Bibliothek bereitgestellt. <br><br>  <b>Aufgabe 1.</b> Beginnen wir mit einer einfachen.  Wir bauen ein neuronales Netzwerk auf, das sich dem Sinus ann√§hert. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, np.pi, n) y = np.sin(X) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n = <span class="hljs-number"><span class="hljs-number">40</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">6</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">4</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">1000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">4</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=np.pi, num=<span class="hljs-number"><span class="hljs-number">500</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Sinus approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre> <br>  Wir erhalten die folgende Tabelle: <br><br><img src="https://habrastorage.org/webt/t3/xv/_o/t3xv_ocq-o9m8yupmxxdrfxqgai.png" width="500" height="500"><br><br>  Wie Sie sehen, hat das neuronale Netzwerk die Aufgabe der Approximation einer einfachen Funktion erfolgreich gemeistert. <br><a name="habracut"></a><br>  <b>Aufgabe 2. Mal</b> sehen, wie das neuronale Netzwerk mit einer komplexeren Aufgabe fertig wird.  Wir werden x-Werte eingeben, die gleichm√§√üig √ºber das Intervall [0, 1] verteilt sind, und y wird zuf√§llig eingestellt: F√ºr x &lt;0,6 ist y eine Zufallsvariable mit dem Wert 0 mit einer Wahrscheinlichkeit von 0,75 und 1 mit einer Wahrscheinlichkeit von 0,25 (dh einem binomischen Zufallswert mit p = 0,25).  F√ºr x&gt; 0,6 ist y eine Zufallsvariable, die den Wert 0 mit einer Wahrscheinlichkeit von 0,3 und den Wert 1 mit einer Wahrscheinlichkeit von 0,7 annimmt.  Als optimierte Funktion nehmen wir den Standardfehler. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.uniform(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, n) y0 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.25</span></span>) y1 = np.random.binomial(size=n, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.7</span></span>) y = np.where(X &lt; <span class="hljs-number"><span class="hljs-number">0.6</span></span>, y0, y1) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y n_inputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n_hidden1 = <span class="hljs-number"><span class="hljs-number">100</span></span> n_hidden2 = <span class="hljs-number"><span class="hljs-number">50</span></span> n_outputs = <span class="hljs-number"><span class="hljs-number">1</span></span> n = <span class="hljs-number"><span class="hljs-number">2000</span></span> X, y = get_X_y(n) print(<span class="hljs-string"><span class="hljs-string">"X shape:"</span></span>, X.shape) model = Sequential() model.add(Dense(n_hidden1, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(n_hidden2, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) model.fit(X, y, epochs=<span class="hljs-number"><span class="hljs-number">200</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">100</span></span>) X_test = np.linspace(start=<span class="hljs-number"><span class="hljs-number">0</span></span>, stop=<span class="hljs-number"><span class="hljs-number">1</span></span>, num=<span class="hljs-number"><span class="hljs-number">100</span></span>) print(<span class="hljs-string"><span class="hljs-string">"X test shape:"</span></span>, X_test.shape) y_test = model.predict(X_test) font = {<span class="hljs-string"><span class="hljs-string">'weight'</span></span>: <span class="hljs-string"><span class="hljs-string">'bold'</span></span>, <span class="hljs-string"><span class="hljs-string">'size'</span></span>: <span class="hljs-number"><span class="hljs-number">25</span></span>} matplotlib.rc(<span class="hljs-string"><span class="hljs-string">'font'</span></span>, **font) axes = plt.gca() axes.set_ylim(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>) plt.plot(X_test, y_test, c=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, markersize=<span class="hljs-number"><span class="hljs-number">5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Binomial distribution approximated by neural network"</span></span>) plt.yticks(np.arange(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>)) plt.grid() plt.show()</code> </pre><br>  Wir erhalten den folgenden Graphen eines neuronalen Netzwerks mit angen√§herter Funktion: <br><br><img src="https://habrastorage.org/webt/y_/rp/lo/y_rplovhyaxioena0tb8iueed9m.png" width="500" height="500"><br><br>  Wie Sie sehen k√∂nnen, hat das neuronale Netzwerk die mathematische Erwartung unserer Zufallsvariablen y angen√§hert.  So k√∂nnen neuronale Netze (im Prinzip) die Durchschnittswerte von Zufallsvariablen approximieren, die von den Parametern abh√§ngen.  Zum Beispiel k√∂nnen wir erwarten, dass sie das folgende Problem l√∂sen: Menschen mit einem Einkommen von bis zu 1.000 USD sind im Durchschnitt ungl√ºcklich, und Menschen mit einem Einkommen von √ºber 1.000 USD sind im Durchschnitt gl√ºcklich;  man muss lernen, das "Gl√ºcksniveau" je nach Einkommen vorherzusagen.  Das neuronale Netz wird in der Lage sein, die Abh√§ngigkeit des durchschnittlichen Gl√ºcksniveaus vom Einkommen zu ermitteln, obwohl es bei Menschen mit jedem Einkommensniveau sowohl gl√ºckliche als auch ungl√ºckliche gibt. <br><br>  <b>Aufgabe 3.</b> Nun wenden wir uns der Vorhersage von Sequenzen zu.  Wir werden Sequenzen von 0 und 1 betrachten, die durch die folgende Regel gegeben sind: 10 Mitglieder - wahrscheinlich 0 oder 1, und das elfte ist gleich 1, wenn der vorherige Term 0 ist, und ebenso wahrscheinlich 0 oder 1, wenn der vorherige Term 1. Wir werden solche Sequenzen der L√§nge 11 (10 Eingabe) erzeugen Sequenzmitglieder und eines, das letzte, das wir vorhersagen) und trainieren sie in unserem wiederkehrenden neuronalen Netzwerk.  Lassen Sie uns nach dem Training √ºberpr√ºfen, wie sie mit der Vorhersage neuer Sequenzen (auch L√§nge 11) umgeht. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(m, n)</span></span></span><span class="hljs-function">:</span></span> X = np.random.binomial(size=(m,n), n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y0 = np.ones(m) y1 = np.random.binomial(size=m, n=<span class="hljs-number"><span class="hljs-number">1</span></span>, p=<span class="hljs-number"><span class="hljs-number">0.5</span></span>) y = np.where(X[:, n<span class="hljs-number"><span class="hljs-number">-1</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>, y0, y1) X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">50</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">20</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) m_test = <span class="hljs-number"><span class="hljs-number">12</span></span> n_test = <span class="hljs-number"><span class="hljs-number">10</span></span> X_test, y_test = get_X_y(m_test, n_test) y_predicted = model.predict(X_test) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(m_test): print(<span class="hljs-string"><span class="hljs-string">"x_last="</span></span>, X_test[i, n_test<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-string"><span class="hljs-string">"y_predicted="</span></span>, y_predicted[i, <span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br>  Mal sehen, welche Vorhersagen unser neuronales Netzwerk f√ºr die getesteten Sequenzen gibt (Ihre Ergebnisse werden unterschiedlich sein, da hier Zuf√§lligkeit sowohl bei der Auswahl der Sequenzen als auch beim Training des neuronalen Netzwerks vorhanden ist). <br><br><div class="scrollable-table"><table><tbody><tr><th>  Sequenznummer </th><th>  Vorletztes Mitglied der Sequenz </th><th>  Voraussichtlicher Wert </th></tr><tr><td>  1 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  2 </td><td>  0 </td><td>  0,95 </td></tr><tr><td>  3 </td><td>  0 </td><td>  0,97 </td></tr><tr><td>  4 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  5 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  6 </td><td>  1 </td><td>  0,45 </td></tr><tr><td>  7 </td><td>  0 </td><td>  0,94 </td></tr><tr><td>  8 </td><td>  1 </td><td>  0,50 </td></tr><tr><td>  9 </td><td>  0 </td><td>  0,96 </td></tr><tr><td>  10 </td><td>  1 </td><td>  0,42 </td></tr><tr><td>  11 </td><td>  1 </td><td>  0,44 </td></tr><tr><td>  12 </td><td>  0 </td><td>  0,92 </td></tr></tbody></table></div><br><br>  Wie Sie sehen k√∂nnen, sagt das neuronale Netzwerk einen Wert nahe 1 voraus, wenn das vorletzte Mitglied der Sequenz 0 ist, und wenn es 1 ist, dann einen Wert nahe 0,5.  Dies liegt nahe an der optimalen Prognose.  Ein √§hnliches Beispiel aus dem "Leben" k√∂nnte so aussehen: "Wenn ich heute ins Kino gehe, werde ich morgen in einem Restaurant zu Mittag essen;  Wenn ich heute ins Theater gehe, werde ich morgen √ºberall zu Mittag essen. "  Wie wir gesehen haben, kann ein neuronales Netzwerk Muster dieser Art erfassen und eine Reise in ein Restaurant vorhersagen, indem es ins Kino geht (und ins Theater geht, um ‚Äûetwas dazwischen‚Äú vorherzusagen). <br><br>  <b>Aufgabe 4.</b> Wir erschweren die Aufgabe des neuronalen Netzes.  Alles sei wie im vorherigen Beispiel, nur das elfte Mitglied der Sequenz wird nicht vom vorherigen, sondern vom zweiten Mitglied der Sequenz bestimmt (nach derselben Regel).  Wir werden den Code hier nicht angeben, da er sich praktisch nicht vom vorherigen unterscheidet.  Mein Experiment zeigte, dass das neuronale Netzwerk immer noch ein Muster findet, aber f√ºr mehr Zeit (ich musste 100 Epochen anstelle von 20 f√ºr das Training verwenden). <br>  Auf diese Weise k√∂nnen neuronale Netze (im Prinzip wieder klarstellen) ziemlich langfristige Abh√§ngigkeiten erkennen (in unserem ‚ÄûLebensbeispiel‚Äú k√∂nnen sie Muster wie ‚ÄûIch gehe heute in ein Restaurant, wenn ich vor einer Woche in einem Film war‚Äú erfassen). <br><br>  <b>Aufgabe 5.</b> Lassen Sie uns sehen, wie das neuronale Netzwerk die verf√ºgbaren Informationen f√ºr die Vorhersage verwendet. <br>  Dazu werden wir an Sequenzen der L√§nge 4 trainieren. Insgesamt werden wir 3 verschiedene gleich wahrscheinliche Sequenzen haben: <br><br> <code>0, 0, 1, 1 <br> 0, 1, 0, 1 <br> 0, 1, 1, 0</code> <br> <br>  Nach der anf√§nglichen Kombination von 0, 0 treffen wir also immer zwei Einheiten, nach der Kombination von 0, 1 treffen wir wahrscheinlich gleich 0 oder 1, aber wir werden die letzte Zahl sicher kennen.  Wir werden nun unser neuronales Netzwerk bitten, Sequenzen zur√ºckzugeben, indem wir return_sequences = True setzen.  Als vorhergesagte Sequenzen nehmen wir dieselben Sequenzen, die um einen Schritt verschoben und rechts durch Null erg√§nzt wurden.  Jetzt k√∂nnen wir bereits davon ausgehen, was passieren wird: Im ersten Schritt erzeugt das neuronale Netzwerk eine Zahl nahe 2/3 (da der zweite Term mit einer Wahrscheinlichkeit von 2/3 1 ist) und dann f√ºr eine Kombination von 0, 0 zwei Zahlen nahe Einheit, und f√ºr 0, 1 wird zuerst eine Zahl nahe 0,5 ausgegeben, und dann wird eine Zahl nahe 0 oder 1 ausgegeben, je nachdem, ob wir die Folge 0, 1, 0 oder 0, 1, 1 erhalten haben. Am Ende des neuronalen Netzwerks wird immer eine Zahl nahe 0 erzeugen. Die √úberpr√ºfung mit dem folgenden Code zeigt, dass unsere Annahmen korrekt sind. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LSTM, Dense <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_X_y</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n)</span></span></span><span class="hljs-function">:</span></span> X = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) z = np.array([random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n)]) X[z == <span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X[z == <span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] y = np.zeros((n, <span class="hljs-number"><span class="hljs-number">4</span></span>)) y[:, :<span class="hljs-number"><span class="hljs-number">3</span></span>] = X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>:] X = np.reshape(X, (X.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], X.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) y = np.reshape(y, (y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], y.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y model = Sequential() model.add(LSTM(units=<span class="hljs-number"><span class="hljs-number">20</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dense(units=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(optimizer = <span class="hljs-string"><span class="hljs-string">'adam'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>) X_train, y_train = get_X_y(<span class="hljs-number"><span class="hljs-number">1000</span></span>) model.fit(X_train, y_train, epochs = <span class="hljs-number"><span class="hljs-number">100</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">32</span></span>) X_test = np.zeros((<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>)) X_test[<span class="hljs-number"><span class="hljs-number">0</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">1</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] X_test[<span class="hljs-number"><span class="hljs-number">2</span></span>, :] = [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>] X_test = np.reshape(X_test, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) y_predicted = model.predict(X_test) print(y_predicted)</code> </pre><br><br>  Aus diesem Beispiel geht hervor, dass das neuronale Netzwerk die Prognose abh√§ngig von den empfangenen Informationen dynamisch √§ndern kann.  Wir w√ºrden dasselbe tun und versuchen, eine bestimmte Reihenfolge vorherzusagen: Wenn die verf√ºgbaren Informationen es uns erm√∂glichen, die Wahrscheinlichkeiten der Ergebnisse im n√§chsten Schritt abzusch√§tzen, sagen wir dies basierend auf diesen Informationen voraus.  Wenn wir jedoch im n√§chsten Schritt zus√§tzliche Informationen erhalten, √§ndern wir die Prognose abh√§ngig davon. <br>  Wenn wir also sehen, dass jemand aus der Dunkelheit zu uns kommt, sagen wir: "Dies ist eine Person, die wir nicht genauer kennen."  Wenn wir anfangen, lange Haare im Dunkeln zu unterscheiden, sagen wir: "Dies ist wahrscheinlich eine Frau."  Aber wenn wir danach denken, dass eine Person einen Schnurrbart hat, dann sagen wir, dass dies wahrscheinlich ein Mann ist (wenn auch mit langen Haaren).  Wie wir gesehen haben, verh√§lt sich ein neuronales Netzwerk √§hnlich und verwendet die Gesamtheit der derzeit verf√ºgbaren Informationen f√ºr die Vorhersage. <br><br>  Wir haben uns also einfache Beispiele angesehen, wie neuronale Netze funktionieren und welche Muster sie finden k√∂nnen.  Im Allgemeinen haben wir gesehen, dass sich neuronale Netze oft recht ‚Äûvern√ºnftig‚Äú verhalten und Vorhersagen treffen, die denen einer Person nahe kommen.  Es sollte jedoch beachtet werden, dass sie viel mehr Daten ben√∂tigen als Menschen, um einfache Muster zu erfassen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de467895/">https://habr.com/ru/post/de467895/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de467883/index.html">Kubernetes 1.16 - wie man ein Upgrade durchf√ºhrt und nichts kaputt macht</a></li>
<li><a href="../de467885/index.html">Produkt- und Segmentminen</a></li>
<li><a href="../de467887/index.html">Realistische √§tzende Reflexionen</a></li>
<li><a href="../de467891/index.html">FAQ zur Cloud [elektronischen] Signatur</a></li>
<li><a href="../de467893/index.html">Nur ein weiterer Qt-Wrapper f√ºr gRPC und Protobuf</a></li>
<li><a href="../de467897/index.html">Autotest-Tools, Yandex Mapkit 3-Integration, cooles Design und Server Driven UI-Ansatz - Android Mitap-Ank√ºndigung</a></li>
<li><a href="../de467901/index.html">Widerlegen Sie vier Stereotypen √ºber die Programmiersprache Rust</a></li>
<li><a href="../de467903/index.html">Top 20 Navigationsfunktionen bei IntelliJ IDEA. Teil 1</a></li>
<li><a href="../de467905/index.html">Wie wir in Cloud Mail.ru Meilensteine ‚Äã‚Äãerkannt haben und warum</a></li>
<li><a href="../de467907/index.html">Vor- und Nachteile des Outsourcings</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>