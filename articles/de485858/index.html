<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📓 👨🏿‍🤝‍👨🏽 🤶🏾 Wie man einem Telefon beibringt, Schönheit zu sehen 🔒 🏈 ⏹️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kürzlich las ich ein Buch über Mathematik und die Schönheit von Menschen und dachte darüber nach, wie man vor einem Jahrzehnt verstehen kann, welche m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man einem Telefon beibringt, Schönheit zu sehen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485858/"><img src="https://habrastorage.org/webt/-f/0z/on/-f0zonxrb_qtnmaxp2bt34gu-d4.png" alt="Bild"><br><br>  Kürzlich las ich ein Buch über Mathematik und die Schönheit von Menschen und dachte darüber nach, wie man vor einem Jahrzehnt verstehen kann, welche menschliche Schönheit ziemlich primitiv war.  Die Überlegung, welches Gesicht aus mathematischer Sicht als schön angesehen wird, beruhte auf der Tatsache, dass es symmetrisch sein sollte.  Seit der Renaissance gab es auch Versuche, schöne Gesichter anhand der Entfernungsverhältnisse an einigen Gesichtspunkten zu beschreiben und zum Beispiel zu zeigen, dass schöne Gesichter eine Beziehung haben, die nahe am goldenen Schnitt liegt.  Ähnliche Vorstellungen über die Position von Punkten werden jetzt als eine der Methoden zum Identifizieren von Gesichtern (Suche nach Gesichtspunkten) verwendet.  Wie die Erfahrung zeigt, können Sie jedoch bei einer Reihe von Aufgaben bessere Ergebnisse erzielen, z. <a href="https://arxiv.org/abs/1603.01249" rel="nofollow">B. bei der Bestimmung des Alters, des Geschlechts</a> oder sogar der <a href="https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/wang_kosinski.pdf" rel="nofollow">sexuellen Orientierung</a> , wenn Sie die Anzahl der Zeichen nicht auf die Position bestimmter Punkte im Gesicht beschränken.  Es ist hier bereits offensichtlich, dass die Frage der Ethik der Veröffentlichung der Ergebnisse solcher Studien eine akute sein kann. <br><a name="habracut"></a><br>  Das Thema der Schönheit der Menschen und ihre Bewertung kann auch ethisch umstritten sein.  Bei der Entwicklung der Anwendung haben sich viele meiner Freunde geweigert, ihre Fotos für Tests zu verwenden, oder wollten das Ergebnis einfach nicht wissen (es ist lustig, dass sich die meisten Mädchen weigerten, die Ergebnisse zu kennen).  Auch das Ziel der Automatisierung der Schönheitsprüfung kann interessante philosophische Fragen aufwerfen.  Inwieweit wird der Schönheitsbegriff von der Kultur bestimmt?  Wie wahr ist „Schönheit im Auge des Betrachters“?  Ist es möglich, objektive Anzeichen von Schönheit hervorzuheben? <br><br>  Um diese Fragen zu beantworten, müssen Sie die Statistiken über die Bewertungen einiger Personen durch andere Personen studieren.  Ich habe versucht, ein neuronales Netzwerkmodell zu entwerfen und zu trainieren, das die Schönheit bewertet und auf einem Android-Telefon ausführt. <br><br><h2>  Teil 0. Pipeline </h2><br>  Um zu verstehen, wie die nächsten Schritte miteinander zusammenhängen, habe ich ein Diagramm des Projekts gezeichnet: <br><br><img src="https://habrastorage.org/webt/cy/jp/zh/cyjpzhhy_hiczxqefjp9qgaohf0.png" alt="Bild"><br><br>  Blau - wichtige Bibliotheken und externe Daten.  Gelb - Steuerelemente in der Anwendung. <br><br><h2>  Teil 1. Python </h2><br>  Da die Bewertung von Schönheit ein ziemlich heikles Thema ist, gibt es nicht sehr viele Datensätze im öffentlichen Bereich, die Fotos mit einer Bewertung enthalten (ich bin sicher, dass Online-Dating-Dienste wie der Zunder viel umfangreichere Statistiken haben).  Ich fand <a href="https://github.com/HCIILAB/SCUT-FBP5500-Database-Release" rel="nofollow">eine Datenbank,</a> die an einer der Universitäten in China zusammengestellt wurde und 5500 Fotos enthielt, die jeweils von 7 Bewertern unter chinesischen Studenten ausgewertet wurden.  Von den 5.500 Fotografien sind 2.000 asiatische Männer (AM), 2.000 asiatische Frauen (AF) und jeweils 750 europäische Männer (CM) und Frauen (CF). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcf/1db/f9b/fcf1dbf9b67fee5543fdc9833d429676.jpg" alt="Bild"><br><br>  Lassen Sie uns die Daten mit dem Python-Pandas-Modul lesen und einen kurzen Blick auf die Daten werfen.  Geschätzte Verteilung für verschiedene Geschlechter und Rassen: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt ratingDS=pd.read_excel(<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/All_Ratings.xlsx'</span></span>) Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>] ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=ratingDS[<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, sharex=<span class="hljs-string"><span class="hljs-string">'col'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'CF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CM'</span></span>,<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'AM'</span></span>]): sbp=ax[i%<span class="hljs-number"><span class="hljs-number">2</span></span>,i//<span class="hljs-number"><span class="hljs-number">2</span></span>] ratingDS[ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]==race].groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].mean().hist(alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, bins=<span class="hljs-number"><span class="hljs-number">20</span></span>,label=race,grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.9</span></span>,ax=sbp) sbp.set_title(race)</code> </pre> <br><img src="https://habrastorage.org/webt/fz/1q/fo/fz1qfoby_-ijefbbl4ifctz3llo.png" alt="Bild"><br><br>  Es ist zu sehen, dass Männer im Allgemeinen als weniger schön gelten als Frauen, die Verteilung ist bimodal - es gibt solche.  die als schön und "durchschnittlich" gelten.  Es gibt fast keine niedrigen Bewertungen, sodass die Daten renormiert werden könnten.  Aber lassen wir sie jetzt. <br><br>  Sehen wir uns die Standardabweichung in den Schätzungen an: <br><br><pre> <code class="python hljs">ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].std().mean()</code> </pre><br>  Es ist 0,64, was bedeutet, dass der Unterschied bei den Bewertungen der verschiedenen Bewerter weniger als 1 von 5 Punkten beträgt, was auf Einstimmigkeit bei den Bewertungen der Schönheit hinweist.  Man kann mit Recht sagen, dass "Schönheit NICHT im Auge des Betrachters liegt".  Bei der Mittelwertbildung können Sie die Daten zuverlässig zum Trainieren des Modells verwenden und müssen sich nicht um die grundsätzliche Unmöglichkeit einer programmgesteuerten Auswertung kümmern. <br><br>  Trotz des geringen Wertes der Standardabweichung der Schätzung kann sich die Meinung einiger Bewerter stark von der "normalen" unterscheiden.  Bauen wir die Verteilung der Differenz zwischen Schätzung und Median auf: <br><br><pre> <code class="python hljs">R2=ratingDS.join(ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].median(), on=<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>, how=<span class="hljs-string"><span class="hljs-string">'inner'</span></span>,rsuffix =<span class="hljs-string"><span class="hljs-string">' median'</span></span>) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>]=(R2[<span class="hljs-string"><span class="hljs-string">'Rating median'</span></span>]-R2[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]).astype(int) print(set(R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>])) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].hist(label=<span class="hljs-string"><span class="hljs-string">'difference of raings'</span></span>,bins=[<span class="hljs-number"><span class="hljs-number">-3.5</span></span>,<span class="hljs-number"><span class="hljs-number">-2.5</span></span>,<span class="hljs-number"><span class="hljs-number">-1.5</span></span>,<span class="hljs-number"><span class="hljs-number">-0.5</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>,<span class="hljs-number"><span class="hljs-number">1.5</span></span>,<span class="hljs-number"><span class="hljs-number">2.5</span></span>,<span class="hljs-number"><span class="hljs-number">3.5</span></span>,<span class="hljs-number"><span class="hljs-number">4.5</span></span>],grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/ww/qb/7w/wwqb7wdyk_neg_semros1qthb_g.png" alt="Bild"><br><br>  Es wurde ein interessantes Muster gefunden.  Personen, deren Punktzahl um mehr als 1 Punkt vom Median abweicht <br><br><pre> <code class="python hljs">len(R2[R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].abs()&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>])/len(R2)</code> </pre><br>  0.02943333333333333332 <br>  Weniger als 3%.  Das heißt, eine auffallende Einstimmigkeit wird erneut in Bezug auf die Beurteilung der Schönheit bestätigt. <br>  Erstellen Sie eine Tabelle mit den erforderlichen Durchschnittsbewertungen <br><br><pre> <code class="python hljs">Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]</code> </pre><br>  Unsere Datenbank ist klein;  Außerdem enthalten alle Fotos meist Vollbildaufnahmen, und ich hätte gerne ein zuverlässiges Ergebnis für jede Position des Gesichts.  Um Probleme mit einer kleinen Datenmenge zu lösen, wird häufig die Transfer-Lerntechnik verwendet - die Verwendung von Modellen, die für ähnliche Aufgaben vortrainiert sind, und deren Modifikation.  Nah an meiner Aufgabe ist die Gesichtserkennungsaufgabe.  Es wird in der Regel dreistufig gelöst. <br><br>  1. Auf dem Bild und seiner Skalierung befindet sich eine Gesichtserkennung. <br><br>  2. Unter Verwendung eines neuronalen Faltungsnetzwerks wird das Bild des Gesichts in einen Merkmalsvektor umgewandelt, und die Eigenschaften einer solchen Transformation sind derart, dass die Transformation in Bezug auf die Drehung des Gesichts und die Änderung der Frisur invariant ist.  Manifestationen von Emotionen und temporären Bildern.  Ein solches Netzwerk zu trainieren ist eine interessante Aufgabe für sich, die für eine lange Zeit geschrieben werden kann.  Darüber hinaus erscheinen ständig neue Entwicklungen, um diese Umwandlung zu verbessern und die Algorithmen zur Massenverfolgung und Identifizierung zu verbessern.  Sie optimieren sowohl die Netzwerkarchitektur als auch die Trainingsmethode (Beispiel Triplett-Verlust - Oberflächen-Bogen-Verlust). <br><br>  3. Vergleich des Merkmalsvektors mit den in der Datenbank gespeicherten. <br><br>  Für unsere Aufgabe habe ich fertige Lösungen von 1-2 Punkten verwendet.  Das Erkennen von Gesichtern wird in der Regel auf vielfältige Weise gelöst. Darüber hinaus verfügt fast jedes Mobilgerät über Gesichtserkenner (unter Android sind sie Teil des standardmäßigen GooglePlay-Servicepakets), mit denen Gesichter beim Fotografieren scharfgestellt werden.  Bezüglich der Übersetzung von Personen in Vektorform gibt es einen nicht offensichtlichen subtilen Punkt.  Tatsache ist, dass die Zeichen.  extrahiert, um das Erkennungsproblem zu lösen - sind charakteristisch für eine Person, aber sie können überhaupt nicht mit Schönheit korrelieren.  Außerdem.  Aufgrund der Besonderheiten von neuronalen Faltungsnetzen sind diese Zeichen hauptsächlich lokal und können im Allgemeinen viele Probleme verursachen (Single-Pixel-Angriff).  Trotzdem stellte ich fest, dass die Ergebnisse stark von der Dimension des Vektors abhängen. Wenn 128 Zeichen nicht ausreichen, um die Schönheit zu bestimmen, ist 512 ausreichend.  Auf dieser Grundlage wurde ein <a href="https://github.com/shaoanlu/face_toolbox_keras" rel="nofollow">vorab geschultes, auf Reset basierendes insightFace-Netzwerk</a> ausgewählt.  Wir werden Keras auch als Rahmen für maschinelles Lernen verwenden. <br>  Einen ausführlichen Code zum Herunterladen von vorgefertigten Modellen finden Sie <a href="https://www.kaggle.com/alexanderkhar/face-beauty-ranking-ported-to-android" rel="nofollow">hier.</a> <br><br><pre> <code class="python hljs">model=LResNet100E_IR()</code> </pre><br>  Der <a href="https://github.com/ipazc/mtcnn" rel="nofollow">mtcnn-Gesichtsdetektor</a> wurde als Gesichtsdetektor für die Vorverarbeitung verwendet <a href="https://github.com/ipazc/mtcnn" rel="nofollow">.</a> <br><br><pre> <code class="python hljs">detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=<span class="hljs-number"><span class="hljs-number">1</span></span>, accurate_landmark = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, threshold=det_threshold)</code> </pre><br>  Bilder aus dem Datensatz ausrichten, zuschneiden und vektorisieren: <br><br><pre> <code class="python hljs">imgpath=<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/Images/'</span></span> <span class="hljs-comment"><span class="hljs-comment">#    facevecs=[] for name in tqdm.tqdm(Answer.index): #   img1 = cv2.imread(imgpath+name) # ,     pre1 = np.moveaxis(get_input(detector,img1),0,-1) #  vec = model.predict(np.stack([pre1])) #   facevecs.append(vec)</span></span></code> </pre><br>  Wir werden die Daten aufbereiten, indem wir sie in Übungsvektoren aufteilen (90% von ihnen werden untersucht) und validieren (wir werden die Arbeit des Modells an ihnen überprüfen).  Wir normalisieren die Daten auf einen Bereich von 0-1. <br><br><pre> <code class="python hljs">X=np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:] Y=(Answer[:])/<span class="hljs-number"><span class="hljs-number">5</span></span> Indicies=np.arange(len(Answer)) X,Y,Indicies=sklearn.utils.shuffle(X,Y,Indicies) Xtrain=X[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Ytrain=Y[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Indtrain=Indicies[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Xval=X[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Yval=Y[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Indval=Indicies[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):]</code> </pre><br>  Kommen wir nun zum Modell.  Schönheit beschreiben. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Createheadmodel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> inp=keras.layers.Input((<span class="hljs-number"><span class="hljs-number">512</span></span>,)) x=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">32</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(inp) x=keras.layers.Dropout(<span class="hljs-number"><span class="hljs-number">0.1</span></span>)(x) out=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'hard_sigmoid'</span></span>,use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,kernel_initializer=keras.initializers.Ones())(x) model=keras.models.Model(input=inp,output=out) model.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span> model.compile(optimizer=keras.optimizers.Adam(lr=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model modelhead=Createheadmodel()</code> </pre><br>  Dieses Modell ist ein einschichtiges, vollständig verbundenes neuronales Netzwerk mit 32 Neuronen und 512 Eingangsknoten - eine der einfachsten Architekturen, die jedoch gut trainiert ist: <br><br><pre> <code class="python hljs">hist=modelhead.fit(Xtrain,Ytrain, epochs=<span class="hljs-number"><span class="hljs-number">4000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">5000</span></span>, validation_data=(Xval,Yval) )</code> </pre><br>  4950/4950 [==============================] - 0s 3us / Schritt - Verlust: 0,0069 - val_loss: 0,0071 <br>  Bauen wir Lernkurven auf <br><br><pre> <code class="python hljs">plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:], label=<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:],label=<span class="hljs-string"><span class="hljs-string">'validation_loss'</span></span>) plt.legend(bbox_to_anchor=(<span class="hljs-number"><span class="hljs-number">0.95</span></span>, <span class="hljs-number"><span class="hljs-number">0.95</span></span>), loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>, borderaxespad=<span class="hljs-number"><span class="hljs-number">0.</span></span>)</code> </pre><br>  Wir sehen, dass der Verlust (mittlere quadratische Abweichung) bei den Validierungsdaten 0,0071 beträgt, daher beträgt die Standardabweichung 0,084 oder 0,42 Punkte auf einer Fünf-Punkte-Skala, was weniger ist als die Streuung in Schätzungen, die von Personen angegeben wurden (0,6 Punkte).  Unser Modell arbeitet. <br><br>  Um die Funktionsweise des Modells zu veranschaulichen, können Sie das Streudiagramm verwenden. Für jedes Foto wird aus den Validierungsdaten ein Punkt erstellt, an dem eine der Koordinaten der durchschnittlichen Gesichtsbewertung und die zweite der durchschnittlichen vorhergesagten Bewertung entspricht: <br><br><pre> <code class="python hljs">Answer2=Answer.to_frame()[:<span class="hljs-number"><span class="hljs-number">5500</span></span>] Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=<span class="hljs-number"><span class="hljs-number">0</span></span> Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2.index Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=modelhead.predict(np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:])*<span class="hljs-number"><span class="hljs-number">5</span></span> xy=np.array(Answer2.iloc[Indval][[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>,<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]]) plt.scatter(xy[:,<span class="hljs-number"><span class="hljs-number">1</span></span>],xy[:,<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/w2/t9/dd/w2t9ddnfyzjpx-xp7q3_wmsubzk.png" alt="Bild"><br><br>  Y-Achse - vom Modell vorhergesagte Werte, X-Achse - Durchschnittswerte der Schätzungen von Personen.  Wir sehen eine hohe Korrelation (das Diagramm ist entlang der Diagonale verlängert).  Sie können unsere Ergebnisse auch visuell überprüfen - nehmen Sie die Gesichter der einzelnen Kategorien mit vorhergesagten Bewertungen von 1 bis 5 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg f, axarr = plt.subplots(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CF'</span></span>, <span class="hljs-string"><span class="hljs-string">"AM"</span></span>, <span class="hljs-string"><span class="hljs-string">'CM'</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rating <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>): <span class="hljs-comment"><span class="hljs-comment">#axarr[i,rating-1].axis('off') axarr[i,rating-1].tick_params(# changes apply to the x-axis which='both', # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off right=False, left=False, labelbottom=False, labelleft=False ) picname=(Answer2[Answer2['race']==race]['ans']-rating).abs().argmin() axarr[i,rating-1].set_xlabel(Answer2.loc[picname]['ans']) axarr[i,rating-1].imshow(mpimg.imread(imgpath+picname))</span></span></code> </pre><br><img src="https://habrastorage.org/webt/i4/az/pe/i4azpe-biju4pozojrgpiyxkoag.png" alt="Bild"><br><br>  Wir sehen, dass das Ergebnis beim Sortieren nach Schönheit vernünftig aussieht. <br><br>  Jetzt erstellen wir ein vollständiges Modell, in dem wir dem Eingang ein Gesicht übergeben, am Ausgang eine Bewertung von 0 bis 1 erhalten und es in das für das Telefon geeignete tflite-Format konvertieren <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf finmodel=Model(input=model.input, output=modelhead(model.output)) finmodel.save(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter = tf.lite.TFLiteConverter.from_keras_model_file(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] tflite_quant_model = converter.convert() open (<span class="hljs-string"><span class="hljs-string">"modelquant.tflite"</span></span> , <span class="hljs-string"><span class="hljs-string">"wb"</span></span>).write(tflite_quant_model) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FileLink FileLink(<span class="hljs-string"><span class="hljs-string">r'modelquant.tflite'</span></span>)</code> </pre><br>  Dieses Modell erhält am Eingang ein Bild eines Gesichts mit einer Größe von 112 * 112 * 3 und am Ausgang eine einzelne Zahl von 0 bis 1, was die Schönheit des Gesichts bedeutet (obwohl wir bedenken müssen, dass die Bewertungen im Datensatz nicht von 0 bis 5, sondern von 1 bis 5 variierten). <br><br><h2>  Teil 2. JAVA </h2><br>  Lassen Sie uns versuchen, eine einfache Anwendung für ein Android-Handy zu schreiben.  Die Java-Sprache ist für mich neu und ich war noch nie in der Entwicklung für Android involviert. Daher verwendet das Projekt keine Optimierung der Arbeit, keine Flusskontrolle und andere Dinge, die für Anfänger arbeitsintensiv sind.  Da Java-Code ziemlich umständlich ist, gebe ich hier nur die wichtigsten Teile an, damit das Programm funktioniert.  Den vollständigen Anwendungscode finden Sie <a href="https://github.com/Alexankharin/HowCuteAmI" rel="nofollow">hier</a> .  Die Anwendung öffnet ein Foto, erkennt und bewertet ein Gesicht in einem zuvor gespeicherten Netzwerk und zeigt das Ergebnis an: <br><br><img src="https://habrastorage.org/webt/7s/si/a-/7ssia-98n-lxqpitskjaudyohpc.png" alt="Bild"><br><br>  Aus entwicklungspolitischer Sicht sind dabei folgende Funktionen wichtig. <br><br>  1. Die Funktion zum Laden des neuronalen Netzwerks aus der Datei model.tflite im Assets-Ordner in das Interpreter-Objekt <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.tensorflow.lite.Interpreter; Interpreter interpreter; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { interpreter=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Interpreter(loadModelFile(MainActivity.<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>)); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter_started "</span></span>); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { e.printStackTrace(); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter NOT started "</span></span>); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> MappedByteBuffer </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loadModelFile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Activity activity)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(<span class="hljs-string"><span class="hljs-string">"model.tflite"</span></span>); FileInputStream inputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> FileInputStream(fileDescriptor.getFileDescriptor()); FileChannel fileChannel = inputStream.getChannel(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> startOffset = fileDescriptor.getStartOffset(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> declaredLength = fileDescriptor.getDeclaredLength(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); }</code> </pre><br>  2. Erkennen von Gesichtern mithilfe des FaceDetector-Moduls, das Teil des Standardbibliothekspakets von Google ist, mithilfe eines neuronalen Netzwerks und Anzeigen der Ergebnisse. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.Face; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.FaceDetector; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectFace</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-comment"><span class="hljs-comment">//Create a Paint object for drawing with Paint myRectPaint = new Paint(); myRectPaint.setStrokeWidth(5); myRectPaint.setColor(Color.GREEN); myRectPaint.setStyle(Paint.Style.STROKE); Paint fontPaint = new Paint(); fontPaint.setStrokeWidth(3); fontPaint.setTextSize(70); fontPaint.setColor(Color.BLUE); fontPaint.setStyle(Paint.Style.FILL_AND_STROKE); //Create a Canvas object for drawing on tempBitmap = Bitmap.createBitmap(myBitmap.getWidth(), myBitmap.getHeight(), Bitmap.Config.RGB_565); Canvas tempCanvas = new Canvas(tempBitmap); tempCanvas.drawBitmap(myBitmap, 0, 0, null); //Detect the Faces FaceDetector faceDetector = new FaceDetector.Builder(getApplicationContext()).build(); Frame frame = new Frame.Builder().setBitmap(myBitmap).build(); SparseArray&lt;Face&gt; faces = faceDetector.detect(frame); Face face; float[][] labelProbArray = new float[1][1]; imgData.order(ByteOrder.nativeOrder()); //Draw Rectangles on the Faces if (faces.size()&gt;0){ for (int i = 0; i &lt; faces.size(); i++) { face = faces.valueAt(i); isFaceFound=true; float x1 = Math.max(face.getPosition().x,0); float y1 = Math.max(face.getPosition().y,0); float x2 = Math.min(x1 + face.getWidth(),frame.getBitmap().getWidth()); float y2 = Math.min(y1 + face.getHeight(),frame.getBitmap().getHeight()); Bitmap tempbitmap2 = Bitmap.createBitmap(tempBitmap, (int)x1, (int)y1, (int) (x2-x1), (int) (y2-y1)); tempbitmap2 = Bitmap.createScaledBitmap(tempbitmap2, 112, 112, true); convertBitmapToByteBuffer(tempbitmap2); interpreter.run(imgData, labelProbArray); String textToShow = String.format("%.1f", (Answer[0][0]*5-1)/4 * 10); textToShow = textToShow + "/10"; int width= tempCanvas.getWidth(); //int height=tempCanvas.getHeight(); int fontsize=Math.max(width/20,imgView.getWidth()/20); fontPaint.setTextSize(fontsize); tempCanvas.drawText(textToShow, x1, y1-10, fontPaint); tempCanvas.drawRoundRect(new RectF(x1, y1, x2, y2), 2, 2, myRectPaint) } imgView.setImageDrawable(new BitmapDrawable(getResources(),tempBitmap)); } }</span></span></code> </pre><br>  Wenn Sie mit der Einstufung auf Ihrem Handy spielen möchten, können Sie die <a href="https://play.google.com/store/apps/details%3Fid%3Dcom.beautyfromphoto.androidfacedetection" rel="nofollow">Anwendung vom GooglePlay-Markt</a> herunterladen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485858/">https://habr.com/ru/post/de485858/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485846/index.html">Gemeinsames Webinar von Fujitsu und SUSE: „Offene und zuverlässige Lösungen für das digitale Zeitalter“</a></li>
<li><a href="../de485850/index.html">Wie Clickhouse in der Solar Galaxy ausgewählt wurde</a></li>
<li><a href="../de485852/index.html">10 Gründe, den Usability Audit Online-Shop NICHT zu bestellen</a></li>
<li><a href="../de485854/index.html">Helfen Sie dem C ++ - Compiler, die Funktionsüberladung zu beheben</a></li>
<li><a href="../de485856/index.html">Wie wir Hexapod druckten und was daraus wurde</a></li>
<li><a href="../de485862/index.html">DDoS von der Kaffeemaschine</a></li>
<li><a href="../de485866/index.html">Integration der Zimbra Open-Source Edition mit dem Enterprise Portal</a></li>
<li><a href="../de485870/index.html">Gibt es ein GameDev in Sachalin? 2.V</a></li>
<li><a href="../de485872/index.html">Logistische Regression kauen</a></li>
<li><a href="../de485874/index.html">Das Buch „Learning Python: Spielprogrammierung, Datenvisualisierung, Webanwendungen. 3rd ed.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>