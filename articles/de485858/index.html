<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üìì üë®üèø‚Äçü§ù‚Äçüë®üèΩ ü§∂üèæ Wie man einem Telefon beibringt, Sch√∂nheit zu sehen üîí üèà ‚èπÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="K√ºrzlich las ich ein Buch √ºber Mathematik und die Sch√∂nheit von Menschen und dachte dar√ºber nach, wie man vor einem Jahrzehnt verstehen kann, welche m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Wie man einem Telefon beibringt, Sch√∂nheit zu sehen</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/485858/"><img src="https://habrastorage.org/webt/-f/0z/on/-f0zonxrb_qtnmaxp2bt34gu-d4.png" alt="Bild"><br><br>  K√ºrzlich las ich ein Buch √ºber Mathematik und die Sch√∂nheit von Menschen und dachte dar√ºber nach, wie man vor einem Jahrzehnt verstehen kann, welche menschliche Sch√∂nheit ziemlich primitiv war.  Die √úberlegung, welches Gesicht aus mathematischer Sicht als sch√∂n angesehen wird, beruhte auf der Tatsache, dass es symmetrisch sein sollte.  Seit der Renaissance gab es auch Versuche, sch√∂ne Gesichter anhand der Entfernungsverh√§ltnisse an einigen Gesichtspunkten zu beschreiben und zum Beispiel zu zeigen, dass sch√∂ne Gesichter eine Beziehung haben, die nahe am goldenen Schnitt liegt.  √Ñhnliche Vorstellungen √ºber die Position von Punkten werden jetzt als eine der Methoden zum Identifizieren von Gesichtern (Suche nach Gesichtspunkten) verwendet.  Wie die Erfahrung zeigt, k√∂nnen Sie jedoch bei einer Reihe von Aufgaben bessere Ergebnisse erzielen, z. <a href="https://arxiv.org/abs/1603.01249" rel="nofollow">B. bei der Bestimmung des Alters, des Geschlechts</a> oder sogar der <a href="https://www.gsb.stanford.edu/sites/gsb/files/publication-pdf/wang_kosinski.pdf" rel="nofollow">sexuellen Orientierung</a> , wenn Sie die Anzahl der Zeichen nicht auf die Position bestimmter Punkte im Gesicht beschr√§nken.  Es ist hier bereits offensichtlich, dass die Frage der Ethik der Ver√∂ffentlichung der Ergebnisse solcher Studien eine akute sein kann. <br><a name="habracut"></a><br>  Das Thema der Sch√∂nheit der Menschen und ihre Bewertung kann auch ethisch umstritten sein.  Bei der Entwicklung der Anwendung haben sich viele meiner Freunde geweigert, ihre Fotos f√ºr Tests zu verwenden, oder wollten das Ergebnis einfach nicht wissen (es ist lustig, dass sich die meisten M√§dchen weigerten, die Ergebnisse zu kennen).  Auch das Ziel der Automatisierung der Sch√∂nheitspr√ºfung kann interessante philosophische Fragen aufwerfen.  Inwieweit wird der Sch√∂nheitsbegriff von der Kultur bestimmt?  Wie wahr ist ‚ÄûSch√∂nheit im Auge des Betrachters‚Äú?  Ist es m√∂glich, objektive Anzeichen von Sch√∂nheit hervorzuheben? <br><br>  Um diese Fragen zu beantworten, m√ºssen Sie die Statistiken √ºber die Bewertungen einiger Personen durch andere Personen studieren.  Ich habe versucht, ein neuronales Netzwerkmodell zu entwerfen und zu trainieren, das die Sch√∂nheit bewertet und auf einem Android-Telefon ausf√ºhrt. <br><br><h2>  Teil 0. Pipeline </h2><br>  Um zu verstehen, wie die n√§chsten Schritte miteinander zusammenh√§ngen, habe ich ein Diagramm des Projekts gezeichnet: <br><br><img src="https://habrastorage.org/webt/cy/jp/zh/cyjpzhhy_hiczxqefjp9qgaohf0.png" alt="Bild"><br><br>  Blau - wichtige Bibliotheken und externe Daten.  Gelb - Steuerelemente in der Anwendung. <br><br><h2>  Teil 1. Python </h2><br>  Da die Bewertung von Sch√∂nheit ein ziemlich heikles Thema ist, gibt es nicht sehr viele Datens√§tze im √∂ffentlichen Bereich, die Fotos mit einer Bewertung enthalten (ich bin sicher, dass Online-Dating-Dienste wie der Zunder viel umfangreichere Statistiken haben).  Ich fand <a href="https://github.com/HCIILAB/SCUT-FBP5500-Database-Release" rel="nofollow">eine Datenbank,</a> die an einer der Universit√§ten in China zusammengestellt wurde und 5500 Fotos enthielt, die jeweils von 7 Bewertern unter chinesischen Studenten ausgewertet wurden.  Von den 5.500 Fotografien sind 2.000 asiatische M√§nner (AM), 2.000 asiatische Frauen (AF) und jeweils 750 europ√§ische M√§nner (CM) und Frauen (CF). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcf/1db/f9b/fcf1dbf9b67fee5543fdc9833d429676.jpg" alt="Bild"><br><br>  Lassen Sie uns die Daten mit dem Python-Pandas-Modul lesen und einen kurzen Blick auf die Daten werfen.  Gesch√§tzte Verteilung f√ºr verschiedene Geschlechter und Rassen: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt ratingDS=pd.read_excel(<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/All_Ratings.xlsx'</span></span>) Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>] ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=ratingDS[<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) fig, ax = plt.subplots(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, sharex=<span class="hljs-string"><span class="hljs-string">'col'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'CF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CM'</span></span>,<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'AM'</span></span>]): sbp=ax[i%<span class="hljs-number"><span class="hljs-number">2</span></span>,i//<span class="hljs-number"><span class="hljs-number">2</span></span>] ratingDS[ratingDS[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]==race].groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].mean().hist(alpha=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, bins=<span class="hljs-number"><span class="hljs-number">20</span></span>,label=race,grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.9</span></span>,ax=sbp) sbp.set_title(race)</code> </pre> <br><img src="https://habrastorage.org/webt/fz/1q/fo/fz1qfoby_-ijefbbl4ifctz3llo.png" alt="Bild"><br><br>  Es ist zu sehen, dass M√§nner im Allgemeinen als weniger sch√∂n gelten als Frauen, die Verteilung ist bimodal - es gibt solche.  die als sch√∂n und "durchschnittlich" gelten.  Es gibt fast keine niedrigen Bewertungen, sodass die Daten renormiert werden k√∂nnten.  Aber lassen wir sie jetzt. <br><br>  Sehen wir uns die Standardabweichung in den Sch√§tzungen an: <br><br><pre> <code class="python hljs">ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].std().mean()</code> </pre><br>  Es ist 0,64, was bedeutet, dass der Unterschied bei den Bewertungen der verschiedenen Bewerter weniger als 1 von 5 Punkten betr√§gt, was auf Einstimmigkeit bei den Bewertungen der Sch√∂nheit hinweist.  Man kann mit Recht sagen, dass "Sch√∂nheit NICHT im Auge des Betrachters liegt".  Bei der Mittelwertbildung k√∂nnen Sie die Daten zuverl√§ssig zum Trainieren des Modells verwenden und m√ºssen sich nicht um die grunds√§tzliche Unm√∂glichkeit einer programmgesteuerten Auswertung k√ºmmern. <br><br>  Trotz des geringen Wertes der Standardabweichung der Sch√§tzung kann sich die Meinung einiger Bewerter stark von der "normalen" unterscheiden.  Bauen wir die Verteilung der Differenz zwischen Sch√§tzung und Median auf: <br><br><pre> <code class="python hljs">R2=ratingDS.join(ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>)[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>].median(), on=<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>, how=<span class="hljs-string"><span class="hljs-string">'inner'</span></span>,rsuffix =<span class="hljs-string"><span class="hljs-string">' median'</span></span>) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>]=(R2[<span class="hljs-string"><span class="hljs-string">'Rating median'</span></span>]-R2[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]).astype(int) print(set(R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>])) R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].hist(label=<span class="hljs-string"><span class="hljs-string">'difference of raings'</span></span>,bins=[<span class="hljs-number"><span class="hljs-number">-3.5</span></span>,<span class="hljs-number"><span class="hljs-number">-2.5</span></span>,<span class="hljs-number"><span class="hljs-number">-1.5</span></span>,<span class="hljs-number"><span class="hljs-number">-0.5</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>,<span class="hljs-number"><span class="hljs-number">1.5</span></span>,<span class="hljs-number"><span class="hljs-number">2.5</span></span>,<span class="hljs-number"><span class="hljs-number">3.5</span></span>,<span class="hljs-number"><span class="hljs-number">4.5</span></span>],grid=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,rwidth=<span class="hljs-number"><span class="hljs-number">0.5</span></span>)</code> </pre><br><img src="https://habrastorage.org/webt/ww/qb/7w/wwqb7wdyk_neg_semros1qthb_g.png" alt="Bild"><br><br>  Es wurde ein interessantes Muster gefunden.  Personen, deren Punktzahl um mehr als 1 Punkt vom Median abweicht <br><br><pre> <code class="python hljs">len(R2[R2[<span class="hljs-string"><span class="hljs-string">'ratingdiff'</span></span>].abs()&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>])/len(R2)</code> </pre><br>  0.02943333333333333332 <br>  Weniger als 3%.  Das hei√üt, eine auffallende Einstimmigkeit wird erneut in Bezug auf die Beurteilung der Sch√∂nheit best√§tigt. <br>  Erstellen Sie eine Tabelle mit den erforderlichen Durchschnittsbewertungen <br><br><pre> <code class="python hljs">Answer=ratingDS.groupby(<span class="hljs-string"><span class="hljs-string">'Filename'</span></span>).mean()[<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]</code> </pre><br>  Unsere Datenbank ist klein;  Au√üerdem enthalten alle Fotos meist Vollbildaufnahmen, und ich h√§tte gerne ein zuverl√§ssiges Ergebnis f√ºr jede Position des Gesichts.  Um Probleme mit einer kleinen Datenmenge zu l√∂sen, wird h√§ufig die Transfer-Lerntechnik verwendet - die Verwendung von Modellen, die f√ºr √§hnliche Aufgaben vortrainiert sind, und deren Modifikation.  Nah an meiner Aufgabe ist die Gesichtserkennungsaufgabe.  Es wird in der Regel dreistufig gel√∂st. <br><br>  1. Auf dem Bild und seiner Skalierung befindet sich eine Gesichtserkennung. <br><br>  2. Unter Verwendung eines neuronalen Faltungsnetzwerks wird das Bild des Gesichts in einen Merkmalsvektor umgewandelt, und die Eigenschaften einer solchen Transformation sind derart, dass die Transformation in Bezug auf die Drehung des Gesichts und die √Ñnderung der Frisur invariant ist.  Manifestationen von Emotionen und tempor√§ren Bildern.  Ein solches Netzwerk zu trainieren ist eine interessante Aufgabe f√ºr sich, die f√ºr eine lange Zeit geschrieben werden kann.  Dar√ºber hinaus erscheinen st√§ndig neue Entwicklungen, um diese Umwandlung zu verbessern und die Algorithmen zur Massenverfolgung und Identifizierung zu verbessern.  Sie optimieren sowohl die Netzwerkarchitektur als auch die Trainingsmethode (Beispiel Triplett-Verlust - Oberfl√§chen-Bogen-Verlust). <br><br>  3. Vergleich des Merkmalsvektors mit den in der Datenbank gespeicherten. <br><br>  F√ºr unsere Aufgabe habe ich fertige L√∂sungen von 1-2 Punkten verwendet.  Das Erkennen von Gesichtern wird in der Regel auf vielf√§ltige Weise gel√∂st. Dar√ºber hinaus verf√ºgt fast jedes Mobilger√§t √ºber Gesichtserkenner (unter Android sind sie Teil des standardm√§√üigen GooglePlay-Servicepakets), mit denen Gesichter beim Fotografieren scharfgestellt werden.  Bez√ºglich der √úbersetzung von Personen in Vektorform gibt es einen nicht offensichtlichen subtilen Punkt.  Tatsache ist, dass die Zeichen.  extrahiert, um das Erkennungsproblem zu l√∂sen - sind charakteristisch f√ºr eine Person, aber sie k√∂nnen √ºberhaupt nicht mit Sch√∂nheit korrelieren.  Au√üerdem.  Aufgrund der Besonderheiten von neuronalen Faltungsnetzen sind diese Zeichen haupts√§chlich lokal und k√∂nnen im Allgemeinen viele Probleme verursachen (Single-Pixel-Angriff).  Trotzdem stellte ich fest, dass die Ergebnisse stark von der Dimension des Vektors abh√§ngen. Wenn 128 Zeichen nicht ausreichen, um die Sch√∂nheit zu bestimmen, ist 512 ausreichend.  Auf dieser Grundlage wurde ein <a href="https://github.com/shaoanlu/face_toolbox_keras" rel="nofollow">vorab geschultes, auf Reset basierendes insightFace-Netzwerk</a> ausgew√§hlt.  Wir werden Keras auch als Rahmen f√ºr maschinelles Lernen verwenden. <br>  Einen ausf√ºhrlichen Code zum Herunterladen von vorgefertigten Modellen finden Sie <a href="https://www.kaggle.com/alexanderkhar/face-beauty-ranking-ported-to-android" rel="nofollow">hier.</a> <br><br><pre> <code class="python hljs">model=LResNet100E_IR()</code> </pre><br>  Der <a href="https://github.com/ipazc/mtcnn" rel="nofollow">mtcnn-Gesichtsdetektor</a> wurde als Gesichtsdetektor f√ºr die Vorverarbeitung verwendet <a href="https://github.com/ipazc/mtcnn" rel="nofollow">.</a> <br><br><pre> <code class="python hljs">detector = MtcnnDetector(model_folder=mtcnn_path, ctx=ctx, num_worker=<span class="hljs-number"><span class="hljs-number">1</span></span>, accurate_landmark = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, threshold=det_threshold)</code> </pre><br>  Bilder aus dem Datensatz ausrichten, zuschneiden und vektorisieren: <br><br><pre> <code class="python hljs">imgpath=<span class="hljs-string"><span class="hljs-string">'../input/faces-scut/scut-fbp5500_v2/SCUT-FBP5500_v2/Images/'</span></span> <span class="hljs-comment"><span class="hljs-comment">#    facevecs=[] for name in tqdm.tqdm(Answer.index): #   img1 = cv2.imread(imgpath+name) # ,     pre1 = np.moveaxis(get_input(detector,img1),0,-1) #  vec = model.predict(np.stack([pre1])) #   facevecs.append(vec)</span></span></code> </pre><br>  Wir werden die Daten aufbereiten, indem wir sie in √úbungsvektoren aufteilen (90% von ihnen werden untersucht) und validieren (wir werden die Arbeit des Modells an ihnen √ºberpr√ºfen).  Wir normalisieren die Daten auf einen Bereich von 0-1. <br><br><pre> <code class="python hljs">X=np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:] Y=(Answer[:])/<span class="hljs-number"><span class="hljs-number">5</span></span> Indicies=np.arange(len(Answer)) X,Y,Indicies=sklearn.utils.shuffle(X,Y,Indicies) Xtrain=X[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Ytrain=Y[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Indtrain=Indicies[:int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>)] Xval=X[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Yval=Y[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):] Indval=Indicies[int(len(facevecs)*<span class="hljs-number"><span class="hljs-number">0.9</span></span>):]</code> </pre><br>  Kommen wir nun zum Modell.  Sch√∂nheit beschreiben. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Createheadmodel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> inp=keras.layers.Input((<span class="hljs-number"><span class="hljs-number">512</span></span>,)) x=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">32</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(inp) x=keras.layers.Dropout(<span class="hljs-number"><span class="hljs-number">0.1</span></span>)(x) out=keras.layers.Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>,activation=<span class="hljs-string"><span class="hljs-string">'hard_sigmoid'</span></span>,use_bias=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>,kernel_initializer=keras.initializers.Ones())(x) model=keras.models.Model(input=inp,output=out) model.layers[<span class="hljs-number"><span class="hljs-number">-1</span></span>].trainable=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span> model.compile(optimizer=keras.optimizers.Adam(lr=<span class="hljs-number"><span class="hljs-number">0.0001</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model modelhead=Createheadmodel()</code> </pre><br>  Dieses Modell ist ein einschichtiges, vollst√§ndig verbundenes neuronales Netzwerk mit 32 Neuronen und 512 Eingangsknoten - eine der einfachsten Architekturen, die jedoch gut trainiert ist: <br><br><pre> <code class="python hljs">hist=modelhead.fit(Xtrain,Ytrain, epochs=<span class="hljs-number"><span class="hljs-number">4000</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">5000</span></span>, validation_data=(Xval,Yval) )</code> </pre><br>  4950/4950 [==============================] - 0s 3us / Schritt - Verlust: 0,0069 - val_loss: 0,0071 <br>  Bauen wir Lernkurven auf <br><br><pre> <code class="python hljs">plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:], label=<span class="hljs-string"><span class="hljs-string">'loss'</span></span>) plt.plot(hist.history[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>][<span class="hljs-number"><span class="hljs-number">100</span></span>:],label=<span class="hljs-string"><span class="hljs-string">'validation_loss'</span></span>) plt.legend(bbox_to_anchor=(<span class="hljs-number"><span class="hljs-number">0.95</span></span>, <span class="hljs-number"><span class="hljs-number">0.95</span></span>), loc=<span class="hljs-string"><span class="hljs-string">'upper right'</span></span>, borderaxespad=<span class="hljs-number"><span class="hljs-number">0.</span></span>)</code> </pre><br>  Wir sehen, dass der Verlust (mittlere quadratische Abweichung) bei den Validierungsdaten 0,0071 betr√§gt, daher betr√§gt die Standardabweichung 0,084 oder 0,42 Punkte auf einer F√ºnf-Punkte-Skala, was weniger ist als die Streuung in Sch√§tzungen, die von Personen angegeben wurden (0,6 Punkte).  Unser Modell arbeitet. <br><br>  Um die Funktionsweise des Modells zu veranschaulichen, k√∂nnen Sie das Streudiagramm verwenden. F√ºr jedes Foto wird aus den Validierungsdaten ein Punkt erstellt, an dem eine der Koordinaten der durchschnittlichen Gesichtsbewertung und die zweite der durchschnittlichen vorhergesagten Bewertung entspricht: <br><br><pre> <code class="python hljs">Answer2=Answer.to_frame()[:<span class="hljs-number"><span class="hljs-number">5500</span></span>] Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=<span class="hljs-number"><span class="hljs-number">0</span></span> Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2.index Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>]=Answer2[<span class="hljs-string"><span class="hljs-string">'race'</span></span>].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) Answer2[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>]=modelhead.predict(np.stack(facevecs)[:,<span class="hljs-number"><span class="hljs-number">0</span></span>,:])*<span class="hljs-number"><span class="hljs-number">5</span></span> xy=np.array(Answer2.iloc[Indval][[<span class="hljs-string"><span class="hljs-string">'ans'</span></span>,<span class="hljs-string"><span class="hljs-string">'Rating'</span></span>]]) plt.scatter(xy[:,<span class="hljs-number"><span class="hljs-number">1</span></span>],xy[:,<span class="hljs-number"><span class="hljs-number">0</span></span>])</code> </pre><br><img src="https://habrastorage.org/webt/w2/t9/dd/w2t9ddnfyzjpx-xp7q3_wmsubzk.png" alt="Bild"><br><br>  Y-Achse - vom Modell vorhergesagte Werte, X-Achse - Durchschnittswerte der Sch√§tzungen von Personen.  Wir sehen eine hohe Korrelation (das Diagramm ist entlang der Diagonale verl√§ngert).  Sie k√∂nnen unsere Ergebnisse auch visuell √ºberpr√ºfen - nehmen Sie die Gesichter der einzelnen Kategorien mit vorhergesagten Bewertungen von 1 bis 5 <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.image <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpimg f, axarr = plt.subplots(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>,figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, race <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate([<span class="hljs-string"><span class="hljs-string">'AF'</span></span>,<span class="hljs-string"><span class="hljs-string">'CF'</span></span>, <span class="hljs-string"><span class="hljs-string">"AM"</span></span>, <span class="hljs-string"><span class="hljs-string">'CM'</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> rating <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>): <span class="hljs-comment"><span class="hljs-comment">#axarr[i,rating-1].axis('off') axarr[i,rating-1].tick_params(# changes apply to the x-axis which='both', # both major and minor ticks are affected bottom=False, # ticks along the bottom edge are off top=False, # ticks along the top edge are off right=False, left=False, labelbottom=False, labelleft=False ) picname=(Answer2[Answer2['race']==race]['ans']-rating).abs().argmin() axarr[i,rating-1].set_xlabel(Answer2.loc[picname]['ans']) axarr[i,rating-1].imshow(mpimg.imread(imgpath+picname))</span></span></code> </pre><br><img src="https://habrastorage.org/webt/i4/az/pe/i4azpe-biju4pozojrgpiyxkoag.png" alt="Bild"><br><br>  Wir sehen, dass das Ergebnis beim Sortieren nach Sch√∂nheit vern√ºnftig aussieht. <br><br>  Jetzt erstellen wir ein vollst√§ndiges Modell, in dem wir dem Eingang ein Gesicht √ºbergeben, am Ausgang eine Bewertung von 0 bis 1 erhalten und es in das f√ºr das Telefon geeignete tflite-Format konvertieren <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf finmodel=Model(input=model.input, output=modelhead(model.output)) finmodel.save(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter = tf.lite.TFLiteConverter.from_keras_model_file(<span class="hljs-string"><span class="hljs-string">'finmodel.h5'</span></span>) converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] tflite_quant_model = converter.convert() open (<span class="hljs-string"><span class="hljs-string">"modelquant.tflite"</span></span> , <span class="hljs-string"><span class="hljs-string">"wb"</span></span>).write(tflite_quant_model) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FileLink FileLink(<span class="hljs-string"><span class="hljs-string">r'modelquant.tflite'</span></span>)</code> </pre><br>  Dieses Modell erh√§lt am Eingang ein Bild eines Gesichts mit einer Gr√∂√üe von 112 * 112 * 3 und am Ausgang eine einzelne Zahl von 0 bis 1, was die Sch√∂nheit des Gesichts bedeutet (obwohl wir bedenken m√ºssen, dass die Bewertungen im Datensatz nicht von 0 bis 5, sondern von 1 bis 5 variierten). <br><br><h2>  Teil 2. JAVA </h2><br>  Lassen Sie uns versuchen, eine einfache Anwendung f√ºr ein Android-Handy zu schreiben.  Die Java-Sprache ist f√ºr mich neu und ich war noch nie in der Entwicklung f√ºr Android involviert. Daher verwendet das Projekt keine Optimierung der Arbeit, keine Flusskontrolle und andere Dinge, die f√ºr Anf√§nger arbeitsintensiv sind.  Da Java-Code ziemlich umst√§ndlich ist, gebe ich hier nur die wichtigsten Teile an, damit das Programm funktioniert.  Den vollst√§ndigen Anwendungscode finden Sie <a href="https://github.com/Alexankharin/HowCuteAmI" rel="nofollow">hier</a> .  Die Anwendung √∂ffnet ein Foto, erkennt und bewertet ein Gesicht in einem zuvor gespeicherten Netzwerk und zeigt das Ergebnis an: <br><br><img src="https://habrastorage.org/webt/7s/si/a-/7ssia-98n-lxqpitskjaudyohpc.png" alt="Bild"><br><br>  Aus entwicklungspolitischer Sicht sind dabei folgende Funktionen wichtig. <br><br>  1. Die Funktion zum Laden des neuronalen Netzwerks aus der Datei model.tflite im Assets-Ordner in das Interpreter-Objekt <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.tensorflow.lite.Interpreter; Interpreter interpreter; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { interpreter=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Interpreter(loadModelFile(MainActivity.<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>)); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter_started "</span></span>); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { e.printStackTrace(); Log.e(<span class="hljs-string"><span class="hljs-string">"TIME"</span></span>, <span class="hljs-string"><span class="hljs-string">"Interpreter NOT started "</span></span>); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> MappedByteBuffer </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loadModelFile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Activity activity)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> IOException </span></span>{ AssetFileDescriptor fileDescriptor = activity.getAssets().openFd(<span class="hljs-string"><span class="hljs-string">"model.tflite"</span></span>); FileInputStream inputStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> FileInputStream(fileDescriptor.getFileDescriptor()); FileChannel fileChannel = inputStream.getChannel(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> startOffset = fileDescriptor.getStartOffset(); <span class="hljs-keyword"><span class="hljs-keyword">long</span></span> declaredLength = fileDescriptor.getDeclaredLength(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength); }</code> </pre><br>  2. Erkennen von Gesichtern mithilfe des FaceDetector-Moduls, das Teil des Standardbibliothekspakets von Google ist, mithilfe eines neuronalen Netzwerks und Anzeigen der Ergebnisse. <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.Face; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> com.google.android.gms.vision.face.FaceDetector; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectFace</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>{ <span class="hljs-comment"><span class="hljs-comment">//Create a Paint object for drawing with Paint myRectPaint = new Paint(); myRectPaint.setStrokeWidth(5); myRectPaint.setColor(Color.GREEN); myRectPaint.setStyle(Paint.Style.STROKE); Paint fontPaint = new Paint(); fontPaint.setStrokeWidth(3); fontPaint.setTextSize(70); fontPaint.setColor(Color.BLUE); fontPaint.setStyle(Paint.Style.FILL_AND_STROKE); //Create a Canvas object for drawing on tempBitmap = Bitmap.createBitmap(myBitmap.getWidth(), myBitmap.getHeight(), Bitmap.Config.RGB_565); Canvas tempCanvas = new Canvas(tempBitmap); tempCanvas.drawBitmap(myBitmap, 0, 0, null); //Detect the Faces FaceDetector faceDetector = new FaceDetector.Builder(getApplicationContext()).build(); Frame frame = new Frame.Builder().setBitmap(myBitmap).build(); SparseArray&lt;Face&gt; faces = faceDetector.detect(frame); Face face; float[][] labelProbArray = new float[1][1]; imgData.order(ByteOrder.nativeOrder()); //Draw Rectangles on the Faces if (faces.size()&gt;0){ for (int i = 0; i &lt; faces.size(); i++) { face = faces.valueAt(i); isFaceFound=true; float x1 = Math.max(face.getPosition().x,0); float y1 = Math.max(face.getPosition().y,0); float x2 = Math.min(x1 + face.getWidth(),frame.getBitmap().getWidth()); float y2 = Math.min(y1 + face.getHeight(),frame.getBitmap().getHeight()); Bitmap tempbitmap2 = Bitmap.createBitmap(tempBitmap, (int)x1, (int)y1, (int) (x2-x1), (int) (y2-y1)); tempbitmap2 = Bitmap.createScaledBitmap(tempbitmap2, 112, 112, true); convertBitmapToByteBuffer(tempbitmap2); interpreter.run(imgData, labelProbArray); String textToShow = String.format("%.1f", (Answer[0][0]*5-1)/4 * 10); textToShow = textToShow + "/10"; int width= tempCanvas.getWidth(); //int height=tempCanvas.getHeight(); int fontsize=Math.max(width/20,imgView.getWidth()/20); fontPaint.setTextSize(fontsize); tempCanvas.drawText(textToShow, x1, y1-10, fontPaint); tempCanvas.drawRoundRect(new RectF(x1, y1, x2, y2), 2, 2, myRectPaint) } imgView.setImageDrawable(new BitmapDrawable(getResources(),tempBitmap)); } }</span></span></code> </pre><br>  Wenn Sie mit der Einstufung auf Ihrem Handy spielen m√∂chten, k√∂nnen Sie die <a href="https://play.google.com/store/apps/details%3Fid%3Dcom.beautyfromphoto.androidfacedetection" rel="nofollow">Anwendung vom GooglePlay-Markt</a> herunterladen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de485858/">https://habr.com/ru/post/de485858/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de485846/index.html">Gemeinsames Webinar von Fujitsu und SUSE: ‚ÄûOffene und zuverl√§ssige L√∂sungen f√ºr das digitale Zeitalter‚Äú</a></li>
<li><a href="../de485850/index.html">Wie Clickhouse in der Solar Galaxy ausgew√§hlt wurde</a></li>
<li><a href="../de485852/index.html">10 Gr√ºnde, den Usability Audit Online-Shop NICHT zu bestellen</a></li>
<li><a href="../de485854/index.html">Helfen Sie dem C ++ - Compiler, die Funktions√ºberladung zu beheben</a></li>
<li><a href="../de485856/index.html">Wie wir Hexapod druckten und was daraus wurde</a></li>
<li><a href="../de485862/index.html">DDoS von der Kaffeemaschine</a></li>
<li><a href="../de485866/index.html">Integration der Zimbra Open-Source Edition mit dem Enterprise Portal</a></li>
<li><a href="../de485870/index.html">Gibt es ein GameDev in Sachalin? 2.V</a></li>
<li><a href="../de485872/index.html">Logistische Regression kauen</a></li>
<li><a href="../de485874/index.html">Das Buch ‚ÄûLearning Python: Spielprogrammierung, Datenvisualisierung, Webanwendungen. 3rd ed.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>