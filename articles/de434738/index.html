<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😲 🛷 ⏳ Verstärkungslernen in Python 🤙🏿 🏆 👩🏽‍🤝‍👨🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Kollegen! 



 In der letzten Veröffentlichung des kommenden Jahres wollten wir Reinforcement Learning erwähnen - ein Thema, in das wir bereits ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verstärkungslernen in Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Hallo Kollegen! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  In der letzten Veröffentlichung des kommenden Jahres wollten wir Reinforcement Learning erwähnen - ein Thema, in das wir bereits ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch</a> übersetzen. <br><br>  Überzeugen Sie sich selbst: Es gab einen elementaren Artikel mit Medium, in dem der Kontext des Problems beschrieben und der einfachste Algorithmus mit Implementierung in Python beschrieben wurde.  Der Artikel hat mehrere Gifs.  Und Motivation, Belohnung und die Wahl der richtigen Strategie auf dem Weg zum Erfolg sind Dinge, die für jeden von uns im kommenden Jahr äußerst nützlich sein werden. <br><br>  Viel Spaß beim Lesen! <br><a name="habracut"></a><br>  Verstärktes Lernen ist eine Form des maschinellen Lernens, bei der der Agent lernt, in der Umgebung zu handeln, Aktionen auszuführen und dadurch die Intuition zu entwickeln. Danach beobachtet er die Ergebnisse seiner Aktionen.  In diesem Artikel werde ich Ihnen erklären, wie Sie das Problem des Lernens mit Verstärkung verstehen und formulieren und es dann in Python lösen können. <br><br><br>  In letzter Zeit haben wir uns daran gewöhnt, dass Computer Spiele gegen Menschen spielen - entweder als Bots in Multiplayer-Spielen oder als Rivalen in Einzelspielen: beispielsweise in Dota2, PUB-G, Mario.  Das Forschungsunternehmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deepmind</a> machte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viel Aufhebens</a> um die Neuigkeiten, als sein AlphaGo-Programm 2016 den südkoreanischen Meister 2016 besiegte.  Wenn Sie ein begeisterter Spieler sind, können Sie von den fünf Spielen von Dota 2 OpenAI Five hören, bei denen Autos gegen Menschen kämpften und die besten Spieler in Dota2 in mehreren Spielen besiegten.  (Wenn Sie an den Details interessiert sind, wird der Algorithmus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> detailliert analysiert und untersucht, wie die Maschinen gespielt haben). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  Die neueste Version von OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nimmt Roshan</a> . <br><br>  Beginnen wir also mit der zentralen Frage.  Warum brauchen wir eine verstärkte Ausbildung?  Wird es nur in Spielen verwendet oder ist es in realistischen Szenarien zur Lösung angewandter Probleme anwendbar?  Wenn Sie zum ersten Mal ein Verstärkungstraining lesen, können Sie sich die Antwort auf diese Fragen einfach nicht vorstellen.  In der Tat ist verstärktes Lernen eine der am weitesten verbreiteten und sich schnell entwickelnden Technologien auf dem Gebiet der künstlichen Intelligenz. <br>  Hier einige Themenbereiche, in denen Lernsysteme zur Stärkung besonders gefragt sind: <br><br><ol><li>  Unbemannte Fahrzeuge </li><li>  Spielebranche </li><li>  Robotik </li><li>  Empfehlungssysteme </li><li>  Werbung und Marketing </li></ol><br>  <b>Überblick und Hintergrund des verstärkenden Lernens</b> <br><br>  Wie hat sich das Lernphänomen mit Verstärkung entwickelt, als wir über so viele Methoden des maschinellen und tiefen Lernens verfügten?  "Er wurde von Rich Sutton und Andrew Barto, Richs Forschungsleiter, erfunden, die ihm bei der Vorbereitung der Promotion geholfen haben."  Das Paradigma nahm erstmals in den 1980er Jahren Gestalt an und war dann archaisch.  Anschließend glaubte Rich, dass sie eine große Zukunft hatte und schließlich Anerkennung erhalten würde. <br><br>  Verstärktes Lernen unterstützt die Automatisierung in der Umgebung, in der es bereitgestellt wird.  Sowohl maschinelles als auch tiefes Lernen funktionieren ungefähr gleich - sie sind strategisch unterschiedlich angeordnet, aber beide Paradigmen unterstützen die Automatisierung.  Warum entstand ein Verstärkungstraining? <br><br>  Es erinnert sehr an den natürlichen Lernprozess, in dem der Prozess / das Modell agiert, und erhält Feedback darüber, wie sie es schafft, die Aufgabe zu bewältigen: gut und nicht. <br><br>  Maschinelles und tiefes Lernen sind ebenfalls Trainingsoptionen, sie sind jedoch besser darauf zugeschnitten, Muster in den verfügbaren Daten zu identifizieren.  Beim verstärkten Lernen hingegen werden solche Erfahrungen durch Versuch und Irrtum gewonnen.  Das System findet nach und nach die richtigen Optionen oder das globale Optimum.  Ein schwerwiegender zusätzlicher Vorteil des verstärkten Lernens besteht darin, dass in diesem Fall keine umfangreichen Trainingsdaten bereitgestellt werden müssen, wie dies beim Unterrichten mit einem Lehrer der Fall ist.  Ein paar kleine Fragmente werden ausreichen. <br><br>  <b>Das Konzept des verstärkenden Lernens</b> <br><br>  Stellen Sie sich vor, Sie bringen Ihren Katzen neue Tricks bei.  Leider verstehen Katzen die menschliche Sprache nicht, sodass Sie ihnen nicht sagen können, was Sie mit ihnen spielen werden.  Daher werden Sie anders handeln: Imitieren Sie die Situation, und die Katze wird versuchen, auf die eine oder andere Weise zu reagieren.  Wenn die Katze so reagiert hat, wie Sie es wollten, gießen Sie Milch hinein.  Verstehst du, was als nächstes passieren wird?  In einer ähnlichen Situation wird die Katze erneut die gewünschte Aktion ausführen und dies mit noch größerer Begeisterung, in der Hoffnung, dass sie noch besser gefüttert wird.  So findet das Lernen an einem positiven Beispiel statt;  Wenn Sie jedoch versuchen, eine Katze mit negativen Anreizen zu „erziehen“, z. B. sie genau betrachten und die Stirn runzeln, lernt sie in solchen Situationen normalerweise nicht. <br><br>  Verstärktes Lernen funktioniert ähnlich.  Wir teilen der Maschine einige Eingaben und Aktionen mit und belohnen die Maschine dann abhängig von der Ausgabe.  Unser oberstes Ziel ist es, die Belohnungen zu maximieren.  Schauen wir uns nun an, wie das oben genannte Problem im Hinblick auf das verstärkte Lernen neu formuliert werden kann. <br><br><ul><li>  Die Katze fungiert als "Agent", der der "Umwelt" ausgesetzt ist. </li><li>  Die Umgebung ist ein Heim oder ein Spielbereich, je nachdem, was Sie der Katze beibringen. </li><li>  Situationen, die sich aus dem Training ergeben, werden als „Zustände“ bezeichnet.  Im Fall einer Katze sind Beispiele für Bedingungen, wenn die Katze "rennt" oder "unter das Bett kriecht". </li><li>  Agenten reagieren, indem sie Aktionen ausführen und von einem „Zustand“ in einen anderen wechseln. </li><li>  Nachdem sich der Status geändert hat, erhält der Agent je nach der von ihm ergriffenen Maßnahme eine „Belohnung“ oder eine „Geldstrafe“. </li><li>  "Strategie" ist eine Methode zur Auswahl einer Aktion, um die besten Ergebnisse zu erzielen. </li></ul><br>  Nachdem wir herausgefunden haben, was Bestärkungslernen ist, lassen Sie uns detailliert über die Ursprünge und die Entwicklung des Bestärkungslernens und des Tiefenverstärkungslernens sprechen, diskutieren, wie dieses Paradigma es uns ermöglicht, Probleme zu lösen, die für das Lernen mit oder ohne Lehrer unmöglich sind, und beachten Sie auch Folgendes Merkwürdige Tatsache: Derzeit wird die Google-Suchmaschine mithilfe von Algorithmen zum Lernen der Verstärkung optimiert. <br><br>  <b>Verständnis der Terminologie des Verstärkungslernens</b> <br><br>  Agent und Umgebung spielen eine Schlüsselrolle im Verstärkungslernalgorithmus.  Die Umgebung ist die Welt, in der der Agent überleben muss.  Zusätzlich erhält der Agent verstärkende Signale von der Umgebung (Belohnung): Dies ist eine Zahl, die angibt, wie gut oder schlecht der aktuelle Zustand der Welt berücksichtigt werden kann.  Der Zweck des Agenten ist es, die Gesamtbelohnung, den sogenannten "Gewinn", zu maximieren.  Bevor Sie unsere ersten Verstärkungslernalgorithmen schreiben, müssen Sie die folgende Terminologie verstehen. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>Zustände</b> : Ein Zustand ist eine vollständige Beschreibung einer Welt, in der kein einziges Fragment der Informationen, die diese Welt charakterisieren, fehlt.  Es kann eine feste oder dynamische Position sein.  Solche Zustände werden in der Regel in Form von Arrays, Matrizen oder Tensoren höherer Ordnung geschrieben. </li><li>  <b>Aktion</b> : Die Aktion hängt normalerweise von den Umgebungsbedingungen ab. In verschiedenen Umgebungen ergreift der Agent verschiedene Aktionen.  Viele gültige Agentenaktionen werden in einem Bereich aufgezeichnet, der als "Aktionsbereich" bezeichnet wird.  Normalerweise ist die Anzahl der Aktionen im Raum begrenzt. </li><li>  <b>Umgebung</b> : Dies ist der Ort, an dem der Agent existiert und mit dem er interagiert.  Verschiedene Arten von Belohnungen, Strategien usw. werden für verschiedene Umgebungen verwendet. </li><li>  <b>Belohnungen</b> und <b>Gewinne</b> : Sie müssen die Belohnungsfunktion R ständig überwachen, wenn Sie mit Verstärkungen trainieren.  Dies ist wichtig, wenn Sie einen Algorithmus einrichten, optimieren und wenn Sie aufhören zu lernen.  Es hängt vom aktuellen Zustand der Welt, den gerade ergriffenen Maßnahmen und dem nächsten Zustand der Welt ab. </li><li>  <b>Strategien</b> : Eine Strategie ist eine Regel, nach der ein Agent die nächste Aktion auswählt.  Der Satz von Strategien wird auch als "Gehirn" des Agenten bezeichnet. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Nachdem wir uns mit der Terminologie des verstärkenden Lernens vertraut gemacht haben, lösen wir das Problem mithilfe der entsprechenden Algorithmen.  Zuvor müssen Sie verstehen, wie ein solches Problem zu formulieren ist, und sich bei der Lösung dieses Problems auf die Terminologie des Trainings mit Verstärkung verlassen. <br><br>  <b>Taxilösung</b> <br><br>  Also lösen wir das Problem mithilfe von Verstärkungsalgorithmen. <br>  Angenommen, wir haben eine Trainingszone für ein unbemanntes Taxi, die wir trainieren, um Passagiere an vier verschiedenen Punkten ( <code>R,G,Y,B</code> ) zum Parkplatz zu bringen.  Vorher müssen Sie die Umgebung verstehen und festlegen, in der wir mit der Programmierung in Python beginnen.  Wenn Sie gerade erst anfangen, Python zu lernen, empfehle ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Artikel für Sie</a> . <br><br>  Die Umgebung zum Lösen eines Problems mit einem Taxi kann mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gym</a> von OpenAI konfiguriert werden - dies ist eine der beliebtesten Bibliotheken zum Lösen von Problemen mit dem Verstärkungstraining.  Bevor Sie das Fitnessstudio verwenden, müssen Sie es auf Ihrem Computer installieren. Ein Python-Paketmanager namens pip ist hierfür geeignet.  Das Folgende ist der Installationsbefehl. <br><br> <code>pip install gym</code> <br> <br>  Als nächstes wollen wir sehen, wie unsere Umgebung angezeigt wird.  Alle Modelle und die Schnittstelle für diese Aufgabe sind bereits im Fitnessstudio konfiguriert und unter <code>Taxi-V2</code> .  Das folgende Codefragment wird verwendet, um diese Umgebung anzuzeigen. <br><br>  „Wir haben 4 Standorte (durch verschiedene Buchstaben gekennzeichnet);  Unsere Aufgabe ist es, einen Passagier an einem Punkt abzuholen und an einem anderen abzusetzen.  Wir erhalten +20 Punkte für eine erfolgreiche Passagierlandung und verlieren 1 Punkt für jeden Schritt, der dafür ausgegeben wird.  Es gibt auch eine Strafe von 10 Punkten für jedes unbeabsichtigte Ein- und Aussteigen eines Passagiers.  (Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Hier ist die Ausgabe, die wir in unserer Konsole sehen: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  Großartig, <code>env</code> ist das Herz von OpenAi Gym, es ist eine einheitliche Umgebung.  Die folgenden env-Methoden finden wir nützlich: <br><br>  <code>env.reset</code> : <code>env.reset</code> die Umgebung zurück und gibt einen zufälligen Anfangszustand zurück. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> Entwicklung der Umwelt schrittweise. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> die folgenden Variablen zurück <br><br><ul><li>  <code>observation</code> : Beobachtung der Umwelt. </li><li>  <code>reward</code> : <code>reward</code> ob Ihre Aktion von Vorteil war. </li><li>  <code>done</code> : Gibt an, ob es uns gelungen ist, den Passagier ordnungsgemäß aufzunehmen und abzusetzen, was auch als "eine Episode" bezeichnet wird. </li><li>  <code>info</code> : Zusätzliche Informationen wie Leistung und Latenz, die für Debugging-Zwecke benötigt werden. </li><li>  <code>env.render</code> : Zeigt einen Frame der Umgebung an (nützlich zum Rendern) </li></ul><br>  Nachdem wir die Umgebung untersucht haben, versuchen wir, das Problem besser zu verstehen.  Taxis sind das einzige Auto auf diesem Parkplatz.  Das Parken kann in ein <code>5x5</code> Raster unterteilt werden, in dem wir 25 mögliche Taxistandorte erhalten.  Diese 25 Werte sind eines der Elemente unseres Zustandsraums.  Bitte beachten Sie: Im Moment befindet sich unser Taxi am Punkt mit den Koordinaten (3, 1). <br><br>  Es gibt 4 Punkte in der Umgebung, an denen Passagiere einsteigen dürfen: Dies sind: <code>R, G, Y, B</code> oder <code>[(0,0), (0,4), (4,0), (4,3)]</code> in Koordinaten ( horizontal; vertikal), wenn es möglich wäre, die obige Umgebung in kartesischen Koordinaten zu interpretieren.  Wenn Sie auch einen weiteren (1) Zustand des Passagiers berücksichtigen: Innerhalb des Taxis können Sie alle Kombinationen von Passagierstandorten und deren Zielen berücksichtigen, um die Gesamtzahl der Zustände in unserer Umgebung für das Taxitraining zu berechnen: Wir haben vier (4) Ziele und fünf (4+) 1) Passagierstandorte. <br><br>  In unserer Umgebung für ein Taxi gibt es also 5 × 5 × 5 × 4 = 500 mögliche Zustände.  Ein Agent behandelt eine von 500 Bedingungen und ergreift Maßnahmen.  In unserem Fall stehen folgende Optionen zur Verfügung: Bewegung in die eine oder andere Richtung oder Entscheidung, den Passagier abzuholen / abzusetzen.  Mit anderen Worten, wir haben sechs mögliche Aktionen zur Verfügung: <br>  Pickup, Drop, Nord, Ost, Süd, West (Die letzten vier Werte sind Richtungen, in die sich ein Taxi bewegen kann.) <br><br>  Dies ist der <code>action space</code> : Die Menge aller Aktionen, die unser Agent in einem bestimmten Status ausführen kann. <br><br>  Wie aus der obigen Abbildung hervorgeht, kann ein Taxi in bestimmten Situationen bestimmte Aktionen nicht ausführen (Wände stören).  In dem Code, der die Umgebung beschreibt, weisen wir einfach eine Strafe von -1 für jeden Treffer in der Wand und ein Taxi zu, das mit der Wand kollidiert.  Daher werden sich solche Geldstrafen ansammeln, sodass das Taxi versucht, nicht gegen die Wände zu stoßen. <br><br>  Belohnungstabelle: Beim Erstellen einer Taxi-Umgebung kann auch eine primäre Belohnungstabelle mit dem Namen P erstellt werden. Sie können sie als Matrix betrachten, wobei die Anzahl der Status der Anzahl der Zeilen und die Anzahl der Aktionen der Anzahl der Spalten entspricht.  Das heißt, wir sprechen über die Matrix <code>states × actions</code> . <br><br>  Da absolut alle Bedingungen in dieser Matrix aufgezeichnet sind, können Sie die Standardbelohnungswerte anzeigen, die dem Status zugewiesen sind, den wir zur Veranschaulichung ausgewählt haben: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  Die Struktur dieses Wörterbuchs ist wie folgt: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Die Werte 0–5 entsprechen den Aktionen (Süd, Nord, Ost, West, Abholung, Rückgabe), die ein Taxi im in der Abbildung gezeigten aktuellen Zustand ausführen kann. </li><li>  Mit done können Sie beurteilen, wann wir den Passagier erfolgreich an der gewünschten Stelle abgesetzt haben. </li></ul><br>  Um dieses Problem ohne Training mit Verstärkung zu lösen, können Sie den Zielstatus festlegen, eine Auswahl von Feldern treffen und dann, wenn Sie den Zielstatus für eine bestimmte Anzahl von Iterationen erreichen können, davon ausgehen, dass dieser Moment der maximalen Belohnung entspricht.  In anderen Staaten nähert sich der Wert der Belohnung entweder dem Maximum, wenn das Programm korrekt funktioniert (nähert sich dem Ziel) oder sammelt Geldstrafen, wenn es Fehler macht.  Darüber hinaus kann der Wert der Geldbuße nicht unter -10 liegen. <br><br>  Schreiben wir Code, um dieses Problem ohne Verstärkungstraining zu lösen. <br>  Da wir für jeden Staat eine P-Tabelle mit Standardbelohnungswerten haben, können wir versuchen, die Navigation unseres Taxis nur anhand dieser Tabelle zu organisieren. <br><br>  Wir erstellen eine Endlosschleife, in der gescrollt wird, bis der Passagier das Ziel erreicht (eine Episode) oder mit anderen Worten, bis die Belohnungsrate 20 erreicht. Die Methode <code>env.action_space.sample()</code> wählt automatisch eine zufällige Aktion aus der Menge aller verfügbaren Aktionen aus .  Überlegen Sie, was passiert: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Fazit: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  Credits: OpenAI <br><br>  Das Problem ist gelöst, aber nicht optimiert, oder dieser Algorithmus funktioniert nicht in allen Fällen.  Wir benötigen einen geeigneten Interaktionsagenten, damit die Anzahl der von der Maschine / dem Algorithmus zur Lösung des Problems aufgewendeten Iterationen minimal bleibt.  Hier hilft uns der Q-Learning-Algorithmus, dessen Implementierung wir im nächsten Abschnitt betrachten werden. <br><br>  <b>Einführung in Q-Learning</b> <br><br>  Im Folgenden finden Sie den beliebtesten und einfachsten Algorithmus zum Erlernen von Verstärkungen.  Die Umgebung belohnt den Agenten für die schrittweise Ausbildung und für die Tatsache, dass er in einem bestimmten Zustand den optimalsten Schritt unternimmt.  In der oben diskutierten Implementierung hatten wir eine Belohnungstabelle "P", nach der unser Agent lernen wird.  Basierend auf der Belohnungstabelle wählt er die nächste Aktion aus, je nachdem, wie nützlich sie ist, und aktualisiert dann einen anderen Wert, den Q-Wert.  Als Ergebnis wird eine neue Tabelle erstellt, die als Q-Tabelle bezeichnet wird und in der Kombination angezeigt wird (Status, Aktion).  Wenn die Q-Werte besser sind, erhalten wir optimierte Belohnungen. <br><br>  Befindet sich ein Taxi beispielsweise in einem Zustand, in dem sich der Passagier am selben Punkt wie das Taxi befindet, ist es sehr wahrscheinlich, dass der Q-Wert für die Aktion „Abholen“ höher ist als für andere Aktionen, z. B. „Passagier absetzen“ oder „Nach Norden fahren“ ". <br>  Q-Werte werden mit zufälligen Werten initialisiert. Wenn der Agent mit der Umgebung interagiert und durch Ausführen bestimmter Aktionen verschiedene Belohnungen erhält, werden die Q-Werte gemäß der folgenden Gleichung aktualisiert: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Dies wirft die Frage auf: Wie werden Q-Werte initialisiert und wie werden sie berechnet?  Während Aktionen ausgeführt werden, werden Q-Werte in dieser Gleichung ausgeführt. <br><br>  Hier sind Alpha und Gamma die Parameter des Q-Learning-Algorithmus.  Alpha ist das Lerntempo und Gamma ist der Abzinsungsfaktor.  Beide Werte können zwischen 0 und 1 liegen und sind manchmal gleich eins.  Gamma kann gleich Null sein, Alpha jedoch nicht, da der Wert der Verluste während der Aktualisierung kompensiert werden muss (die Lernrate ist positiv).  Der Alpha-Wert ist hier der gleiche wie beim Unterrichten mit einem Lehrer.  Gamma bestimmt, wie wichtig wir die Belohnungen geben möchten, die uns in Zukunft erwarten. <br><br>  Dieser Algorithmus ist unten zusammengefasst: <br><br><ul><li>  Schritt 1: Initialisieren Sie die Q-Tabelle, füllen Sie sie mit Nullen und setzen Sie für Q-Werte beliebige Konstanten. </li><li>  Schritt 2: Lassen Sie den Agenten nun auf die Umgebung reagieren und verschiedene Aktionen ausprobieren.  Für jede Zustandsänderung wählen wir eine aller in diesem Zustand möglichen Aktionen aus (S). </li><li>  Schritt 3: Gehen Sie basierend auf den Ergebnissen der vorherigen Aktion (a) zum nächsten Zustand (S '). </li><li>  Schritt 4: Wählen Sie für alle möglichen Aktionen aus dem Status (S ') eine mit dem höchsten Q-Wert aus. </li><li>  Schritt 5: Aktualisieren Sie die Werte der Q-Tabelle gemäß der obigen Gleichung. </li><li>  Schritt 6: Verwandeln Sie den nächsten Zustand in den aktuellen. </li><li>  Schritt 7: Wenn der Zielzustand erreicht ist, schließen wir den Vorgang ab und wiederholen ihn. </li></ul><br>  <b>Q-Learning in Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Großartig, jetzt werden alle Ihre Werte in der Variablen <code>q_table</code> gespeichert. <br><br>  Ihr Modell ist also unter Umgebungsbedingungen geschult und weiß jetzt, wie Passagiere genauer ausgewählt werden können.  Sie haben das Phänomen des verstärkenden Lernens kennengelernt und können den Algorithmus so programmieren, dass ein neues Problem gelöst wird. <br><br>  Andere Techniken des verstärkenden Lernens: <br><br><ul><li>  Markov-Entscheidungsprozesse (MDP) und Bellman-Gleichungen </li><li>  Dynamische Programmierung: Modellbasierte RL, Strategieiteration und Wertiteration </li><li>  Tiefes Q-Training </li><li>  Strategie Gradientenabstiegsmethoden </li><li>  Sarsa </li></ul><br>  Der Code für diese Übung befindet sich unter: <br><br>  Vihar / Python-Verstärkung-Lernen </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434738/">https://habr.com/ru/post/de434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434728/index.html">Menschen und Prozesse: Warum ist udalenka nicht für jedes Unternehmen geeignet?</a></li>
<li><a href="../de434730/index.html">In-Memory-Datenbanken: Anwendung, Skalierung und wichtige Ergänzungen</a></li>
<li><a href="../de434732/index.html">Leben bei 6200 DPI. HyperX Pulsefire Core Review</a></li>
<li><a href="../de434734/index.html">Fourier-Transformation. Das schnelle und das wütende</a></li>
<li><a href="../de434736/index.html">Verwenden der Mikrotik-Protokolldatenbank zur Unterdrückung von Brute Force</a></li>
<li><a href="../de434740/index.html">Das neuronale Netz lehrte, Sonnenkollektoren in Satellitenbildern zu erkennen und das Ausmaß ihrer Verteilung vorherzusagen</a></li>
<li><a href="../de434742/index.html">Teil 2: Verwenden der UDB-PSoC-Controller von Cypress, um die Anzahl der Interrupts in einem 3D-Drucker zu reduzieren</a></li>
<li><a href="../de434744/index.html">Samsung SSD 860 QVO 1 TB und 4 TB: der erste Consumer SATA QLC (2 Teile)</a></li>
<li><a href="../de434746/index.html">BLE unter dem Mikroskop 4</a></li>
<li><a href="../de434750/index.html">So übernehmen Sie die Kontrolle über Ihre Netzwerkinfrastruktur. Kapitel zwei Reinigung und Dokumentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>