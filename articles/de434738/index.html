<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üò≤ üõ∑ ‚è≥ Verst√§rkungslernen in Python ü§ôüèø üèÜ üë©üèΩ‚Äçü§ù‚Äçüë®üèº</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo Kollegen! 



 In der letzten Ver√∂ffentlichung des kommenden Jahres wollten wir Reinforcement Learning erw√§hnen - ein Thema, in das wir bereits ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Verst√§rkungslernen in Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/piter/blog/434738/">  Hallo Kollegen! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  In der letzten Ver√∂ffentlichung des kommenden Jahres wollten wir Reinforcement Learning erw√§hnen - ein Thema, in das wir bereits ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Buch</a> √ºbersetzen. <br><br>  √úberzeugen Sie sich selbst: Es gab einen elementaren Artikel mit Medium, in dem der Kontext des Problems beschrieben und der einfachste Algorithmus mit Implementierung in Python beschrieben wurde.  Der Artikel hat mehrere Gifs.  Und Motivation, Belohnung und die Wahl der richtigen Strategie auf dem Weg zum Erfolg sind Dinge, die f√ºr jeden von uns im kommenden Jahr √§u√üerst n√ºtzlich sein werden. <br><br>  Viel Spa√ü beim Lesen! <br><a name="habracut"></a><br>  Verst√§rktes Lernen ist eine Form des maschinellen Lernens, bei der der Agent lernt, in der Umgebung zu handeln, Aktionen auszuf√ºhren und dadurch die Intuition zu entwickeln. Danach beobachtet er die Ergebnisse seiner Aktionen.  In diesem Artikel werde ich Ihnen erkl√§ren, wie Sie das Problem des Lernens mit Verst√§rkung verstehen und formulieren und es dann in Python l√∂sen k√∂nnen. <br><br><br>  In letzter Zeit haben wir uns daran gew√∂hnt, dass Computer Spiele gegen Menschen spielen - entweder als Bots in Multiplayer-Spielen oder als Rivalen in Einzelspielen: beispielsweise in Dota2, PUB-G, Mario.  Das Forschungsunternehmen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Deepmind</a> machte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viel Aufhebens</a> um die Neuigkeiten, als sein AlphaGo-Programm 2016 den s√ºdkoreanischen Meister 2016 besiegte.  Wenn Sie ein begeisterter Spieler sind, k√∂nnen Sie von den f√ºnf Spielen von Dota 2 OpenAI Five h√∂ren, bei denen Autos gegen Menschen k√§mpften und die besten Spieler in Dota2 in mehreren Spielen besiegten.  (Wenn Sie an den Details interessiert sind, wird der Algorithmus <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> detailliert analysiert und untersucht, wie die Maschinen gespielt haben). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  Die neueste Version von OpenAI Five <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">nimmt Roshan</a> . <br><br>  Beginnen wir also mit der zentralen Frage.  Warum brauchen wir eine verst√§rkte Ausbildung?  Wird es nur in Spielen verwendet oder ist es in realistischen Szenarien zur L√∂sung angewandter Probleme anwendbar?  Wenn Sie zum ersten Mal ein Verst√§rkungstraining lesen, k√∂nnen Sie sich die Antwort auf diese Fragen einfach nicht vorstellen.  In der Tat ist verst√§rktes Lernen eine der am weitesten verbreiteten und sich schnell entwickelnden Technologien auf dem Gebiet der k√ºnstlichen Intelligenz. <br>  Hier einige Themenbereiche, in denen Lernsysteme zur St√§rkung besonders gefragt sind: <br><br><ol><li>  Unbemannte Fahrzeuge </li><li>  Spielebranche </li><li>  Robotik </li><li>  Empfehlungssysteme </li><li>  Werbung und Marketing </li></ol><br>  <b>√úberblick und Hintergrund des verst√§rkenden Lernens</b> <br><br>  Wie hat sich das Lernph√§nomen mit Verst√§rkung entwickelt, als wir √ºber so viele Methoden des maschinellen und tiefen Lernens verf√ºgten?  "Er wurde von Rich Sutton und Andrew Barto, Richs Forschungsleiter, erfunden, die ihm bei der Vorbereitung der Promotion geholfen haben."  Das Paradigma nahm erstmals in den 1980er Jahren Gestalt an und war dann archaisch.  Anschlie√üend glaubte Rich, dass sie eine gro√üe Zukunft hatte und schlie√ülich Anerkennung erhalten w√ºrde. <br><br>  Verst√§rktes Lernen unterst√ºtzt die Automatisierung in der Umgebung, in der es bereitgestellt wird.  Sowohl maschinelles als auch tiefes Lernen funktionieren ungef√§hr gleich - sie sind strategisch unterschiedlich angeordnet, aber beide Paradigmen unterst√ºtzen die Automatisierung.  Warum entstand ein Verst√§rkungstraining? <br><br>  Es erinnert sehr an den nat√ºrlichen Lernprozess, in dem der Prozess / das Modell agiert, und erh√§lt Feedback dar√ºber, wie sie es schafft, die Aufgabe zu bew√§ltigen: gut und nicht. <br><br>  Maschinelles und tiefes Lernen sind ebenfalls Trainingsoptionen, sie sind jedoch besser darauf zugeschnitten, Muster in den verf√ºgbaren Daten zu identifizieren.  Beim verst√§rkten Lernen hingegen werden solche Erfahrungen durch Versuch und Irrtum gewonnen.  Das System findet nach und nach die richtigen Optionen oder das globale Optimum.  Ein schwerwiegender zus√§tzlicher Vorteil des verst√§rkten Lernens besteht darin, dass in diesem Fall keine umfangreichen Trainingsdaten bereitgestellt werden m√ºssen, wie dies beim Unterrichten mit einem Lehrer der Fall ist.  Ein paar kleine Fragmente werden ausreichen. <br><br>  <b>Das Konzept des verst√§rkenden Lernens</b> <br><br>  Stellen Sie sich vor, Sie bringen Ihren Katzen neue Tricks bei.  Leider verstehen Katzen die menschliche Sprache nicht, sodass Sie ihnen nicht sagen k√∂nnen, was Sie mit ihnen spielen werden.  Daher werden Sie anders handeln: Imitieren Sie die Situation, und die Katze wird versuchen, auf die eine oder andere Weise zu reagieren.  Wenn die Katze so reagiert hat, wie Sie es wollten, gie√üen Sie Milch hinein.  Verstehst du, was als n√§chstes passieren wird?  In einer √§hnlichen Situation wird die Katze erneut die gew√ºnschte Aktion ausf√ºhren und dies mit noch gr√∂√üerer Begeisterung, in der Hoffnung, dass sie noch besser gef√ºttert wird.  So findet das Lernen an einem positiven Beispiel statt;  Wenn Sie jedoch versuchen, eine Katze mit negativen Anreizen zu ‚Äûerziehen‚Äú, z. B. sie genau betrachten und die Stirn runzeln, lernt sie in solchen Situationen normalerweise nicht. <br><br>  Verst√§rktes Lernen funktioniert √§hnlich.  Wir teilen der Maschine einige Eingaben und Aktionen mit und belohnen die Maschine dann abh√§ngig von der Ausgabe.  Unser oberstes Ziel ist es, die Belohnungen zu maximieren.  Schauen wir uns nun an, wie das oben genannte Problem im Hinblick auf das verst√§rkte Lernen neu formuliert werden kann. <br><br><ul><li>  Die Katze fungiert als "Agent", der der "Umwelt" ausgesetzt ist. </li><li>  Die Umgebung ist ein Heim oder ein Spielbereich, je nachdem, was Sie der Katze beibringen. </li><li>  Situationen, die sich aus dem Training ergeben, werden als ‚ÄûZust√§nde‚Äú bezeichnet.  Im Fall einer Katze sind Beispiele f√ºr Bedingungen, wenn die Katze "rennt" oder "unter das Bett kriecht". </li><li>  Agenten reagieren, indem sie Aktionen ausf√ºhren und von einem ‚ÄûZustand‚Äú in einen anderen wechseln. </li><li>  Nachdem sich der Status ge√§ndert hat, erh√§lt der Agent je nach der von ihm ergriffenen Ma√ünahme eine ‚ÄûBelohnung‚Äú oder eine ‚ÄûGeldstrafe‚Äú. </li><li>  "Strategie" ist eine Methode zur Auswahl einer Aktion, um die besten Ergebnisse zu erzielen. </li></ul><br>  Nachdem wir herausgefunden haben, was Best√§rkungslernen ist, lassen Sie uns detailliert √ºber die Urspr√ºnge und die Entwicklung des Best√§rkungslernens und des Tiefenverst√§rkungslernens sprechen, diskutieren, wie dieses Paradigma es uns erm√∂glicht, Probleme zu l√∂sen, die f√ºr das Lernen mit oder ohne Lehrer unm√∂glich sind, und beachten Sie auch Folgendes Merkw√ºrdige Tatsache: Derzeit wird die Google-Suchmaschine mithilfe von Algorithmen zum Lernen der Verst√§rkung optimiert. <br><br>  <b>Verst√§ndnis der Terminologie des Verst√§rkungslernens</b> <br><br>  Agent und Umgebung spielen eine Schl√ºsselrolle im Verst√§rkungslernalgorithmus.  Die Umgebung ist die Welt, in der der Agent √ºberleben muss.  Zus√§tzlich erh√§lt der Agent verst√§rkende Signale von der Umgebung (Belohnung): Dies ist eine Zahl, die angibt, wie gut oder schlecht der aktuelle Zustand der Welt ber√ºcksichtigt werden kann.  Der Zweck des Agenten ist es, die Gesamtbelohnung, den sogenannten "Gewinn", zu maximieren.  Bevor Sie unsere ersten Verst√§rkungslernalgorithmen schreiben, m√ºssen Sie die folgende Terminologie verstehen. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>Zust√§nde</b> : Ein Zustand ist eine vollst√§ndige Beschreibung einer Welt, in der kein einziges Fragment der Informationen, die diese Welt charakterisieren, fehlt.  Es kann eine feste oder dynamische Position sein.  Solche Zust√§nde werden in der Regel in Form von Arrays, Matrizen oder Tensoren h√∂herer Ordnung geschrieben. </li><li>  <b>Aktion</b> : Die Aktion h√§ngt normalerweise von den Umgebungsbedingungen ab. In verschiedenen Umgebungen ergreift der Agent verschiedene Aktionen.  Viele g√ºltige Agentenaktionen werden in einem Bereich aufgezeichnet, der als "Aktionsbereich" bezeichnet wird.  Normalerweise ist die Anzahl der Aktionen im Raum begrenzt. </li><li>  <b>Umgebung</b> : Dies ist der Ort, an dem der Agent existiert und mit dem er interagiert.  Verschiedene Arten von Belohnungen, Strategien usw. werden f√ºr verschiedene Umgebungen verwendet. </li><li>  <b>Belohnungen</b> und <b>Gewinne</b> : Sie m√ºssen die Belohnungsfunktion R st√§ndig √ºberwachen, wenn Sie mit Verst√§rkungen trainieren.  Dies ist wichtig, wenn Sie einen Algorithmus einrichten, optimieren und wenn Sie aufh√∂ren zu lernen.  Es h√§ngt vom aktuellen Zustand der Welt, den gerade ergriffenen Ma√ünahmen und dem n√§chsten Zustand der Welt ab. </li><li>  <b>Strategien</b> : Eine Strategie ist eine Regel, nach der ein Agent die n√§chste Aktion ausw√§hlt.  Der Satz von Strategien wird auch als "Gehirn" des Agenten bezeichnet. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Nachdem wir uns mit der Terminologie des verst√§rkenden Lernens vertraut gemacht haben, l√∂sen wir das Problem mithilfe der entsprechenden Algorithmen.  Zuvor m√ºssen Sie verstehen, wie ein solches Problem zu formulieren ist, und sich bei der L√∂sung dieses Problems auf die Terminologie des Trainings mit Verst√§rkung verlassen. <br><br>  <b>Taxil√∂sung</b> <br><br>  Also l√∂sen wir das Problem mithilfe von Verst√§rkungsalgorithmen. <br>  Angenommen, wir haben eine Trainingszone f√ºr ein unbemanntes Taxi, die wir trainieren, um Passagiere an vier verschiedenen Punkten ( <code>R,G,Y,B</code> ) zum Parkplatz zu bringen.  Vorher m√ºssen Sie die Umgebung verstehen und festlegen, in der wir mit der Programmierung in Python beginnen.  Wenn Sie gerade erst anfangen, Python zu lernen, empfehle ich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Artikel f√ºr Sie</a> . <br><br>  Die Umgebung zum L√∂sen eines Problems mit einem Taxi kann mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Gym</a> von OpenAI konfiguriert werden - dies ist eine der beliebtesten Bibliotheken zum L√∂sen von Problemen mit dem Verst√§rkungstraining.  Bevor Sie das Fitnessstudio verwenden, m√ºssen Sie es auf Ihrem Computer installieren. Ein Python-Paketmanager namens pip ist hierf√ºr geeignet.  Das Folgende ist der Installationsbefehl. <br><br> <code>pip install gym</code> <br> <br>  Als n√§chstes wollen wir sehen, wie unsere Umgebung angezeigt wird.  Alle Modelle und die Schnittstelle f√ºr diese Aufgabe sind bereits im Fitnessstudio konfiguriert und unter <code>Taxi-V2</code> .  Das folgende Codefragment wird verwendet, um diese Umgebung anzuzeigen. <br><br>  ‚ÄûWir haben 4 Standorte (durch verschiedene Buchstaben gekennzeichnet);  Unsere Aufgabe ist es, einen Passagier an einem Punkt abzuholen und an einem anderen abzusetzen.  Wir erhalten +20 Punkte f√ºr eine erfolgreiche Passagierlandung und verlieren 1 Punkt f√ºr jeden Schritt, der daf√ºr ausgegeben wird.  Es gibt auch eine Strafe von 10 Punkten f√ºr jedes unbeabsichtigte Ein- und Aussteigen eines Passagiers.  (Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Hier ist die Ausgabe, die wir in unserer Konsole sehen: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  Gro√üartig, <code>env</code> ist das Herz von OpenAi Gym, es ist eine einheitliche Umgebung.  Die folgenden env-Methoden finden wir n√ºtzlich: <br><br>  <code>env.reset</code> : <code>env.reset</code> die Umgebung zur√ºck und gibt einen zuf√§lligen Anfangszustand zur√ºck. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> Entwicklung der Umwelt schrittweise. <br>  <code>env.step(action)</code> : <code>env.step(action)</code> die folgenden Variablen zur√ºck <br><br><ul><li>  <code>observation</code> : Beobachtung der Umwelt. </li><li>  <code>reward</code> : <code>reward</code> ob Ihre Aktion von Vorteil war. </li><li>  <code>done</code> : Gibt an, ob es uns gelungen ist, den Passagier ordnungsgem√§√ü aufzunehmen und abzusetzen, was auch als "eine Episode" bezeichnet wird. </li><li>  <code>info</code> : Zus√§tzliche Informationen wie Leistung und Latenz, die f√ºr Debugging-Zwecke ben√∂tigt werden. </li><li>  <code>env.render</code> : Zeigt einen Frame der Umgebung an (n√ºtzlich zum Rendern) </li></ul><br>  Nachdem wir die Umgebung untersucht haben, versuchen wir, das Problem besser zu verstehen.  Taxis sind das einzige Auto auf diesem Parkplatz.  Das Parken kann in ein <code>5x5</code> Raster unterteilt werden, in dem wir 25 m√∂gliche Taxistandorte erhalten.  Diese 25 Werte sind eines der Elemente unseres Zustandsraums.  Bitte beachten Sie: Im Moment befindet sich unser Taxi am Punkt mit den Koordinaten (3, 1). <br><br>  Es gibt 4 Punkte in der Umgebung, an denen Passagiere einsteigen d√ºrfen: Dies sind: <code>R, G, Y, B</code> oder <code>[(0,0), (0,4), (4,0), (4,3)]</code> in Koordinaten ( horizontal; vertikal), wenn es m√∂glich w√§re, die obige Umgebung in kartesischen Koordinaten zu interpretieren.  Wenn Sie auch einen weiteren (1) Zustand des Passagiers ber√ºcksichtigen: Innerhalb des Taxis k√∂nnen Sie alle Kombinationen von Passagierstandorten und deren Zielen ber√ºcksichtigen, um die Gesamtzahl der Zust√§nde in unserer Umgebung f√ºr das Taxitraining zu berechnen: Wir haben vier (4) Ziele und f√ºnf (4+) 1) Passagierstandorte. <br><br>  In unserer Umgebung f√ºr ein Taxi gibt es also 5 √ó 5 √ó 5 √ó 4 = 500 m√∂gliche Zust√§nde.  Ein Agent behandelt eine von 500 Bedingungen und ergreift Ma√ünahmen.  In unserem Fall stehen folgende Optionen zur Verf√ºgung: Bewegung in die eine oder andere Richtung oder Entscheidung, den Passagier abzuholen / abzusetzen.  Mit anderen Worten, wir haben sechs m√∂gliche Aktionen zur Verf√ºgung: <br>  Pickup, Drop, Nord, Ost, S√ºd, West (Die letzten vier Werte sind Richtungen, in die sich ein Taxi bewegen kann.) <br><br>  Dies ist der <code>action space</code> : Die Menge aller Aktionen, die unser Agent in einem bestimmten Status ausf√ºhren kann. <br><br>  Wie aus der obigen Abbildung hervorgeht, kann ein Taxi in bestimmten Situationen bestimmte Aktionen nicht ausf√ºhren (W√§nde st√∂ren).  In dem Code, der die Umgebung beschreibt, weisen wir einfach eine Strafe von -1 f√ºr jeden Treffer in der Wand und ein Taxi zu, das mit der Wand kollidiert.  Daher werden sich solche Geldstrafen ansammeln, sodass das Taxi versucht, nicht gegen die W√§nde zu sto√üen. <br><br>  Belohnungstabelle: Beim Erstellen einer Taxi-Umgebung kann auch eine prim√§re Belohnungstabelle mit dem Namen P erstellt werden. Sie k√∂nnen sie als Matrix betrachten, wobei die Anzahl der Status der Anzahl der Zeilen und die Anzahl der Aktionen der Anzahl der Spalten entspricht.  Das hei√üt, wir sprechen √ºber die Matrix <code>states √ó actions</code> . <br><br>  Da absolut alle Bedingungen in dieser Matrix aufgezeichnet sind, k√∂nnen Sie die Standardbelohnungswerte anzeigen, die dem Status zugewiesen sind, den wir zur Veranschaulichung ausgew√§hlt haben: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  Die Struktur dieses W√∂rterbuchs ist wie folgt: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Die Werte 0‚Äì5 entsprechen den Aktionen (S√ºd, Nord, Ost, West, Abholung, R√ºckgabe), die ein Taxi im in der Abbildung gezeigten aktuellen Zustand ausf√ºhren kann. </li><li>  Mit done k√∂nnen Sie beurteilen, wann wir den Passagier erfolgreich an der gew√ºnschten Stelle abgesetzt haben. </li></ul><br>  Um dieses Problem ohne Training mit Verst√§rkung zu l√∂sen, k√∂nnen Sie den Zielstatus festlegen, eine Auswahl von Feldern treffen und dann, wenn Sie den Zielstatus f√ºr eine bestimmte Anzahl von Iterationen erreichen k√∂nnen, davon ausgehen, dass dieser Moment der maximalen Belohnung entspricht.  In anderen Staaten n√§hert sich der Wert der Belohnung entweder dem Maximum, wenn das Programm korrekt funktioniert (n√§hert sich dem Ziel) oder sammelt Geldstrafen, wenn es Fehler macht.  Dar√ºber hinaus kann der Wert der Geldbu√üe nicht unter -10 liegen. <br><br>  Schreiben wir Code, um dieses Problem ohne Verst√§rkungstraining zu l√∂sen. <br>  Da wir f√ºr jeden Staat eine P-Tabelle mit Standardbelohnungswerten haben, k√∂nnen wir versuchen, die Navigation unseres Taxis nur anhand dieser Tabelle zu organisieren. <br><br>  Wir erstellen eine Endlosschleife, in der gescrollt wird, bis der Passagier das Ziel erreicht (eine Episode) oder mit anderen Worten, bis die Belohnungsrate 20 erreicht. Die Methode <code>env.action_space.sample()</code> w√§hlt automatisch eine zuf√§llige Aktion aus der Menge aller verf√ºgbaren Aktionen aus .  √úberlegen Sie, was passiert: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Fazit: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  Credits: OpenAI <br><br>  Das Problem ist gel√∂st, aber nicht optimiert, oder dieser Algorithmus funktioniert nicht in allen F√§llen.  Wir ben√∂tigen einen geeigneten Interaktionsagenten, damit die Anzahl der von der Maschine / dem Algorithmus zur L√∂sung des Problems aufgewendeten Iterationen minimal bleibt.  Hier hilft uns der Q-Learning-Algorithmus, dessen Implementierung wir im n√§chsten Abschnitt betrachten werden. <br><br>  <b>Einf√ºhrung in Q-Learning</b> <br><br>  Im Folgenden finden Sie den beliebtesten und einfachsten Algorithmus zum Erlernen von Verst√§rkungen.  Die Umgebung belohnt den Agenten f√ºr die schrittweise Ausbildung und f√ºr die Tatsache, dass er in einem bestimmten Zustand den optimalsten Schritt unternimmt.  In der oben diskutierten Implementierung hatten wir eine Belohnungstabelle "P", nach der unser Agent lernen wird.  Basierend auf der Belohnungstabelle w√§hlt er die n√§chste Aktion aus, je nachdem, wie n√ºtzlich sie ist, und aktualisiert dann einen anderen Wert, den Q-Wert.  Als Ergebnis wird eine neue Tabelle erstellt, die als Q-Tabelle bezeichnet wird und in der Kombination angezeigt wird (Status, Aktion).  Wenn die Q-Werte besser sind, erhalten wir optimierte Belohnungen. <br><br>  Befindet sich ein Taxi beispielsweise in einem Zustand, in dem sich der Passagier am selben Punkt wie das Taxi befindet, ist es sehr wahrscheinlich, dass der Q-Wert f√ºr die Aktion ‚ÄûAbholen‚Äú h√∂her ist als f√ºr andere Aktionen, z. B. ‚ÄûPassagier absetzen‚Äú oder ‚ÄûNach Norden fahren‚Äú ". <br>  Q-Werte werden mit zuf√§lligen Werten initialisiert. Wenn der Agent mit der Umgebung interagiert und durch Ausf√ºhren bestimmter Aktionen verschiedene Belohnungen erh√§lt, werden die Q-Werte gem√§√ü der folgenden Gleichung aktualisiert: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  Dies wirft die Frage auf: Wie werden Q-Werte initialisiert und wie werden sie berechnet?  W√§hrend Aktionen ausgef√ºhrt werden, werden Q-Werte in dieser Gleichung ausgef√ºhrt. <br><br>  Hier sind Alpha und Gamma die Parameter des Q-Learning-Algorithmus.  Alpha ist das Lerntempo und Gamma ist der Abzinsungsfaktor.  Beide Werte k√∂nnen zwischen 0 und 1 liegen und sind manchmal gleich eins.  Gamma kann gleich Null sein, Alpha jedoch nicht, da der Wert der Verluste w√§hrend der Aktualisierung kompensiert werden muss (die Lernrate ist positiv).  Der Alpha-Wert ist hier der gleiche wie beim Unterrichten mit einem Lehrer.  Gamma bestimmt, wie wichtig wir die Belohnungen geben m√∂chten, die uns in Zukunft erwarten. <br><br>  Dieser Algorithmus ist unten zusammengefasst: <br><br><ul><li>  Schritt 1: Initialisieren Sie die Q-Tabelle, f√ºllen Sie sie mit Nullen und setzen Sie f√ºr Q-Werte beliebige Konstanten. </li><li>  Schritt 2: Lassen Sie den Agenten nun auf die Umgebung reagieren und verschiedene Aktionen ausprobieren.  F√ºr jede Zustands√§nderung w√§hlen wir eine aller in diesem Zustand m√∂glichen Aktionen aus (S). </li><li>  Schritt 3: Gehen Sie basierend auf den Ergebnissen der vorherigen Aktion (a) zum n√§chsten Zustand (S '). </li><li>  Schritt 4: W√§hlen Sie f√ºr alle m√∂glichen Aktionen aus dem Status (S ') eine mit dem h√∂chsten Q-Wert aus. </li><li>  Schritt 5: Aktualisieren Sie die Werte der Q-Tabelle gem√§√ü der obigen Gleichung. </li><li>  Schritt 6: Verwandeln Sie den n√§chsten Zustand in den aktuellen. </li><li>  Schritt 7: Wenn der Zielzustand erreicht ist, schlie√üen wir den Vorgang ab und wiederholen ihn. </li></ul><br>  <b>Q-Learning in Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Gro√üartig, jetzt werden alle Ihre Werte in der Variablen <code>q_table</code> gespeichert. <br><br>  Ihr Modell ist also unter Umgebungsbedingungen geschult und wei√ü jetzt, wie Passagiere genauer ausgew√§hlt werden k√∂nnen.  Sie haben das Ph√§nomen des verst√§rkenden Lernens kennengelernt und k√∂nnen den Algorithmus so programmieren, dass ein neues Problem gel√∂st wird. <br><br>  Andere Techniken des verst√§rkenden Lernens: <br><br><ul><li>  Markov-Entscheidungsprozesse (MDP) und Bellman-Gleichungen </li><li>  Dynamische Programmierung: Modellbasierte RL, Strategieiteration und Wertiteration </li><li>  Tiefes Q-Training </li><li>  Strategie Gradientenabstiegsmethoden </li><li>  Sarsa </li></ul><br>  Der Code f√ºr diese √úbung befindet sich unter: <br><br>  Vihar / Python-Verst√§rkung-Lernen </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434738/">https://habr.com/ru/post/de434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434728/index.html">Menschen und Prozesse: Warum ist udalenka nicht f√ºr jedes Unternehmen geeignet?</a></li>
<li><a href="../de434730/index.html">In-Memory-Datenbanken: Anwendung, Skalierung und wichtige Erg√§nzungen</a></li>
<li><a href="../de434732/index.html">Leben bei 6200 DPI. HyperX Pulsefire Core Review</a></li>
<li><a href="../de434734/index.html">Fourier-Transformation. Das schnelle und das w√ºtende</a></li>
<li><a href="../de434736/index.html">Verwenden der Mikrotik-Protokolldatenbank zur Unterdr√ºckung von Brute Force</a></li>
<li><a href="../de434740/index.html">Das neuronale Netz lehrte, Sonnenkollektoren in Satellitenbildern zu erkennen und das Ausma√ü ihrer Verteilung vorherzusagen</a></li>
<li><a href="../de434742/index.html">Teil 2: Verwenden der UDB-PSoC-Controller von Cypress, um die Anzahl der Interrupts in einem 3D-Drucker zu reduzieren</a></li>
<li><a href="../de434744/index.html">Samsung SSD 860 QVO 1 TB und 4 TB: der erste Consumer SATA QLC (2 Teile)</a></li>
<li><a href="../de434746/index.html">BLE unter dem Mikroskop 4</a></li>
<li><a href="../de434750/index.html">So √ºbernehmen Sie die Kontrolle √ºber Ihre Netzwerkinfrastruktur. Kapitel zwei Reinigung und Dokumentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>