<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤽🏿 🏂🏾 ⚒️ Computervideo in 755 Megapixeln: Plenoptik gestern, heute und morgen 🚏 📀 👋🏻</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vor einiger Zeit hatte der Autor die Gelegenheit, einen Vortrag bei VGIK zu halten, und es waren viele Leute aus dem Kamerahaus im Publikum. Das Publi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Computervideo in 755 Megapixeln: Plenoptik gestern, heute und morgen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440652/"><img src="https://habrastorage.org/getpro/habr/post_images/edf/637/bcd/edf637bcd51db4e6b8b3e8c13c3491f0.png"><br><br>  Vor einiger Zeit hatte der Autor die Gelegenheit, einen Vortrag bei VGIK zu halten, und es waren viele Leute aus dem Kamerahaus im Publikum.  Das Publikum wurde gefragt: „Mit welcher maximalen Auflösung haben Sie geschossen?“, Und es stellte sich heraus, dass etwa eine dritte Aufnahme 4K oder 8 Megapixel, der Rest - nicht mehr als 2K oder 2 Megapixel.  Es war eine Herausforderung!  Ich musste über die Kamera mit einer Auflösung von 755 Megapixeln (Rohauflösung, um genau zu sein, da sie eine endgültige 4K hat) und welche bezaubernden Möglichkeiten dies für professionelle Aufnahmen bietet, erzählen. <br><br>  Die Kamera selbst sieht so aus (eine Art kleiner Elefant): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/824/93c/286/82493c2865db1d6519d69121b0ba8d9c.png"><br><br>  Außerdem werde ich ein schreckliches Geheimnis eröffnen, um dieses Bild aufzunehmen, suchten wir nach einem besseren Blickwinkel und einer größeren Person.  Ich habe diese Kamera zufällig live gespürt, ich werde sagen, dass sie <b>viel</b> größer aussieht.  Das Bild unten mit Yon Karafin, mit dem wir ungefähr gleich groß sind, spiegelt das Ausmaß der Katastrophe genauer wider: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb5/954/8b6/eb59548b69bc178ace2a379120ca6b33.png"><br><br>  Wer kümmert sich im Prinzip um die Fähigkeiten des berechneten Videos, über das sie selten schreiben - die ganze Wahrheit ist unter dem Strich!  ) <br><a name="habracut"></a><br>  Diskussionen über die Möglichkeiten der Plenoptik erinnern mich an Diskussionen über das erste Flugzeug: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d04/b2f/00f/d04b2f00fc8d68a6b3d830665fdfb433.png"><br><br><hr><br>  Was die Leute sagten: <br>  - <i>Es ist sehr teuer ...</i> <br>  - <i>JA!</i> <br>  - <i>Es ist völlig unpraktisch und zahlt sich jetzt nicht aus ...</i> <br>  - <i>JA!</i> <br>  - <i>Es ist gefährlich zu benutzen ...</i> <br>  - <i>Verdammt, JA!</i> <br>  - <i>Es sieht elend aus!</i> <br>  - <i>Verdammt, JA, aber es <b>fliegt, siehst du, LE-TA-ET !!!</b></i> <br><hr><br>  Nach 50 Jahren führten diese Experimente zu einer grundlegend neuen, bequemen und ziemlich sicheren Art, sich über die Ozeane zu bewegen (und nicht nur). <br><br>  Die Situation ist hier die gleiche.  Absolut das gleiche!  Jetzt erfordert es unvernünftige Kosten, die verdammt unpraktisch zu bedienen sind und elend aussehen, aber es fliegt wirklich!  Und sich der Prinzipien und neuen Perspektiven bewusst zu sein - es ist einfach unglaublich inspirierend (zumindest Ihr bescheidener Diener)! <br><br>  Das Monster auf dem Foto oben erinnert etwas an die ersten Kameras, die ebenfalls groß waren, ein riesiges Objektiv hatten und auf einem speziellen Rahmen gedreht wurden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/feb/153/d5e/feb153d5ebe10e4c0a529270d7244fcc.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fernsehaufnahmen der Olympischen Spiele 1936</a> zu Beginn des Fernsehens</i> <br><br>  Hier sieht viel aus, auch ein riesiges Objektiv (weil es auch nicht genug Licht gibt), eine schwere Kamera mit einer speziellen Federung, riesige Größen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b38/770/bbe/b38770bbefae6795d4fb673436906e6f.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lytro ist bereit, das Filmemachen für immer zu verändern</a></i> <br><br>  Aus dem Interessanten: Der „Koffer“ unten ist ein Flüssigkeitskühlsystem, bei dessen Ausfall die Kamera keine weiteren Bilder aufnehmen kann. <br><br>  Das große Rechteck unter dem Objektiv (auf dem ersten Foto oben deutlich sichtbar) ist nicht die Hintergrundbeleuchtung, sondern das Lidar - ein Laser-Entfernungsmesser, der eine dreidimensionale Szene vor der Kamera liefert. <br><br>  Ein dickes schwarzes Tourniquet unten links im Hintergrund ist ein Bündel von Glasfaserkabeln mit einem Durchmesser von etwa 4 Zentimetern, durch das die Kamera einen ungeheuren Informationsstrom an einen speziellen Speicher sendet. <br><br>  Diejenigen in diesem Thema haben bereits das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lytro-Kino</a> erkannt, von dem 3 Stücke geschaffen wurden.  Die erste, die normalerweise in allen Bildern erscheint, ist an einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lidar</a> unter der Linse leicht zu erkennen.  Nach 1,5 Jahren wurde eine zweite Kammer mit korrigierten strukturellen Problemen des ersten und der beiden Lidare geschaffen (und leider schreibt fast niemand darüber).  Darüber hinaus war der Entwicklungsfortschritt beispielsweise von Lidaren in 1,5 Jahren so groß, dass zwei Lidare der zweiten Kammer zehnmal mehr Punkte ergaben als ein Lidar der ersten.  Die dritte Kamera sollte ungefähr die gleichen Auflösungseigenschaften haben (um die Anforderungen der Filmemacher zu erfüllen), aber halb so groß sein (was für den praktischen Gebrauch entscheidend ist).  Aber ... aber dazu später mehr. <br><br>  In jedem Fall möchte ich darauf hinweisen, dass das Licht nicht darauf konvergierte, da es die herausragende Rolle, die Lytro bei der Popularisierung der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">plenoptischen Schießtechnologie</a> gespielt hat, in jeder Hinsicht anerkannte.  Eine ähnliche Kamera wurde am <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Fraunhofer-Institut hergestellt</a> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Raytrix</a> , ein Hersteller von plenoptischen Kameras <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">, existiert sicher</a> , und ich durfte das Lytro-Kino nicht fotografieren, da ich große Angst vor mehreren chinesischen Unternehmen hatte, die aktiv in dieser Richtung arbeiteten (es wurden keine neuen Informationen darüber gefunden). <br><br>  Schreiben wir daher die Punkte auf, die alle Vorteile der Plenoptik bieten und über die sie fast nicht schreiben.  Aber gerade deshalb ist es garantiert erfolgreich. <br><br><h1>  Anstatt vorzustellen </h1><br>  Nur auf Habré und nur auf Lytro wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">auf ca. 300 Seiten erwähnt</a> , daher werden wir kurz sein. <br><br>  Das Schlüsselkonzept für die plenoptische Aufnahme ist das Lichtfeld, dh wir fixieren nicht die Pixelfarbe an jedem Punkt, sondern eine zweidimensionale Pixelmatrix, die aus einem miserablen zweidimensionalen Rahmen einen normalen vierdimensionalen Rahmen macht (normalerweise mit sehr geringer Auflösung in t und s): <br><br><img src="https://habrastorage.org/webt/kr/tg/ex/krtgexrapzcv70xbte74_cepamo.png"><br><br>  In der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">englischen Wikipedia</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dann in der russischen</a> in Artikeln über das Lichtfeld heißt es, dass "der Ausdruck" Lichtfeld " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von A. Gershun</a> im Jahr 1936 verwendet wurde."  Fairerweise stellen wir fest, dass dies nicht <s>zufällig eine</s> „verwendete Phrase“ war, sondern der Name eines <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kleinen Buches mit</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">178 Seiten</a> , das „Lichtfeld“ genannt wurde: <br><br><img width="300" src="https://habrastorage.org/webt/ba/lw/q_/balwq_fptbhjelv24etahctr5-8.png"><br><br>  Und schon damals waren diese Werke durchaus in der Natur angewendet und der Autor erhielt 6 Jahre später auf dem Höhepunkt des Krieges 1942 den Stalin-Preis zweiten Grades für die Blackout-Methode. <br><br>  In der Praxis bietet das Aufnehmen eines vierdimensionalen Rahmens des Lichtfelds eine Reihe von Mikrolinsen vor dem Sensor der Kamera: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/90f/203/fb4/90f203fb49d7e464638a4f164e96fb26.gif"></a> <br>  <i>Quelle: <a href="">plenoptic.inf (Sie können klicken und in voller Auflösung sehen)</a></i> <br><br>  Gerade weil wir eine Reihe von Mikrolinsen mit einem geringen Abstand zwischen ihnen haben, war es möglich, Lytro Cinema zu erstellen, dessen Sensor aus einer Reihe von Sensoren zusammengesetzt wurde, deren Grenzen an den Rand der Linsen fallen.  Eine Flüssigkeitskühlung war erforderlich, um ein sehr heißes Massiv abzukühlen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f2c/bc4/c3a/f2cbc4c3ad04cfa63d0fd1976fda10a8.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verwenden fokussierter plenoptischer Kameras für die Erfassung umfassender Bilder</a></i> <br><br>  Als Ergebnis erhalten wir die berühmteste Möglichkeit von Plenoptikkameras, über die nur die Faulen nicht geschrieben haben - um die Fokussierentfernung nach der Aufnahme zu ändern (diese niedlichen Vögel würden nicht auf den Fokus warten): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb2/49a/815/eb249a815ede8955910d24aadc495a66.gif"><br>  <i>Quelle: Lytro</i> <br><br>  Und es wurde wenig darüber geschrieben, wie Plenoptik in einer für einen Film geeigneten Auflösung aussieht (wird durch Klicken geöffnet): <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/0bf/247/057/0bf2470573938428dae4ae4f2c8ca217.png"></a> <br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Watch Lytro Change Cinematography Forever</a></i> <br><br>  Ich öffne ein weiteres Geheimnis: Der berechnete Fokus ist die Spitze des Eisbergs der Plenoptik.  Noch interessanter sind die 90% der Möglichkeiten, die meiner Meinung nach unter dem Interesse von Journalisten liegen.  Viele von ihnen verdienen separate Artikel, aber lassen Sie uns zumindest die Ungerechtigkeit korrigieren und sie zumindest nennen.  Lass uns gehen! <br><br><h1>  Berechnete Aperturform </h1><br>  Da es sich um Defokussierung handelt, ist der sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bokeh-Effekt</a> zu erwähnen, bei dem die Blendung nicht unbedingt rund ist <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4b6/f15/b53/4b6f15b5341de52d29714f789d5f5a52.png"><br><br>  Für Fotografen ist das sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bokeh Kit erhältlich</a> , mit dem Sie verschiedene Formen blenden können: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a8/f3d/c8c/8a8f3dc8cc02ff1ccd7e00272aec9a9f.png"><br><br>  Auf diese Weise erhalten Sie hübsche Bilder, und das Foto unten zeigt deutlich, dass die Glitzer in Form von Herzen sogar Steine ​​im Vordergrund haben, nur die Herzen sind kleiner.  Und es ist klar, dass es äußerst schwierig ist, in Photoshop auf ähnliche Weise auf natürliche Weise zu verwischen. Zumindest ist es ratsam, eine Tiefenkarte zu verwenden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c14/ca9/61f/c14ca961fa1bce2cb067c228dba7f342.png"><br><br>  Gleichzeitig ist es für die Plenoptik relativ einfach, eine andere Blende zu „berechnen“: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1df/6b2/b1c/1df6b2b1cf75edf87faaa0e1c9e689e1.png"><br><br>  Also!  Wir haben die Fokussierentfernung berechnet, die Form der Blende berechnet, sind weiter gegangen, es ist dort interessanter! <br><br><h1>  Berechnetes Stereo </h1><br>  Ein sehr nützliches Merkmal des plenoptischen Sensors, von dem fast niemand spricht, das ihm aber bereits ein zweites Leben gibt, ist die Fähigkeit, ein Bild aus verschiedenen Punkten zu berechnen.  Die Position des Punktes wird tatsächlich durch die Größe der Linse bestimmt.  Wenn das Objektiv klein ist, kann sich der Rahmen wie in diesem Beispiel leicht nach links und rechts verschieben (dieses Stereofoto ist sichtbar, wenn Sie auf die Brust des nächsten Vogels schauen): <br><br><img width="750" src="https://habrastorage.org/getpro/habr/post_images/2bc/0ef/292/2bc0ef292a64c5ba65b626d374f8f0a8.gif"><br>  <i>Quelle: Lytro</i> <br><br>  Wenn das Objektiv groß ist, wie zum Beispiel Lytro Cinema, kann der Aufnahmepunkt um 10 Zentimeter verschoben werden.  Ich möchte Sie daran erinnern, dass wir zwischen unseren Augen ungefähr 6,5 Zentimeter haben, dh mit einem maximalen Abstand von 10 cm können Sie sowohl den allgemeinen Plan (und er ist „dreidimensional“ als wenn Sie mit Ihren Augen schauen) als auch die Nahaufnahme (mit jeder Parallaxe ohne Unbehagen) aufnehmen.  Tatsächlich bedeutet dies, dass wir mit einem Objektiv vollständige Stereovideos aufnehmen können.  Darüber hinaus wird es nicht nur Stereo sein, sondern Stereo mit perfekter Qualität, und das klingt heute fantastisch - mit einem variablen Abstand zwischen den optischen Achsen virtueller Kameras NACH der Aufnahme.  Sowohl das als auch eine andere - absolut undenkbare Möglichkeiten, die eine Situation mit Stereoaufnahmen in Zukunft grundlegend verändern können. <br><br>  Da die Zukunft irgendwie im dreidimensionalen Schießen liegt, wollen wir uns näher mit diesen beiden Punkten befassen.  Es ist kein Geheimnis, dass heute die meisten 3D-Filme, insbesondere Blockbuster, nicht entfernt, sondern in 3D konvertiert werden.  Dies geschieht, weil beim Aufnehmen eine lange Liste von Problemen auftritt (unten ist ein Teil der Liste): <br><br><ul><li>  Die Frames können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">relativ zueinander gedreht, vertikal verschoben oder vergrößert werden</a> (sagen Sie nicht, dass dies leicht zu beheben ist, sehen Sie, wie oft solche Frames in Filmveröffentlichungen fallen - dies ist ein leiser Horror ... und Kopfschmerzen).  Also - die plenoptische Stereoanlage ist PERFEKT ausgerichtet, selbst wenn der Bediener beschlossen hat, den Plan zu vergrößern (aufgrund der Mechanik der beiden Kameras ist es äußerst schwierig, die Annäherung streng synchron zu machen). <br></li><li>  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Farbe der</a> Bilder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kann variieren</a> , insbesondere wenn für die Aufnahme ein geringer Abstand zwischen den optischen Achsen der Kameras erforderlich ist und Sie einen Strahlteiler verwenden müssen (es gab einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ausführlichen Beitrag</a> dazu).  Es stellt sich heraus, dass wir in einer Situation, in der wir bei der Aufnahme eines Fackelobjekts (das Auto im Hintergrund an einem sonnigen Tag) mit der überwiegenden Mehrheit der Kameras zur Unterhaltung in der Postproduktion verdammt sind und mit Plenoptik ein PERFEKTES Bild mit jeglicher Blendung erhalten.  Dies ist ein Feiertag! <br></li><li>  Die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Schärfe der</a> Rahmen kann <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">variieren</a> - wiederum ein spürbares Problem bei Strahlteilern, das bei der Plenoptik völlig fehlt.  Mit welcher Schärfe benötigt wird, berechnen wir damit absolut PERFEKT aus den Winkeln. <br></li><li>  Frames können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Zeit schweben</a> .  Kollegen, die mit dem Stalingrad-Material gearbeitet haben, sprachen über die Diskrepanz zwischen den Zeitstempeln der Tracks um 4 Frames, weshalb der Cracker immer noch relevant ist.  Wir haben in 105 Filmen mehr als 500 Szenen mit Zeitunterschieden gefunden.  Der Zeitunterschied ist leider schwer zu erkennen, insbesondere in Szenen mit Zeitlupe, und gleichzeitig das schmerzhafteste Artefakt nach unseren Messungen.  Im Falle der Plenoptik haben wir eine PERFEKTE Zeitsynchronisation. <br></li><li>  Ein weiteres Problem beim Aufnehmen von 3D sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zu große Parallaxen, die beim Betrachten</a> auf einem großen Bildschirm <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unangenehm sind</a> , wenn Ihre Augen bei Objekten „jenseits der Unendlichkeit“ zur Seite divergieren oder bei einigen Objekten im Vordergrund zu konvergieren können.  Die korrekte Parallaxenberechnung ist ein separates schwieriges Thema, über das die Bediener gut verfügen sollten, und ein Objekt, das versehentlich in den Rahmen gelangt, kann die Aufnahme ruinieren und es unangenehm machen.  Bei der Plenoptik wählen wir selbst die Parallaxe NACH der Aufnahme, sodass jede Szene mit der PERFEKTEN Parallaxe berechnet werden kann. Darüber hinaus kann dieselbe bereits aufgenommene Szene problemlos für große und kleine Bildschirme berechnet werden.  Dies ist die sogenannte Parallaxenänderung, die in der Regel äußerst schwierig und teuer ist, ohne Qualitätsverlust zu verursachen, insbesondere wenn Sie durchscheinende Objekte oder Ränder im Vordergrund hatten. <br></li></ul><br>  Im Allgemeinen ist die Fähigkeit, die perfekte Stereoanlage zu berechnen, die eigentliche Grundlage für die nächste Welle der 3D-Popularität, da in den letzten 100 Jahren 5 solcher Wellen unterschieden wurden.  Gemessen an den Fortschritten bei Laserprojektion und Plenoptik werden wir für maximal 10 Jahre (wenn die Hauptpatente von Lytro in China auslaufen oder etwas früher) in China ein neues Eisen und ein neues Qualitätsniveau für 3D-Filme erwarten. <br><br>  Also!  Sie haben die perfekt berechnete Stereoanlage von einem Objektiv (was für die meisten übrigens unmöglich ist).  Und sie haben es nicht nur verstanden, sondern auch die Parallaxe nacherzählt, selbst wenn das Objekt im Vordergrund unscharf ist.  Lass uns tiefer gehen! <br><br><h1>  Berechneter Aufnahmepunkt </h1><br>  Das Ändern des Aufnahmepunkts hat eine Anwendung in normalen 2D-Videos. <br><br>  Wenn Sie am Set waren oder zumindest Fotos gesehen haben, haben Sie wahrscheinlich auf die Schienen geachtet, auf denen die Kamera fährt.  Oft muss man sie für eine Szene in mehrere Richtungen legen, was eine spürbare Zeit in Anspruch nimmt.  Schienen sorgen für einen reibungslosen Betrieb der Kamera.  Ja, natürlich gibt es eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Steadicam</a> , aber in vielen Fällen haben die Schienen leider keine Alternative. <br><br>  Die Möglichkeit, den Aufnahmepunkt in Lytro zu ändern, wurde verwendet, um zu demonstrieren, dass genügend Räder vorhanden sind.  Wenn sich die Kamera bewegt, bilden wir eine virtuelle „Röhre“ mit einem Raumdurchmesser von 10 cm, in der wir den Aufnahmepunkt ändern können.  Mit diesem Rohr können Sie kleine Schwankungen ausgleichen.  Dies wird in diesem Video gut gezeigt: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/4qXE4sA-hLQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Darüber hinaus müssen Sie manchmal das umgekehrte Problem lösen, dh während der Bewegung Schwingungen hinzufügen.  Zum Beispiel gab es nicht genug Dynamik und der Regisseur entschied sich für eine Explosion.  Es dauert nicht lange, einen Ton anzulegen, aber es ist ratsam, die Kamera synchron zu schütteln, und es ist besser, das Bild nicht nur flach zu bewegen, es wird sichtbar, sondern es „ehrlich“ zu schütteln.  Mit der Plenoptik kann der Aufnahmepunkt vollständig „ehrlich“ geschüttelt werden - mit einer Änderung des Aufnahmepunkts, des Winkels, der Bewegungsunschärfe usw.  Es wird eine völlige Illusion geben, dass die Kamera heftig zitterte, die sanft und sicher auf Rädern fuhr.  Stimmen Sie zu - eine erstaunliche Gelegenheit!  Die Steadicam der nächsten Generation wird Wunder zeigen. <br><br>  Damit dies Realität wird, müssen Sie natürlich warten, bis die Größe der plenoptischen Kamera auf einen vernünftigen Wert reduziert ist.  Angesichts der Tatsache, wie viel Geld in die Miniaturisierung und die Erhöhung der Auflösung von Sensoren für Smartphones und Tablets investiert wird, scheint das Warten nicht so lange zu dauern.  Und es wird möglich sein, eine neue plenoptische Kamera zu erstellen!  Jedes Jahr wird es einfacher und einfacher. <br><br>  Also!  Der Aufnahmepunkt wurde berechnet und gegebenenfalls bewegt, wodurch die Kamera stabilisiert oder destabilisiert wurde.  Lass uns weiter gehen! <br><br><h1>  Berechnete Beleuchtung </h1><br>  Da wir über eine Explosion in der Nähe sprechen ... <br><br>  Manchmal ist es notwendig, die Beleuchtung des Motivs zu ändern, z. B. einen Blitz realistisch hinzuzufügen.  Postproduktionsstudios wissen genau, was für ein schwieriges Problem dies ist, auch wenn die Kleidung unseres Charakters relativ einfach ist. <br><br>  Hier ist ein Beispiel für ein Video mit einer Blitzüberlagerung unter Berücksichtigung der Kleidung, ihrer Schatten usw. (wird durch Klicken geöffnet): <br><br> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><img src="https://habrastorage.org/getpro/habr/post_images/9f9/5d9/71b/9f95d971be215c71291ab04052bf4c24.png"></a> <br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Interview mit Yon Karafin</a></i> <br><br>  Unter der Haube sieht es aus wie ein Plug-In für Nuke, das mit einem vollständigen dreidimensionalen Modell des Schauspielers arbeitet und es unter Berücksichtigung der normalen Karte, Materialien, Bewegungsunschärfe usw. neu beleuchtet: <br><br><img width="650" src="https://habrastorage.org/getpro/habr/post_images/657/214/4b7/6572144b7afc8bc6638984b17ce45d88.png"><br><br><img width="650" src="https://habrastorage.org/getpro/habr/post_images/631/766/a83/631766a838c01a2759931f89b44dd2c7.png"><br>  <i>Quelle: Lytro Materials</i> <br><br>  Also!  Die neue Beleuchtung des bereits erfassten Objekts wurde berechnet.  Lass uns weiter gehen! <br><br><h1>  Berechnete Auflösung </h1><br>  Diejenigen, die mit dem ersten Lytro experimentierten, bemerkten oft - ja, das Spielzeug ist cool, aber die Auflösung ist für normale Fotografie völlig unzureichend. <br><br>  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserem letzten Artikel</a> über Habr wurde beschrieben, wie kleine Verschiebungen des Aufnahmepunkts dazu beitragen können, die Auflösung signifikant zu erhöhen.  Frage: Sind diese Algorithmen auf die Plenoptik anwendbar?  Antwort: Ja!  Darüber hinaus lassen sich Super-Resolution-Algorithmen leichter mit plenoptischen Bildern bearbeiten, da eine Tiefenkarte vorhanden ist, alle Pixel gleichzeitig aufgenommen werden und die Verschiebungen recht genau bekannt sind.  Das heißt, die Situation der Plenoptik ist einfach magisch im Vergleich zu den Bedingungen, unter denen Super Resolution-Algorithmen in gewöhnlichem 2D arbeiten.  Das Ergebnis ist entsprechend: <br><br><img width="650" src="https://habrastorage.org/getpro/habr/post_images/bc9/6cd/e55/bc96cde55e276f4420757de5359467b1.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Naive, intelligente und hochauflösende plenoptische Rahmenwiederherstellung</a> aus dem technischen Bericht von Adobe „Superauflösung mit plenoptischer Kamera 2.0“</i> <br><br>  Das einzige große Problem (wirklich groß!) Ist die Datenmenge und der Rechenaufwand, der erforderlich ist.  Trotzdem ist es heute völlig realistisch, eine Computerfarm nachts zu laden und morgen dort, wo sie benötigt wird, doppelt so viel Auflösung zu erhalten. <br><br>  Also - die berechnete Auflösung herausgefunden!  Lass uns weiter gehen! <br><br><h1>  Berechnete Umgebung </h1><br>  Eine separate Aufgabe, die sich in der Postproduktion ergibt, insbesondere wenn es sich um ein fantastisches oder Fantasy-Projekt handelt, besteht darin, die aufgenommenen Objekte sorgfältig in eine dreidimensionale Szene zu kleben.  Weltweit gibt es kein großes Problem.  Heutzutage ist das dreidimensionale Scannen des Aufnahmebereichs die Norm. Alles, was übrig bleibt, ist, das Aufnahmemodell und die dreidimensionale Szene sorgfältig zu kombinieren, damit es keine Klassiker des Genres gibt, wenn die Beine der Schauspieler manchmal leicht in den bemalten unebenen Boden fallen (dieser Pfosten ist auf dem kleinen Bildschirm nicht sehr sichtbar, aber deutlich zu sehen im Kino, besonders irgendwo in IMAX). <br><br>  Nicht ohne Humor wurde als Beispiel für den Betrieb der Kamera die Szene verwendet, in der die Mondlandung aufgenommen wurde („dreidimensionale Schatten“ sind an Stellen deutlich sichtbar, die die Kamera nicht sieht): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/094/809/2b9/0948092b96926ae88866cd222e67f0fe.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mond |</a></i>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lytro |</a></i>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">VR Playhouse |</a></i>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lightfield Postproduktion</a></i> <br><br>  Da ein Mehrwinkelvideo per Definition in einer plenoptischen Kamera aufgenommen wird, ist das Erstellen einer Tiefenkarte kein großes Problem.  Und wenn die Kamera mit dem Lidar kombiniert wird, nehmen wir sofort eine dreidimensionale Szene auf, was die Kombination von echten Aufnahmen und Grafiken in der nächsten Phase kategorisch vereinfacht. <br><br>  Also haben wir die dreidimensionale Szene gezählt (Lidar-Daten und Plenoptik-Daten kombiniert).  Nun, Sie haben es erraten, wird die Darstellung der Mondvermessung noch besser sein!  Mach weiter. <br><br><h1>  Berechneter Chroma Key </h1><br>  Das ist aber noch nicht alles! <br><br>  Die übliche Technologie eines modernen Filmsets sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">grüne Bildschirme</a> , wenn an Orten, an denen ein Computerhintergrund auf grünen Sperrholzschildern angebracht werden soll, oder grüne Banner gezogen werden.  Dies geschieht so, dass es keine Probleme an den durchscheinenden Rändern gibt, wenn sich der Hintergrund ändert.    4           —   (       )   . <br><br>     ,        ,           ,  ,         .       ,      ,     : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/719/321/028/719321028fe5117bbd95f2a5eb53eef4.gif"><br> <i>: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">   </a></i> <br><br>  Lytro   ,        (    <b>    </b> -      —    !),        (         challenge),                ,        ( ,       ,    ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e3c/ca5/ecd/e3cca5ecdfa044f5a126210adee71c5f.jpg"><br><br>   ,        ?    .  , ,   (  ).  ,                         <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="> </a> . <br><br>                 « »,         .      .     ,                      2D .   . <br><br> ,       ,  ! <br><br><h1>   </h1><br>    —     .        ,    ,  ,        ,       .    ,          «» . <br><br>    ,   «»   «» , ,      ,       ,    ,       ,    ( Lytro Cinema  300 fps,   ,    ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/77a/c8f/887/77ac8f887c64f3d1032dff17ed4a8ade.png"><br><br>    «»   ,     ,     ,     «»  «»         ,       (     ,   ,       ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb7/cd2/51a/eb7cd251a31f2c79d3f8184af7126321.gif"><br> <i>: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://vimeo.com/114743605</a></i> <br><br>     ,           ,    : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/e3a/ebc/52ce3aebcee45ffc1d48baca9ea3033b.gif"><br> <i>: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">https://vimeo.com/114743605</a></i> <br><br> ,  .  ! <br><br><h1>      </h1><br>  Wenn es heute um Plenoptik geht, gibt es normalerweise typische Gespräche im Stil von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„Der Chef ist weg: Der Gips wird entfernt, der Kunde geht!“.</a>  Lytro hat Google letztes Jahr günstig gekauft, die Pelican Imaging-Experimente gingen nicht in die Massenproduktion und im Allgemeinen blieb alles nur eine schöne Theorie ... <br><br><img width="300" src="https://habrastorage.org/getpro/habr/post_images/db1/c6c/d34/db1c6cd34bdfed6d5b6775a934a20e57.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pelican Imaging: Smartphone Plenoptic Camera Module für 20 Dollar</a></i> <br><br>  Also!  Gerüchte über den Tod von Plenoptikern sind stark übertrieben.  Sehr stark! <br><br>  Im Gegenteil, derzeit werden plenoptische Sensoren in einem noch <b>nie</b> dagewesenen Maßstab hergestellt und eingesetzt. <br><br>  Das berüchtigte Google-Unternehmen ohne große Fanfare veröffentlichte Google Pixel 2 und Google Pixel 3 mit plenoptischen Sensoren.  Wenn Sie auf das Telefon schauen, können Sie deutlich sehen, dass die Kamera eine Kamera hat: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/683/8ba/197/6838ba19764118b6b5a264770ce5f8dc.png"><br><br>  Gleichzeitig verwischt das Telefon den Hintergrund jedoch sehr gut, im Allgemeinen nicht schlechter als seine Kollegen mit zwei, drei, drei und vier Augen (stimmen Sie zu, dass das folgende Foto besonders von diesem Effekt profitiert): <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/78d/e6e/e8a/78de6ee8a7b0a719ae7c3cc108ac305f.png"></a> <br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI Google Blog (Sie können klicken und eine größere Größe sehen, hauptsächlich an den Rändern des Vordergrundobjekts)</a></i> <br><br>  Wie machen sie das ?! <br><br>  Eine Quelle magischer Wunder, die in letzter Zeit unvermeidlich war, wird genutzt - nicht-kindliche Magie neuronaler Netze? <br><br>  Neuronale Netze, insbesondere in Pixel 3, werden bei dieser Aufgabe ebenfalls aktiv verwendet, dazu später mehr.  Das Geheimnis der Effektqualität besteht darin, dass der Sensor des Smartphones plenoptisch ist, obwohl das Objektiv nur zwei Pixel abdeckt, wie im Bild gezeigt: <br><br><img width="400" src="https://habrastorage.org/getpro/habr/post_images/852/c36/209/852c36209a7528f576e20dc37e0ef1f3.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI Google Blog</a></i> <br><br>  Als Ergebnis erhalten wir ein Mikrostereobild, hier ist eine sehr leichte Verschiebung in der Bewegung sichtbar (das rechte Bild ist animiert und bewegt sich auf und ab, da das Telefon beim Aufnehmen so gehalten wurde): <br><br><img src="https://habrastorage.org/webt/pq/bw/ip/pqbwipy4ne_96amqgpvbinmnehy.gif"><br><br>  Aber selbst diese extrem kleine Verschiebung reicht aus, um eine Tiefenkarte für das aufgenommene Foto zu erstellen (über die Verwendung von Subpixel-Verschiebungen wurde <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in einem früheren Artikel berichtet</a> ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/066/2be/93e/0662be93e0e02b6f87f7c3fa8b1f8e7c.png"><br><br>  Gleichzeitig können durch den Einsatz von maschinellem Lernen die Ergebnisse erheblich verbessert werden, und dies ist bereits in Pixel 3 implementiert. Im folgenden Beispiel bedeutet "Gelernt" Stereo + Gelernt. Mehr dazu gibt es hoffentlich einen separaten Beitrag: <br><br><img width="230" src="https://habrastorage.org/getpro/habr/post_images/566/8e0/05b/5668e005b8efd21ce9f216ed9d16edf1.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/988/e31/9d2/988e319d2619f39a50d19cde8904d891.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/638/a17/966/638a1796697b9f2a675c509ff870465f.png"><br><br><img width="230" src="https://habrastorage.org/getpro/habr/post_images/110/4df/a5e/1104dfa5e50f1d476bc1ee7024520834.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/1ed/beb/daa/1edbebdaa7934c6525005fd8118fbd83.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/77e/ef2/942/77eef2942f1e6b6af44ccc720ae2c3a5.png"><br><br><img width="230" src="https://habrastorage.org/getpro/habr/post_images/429/442/99e/42944299e72b7eb720f3babc5a757b79.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/da3/fe1/3ea/da3fe13eacb3cd5b0a558db1d843b686.png"><img width="230" src="https://habrastorage.org/getpro/habr/post_images/c6f/a27/c4b/c6fa27c4bac8b9c5cb122e0c69ab814d.png"><br><br>  Für diejenigen, die die Tiefe in voller Auflösung betrachten möchten, gibt es weitere Beispiele <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in einer speziellen Galerie</a> . <br><br>  Es ist deutlich zu sehen, dass nicht alles perfekt ist, und wir haben typische Artefakte, die typisch für die Erstellung einer Tiefenkarte aus Stereo sind (die übrigens auch bei Pixels zweiäugigen Kollegen verfügbar sind), und ein wenig Parallaxeneffekte.  Es ist jedoch bereits klar, dass die Qualität der Tiefe völlig ausreicht, um das Bild sicher nach Tiefe zu segmentieren und weitere Effekte zu erzielen, Objekte sicherer zur Szene hinzuzufügen und so weiter.  Die Ergebnisse der gemessenen Tiefe sind um eine Größenordnung besser als die Ergebnisse, die auf verschiedenen Annahmen basieren (willkürlich neuronale Netze). <br><br>  Herzlichen Glückwunsch an alle, die bis zu diesem Punkt gelesen haben!  Sie leben während der Veröffentlichung von plenoptischen Kameras in einem erfolgreichen Massenmarktprodukt, auch wenn Sie nicht einmal davon wissen! <br><br>  Die Geschichte endet natürlich nicht dort: <br><br><ul><li>  Erstens ist es interessant, dass die Plenoptik „fast kostenlos“ war, da bei modernen Smartphone-Kameras mit Miniaturisierung des Sensors und einer Erhöhung der Auflösung ein katastrophaler Mangel an Lichtfluss auftritt, sodass jedes Pixel mit einer Mikrolinse bedeckt ist.  Das heißt, ein solcher Sensor kostet nicht mehr (das ist sehr wichtig!), Obwohl wir eine Auflösung, die wir gerade <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aufgrund einer anderen Technologie</a> erhöht haben, etwas opfern.  Durch die Anwendung von zwei Technologien wird das Ergebnis in 2D besser (weniger Rauschen, höhere Auflösung, HDR) und gleichzeitig durch die gemessene Tiefe ergänzt, dh es wird 3D.  Der Preis der Ausgabe ist ein dramatischer Anstieg der Anzahl der Berechnungen pro Frame.  Aber für Fotos ist dies bereits heute möglich und funktioniert bereits in echten Smartphones. <br></li><li>  Zweitens sagte ein Google-Mitarbeiter auf einer Konferenz, dass er daran denke, 4 Pixel mit einem Objektiv abzudecken. Danach wird die Qualität der Tiefenkarte dramatisch höher sein, da es 2 Stereopaare mit einer 1,4-mal größeren Stereobasis (zwei Diagonalen) geben wird. Dies wird die Qualität der Tiefenkarte dramatisch verbessern. darunter viele Stereo-Artefakte an den Grenzen.  Mitbewerber können diese Qualität nur erreichen, wenn sie mindestens 3 Kameras hintereinander platzieren.  Eine solche Qualitätssteigerung ist für AR wichtig. <br></li><li>  Drittens ist Google nicht mehr allein. Hier ist ein Beispiel für eine Beschreibung einer ähnlichen Technologie im Vivo V11 Pro. Sie haben gerade ein ähnliches Bild gesehen: <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/096/4b1/ff8/0964b1ff86f0970e6e7c1ef3611ec11d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was ist Dual Pixel-Technologie?</a></i> <br><br><ul><li>  Schließlich ist in den letzten Jahren die Zahl der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Veröffentlichungen zur Interpolation von Lichtfelddaten um</a> Lawinen gestiegen.  Und das ist wunderbar, denn um die Rechenkomplexität drastisch zu reduzieren, wird nicht-kindliche Mathematik benötigt. </li></ul><br>  Plenoptics wird auch im Autofokus von professionellen Kameras verwendet, beispielsweise bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Canon</a> (Google <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DPAF</a> - Dual Pixel Auto Focus).  Wer hätte gedacht, dass ein theoretischer Witz von vor 30 Jahren - die Fähigkeit, Stereo mit einem Objektiv aufzunehmen - die erste Massenanwendung von Plenoptik wäre ... <br><br>  Generell - das Thema ging an die Produkte! <br><br>  Sie fliegt schon!  Sie sehen, <b>LE-TA-ET!</b> <br><br><hr><br><h1>  Zusammenfassend </h1><br><h2>  Filmoptik im Kino </h2><br>  Oben haben wir zwei Fälle des Einsatzes von Plenoptik untersucht - in der Filmproduktion und in Smartphones.  Dies ist keine vollständige Liste. Beispielsweise ist die Plenoptik in der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Mikroskopie</a> sehr relevant. Sie können <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">berechnete Stereomikroskopaufnahmen</a> mit einer großen "ehrlichen" Schärfentiefe erstellen.  Die Plenoptik ist für Industriekameras relevant, insbesondere wenn Sie Fotos von durchscheinenden mehrstufigen Objekten usw. aufnehmen müssen.  Aber irgendwie ein anderes Mal. <br><br><hr><br>  Denken Sie daran, dass für die Verwendung in der Filmproduktion relevant sind: <br><br><ol><li>  Berechnete Fokusentfernung <br></li><li>  Berechnetes Bokeh <br></li><li>  Berechnete Auflösung <br></li><li>  Computed Perfect Stereo <br></li><li>  Berechneter Aufnahmepunkt <br></li><li>  Berechnete Beleuchtung <br></li><li>  Berechnete Umgebung <br></li><li>  Berechneter Greenscreen <br></li><li>  Berechneter Verschluss <br></li></ol><hr><br>  In den kommenden Jahren kann durch Miniaturisierung und erhöhte Auflösung von Sensoren eine Technologie entwickelt werden, mit der eine praktische, grundlegend neue Filmkamera erstellt werden kann, mit der Sie Material zum Anwenden von Spezialeffekten schneller (ohne Greenscreen) und mit weniger Einstellungen entfernen können (mehr Punkte sind einfacher zu reparieren).  Die neuen Funktionen sind so interessant, dass solche Kameras zum Erfolg verurteilt sind, wenn der plenoptische Sensor mit einer geeigneten Filmauflösung kompakt gemacht werden kann. <br><br>  <b>Wann kann eine solche Kamera erscheinen?</b> <br><br>  Eine vorsichtige Antwort ist in den nächsten 10 Jahren. <br><br>  <b>Wovon hängt der Begriff ab?</b> <br><br>  Aus vielen Faktoren.  Gute Frage: Wie wird die Situation mit der Möglichkeit sein, Lytro-Patente zu lizenzieren, die Google jetzt besitzt?  Dies kann kritisch sein.  Glücklicherweise laufen die wichtigsten in 10 Jahren ab (hier erinnern wir uns nicht höflich an die chinesischen Lytro-Kollegen, die den Prozess beschleunigen können).  Auch die Arbeit mit riesigen Datenmengen, die von der plenoptischen Kamera erzeugt werden, sollte vereinfacht werden: Mit modernen Wolken wird dies immer einfacher.  Von den guten Nachrichten - einmal dank Lytro, in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">sehr beliebten Kompositionsprogramm</a> , das in einer Vielzahl von Studios für die Stereoverarbeitung verwendet wird, wurde das plenoptische Datenformat unterstützt.  Wie sie sagen, ist Lytro gestorben, aber die Möglichkeit, Plug-Ins für Nuke mit Unterstützung für plenoptische Videos zu schreiben, blieb uns erhalten.  Dies vereinfacht den „Eintritt“ in diesen Markt mit einem professionellen Produkt, da es wichtig ist, dass die Studios sofort mit dem Format neuer Kameras arbeiten können, ohne Personal und in denselben Programmen zu schulen. <br><br><h2>  Plenoptika in Smartphones </h2><br>  Wenn wir über Smartphones sprechen, sieht alles noch besser aus.  Am relevantesten für diese Branche ist die Fähigkeit der plenoptischen Kamera, die Tiefe mit einem einzigen Sensor (möglicherweise schnell) zu messen, und diese Funktion wird bald eine große Nachfrage sein. <br><br>  <b>Wann kann eine solche Kamera erscheinen?</b> <br><br>  Existiert bereits.  Und morgen wird die Technologie von anderen Herstellern wiederholt.  Darüber hinaus wird <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Augmented Reality</a> auf Smartphones und Tablets der Haupttreiber der nächsten Stufe sein, der es heute an Genauigkeit und der Fähigkeit mangelt, eine dreidimensionale Szene zu "sehen". <br><br>  <b>Wovon hängt der Begriff ab?</b> <br><br>  Die Möglichkeit, die Entfernung mit dem Hauptsensor in Echtzeit zu messen, wird mit Google Pixel wahrscheinlich bald verfügbar sein, da sich Google seit langem in diese Richtung entwickelt (siehe <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Projekt Tango</a> , das geschlossen ist, dessen Geschäft jedoch weiterlebt).  Und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ARCore</a> sieht ebenso vielversprechend aus wie das konkurrierende <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ARKit</a> .  Und der Durchbruch muss, wie Sie jetzt erraten haben, nicht lange warten, da der Wert des Sensorpixels exponentiell abfällt, die Durchschnittsgeschwindigkeit in 10 Jahren 10-mal genannt wird und wir einen 2-mal-Abfall benötigen.  Dann zählen Sie selbst.  Nicht morgen, aber nicht lange. <br><br><h1>  Anstelle einer Schlussfolgerung </h1><br>  Denken Sie daran, dass wir ganz am Anfang über einen Vortrag bei VGIK gesprochen haben?  Ich muss sagen, dass ich nicht mit der Reaktion darauf gerechnet habe, die am Ende war.  Um es in einem Wort auszudrücken, es war Trauer.  Wenn in zwei Worten, dann universelle Trauer.  Und zuerst verstand ich nicht, was los war.  Der Operator, der nach der Vorlesung auftauchte, erklärte mir die Situation sehr gut.  Es war nicht einmal so, dass die Kamerakunst nachließ.  Obwohl es ein großartiges Beispiel gab: Ein Filmfragment für ungefähr 6 Sekunden, wenn sich eine Person der Tür der Wohnung nähert, klopft, eine andere Person die Tür öffnet, grüßt und sich ein wenig vorwärts bewegt, während die Kamera auf den Korridor, dann auf den Türrahmen fokussiert und dann sofort fokussiert auf die Person, die es entdeckt hat, und dann in den Raum.  Und der Bediener muss die Kamera perfekt beherrschen, damit er bei einer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kinoähnlichen geringen Schärfentiefe</a> den Zoom meisterhaft bedienen und dabei die Hintergrundbeleuchtung, die Zusammensetzung des Rahmens, die Reisekrankheit beim Fotografieren mit den Händen und weitere 1000 wichtige Kleinigkeiten berücksichtigen kann.  Also.  Es ist nicht einmal so, dass dies einfacher wird.  Das ist sogar gut.  Weniger Aufnahmen werden durch die Tatsache verdorben, dass der Bediener irgendwo keine Zeit hatte oder versäumt hat.  Er erzählte, wie kürzlich er eine Serie in 4K für den Sender gedreht hat.  Und der Bestand an Auflösungen erwies sich als groß.  Infolgedessen schnitt der Regisseur die Rahmen in der Postproduktion, und an einigen Stellen wurden nur Fragmente des Rahmens für Unterbrechungen verwendet.  Infolgedessen war die Komposition einfach schrecklich und dieser Betreiber wollte seinen Namen aus dem Abspann entfernen. <br><br>  Die oben beschriebenen Merkmale von Kameras für Filmemacher bedeuten die Übertragung vieler Effekte von der Drehphase auf die Postproduktion.  Und es wird sehr traurig sein, wenn diejenigen, die die aufgenommenen Szenen verarbeiten, Analphabeten in Sachen Komposition, Fokusentfernungskontrolle usw. sind.  Wenn sie lesen und schreiben können, sind dies neue fantastische Möglichkeiten. <br><br>  <b>Wir wünschen uns allen mehr Kompetenz, was in dieser sich schnell verändernden Welt nicht immer einfach ist!</b> <br><br>  <b>Und <s>Karthago wird ...</s> bis zum Ende des Jahrhunderts <s>wird das</s> gesamte Video dreidimensional sein!</b> <br><br><h1>  Danksagung </h1><br>  Ich möchte mich herzlich bedanken bei: <br><br><ul><li>  Labor für Computergrafik VMK Moscow State University  MV Lomonosov für seinen Beitrag zur Entwicklung der Computergrafik in Russland und nicht nur <br></li><li>  Unsere Kollegen aus der Videogruppe, dank derer Sie diesen Artikel gesehen haben, <br></li><li>  persönlich Konstantin Kozhemyakov, der viel getan hat, um diesen Artikel besser und visueller zu machen, <br></li><li>  Jon Karafin, als er Leiter des Lichtfeldvideos in Lytro war, dank dessen wir fast angefangen haben, an der Verbesserung ihres Produkts zu arbeiten (und nicht aus Gründen gestartet sind, die außerhalb unserer Kontrolle oder von uns liegen), <br></li><li>  Lytro für ihren Beitrag zur Popularisierung von Lichtfeldern und ihre Fähigkeiten, Google, das die fallende Flagge eroberte, und andere Unternehmen, die Produkte auf der Basis dieser interessanten Technologie herstellen, <br></li><li>  und schließlich vielen Dank an Sergey Lavrushkin, Roman Kazantsev, Ivan Molodetsky, Evgeny Kuptsov, Jegor Sklyarov, Evgeny Lyapustin und Denis Kondranin für eine große Anzahl vernünftiger Kommentare und Korrekturen, die diesen Text viel besser gemacht haben! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de440652/">https://habr.com/ru/post/de440652/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de440642/index.html">JetBrains Nacht in Moskau, 13. April</a></li>
<li><a href="../de440644/index.html">Die Verdauung von frischen Materialien aus der Welt des Frontends für die letzte Woche Nr. 352 (11. - 17. Februar 2019)</a></li>
<li><a href="../de440646/index.html">Frontend Weekly Digest (11. - 17. Februar 2019)</a></li>
<li><a href="../de440648/index.html">Überblick über die russische Gesetzgebung im Bereich der Barrierefreiheit im Internet</a></li>
<li><a href="../de440650/index.html">Wie Bewusstsein funktioniert: Schlussfolgerungen aus dem Buch von Alexander Nevzorov</a></li>
<li><a href="../de440654/index.html">Python lernen: Argparse-Modul</a></li>
<li><a href="../de440656/index.html">Professionelle Containerisierung von Node.js-Anwendungen mit Docker</a></li>
<li><a href="../de440658/index.html">Exploring Docker, Teil 4: Reduzieren der Größe von Bildern und Beschleunigen ihrer Montage</a></li>
<li><a href="../de440660/index.html">Docker lernen, Teil 5: Befehle</a></li>
<li><a href="../de440662/index.html">React Tutorial Teil 18: Die sechste Phase der Arbeit an einer TODO-Anwendung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>