<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ùî ‚Ü™Ô∏è üë®üèæ‚Äçüé® Migration d'une application r√©elle de MySQL autonome vers le cluster Percona XtraDB üö£üèæ ‚ôæ ‚ú®</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Malheureusement, sur Internet, il n'y a pas suffisamment d'informations sur la migration des applications r√©elles et le fonctionnement de production d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Migration d'une application r√©elle de MySQL autonome vers le cluster Percona XtraDB</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422347/"><img src="https://habrastorage.org/getpro/habr/post_images/2b3/23e/2d3/2b323e2d35539763c7a0d17a4694d5af.png" alt="image"><br><br>  Malheureusement, sur Internet, il n'y a pas suffisamment d'informations sur la migration des applications r√©elles et le fonctionnement de production du Percona XtraDB Cluster (ci-apr√®s PXC).  Je vais essayer de corriger cette situation et raconter notre exp√©rience avec mon histoire.  Il n'y aura pas d'instructions d'installation √©tape par √©tape et l'article ne doit pas √™tre consid√©r√© comme un remplacement de la documentation hors site, mais comme un ensemble de recommandations. <br><a name="habracut"></a><br><h3>  Le probl√®me </h3><br>  Je travaille en tant qu'administrateur syst√®me sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ultimate-guitar.com</a> .  Puisque nous fournissons un service Web, nous avons naturellement des backends et une base de donn√©es, qui est le c≈ìur du service.  La disponibilit√© du service d√©pend directement des performances de la base de donn√©es. <br><br>  Percona MySQL 5.7 a √©t√© utilis√© comme base de donn√©es.  La r√©servation a √©t√© impl√©ment√©e √† l'aide du ma√Ætre du sch√©ma de r√©plication ma√Ætre.  Les esclaves ont √©t√© utilis√©s pour lire certaines donn√©es. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/05a/ca3/bc6/05aca3bc6ba1c357cd98c280e8a07772.png" alt="image"><br><br>  Mais ce sch√©ma ne nous convenait pas avec les inconv√©nients suivants: <br><br><ul><li>  En raison du fait que dans la r√©plication MySQL, les esclaves asynchrones peuvent √™tre ind√©finiment en retard.  Toutes les donn√©es critiques devaient √™tre lues par le ma√Ætre. </li><li>  Du paragraphe pr√©c√©dent suit la complexit√© du d√©veloppement.  Le d√©veloppeur ne pouvait pas simplement faire une demande √† la base de donn√©es, mais √©tait oblig√© de se demander s'il √©tait pr√™t dans chaque cas particulier √† l'arri√©r√© de l'esclave et sinon, lire les donn√©es de l'assistant. </li><li>  Commutation manuelle en cas d'accident.  L'impl√©mentation de la commutation automatique √©tait probl√©matique en raison du fait que l'architecture MySQL n'a pas de protection int√©gr√©e contre le split brain.  Il faudrait nous √©crire un arbitre avec une logique complexe de choix d'un ma√Ætre.  Lorsque vous √©crivez aux deux ma√Ætres, des conflits peuvent survenir en m√™me temps, brisant la r√©plication du ma√Ætre et conduisant au split brain classique. </li></ul><br>  Quelques chiffres secs, pour que vous compreniez avec quoi nous avons travaill√©: <br><br>  Taille de la base de donn√©es: 300 Go <br>  QPS: ~ 10k <br>  Rapport RW: 96/4% <br>  Configuration du serveur ma√Ætre: <br>  CPU: 2x E5-2620 v3 <br>  RAM: 128 Go <br>  SSD: Intel Optane 905p 960 Go <br>  R√©seau: 1 Gbit / s <br><br>  Nous avons une charge OLTP classique avec beaucoup de lecture, ce qui doit √™tre fait tr√®s rapidement et avec une petite quantit√© d'√©criture.  La charge sur la base de donn√©es est assez faible en raison du fait que la mise en cache est activement utilis√©e dans Redis et Memcached. <br><br><h3>  S√©lection de d√©cision </h3><br>  Comme vous l'avez peut-√™tre devin√© √† partir du titre, nous avons choisi PXC, mais ici je vais expliquer pourquoi nous l'avons choisi. <br><br>  Nous avions 4 options: <br><br><ol><li>  Changer de SGBD </li><li>  R√©plication de groupe MySQL </li><li>  Vissez nous-m√™mes les fonctionnalit√©s n√©cessaires √† l'aide de scripts sur le ma√Ætre de r√©plication ma√Ætre. </li><li>  Cluster MySQL Galera (ou ses fourches, par exemple PXC) </li></ol><br>  L'option de modification de la base de donn√©es n'a pratiquement pas √©t√© envisag√©e, car  l'application est grande, dans de nombreux endroits, elle est li√©e √† la fonctionnalit√© ou √† la syntaxe mysql, et la migration vers PostgreSQL, par exemple, prendra beaucoup de temps et de ressources. <br><br>  La deuxi√®me option √©tait la r√©plication de groupe MySQL.  Un avantage incontestable est qu'il se d√©veloppe dans la branche vanille de MySQL, ce qui signifie qu'√† l'avenir il se g√©n√©ralisera et aura un large pool d'utilisateurs actifs. <br><br>  Mais il a quelques inconv√©nients.  Premi√®rement, il impose plus de restrictions sur le sch√©ma d'application et de base de donn√©es, ce qui signifie qu'il sera plus difficile de migrer.  Deuxi√®mement, la r√©plication de groupe r√©sout le probl√®me de la tol√©rance aux pannes et du fractionnement du cerveau, mais la r√©plication dans le cluster est toujours asynchrone. <br><br>  Nous n'avons pas non plus aim√© la troisi√®me option pour trop de v√©los, que nous devons in√©vitablement mettre en ≈ìuvre pour r√©soudre le probl√®me de cette mani√®re. <br><br>  Galera a permis de r√©soudre compl√®tement le probl√®me de basculement MySQL et de r√©soudre partiellement le probl√®me avec la pertinence des donn√©es sur les esclaves.  En partie parce que l'asynchronie de r√©plication est maintenue.  Une fois qu'une transaction est valid√©e sur un n≈ìud local, les modifications sont transmises aux n≈ìuds restants de mani√®re asynchrone, mais le cluster s'assure que les n≈ìuds ne tra√Ænent pas trop et s'ils commencent √† ralentir, cela ralentit artificiellement le travail.  Le cluster garantit qu'apr√®s avoir valid√© la transaction, personne ne peut valider les modifications conflictuelles, m√™me sur le n≈ìud qui n'a pas encore r√©pliqu√© les modifications. <br><br>  Apr√®s la migration, le sch√©ma de fonctionnement de la base de donn√©es devrait ressembler √† ceci: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d8/435/190/2d8435190207a5bb711d131d852a572f.png" alt="image"><br><br><h3>  La migration </h3><br>  Pourquoi la migration est-elle le deuxi√®me √©l√©ment apr√®s avoir choisi une solution?  C'est simple - le cluster contient un certain nombre d'exigences que l'application et la base de donn√©es doivent suivre, et nous devons les remplir avant la migration. <br><br><ul><li>  <b>Moteur InnoDB pour toutes les tables.</b>  MyISAM, Memory et autres backends ne sont pas pris en charge.  Il est corrig√© assez simplement - nous convertissons toutes les tables en InnoDB. </li><li>  <b>Binlog au format ROW.</b>  Le cluster n'a pas besoin d'un binlog pour fonctionner, et si vous n'avez pas besoin d'esclaves classiques, vous pouvez le d√©sactiver, mais le format binlog doit √™tre ROW. </li><li>  <b>Toutes les tables doivent avoir une CL√â PRIMAIRE / √âTRANG√àRE.</b>  Cela est n√©cessaire pour une √©criture simultan√©e correcte dans la m√™me table √† partir de diff√©rents n≈ìuds.  Pour les tables qui ne contiennent pas de cl√© unique, vous pouvez utiliser la cl√© primaire composite ou l'incr√©mentation automatique. </li><li>  <b>N'utilisez pas 'LOCK TABLES', 'GET_LOCK () / RELEASE_LOCK ()', 'FLUSH TABLES {{table}} WITH READ LOCK' ou le niveau d'isolement 'SERIALIZABLE' pour les transactions.</b> </li><li>  <b>N'utilisez pas les requ√™tes 'CREATE TABLE ... AS SELECT'</b> , car  ils combinent le sch√©ma et le changement de donn√©es.  Il est facilement divis√© en 2 requ√™tes, la premi√®re cr√©e une table et la seconde remplit de donn√©es. </li><li>  <b>N'utilisez pas 'DISCARD TABLESPACE' et 'IMPORT TABLESPACE'</b> , car  ils ne sont pas r√©pliqu√©s </li><li>  <b>D√©finissez les options ¬´innodb_autoinc_lock_mode¬ª sur ¬´2¬ª.</b>  Cette option peut corrompre les donn√©es lorsque vous travaillez avec la r√©plication STATEMENT, mais comme seule la r√©plication ROW est autoris√©e dans le cluster, il n'y aura aucun probl√®me. </li><li>  <b>Comme ¬´log_output¬ª, seul ¬´FILE¬ª est pris en charge.</b>  Si vous avez une entr√©e de journal dans le tableau, vous devrez la supprimer. </li><li>  <b>Les transactions XA ne sont pas prises en charge.</b>  S'ils ont √©t√© utilis√©s, vous devrez r√©√©crire le code sans eux. </li></ul><br>  Je dois noter que presque toutes ces restrictions peuvent √™tre supprim√©es si vous d√©finissez la variable 'pxc_strict_mode = PERMISSIVE', mais si vos donn√©es sont importantes pour vous, il est pr√©f√©rable de ne pas le faire.  Si vous avez d√©fini ¬´pxc_strict_mode = ENFORCING¬ª, alors MySQL ne vous permettra pas d'effectuer les op√©rations ci-dessus ni d'emp√™cher le n≈ìud de d√©marrer. <br><br>  Apr√®s avoir rempli toutes les exigences de la base de donn√©es et test√© minutieusement le fonctionnement de notre application dans l'environnement de d√©veloppement, nous pouvons passer √† l'√©tape suivante. <br><br><h3>  D√©ploiement et configuration de cluster </h3><br>  Nous avons plusieurs bases de donn√©es en cours d'ex√©cution sur nos serveurs de bases de donn√©es et d'autres bases de donn√©es n'ont pas besoin de migrer vers le cluster.  Mais un package avec un cluster MySQL remplace le mysql classique.  Nous avions plusieurs solutions √† ce probl√®me: <br><br><ul><li>  <b>Utilisez la virtualisation et d√©marrez le cluster dans VM.</b>  Nous n'avons pas aim√© cette option en raison des frais g√©n√©raux √©lev√©s (par rapport au reste) et de l'apparition d'une autre entit√© qui doit √™tre desservie </li><li>  <b>Construisez votre version du paquet, qui mettra mysql dans un endroit non standard.</b>  Ainsi, il sera possible d'avoir plusieurs versions de mysql sur un m√™me serveur.  Une bonne option si vous avez de nombreux serveurs, mais le support constant de votre package, qui doit √™tre mis √† jour r√©guli√®rement, peut prendre beaucoup de temps. </li><li>  <b>Utilisez Docker.</b> </li></ul><br>  Nous avons choisi Docker, mais nous l'utilisons dans l'option minimale.  Pour le stockage de donn√©es, le volume local est utilis√©.  Le mode de fonctionnement ¬´--net host¬ª est utilis√© pour r√©duire la latence du r√©seau et la charge du processeur. <br><br>  Nous avons √©galement d√ª cr√©er notre propre version de l'image Docker.  La raison en est que l'image standard de Percona ne prend pas en charge la position de restauration au d√©marrage.  Cela signifie que chaque fois que l'instance est red√©marr√©e, elle n'effectue pas de synchronisation IST rapide, qui t√©l√©charge uniquement les modifications n√©cessaires, mais un SST lent, qui recharge compl√®tement la base de donn√©es. <br><br>  Un autre probl√®me est la taille du cluster.  Dans un cluster, chaque n≈ìud stocke l'int√©gralit√© de l'ensemble de donn√©es.  Par cons√©quent, la lecture √©volue parfaitement avec l'augmentation de la taille du cluster.  Avec l'enregistrement, la situation est inverse - lors de la validation, chaque transaction est valid√©e pour l'absence de conflits sur tous les n≈ìuds.  Naturellement, plus il y a de n≈ìuds, plus la validation prendra de temps. <br>  Ici, nous avons √©galement plusieurs options: <br><br><ul><li>  <b>2 n≈ìuds + arbitre.</b>  2 n≈ìuds + arbitre.  Une bonne option pour les tests.  Pendant le d√©ploiement du deuxi√®me n≈ìud, le ma√Ætre ne doit pas enregistrer. <br></li><li>  <b>3 n≈ìuds.</b>  La version classique.  √âquilibre entre vitesse et fiabilit√©.  Veuillez noter que dans cette configuration, un n≈ìud doit √©tirer la charge enti√®re, car  au moment de l'ajout du 3e noeud, le second sera le donneur. <br></li><li>  <b>4+ n≈ìuds.</b>  Avec un nombre pair de n≈ìuds, il est n√©cessaire d'ajouter un arbitre pour √©viter le split-brain.  Une option qui fonctionne bien pour une tr√®s grande quantit√© de lecture.  La fiabilit√© du cluster augmente √©galement. </li></ul><br>  Nous avons jusqu'√† pr√©sent opt√© pour l'option √† 3 n≈ìuds. <br><br>  La configuration du cluster copie presque compl√®tement la configuration MySQL autonome et ne diff√®re que par quelques options: <br><br>  <b>"Wsrep_sst_method = xtrabackup-v2"</b> Cette option d√©finit la m√©thode de copie des n≈ìuds.  Les autres options sont mysqldump et rsync, mais elles bloquent le n≈ìud pendant la dur√©e de la copie.  Je ne vois aucune raison d'utiliser la m√©thode de copie non xtrabackup-v2. <br><br>  <b>¬´Gcache¬ª</b> est un analogue du binlog de cluster.  Il s'agit d'un tampon circulaire (dans un fichier) de taille fixe dans lequel toutes les modifications sont √©crites.  Si vous d√©sactivez l'un des n≈ìuds du cluster, puis le rallumez, il essaiera de lire les modifications manquantes de Gcache (synchronisation IST).  S'il ne comporte pas les modifications requises par le n≈ìud, un rechargement complet du n≈ìud (synchronisation SST) sera n√©cessaire.  La taille de gcache est d√©finie comme suit: wsrep_provider_options = 'gcache.size = 20G;'. <br><br>  <b>wsrep_slave_threads</b> Contrairement √† la r√©plication classique dans un cluster, il est possible d'appliquer plusieurs "sets d'√©criture" √† la m√™me base de donn√©es en parall√®le.  Cette option indique le nombre de travailleurs appliquant les modifications.  Il vaut mieux ne pas laisser la valeur par d√©faut de 1, car  lors de l'application par le travailleur d'un grand ensemble d'√©criture, le reste attendra dans la file d'attente et la r√©plication du n≈ìud commencera √† √™tre en retard.  Certains conseillent de d√©finir ce param√®tre sur 2 * CPU THREADS, mais je pense que vous devez examiner le nombre d'op√©rations d'√©criture simultan√©es que vous avez. <br><br>  Nous avons opt√© pour la valeur 64. √Ä une valeur inf√©rieure, le cluster n'a parfois pas r√©ussi √† appliquer tous les jeux d'√©criture de la file d'attente lors des rafales de charge (par exemple, lors du d√©marrage de couronnes lourdes). <br><br>  <b>wsrep_max_ws_size La</b> taille d'une seule transaction dans un cluster est limit√©e √† 2 Go.  Mais les transactions importantes ne correspondent pas bien au concept PXC.  Il est pr√©f√©rable d'effectuer 100 transactions de 20 Mo chacune plut√¥t qu'une par 2 Go.  Par cons√©quent, nous avons d'abord limit√© la taille des transactions dans le cluster √† 100 Mo, puis r√©duit la limite √† 50 Mo. <br><br>  Si le mode strict est activ√©, vous pouvez d√©finir la variable " <b>binlog_row_image</b> " sur "minimal".  Cela r√©duira la taille des entr√©es dans le binlog de plusieurs fois (10 fois dans le test de Percona).  Cela permettra d'√©conomiser de l'espace disque et d'autoriser les transactions qui ne correspondent pas √† la limite avec "binlog_row_image = full". <br><br>  <b>Limites pour SST.</b>  Pour Xtrabackup, qui est utilis√© pour remplir les n≈ìuds, vous pouvez d√©finir une limite sur l'utilisation du r√©seau, le nombre de threads et la m√©thode de compression.  Cela est n√©cessaire pour que lorsque le n≈ìud est rempli, le serveur donneur ne commence pas √† ralentir.  Pour ce faire, la section ¬´sst¬ª est ajout√©e au fichier my.cnf: <br><br><pre><code class="hljs powershell">[<span class="hljs-type"><span class="hljs-type">sst</span></span>] rlimit = <span class="hljs-number"><span class="hljs-number">80</span></span>m compressor = <span class="hljs-string"><span class="hljs-string">"pigz -3"</span></span> decompressor = <span class="hljs-string"><span class="hljs-string">"pigz -dc"</span></span> backup_threads = <span class="hljs-number"><span class="hljs-number">4</span></span></code> </pre> <br>  Nous limitons la vitesse de copie √† 80 Mb / s.  Nous utilisons pigz pour la compression, c'est une version multi-thread de gzip. <br><br>  <b>GTID</b> Si vous utilisez des esclaves classiques, je recommande d'activer GTID sur le cluster.  Cela vous permettra de connecter l'esclave √† n'importe quel n≈ìud du cluster sans recharger l'esclave. <br><br>  De plus, je veux parler de 2 m√©canismes de cluster, leur signification et leur configuration. <br><br><h4>  Contr√¥le de flux </h4><br>  Le contr√¥le de flux est un moyen de g√©rer la charge d'√©criture dans un cluster.  Il ne permet pas aux n≈ìuds de trop tarder dans la r√©plication.  De cette fa√ßon, la r√©plication ¬´presque synchrone¬ª est obtenue.  Le m√©canisme de fonctionnement est assez simple - d√®s que la longueur de la file d'attente de r√©ception atteint la valeur d√©finie, il envoie le message ¬´Flow control pause¬ª aux autres n≈ìuds, qui leur dit de faire une pause avec la validation de nouvelles transactions jusqu'√† ce que le n≈ìud en retard finisse de ratisser la file d'attente . <br><br>  Plusieurs choses en d√©coulent: <br><br><ol><li>  L'enregistrement dans le cluster se fera √† la vitesse du n≈ìud le plus lent.  (Mais il peut √™tre resserr√©.) </li><li>  Si vous rencontrez de nombreux conflits lors de la validation des transactions, vous pouvez configurer Flow Control de mani√®re plus agressive, ce qui devrait r√©duire leur nombre. </li><li>  Le d√©calage maximal d'un n≈ìud dans un cluster est une constante, mais pas par le temps, mais par le nombre de transactions dans la file d'attente.  Le d√©lai d√©pend de la taille moyenne des transactions et du nombre de wsrep_slave_threads. </li></ol><br>  Vous pouvez afficher les param√®tres de contr√¥le de flux comme ceci: <br><br> <code>mysql&gt; SHOW GLOBAL STATUS LIKE 'wsrep_flow_control_interval_%'; <br> wsrep_flow_control_interval_low | 36 <br> wsrep_flow_control_interval_high | 71 <br></code> <br>  Tout d'abord, nous nous int√©ressons au param√®tre wsrep_flow_control_interval_high.  Il contr√¥le la longueur de la file d'attente, apr√®s quoi la pause FC est activ√©e.  Ce param√®tre est calcul√© par la formule: gcs.fc_limit * ‚àöN (o√π N = le nombre de n≈ìuds dans le cluster.). <br><br>  Le deuxi√®me param√®tre est wsrep_flow_control_interval_low.  Il est responsable de la valeur de la longueur de la file d'attente, lorsqu'il atteint quel FC est d√©sactiv√©.  Calcul√© par la formule: wsrep_flow_control_interval_high * gcs.fc_factor.  Par d√©faut, gcs.fc_factor = 1. <br><br>  Ainsi, en modifiant la longueur de la file d'attente, nous pouvons contr√¥ler le retard de r√©plication.  La r√©duction de la longueur de la file d'attente augmentera le temps que le cluster passe en pause FC, mais r√©duira le d√©calage des n≈ìuds. <br><br>  Vous pouvez d√©finir la variable de session " <b>wsrep_sync_wait</b> = 7".  Cela forcera le PXC √† ex√©cuter des requ√™tes de lecture ou d'√©criture uniquement apr√®s avoir appliqu√© tous les ensembles d'√©criture dans la file d'attente actuelle.  Naturellement, cela augmentera la latence des requ√™tes.  L'augmentation de la latence est directement proportionnelle √† la longueur de la file d'attente. <br><br>  Il est √©galement souhaitable de r√©duire la taille maximale des transactions au minimum possible, afin que les transactions longues ne passent pas accidentellement. <br><br><h4>  EVS ou Auto Evict </h4><br>  Ce m√©canisme vous permet de rejeter des n≈ìuds instables (par exemple, perte de paquets ou longs retards) ou qui r√©pondent lentement.  Gr√¢ce √† cela, les probl√®mes de communication avec un n≈ìud ne mettront pas l'ensemble du cluster, mais laisseront le n≈ìud d√©sactiv√© et continuer √† fonctionner en mode normal.  Ce m√©canisme est particuli√®rement utile lorsque le cluster fonctionne via le WAN ou des parties du r√©seau qui ne sont pas sous votre contr√¥le.  Par d√©faut, EVS est d√©sactiv√©. <br><br>  Pour l'activer, ajoutez l'option ¬´evs.version = 1;¬ª au param√®tre <b>wsrep_provider_options</b>  et "evs.auto_evict = 5;"  (le nombre d'op√©rations apr√®s lesquelles le n≈ìud s'√©teint. Une valeur de 0 d√©sactive EVS.) Il existe √©galement plusieurs param√®tres qui vous permettent d'affiner EVS: <br><br><ul><li>  <b>evs.delayed_margin Le</b> temps n√©cessaire √† un n≈ìud pour r√©pondre.  Par d√©faut, 1 seconde, mais lorsque vous travaillez sur un r√©seau local, il peut √™tre r√©duit √† 0,05-0,1 seconde ou moins. </li><li>  <b>evs.inactive_check_period</b> P√©riode de v√©rifications.  Par d√©faut 0,5 sec </li></ul><br>  En fait, le temps pendant lequel un n≈ìud peut fonctionner en cas de probl√®me avant le d√©clenchement d'EVS est evs.inactive_check_period * evs.auto_evict.  Vous pouvez √©galement d√©finir "evs.inactive_timeout" et un n≈ìud qui ne r√©pond pas sera imm√©diatement supprim√©, par d√©faut 15 secondes. <br><br>  Une nuance importante est que ce m√©canisme lui-m√™me ne renverra pas le n≈ìud lors de la restauration de la communication.  Il doit √™tre red√©marr√© √† la main. <br><br>  Nous avons install√© EVS √† la maison, mais nous n'avons pas eu l'occasion de le tester au combat. <br><br><h3>  √âquilibrage de charge </h3><br>  Pour que les clients utilisent les ressources de chaque n≈ìud de mani√®re uniforme et ex√©cutent les demandes uniquement sur les n≈ìuds de cluster actifs, nous avons besoin d'un √©quilibreur de charge.  Percona propose 2 solutions: <br><br><ul><li>  <b>ProxySQL.</b>  Il s'agit du proxy L7 pour MySQL. </li><li>  <b>Haproxy.</b>  Mais Haproxy ne sait pas comment v√©rifier l'√©tat d'un n≈ìud de cluster et d√©terminer s'il est pr√™t √† ex√©cuter des requ√™tes.  Pour r√©soudre ce probl√®me, il est propos√© d'utiliser un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">script percona-clustercheck</a> suppl√©mentaire </li></ul><br>  Au d√©but, nous voulions utiliser ProxySQL, mais apr√®s l'analyse comparative, il s'est av√©r√© que la latence perdait √† Haproxy d'environ 15 √† 20% m√™me en utilisant le mode fast_forward (les r√©√©crivains de requ√™tes, le routage et de nombreuses autres fonctions ProxySQL ne fonctionnent pas dans ce mode, les demandes sont mandat√©es en l'√©tat) . <br><br>  Haproxy est plus rapide, mais le script Percona a quelques inconv√©nients. <br><br>  Tout d'abord, il est √©crit en bash, ce qui ne contribue pas √† sa personnalisation.  Un probl√®me plus grave est qu'il ne met pas en cache le r√©sultat de la v√©rification MySQL.  Ainsi, si nous avons 100 clients, chacun v√©rifiant l'√©tat du n≈ìud une fois toutes les 1 seconde, le script fera une requ√™te √† MySQL toutes les 10 ms.  Si pour une raison quelconque MySQL commence √† fonctionner lentement, le script de validation commencera √† cr√©er un grand nombre de processus, ce qui n'am√©liorera certainement pas la situation. <br><br>  Il a √©t√© d√©cid√© d'√©crire <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une solution</a> dans laquelle la v√©rification d'√©tat MySQL et la r√©ponse Haproxy ne sont pas li√©es.  Le script v√©rifie r√©guli√®rement l'√©tat du n≈ìud en arri√®re-plan et met le r√©sultat en cache.  Le serveur Web donne √† Haproxy le r√©sultat mis en cache. <br><br><div class="spoiler">  <b class="spoiler_title">Exemple de configuration haproxy</b> <div class="spoiler_text"> <code>listen db <br> bind 127.0.0.1:3302 <br> mode tcp <br> balance first <br> default-server inter 200 rise 6 fall 6 <br> option httpchk HEAD / <br> server node1 192.168.0.1:3302 check port 9200 id 1 <br> server node2 192.168.0.2:3302 check port 9200 backup id 2 <br> server node3 192.168.0.3:3302 check port 9200 backup id 3 <br> <br> listen db_slave <br> bind 127.0.0.1:4302 <br> mode tcp <br> balance leastconn <br> default-server inter 200 rise 6 fall 6 <br> option httpchk HEAD / <br> server node1 192.168.0.1:3302 check port 9200 backup <br> server node2 192.168.0.2:3302 check port 9200 <br> server node3 192.168.0.3:3302 check port 9200 <br></code> <br>  Cet exemple montre une configuration d'assistant unique.  Les serveurs de cluster restants agissent comme des esclaves. <br></div></div><br><h3>  Suivi </h3><br>  Pour surveiller l'√©tat du cluster, nous avons utilis√© Prometheus + mysqld_exporter et Grafana pour visualiser les donn√©es.  Parce que  mysqld_exporter recueille un tas de m√©triques pour cr√©er vous-m√™me des tableaux de bord est assez fastidieux.  Vous pouvez prendre des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tableaux de bord</a> pr√™ts <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√† l'emploi de Percona</a> et les personnaliser pour vous-m√™me. <br><br>  Nous utilisons √©galement Zabbix pour collecter des mesures et alertes de cluster de base. <br><br>  Les principales mesures de cluster que vous souhaitez surveiller: <br><br><ul><li>  <b>wsrep_cluster_status</b> Doit √™tre d√©fini sur Primary sur tous les n≈ìuds.  Si la valeur est ¬´non primaire¬ª, ce n≈ìud a perdu le contact avec le quorum du cluster. </li><li>  <b>wsrep_cluster_size</b> Le nombre de n≈ìuds dans le cluster.  Cela inclut √©galement les n≈ìuds ¬´perdus¬ª, qui doivent √™tre dans le cluster, mais pour une raison quelconque, ils ne sont pas disponibles.  Lorsque le n≈ìud est d√©sactiv√© doucement, la valeur de cette variable diminue. </li><li>  <b>wsrep_local_state</b> Indique si le n≈ìud est un membre actif du cluster et est pr√™t √† fonctionner. </li><li>  <b>wsrep_evs_state</b> Un param√®tre important si vous avez Auto Eviction activ√© (d√©sactiv√© par d√©faut).  Cette variable indique qu'EVS consid√®re ce n≈ìud sain. </li><li>  <b>wsrep_evs_evict_list La</b> liste des n≈ìuds qui ont √©t√© lanc√©s par EVS √† partir du cluster.  Dans une situation normale, la liste doit √™tre vide. </li><li>  <b>wsrep_evs_delayed</b> Liste des candidats √† la suppression d'EVS.  Doit √©galement √™tre vide. </li></ul><br>  Mesures de performance cl√©s: <br><br><ul><li>  <b>wsrep_evs_repl_latency</b> Affiche ( <b>d√©lai</b> minimum / moyen / maximum / √©cart senior / taille de paquet) le d√©lai de communication au sein du cluster.  Autrement dit, il mesure la latence du r√©seau.  Des valeurs croissantes peuvent indiquer une surcharge du r√©seau ou des n≈ìuds de cluster.  Cette m√©trique est enregistr√©e m√™me lorsque l'EVS est √©teint. </li><li>  <b>wsrep_flow_control_paused_ns Le</b> temps (en ns) depuis le d√©marrage du noeud qu'il a pass√© dans la pause du contr√¥le de flux.  Id√©alement, il devrait √™tre 0. La croissance de ce param√®tre indique des probl√®mes avec les performances du cluster ou le manque de "wsrep_slave_threads".  Vous pouvez d√©terminer quel n≈ìud ralentit par le param√®tre " <b>wsrep_flow_control_sent</b> ". </li><li>  <b>wsrep_flow_control_paused</b> Pourcentage de temps √©coul√© depuis la derni√®re ex√©cution de ¬´FLUSH STATUS;¬ª que le n≈ìud a pass√© dans la pause du contr√¥le de flux.  En plus de la variable pr√©c√©dente, elle devrait tendre vers z√©ro. </li><li>  <b>wsrep_flow_control_status</b> Indique si Flow Control est en cours d'ex√©cution.  Sur le n≈ìud initiateur de pause FC, la valeur de cette variable sera ON. </li><li>  <b>wsrep_local_recv_queue_avg Longueur</b> moyenne de la file d'attente de r√©ception.  La croissance de ce param√®tre indique des probl√®mes avec les performances du n≈ìud. </li><li>  <b>wsrep_local_send_queue_avg</b> La longueur moyenne de la file d'attente d'envoi.  La croissance de ce param√®tre indique des probl√®mes de performances r√©seau. </li></ul><br>  Il n'y a pas de recommandations universelles sur les valeurs de ces param√®tres.  Il est clair qu'ils devraient tendre √† z√©ro, mais √† charge r√©elle, ce ne sera probablement pas le cas et vous devrez d√©terminer par vous-m√™me o√π passe la limite de l'√©tat normal du cluster. <br><br><h3>  Sauvegarde </h3><br>  La sauvegarde de cluster n'est pratiquement pas diff√©rente de mysql autonome.  Pour une utilisation en production, nous avons plusieurs options. <br><br><ul><li>  Supprimez la sauvegarde de l'un des n≈ìuds "de gain" √† l'aide de xtrabackup.  L'option la plus simple, mais pendant la performance du cluster de sauvegarde sera gaspill√©e. </li><li>  Utilisez des esclaves classiques et sauvegardez √† partir de r√©pliques. </li></ul><br>  Les sauvegardes autonomes et avec la version de cluster cr√©√©e √† l'aide de xtrabackup sont portables entre elles.  Autrement dit, la sauvegarde prise √† partir du cluster peut √™tre d√©ploy√©e sur mysql autonome et vice versa.  Naturellement, la version principale de MySQL doit correspondre, de pr√©f√©rence la mineure.  Les sauvegardes effectu√©es √† l'aide de mysqldump sont naturellement portables √©galement. <br><br>  La seule mise en garde est qu'apr√®s avoir d√©ploy√© la sauvegarde, vous devez ex√©cuter le script mysql_upgrade, qui v√©rifiera et corrigera la structure de certaines tables syst√®me. <br><br><h3>  Migration de donn√©es </h3><br>  Maintenant que nous avons compris la configuration, la surveillance et d'autres choses, nous pouvons commencer la migration vers le prod. <br><br>  La migration des donn√©es dans notre sch√©ma √©tait assez simple, mais nous avons un peu g√¢ch√©;). <br>  L√©gende - le ma√Ætre 1 et le ma√Ætre 2 sont connect√©s par le ma√Ætre de r√©plication ma√Ætre.  L'enregistrement va uniquement au master 1. Master 3 est un serveur propre. <br><br>  Notre plan de migration (dans le plan je vais omettre les op√©rations avec des esclaves pour plus de simplicit√© et ne parlerai que des serveurs ma√Ætres). <br><br><h4>  Tentative 1 </h4><br><ol><li>  Supprimez la sauvegarde de la base de donn√©es du ma√Ætre 1 √† l'aide de xtrabackup. </li><li>  Copiez la sauvegarde sur le ma√Ætre 3 et ex√©cutez le cluster en mode n≈ìud unique. </li><li>  Configurez la r√©plication principale entre les ma√Ætres 3 et 1. </li><li>  Basculez la lecture et l'√©criture sur le ma√Ætre 3. V√©rifiez l'application. </li><li>  Sur le ma√Ætre 2, d√©sactivez la r√©plication et d√©marrez MySQL en cluster.  Nous attendons qu'il copie la base de donn√©es du ma√Ætre 3. Lors de la copie, nous avions un cluster d'un n≈ìud √† l'√©tat ¬´Donateur¬ª et un n≈ìud toujours non fonctionnel.  Pendant la copie, nous avons obtenu un tas de verrous et, √† la fin, les deux n≈ìuds sont tomb√©s avec une erreur (la cr√©ation d'un nouveau n≈ìud ne peut pas √™tre termin√©e en raison de verrous morts).  Cette petite exp√©rience nous a co√ªt√© quatre minutes de temps d'arr√™t. </li><li>  Basculez la lecture et l'√©criture sur le ma√Ætre 1. </li></ol><br>  La migration n'a pas fonctionn√© du fait que lors du test du circuit dans l'environnement de d√©veloppement sur la base de donn√©es, il n'y avait pratiquement pas de trafic d'√©criture, et lors de la r√©p√©tition du m√™me circuit sous charge, des probl√®mes √©taient survenus. <br>  Nous avons l√©g√®rement modifi√© le sch√©ma de migration pour √©viter ces probl√®mes et r√©essay√©, pour la deuxi√®me fois avec succ√®s;). <br><br><h4>  Tentative 2 </h4><br><ol><li>  Nous red√©marrons le ma√Ætre 3 pour qu'il fonctionne √† nouveau en mode n≈ìud unique. </li><li>  Nous remontons le cluster MySQL sur le ma√Ætre 2.  Pour le moment, le trafic provenant de la r√©plication n'est all√© qu'au cluster, il n'y a donc pas eu de probl√®mes r√©p√©t√©s avec les verrous et le deuxi√®me n≈ìud a √©t√© ajout√© avec succ√®s au cluster. </li><li>  Encore une fois, passez la lecture et l'√©criture au ma√Ætre 3. Nous v√©rifions le fonctionnement de l'application. </li><li>  D√©sactivez la r√©plication ma√Ætre avec ma√Ætre 1. Activez le cluster mysql sur ma√Ætre 1 et attendez qu'il d√©marre.  Afin de ne pas marcher sur le m√™me r√¢teau, il est important que l'application n'√©crive pas sur le n≈ìud Donateur (pour plus de d√©tails, voir la section sur l'√©quilibrage de charge).  Apr√®s avoir d√©marr√© le troisi√®me n≈ìud, nous aurons un cluster enti√®rement fonctionnel de trois n≈ìuds. </li><li>  Vous pouvez supprimer une sauvegarde de l'un des n≈ìuds du cluster et cr√©er le nombre d'esclaves classiques dont vous avez besoin. </li></ol><br>  La diff√©rence entre le deuxi√®me sch√©ma et le premier est que nous avons commut√© le trafic vers le cluster uniquement apr√®s avoir √©lev√© le deuxi√®me n≈ìud du cluster. <br><br>  Cette proc√©dure nous a pris environ 6 heures. <br><br><h3>  Multi-ma√Ætre </h3><br>  Apr√®s la migration, notre cluster a fonctionn√© en mode ma√Ætre unique, c'est-√†-dire que la totalit√© de l'enregistrement est all√©e sur l'un des serveurs, et seules les donn√©es ont √©t√© lues √† partir du reste. <br><br>  Apr√®s avoir fait passer la production en mode multi-ma√Ætre, nous avons rencontr√© un probl√®me - les conflits de transaction sont survenus plus souvent que pr√©vu.  Cela √©tait particuli√®rement mauvais avec les requ√™tes qui modifient de nombreux enregistrements, par exemple, en mettant √† jour la valeur de tous les enregistrements d'une table.  Les transactions qui ont √©t√© ex√©cut√©es avec succ√®s sur le m√™me n≈ìud de mani√®re s√©quentielle sur le cluster sont ex√©cut√©es en parall√®le et une transaction plus longue re√ßoit une erreur de blocage.  Je ne tarderai pas, apr√®s plusieurs tentatives pour r√©soudre ce probl√®me au niveau de l'application, nous avons abandonn√© l'id√©e du multi-ma√Ætre. <br><br><h3>  Autres nuances </h3><br><ul><li>  Un cluster peut √™tre un esclave.  Lors de l'utilisation de cette fonction, je recommande d'ajouter √† la config tous les n≈ìuds sauf celui qui est l'option esclave "skip_slave_start = 1".  Sinon, chaque nouveau n≈ìud d√©marrera la r√©plication √† partir du ma√Ætre, ce qui entra√Ænera des erreurs de r√©plication ou une corruption des donn√©es sur la r√©plique. </li><li>  Comme je l'ai dit Donor, un n≈ìud ne peut pas servir correctement les clients.  Il ne faut pas oublier que dans un cluster de trois n≈ìuds, des situations sont possibles lorsqu'un n≈ìud s'est envol√©, que le second est un donneur et qu'il ne reste qu'un n≈ìud pour le service client. </li></ul><br><h3>  Conclusions </h3><br>  Apr√®s la migration et un certain temps de fonctionnement, nous sommes arriv√©s aux conclusions suivantes. <br><br><ul><li>  Le cluster Galera fonctionne et est assez stable (au moins tant qu'il n'y a pas eu de gouttes anormales de n≈ìuds ou de leur comportement anormal).  En termes de tol√©rance aux pannes, nous avons obtenu exactement ce que nous voulions. </li><li>  Les relev√©s multima√Ætres de Percona sont principalement marketing.  Oui, il est possible d'utiliser le cluster dans ce mode, mais cela n√©cessitera une profonde modification de l'application pour ce mod√®le d'utilisation. </li><li>  Il n'y a pas de r√©plication synchrone, mais maintenant nous contr√¥lons le retard maximum des n≈ìuds (dans les transactions).  Avec la limitation de la taille maximale des transactions de 50 Mo, nous pouvons pr√©dire assez pr√©cis√©ment le temps de latence maximal des n≈ìuds.  Il est devenu plus facile pour les d√©veloppeurs d'√©crire du code. </li><li>  Lors de la surveillance, nous observons des pics √† court terme dans la croissance de la file d'attente de r√©plication.  La raison en est dans notre r√©seau √† 1 Gbit / s.  Il est possible de faire fonctionner un cluster sur un tel r√©seau, mais des probl√®mes apparaissent lors des rafales de charge.  Nous pr√©voyons maintenant de mettre √† niveau le r√©seau √† 10 Gbit / s. </li></ul><br>  Au total, trois ¬´Wishlist¬ª que nous avons re√ßues environ un an et demi.  L'exigence la plus importante est la tol√©rance aux pannes. <br><br>  Notre fichier de configuration PXC pour les personnes int√©ress√©es: <br><br><div class="spoiler">  <b class="spoiler_title">my.cnf</b> <div class="spoiler_text"> <code>[mysqld] <br> #Main <br> server-id = 1 <br> datadir = /var/lib/mysql <br> socket = mysql.sock <br> port = 3302 <br> pid-file = mysql.pid <br> tmpdir = /tmp <br> large_pages = 1 <br> skip_slave_start = 1 <br> read_only = 0 <br> secure-file-priv = /tmp/ <br> <br> #Engine <br> innodb_numa_interleave = 1 <br> innodb_flush_method = O_DIRECT <br> innodb_flush_log_at_trx_commit = 2 <br> innodb_file_format = Barracuda <br> join_buffer_size = 1048576 <br> tmp-table-size = 512M <br> max-heap-table-size = 1G <br> innodb_file_per_table = 1 <br> sql_mode = "NO_ENGINE_SUBSTITUTION,NO_AUTO_CREATE_USER,ERROR_FOR_DIVISION_BY_ZERO" <br> default_storage_engine = InnoDB <br> innodb_autoinc_lock_mode = 2 <br> <br> #Wsrep <br> wsrep_provider = "/usr/lib64/galera3/libgalera_smm.so" <br> wsrep_cluster_address = "gcomm://192.168.0.1:4577,192.168.0.2:4577,192.168.0.3:4577" <br> wsrep_cluster_name = "prod" <br> wsrep_node_name = node1 <br> wsrep_node_address = "192.168.0.1" <br> wsrep_sst_method = xtrabackup-v2 <br> wsrep_sst_auth = "USER:PASS" <br> pxc_strict_mode = ENFORCING <br> wsrep_slave_threads = 64 <br> wsrep_sst_receive_address = "192.168.0.1:4444" <br> wsrep_max_ws_size = 50M <br> wsrep_retry_autocommit = 2 <br> wsrep_provider_options = "gmcast.listen_addr=tcp://192.168.0.1:4577; ist.recv_addr=192.168.0.1:4578; gcache.size=30G; pc.checksum=true; evs.version=1; evs.auto_evict=5; gcs.fc_limit=80; gcs.fc_factor=0.75; gcs.max_packet_size=64500;" <br> <br> #Binlog <br> expire-logs-days = 4 <br> relay-log = mysql-relay-bin <br> log_slave_updates = 1 <br> binlog_format = ROW <br> binlog_row_image = minimal <br> log_bin = mysql-bin <br> log_bin_trust_function_creators = 1 <br> <br> #Replication <br> slave-skip-errors = OFF <br> relay_log_info_repository = TABLE <br> relay_log_recovery = ON <br> master_info_repository = TABLE <br> gtid-mode = ON <br> enforce-gtid-consistency = ON <br> <br> #Cache <br> query_cache_size = 0 <br> query_cache_type = 0 <br> thread_cache_size = 512 <br> table-open-cache = 4096 <br> innodb_buffer_pool_size = 72G <br> innodb_buffer_pool_instances = 36 <br> key_buffer_size = 16M <br> <br> #Logging <br> log-error = /var/log/stdout.log <br> log_error_verbosity = 1 <br> slow_query_log = 0 <br> long_query_time = 10 <br> log_output = FILE <br> innodb_monitor_enable = "all" <br> <br> #Timeout <br> max_allowed_packet = 512M <br> net_read_timeout = 1200 <br> net_write_timeout = 1200 <br> interactive_timeout = 28800 <br> wait_timeout = 28800 <br> max_connections = 22000 <br> max_connect_errors = 18446744073709551615 <br> slave-net-timeout = 60 <br> <br> #Static Values <br> ignore_db_dir = "lost+found" <br> <br> [sst] <br> rlimit = 80m <br> compressor = "pigz -3" <br> decompressor = "pigz -dc" <br> backup_threads = 8 <br></code> <br></div></div><br><h3>  Sources et liens utiles </h3><br>  ‚Üí <a href="">Notre image Docker</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Documentation Percona XtraDB Cluster 5.7</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Surveillance de l'√©tat du cluster - Documentation du cluster Galera</a> <br>  ‚Üí <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Variables d'√©tat Galera - Documentation du cluster Galera</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr422347/">https://habr.com/ru/post/fr422347/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr422335/index.html">Les plus petits ordinateurs Linux</a></li>
<li><a href="../fr422337/index.html">Yandex a lanc√© un cloud</a></li>
<li><a href="../fr422339/index.html">"Je pense que JavaScript n'est pas adapt√© au Web." 10 questions au programmeur, 4 sorties (depuis Berlin)</a></li>
<li><a href="../fr422341/index.html">IdO - promouvoir pendant que les autres pensent</a></li>
<li><a href="../fr422345/index.html">Serveur dans les nuages: R√©sum√© du projet</a></li>
<li><a href="../fr422357/index.html">Apprentissage en profondeur pour d√©terminer le style et le genre des peintures</a></li>
<li><a href="../fr422361/index.html">Syndrome d'entreprise</a></li>
<li><a href="../fr422363/index.html">Conf√©rence PyCon Russie 2018: vid√©o de tous les rapports et pr√©sentations</a></li>
<li><a href="../fr422365/index.html">Yandex a d√©pos√© une plainte contre une d√©cision de justice de supprimer les liens vers des contenus pirat√©s</a></li>
<li><a href="../fr422367/index.html">Comment restaurer des vid√©os pour Full Throttle Remastered. 2e partie</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>