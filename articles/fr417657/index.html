<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üêü üçõ üî° Cr√©ation d'un bot pour participer √† l'IA mini cup 2018 bas√© sur un r√©seau neuronal r√©current (partie 2) üëåüèæ ü§∏üèΩ üë®üèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Il s'agit d'une continuation de la premi√®re partie de l'article. 


 Dans la premi√®re partie de l'article, l'auteur a parl√© des conditions du concours...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cr√©ation d'un bot pour participer √† l'IA mini cup 2018 bas√© sur un r√©seau neuronal r√©current (partie 2)</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/417657/"><p><img src="https://habrastorage.org/webt/of/tw/ke/oftwke-wbh5-w_nlsm_p5rmhano.jpeg"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Il s'agit d'une continuation de la premi√®re partie de l'article.</a> </p><br><p> Dans la premi√®re partie de l'article, l'auteur a parl√© des conditions du concours pour le jeu Agario sur mail.ru, de la structure du monde du jeu et en partie de la structure du bot.  En partie, car ils n'affectaient que le dispositif des capteurs d'entr√©e et des commandes en sortie du r√©seau neuronal (ci-apr√®s en images et texte il y aura une abr√©viation NN).  Essayons donc d'ouvrir la bo√Æte noire et de comprendre comment tout y est organis√©. </p><a name="habracut"></a><br><p>  Et voici la premi√®re photo: </p><br><p><img src="https://habrastorage.org/webt/v2/pb/_s/v2pb_sha05gqjtqcqp9bv5mnm0o.jpeg"></p><br><p>  Il d√©crit sch√©matiquement ce qui devrait provoquer un sourire ennuy√© de mon lecteur, disent-ils encore en premi√®re ann√©e, ils ont √©t√© vus plusieurs fois dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">diverses sources</a> .  Mais nous voulons vraiment appliquer pratiquement cette image √† la gestion du bot, donc apr√®s la note importante, nous l'examinons de plus pr√®s. </p><br><p>  <strong>Remarque importante:</strong> il existe un grand nombre de solutions pr√™tes √† l'emploi (frameworks) pour travailler avec les r√©seaux de neurones: </p><br><p><img src="https://habrastorage.org/webt/jt/il/97/jtil97rhtusvazj6iared1q_hnc.png"></p><br><p>  Tous ces packages r√©solvent les t√¢ches principales du d√©veloppeur de r√©seaux de neurones: la construction et l'entra√Ænement de NN ou la recherche de poids "optimaux".  Et la m√©thode principale de cette recherche est la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©tropropagation</a> .  Il a √©t√© invent√© dans les ann√©es 70 du si√®cle dernier, comme l'indique l'article au lien ci-dessus, pendant ce temps, en tant que bas du navire, il a acquis diverses am√©liorations, mais l'essence est la m√™me: trouver des coefficients de poids avec une base d'exemples de formation et il est hautement souhaitable que tout le monde de ces exemples contenaient une r√©ponse toute faite sous la forme d'un signal de sortie d'un r√©seau neuronal.  Le lecteur peut m'objecter.  que des r√©seaux d'auto-apprentissage de diverses classes et principes ont d√©j√† √©t√© invent√©s, mais tout ne se passe pas bien l√†-bas, pour autant que je sache.  Bien s√ªr, il est pr√©vu d'√©tudier ce zoo plus en d√©tail, mais je pense que je trouverai des personnes partageant les m√™mes id√©es qu'un v√©lo fabriqu√© sans dessin sp√©cial est encore plus incurv√© au c≈ìur du cr√©ateur qu'un clone de convoyeur d'un v√©lo id√©al. <br>  Comprenant que le serveur de jeu n'aura probablement pas ces biblioth√®ques et que la puissance de calcul allou√©e par les organisateurs en tant que c≈ìur de processeur n'est clairement pas suffisante pour un cadre lourd, l'auteur a ensuite cr√©√© son propre v√©lo.  Un commentaire important √† ce sujet a pris fin. </p><br><p>  Revenons √† l'image repr√©sentant probablement le plus simple des r√©seaux de neurones possibles avec une couche cach√©e (aka couche cach√©e ou couche cach√©e).  Maintenant, l'auteur lui-m√™me a regard√© fixement l'image avec des id√©es sur cet exemple simple pour r√©v√©ler au lecteur les profondeurs des r√©seaux de neurones artificiels.  Lorsque tout est simplifi√© en primitif, il est plus facile de comprendre l'essence.  L'essentiel est que le neurone de la couche cach√©e n'a rien √† r√©sumer.  Et tr√®s probablement, ce n'est m√™me pas un r√©seau de neurones, dans les manuels, le NN le plus simple est un r√©seau √† deux entr√©es.  Nous voici donc pour ainsi dire les d√©couvreurs des r√©seaux les plus simples. </p><br><p>  Essayons de d√©crire ce r√©seau neuronal (pseudocode): <br>  Nous introduisons la topologie du r√©seau sous la forme d'un tableau, o√π chaque √©l√©ment correspond √† la couche et au nombre de neurones qu'elle contient: </p><br><p><code>int array Topology= { 1, 1, 1}</code> <br>  Nous avons √©galement besoin d'un tableau flottant de poids du r√©seau de neurones W, consid√©rant notre r√©seau comme un ¬´r√©seaux de neurones √† action directe (FF ou FFNN)¬ª, o√π chaque neurone de la couche actuelle est connect√© √† chaque neurone de la couche suivante, nous obtenons la dimension du r√©seau W [nombre de couches , le nombre de neurones dans la couche, le nombre de neurones dans la couche].  Pas tout √† fait l'encodage optimal, mais √©tant donn√© le souffle chaud du GPU quelque part tr√®s proche dans le texte, il est compr√©hensible. <br>  Une courte proc√©dure <code>CalculateSize</code> pour compter le nombre de neurones <code>neuroncount</code> et le nombre de leurs connexions dans le r√©seau de neurones <code>dendritecount</code> , je pense, expliquera mieux √† l'auteur la nature de ces connexions: </p><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span> CalculateSize(<span class="hljs-keyword"><span class="hljs-keyword">array</span></span> <span class="hljs-type"><span class="hljs-type">int</span></span> Topology, <span class="hljs-type"><span class="hljs-type">int</span></span> neuroncount, <span class="hljs-type"><span class="hljs-type">int</span></span> dendritecount) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i : Topology) // i         neuroncount += i; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt;Topology.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>, i &lt; Topology[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, i++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> j = <span class="hljs-number"><span class="hljs-number">0</span></span>, j &lt; Topology[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>], j++) //   dendritecount++; }</code> </pre> <br><p>  Mon lecteur, celui qui sait d√©j√† tout cela, l'auteur est venu √† cette opinion dans le premier article, ne se demandera certainement pas: pourquoi dans la troisi√®me boucle imbriqu√©e Topologie [couche1 + 1] au lieu de Topologie [couche1], qui donne plus au neurone que dans la topologie du r√©seau .  Je ne r√©pondrai pas.  Il est √©galement utile pour le lecteur de demander des devoirs. </p><br><p>  Nous sommes presque √† deux pas de la construction d'un r√©seau de neurones fonctionnel.  Il reste √† ajouter la fonction de sommation des signaux √† l'entr√©e du neurone et son activation.  Il existe de nombreuses fonctions d'activation, mais les plus proches de la nature du neurone sont Sigmoid et Tangensoid <em>(il est probablement pr√©f√©rable de l'appeler ainsi, bien que ce nom ne soit pas utilis√© dans la litt√©rature, le maximum est tangent, mais c'est le nom du graphique, bien que qu'est-ce qu'un graphique s'il ne refl√®te pas la fonction?)</em> </p><br><p>  Nous avons donc ici les fonctions d'activation des neurones (elles sont pr√©sentes dans l'image, dans sa partie inf√©rieure) </p><br><pre> <code class="hljs go">float Sigmoid(float x) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (float)(<span class="hljs-number"><span class="hljs-number">1.0f</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0f</span></span> + expf(-x))); }</code> </pre> <br><p>  Sigmoid renvoie des valeurs de 0 √† 1. </p><br><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Tanh</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)(tanhf(x)); }</code> </pre> <br><p>  La tangente renvoie des valeurs de -1 √† 1. </p><br><p>  L'id√©e principale d'un signal passant √† travers un r√©seau de neurones est une onde: un signal est envoy√© aux neurones d'entr√©e -&gt; via des connexions neuronales, le signal va √† la deuxi√®me couche -&gt; les neurones de la deuxi√®me couche r√©sument les signaux qui les ont atteints modifi√©s par des poids interneuronaux -&gt; est ajout√© √† travers un poids de polarisation suppl√©mentaire -&gt; nous utilisons la fonction d'activation-&gt; et wu-al nous allons √† la couche suivante (lisez le premier cycle de l'exemple par couches), c'est-√†-dire en r√©p√©tant la cha√Æne depuis le tout d√©but, seuls les neurones de la couche suivante deviendront des neurones d'entr√©e.  Pour simplifier, vous n'avez m√™me pas besoin de stocker les valeurs des neurones de l'ensemble du r√©seau, il vous suffit de stocker uniquement les poids NN et les valeurs des neurones de la couche active. </p><br><p>  Encore une fois, nous envoyons un signal √† l'entr√©e NN, l'onde a travers√© les couches et sur la couche de sortie, nous supprimons la valeur obtenue. </p><br><p>  Ici, du go√ªt du lecteur, il est possible de r√©soudre la r√©cursivit√© par programmation ou simplement un triple cycle comme celui de l'auteur, pour acc√©l√©rer les calculs, vous n'avez pas besoin de cl√¥turer des objets sous forme de neurones et de leurs connexions et autres POO.  Encore une fois, cela est d√ª au sentiment de calculs GPU proches, et sur les GPU, en raison de leur nature de parall√©lisme de masse, la POO s'arr√™te un peu, cela est relatif √† c # et C ++. </p><br><p>  De plus, le lecteur est invit√© √† suivre ind√©pendamment la voie de la construction d'un r√©seau neuronal en code, avec son lecteur qui le veut volontairement, dont l'absence est assez claire et famili√®re √† l'auteur, comme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">pour les exemples de construction de NN √†</a> partir de z√©ro, il y a beaucoup d'exemples dans le r√©seau, donc il sera difficile de s'√©garer, c'est comme √ßa aussi simple qu'un r√©seau de neurones √† distribution directe dans l'image ci-dessus. </p><br><p>  Mais o√π s'exclamera le lecteur, qui n'est pas encore sorti du passage pr√©c√©dent, et aura raison, dans l'enfance, l'auteur a d√©termin√© la valeur du livre par des illustrations.  Vous voil√†: </p><br><p><img src="https://habrastorage.org/webt/03/jl/dw/03jldwzryoeaxscxxtfi3deakie.jpeg"></p><br><p>  Dans l'image, nous voyons un neurone r√©current et un NN construit √† partir de ces neurones est appel√© r√©current ou RNN.  Le r√©seau neuronal sp√©cifi√© a une m√©moire √† court terme et a √©t√© s√©lectionn√© par l'auteur pour le bot comme le plus prometteur en termes d'adaptation au processus de jeu.  Bien s√ªr, l'auteur a construit un r√©seau de neurones √† distribution directe, mais dans le processus de recherche d'une solution ¬´efficace¬ª, il est pass√© au RNN. </p><br><p>  Un neurone r√©current a un √©tat suppl√©mentaire C, qui se forme apr√®s le premier passage d'un signal √† travers un neurone, Tick + 0 sur la chronologie.  En termes simples, il s'agit d'une copie du signal de sortie d'un neurone.  Dans la deuxi√®me √©tape, lisez Tick + 1 (puisque le r√©seau fonctionne √† la fr√©quence du bot de jeu et du serveur), la valeur C revient √† l'entr√©e de la couche neuronale √† travers des poids suppl√©mentaires et participe ainsi √† la formation du signal, mais d√©j√† √† Tick + 1 fois. </p><br><p>  <em>Remarque: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans les travaux des groupes de recherche concernant la gestion des bots de jeu NN</a> , il y a une tendance √† utiliser deux rythmes pour un r√©seau neuronal, un rythme est la fr√©quence du jeu Tick, le deuxi√®me rythme, par exemple, est deux fois plus lent que le premier.</em>  <em>Diff√©rentes parties du NN fonctionnent √† diff√©rentes fr√©quences, ce qui donne une vision diff√©rente de la situation de jeu √† l'int√©rieur du NN, augmentant ainsi sa flexibilit√©.</em> </p><br><p>  Pour construire RNN dans le code bot, nous introduisons un tableau suppl√©mentaire dans la topologie, o√π chaque √©l√©ment correspond √† la couche et au nombre d'√©tats neuronaux qu'elle contient: </p><br><p> <code>int array TopologyNN= { numberofSensors, 16, 8, 4}</code> <br> <code>int array TopologyRNN= { 0, 16, 0, 0 }</code> </p> <br><p>  Il ressort de la topologie ci-dessus que la deuxi√®me couche est r√©currente, car elle contient des √©tats neuronaux.  Nous introduisons √©galement des poids suppl√©mentaires sous la forme d'un flotteur du tableau WRR, de la m√™me dimension que le tableau W. </p><br><p>  Le nombre de connexions dans notre r√©seau de neurones changera un peu: </p><br><pre> <code class="hljs matlab"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> &lt; TopologyNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> &lt; TopologyNN[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>] , <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyRNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>&lt; TopologyRNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>&lt; TopologyRNN[layer], <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++;</code> </pre> <br><p>  L'auteur joindra le code g√©n√©ral d'un r√©seau neuronal r√©current √† la fin de cet article, mais l'essentiel √† comprendre est le principe: le passage d'une onde √† travers des couches dans le cas d'un NN r√©current ne change rien fondamentalement, un seul terme suppl√©mentaire est ajout√© √† la fonction d'activation des neurones.  C'est le terme de l'√©tat des neurones sur le Tick pr√©c√©dent multipli√© par le poids de la connexion neuronale. </p><br><p>  Nous supposons que la th√©orie et la pratique des r√©seaux de neurones ont √©t√© rafra√Æchies, mais l'auteur est clairement conscient qu'il n'a pas rapproch√© le lecteur de la fa√ßon d'enseigner cette structure simple des r√©seaux de neurones pour prendre des d√©cisions dans le gameplay.  Nous n'avons pas de biblioth√®ques avec des exemples pour enseigner NN.  Dans les groupes Internet de d√©veloppeurs de bots, il y avait un avis: donnez-nous un fichier journal sous forme de coordonn√©es de bots et d'autres informations de jeu pour former une biblioth√®que d'exemples.  Mais l'auteur, malheureusement, n'a pas pu comprendre comment utiliser ce fichier journal pour la formation NN.  Je serai heureux d'en discuter dans les commentaires de l'article.  Par cons√©quent, la seule m√©thode dont dispose l'auteur pour comprendre la m√©thode d'entra√Ænement, ou plut√¥t pour trouver des neurobalances "efficaces" (neuroconnexions), √©tait l'algorithme g√©n√©tique. </p><br><p>  Pr√©parer une image sur les principes de l'algorithme g√©n√©tique: </p><br><p><img src="https://habrastorage.org/webt/-3/ex/ls/-3exlspxldh64avmkbi_3vuewou.jpeg"></p><br><p>  Donc, l' <strong>algorithme g√©n√©tique</strong> . </p><br><p>  L'auteur tentera de ne pas se plonger <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dans la th√©orie de ce processus</a> , mais ne rappellera que le minimum n√©cessaire pour poursuivre une lecture compl√®te de l'article. <br>  Dans l'algorithme g√©n√©tique, le principal fluide de travail est le g√®ne (l'ADN est le nom de la mol√©cule).  Le g√©nome dans notre cas est un ensemble s√©quentiel de g√®nes ou un tableau unidimensionnel de flotteur long ... </p><br><p>  Au stade initial du travail avec un r√©seau neuronal nouvellement construit, il est n√©cessaire de l'initialiser.  L'initialisation fait r√©f√©rence √† l'attribution de valeurs al√©atoires de -1 √† 1 aux √©quilibres neuronaux. L'auteur a rencontr√© mentionne que la plage de valeurs de -1 √† 1 est trop <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">extr√™me</a> et que les r√©seaux form√©s ont des poids dans une plage plus petite, par exemple de -0,5 √† 0,5 et qu'une plage de valeurs initiale doit √™tre accept√©e comme excellente de -1 √† 1. Mais nous allons suivre la m√©thode classique de collecte de toutes les difficult√©s dans une seule porte et prendre le segment le plus large possible de variables al√©atoires initiales comme base pour initialiser le r√©seau neuronal. </p><br><p>  Maintenant, une <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bijection</a> va se produire.  Nous supposerons que la longueur (taille) du g√©nome du bot sera √©gale √† la longueur totale des r√©seaux du r√©seau neuronal TopologyNN.Length + TopologyRNN.Length non pour rien que l'auteur a pass√© le temps du lecteur sur la proc√©dure de comptage des connexions neuronales. </p><br><p>  <em>Remarque: Comme le lecteur l'a d√©j√† not√© par lui-m√™me, nous transf√©rons uniquement les poids du r√©seau neuronal au g√©notype, la structure de connexion, les fonctions d'activation et les √©tats des neurones ne sont pas transmis.</em>  <em>Pour un algorithme g√©n√©tique, seules les connexions neuronales sont suffisantes, ce qui sugg√®re qu'elles sont porteuses d'informations.</em>  <em>Il y a des d√©veloppements o√π l'algorithme g√©n√©tique modifie √©galement la structure des connexions dans le r√©seau neuronal et il est assez simple √† mettre en ≈ìuvre.</em>  <em>Ici, l'auteur laisse place √† la cr√©ativit√© du lecteur, bien qu'il y r√©fl√©chisse avec int√©r√™t: il faut comprendre l'utilisation de deux g√©nomes ind√©pendants et de deux fonctions de fitness (simplifi√© deux algorithmes g√©n√©tiques ind√©pendants) ou vous pouvez tous utiliser le m√™me g√®ne et algorithme.</em> </p><br><p>  Et puisque nous avons initialis√© NN avec des variables al√©atoires, nous avons ainsi initialis√© le g√©nome.  Le processus inverse est √©galement possible: initialisation du g√©notype par des variables al√©atoires et sa copie ult√©rieure en poids neuronaux.  La deuxi√®me option est courante.  Puisque l'algorithme g√©n√©tique dans le programme existe souvent en dehors de l'essence elle-m√™me et n'y est associ√© que par les donn√©es du g√©nome et la valeur de la fonction de fitness ... Arr√™tez, arr√™tez, dira le lecteur, l'image montre clairement la population et pas un mot sur le g√©nome individuel. </p><br><p>  Ok, ajoutez quelques images au four de l'esprit du lecteur: </p><br><p><img src="https://habrastorage.org/webt/h5/dz/ho/h5dzhojhlf6b1xqsol30qo63mdo.jpeg"></p><br><p>  Puisque l'auteur a peint les images avant d'√©crire le texte de l'article, elles soutiennent le texte, mais ne suivent pas la lettre √† la lettre de l'histoire actuelle. </p><br><p>  D'apr√®s les informations recueillies, il s'ensuit que le principal organe de travail de l'algorithme g√©n√©tique est une population de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">g√©nomes</a> .  C'est un peu contraire √† ce que l'auteur a dit plus t√¥t, mais comment faire dans le monde r√©el sans petites contradictions.  Hier, le soleil tournait autour de la terre et aujourd'hui, l'auteur parle du r√©seau neuronal √† l'int√©rieur du bot logiciel.  Pas √©tonnant qu'il se souvienne du four de la raison. <br>  Je fais confiance au lecteur lui-m√™me pour r√©gler la question des contradictions du monde.  Le monde des robots est compl√®tement autosuffisant pour l'article. </p><br><p>  Mais ce que l'auteur a d√©j√† r√©ussi √† faire, dans cette partie de l'article, c'est de former une population de bots. <br>  Regardons cela du c√¥t√© logiciel: </p><br><p>  Il existe un Bot (il peut s'agir d'un objet en POO, d'une structure, bien qu'il s'agisse probablement aussi d'un objet ou simplement d'un tableau de donn√©es).  √Ä l'int√©rieur, le Bot contient des informations sur ses coordonn√©es, sa vitesse, sa masse et d'autres informations utiles dans le processus de jeu, mais l'essentiel pour nous maintenant est qu'il contient un lien vers son g√©notype ou son g√©notype lui-m√™me, en fonction de la mise en ≈ìuvre.  Ensuite, vous pouvez proc√©der de diff√©rentes mani√®res, vous limiter √† des tableaux de poids de r√©seau neuronal ou introduire un tableau suppl√©mentaire de g√©notypes, car il sera commode pour le lecteur d'imaginer cela dans son imagination.  Dans les premi√®res √©tapes, l'auteur du programme a attribu√© des tableaux de neurobalances et de g√©notypes.  Puis il a refus√© de dupliquer les informations et s'est limit√© aux poids du r√©seau neuronal. </p><br><p>  En suivant la logique de l'histoire, vous devez dire que la population de bots est un tableau des bots ci-dessus.  Quelle boucle de jeu ... Arr√™tez-vous encore, quel cycle de jeu?  les d√©veloppeurs ont poliment fourni une place pour un seul bot √† bord d'un programme de simulation du monde du jeu sur un serveur ou un maximum de quatre bots dans un simulateur local.  Et si vous vous souvenez de la topologie du r√©seau neuronal choisi par l'auteur: </p><br><p><img src="https://habrastorage.org/webt/vh/1d/xq/vh1dxqmwqoejzszyh3zpfyykve4.jpeg"></p><br><p>  Et pour simplifier l'histoire, supposons que le g√©notype contient environ 1000 connexions neuronales, soit dit en passant, dans le simulateur, les g√©notypes ressemblent √† ceci (le rouge est une valeur de g√®ne n√©gative, le vert est une valeur positive, chaque lign√©e est un g√©nome distinct): </p><br><p><img src="https://habrastorage.org/webt/ri/h0/s-/rih0s-ss3gaflldpts95rm9yo4u.jpeg"></p><br><p>  <em>Remarque sur la photo: au fil du temps, le motif change dans le sens de la dominance de l'une des solutions, les rayures verticales sont des g√®nes de g√©notype courants.</em> </p><br><p>  Ainsi, nous avons 1000 g√®nes dans le g√©notype et un maximum de quatre robots dans le programme de simulation du monde du jeu des organisateurs de la comp√©tition.  Combien de fois avez-vous besoin d'ex√©cuter une simulation d'une bataille de bots pour que, par la force brute, m√™me la plus intelligente, se rapproche √† la recherche de "efficace" <br>  g√©notype, lisez la combinaison "efficace" de connexions neuronales, √† condition que chaque connexion neuronale varie de -1 √† 1 par √©tapes, et quelle √©tape?  l'initialisation √©tait flottante al√©atoire, elle est de 15 d√©cimales.  L'√©tape n'est pas encore claire pour nous.  Sur le nombre de variantes de combinaisons de poids neuronaux, l'auteur suppose qu'il s'agit d'un nombre infini, lors du choix d'une certaine taille de pas, probablement un nombre fini, mais dans tous les cas, ces nombres sont bien plus de 4 places dans le simulateur, m√™me en consid√©rant le lancement s√©quentiel √† partir de la file d'attente des robots plus le lancement parall√®le simultan√© des simulateurs officiels, jusqu'√† 10 sur un ordinateur (pour les amateurs de programmation vintage: ordinateurs). </p><br><p><img src="https://habrastorage.org/webt/it/-8/if/it-8ifszotccnx4wguexdtditpm.jpeg"></p><br><p>  J'esp√®re que les photos aideront le lecteur. </p><br><p>  Ici, vous devez faire une pause et parler de l'architecture de la solution logicielle.  √âtant donn√© que la solution sous la forme d'un bot logiciel distinct t√©l√©charg√© sur le site du concours n'√©tait plus appropri√©e.  Il fallait s√©parer le bot jouant selon les r√®gles de la comp√©tition dans le cadre de l'√©cosyst√®me d'organisateurs et du programme essayant de lui trouver la configuration du r√©seau neuronal.  Le diagramme suivant est tir√© de la pr√©sentation de la conf√©rence, mais refl√®te g√©n√©ralement l'image r√©elle. </p><br><p><img src="https://habrastorage.org/webt/-i/hb/ph/-ihbph2b0hlv3lfzxze7ihmu3-q.jpeg"></p><br><p>  Il a rappel√© une blague barbu: </p><br><p>  <em>Grande organisation.</em> <em><br></em>  <em>Heure 18.00, tous les employ√©s travaillent comme un.</em>  <em>Soudain, l'un des employ√©s √©teint l'ordinateur, s'habille et s'en va.</em> <em><br></em>  <em>Tout le monde le suit avec un regard surpris.</em> <em><br></em>  <em>Le lendemain.</em>  <em>√Ä 18 heures, le m√™me employ√© √©teint l'ordinateur et part.</em>  <em>Tout le monde continue de travailler et commence √† chuchoter de m√©contentement.</em> <em><br></em>  <em>Le lendemain.</em>  <em>A 18h00, le m√™me employ√© √©teint l'ordinateur ...</em> <em><br></em>  <em>Un coll√®gue s'approche de lui:</em> <em><br></em>  <em>-Comme tu n'as pas honte, on travaille, la fin du trimestre, tant de reportages, on veut aussi rentrer √† la maison √† temps et tu es une telle personne ...</em> <em><br></em>  <em>- Les gars, je suis g√©n√©ralement en vacances!</em> </p><br><p>  ... √† suivre. </p><br><p>  Oui, j'ai presque oubli√© de joindre le code de proc√©dure de calcul RNN, il est valide et √©crit ind√©pendamment, il y a donc peut-√™tre des erreurs.  Pour l'amplification, je vais l'apporter tel quel, c'est en c ++ tel qu'appliqu√© √† CUDA (une librairie de calcul sur le GPU). </p><br><p>  Remarque: les tableaux multidimensionnels ne s'entendent pas bien sur les GPU, il existe bien s√ªr des textures et des calculs matriciels, mais ils recommandent d'utiliser des tableaux unidimensionnels. </p><br><p>  Un exemple un tableau [i, j] de dimension M par j se transforme en un tableau de la forme [i * M + j]. </p><br><div class="spoiler">  <b class="spoiler_title">Code source de la proc√©dure de calcul RNN</b> <div class="spoiler_text"><pre> <code class="hljs powershell">__global__ void cudaRNN(Bot *bot, argumentsRNN *RNN, ConstantStruct *Const, int *Topology, int *TopologyRNN, int numElements, int gameTick) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int threadN = gridDim.x * blockDim.x; int TopologySize = Const-&gt;TopologySize; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int pos = tid; pos &lt; numElements; pos += threadN) { const int ii = pos; const int iiA = pos*Const-&gt;ArrayDim; int ArrayDim = Const-&gt;ArrayDim; const int iiAT = ii*TopologySize*ArrayDim; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].TTF != <span class="hljs-number"><span class="hljs-number">0</span></span> &amp;&amp; bot[<span class="hljs-type"><span class="hljs-type">pos</span></span>].Mass&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-number"><span class="hljs-number">0</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; //bias int neuroncount7 = Topology[<span class="hljs-number"><span class="hljs-number">0</span></span>]; neuroncount7++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; layer1 &lt; TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>; layer1++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j4 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j4 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; j4++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i5 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i5 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] + <span class="hljs-number"><span class="hljs-number">1</span></span>; i5++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] = RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>] + RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>] * RNN-&gt;NNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i5</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j4</span></span>]; } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (TopologyRNN[<span class="hljs-type"><span class="hljs-type">layer1</span></span>] &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int j14 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j14 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; j14++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i15 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i15 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span>]; i15++) { RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>] * RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">i15</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; } RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] = RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>] + <span class="hljs-number"><span class="hljs-number">1.0</span></span>f* RNN-&gt;MNweights[((<span class="hljs-type"><span class="hljs-type">ii</span></span>*<span class="hljs-type"><span class="hljs-type">TopologySize</span></span> + <span class="hljs-type"><span class="hljs-type">layer1</span></span>)*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span>])*<span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> + <span class="hljs-type"><span class="hljs-type">j14</span></span>]; //bias=<span class="hljs-number"><span class="hljs-number">1</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int t = <span class="hljs-number"><span class="hljs-number">0</span></span>; t &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; t++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = Tanh(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] + RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]); RNN-&gt;neuronContext[<span class="hljs-type"><span class="hljs-type">iiAT</span></span> + <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span><span class="hljs-type"><span class="hljs-type">Dim</span></span> * <span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>] = RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">t</span></span>]; } //SoftMax /* double sum = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt;ArrayDim; ++k) sum += exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt; ArrayDim; ++k) RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>] = exp(RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">k</span></span>]) / sum; */ } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i1 &lt; Topology[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; i1++) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>] = Sigmoid(RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i1</span></span>]); //sigma } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (layer1 + <span class="hljs-number"><span class="hljs-number">1</span></span> != TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>) { RNN-&gt;outputs[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">Topology</span></span>[<span class="hljs-type"><span class="hljs-type">layer1</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int i2 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i2 &lt; ArrayDim; i2++) { RNN-&gt;sums[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; RNN-&gt;sumsContext[<span class="hljs-type"><span class="hljs-type">iiA</span></span> + <span class="hljs-type"><span class="hljs-type">i2</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span>.f; } } } } }</code> </pre> </div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417657/">https://habr.com/ru/post/fr417657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr417647/index.html">Un projet de loi sur la protection des donn√©es personnelles pr√©sent√© au B√©larus - ce qu'il y a "√† l'int√©rieur"</a></li>
<li><a href="../fr417649/index.html">OpenAI surmonte les limitations importantes de l'IA pour Dota 2</a></li>
<li><a href="../fr417651/index.html">Que devrait faire le lecteur pour en savoir plus?</a></li>
<li><a href="../fr417653/index.html">Le√ßon ouverte "Concepts de base des bases de donn√©es"</a></li>
<li><a href="../fr417655/index.html">Contexte: Roscosmos State Corporation et ses travaux</a></li>
<li><a href="../fr417659/index.html">Neuropoet et autres pop stars du futur</a></li>
<li><a href="../fr417661/index.html">Aubrey de Gray visite Joe Rogan</a></li>
<li><a href="../fr417665/index.html">La grammaire anglaise comme math√©matique. Par o√π commencer pour ceux qui n'ont pas travaill√©</a></li>
<li><a href="../fr417667/index.html">AI. Traqueur de barri√®re tactique</a></li>
<li><a href="../fr417671/index.html">Nouvelles fonctionnalit√©s du langage de programmation ABAP dans les webinaires de SAP</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>