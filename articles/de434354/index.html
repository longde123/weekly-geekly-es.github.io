<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∫ üñ®Ô∏è üìù Erstellen eines Gesichtserkennungsmodells mithilfe von Deep Learning in Python üï• ü¶Ç üë©üèª‚Äçü§ù‚Äçüë®üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Die √úbersetzerin Elena Bornovolokova hat speziell f√ºr Netology einen Artikel von Fayzan Shaykh angepasst, in dem erl√§utert wird, wie ein Modell der Ge...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen eines Gesichtserkennungsmodells mithilfe von Deep Learning in Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/netologyru/blog/434354/">  <i>Die √úbersetzerin Elena Bornovolokova hat speziell f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Netology</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">einen Artikel von</a> Fayzan Shaykh angepasst, in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dem</a> erl√§utert wird, wie ein Modell der Gesichtserkennung erstellt wird und in welchen Bereichen es angewendet werden kann.</i> <br><br><h2>  Einf√ºhrung </h2><br>  In den letzten Jahren hat Computer Vision an Popularit√§t gewonnen und sich in einer anderen Richtung hervorgetan.  Entwickler erstellen neue Anwendungen, die weltweit eingesetzt werden. <a name="habracut"></a><br>  In dieser Richtung zieht mich das Konzept von Open Source an.  Selbst Technologieriesen sind bereit, neue Entdeckungen und Innovationen mit allen zu teilen, damit Technologie nicht das Privileg der Reichen bleibt. <br><br>  Eine dieser Technologien ist die Gesichtserkennung.  Bei korrekter und ethischer Anwendung kann diese Technologie in vielen Lebensbereichen eingesetzt werden. <br><br>  In diesem Artikel werde ich Ihnen zeigen, wie Sie mit Open Source-Tools einen effektiven Gesichtserkennungsalgorithmus erstellen.  Bevor Sie mit diesen Informationen fortfahren, m√∂chten wir, dass Sie sich vorbereiten und sich von diesem Video inspirieren lassen: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wr4rx0Spihs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Gesichtserkennung: M√∂gliche Anwendungen </h2><br>  Hier sind einige m√∂gliche Anwendungsbereiche der Gesichtserkennungstechnologie. <br><br>  <b>Gesichtserkennung in sozialen Netzwerken</b> .  Facebook hat das manuelle Markieren von Bildern durch automatisch generierte Tag-Vorschl√§ge f√ºr jedes auf die Plattform hochgeladene Bild ersetzt.  Facebook verwendet einen einfachen Gesichtserkennungsalgorithmus, um die Pixel im Bild zu analysieren und mit den jeweiligen Benutzern zu vergleichen. <br><br>  <b>Gesichtserkennung in Sicherheit</b> .  Ein einfaches Beispiel f√ºr die Verwendung der Gesichtserkennungstechnologie zum Schutz pers√∂nlicher Daten ist das Entsperren Ihres Smartphones ‚Äûim Gesicht‚Äú.  Diese Technologie kann auch im Zugangssystem implementiert werden: Eine Person schaut in die Kamera und bestimmt, ob sie sie betreten soll oder nicht. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/bYrRQQX2PvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <b>Gesichtserkennung, um die Anzahl der Personen zu z√§hlen</b> .  Mithilfe der Gesichtserkennungstechnologie kann die Anzahl der Personen gez√§hlt werden, die an einer Veranstaltung (z. B. einer Konferenz oder einem Konzert) teilnehmen.  Anstatt die Teilnehmer manuell zu z√§hlen, installieren wir eine Kamera, die Bilder der Gesichter der Teilnehmer aufnehmen und die Gesamtzahl der Besucher ausgeben kann.  Dies hilft, den Prozess zu automatisieren und Zeit zu sparen. <br><br><img src="https://habrastorage.org/webt/fn/bc/-k/fnbc-kgpcaeogtd4attryczyjfq.png"><br><br><h2>  System-Setup: Hardware- und Softwareanforderungen </h2><br>  √úberlegen Sie, wie wir die Gesichtserkennungstechnologie nutzen k√∂nnen, indem Sie sich an die uns zur Verf√ºgung stehenden Open Source-Tools wenden. <br><br>  Ich habe die folgenden Tools verwendet, die ich Ihnen empfehle: <br><br><ul><li>  Webcam (Logitech C920) zum Erstellen eines Gesichtserkennungsmodells in Echtzeit auf einem Lenovo E470 ThinkPad-Laptop (Core i5 7. Generation).  Sie k√∂nnen auch die integrierte Kamera oder den Camcorder Ihres Laptops mit einem beliebigen geeigneten System f√ºr die Echtzeit-Videoanalyse anstelle der von mir verwendeten verwenden. <br></li><li> Es ist vorzuziehen, einen Grafikprozessor f√ºr eine schnellere Videoverarbeitung zu verwenden. <br></li><li>  Wir haben das Betriebssystem Ubuntu 18.04 mit der gesamten erforderlichen Software verwendet. <br></li></ul><br>  Bevor wir mit der Erstellung unseres Gesichtserkennungsmodells fortfahren, werden wir diese Punkte genauer analysieren. <br><br><h3>  Schritt 1: Hardware-Setup </h3><br>  √úberpr√ºfen Sie, ob die Kamera richtig konfiguriert ist.  Mit Ubuntu ist dies einfach: √úberpr√ºfen Sie, ob das Ger√§t vom Betriebssystem erkannt wird.  Gehen Sie dazu folgenderma√üen vor: <br><br><ol><li> √úberpr√ºfen Sie vor dem Anschlie√üen der Webcam an den Laptop alle angeschlossenen Videoger√§te, indem Sie an der Eingabeaufforderung <code>ls /dev/video*</code> eingeben.  Als Ergebnis wird eine Liste aller an das System angeschlossenen Videoger√§te angezeigt. <img src="https://habrastorage.org/webt/za/h8/9m/zah89mjqr1gezzo8xekib9fhmzw.png"></li><li>  Schlie√üen Sie die Webcam an und geben Sie den Befehl erneut aus.  Wenn die Webcam richtig angeschlossen ist, wird das neue Ger√§t als Ergebnis des Befehls angezeigt. <img src="https://habrastorage.org/webt/bs/og/lc/bsoglcdg4tevubdfds6reijtogw.png"></li><li>  Sie k√∂nnen auch die Webcam-Software verwenden, um den korrekten Betrieb zu √ºberpr√ºfen.  Ubuntu kann hierf√ºr das Cheese-Programm verwenden. <img src="https://habrastorage.org/webt/jl/9t/f3/jl9tf3b3qke5udd2fu3h1mzbcmo.png"></li></ol><br><h3>  Schritt 2: Software-Setup </h3><br>  <b>Schritt 2.1: Installieren Sie Python</b> <br><br>  Der Code in diesem Artikel wurde mit Python (Version 3.5) geschrieben.  F√ºr die Installation von Python empfehle ich die Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anaconda</a> , einer beliebten Python-Distribution zum Verarbeiten und Analysieren von Daten. <br><br>  <b>Schritt 2.2: Installieren Sie OpenCV</b> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">OpenCV</a> ist eine Open Source-Bibliothek, mit der Computer Vision-Anwendungen erstellt werden k√∂nnen.  Die Installation von OpenCV erfolgt mit <code>pip</code> : <br><br><pre> <code class="python hljs">pip3 install opencv-python</code> </pre> <br>  <b>Schritt 2.3: Legen Sie die face_recognition-API fest</b> <br><br>  Wir werden die <code>face_recognition API</code> , die als die einfachste Python-Gesichtserkennungs-API der Welt gilt.  Verwenden Sie zum Installieren: <br><br><pre> <code class="python hljs">pip install dlib pip install face_recognition</code> </pre> <br><h2>  Implementierung </h2><br>  Nach dem Einrichten des Systems fahren wir mit der Implementierung fort.  Zun√§chst erstellen wir ein Programm und erkl√§ren dann, was wir getan haben. <br><br><h3>  Exemplarische Vorgehensweise </h3><br>  Erstellen Sie eine Datei <code>face_detector.py</code> und kopieren Sie den folgenden Code: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># import libraries import cv2 import face_recognition # Get a reference to webcam video_capture = cv2.VideoCapture("/dev/video1") # Initialize variables face_locations = [] while True: # Grab a single frame of video ret, frame = video_capture.read() # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses) rgb_frame = frame[:, :, ::-1] # Find all the faces in the current frame of video face_locations = face_recognition.face_locations(rgb_frame) # Display the results for top, right, bottom, left in face_locations:  # Draw a box around the face  cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2) # Display the resulting image cv2.imshow('Video', frame) # Hit 'q' on the keyboard to quit! if cv2.waitKey(1) &amp; 0xFF == ord('q'):  break # Release handle to the webcam video_capture.release() cv2.destroyAllWindows()</span></span></code> </pre> <br>  F√ºhren Sie dann diese Python-Datei aus, indem Sie Folgendes eingeben: <br><br><pre> <code class="python hljs">python face_detector.py</code> </pre> <br>  Wenn alles richtig funktioniert, wird ein neues Fenster ge√∂ffnet, in dem der Gesichtserkennungsmodus in Echtzeit gestartet wird. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/eh14NomINOs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Um zusammenzufassen und zu erkl√§ren, was unser Code getan hat: <br><br><ol><li>  Zuerst haben wir <b>die Hardware angegeben,</b> auf der das Video analysiert werden soll. <br></li><li>  Dann haben wir Bild f√ºr Bild eine Echtzeit- <b>Videoaufnahme gemacht</b> . <br></li><li>  Dann wurde <b>jedes Bild verarbeitet</b> und die <b>Position aller Gesichter</b> im Bild <b>extrahiert</b> . <br></li><li>  Infolgedessen wurden <b>diese Bilder in Form eines Videos</b> zusammen mit einer Angabe, wo sich die Gesichter befinden, <b>reproduziert</b> . <br></li></ol><br><h3>  Anwendungsbeispiel f√ºr die Gesichtserkennung </h3><br>  Dies ist nicht alles, was Spa√ü macht.  Wir werden noch eine coole Sache machen: Wir werden ein vollwertiges Anwendungsbeispiel erstellen, das auf dem obigen Code basiert.  Wir werden kleine √Ñnderungen am Code vornehmen und alles wird fertig sein. <br><br>  Angenommen, Sie m√∂chten mit einem Camcorder ein automatisiertes System erstellen, um zu verfolgen, wo sich der Lautsprecher gerade befindet.  Abh√§ngig von seiner Position dreht das System die Kamera so, dass der Lautsprecher immer in der Mitte des Rahmens bleibt. <br>  Der erste Schritt besteht darin, ein System zu erstellen, das die Person oder Personen im Video identifiziert und sich auf den Standort des Sprechers konzentriert. <br><br><img src="https://habrastorage.org/webt/xg/q-/ub/xgq-ubchs7yktjujlxcoakih7d0.png"><br><br>  Lassen Sie uns herausfinden, wie es geht.  Als Beispiel habe ich ein Video auf YouTube mit einer Rede der Sprecher der DataHack Summit 2017-Konferenz ausgew√§hlt. <br><br>  Importieren Sie zun√§chst die erforderlichen Bibliotheken: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> face_recognition</code> </pre> <br>  Dann lesen wir das Video und stellen die L√§nge ein: <br><br><pre> <code class="python hljs">input_movie = cv2.VideoCapture(<span class="hljs-string"><span class="hljs-string">"sample_video.mp4"</span></span>) length = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))</code> </pre> <br>  Danach erstellen wir eine Ausgabedatei mit der erforderlichen Aufl√∂sung und Bildrate √§hnlich der in der Eingabedatei. <br><br>  Wir laden das Lautsprecherbild als Beispiel, um es im Video zu erkennen: <br><br><pre> <code class="python hljs">image = face_recognition.load_image_file(<span class="hljs-string"><span class="hljs-string">"sample_image.jpeg"</span></span>) face_encoding = face_recognition.face_encodings(image)[<span class="hljs-number"><span class="hljs-number">0</span></span>] known_faces = [ face_encoding, ]</code> </pre> <br>  Nachdem wir fertig sind, starten wir den Zyklus, der sein wird: <br><br><ul><li>  Bild aus dem Video extrahieren. <br></li><li>  Finde alle Gesichter und identifiziere sie. <br></li><li>  Erstellen Sie ein neues Video, das den Originalrahmen mit der Position des Gesichts des Sprechers mit einer Signatur kombiniert. <br></li></ul><br>  Schauen wir uns den Code an, der dies ausf√ºhrt: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize variables face_locations = [] face_encodings = [] face_names = [] frame_number = 0 while True: # Grab a single frame of video ret, frame = input_movie.read() frame_number += 1 # Quit when the input video file ends if not ret:  break # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses) rgb_frame = frame[:, :, ::-1] # Find all the faces and face encodings in the current frame of video face_locations = face_recognition.face_locations(rgb_frame, model="cnn") face_encodings = face_recognition.face_encodings(rgb_frame, face_locations) face_names = [] for face_encoding in face_encodings:  # See if the face is a match for the known face(s)  match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)  name = None  if match[0]:      name = "Phani Srikant"  face_names.append(name) # Label the results for (top, right, bottom, left), name in zip(face_locations, face_names):  if not name:      continue      # Draw a box around the face  cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)  # Draw a label with a name below the face  cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)       font = cv2.FONT_HERSHEY_DUPLEX  cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1) # Write the resulting image to the output video file print("Writing frame {} / {}".format(frame_number, length)) output_movie.write(frame) # All done! input_movie.release() cv2.destroyAllWindows()</span></span></code> </pre> <br>  Der Code gibt Ihnen dieses Ergebnis: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/uOcN6FhX6gY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Von den Redakteuren </h2><br>  Netologiekurse zum Thema: <br><br><ul><li>  Online-Beruf " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Entwickler</a> " </li><li>  Online-Beruf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Data Scientist</a> </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434354/">https://habr.com/ru/post/de434354/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434340/index.html">Wie wir Wi-Fi in der U-Bahn von Delhi gepr√ºft haben und was daraus wurde</a></li>
<li><a href="../de434342/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 22: ‚ÄûInformationssicherheit MIT‚Äú, Teil 1</a></li>
<li><a href="../de434344/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 22: ‚ÄûInformationssicherheit MIT‚Äú, Teil 2</a></li>
<li><a href="../de434346/index.html">MIT-Kurs "Computer Systems Security". Vorlesung 22: ‚ÄûInformationssicherheit MIT‚Äú, Teil 3</a></li>
<li><a href="../de434348/index.html">Erinnerst du dich an dein Passwort bei Habr√©?</a></li>
<li><a href="../de434356/index.html">Python Stiller mit E-Mail</a></li>
<li><a href="../de434358/index.html">Importsubstitution von Betriebssystemen. Wie sehe ich ein inl√§ndisches Betriebssystem?</a></li>
<li><a href="../de434360/index.html">Erkl√§rtes Gespr√§ch √ºber asynchrone Programmierung in Javascript</a></li>
<li><a href="../de434362/index.html">NICHT f√ºr 2019 prognostiziert</a></li>
<li><a href="../de434364/index.html">Hangfire Queue-Unterst√ºtzung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>