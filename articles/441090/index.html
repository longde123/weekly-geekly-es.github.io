<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üôÖüèΩ üòâ ‚ö™Ô∏è AI de audio: extracci√≥n de voces de la m√∫sica usando redes neuronales convolucionales ‚ò†Ô∏è üö∞ ü¶ï</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hackear m√∫sica para democratizar el contenido derivado 

 Descargo de responsabilidad: toda la propiedad intelectual, dise√±os y m√©todos descritos en e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI de audio: extracci√≥n de voces de la m√∫sica usando redes neuronales convolucionales</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441090/">  <i>Hackear m√∫sica para democratizar el contenido derivado</i> <br><br><blockquote>  <b>Descargo de responsabilidad:</b> toda la propiedad intelectual, dise√±os y m√©todos descritos en este art√≠culo se divulgan en los documentos US10014002B2 y US9842609B2. </blockquote><br>  Ojal√° pudiera volver a 1965, llamar a la puerta del estudio de Abby Road con un pase, entrar y escuchar las voces reales de Lennon y McCartney ... Bueno, intent√©moslo.  Entrada: MP3 de calidad media de los Beatles <i>Podemos solucionarlo</i> .  La pista superior es la mezcla de entrada, la pista inferior son las voces aisladas que nuestra red neuronal ha resaltado. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/305275806" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  Formalmente, este problema se conoce como <i>separaci√≥n de fuentes de sonido</i> o <i>separaci√≥n de la se√±al</i> (separaci√≥n de fuente de audio).  Consiste en restaurar o reconstruir una o m√°s de las se√±ales originales, que, como resultado de un proceso <i>lineal o convolucional</i> , se mezclan con otras se√±ales.  Este campo de investigaci√≥n tiene muchas aplicaciones pr√°cticas, incluida la mejora de la calidad del sonido (voz) y la eliminaci√≥n del ruido, las mezclas de m√∫sica, la distribuci√≥n espacial del sonido, la remasterizaci√≥n, etc. Los ingenieros de sonido a veces llaman a esta t√©cnica desmezcla.  Hay muchos recursos sobre este tema, desde la separaci√≥n de se√±ales ciegas con an√°lisis de componentes independientes (ICA) hasta la factorizaci√≥n semi-controlada de matrices no negativas y terminando con enfoques posteriores basados ‚Äã‚Äãen redes neuronales.  Puede encontrar buena informaci√≥n sobre los dos primeros puntos en <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">estas mini-gu√≠as</a> de CCRMA, que en alg√∫n momento me fueron muy √∫tiles. <br><br>  <b>Pero antes de sumergirse en el desarrollo ... bastante filosof√≠a de aprendizaje autom√°tico aplicada ...</b> <br><br>  Estaba involucrado en el procesamiento de se√±ales e im√°genes incluso antes de que el eslogan "el aprendizaje profundo resuelva todo" se haya extendido, por lo que puedo presentarle una soluci√≥n como un viaje de <i>ingenier√≠a de caracter√≠sticas</i> y mostrar <b>por qu√© una red neuronal es el mejor enfoque para este problema en particular</b> .  Por qu√©  Muy a menudo, veo que la gente escribe algo como esto: <br><br>  <i>‚ÄúCon el aprendizaje profundo, ya no tiene que preocuparse por elegir funciones;</i>  <i>lo har√° por ti ".</i> <br><br>  o peor ... <br><br>  <i>"La diferencia entre el aprendizaje autom√°tico y el aprendizaje profundo</i> [oye ... ¬°el aprendizaje profundo sigue siendo el aprendizaje autom√°tico!] Es <i>que en ML t√∫ mismo extraes los atributos, y en el aprendizaje profundo esto ocurre autom√°ticamente dentro de la red".</i> <br><br>  Estas generalizaciones probablemente provienen del hecho de que los DNN pueden ser muy efectivos para explorar buenos espacios ocultos.  Pero entonces es imposible generalizar.  Estoy muy molesto cuando los reci√©n graduados y practicantes sucumben a los conceptos err√≥neos anteriores y adoptan el enfoque de "aprendizaje profundo".  Como, es suficiente arrojar un mont√≥n de datos sin procesar (incluso despu√©s de un peque√±o procesamiento preliminar), y todo funcionar√° como deber√≠a.  En el mundo real, debe ocuparse de cosas como el rendimiento, la ejecuci√≥n en tiempo real, etc. Debido a tales conceptos err√≥neos, estar√° atrapado en el modo de experimento durante mucho tiempo ... <br><br>  <b>Feature Engineering sigue siendo una disciplina muy importante en el dise√±o de redes neuronales artificiales.</b>  <b>Como en cualquier otra t√©cnica de ML, en la mayor√≠a de los casos, es lo que distingue las soluciones efectivas del nivel de producci√≥n de los experimentos fallidos o ineficaces.</b>  <b>Una comprensi√≥n profunda de sus datos y su naturaleza a√∫n significa mucho ...</b> <br><br><h1>  De la A a la Z </h1><br>  Ok, termin√© el serm√≥n.  ¬°Ahora veamos por qu√© estamos aqu√≠!  Al igual que con cualquier problema de procesamiento de datos, primero veamos c√≥mo se ve.  Eche un vistazo a la siguiente pieza de voz de la grabaci√≥n original del estudio. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/305288385" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Voces de estudio 'One Last Time', Ariana Grande</font></i> <br><br>  No es demasiado interesante, ¬øverdad?  Bueno, esto es porque visualizamos la se√±al <i>a tiempo</i> .  Aqu√≠ solo vemos cambios de amplitud con el tiempo.  Pero puede extraer todo tipo de otras cosas, como las envolventes de amplitud (envoltura), los valores cuadrados medios de ra√≠z (RMS), la tasa de cambio de valores positivos de amplitud a negativos (tasa de cruce por cero), etc., pero estos <i>signos son</i> demasiado <i>primitivos</i> y no lo suficientemente distintivos, para ayudar en nuestro problema  Si queremos extraer las voces de una se√±al de audio, primero debemos determinar de alguna manera la estructura del habla humana.  Afortunadamente, la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Transformada de Fourier de</a> Ventana (STFT) viene al rescate. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/305391461" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Espectro de amplitud STFT - tama√±o de ventana = 2048, superposici√≥n = 75%, escala de frecuencia logar√≠tmica [Sonic Visualizer]</font></i> <br><br>  Aunque me encanta el procesamiento del habla y definitivamente me encanta jugar con <i>simulaciones de filtros de entrada, cepstrums, sottotami, LPC, MFCC, etc., omitiremos</i> todas estas tonter√≠as y nos centraremos en los elementos principales relacionados con nuestro problema para que el art√≠culo pueda ser entendido por la mayor cantidad de personas posible, no solo especialistas en procesamiento de se√±ales. <br><br>  Entonces, ¬øqu√© nos dice la estructura del discurso humano? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/541/f8d/c74/541f8dc74fecdb1e994d560b44da112d.png"><br><br>  Bueno, podemos definir tres elementos principales aqu√≠: <br><br><ul><li>  <b>La frecuencia fundamental</b> (f0), que est√° determinada por la frecuencia de vibraci√≥n de nuestras cuerdas vocales.  En este caso, Ariana canta en el rango de 300-500 Hz. <br></li><li>  Una serie de <b>arm√≥nicos por</b> encima de f0 que siguen una forma o patr√≥n similar.  Estos arm√≥nicos aparecen en frecuencias que son m√∫ltiplos de f0. <br></li><li>  Discurso <b>sordo</b> , que incluye consonantes como 't', 'p', 'k', 's' (que no se produce por la vibraci√≥n de las cuerdas vocales), respiraci√≥n, etc. Todo esto se manifiesta en forma de r√°fagas cortas en la regi√≥n de alta frecuencia. </li></ul><br><h1>  Primer intento con reglas </h1><br>  Olvidemos por un segundo lo que se llama aprendizaje autom√°tico.  ¬øSe puede desarrollar un m√©todo de extracci√≥n vocal basado en nuestro conocimiento de la se√±al?  D√©jame intentarlo ... <br><br>  <b><i>Ingenuo</i> aislamiento vocal V1.0:</b> <br><br><ol><li>  Identificar √°reas con voces.  Hay muchas cosas en la se√±al original.  Queremos centrarnos en aquellas √°reas que realmente contienen contenido vocal e ignorar todo lo dem√°s. <br></li><li>  Distinguir entre voz y voz no hablada.  Como hemos visto, son muy diferentes.  Probablemente necesiten ser manejados de manera diferente. <br></li><li>  Evaluar el cambio en la frecuencia fundamental a lo largo del tiempo. <br></li><li>  Basado en el pin 3, aplique alg√∫n tipo de m√°scara para capturar arm√≥nicos. <br></li><li>  Haz algo con fragmentos de discursos sordos ... </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/bb4/82e/ecfbb482ea6c1b29b96a13ce5cf8a5a2.gif"><br><br>  Si trabajamos dignamente, el resultado debe ser una <i>m√°scara</i> <i>suave</i> o de <i>bits</i> , cuya aplicaci√≥n a la amplitud del STFT (multiplicaci√≥n por elementos) da una reconstrucci√≥n aproximada de la amplitud de las voces STFT.  Luego combinamos este STFT vocal con informaci√≥n sobre la fase de la se√±al original, calculamos el STFT inverso y obtenemos la se√±al de tiempo del vocal reconstruido. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/56d/fb7/012/56dfb70125de59f0e1ee03b58e6c79e6.png"><br><br>  Hacerlo desde cero ya es un gran trabajo.  Pero en aras de la demostraci√≥n, la implementaci√≥n del <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">algoritmo pYIN es aplicable</a> .  Aunque est√° destinado a resolver el paso 3, pero con la configuraci√≥n correcta, realiza decentemente los pasos 1 y 2, rastreando la base vocal incluso en presencia de m√∫sica.  El siguiente ejemplo contiene la salida despu√©s de procesar este algoritmo, sin procesar el habla sin voz. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/305636014" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Y que ...?  Parece haber hecho todo el trabajo, pero no hay buena calidad y cierre.  Quiz√°s al gastar m√°s tiempo, energ√≠a y dinero, mejoraremos este m√©todo ... <br><br>  Pero d√©jame preguntarte ... <br><br>  ¬øQu√© sucede si aparecen <b>algunas voces</b> en la pista y, sin embargo, a menudo se encuentra en al menos el 50% de las pistas profesionales modernas? <br><br>  ¬øQu√© sucede si las voces son procesadas por <b>reverberaci√≥n, retrasos</b> y otros efectos?  Echemos un vistazo al √∫ltimo coro de Ariana Grande de esta canci√≥n. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/306589126" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  ¬øYa sientes dolor ...?  Yo soy <br><br>  Tales m√©todos sobre reglas estrictas se convierten r√°pidamente en un castillo de naipes.  El problema es muy complicado.  Demasiadas reglas, demasiadas excepciones y demasiadas condiciones diferentes (efectos y configuraciones de mezcla).  Un enfoque de varios pasos tambi√©n implica que los errores en un paso extienden los problemas al siguiente paso.  Mejorar cada paso ser√° muy costoso: tomar√° una gran cantidad de iteraciones para hacerlo bien.  Y por √∫ltimo, pero no menos importante, es probable que al final obtengamos un transportador muy intensivo en recursos, que en s√≠ mismo puede negar todos los esfuerzos. <br><br>  <b>En tal situaci√≥n, es hora de comenzar a pensar en un enfoque m√°s <i>integral</i> y dejar que ML descubra parte de los procesos y operaciones b√°sicos necesarios para resolver el problema.</b>  <b>Pero a√∫n tenemos que mostrar nuestras habilidades y participar en la ingenier√≠a de caracter√≠sticas, y ver√° por qu√©.</b> <br><br><h1>  Hip√≥tesis: use la red neuronal como una funci√≥n de transferencia que traduce mezclas en voces </h1><br>  Mirando los logros de las redes neuronales convolucionales en el procesamiento de fotos, ¬øpor qu√© no aplicar el mismo enfoque aqu√≠? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f0/356/00a/1f035600aadc5f6bb50d7478984aa1d1.png"><br>  <i><font color="gray">Las redes neuronales resuelven con √©xito problemas tales como la coloraci√≥n de im√°genes, el enfoque y la resoluci√≥n.</font></i> <br><br>  Al final, puedes imaginar la se√±al de sonido "como una imagen" usando la transformada de Fourier a corto plazo, ¬øverdad?  Aunque estas <i>im√°genes de sonido</i> no corresponden a la distribuci√≥n estad√≠stica de im√°genes naturales, todav√≠a tienen patrones espaciales (en el espacio de tiempo y frecuencia) sobre los cuales entrenar la red. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/54c/b43/7fa/54cb437fa908fbe8b4d36bd120e2d009.png"><br>  <i><font color="gray">Izquierda: ritmo de bater√≠a y l√≠nea de base a continuaci√≥n, varios sonidos de sintetizador en el medio, todos mezclados con voces.</font></i>  <i><font color="gray">Derecha: solo voces</font></i> <br><br>  Realizar tal experimento ser√≠a una tarea costosa ya que es dif√≠cil obtener o generar los datos de capacitaci√≥n necesarios.  Pero en la investigaci√≥n aplicada, siempre trato de usar este enfoque: primero, <b>para identificar un problema m√°s simple que confirme los mismos principios</b> , pero que no requiera mucho trabajo.  Esto le permite evaluar la hip√≥tesis, iterar m√°s r√°pido y corregir el modelo con p√©rdidas m√≠nimas si no funciona como deber√≠a. <br><br>  La condici√≥n impl√≠cita es que la <b>red neuronal debe comprender la estructura del habla humana</b> .  Un problema m√°s simple puede ser este: ¬ø <i>puede una red neuronal determinar la presencia del habla en un fragmento arbitrario de una grabaci√≥n de sonido</i> ?  Estamos hablando de un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">detector de actividad de voz (VAD)</a> confiable, implementado en forma de un clasificador binario. <br><br><h3>  Dise√±amos el espacio de los signos. </h3><br>  Sabemos que las se√±ales de sonido, como la m√∫sica y el habla humana, se basan en dependencias del tiempo.  En pocas palabras, nada sucede de forma aislada en un momento dado.  Si quiero saber si hay una voz en una pieza particular de grabaci√≥n de sonido, entonces tengo que mirar las regiones vecinas.  Tal <i>contexto de tiempo</i> proporciona buena informaci√≥n sobre lo que est√° sucediendo en el √°rea de inter√©s.  Al mismo tiempo, es deseable realizar una clasificaci√≥n con incrementos de tiempo muy peque√±os para reconocer una voz humana con la resoluci√≥n de tiempo m√°s alta posible. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/471/d58/2d4/471d582d4d8dad39249584940137d4e3.gif"><br><br>  Vamos a contar un poco ... <br><br><ul><li>  Frecuencia de muestreo (fs): 22050 Hz (disminuimos la muestra de 44100 a 22050) <br></li><li>  Dise√±o STFT: tama√±o de ventana = 1024, tama√±o de salto = 256, interpolaci√≥n de la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">escala de tiza</a> para el filtro de ponderaci√≥n, teniendo en cuenta la percepci√≥n.  Dado que nuestra entrada es <i>real</i> , puede trabajar con la mitad de STFT (una explicaci√≥n est√° m√°s all√° del alcance de este art√≠culo ...) mientras mantiene el componente DC (opcional), que nos da 513 bins de frecuencia. <br></li><li>  Resoluci√≥n de clasificaci√≥n de destino: un cuadro STFT (~ 11,6 ms = 256/22050) <br></li><li>  Contexto de tiempo objetivo: ~ 300 milisegundos = 25 cuadros STFT. <br></li><li>  El n√∫mero objetivo de ejemplos de entrenamiento: 500 mil. <br></li><li>  Suponiendo que usemos una ventana deslizante en incrementos de 1 marco de tiempo STFT para generar datos de entrenamiento, necesitamos aproximadamente 1.6 horas de sonido etiquetado para generar 500 mil muestras de datos </li></ul><br>  Con los requisitos anteriores, la entrada y salida de nuestro clasificador binario son las siguientes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f57/01c/bd9/f5701cbd91fe9ba38f89b541b9d4492e.png"><br><br><h3>  Modelo </h3><br>  Usando Keras, construiremos un peque√±o modelo de una red neuronal para probar nuestra hip√≥tesis. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGD <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU model = Sequential() model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">513</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)) model.add(LeakyReLU()) model.add(MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>))) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)) model.add(Flatten()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">64</span></span>)) model.add(LeakyReLU()) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, decay=<span class="hljs-number"><span class="hljs-number">1e-6</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=keras.losses.binary_crossentropy, optimizer=sgd, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/447/ebe/19f/447ebe19f6ba953b4af0b4bed1d9e7af.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/002/083/7d1002083c78486e36dd92959ec5afbd.png"><br><br>  Al dividir los datos 80/20 en entrenamiento y pruebas despu√©s de ~ 50 √©pocas, obtenemos la <b>precisi√≥n al probar ~ 97%</b> .  Esto es evidencia suficiente de que nuestro modelo es capaz de distinguir entre voces en fragmentos de sonido musical (y fragmentos sin voces).  Si revisamos algunos mapas de caracter√≠sticas de la cuarta capa convolucional, podemos concluir que la red neuronal parece haber optimizado sus n√∫cleos para realizar dos tareas: filtrar m√∫sica y filtrar voces ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/438/bce/536/438bce536c3aa746a3120e2364b512c8.png"><br>  <i><font color="gray">Un ejemplo de un mapa de objetos a la salida de la cuarta capa convolucional.</font></i>  <i><font color="gray">Aparentemente, la salida a la izquierda es el resultado de las operaciones del n√∫cleo en un intento de preservar el contenido vocal mientras se ignora la m√∫sica.</font></i>  <i><font color="gray">Los valores altos se asemejan a la estructura armoniosa del habla humana.</font></i>  <i><font color="gray">El mapa de objetos a la derecha parece ser el resultado de la tarea opuesta.</font></i> <br><br><h1>  Del detector de voz a la se√±al de desconexi√≥n </h1><br>  Habiendo resuelto el problema de clasificaci√≥n m√°s simple, ¬øc√≥mo podemos pasar a la separaci√≥n real de las voces de la m√∫sica?  Bueno, mirando el primer m√©todo <i>ingenuo</i> , todav√≠a queremos obtener de alguna manera un espectrograma de amplitud para las voces.  Ahora esto se est√° convirtiendo en una tarea de regresi√≥n.  Lo que queremos hacer es calcular el espectro de amplitud correspondiente para las voces en este marco de tiempo a partir del STFT de la se√±al original, es decir, de la mezcla (con un contexto de tiempo suficiente). <br><br>  <b>¬øQu√© pasa con el conjunto de datos de entrenamiento?</b>  <b>(puedes preguntarme en este momento)</b> <br><br>  Maldici√≥n ... ¬øpor qu√©?  ¬°Iba a considerar esto al final del art√≠culo para no distraerme del tema! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ac/67f/91d/9ac67f91d85022f6bbc75f296ce3f04a.png"><br><br>  Si nuestro modelo est√° bien entrenado, entonces, para una conclusi√≥n l√≥gica, solo necesita implementar una ventana deslizante simple para la mezcla STFT.  Despu√©s de cada pron√≥stico, mueva la ventana hacia la derecha en 1 per√≠odo de tiempo, prediga el siguiente cuadro con las voces y as√≥cielo con la predicci√≥n previa.  En cuanto al modelo, tomamos el mismo modelo que se utiliz√≥ para el detector de voz y hacemos peque√±os cambios: la forma de la se√±al de salida es ahora (513.1), activaci√≥n lineal en la salida, MSE en funci√≥n de las p√©rdidas.  Ahora comenzamos a entrenar. <br><br>  <b>No te alegres todav√≠a ...</b> <br><br>  Aunque esta representaci√≥n de E / S tiene sentido, despu√©s de entrenar nuestro modelo varias veces, con varios par√°metros y normalizaciones de datos, no hay resultados.  Parece que estamos pidiendo demasiado ... <br><br>  Hemos pasado de un clasificador binario a <i>regresi√≥n</i> en un vector de 513 dimensiones.  Aunque la red est√° estudiando el problema hasta cierto punto, las voces restauradas a√∫n tienen artefactos obvios e interferencia de otras fuentes.  Incluso despu√©s de agregar capas adicionales y aumentar el n√∫mero de par√°metros del modelo, los resultados no cambian mucho.  Y luego surge la pregunta: <b>¬øc√≥mo "simplificar" la tarea para la red mediante el enga√±o, y al mismo tiempo lograr los resultados deseados?</b> <br><br>  ¬øQu√© sucede si, en lugar de estimar la amplitud de las voces STFT, entrenamos a la red para obtener una m√°scara binaria, que cuando se aplica a la mezcla STFT nos da un espectrograma de amplitud de las voces simplificado, pero <b>perceptualmente aceptable</b> ? <br><br>  Experimentando con varias heur√≠sticas, se nos ocurri√≥ un m√©todo muy simple (y ciertamente poco ortodoxo en t√©rminos de procesamiento de se√±ales ...) para extraer voces de mezclas utilizando m√°scaras binarias.  Sin entrar en detalles, la esencia es la siguiente.  Imagine la salida como una imagen binaria, donde el valor '1' indica la <b>presencia predominante de contenido vocal</b> en una frecuencia y marco de tiempo dados, y el valor '0' indica la presencia predominante de m√∫sica en una ubicaci√≥n dada.  Podemos llamarlo la <i>binarizaci√≥n de la percepci√≥n</i> , solo para encontrar un nombre.  Visualmente, se ve bastante feo, para ser sincero, pero los resultados son sorprendentemente buenos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16f/857/721/16f85772187f89a73baf7fe0158aba2c.png"><br><br>  Ahora nuestro problema se convierte en una especie de regresi√≥n-clasificaci√≥n h√≠brida (m√°s o menos ...).  Le pedimos al modelo que "clasifique p√≠xeles" en la salida como vocal o no vocal, aunque conceptualmente (as√≠ como desde el punto de vista de la funci√≥n de p√©rdida MSE utilizada) la tarea sigue siendo regresiva. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e21/6a0/654/e216a065488c058c37e2758563ff4052.png"><br><br>  Aunque esta distinci√≥n puede parecer inapropiada para algunos, de hecho es de gran importancia en la capacidad del modelo para estudiar la tarea, la segunda de las cuales es m√°s simple y m√°s limitada.  Al mismo tiempo, esto nos permite mantener nuestro modelo relativamente peque√±o en t√©rminos de la cantidad de par√°metros, dada la complejidad de la tarea, algo muy deseable para trabajar en tiempo real, que en este caso era un requisito de dise√±o.  Despu√©s de algunos ajustes menores, el modelo final se ve as√≠. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/416/d25/856/416d2585671e97a1f39c9584a30d4bbf.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/21e/04e/4d5/21e04e4d5642a3282aa445846c64c576.png"><br><br><h3>  ¬øC√≥mo recuperar una se√±al de dominio de tiempo? </h3><br>  De hecho, como en el <i>m√©todo ingenuo</i> .  En este caso, para cada pase, predecimos un marco de tiempo de la m√°scara de voz binaria.  Nuevamente, al darnos cuenta de una ventana deslizante simple con un paso de un marco de tiempo, continuamos evaluando y combinando marcos de tiempo sucesivos, que finalmente forman la m√°scara binaria vocal completa. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/2a5/ae1/a342a5ae1b0ca37825978f7b92d574cb.gif"><br><br><h3>  Crea un conjunto de entrenamiento </h3><br>  Como saben, uno de los principales problemas cuando se ense√±a con un maestro (deje estos ejemplos de juguetes con conjuntos de datos listos para usar) es la informaci√≥n correcta (en cantidad y calidad) para el problema espec√≠fico que est√° tratando de resolver.  Seg√∫n las representaciones de entrada y salida descritas, para entrenar nuestro modelo, primero necesitar√° un n√∫mero significativo de mezclas y sus correspondientes pistas vocales perfectamente alineadas y normalizadas.  Este conjunto se puede crear de varias maneras, y utilizamos una combinaci√≥n de estrategias, desde la creaci√≥n manual de pares [mezclar &lt;-&gt; voces] basadas en varias capillas encontradas en Internet, hasta la b√∫squeda de material de m√∫sica de banda de rock y √°lbumes de recortes de Youtube.  Solo para darle una idea de lo laborioso y doloroso que es este proceso, parte del proyecto fue el desarrollo de una herramienta para crear pares autom√°ticamente [mezclar voces &lt;-&gt;]: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/617/b71/5ac/617b715acc8d8c913752054f84214c8b.png"><br><br>  Se necesita una gran cantidad de datos para que la red neuronal aprenda la funci√≥n de transferencia para transmitir mezclas a voces.  Nuestro conjunto final consisti√≥ en aproximadamente 15 millones de muestras de mezclas de 300 ms y sus correspondientes m√°scaras binarias vocales. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/21f/01a/02d/21f01a02d22dfc4f2d615e511cf470a6.png"><br><br><h3>  Arquitectura de tuber√≠a </h3><br>  Como probablemente sepa, crear un modelo de ML para una tarea espec√≠fica es solo la mitad de la batalla.  En el mundo real, debe pensar en la arquitectura del software, especialmente si necesita trabajar en tiempo real o cerca de ella. <br><br>  En esta implementaci√≥n particular, la reconstrucci√≥n en el dominio del tiempo puede ocurrir inmediatamente despu√©s de predecir la m√°scara de voz binaria completa (modo independiente) o, lo que es m√°s interesante, en modo de subprocesos m√∫ltiples, donde recibimos y procesamos datos, restauramos voces y reproducimos sonido, todo en segmentos peque√±os, cerca de transmisi√≥n e incluso casi en tiempo real, procesando m√∫sica que se graba sobre la marcha con un retraso m√≠nimo.  En realidad, este es un tema separado, y lo dejar√© para otro art√≠culo <b>sobre tuber√≠as de ML en tiempo real</b> ... <br><br><h1>  Supongo que dije lo suficiente, ¬øpor qu√© no escuchar un par de ejemplos? </h1><br><h3>  Daft Punk - Get Lucky (grabaci√≥n de estudio) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/315172280" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Aqu√≠ puedes escuchar algunas interferencias m√≠nimas de la bater√≠a ...</font></i> <br><br><h3>  Adele: prende fuego a la lluvia (¬°grabaci√≥n en vivo!) </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/315172388" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><font color="gray">Observe c√≥mo al principio nuestro modelo extrae los gritos de la multitud como contenido vocal :).</font></i>  <i><font color="gray">En este caso, hay alguna interferencia de otras fuentes.</font></i>  <i><font color="gray">Como se trata de una grabaci√≥n en vivo, parece aceptable que las voces extra√≠das sean de peor calidad que las anteriores.</font></i> <br><br><h1>  S√≠, y "algo m√°s" ... </h1><br><h1>  Si el sistema funciona para voces, ¬øpor qu√© no aplicarlo a otros instrumentos ...? </h1><br>  El art√≠culo ya es bastante extenso, pero dado el trabajo realizado, mereces escuchar la √∫ltima demostraci√≥n.  Con la misma l√≥gica que cuando extraemos voces, podemos tratar de dividir la m√∫sica est√©reo en componentes (bater√≠a, bajos, voces, otros), haciendo algunos cambios en nuestro modelo y, por supuesto, teniendo el conjunto de entrenamiento apropiado :). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=https://player.vimeo.com/video/315173879" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Gracias por leer  Como nota final: como puede ver, el modelo real de nuestra red neuronal convolucional no es tan especial.  El √©xito de este trabajo fue determinado por <b>Feature Engineering</b> y el claro proceso de prueba de hip√≥tesis, sobre el que escribir√© en futuros art√≠culos. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/441090/">https://habr.com/ru/post/441090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441072/index.html">Data Science en CodeFest: El sabor de la revoluci√≥n</a></li>
<li><a href="../441076/index.html">Optimizaci√≥n de scripts con Webpack SplitChunksPlugin</a></li>
<li><a href="../441078/index.html">LG presentar√° un tel√©fono inteligente con una pantalla de altavoz OLED: algunas palabras sobre el nuevo dispositivo y tecnolog√≠a</a></li>
<li><a href="../441082/index.html">Los boletos para Marte costar√°n menos de $ 500,000</a></li>
<li><a href="../441084/index.html">¬øA d√≥nde fueron los primeros adoptadores?</a></li>
<li><a href="../441092/index.html">Embedded World 2019: la mayor exposici√≥n de electr√≥nica integrada</a></li>
<li><a href="../441096/index.html">Simulador de lectura de art√≠culos</a></li>
<li><a href="../441098/index.html">Profundidades SIEM: correlaciones listas para usar. Parte 4. Modelo del sistema como contexto de reglas de correlaci√≥n</a></li>
<li><a href="../441102/index.html">Kaspersky Mobile Talks: una reuni√≥n para desarrolladores avanzados</a></li>
<li><a href="../441104/index.html">Obtenci√≥n de informaci√≥n y omisi√≥n de la autenticaci√≥n de dos factores en tarjetas bancarias de TOP-10 (Ucrania)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>