<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üåí üçì üë®üèº‚Äçüöí In-Memory-Datenbanken: Anwendung, Skalierung und wichtige Erg√§nzungen üï≥Ô∏è üë¶üèø üöë</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Wir experimentieren weiter mit den Formaten der Mitaps. K√ºrzlich kollidierten wir in einem Boxring mit einem zentralen Datenbus und Service Mesh. Dies...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>In-Memory-Datenbanken: Anwendung, Skalierung und wichtige Erg√§nzungen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/psb/blog/434730/">  Wir experimentieren weiter mit den Formaten der Mitaps.  K√ºrzlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kollidierten</a> wir in einem Boxring <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">mit einem</a> zentralen Datenbus und Service Mesh.  Diesmal haben wir uns entschlossen, etwas friedlicheres auszuprobieren - StandUp, also ein offenes Mikrofon.  Das Thema wurde als In-Memory-Datenbank ausgew√§hlt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ffd/790/486/ffd790486ff5792ffd1bbebb3cc69be0.png"><br><br>  In welchen F√§llen sollte ich auf In-Memory umschalten?  Wie und warum skalieren?  Und worauf lohnt es sich zu achten?  Die Antworten finden Sie in den Reden der Redner, die wir in diesem Beitrag behandeln werden. <br><a name="habracut"></a><br>  Aber stellen Sie sich zuerst die Lautsprecher vor: <br><br><ul><li>  Andrey Trushkin, Leiter des Zentrums f√ºr Innovation und fortschrittliche Technologien der Promsvyazbank <br></li><li>  Vladislav Shpileva, Tarantool-Entwickler <br></li><li>  Artyom Shitov, GridGain-L√∂sungsarchitekt <br></li></ul><br><h2>  Wechseln Sie in den Arbeitsspeicher </h2><br>  Aktuelle Trends auf dem Finanzmarkt stellen viel strengere Anforderungen an die Reaktionszeit und den Betrieb der Prozessautomatisierung im Allgemeinen.  Dar√ºber hinaus versuchen heute fast alle gro√üen Finanzinstitute, ihre eigenen √ñkosysteme aufzubauen. <br><br>  In dieser Hinsicht sehen wir uns zwei Hauptanwendungen von In-Memory-L√∂sungen.  Das erste ist das Zwischenspeichern von Integrationsdaten.  Nach dem klassischen Szenario gibt es in gro√üen Unternehmen mehrere automatisierte Systeme, die auf Wunsch des Benutzers Daten bereitstellen.  Oder ein externes System - aber in diesem Fall ist der Initiator in den meisten F√§llen der Benutzer.  Traditionell speicherten diese Systeme Daten, die auf bestimmte Weise strukturiert waren, in der Datenbank und griffen bei Bedarf darauf zu. <br><br>  Solche Systeme erf√ºllen heute nicht mehr die Anforderungen an die Belastung.  Hier sollten wir die Fernaufrufe dieser Systeme durch Verbrauchersysteme nicht vergessen.  Dies impliziert die Notwendigkeit, die Ans√§tze f√ºr die Speicherung und Pr√§sentation von Daten zu √ºberarbeiten - f√ºr Benutzer, automatisierte Systeme oder einzelne Dienste.  Logische Ausgabe - Speicherung relevanter Daten, die von Diensten auf der Ebene der In-Memory-Schicht verwendet werden;  Es gibt viele √§hnliche erfolgreiche F√§lle auf dem Markt. <br><br>  Dies war der erste Fall.  Die zweite ist aus technischer Sicht das Gesch√§ftsprozessmanagement.  Herk√∂mmliche BPM-Systeme automatisieren die Ausf√ºhrung bestimmter Vorg√§nge gem√§√ü einem vordefinierten Algorithmus.  In vielen F√§llen stellen sich Fragen: Warum sind diese Systeme nicht effizient und schnell genug? <br><br>  In der Regel schreiben solche Systeme jeden Schritt (oder einen kleinen Satz von Schritten, die als Gesch√§ftstransaktion konzipiert sind) in die Datenbank.  Sie sind also an die Reaktionszeit und die Interaktion mit diesen Systemen gebunden.  Die Anzahl der Gesch√§ftsprozessinstanzen, die gleichzeitig in Echtzeit ausgef√ºhrt werden, liegt vor mehr als 10 Jahren um Gr√∂√üenordnungen.  Moderne Gesch√§ftsprozessmanagementsysteme sollten daher eine deutlich h√∂here Leistung aufweisen und die Ausf√ºhrung dezentraler Anwendungen sicherstellen.  Dar√ºber hinaus streben heute alle Unternehmen die Bildung einer gro√üen Microservice-Umgebung an.  Die Herausforderung besteht darin, dass verschiedene Instanzen von Gesch√§ftsprozessen Betriebsdaten gemeinsam nutzen und effizient nutzen k√∂nnen.  Im Rahmen der Orchestrierung ist es sinnvoll, diese in einer In-Memory-L√∂sung zu speichern. <br><br><h2>  Vers√∂hnungsproblem </h2><br>  Angenommen, wir haben eine gro√üe Anzahl von Knoten und Diensten, es werden eine Reihe von Gesch√§ftsprozessen ausgef√ºhrt, deren Aktionen in Form von Mikrodiensten implementiert werden.  Um die Leistung zu verbessern, beginnt jeder von ihnen, seinen Status in eine lokale Speicherinstanz zu schreiben.  Wir erhalten eine gro√üe Anzahl lokaler Instanzen.  Wie kann Relevanz und Konsistenz f√ºr alle sichergestellt werden? <br><br>  Wir verwenden In-Memory-Bereiche f√ºr Zonen.  Zum Beispiel abh√§ngig von der Gesch√§ftsdom√§ne.  Wenn wir eine Gesch√§ftsdom√§ne ausschneiden, stellen wir fest, dass bestimmte Microservices / Gesch√§ftsprozesse nur im Rahmen der Zone funktionieren, die f√ºr die entsprechende Dom√§ne verantwortlich ist.  Auf diese Weise k√∂nnen wir das Cache-Update und die gesamte In-Memory-L√∂sung beschleunigen. <br><br>  Gleichzeitig arbeitet der f√ºr die Dom√§ne verantwortliche Cache im vollst√§ndigen Replikationsmodus. Die begrenzte Anzahl von Knoten aufgrund der Verteilung auf die Dom√§nen gew√§hrleistet die Geschwindigkeit und Richtigkeit der L√∂sung in diesem Modus.  Zoning und maximale Fragmentierung helfen, die Probleme der Synchronisation, des Clusterbetriebs usw. zu l√∂sen.  auf einer gro√üen Gesamtzahl von Knoten. <br><br>  Nat√ºrlich stellen sich h√§ufig Fragen zur Zuverl√§ssigkeit von In-Memory-L√∂sungen.  Ja, da kann nicht alles hingelegt werden.  Um die Zuverl√§ssigkeit zu gew√§hrleisten, haben wir immer Datenbanken neben dem In-Memory.  Zum Beispiel f√ºr wichtige Probleme mit der Berichterstellung, die zusammengef√ºhrt werden m√ºssen, was auf einer gro√üen Anzahl von Knoten schwierig sein kann.  Was ist unsere heutige Vision: die <i>Synergie der beiden Ans√§tze</i> . <br><br>  Es ist auch erw√§hnenswert, dass diese beiden Ans√§tze auch nur im Gegensatz nicht ganz richtig sind.  Und gleichzeitig konzentrieren Sie sich auf sie.  Hersteller und Anbieter fortschrittlicher containerisierter Virtualisierungssysteme wie Kubernetes bieten uns bereits Optionen f√ºr eine langfristig zuverl√§ssige Speicherung.  Es sind bereits gute industrielle F√§lle f√ºr die Implementierung von L√∂sungen aufgetreten, in denen die Speicherung in einem solchen virtualisierten Format erfolgt. <br><br>  Eine der gr√∂√üten US-Zeitungen bietet ihren Lesern die M√∂glichkeit, jede Ausgabe online zu erhalten, die seit Beginn der Ver√∂ffentlichung dieser Zeitung im 19. Jahrhundert ver√∂ffentlicht wurde.  Wir k√∂nnen uns das Belastungsniveau vorstellen.  Der Speicher wird von ihnen √ºber die Apache Kafka-Plattform implementiert, die f√ºr Kubernetes bereitgestellt wird.  Hier ist eine weitere Option zum Speichern von Informationen und zum Bereitstellen des Zugriffs unter einer gro√üen Last f√ºr eine gro√üe Anzahl von Kunden.  Bei der Entwicklung neuer L√∂sungen sollte diese Option ebenfalls beachtet werden. <br><br><h2>  Skalieren von In-Memory-Datenbanken mit Tarantool </h2><br>  Angenommen, wir haben einen Server.  Es akzeptiert Anfragen, speichert Daten.  Pl√∂tzlich gibt es mehr Anfragen und Daten, der Server h√∂rt auf, mit der Last fertig zu werden.  Sie k√∂nnen mehr Hardware auf den Server hochladen, der weitere Anforderungen akzeptiert.  Dies ist jedoch aus drei Gr√ºnden gleichzeitig eine Sackgasse: hohe Kosten, begrenzte technische F√§higkeiten und Probleme mit der Fehlertoleranz.  Stattdessen erfolgt eine horizontale Skalierung: ‚ÄûFreunde‚Äú kommen zum Server, um Aufgaben zu erledigen.  Die beiden Haupttypen der horizontalen Skalierung sind Replikation und Sharding. <br><br>  Bei der Replikation werden viele Server gespeichert, die alle dieselben Daten speichern, und Clientanforderungen werden auf alle diese Server verteilt.  So skalieren Computer und nicht Daten.  Dies funktioniert, wenn Daten auf einem Knoten platziert werden, es jedoch so viele Clientanforderungen gibt, dass ein Server diese nicht verarbeiten kann.  Auch hier wird die Fehlertoleranz stark verbessert. <br><br>  Sharding wird zum Skalieren von Daten verwendet: Viele Server werden erstellt und speichern unterschiedliche Daten.  Sie skalieren also sowohl Berechnungen als auch Daten.  Die Fehlertoleranz ist in diesem Fall jedoch gering.  Wenn ein Server ausf√§llt, geht ein Teil der Daten verloren. <br><br>  Es gibt einen dritten Ansatz - sie zu kombinieren.  Wir unterteilen den Cluster in Subcluster und nennen sie Replikats√§tze.  Jeder von ihnen speichert dieselben Daten, und Daten √ºberschneiden sich nicht zwischen Replikats√§tzen.  Das Ergebnis ist die Skalierung von Daten sowie die Berechnung und Fehlertoleranz. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/afe/63c/657/afe63c657f8ffb83527f4a5b298425a1.png"><br><br><h2>  Replikation </h2><br>  Es gibt zwei Arten der Replikation: asynchron und synchron.  Asynchron ist, wenn Clientanforderungen nicht warten, bis die Daten √ºber die Replikate verteilt sind: Das Schreiben in ein Replikat ist ausreichend.  Sobald die Daten auf die Festplatte im Protokoll gelangt sind, ist die Transaktion erfolgreich und eines Tages im Hintergrund werden diese Daten repliziert.  Synchron - wenn eine Transaktion in zwei Phasen unterteilt ist: Vorbereiten und Festschreiben.  Commit gibt keinen Erfolg zur√ºck, bis die Daten auf ein Quorum von Replikaten repliziert wurden. <br><br>  Die asynchrone Replikation ist offensichtlich schneller, da im Netzwerk nichts ruht.  Die Daten werden im Hintergrund an das Netzwerk gesendet, und die im Protokoll aufgezeichnete Transaktion selbst ist abgeschlossen.  Es gibt jedoch ein Problem: Replikate k√∂nnen hintereinander zur√ºckbleiben und nicht synchron angezeigt werden. <br>  Die synchrone Replikation ist zuverl√§ssiger, aber viel langsamer und schwieriger zu implementieren.  Es gibt komplexe Protokolle.  In Tarantool k√∂nnen Sie je nach Aufgabe einen dieser Replikationstypen ausw√§hlen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32f/ca3/bd2/32fca3bd2ad0289626e48a5efc6a8e8d.png"><br><br>  Die Verz√∂gerung von Replikaten f√ºhrt nicht nur zur Desynchronisation, sondern auch zum Unwissenheitsproblem des Masters: Er wei√ü nicht, wie er seine √Ñnderungen an das Replikat weitergeben soll.  √Ñnderungen werden normalerweise schrittweise angegeben - sie werden angewendet und fliegen in derselben Form zum Replikat.  Aber was tun mit ihnen, wenn das Replikat nicht verf√ºgbar ist?  Zum Beispiel kann alles in Tarantool konfiguriert werden, und der Assistent wird sehr flexibel. <br><br>  Eine weitere Herausforderung: Wie kann die Topologie komplex gemacht werden?  Mail.ru hat beispielsweise eine Topologie mit Hunderten von Tarantool.  Es verf√ºgt √ºber einen Tarantool-Kernel, an den Replikat-Taranteln f√ºr Sicherungen in einem Kreis gebunden sind.  In Tarantool k√∂nnen Sie v√∂llig beliebige Topologien erstellen, mit denen die Replikation perfekt funktioniert. <br><br><h2>  Scherben </h2><br>  Fahren wir nun mit der Datenskalierung fort: Sharding.  Es gibt zwei Arten: Bereiche und Hashes.  Range Sharding ist, wenn alle Daten nach einem Sharding-Schl√ºssel sortiert sind und diese gro√üe Sequenz in Bereiche unterteilt ist, sodass jeder Bereich ungef√§hr die gleiche Datenmenge enth√§lt.  Und jeder Bereich wird vollst√§ndig auf einem beliebigen physischen Knoten gespeichert.  Aber normalerweise ist eine solche Scherbe nicht erforderlich.  Dar√ºber hinaus ist es immer sehr kompliziert. <br><br>  Es gibt auch Scherben mit Hashes.  Es wird nur in Tarantool vorgestellt.  Es ist viel einfacher zu implementieren, zu verwenden und fast immer geeignet, anstatt Bereiche zu teilen.  Das funktioniert folgenderma√üen: Wir betrachten die Hash-Funktion aus dem Datensatz und geben die Nummer des physischen Knotens zur√ºck, auf dem gespeichert werden soll.  Es gibt Probleme: Erstens ist es schwierig, eine komplexe Abfrage schnell abzuschlie√üen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dcc/eb9/f36/dcceb9f36f82f045eb0c461f0f934459.png"><br><br>  Zweitens gibt es das Problem des Resharding.  Es gibt eine Art Shard-Funktion, die die Nummer des physischen Shards zur√ºckgibt, in dem der Schl√ºssel gespeichert werden muss.  Und wenn sich die Anzahl der Knoten √§ndert, √§ndert sich auch die Shard-Funktion.  Dies bedeutet, dass alle Daten im Cluster neu berechnet und √ºberpr√ºft werden m√ºssen.  Dar√ºber hinaus werden beim klassischen Sharding einige Daten nicht auf einen neuen Knoten √ºbertragen, sondern einfach zwischen den alten Knoten gemischt.  Nutzlose √úbertragungen k√∂nnen beim klassischen Sharding nicht auf Null reduziert werden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/479/b1f/4e4/479b1f4e48b552723c900d49a4104412.png"><br><br>  Tarantool verwendet virtuelles Sharding: Daten werden nicht auf physischen, sondern auf virtuellen Knoten verteilt.  Virtueller Bucket in einem virtuellen Cluster.  Und virtuelle Geschichten sind auf physischen Geschichten aufgebaut.  Und schon dort ist garantiert, dass jedes virtuelle Stockwerk vollst√§ndig auf einem physischen Stockwerk liegt. <br><br>  Wie l√∂st dies das Problem des Weiterverkaufs?  Tatsache ist, dass die Anzahl der Buckets festgelegt ist und die Anzahl der physischen Knoten erheblich √ºberschreitet.  Unabh√§ngig davon, wie stark Sie Ihren Cluster physisch skalieren, reicht der Bucket immer aus, um Daten zu speichern und gleichm√§√üig zu verteilen.  Und da die Shard-Funktion unver√§ndert bleibt, m√ºssen Sie sie nicht neu berechnen, wenn sich die Zusammensetzung des Clusters √§ndert. <br><br>  Als Ergebnis erhalten wir <i>drei Arten von Sharding: Bereiche, Hashes und virtuelle Buckets</i> .  Bei Bereichen und Buckets liegt ein physikalisches Suchproblem vor. <br><br>  Wie kann man es l√∂sen?  Der erste Weg: Verbieten Sie einfach das Resharding.  Dann m√ºssen Sie zum Resharding einen neuen Cluster erstellen und alles dorthin √ºbertragen.  Der zweite Weg: Gehen Sie immer zu allen Knoten.  Dies macht jedoch keinen Sinn, da Sie skalieren m√ºssen und die Berechnungen nicht so skalieren.  Dritte Option: ein Proxy-Modul, das als eine Art Router f√ºr Buckets dient.  Sie starten es, senden dort eine Anfrage, die die Nummer des Buckets angibt, und es sendet Ihre Anfrage als Proxy an den gew√ºnschten physischen Knoten. <br><br><h2>  Erweiterter In-Memory mit dem Beispiel der GridGain-Plattform </h2><br>  Das Unternehmen hat zus√§tzliche Datenbankanforderungen.  Er m√∂chte, dass all dies fehlertolerant und katastrophal ist.  Er will hohe Verf√ºgbarkeit: damit nie etwas verloren geht, damit Sie sich schnell erholen k√∂nnen.  Ebenfalls erforderlich sind einfache und kosteng√ºnstige Skalierbarkeit, unkomplizierter Support, Vertrauen in die Plattform und effiziente Zugriffsmechanismen. <br><br>  Alle diese Ideen sind nicht neu.  Viele dieser Dinge sind bis zu einem gewissen Grad in klassischen DBMS implementiert, insbesondere die Replikation zwischen Rechenzentren. <br><br>  In-Memory ist keine Startup-Technologie mehr, sondern ausgereifte Produkte, die von den gr√∂√üten Unternehmen der Welt (Barclays, Citi Group, Microsoft usw.) eingesetzt werden.  Es wird davon ausgegangen, dass dort alle diese Anforderungen erf√ºllt sind. <br><br>  Wenn also pl√∂tzlich eine Katastrophe eintritt, sollte die M√∂glichkeit bestehen, sich von der Sicherung zu erholen.  Wenn es sich um eine Finanzorganisation handelt, ist es wichtig, dass diese Sicherung konsistent ist und nicht nur eine Kopie aller Laufwerke.  Damit es keine Situation gibt, in der auf einigen Teilen der Knoten die Daten zum Zeitpunkt X und auf dem anderen Teil zum Zeitpunkt Y wiederhergestellt wurden. Es ist sehr wichtig, eine Wiederherstellung zu einem bestimmten Zeitpunkt durchzuf√ºhren, damit selbst in einer Situation der Datenbesch√§digung oder eines besonders schweren Unfalls der Verlust minimiert wird. <br><br>  Es ist wichtig, Daten auf die Festplatte √ºbertragen zu k√∂nnen.  Damit der Cluster nicht √ºberlastet wird und noch langsamer arbeitet.  Und schnell von der Festplatte aufzusteigen und dann schon die Daten in den Speicher zu pumpen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/042/edc/328/042edc32872f7b8687bf5677818c712e.png"><br>  <i>In-Memory-Reaktion auf Abst√ºrze mit und ohne GridGain-Fehlertoleranzkomponenten</i> <br><br>  Ein Failovercluster sollte sich leicht horizontal und vertikal skalieren lassen.  Ich m√∂chte nicht f√ºr meinen Server bezahlen und beobachten, wie die H√§lfte der Ressourcen leer ist.  Ich m√∂chte nicht die H√∂lle aus Hunderten von Prozessen heraus haben, die verwaltet werden m√ºssen.  Ich m√∂chte ein einfaches System aus Sicht der Unterst√ºtzung mit einer einfachen Eingabe / Ausgabe von Knoten aus dem Cluster und einem entwickelten, ausgereiften √úberwachungssystem. <br><br>  Betrachten Sie MongoDB in dieser Perspektive.  Jeder, der mit MongoDB gearbeitet hat, kennt eine Vielzahl von Prozessen.  Wenn wir eine schattierte MongoDB von 5 Shards haben, hat jeder Shard einen Replikatsatz von drei Prozessen (mit einem Redundanzverh√§ltnis von 3).  Und das sind 15 Prozesse nur f√ºr die Daten selbst.  Der Cluster-Konfigurationsspeicher ist ein weiterer Plus-3-Prozess, insgesamt werden es 18, und dies schlie√üt keine Router ein.  Wenn Sie 20 Shards m√∂chten, begr√º√üen Sie die H√∂lle aus 63+ (zum Beispiel weiteren 8, insgesamt 71) Prozessen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/015/389/b64/015389b641f5babfe44815fd85f258cc.png"><br><br>  Vergleiche mit Cassandra.  Wir nehmen alle die gleichen 5 Shards - dies sind 5 Prozesse und 5 Knoten mit dem gleichen Redundanzverh√§ltnis von 3, was in Bezug auf die Steuerung viel einfacher ist.  Ich m√∂chte 20 Shards - das sind 20 Prozesse.  Ich kann meinen Cluster auf eine beliebige Anzahl von Knoten skalieren, nicht unbedingt auf ein Vielfaches von 3 (oder auf einen anderen Wert des Redundanzkoeffizienten).  Viel einfacher und billiger zu implementieren und zu warten als Replikats√§tze. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bcf/70d/4a4/bcf70d4a4883f9fdc951c871b99c57be.png"><br><br>  Dar√ºber hinaus m√ºssen Sie dem System vertrauen, um zu verstehen, welche Personen hinter jedem einzelnen Produkt stehen.  Idealerweise sollte die Lizenz Open Source oder Open Core sein.  Damit im Falle des Todes des Verk√§ufers etwas getan werden kann.  Es ist auch gut, wenn der Quellcode von einer unabh√§ngigen Community verwaltet wird - wir alle erinnern uns, wie MongoDB und Redis auf Anfrage der Verwaltungsgesellschaft die Lizenzen ge√§ndert haben.  Wie Aerospike Anfang des Jahres Beschr√§nkungen f√ºr die Community-Edition ‚ÄûOpen Source‚Äú einf√ºhrte. <br><br>  Ben√∂tigen Sie einen effektiven Zugriff auf Daten.  Fast alle haben eine strukturierte Abfragesprache in der einen oder anderen Form.  Meistens verwenden sie SQL, es ist notwendig, dass die Anpassung mit dieser Sprache so einfach wie m√∂glich ist.  Dies hilft bei der Ausf√ºhrung verteilter Abfragen, wenn Sie nicht eine Anforderung separat an jeden Knoten senden m√ºssen, sondern mit dem Cluster wie mit einem ‚Äûeinzelnen Fenster‚Äú kommunizieren k√∂nnen.  Ohne aus Sicht der API zu denken, handelt es sich um eine Reihe von Knoten (denken Sie daran, wie schwierig es ist, mit Memcache auf gro√üen Volumes zu arbeiten, selbst auf der einfachsten Put / Get-Ebene, ohne potenziell komplexe SQL-Abfragen), verteilte DDL- und ACID-Garantien. <br><br>  Und schlie√ülich Unterst√ºtzung.  Wenn etwas pl√∂tzlich nicht mehr funktioniert, verliert das Unternehmen einfach Geld.  F√ºr einige Bereiche ist dies nicht kritisch, aber es ist oft wichtig, dass jemand die Verantwortung f√ºr das Produkt und seine Arbeit tr√§gt.  Dass es jederzeit m√∂glich war, einen Anspruch geltend zu machen, und es wurde schnell gel√∂st. <br><br>  Mit diesem Beitrag beenden wir das Jahr der Promsvyazbank auf Habr√©.  In einem kurzen Video haben wir Neujahrsw√ºnsche f√ºr die Bewohner von Chabrowsk gesammelt: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yqp6V3Wqniw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de434730/">https://habr.com/ru/post/de434730/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de434720/index.html">Kehrseite des Nullwissens: Eine Hintert√ºr in zk-SNARK, die nicht erkannt werden kann</a></li>
<li><a href="../de434722/index.html">Schmerzen, Pillen und zwei Krankenwagen oder wie wir alle in Sotschi auf den f√ºnften Platz IronStar 226 geklettert sind</a></li>
<li><a href="../de434724/index.html">Chinesische Bauern machen Live-Streaming</a></li>
<li><a href="../de434726/index.html">Fallstricke bei der Identifizierung eines Android-Ger√§ts</a></li>
<li><a href="../de434728/index.html">Menschen und Prozesse: Warum ist udalenka nicht f√ºr jedes Unternehmen geeignet?</a></li>
<li><a href="../de434732/index.html">Leben bei 6200 DPI. HyperX Pulsefire Core Review</a></li>
<li><a href="../de434734/index.html">Fourier-Transformation. Das schnelle und das w√ºtende</a></li>
<li><a href="../de434736/index.html">Verwenden der Mikrotik-Protokolldatenbank zur Unterdr√ºckung von Brute Force</a></li>
<li><a href="../de434738/index.html">Verst√§rkungslernen in Python</a></li>
<li><a href="../de434740/index.html">Das neuronale Netz lehrte, Sonnenkollektoren in Satellitenbildern zu erkennen und das Ausma√ü ihrer Verteilung vorherzusagen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>