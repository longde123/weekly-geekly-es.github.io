<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñïüèæ ‚ò†Ô∏è üßê An√°lise de incidentes em 21 de outubro no github üî± üöã üôÅ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fatal 43 segundos, o que causou a degrada√ß√£o di√°ria do servi√ßo 

 Ocorreu um incidente no GitHub na semana passada que degradou o servi√ßo por 24 horas...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>An√°lise de incidentes em 21 de outubro no github</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428409/"> <b>Fatal 43 segundos, o que causou a degrada√ß√£o di√°ria do servi√ßo</b> <br><br>  Ocorreu um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">incidente</a> no GitHub na semana passada que degradou o servi√ßo por 24 horas e 11 minutos.  O incidente n√£o afetou toda a plataforma, mas apenas alguns sistemas internos, o que levou √† exibi√ß√£o de informa√ß√µes desatualizadas e inconsistentes.  Por fim, os dados do usu√°rio n√£o foram perdidos, mas a reconcilia√ß√£o manual de v√°rios segundos de grava√ß√£o no banco de dados ainda est√° em andamento.  Na maior parte do acidente, o GitHub tamb√©m n√£o conseguiu lidar com webhooks, criar e publicar p√°ginas do GitHub. <br><br>  Todos n√≥s do GitHub queremos sinceramente pedir desculpas pelos problemas que todos voc√™s encontraram.  Conhecemos sua confian√ßa no GitHub e estamos orgulhosos de criar sistemas sustent√°veis ‚Äã‚Äãque suportam a alta disponibilidade de nossa plataforma.  N√≥s o decepcionamos e lamentamos profundamente.  Embora n√£o possamos desfazer os problemas devido √† degrada√ß√£o da plataforma GitHub por um longo tempo, podemos explicar as raz√µes do que aconteceu, falar sobre as li√ß√µes aprendidas e as medidas que permitir√£o √† empresa se proteger melhor de tais falhas no futuro. <br><a name="habracut"></a><br><h1>  Antecedentes </h1><br>  A maioria dos servi√ßos de usu√°rio do GitHub funciona em nossos pr√≥prios <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">data centers</a> .  A topologia do datacenter foi projetada para fornecer uma rede de fronteira confi√°vel e expans√≠vel na frente de v√°rios datacenters regionais que fornecem o trabalho dos sistemas de computa√ß√£o e armazenamento de dados.  Apesar dos n√≠veis de redund√¢ncia incorporados aos componentes f√≠sicos e l√≥gicos do projeto, ainda √© poss√≠vel que os sites n√£o consigam interagir por algum tempo. <br><br>  Em 21 de outubro, √†s 22h52, UTC, os trabalhos de reparo programados para substituir o equipamento √≥ptico 100G com defeito resultaram em perda de comunica√ß√£o entre o n√≥ da rede na costa leste (costa leste dos EUA) e o principal data center na costa leste.  A conex√£o entre eles foi restaurada ap√≥s 43 segundos, mas essa curta desconex√£o causou uma cadeia de eventos que levou a 24 horas e 11 minutos de degrada√ß√£o do servi√ßo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c1e/fd8/71b/c1efd871b9017afe95d9605703ba7734.png"><br>  <i><font color="gray">A arquitetura de rede de alto n√≠vel do GitHub, incluindo dois data centers f√≠sicos, 3 POPs e armazenamento em nuvem em v√°rias regi√µes, conectados via peering</font></i> <br><br>  No passado, discutimos como usamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MySQL para armazenar metadados do GitHub</a> , bem como nossa abordagem para fornecer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">alta disponibilidade para o MySQL</a> .  O GitHub gerencia v√°rios clusters do MySQL, variando em tamanho, de centenas de gigabytes a quase cinco terabytes.  Cada cluster possui dezenas de r√©plicas de leitura para armazenar outros metadados que n√£o o Git; portanto, nossos aplicativos fornecem solicita√ß√µes de pool, problemas, autentica√ß√£o, processamento em segundo plano e recursos adicionais fora do reposit√≥rio de objetos do Git.  Dados diferentes em diferentes partes do aplicativo s√£o armazenados em diferentes clusters usando segmenta√ß√£o funcional. <br><br>  Para melhorar o desempenho em larga escala, os aplicativos gravam diretamente no servidor prim√°rio apropriado para cada cluster, mas na grande maioria dos casos delegam solicita√ß√µes de leitura a um subconjunto de servidores de r√©plica.  Usamos o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Orchestrator</a> para gerenciar topologias de cluster do MySQL e efetuar failover automaticamente.  Durante esse processo, o Orchestrator leva em considera√ß√£o v√°rias vari√°veis ‚Äã‚Äãe √© montado em cima do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Raft</a> para obter consist√™ncia.  O Orchestrator pode potencialmente implementar topologias que os aplicativos n√£o suportam, portanto, voc√™ precisa garantir que a configura√ß√£o do Orchestrator atenda √†s expectativas no n√≠vel do aplicativo. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/686/ea1/657/686ea165750913fa41b771482266b887.png"><br>  <i><font color="gray">Em uma topologia t√≠pica, todos os aplicativos s√£o lidos localmente com baixa lat√™ncia.</font></i> <br><br><h1>  Cr√¥nica do incidente </h1><br><h4>  10.21.2018, 22:52 UTC </h4><br>  Durante a separa√ß√£o de rede acima mencionada, o Orchestrator no data center principal iniciou o processo de desmarca√ß√£o da lideran√ßa de acordo com o algoritmo de consenso Raft.  O data center da Costa Oeste e os n√≥s de nuvem p√∫blica Orchestrator na Costa Leste conseguiram chegar a um consenso - e come√ßaram a descobrir falhas de cluster para encaminhar registros para o data center ocidental.  O Orchestrator come√ßou a criar uma topologia de cluster de banco de dados no Ocidente.  Ap√≥s a reconex√£o, os aplicativos enviaram imediatamente o tr√°fego de grava√ß√£o para os novos servidores principais no oeste dos EUA. <br><br>  Nos servidores de banco de dados no data center oriental, houve registros por um curto per√≠odo que n√£o foram replicados no data center ocidental.  Como os clusters de banco de dados nos dois datacenters agora continham registros que n√£o estavam no outro datacenter, n√£o conseguimos retornar com seguran√ßa o servidor principal de volta ao datacenter oriental. <br><br><h4>  10.21.2018, 22:54 UTC </h4><br>  Nossos sistemas de monitoramento interno come√ßaram a gerar alertas indicando in√∫meras falhas no sistema.  No momento, v√°rios engenheiros responderam e trabalharam na classifica√ß√£o das notifica√ß√µes recebidas.  √Äs 23:02, os engenheiros do primeiro grupo de resposta determinaram que as topologias para v√°rios clusters de banco de dados estavam em um estado inesperado.  Ao consultar a API do Orchestrator, a topologia de replica√ß√£o do banco de dados foi exibida, contendo apenas servidores do data center ocidental. <br><br><h4>  10.21.2018, 23:07 UTC </h4><br>  Nesse ponto, a equipe de resposta decidiu bloquear manualmente as ferramentas de implanta√ß√£o interna para evitar altera√ß√µes adicionais.  √Äs 23:09, o grupo colocou o site em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">amarelo</a> .  Essa a√ß√£o atribuiu automaticamente √† situa√ß√£o o status de um incidente ativo e enviou um aviso ao coordenador do incidente.  √Äs 23:11, o coordenador entrou no trabalho e dois minutos depois decidiu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mudar o status para vermelho</a> . <br><br><h4>  10.21.2018, 23:13 UTC </h4><br>  Naquele momento, ficou claro que o problema afetava v√°rios clusters de banco de dados.  Desenvolvedores adicionais do grupo de engenharia do banco de dados estiveram envolvidos no trabalho.  Eles come√ßaram a examinar o estado atual para determinar quais a√ß√µes precisavam ser executadas para configurar manualmente o banco de dados da Costa Leste dos EUA como principal para cada cluster e reconstruir a topologia de replica√ß√£o.  Isso n√£o foi f√°cil, porque nesse momento o cluster de banco de dados ocidental estava recebendo registros da camada de aplicativos por quase 40 minutos.  Al√©m disso, no cluster leste, houve v√°rios segundos de registros que n√£o foram replicados para o oeste e n√£o permitiram a replica√ß√£o de novos registros de volta para o leste. <br><br>  Proteger a privacidade e a integridade dos dados do usu√°rio √© a principal prioridade do GitHub.  Portanto, decidimos que mais de 30 minutos de dados gravados no data center ocidental nos deixam com apenas uma solu√ß√£o para a situa√ß√£o, a fim de salvar esses dados: encaminhar para frente (encaminhar para frente).  No entanto, aplicativos no leste, que dependem da grava√ß√£o de informa√ß√µes no cluster ocidental do MySQL, atualmente n√£o conseguem lidar com o atraso adicional devido √† transfer√™ncia da maioria de suas chamadas de banco de dados.  Essa decis√£o levar√° ao fato de que nosso servi√ßo se tornar√° inadequado para muitos usu√°rios.  Acreditamos que a degrada√ß√£o a longo prazo da qualidade do servi√ßo valeu a pena garantir a consist√™ncia dos dados de nossos usu√°rios. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a66/119/b52/a66119b52dbb23111ddfe47bca1194d8.png"><br>  <i><font color="gray">Na topologia incorreta, a replica√ß√£o do oeste para o leste √© violada e os aplicativos n√£o podem ler dados das r√©plicas atuais, porque dependem de baixa lat√™ncia para manter o desempenho da transa√ß√£o</font></i> <br><br><h4>  10.21.2018, 23:19 UTC </h4><br>  As consultas sobre o estado dos clusters de banco de dados mostraram que √© necess√°rio interromper a execu√ß√£o de tarefas que gravam metadados, como solicita√ß√µes push.  Fizemos uma escolha e deliberadamente fizemos uma degrada√ß√£o parcial do servi√ßo, suspendendo webhooks e a montagem das p√°ginas do GitHub, para n√£o comprometer os dados que j√° recebemos dos usu√°rios.  Em outras palavras, a estrat√©gia era priorizar: integridade dos dados em vez da usabilidade do site e recupera√ß√£o r√°pida. <br><br><h4>  22/10/2018, 00:05 UTC </h4><br>  Os engenheiros da equipe de resposta come√ßaram a desenvolver um plano para resolver inconsist√™ncias de dados e lan√ßaram procedimentos de failover para o MySQL.  O plano era restaurar arquivos do backup, sincronizar r√©plicas nos dois sites, retornar a uma topologia de servi√ßo est√°vel e depois retomar o processamento de trabalhos na fila.  Atualizamos o status para informar aos usu√°rios que vamos executar um failover gerenciado do sistema de armazenamento interno. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb4/cb7/a34/eb4cb7a34add07f272b99fc93f161d56.png"><br>  <i><font color="gray">O plano de recupera√ß√£o envolvia avan√ßar, restaurar os backups, sincronizar, retroceder e resolver o atraso antes de retornar ao status verde</font></i> <br><br>  Embora os backups do MySQL sejam feitos a cada quatro horas e armazenados por muitos anos, eles ficam em um armazenamento remoto na nuvem de objetos de blob.  A recupera√ß√£o de v√°rios terabytes de um backup levou v√°rias horas.  Demorou muito tempo para transferir dados do servi√ßo de backup remoto.  Passava a maior parte do tempo descompactando, verificando a soma de verifica√ß√£o, preparando e carregando grandes arquivos de backup em servidores MySQL rec√©m-preparados.  Esse procedimento √© testado diariamente, para que todos tenham uma boa id√©ia de quanto tempo a recupera√ß√£o levaria.  No entanto, antes desse incidente, nunca tivemos que reconstruir completamente o cluster inteiro a partir de um backup.  Outras estrat√©gias sempre funcionaram, como r√©plicas adiadas. <br><br><h4>  10/22/2018, 00:41 UTC </h4><br>  A essa altura, um processo de backup havia sido iniciado para todos os clusters do MySQL afetados, e os engenheiros acompanharam o progresso.  Ao mesmo tempo, v√°rios grupos de engenheiros estudaram maneiras de acelerar a transfer√™ncia e a recupera√ß√£o sem degradar ainda mais o site ou o risco de corrup√ß√£o de dados. <br><br><h4>  10/22/2018, 06:51 UTC </h4><br>  V√°rios clusters no data center oriental conclu√≠ram a recupera√ß√£o dos backups e come√ßaram a replicar novos dados da costa oeste.  Isso levou a uma desacelera√ß√£o no carregamento de p√°ginas que executaram uma opera√ß√£o de grava√ß√£o em todo o pa√≠s, mas a leitura de p√°ginas desses clusters de banco de dados retornou resultados reais se a solicita√ß√£o de leitura cair em uma r√©plica restaurada recentemente.  Outros clusters de banco de dados maiores continuaram a se recuperar. <br><br>  Nossas equipes identificaram um m√©todo de recupera√ß√£o diretamente da costa oeste para superar as limita√ß√µes de largura de banda causadas pela inicializa√ß√£o a partir do armazenamento externo.  Tornou-se quase 100% claro que a recupera√ß√£o ser√° conclu√≠da com √™xito, e o tempo para criar uma topologia de replica√ß√£o √≠ntegra depende de quanto demora a replica√ß√£o de recupera√ß√£o.  Essa estimativa foi interpolada linearmente com base na replica√ß√£o de telemetria dispon√≠vel e a p√°gina de status foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atualizada</a> para definir a espera de duas horas como o tempo estimado de recupera√ß√£o. <br><br><h4>  10/22/2018, 07:46 UTC </h4><br>  O GitHub postou um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">post informativo no blog</a> .  N√≥s mesmos usamos as p√°ginas do GitHub, e todas as assembl√©ias foram pausadas algumas horas atr√°s, portanto a publica√ß√£o exigiu um esfor√ßo adicional.  Pedimos desculpas pelo atraso.  Pretendemos enviar esta mensagem muito antes e, no futuro, forneceremos a publica√ß√£o de atualiza√ß√µes nas condi√ß√µes de tais restri√ß√µes. <br><br><h4>  10/22/2018, 11:12 UTC </h4><br>  Todos os bancos de dados prim√°rios s√£o novamente transferidos para o leste.  Isso fez com que o site se tornasse muito mais responsivo, pois os registros agora eram roteados para um servidor de banco de dados localizado no mesmo data center f√≠sico da nossa camada de aplicativos.  Embora esse desempenho tenha melhorado significativamente, ainda havia dezenas de r√©plicas de leitura do banco de dados que estavam v√°rias horas atr√°s da c√≥pia principal.  Essas r√©plicas atrasadas levaram os usu√°rios a ver dados inconsistentes ao interagir com nossos servi√ßos.  Distribu√≠mos a carga de leitura por um grande conjunto de r√©plicas de leitura, e cada solicita√ß√£o para nossos servi√ßos tem boas chances de entrar na r√©plica de leitura com um atraso de v√°rias horas. <br><br>  De fato, o tempo de recupera√ß√£o de uma r√©plica atrasada √© reduzido exponencialmente, n√£o linearmente.  Quando os usu√°rios nos EUA e na Europa acordaram, devido ao aumento da carga nos registros nos clusters de bancos de dados, o processo de recupera√ß√£o levou mais tempo do que o previsto. <br><br><h4>  22/10/2018, 13:15 UTC </h4><br>  Est√°vamos nos aproximando do pico de carga no GitHub.com.  A equipe de resposta discutiu as pr√≥ximas etapas.  Ficou claro que o atraso na replica√ß√£o para um estado consistente est√° aumentando, n√£o diminuindo.  Anteriormente, come√ßamos a preparar r√©plicas adicionais de leitura do MySQL na nuvem p√∫blica da Costa Leste.  Uma vez dispon√≠veis, ficou mais f√°cil distribuir o fluxo de solicita√ß√µes de leitura entre v√°rios servidores.  Reduzir a carga m√©dia nas r√©plicas de leitura acelerou a recupera√ß√£o da replica√ß√£o. <br><br><h4>  22/10/2018, 16:24 UTC </h4><br>  Ap√≥s sincronizar as r√©plicas, retornamos √† topologia original, eliminando os problemas de atraso e disponibilidade.  Como parte de uma decis√£o consciente sobre a prioridade da integridade dos dados, em vez de uma r√°pida corre√ß√£o da situa√ß√£o, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">mantivemos o status vermelho do</a> site quando come√ßamos a processar os dados acumulados. <br><br><h4>  22/10/2018, 16:45 UTC </h4><br>  No est√°gio de recupera√ß√£o, era necess√°rio equilibrar o aumento da carga associada ao backlog, sobrecarregando potencialmente nossos parceiros do ecossistema com notifica√ß√µes e retornando a cem por cento de efici√™ncia o mais r√°pido poss√≠vel.  Mais de cinco milh√µes de eventos de gancho e 80 mil solicita√ß√µes para cria√ß√£o de p√°ginas da web permaneceram na fila. <br><br>  Quando reativamos o processamento desses dados, processamos cerca de 200.000 tarefas √∫teis com webhooks que excederam o TTL interno e foram descartados.  Ao saber disso, paramos de processar e come√ßamos a aumentar o TTL. <br><br>  Para evitar uma diminui√ß√£o adicional na confiabilidade de nossas atualiza√ß√µes de status, deixamos o status de degrada√ß√£o at√© concluirmos o processamento de toda a quantidade acumulada de dados e garantir que os servi√ßos retornem claramente ao n√≠vel normal de desempenho. <br><br><h4>  22/10/2018, 23:03 UTC </h4><br>  Todos os eventos incompletos da webhook e conjuntos de p√°ginas s√£o processados, e a integridade e a opera√ß√£o correta de todos os sistemas s√£o confirmadas.  O status do site foi <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">atualizado para verde</a> . <br><br><h1>  A√ß√µes adicionais </h1><br><h4>  Resolu√ß√£o de incompatibilidade de dados </h4><br>  Durante a recupera√ß√£o, corrigimos logs bin√°rios do MySQL com entradas principalmente do data center, que n√£o foram replicadas para o ocidental.  O n√∫mero total de tais entradas √© relativamente pequeno.  Por exemplo, em um dos clusters mais ocupados, existem apenas 954 registros nesses segundos.  No momento, estamos analisando esses logs e determinando quais entradas podem ser reconciliadas automaticamente e quais requerem assist√™ncia ao usu√°rio.  V√°rias equipes participam desse trabalho, e nossa an√°lise j√° determinou a categoria de registros que o usu√°rio repetiu - e foram salvos com sucesso.  Conforme declarado nesta an√°lise, nosso principal objetivo √© manter a integridade e a precis√£o dos dados armazenados no GitHub. <br><br><h4>  Comunica√ß√£o </h4><br>  Tentando transmitir informa√ß√µes importantes para voc√™ durante o incidente, fizemos v√°rias estimativas p√∫blicas do tempo de recupera√ß√£o com base na velocidade de processamento dos dados acumulados.  Olhando para tr√°s, nossas estimativas n√£o levaram em considera√ß√£o todas as vari√°veis.  Pedimos desculpas pela confus√£o e nos esfor√ßaremos para fornecer informa√ß√µes mais precisas no futuro. <br><br><h4>  Medidas t√©cnicas </h4><br>  V√°rias medidas t√©cnicas foram identificadas durante esta an√°lise.  A an√°lise continua, a lista pode ser complementada. <br><br><ul><li>  Ajuste a configura√ß√£o do Orchestrator para impedir que os bancos de dados principais se movam para fora da regi√£o.  O orquestrador funcionou de acordo com as configura√ß√µes, embora a camada de aplicativo n√£o suporte essa altera√ß√£o de topologia.  A escolha de um l√≠der em uma regi√£o geralmente √© segura, mas o aparecimento repentino de um atraso devido ao fluxo de tr√°fego no continente se tornou a principal causa desse incidente.  Este √© um novo comportamento emergente do sistema, porque antes n√£o encontramos a se√ß√£o interna da rede dessa magnitude. </li><li>  Aceleramos a migra√ß√£o para o novo sistema de relat√≥rio de status, que fornecer√° uma plataforma mais adequada para discutir incidentes ativos com linguagem cada vez mais clara.  Embora muitas partes do GitHub estivessem dispon√≠veis durante todo o incidente, s√≥ pudemos selecionar status verde, amarelo e vermelho para todo o site.  Admitimos que isso n√£o fornece uma imagem precisa: o que funciona e o que n√£o funciona.  O novo sistema exibir√° os v√°rios componentes da plataforma para que voc√™ saiba o status de cada servi√ßo. </li><li>  Algumas semanas antes desse incidente, lan√ßamos uma iniciativa de engenharia em toda a empresa para oferecer suporte ao tr√°fego do GitHub de v√°rios data centers usando a arquitetura ativa / ativa / ativa.  O objetivo deste projeto √© oferecer suporte √† redund√¢ncia N + 1 no n√≠vel do datacenter para suportar a falha de um datacenter sem interfer√™ncia externa.  Isso √© muito trabalhoso e levar√° algum tempo, mas acreditamos que v√°rios datacenters bem conectados em diferentes regi√µes fornecer√£o um bom compromisso.  O √∫ltimo incidente levou essa iniciativa ainda mais longe. </li><li>  Tomaremos uma posi√ß√£o mais ativa na verifica√ß√£o de nossas suposi√ß√µes.  O GitHub est√° crescendo rapidamente e acumulou uma quantidade consider√°vel de complexidade na √∫ltima d√©cada.  Est√° se tornando cada vez mais dif√≠cil capturar e transmitir √† nova gera√ß√£o de funcion√°rios o contexto hist√≥rico de compromissos e decis√µes tomadas. </li></ul><br><h4>  Medidas organizacionais </h4><br>  Esse incidente influenciou bastante nosso entendimento da confiabilidade do site.  Aprendemos que restringir o controle operacional ou melhorar os tempos de resposta n√£o s√£o garantias suficientes de confiabilidade em um sistema t√£o complexo de servi√ßos como o nosso.  Para apoiar esses esfor√ßos, tamb√©m iniciaremos uma pr√°tica sistem√°tica de testar cen√°rios de falhas antes que eles realmente ocorram.  Este trabalho inclui a solu√ß√£o de problemas deliberada e o uso de ferramentas de engenharia do caos. <br><br><h1>  Conclus√£o </h1><br>  Sabemos como voc√™ confia no GitHub em seus projetos e neg√≥cios.  Preocupamo-nos mais do que ningu√©m com a disponibilidade do nosso servi√ßo e a seguran√ßa dos seus dados.    ,          . </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt428409/">https://habr.com/ru/post/pt428409/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt428393/index.html">Webinar "Testar ambientes 2.0 na nuvem e como aprender a cozinh√°-los"</a></li>
<li><a href="../pt428395/index.html">O livro ‚ÄúTe√≥rico m√≠nimo para Big Data. Tudo o que voc√™ precisa saber sobre big data ‚Äù</a></li>
<li><a href="../pt428401/index.html">Processos de desenvolvimento atrav√©s dos olhos da explora√ß√£o. Um olhar do outro lado da barricada</a></li>
<li><a href="../pt428403/index.html">Resumo de eventos para profissionais de RH na √°rea de TI em novembro de 2018</a></li>
<li><a href="../pt428407/index.html">6 parcelas t√≠picas da literatura mundial</a></li>
<li><a href="../pt428411/index.html">Radar de tecnologia: uma lista de idiomas, ferramentas e plataformas que passaram pelas m√£os de Lamoda</a></li>
<li><a href="../pt428413/index.html">Sistemas de refrigera√ß√£o nos data centers Selectel</a></li>
<li><a href="../pt428415/index.html">Vis√£o geral do controlador de nuvem TP-Link Omada OC200</a></li>
<li><a href="../pt428417/index.html">Aprendizado de m√°quina no MatLab / Oitava: exemplos de algoritmos suportados por f√≥rmulas</a></li>
<li><a href="../pt428419/index.html">Arraste e deslize o dedo no RecyclerView. Parte 2: arraste e solte controladores, grades e anima√ß√µes personalizadas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>