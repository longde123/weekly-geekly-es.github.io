<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧑🏿‍🤝‍🧑🏼 👨🏻 🐏 Tiefenkameras - stille Revolution (wenn Roboter sehen) Teil 2 👩🏽‍✈️ 🕺🏻 🗾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im ersten Teil dieses Textes haben wir Tiefenkameras untersucht, die auf Messungen des strukturellen Lichts und der Umlauflichtverzögerung basieren un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Tiefenkameras - stille Revolution (wenn Roboter sehen) Teil 2</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/458458/"><img src="https://habrastorage.org/webt/uy/lx/0t/uylx0tpbzxlqa5hnxqzjnriruoy.png"><br><br>  Im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ersten Teil</a> dieses Textes haben wir Tiefenkameras untersucht, die auf Messungen des strukturellen Lichts und der Umlauflichtverzögerung basieren und hauptsächlich Infrarotbeleuchtung verwenden.  Sie eignen sich hervorragend für Innenräume in Entfernungen von 10 bis 10 Metern und sind vor allem sehr billig.  Daher die massive Welle ihrer aktuellen Verwendung in Smartphones.  Aber ... Sobald wir nach draußen gehen, beleuchtet die Sonne sogar durch die Wolken das Infrarotlicht und ihre Arbeit verschlechtert sich stark. <br><br>  Wie Steve Blank ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus einem anderen Grund</a> ) sagt: "Wenn Sie Erfolg haben wollen, verlassen Sie das Gebäude."  Im Folgenden werden wir über Tiefenkameras sprechen, die im Freien arbeiten.  Heute wird dieses Thema stark von autonomen Autos bestimmt, aber, wie wir sehen werden, nicht nur. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/659/8b4/4b2/6598b44b2d0a420dd83a434753cc6ffb.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Innoviz sieht massenproduzierte selbstfahrende Autos mit Festkörper-LiDAR vor</a></i> <br><br>  So können Tiefenkameras, d.h.  Geräte, die Videos aufnehmen, in deren Pixel der Abstand zum Szenenobjekt im Sonnenlicht liegt! <br><br>  Wen kümmert es - willkommen bei kat! <br><a name="habracut"></a><br>  Beginnen wir mit den zeitlosen Klassikern ... <br><br><h1>  Methode 3: Tiefe von der Stereokamera + </h1><br>  Das Erstellen einer Tiefenkarte aus Stereo ist bekannt und wird seit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">über 40 Jahren verwendet</a> .  Im Folgenden finden Sie ein Beispiel für eine Kompaktkamera für 450 US-Dollar, mit der Gesten zusammen mit professioneller Fotografie oder VR-Helmen gesteuert werden können: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/594/0ab/3b0/5940ab3b0cbc72369d80083d8d32be57.png"><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Quelle</a></i> <br><br>  Der Hauptvorteil solcher Kameras besteht darin, dass Sonnenlicht sie nicht nur nicht stört, sondern umgekehrt auch ihre Ergebnisse verbessert. Daher ist die aktive Verwendung solcher Kameras für alle Arten von Straßenkoffern beispielsweise ein hervorragendes Beispiel dafür, wie ein dreidimensionales Modell einer alten Festung in wenigen Minuten aufgenommen werden kann: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/AFH2yN3rM78" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein Beispiel für den Straßengebrauch der ZED-Kamera</a></i> <br><br>  Das spürbare Geld für die Übersetzung der Tiefenkonstruktion von Stereo auf ein neues Niveau wurde natürlich vom Thema autonome Autos beeinflusst.  Von den 5 in Betracht gezogenen Methoden zum Erstellen eines Tiefenvideos stören nur zwei - diese und die nächste (Stereo und Plenoptik) - nicht die Sonne und nicht benachbarte Autos.  Gleichzeitig ist die Plenoptik über große Entfernungen um ein Vielfaches teurer und ungenauer.  Auf jeden Fall können Dinge passieren, es ist schwierig, Prognosen zu erstellen, aber in diesem Fall lohnt es sich, Elon Mask zuzustimmen - Stereo von allen 5 Methoden hat die besten Aussichten.  Und die aktuellen Ergebnisse sind sehr ermutigend: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12e/cec/e3f/12ecece3f83fc654f64023e1391c38bb.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Direct SLAM in großem Maßstab mit Stereokameras</a></i> <br><br>  Es ist jedoch interessant, dass nicht unbemannte Fahrzeuge (von denen bisher nur wenige hergestellt werden), sondern viel massivere Geräte, auf denen derzeit eine Stereotiefenkarte erstellt wird, einen noch stärkeren Einfluss auf die Entwicklung der Bautiefe aus Stereo haben werden, nämlich ... Richtig!  Smartphones! <br><br>  Vor drei Jahren gab es einfach den Boom der „zweiäugigen“ Smartphones, bei denen buchstäblich alle Marken bekannt waren, weil sich die Qualität der mit einer Kamera aufgenommenen Fotos und die Qualität der mit zwei Kameras aufgenommenen Fotos dramatisch unterschieden. Unter dem Gesichtspunkt der Preiserhöhung für ein Smartphone war dies jedoch nicht so bedeutend: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ab7/6b4/b3e/ab76b4b3e9696443e5191d293b59e663.png"><br><br>  Darüber hinaus ging der Prozess im vergangenen Jahr noch aktiv weiter: „Haben Sie 2 Kameras in Ihrem Smartphone?  Scheiße!  Ich habe <s>drei</s> vier !!! ”: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48f/905/554/48f905554fab15bcc6ce668c0c5f9ef7.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Samsung Galaxy A8 &amp; A9</a></i> <br><br>  Sechsäugige Zukunft Sony wurde im ersten Teil erwähnt.  Im Allgemeinen werden mehräugige Smartphones bei Herstellern immer beliebter. <br><br>  Die grundlegenden Ursachen dieses Phänomens sind einfach: <br><br><ul><li>  Die Auflösung von Kamerahandys wächst und die Größe des Objektivs ist gering.  Infolgedessen steigt trotz zahlreicher Tricks der Geräuschpegel und die Qualität nimmt ab, insbesondere bei Aufnahmen im Dunkeln. <br></li><li>  Darüber hinaus können wir bei der zweiten Kamera das sogenannte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bayer-Filter</a> aus der Matrix entfernen, d.h.  Eine Kamera ist schwarzweiß und die zweite Farbe.  Dies erhöht die Schwarzweißempfindlichkeit um etwa das Dreifache.  Das heißt,  Die Empfindlichkeit von 2 Kameras wächst bedingt nicht um das 2-fache, sondern um das 4-fache (!).  Es gibt zahlreiche Nuancen, aber eine solche Erhöhung der Empfindlichkeit ist für das Auge wirklich deutlich sichtbar. <br></li><li>  Wenn ein Stereopaar erscheint, haben wir unter anderem die Möglichkeit, die Schärfentiefe programmgesteuert zu ändern, d. H.  Verwischen Sie den Hintergrund, von dem viele Fotos erheblich profitieren (darüber haben wir in der zweiten Hälfte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier geschrieben</a> ).  Diese Option neuer Smartphone-Modelle wurde schnell sehr beliebt. <br></li><li>  Mit zunehmender Anzahl von Kameras ist es auch möglich, andere Objektive zu verwenden - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Weitwinkel</a> (Kurzfokus) und umgekehrt Langfokus, was die Qualität beim "Annähern" von Objekten erheblich verbessern kann. <br></li><li>  Interessanterweise bringt uns eine Zunahme der Anzahl der Kameras näher an das Thema eines verdünnten Lichtfeldes heran, das viele seiner Merkmale und Vorteile aufweist. Dies ist jedoch eine andere Geschichte. <br></li><li>  Wir stellen außerdem fest, dass Sie durch Erhöhen der Anzahl der Kameras die Auflösung durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Auflösungswiederherstellungsmethoden</a> erhöhen können. <br></li></ul><br>  Im Allgemeinen gibt es so viele Pluspunkte, dass sich die Leute, wenn sie sich ihrer bewusst sind, fragen, warum mindestens zwei Kameras schon lange nicht mehr eingestellt wurden. <br><br>  Und dann stellt sich heraus, dass nicht alles so einfach ist.  Um zusätzliche Kameras sinnvoll einzusetzen, um das Bild zu verbessern (und nicht nur zu einer Kamera mit einem anderen Objektiv zu wechseln), müssen wir eine sogenannte Disparitätskarte erstellen, die direkt in eine Tiefenkarte umgewandelt wird.  Und dies ist eine nicht triviale Aufgabe, zu deren Lösung gerade die Leistung von Smartphones gekommen ist.  Und selbst jetzt sind Tiefenkarten oft von eher zweifelhafter Qualität.  Das heißt, bevor die genaue Übereinstimmung "Pixel auf dem rechten Bild mit dem Pixel auf der linken Seite" noch überleben muss.  Hier sind zum Beispiel echte Beispiele für Tiefenkarten für das iPhone: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/1f2/7d2/2211f27d2bfb2dc01da93559e8d1d8d1.png"><br>  <i>Quelle: <a href="">iPhone XS &amp; XR Tiefenkartenvergleich</a></i> <br><br>  Selbst mit dem Auge sind Massenprobleme im Hintergrund deutlich sichtbar, an den Rändern schweige ich über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">durchscheinendes</a> Haar.  Daher gibt es zahlreiche Probleme, die sowohl beim Übertragen von Farbe von einer Schwarzweißkamera auf eine Farbkamera als auch während der weiteren Verarbeitung auftreten. <br><br>  Hier ist zum Beispiel ein ziemlich gutes Beispiel aus einem Bericht von Apple zum Thema „Erstellen von Foto- und Videoeffekten mit Tiefe“: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/83c/38f/efc/83c38fefc85d9e1ddef931b594e20469.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen von Foto- und Videoeffekten • Verwenden von Depth, Apple, WWDC18</a></i> <br><br>  Sie können deutlich sehen, wie fehlerhaft die Tiefe unter den Haaren ist, und zwar vor einem mehr oder weniger gleichmäßigen Hintergrund, aber was noch wichtiger ist, auf der rechten Mattierungskarte ist der Kragen in den Hintergrund getreten (d. H. Er wird unscharf).  Ein weiteres Problem ist die tatsächliche Auflösung der Tiefe und die Mattierungskarte ist erheblich niedriger als die Bildauflösung, was sich auch auf die Qualität während der Verarbeitung auswirkt: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/848/dcb/5cf/848dcb5cf62ea5991a85b23160fc1ce9.png"><br><br>  All dies ist jedoch ein Wachstumsproblem.  Wenn vor 4 Jahren keine ernsthaften Auswirkungen auf das Video in Frage kamen, hat das Telefon sie einfach nicht "gezogen", aber heute wird sich die Videoverarbeitung mit Tiefe auf seriellen Top-Telefonen gezeigt (in derselben Präsentation von Apple): <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/481/5fe/6f8/4815fe6f8f9e7fd837a4e410df01a0da.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Erstellen von Foto- und Videoeffekten • Verwenden von Depth, Apple, WWDC18</a></i> <br><br>  Im Allgemeinen erobert das Thema Multi-Multi-Handys und damit das Thema Tiefenschärfe von Stereo auf Mobiltelefonen die Massen kampflos: <br><br><img width="66%" src="https://habrastorage.org/webt/t2/ba/sz/t2basz3pj6tskae0fqox5ol3x-q.png"><br>  <i>Quelle: "auf diesen von Ihrem Internet gefunden"</i> <br><br>  Wichtigste Ergebnisse: <br><br><ul><li>  Die Tiefe von Stereo - gemessen an den Ausrüstungskosten - ist der billigste Weg, um Tiefe zu erhalten, da Kameras jetzt kostengünstig sind und immer schneller billiger werden.  Die Schwierigkeit besteht darin, dass die Weiterverarbeitung viel ressourcenintensiver ist als bei anderen Methoden. <br></li><li>  Bei Mobiltelefonen können Sie den Durchmesser des Objektivs nicht vergrößern, während die Auflösung schnell zunimmt.  Infolgedessen kann die Verwendung von zwei oder mehr Kameras die Qualität des Fotos erheblich verbessern, das Rauschen bei schlechten Lichtverhältnissen reduzieren und die Auflösung erhöhen.  Da heutzutage häufig ein Mobiltelefon für die Qualität der Kamera ausgewählt wird, ist dies ein äußerst greifbares Plus.  Das Erstellen einer Tiefenkarte ist ein unauffälliger Nebenbonus. <br></li><li>  Die Hauptnachteile der Gebäudetiefe von Stereo: <br><ul><li>  Sobald die Textur verschwindet oder einfach weniger kontrastreich wird, nimmt das Rauschen in der Tiefe stark zu, so dass selbst bei einfachen Objekten die Tiefe häufig schlecht genutzt wird (schwerwiegende Fehler sind möglich). <br></li><li>  Außerdem ist die Tiefe bei dünnen und kleinen Objekten ("abgeschnittenen" Fingern oder sogar Händen, versenkten Säulen usw.) schlecht bestimmt. <br></li></ul></li><li>  Sobald Sie mit der Leistung von Eisen eine Tiefenkarte für Videos erstellen können, wird die Tiefe auf Smartphones einen starken Impuls für die Entwicklung von AR geben (irgendwann, völlig unerwartet für die Öffentlichkeit, wird die Qualität von AR-Anwendungen auf allen neuen Telefonmodellen, einschließlich der Budget-Modelle, plötzlich merklich höher und eine neue Welle wird losgehen) .  Völlig unerwartet! <br></li></ul><br>  Die nächste Methode ist weniger trivial und berühmt, aber sehr cool.  Triff mich! <br><br><h1>  Methode 4: Lichtfeldtiefenkameras </h1><br>  Das Thema Plenoptik (vom lateinischen Plenus - voll und optikos - visuell) oder Lichtfelder ist der Masse noch relativ wenig bekannt, obwohl Fachleute damit begannen, sich sehr dicht damit zu befassen.  Auf vielen Top-Konferenzen wurden separate Abschnitte für Artikel zum Thema Lichtfeld zugewiesen (der Autor war einmal beeindruckt von der Anzahl der asiatischen Forscher auf der IEEE International Conference on Multimedia and Expo, die sich intensiv mit diesem Thema befassen). <br><br>  Laut Google Trends sind die USA und Australien an Light Field interessiert, gefolgt von Singapur und Korea.  Großbritannien  Russland liegt auf Platz 32 ... Wir werden den Rückstand aus Indien und Südafrika korrigieren: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/651/993/87b/65199387b858541acc6ff99a129c2fe3.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Trends</a></i> <br><br>  Ihr bescheidener Diener hat vor einiger Zeit einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ausführlichen Artikel über Habré verfasst,</a> in dem ausführlich beschrieben wird, wie es funktioniert und was es gibt. Lassen Sie uns kurz darauf eingehen. <br><br>  Die Hauptidee besteht darin, an jedem Punkt nicht nur Licht, sondern eine zweidimensionale Anordnung von Lichtstrahlen zu fixieren, wodurch jeder Rahmen vierdimensional wird.  In der Praxis erfolgt dies mit einer Reihe von Mikrolinsen: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/90f/203/fb4/90f203fb49d7e464638a4f164e96fb26.gif"></a> <br>  <i>Quelle: <a href="">plenoptic.inf (empfohlen zum Klicken und Anzeigen in voller Auflösung)</a></i> <br><br>  Infolgedessen haben wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">viele neue Möglichkeiten</a> , aber die Auflösung fällt ernsthaft.  Nachdem sie viele komplizierte technische Probleme gelöst hatten, erhöhten sie die Auflösung bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Lytro</a> (von Google gekauft) radikal, wo im Lytro Cinema die Auflösung des Kamerasensors auf 755 Megapixel RAW-Daten erhöht wurde und wie die ersten Kameras sperrig aussah: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3ec/631/027/3ec6310271829f9272a4d3ef41462bde.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NAB: Neue Lytro-Lichtfeldkamera, die große Änderungen an der Arbeit mit visuellen Effekten bewirken könnte, wird vorgestellt</a></i> <br><br>  Es ist interessant, dass selbst Profis den Auflösungsabfall von plenoptischen Kameras regelmäßig falsch einschätzen, weil sie unterschätzen, wie gut sie mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Super Resolution-</a> Algorithmen arbeiten können, die viele Details der Bild-Mikroverschiebungen im Lichtfeld wirklich perfekt wiederherstellen (achten Sie auf verschwommene Stricknadeln und sich bewegende Schaukeln im Hintergrund). :: <br><br><img width="200%" src="https://habrastorage.org/getpro/habr/post_images/bc9/6cd/e55/bc96cde55e276f4420757de5359467b1.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Naive, intelligente und hochauflösende plenoptische Rahmenwiederherstellung</a> aus dem technischen Bericht von Adobe „Superauflösung mit plenoptischer Kamera 2.0“</i> <br><br>  All dies wäre von relativ theoretischem Interesse, wenn Google keine Plenoptik in Pixel 2 implementiert hätte, indem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">2 Pixel mit einer Linse abgedeckt wurden</a> : <br><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/852/c36/209/852c36209a7528f576e20dc37e0ef1f3.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">AI Google Blog</a></i> <br><br>  Als Ergebnis wurde ein Mikrostereopaar gebildet, das es ermöglichte, die Tiefe zu messen, bis zu der Google, getreu neuen Traditionen, neuronale Netze hinzufügte, und es stellte sich im Allgemeinen als wunderbar heraus: <br><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/429/442/99e/42944299e72b7eb720f3babc5a757b79.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/da3/fe1/3ea/da3fe13eacb3cd5b0a558db1d843b686.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/c6f/a27/c4b/c6fa27c4bac8b9c5cb122e0c69ab814d.png"><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/2ba/e97/e67/2bae97e67ae10cd150ef28ccdc6cc458.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/88a/159/34f/88a15934f1192deb0da889a630fed232.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/d7d/eab/811/d7deab81164e7e886b42a95173c58641.png"><br><br>  Weitere Beispiele für Tiefe in voller Auflösung <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in einer speziellen Galerie</a> . <br><br>  Interessanterweise wird die Tiefe von Google (wie Huawei und anderen) im Bild selbst gespeichert, sodass Sie sie von dort extrahieren und sehen können: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d63/958/b09/d63958b09a9584752a90f014adfadc24.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Drei geheime Funktionen der neuen Kamera-App von Google, die Sie umhauen werden</a></i> <br><br>  Und dann können Sie das Foto dreidimensional machen: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48b/3a5/e71/48b3a5e7156a14dcf60c600472154f6c.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Drei geheime Funktionen der neuen Kamera-App von Google, die Sie umhauen werden</a></i> <br><br>  Sie können auf der Website <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">http://depthy.me</a> , auf der Sie Ihr Foto hochladen können, unabhängig damit experimentieren.  Interessanterweise ist die Site <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">in der Quelle verfügbar</a> , d. H.  Die Tiefenverarbeitung kann verbessert werden, dafür gibt es viele Möglichkeiten, jetzt werden dort die einfachsten Verarbeitungsalgorithmen angewendet. <br><br>  Wichtige Punkte: <br><br><ul><li>  Auf einer der Google-Konferenzen wurde angekündigt, dass möglicherweise 4 Pixel mit einem Objektiv abgedeckt werden.  Dies verringert die direkte Auflösung des Sensors, verbessert jedoch die Tiefenkarte erheblich.  Erstens aufgrund des Auftretens von Stereopaaren in zwei senkrechten Richtungen und zweitens aufgrund der Tatsache, dass sich die Stereobasis bedingt um das 1,4-fache erhöht (zwei Diagonalen).  Dies bedeutet auch eine deutliche Verbesserung der Tiefengenauigkeit in der Ferne. <br></li><li>  Plenoptics selbst (es ist auch ein kalkuliertes Foto) ermöglicht Folgendes: <br><ul><li>  Das „ehrliche“ Ändern von Fokus und Schärfentiefe nach dem Aufnehmen ist die bekannteste Fähigkeit von plenoptischen Sensoren. <br></li><li>  Blendenform berechnen. <br></li><li>  Berechnen Sie die Szenenbeleuchtung. <br></li><li>  Verschieben Sie den Aufnahmepunkt etwas, einschließlich des Empfangs von Stereo (oder Multi-Angle-Frame) mit einem Objektiv. <br></li><li>  Berechnen Sie die Auflösung, da Sie mit der hohen Rechenkomplexität der Super Resolution-Algorithmen den Frame tatsächlich wiederherstellen können. <br></li><li>  Berechnen Sie eine Transparenzkarte für durchscheinende Ränder. <br></li><li>  Und schließlich erstellen Sie eine Tiefenkarte, die heute wichtig ist. <br></li></ul></li><li>  <b>Wenn die Hauptkamera des Telefons parallel zur Aufnahme in Echtzeit eine hochwertige Tiefenkarte erstellen kann, wird dies</b> möglicherweise <b>eine Revolution auslösen</b> .  Dies hängt weitgehend von der Rechenleistung an Bord ab (dies hindert uns daran, heute in Echtzeit bessere Tiefenkarten mit höherer Auflösung zu erstellen).  Dies ist natürlich für AR am interessantesten, aber es gibt viele Möglichkeiten, Fotos zu ändern. <br></li></ul><br>  Und schließlich fahren wir mit der letzten Methode zur Überprüfung der Tiefe fort. <br><br><h1>  Methode 5: Kameras auf Lidar-Technologien </h1><br>  Im Allgemeinen sind <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Laser-Entfernungsmesser</a> fest in unserem Leben verankert, kostengünstig und bieten eine hohe Genauigkeit.  Die ersten Lidars (von LIDaR - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Light Identification Detection and Ranging</a> ), die als Bündel ähnlicher Geräte gebaut wurden, die sich um eine horizontale Achse drehen, wurden zuerst vom Militär verwendet und dann in Autopiloten von Autos getestet.  Sie erwiesen sich dort als recht gut, was zu einem starken Anstieg der Investitionen in der Region führte.  Anfangs drehten sich die Lidare und ergaben mehrmals pro Sekunde ein ähnliches Bild: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed4/984/265/ed49842653721c8d7e6dae51c290166d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eine Einführung in LIDAR: Der wichtigste selbstfahrende Autosensor</a></i> <br><br>  Es war unangenehm, aufgrund der beweglichen Teile unzuverlässig und ziemlich teuer.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tony Ceba</a> liefert in seinen Vorlesungen interessante Daten zur Senkung der Lidarkosten.  Wenn Lidars für die ersten autonomen Google-Computer 70.000 US-Dollar kosten (zum Beispiel kostet der auf den ersten Computern verwendete spezialisierte <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HDL-64E</a> 75.000 US-Dollar): <br><img src="https://habrastorage.org/webt/w7/n4/4x/w7n44xwcg5gmcyi8v_dxc-yefd4.png"><br>  <i>Quelle: Dies und Weiter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Vom Parken zu Parks - Bellevue &amp; die Störung des Transports</a></i> <br><br>  In der Massenproduktion drohen dann neue Modelle der nächsten Generationen, den Preis deutlich unter 1000 US-Dollar zu senken: <br><img src="https://habrastorage.org/getpro/habr/post_images/178/fb1/712/178fb1712f85c4534ff25d11e54ad098.png"><br><br>  Man kann über das Beispiel von Tony streiten (das Versprechen eines Startups sind nicht die endgültigen Kosten), aber dass es in diesem Bereich einen Boom in der Forschung gibt, die rasche Zunahme der Produktionsläufe, das Erscheinen völlig neuer Produkte und ein allgemeiner Preisverfall unbestritten sind.  Wenig später im Jahr 2017 lautete die Prognose für einen Preisverfall wie folgt (und der Moment der Wahrheit wird kommen, wenn sie massiv in Autos eingebaut werden): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/c6c/5b0/d75c6c5b04e509f698a5d967776ff68c.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LiDAR schließt Sensing Triumvirate ab</a></i> <br><br>  Insbesondere haben vor relativ kurzer Zeit mehrere Hersteller sofort den sogenannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Solid State Lidar auf</a> den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Markt gebracht</a> , der im Grunde keine beweglichen Teile aufweist, die eine radikal höhere Zuverlässigkeit aufweisen, insbesondere beim Schütteln, bei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">geringeren Kosten</a> usw.  Ich empfehle, dieses Video anzuschauen, in dem das Gerät in 84 Sekunden sehr deutlich erklärt wird: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yQi7J0ehKAA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Festkörper-Lidarsensor</a></i> <br><br>  Für uns ist wichtig, dass Solid State Lidar ein rechteckiges Bild liefert, d.h.  Tatsächlich funktioniert es wie eine „normale“ Tiefenkamera: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f6/90e/e65/1f690ee65f60f431037d6c9191896005.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Innoviz sieht massenproduzierte selbstfahrende Autos mit Festkörper-LiDAR vor</a></i> <br><br>  Das obige Beispiel zeigt ein Video mit ungefähr 1024 x 256, 25 FPS und 12 Bit pro Komponente.  Solche Lidars werden unter dem Haubengrill montiert (wenn sich das Gerät gut erwärmt): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18b/8dc/a13/18b8dca13a096a2337ad38b5728b26ea.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Festkörper-LiDAR-Magna-Elektronik</a></i> <br><br>  Wie üblich leuchten die Chinesen, die bei der Herstellung von Elektrofahrzeugen weltweit an erster Stelle stehen und bei autonomen Autos eindeutig den ersten Platz in der Welt anstreben: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c59/b26/b00/c59b26b009b05ce5a872f3203c720a3d.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Alibaba, RoboSense startet unbemanntes Fahrzeug mit Festkörper-LIDAR</a></i> <br><br>  Insbesondere ihre Experimente mit einem nicht quadratischen „Pixel“ der Tiefe sind interessant. Wenn Sie eine gemeinsame Verarbeitung mit einer RGB-Kamera durchführen, können Sie die Auflösung erhöhen, und dies ist ein ziemlich interessanter Kompromiss (das „Quadrat“ der Pixel ist in der Tat nur für eine Person wichtig): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7e9/13f/af2/7e913faf27197e1b893b56f13873e1d6.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MEMS Lidar für fahrerlose Fahrzeuge macht einen weiteren großen Schritt</a></i> <br><br>  Lidare werden in unterschiedlichen Schemata montiert, abhängig von den Kosten des Kits und der Leistung des Bordsystems, das alle diese Daten verarbeiten muss.  Dementsprechend ändern sich auch die allgemeinen Eigenschaften des Autopiloten.  Infolgedessen sind teurere Autos besser auf staubigen Straßen und können Autos, die an Kreuzungen an der Seite in Autos einfahren, leichter „ausweichen“. Billige Autos tragen nur dazu bei, die Anzahl der (zahlreichen) dummen Staus zu verringern: <br><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/00d/dd1/d4e/00ddd1d4e9f980216796306e4a62fe2e.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/025/548/1df/0255481dfb9f5dcd102a9a5979ca2fb8.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/7bf/842/4bf/7bf8424bf82edc861dc15434a03bfcbe.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Beschreibung RoboSense RS-LiDAR-M1</a></i> <br><br>  Beachten Sie, dass niedrige Preise neben Festkörpern ein paar weitere Bereiche versprechen, in denen sich Lidars entwickeln.  Hier etwas vorherzusagen ist eine undankbare Aufgabe, da zu viel nicht von den möglichen technischen Eigenschaften der Technologie abhängt, sondern beispielsweise von Patenten.  Es ist nur so, dass sich bereits mehrere Unternehmen mit Solid-State beschäftigen, daher sieht das Thema am vielversprechendsten aus.  Aber nichts über den Rest zu sagen, wäre unfair: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f9/3aa/265/3f93aa2653a9dde4393c007b925d07a2.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dieser LiDAR-Engpass 2017 verursacht einen modernen Goldrausch</a></i> <br><br>  Wenn wir von Lidars als Kameras sprechen, ist es erwähnenswert, ein weiteres wichtiges Merkmal zu erwähnen, das bei der Verwendung von Festkörper-Lidars von Bedeutung ist.  Sie arbeiten von Natur aus wie Kameras mit einem bereits vergessenen beweglichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Verschluss</a> , der beim Aufnehmen bewegter Objekte zu merklichen Verzerrungen führt: <br><br><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/b83/2e7/2a8/b832e72a8f8c67c65401da694df03c8e.gif"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Was ist der Unterschied zwischen einem globalen (globalen) Verschluss und einem rollenden Verschluss</a> ?</i> <br><br>  Angesichts der Tatsache, dass sich auf der Straße, insbesondere auf der Autobahn mit einer Geschwindigkeit von 150 km / h, alles recht schnell ändert, verzerrt diese Funktion von Lidars Objekte erheblich, einschließlich solcher, die schnell auf uns zufliegen ... Einschließlich der Tiefe ... Beachten Sie, dass die beiden vorherigen Methoden Tiefe zu bekommen ist kein solches Problem. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9b9/15b/979/9b915b9799869f23dcf7b2b3e1f5f1ca.gif"><br>  <i>Quelle: <a href="">Schöne Wikipedia-Animation mit Auto-Verzerrung</a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dieses Merkmal erfordert in Verbindung mit niedrigen FPS die Anpassung von Verarbeitungsalgorithmen, aber aufgrund der hohen Genauigkeit, auch bei großen Entfernungen, haben Lidars auf keinen Fall besondere Konkurrenten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interessanterweise funktionieren Lidars von Natur aus sehr gut in ebenen Bereichen und schlechter an Grenzen, und Stereosensoren sind in geraden Bereichen schlecht und an Grenzen relativ gut. Außerdem ergeben Lidars eine relativ kleine FPS, und die Kameras sind viel größer. Infolgedessen ergänzen sie sich im Wesentlichen, was auch bei Lytro Cinema-Kameras verwendet wurde (auf dem Foto in der Nähe der Kamera befindet sich eine beleuchtete plenoptische Linse mit bis zu 300 FPS und </font><font style="vertical-align: inherit;">das </font><font style="vertical-align: inherit;">schwarze Quadrat von </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Malevich</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Lidar </font><font style="vertical-align: inherit;">darunter </font><font style="vertical-align: inherit;">): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c1/eb9/ede/5c1eb9edeca7d67133487e93290435cd.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lytro ist bereit, das Filmemachen für immer zu verändern : debütiert auf der NAB mit dem Prototyp und dem Kurzfilm des Kinos</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn zwei Tiefensensoren in einer Filmkamera kombiniert wurden, kann bei anderen Geräten (von Smartphones bis zu Autos) erwartet werden, dass die Massenverteilung von Hybridsensoren maximale Qualität gewährleistet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In gewisser Weise handelt es sich bei Lidars immer noch um Investitionen, die sich in den letzten drei Jahren auf etwa 1,5 Milliarden US-Dollar beliefen (glücklicherweise wird dieser Markt </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in 6 Jahren</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> auf </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">10 Milliarden</font></a><font style="vertical-align: inherit;"> US-Dollar geschätzt, </font><font style="vertical-align: inherit;">und die folgende Grafik zeigt, wie die durchschnittliche Transaktionsgröße gewachsen ist): </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e07/fd6/3ae/e07fd63aee838e9a5c8958b0527ac4e9.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Quelle: </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interessant März Rückblick auf die Trends auf dem Lidar-Markt durch Techcrunch</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Der Entwicklungszyklus eines innovativen Produkts selbst in hart umkämpften Märkten beträgt 1,5 bis 2 Jahre, sodass wir bald äußerst interessante Produkte sehen werden. </font><font style="vertical-align: inherit;">Sie existieren bereits als Prototypen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wichtige Punkte:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Pluspunkte von Lidars: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die höchste Genauigkeit, einschließlich der besten unter allen Methoden auf große Entfernungen, </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Arbeiten Sie am besten auf ebenen Flächen, auf denen die Stereoanlage nicht mehr funktioniert. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> fast nicht mit der Sonne scheinen, </font></font><br></li></ul></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nachteile von Lidars: </font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hoher Stromverbrauch </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> relativ niedrige Bildrate </font></font><br></li><li> den laufenden Verschluss und die Notwendigkeit, ihn während der Verarbeitung zu kompensieren, <br></li><li>  In der Nähe befindliche Lidare stören sich gegenseitig, was nicht so einfach zu kompensieren ist. <br></li></ul></li><li>  Trotzdem beobachten wir, wie buchstäblich in 2 Jahren im Wesentlichen eine neue Art von Tiefenkameras mit hervorragenden Aussichten und einem riesigen potenziellen Markt entstanden ist.  In den kommenden Jahren können wir mit einer erheblichen Preissenkung rechnen, einschließlich des Auftretens kleiner universeller Lidars für Industrieroboter. <br></li></ul><br><h1>  Videoverarbeitung mit Tiefe </h1><br>  Überlegen Sie kurz, warum die Tiefe nicht so einfach zu handhaben ist. <br><br>  Im Folgenden finden Sie ein Beispiel für eingehende Rohdaten einer sehr guten Reihe von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Intel RealSense-Kameras</a> .  An den Fingern in diesem Video können Sie den Betrieb der Kamera und die Verarbeitungsalgorithmen relativ einfach „mit dem Auge“ bewerten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b2/319/b14/6b2319b143f48bdf0c57ffbe2acbc891.png"><br>  <i>Quelle: im Folgenden - die Materialien des Autors</i> <br><br>  Typische Probleme von Tiefenkameras sind deutlich sichtbar: <br><br><ul><li>  Daten sind im Wert instabil, Pixel in der Tiefe „Rauschen“. <br></li><li>  Entlang der Grenzen über eine ziemlich große Breite sind die Daten auch sehr verrauscht (aus diesem Grund maskieren sich die Tiefenwerte entlang der Grenzen oft einfach, um die Arbeit nicht zu beeinträchtigen). <br></li><li>  Um den Kopf herum sieht man viele "gehende" Pixel (anscheinend beginnt die Nähe zur Wand im Rücken + Haar zu beeinflussen). <br></li></ul><br>  Das ist aber noch nicht alles.  Wenn wir uns die Daten genauer ansehen, ist klar, dass an einigen Stellen die tieferen Daten die näheren „durchscheinen“: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a30/c5a/911/a30c5a911123f7a8f6ff8ebd8893163a.png"><br><br>  Dies liegt daran, dass das Bild zur Vereinfachung der Verarbeitung von der RGB-Kamera auf das Bild reduziert wird. Da die Grenzen nicht genau bestimmt werden, müssen solche „Überlappungen“ speziell verarbeitet werden, da sonst Probleme auftreten, wie z. B. solche „Löcher“ in der Tiefe des Arms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/22f/1a3/5df/22f1a35dfdb61edcabe8a1675f5c855f.png"><br><br>  Bei der Verarbeitung treten viele Probleme auf, zum Beispiel: <br><br><ul><li>  "Leckage" der Tiefe von einem Objekt zum anderen. <br></li><li>  Instabilität der Tiefe im Laufe der Zeit. <br></li><li>  Aliasing (Bildung einer "Leiter"), wenn die Unterdrückung von Tiefenrauschen zur Diskretisierung von Tiefenwerten führt: <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/e3d/7a4/c10/e3d7a4c10e13824f1d79815661e17773.png"><br><br>  Es ist jedoch klar, dass sich das Bild stark verbessert hat.  Beachten Sie, dass langsamere Algorithmen die Qualität immer noch erheblich verbessern können. <br><br>  Im Allgemeinen ist die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tiefenvideoverarbeitung</a> ein eigenständiges großes Thema, das sich bisher nicht mit so vielen Menschen befasst, aber rasch an Popularität gewinnt.  Der bisherige Markt, den sie komplett verändert hat, ist die Konvertierung von Filmen von 2D nach 3D.  Das Ergebnis der manuellen Erstellung von Stereo aus 2D-Videos verbesserte sich so schnell, zeigte ein viel vorhersehbareres Ergebnis, verursachte so weniger Kopfschmerzen und weniger Kosten, dass sie im 3D-Kino nach fast 100 Jahren Dreharbeiten (!) Ziemlich schnell fast vollständig aufhörten und nur mit der Konvertierung begannen .  Es gab sogar eine eigene Spezialität - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tiefenkünstler</a> , und die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Stereographen</a> der alten Schule standen unter Schock („ka-ak-a-ak?“).  Die Schlüsselrolle bei dieser Revolution spielte die rasche Verbesserung der Videoverarbeitungsalgorithmen.  Vielleicht werde ich eines Tages Zeit finden, eine separate Artikelserie darüber zu schreiben, weil mein Material in großen Mengen vorhanden ist. <br><br>  Etwa das Gleiche ist in naher Zukunft für Tiefenkameras von Robotern, Smartphones und autonomen Autos zu erwarten. <br><br>  <b>Wach auf!</b>  <b>Die Revolution kommt!</b> <br><br><h1>  Anstelle einer Schlussfolgerung </h1><br>  Vor ein paar Jahren brachte Ihr bescheidener Diener einen Vergleich verschiedener Ansätze, um Video mit Tiefe auf eine einzige Platte zu bringen.  Im Allgemeinen hat sich die Situation in dieser Zeit nicht geändert.  Die Trennung ist hier eher willkürlich, da sich die Hersteller der Nachteile von Technologien bewusst sind und versuchen, diese auszugleichen (manchmal äußerst erfolgreich).  Im Allgemeinen können jedoch verschiedene Ansätze verglichen werden: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/34d/491/11f/34d49111ff8005f743a89d9078864f1b.png"><br>  <i>Quelle: Materialien des Autors</i> <br><br>  Gehen wir über den Tisch: <br><br><ul><li>  <b>Bei der Auflösung liegt die</b> Tiefe von Stereo im Vordergrund, aber alles hängt sehr stark von der Szene ab (wenn es monophon ist, ist das Ding eine Pfeife). <br></li><li>  <b>In Bezug auf die Genauigkeit</b> sind Lidars unübertroffen, die Situation mit der Plenoptik ist übrigens die schlechteste. <br></li><li>  Aufgrund <b>der Komplexität der Verarbeitung erhalten</b> nur ToF und Lidars die „Tiefe“ direkt. Um die Tiefe von Stereo und Plenoptik zu erhalten, sind viele sehr nicht triviale Berechnungen erforderlich. <br></li><li>  <b>Laut FPS sind</b> ToF-Kameras strukturell gut und werden in naher Zukunft, wenn das Bügeleisen hochgezogen wird, von Stereokameras eingeholt (die bis zu 300 fps liefern können).  Lidaram träumt bisher nur davon. <br></li><li>  <b>Entsprechend den Ergebnissen bei schlechten Lichtverhältnissen</b> wird erwartet <b>, dass</b> Stereo und Plenoptik verlieren. <br></li><li>  <b>Bei Arbeiten im Freien</b> - ToF- und strukturierte Lichtkameras funktionieren nicht gut. <br></li></ul><br>  Gleichzeitig ist es wichtig, die Situation in bestimmten Märkten gut zu verstehen.  Zum Beispiel scheint der plenoptische Ansatz deutlich hinterherzuhinken.  Wenn sich jedoch herausstellt, dass es das Beste für AR ist (und dies wird in den nächsten drei bis vier Jahren deutlich werden), wird es seinen rechtmäßigen Platz in <i>jedem</i> Telefon und Tablet einnehmen.  Es zeigt sich auch, dass Festkörper-Lidars am besten aussehen (die „grünsten“), aber dies ist die jüngste Technologie, und dies sind ihre eigenen Risiken, insbesondere die patentierten (auf dem Gebiet gibt es viele neue Patente, die nicht bald ablaufen werden): <br><img src="https://habrastorage.org/getpro/habr/post_images/94e/905/f90/94e905f90cdd1e525f148f983c557b00.png"><br>  <i>Quelle: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LiDAR: Ein Überblick über die Patentlandschaft</a></i> <br><br>  Wir vergessen jedoch nicht, dass der Markt für Tiefensensoren für Smartphones für das nächste Jahr auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">6 Milliarden US-Dollar</a> geplant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ist</a> und nicht weniger als 4 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">US-Dollar betragen</a> darf, da er im vergangenen Jahr mehr als 3 Milliarden US-Dollar betrug.  Dies ist eine enorme Geldsumme, die nicht nur zur Kapitalrendite (für die erfolgreichsten Investoren), sondern auch zur Entwicklung neuer Sensorgenerationen verwendet wird.  Es gibt noch kein ähnliches Wachstum bei Lidars, aber nach allen Anzeichen wird es in den nächsten 3 Jahren buchstäblich gehen.  Und dann ist dies ein gewöhnlicher Lawinen-ähnlicher Exponentialprozess, der bereits mehr als einmal stattgefunden hat. <br><br>  Die nahe Zukunft der Tiefenkameras sieht äußerst interessant aus! <br><br>  <b><s>Karthago muss kaputt sein ... Das</s> ganze Video wird bis Ende des Jahrhunderts dreidimensional sein!</b> <br><br>  Bleib dran! <br><br>  Wer hat nicht gelesen - der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erste Teil des Textes</a> ! <br><br><div class="spoiler">  <b class="spoiler_title">Danksagung</b> <div class="spoiler_text">  Ich möchte mich herzlich bedanken bei: <br><ul><li>  Labor für Computergrafik VMK Moscow State University  MV Lomonosov für seinen Beitrag zur Entwicklung der Computergrafik in Russland und nicht nur <br></li><li>  Google, Apple und Intel für großartige kompakte Lösungen und alle Lidar-Hersteller für schnelle Fortschritte. <br></li><li>  persönlich Konstantin Kozhemyakov, der viel getan hat, um diesen Artikel besser und visueller zu machen, <br></li><li>  und schließlich vielen Dank an Roman Kazantsev, Eugene Lyapustin und Yegor Sklyarov für eine große Anzahl vernünftiger Kommentare und Korrekturen, die diesen Text viel besser gemacht haben! <br></li></ul><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de458458/">https://habr.com/ru/post/de458458/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de458442/index.html">Warum ruft LLVM möglicherweise eine nie aufgerufene Funktion auf?</a></li>
<li><a href="../de458444/index.html">Internet für den Sommerbewohner. Teil 4. Eine SIM-Karte reicht aus</a></li>
<li><a href="../de458446/index.html">Hyperscale-Rechenzentren: Wer baut sie und wie viel kosten sie?</a></li>
<li><a href="../de458450/index.html">Eigenschaften von Quantencomputern</a></li>
<li><a href="../de458454/index.html">Alexey Savvateev: Modelle des Internets und sozialer Netzwerke</a></li>
<li><a href="../de458460/index.html">Nachrichten aus der Welt von OpenStreetMap Nr. 466 (18.06.2019 - 26.06.2019)</a></li>
<li><a href="../de458462/index.html">Tauchen Sie tief in Linux-Namespaces ein</a></li>
<li><a href="../de458464/index.html">3 km Gigabit-Verbindung auf Lasermodems</a></li>
<li><a href="../de458468/index.html">Wie führe ich ein beeindruckendes Kanban-StandUp-Meeting durch?</a></li>
<li><a href="../de458470/index.html">Texturierung oder was Sie wissen müssen, um ein Oberflächenkünstler zu werden. Teil 2. Masken und Texturen</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>