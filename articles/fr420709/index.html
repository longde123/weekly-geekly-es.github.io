<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòÖ üë©üèæ‚Äçüéì üéâ T2F: un projet de conversion de texte en dessin facial avec deep learning üèÇüèø üìç üë®‚Äçüîß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le code du projet est disponible dans le r√©f√©rentiel. 

 Pr√©sentation 
 Quand j'ai lu les descriptions de l'apparence des personnages dans les livres,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>T2F: un projet de conversion de texte en dessin facial avec deep learning</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420709/"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/703/0df/5ae7030df8270466b01b81aad0ace49f.jpg"><br><br>  <i>Le code du projet est disponible dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">r√©f√©rentiel.</a></i> <br><br><h2>  Pr√©sentation </h2><br>  Quand j'ai lu les descriptions de l'apparence des personnages dans les livres, j'ai toujours √©t√© int√©ress√© par leur apparence dans la vie.  Il est tout √† fait possible d'imaginer une personne dans son ensemble, mais la description des d√©tails les plus visibles est une t√¢che difficile, et les r√©sultats varient d'une personne √† l'autre.  Plusieurs fois, je ne pouvais rien imaginer d'autre qu'un visage tr√®s flou du personnage jusqu'√† la fin du travail.  Ce n'est que lorsque le livre est transform√© en film que le visage flou se remplit de d√©tails.  Par exemple, je ne pourrais jamais imaginer √† quoi ressemble le visage de Rachel dans le livre " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Girl on the Train</a> ".  Mais quand le film est sorti, j'ai pu faire correspondre le visage d'Emily Blunt avec le personnage de Rachel.  Certes, les personnes impliqu√©es dans la s√©lection des acteurs prennent beaucoup de temps pour repr√©senter correctement les personnages du script. <br><a name="habracut"></a><br>  Ce probl√®me m'a inspir√© et m'a motiv√© √† trouver une solution.  Apr√®s cela, j'ai commenc√© √† √©tudier la litt√©rature sur l'apprentissage profond √† la recherche de quelque chose de similaire.  Heureusement, il y a eu pas mal d'√©tudes sur la synth√®se d'images √† partir de texte.  Voici certains de ceux sur lesquels j'ai b√¢ti: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1605.05396</a> ¬´Synth√®se de textes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">oppos√©s</a> g√©n√©ratifs √† la synth√®se d'images¬ª </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1612.03242</a> ¬´StackGAN: Synth√®se de texte en image photo-r√©aliste avec des r√©seaux contradictoires g√©n√©ratifs empil√©s¬ª </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1710.10916</a> ¬´StackGAN ++: synth√®se d'images r√©aliste avec des r√©seaux contradictoires g√©n√©ratifs empil√©s¬ª </li></ul><br>  [les <i>projets utilisent des r√©seaux contradictoires g√©n√©ratifs, GSS (Generative adversarial network, GAN) / env.</i>  <i>perev.</i>  ] <br><br>  Apr√®s avoir √©tudi√© la litt√©rature, j'ai choisi une architecture qui a √©t√© simplifi√©e par rapport √† StackGAN ++ et qui r√©sout assez bien mon probl√®me.  Dans les sections suivantes, je vais expliquer comment j'ai r√©solu ce probl√®me et partager les r√©sultats pr√©liminaires.  Je d√©crirai √©galement certains des d√©tails de la programmation et de la formation sur lesquels j'ai pass√© beaucoup de temps. <br><br><h2>  Analyse des donn√©es </h2><br>  Sans aucun doute, l'aspect le plus important du travail est les donn√©es utilis√©es pour former le mod√®le.  Comme l'a dit le professeur Andrew Eun dans ses cours deeplearning.ai: ¬´Dans le domaine de l'apprentissage automatique, ce n'est pas celui qui a le meilleur algorithme, mais celui qui a les meilleures donn√©es.¬ª  C'est ainsi qu'a commenc√© ma recherche d'un ensemble de donn√©es sur les visages avec de bonnes, riches et diverses descriptions textuelles.  Je suis tomb√© sur diff√©rents ensembles de donn√©es - soit des visages, soit des visages avec des noms, soit des visages avec une description de la couleur et de la forme des yeux.  Mais il n'y en avait pas dont j'avais besoin.  Ma derni√®re option √©tait d'utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un premier projet</a> - g√©n√©rer une description des donn√©es structurelles dans un langage naturel.  Mais une telle option ajouterait du bruit suppl√©mentaire √† un ensemble de donn√©es d√©j√† assez bruyant. <br><br>  Le temps a pass√© et √† un moment donn√©, un nouveau projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Face2Text est apparu</a> .  Il s'agissait d'une collection d'une base de donn√©es de descriptions textuelles d√©taill√©es de personnes.  Je remercie les auteurs du projet pour l'ensemble de donn√©es fourni. <br><br>  L'ensemble de donn√©es contenait des descriptions textuelles de 400 images s√©lectionn√©es au hasard dans la base de donn√©es LFW (faces √©tiquet√©es).  Les descriptions ont √©t√© nettoy√©es pour √©liminer les caract√©ristiques ambigu√´s et mineures.  Certaines descriptions contenaient non seulement des informations sur les visages, mais √©galement des conclusions tir√©es sur la base des images - par exemple, ¬´la personne sur la photo est probablement un criminel¬ª.  Tous ces facteurs, ainsi que la petite taille de l'ensemble de donn√©es, ont conduit au fait que mon projet jusqu'√† pr√©sent ne d√©montre que des preuves de l'op√©rabilit√© de l'architecture.  Par la suite, ce mod√®le peut √™tre adapt√© √† un ensemble de donn√©es plus grand et plus diversifi√©. <br><br><h2>  L'architecture </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c70/ea1/6de/c70ea16de2bbfb674e618fa556cfc9ff.jpg"><br><br>  L'architecture du projet T2F combine deux architectures stackGAN pour l'encodage de texte incr√©ment√© conditionnellement, et ProGAN ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">croissance GSS progressive</a> ) pour la synth√®se d'images de visage.  L'architecture stackgan ++ originale utilisait plusieurs GSS avec diff√©rentes r√©solutions spatiales, et j'ai d√©cid√© que c'√©tait une approche trop s√©rieuse pour toute t√¢che de distribution de correspondance.  Mais ProGAN n'utilise qu'un seul GSS, progressivement entra√Æn√© √† des r√©solutions toujours plus d√©taill√©es.  J'ai d√©cid√© de combiner ces deux approches. <br><br>  Il y a une explication du flux de donn√©es: les descriptions de texte sont encod√©es dans le vecteur final par int√©gration dans le r√©seau LSTM (Embedding) (psy_t) (voir sch√©ma).  Ensuite, l'incorporation est transmise via le bloc d'augmentation de conditionnement (une couche lin√©aire) pour obtenir la partie texte du vecteur propre (en utilisant la technique de reparam√©trisation VAE) pour le GSS en entr√©e.  La deuxi√®me partie du vecteur propre est le bruit gaussien al√©atoire.  Le vecteur propre r√©sultant est envoy√© au g√©n√©rateur GSS, et l'incorporation est envoy√©e √† la derni√®re couche discriminante pour une distribution conditionnelle de la correspondance.  La formation des processus GSS se d√©roule exactement comme dans l'article sur ProGAN - en couches, avec une augmentation de la r√©solution spatiale.  Une nouvelle couche est introduite √† l'aide de la technique de fondu pour √©viter d'effacer les r√©sultats d'apprentissage pr√©c√©dents. <br><br><h2>  Mise en ≈ìuvre et autres d√©tails </h2><br>  L'application a √©t√© √©crite en python en utilisant le framework PyTorch.  J'avais l'habitude de travailler avec des packages tensorflow et keras, mais maintenant je voulais essayer PyTorch.  J'ai aim√© utiliser le d√©bogueur python int√©gr√© pour travailler avec l'architecture r√©seau - tout cela gr√¢ce √† la strat√©gie d'ex√©cution pr√©coce.  Tensorflow a √©galement r√©cemment activ√© le mode d'ex√©cution impatient.  Cependant, je ne veux pas juger quel cadre est le meilleur, je veux juste souligner que le code de ce projet a √©t√© √©crit en utilisant PyTorch. <br><br>  Plusieurs parties du projet me semblent r√©utilisables, notamment ProGAN.  Par cons√©quent, j'ai √©crit un code s√©par√© pour eux en tant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">qu'extension du</a> module PyTorch, et il peut √©galement √™tre utilis√© sur d'autres ensembles de donn√©es.  Il suffit d'indiquer la profondeur et la taille des caract√©ristiques de l'ESG.  GSS peut √™tre form√© progressivement pour tout ensemble de donn√©es. <br><br><h2>  D√©tails de la formation </h2><br>  J'ai form√© plusieurs versions du r√©seau en utilisant diff√©rents hyperparam√®tres.  Les d√©tails du travail sont les suivants: <br><br><ol><li>  Le discriminateur n'a pas d'op√©rations par lot ou par couche, donc la perte de WGAN-GP peut cro√Ætre de fa√ßon explosive.  J'ai utilis√© une p√©nalit√© de d√©rive avec lambda √©gale √† 0,001. </li><li>  Pour contr√¥ler votre propre diversit√©, obtenue √† partir du texte encod√©, il est n√©cessaire d'utiliser la distance Kullback - Leibler dans les pertes du g√©n√©rateur. </li><li>  Pour que les images r√©sultantes correspondent mieux √† la distribution de texte entrante, il est pr√©f√©rable d'utiliser la version WGAN du discriminateur (Matching-Aware) correspondant. </li><li>  Le temps de fondu pour les niveaux sup√©rieurs doit d√©passer le temps de fondu pour les niveaux inf√©rieurs.  J'ai utilis√© 85% comme valeur de fondu lors de l'entra√Ænement. </li><li>  J'ai trouv√© que les exemples de r√©solution sup√©rieure (32 x 32 et 64 x 64) produisent plus de bruit de fond que les exemples de r√©solution inf√©rieure.  Je pense que cela est d√ª au manque de donn√©es. </li><li>  Pendant un entra√Ænement progressif, il est pr√©f√©rable de passer plus de temps sur des r√©solutions plus basses et de r√©duire le temps pass√© √† travailler avec des r√©solutions plus √©lev√©es. </li></ol><br>  La vid√©o montre le laps de temps du g√©n√©rateur.  La vid√©o est compil√©e √† partir d'images de diff√©rentes r√©solutions spatiales obtenues lors de la formation du GSS. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO_l87rPDb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Conclusion </h2><br>  Selon les r√©sultats pr√©liminaires, on peut juger que le projet T2F est r√©alisable et a des applications int√©ressantes.  Supposons qu'il puisse √™tre utilis√© pour composer des photobots.  Ou pour les cas o√π il faut booster l'imagination.  Je continuerai √† travailler sur la mise √† l'√©chelle de ce projet sur des ensembles de donn√©es tels que Flicker8K, les l√©gendes Coco, etc. <br><br>  La croissance progressive de l'ESG est une technologie ph√©nom√©nale pour une formation GSS plus rapide et plus stable.  Il peut √™tre combin√© avec diverses technologies modernes mentionn√©es dans d'autres articles.  GSS peut √™tre utilis√© dans diff√©rents domaines de MO. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420709/">https://habr.com/ru/post/fr420709/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420697/index.html">Le th√®me √©ternel avec PHP et MySQL</a></li>
<li><a href="../fr420701/index.html">HSBI invite √† une soir√©e conf√©rence sur la conception de jeux le 29 ao√ªt</a></li>
<li><a href="../fr420703/index.html">Synopsis du livre ¬´N√©gociations sans d√©faite. M√©thode Harvard</a></li>
<li><a href="../fr420705/index.html">8 id√©es profondes de la tribu des mentors de Tim Ferris</a></li>
<li><a href="../fr420707/index.html">La startup JITX utilise l'IA pour automatiser le d√©veloppement de cartes de circuits imprim√©s complexes</a></li>
<li><a href="../fr420711/index.html">Moscou Data Science Major: annonce et inscription</a></li>
<li><a href="../fr420713/index.html">Comment Chuck Hull a invent√© l'impression 3D</a></li>
<li><a href="../fr420715/index.html">La dure v√©rit√© sur la gravit√© de l'apprentissage</a></li>
<li><a href="../fr420725/index.html">Comment j'ai appris √† l'IA √† jouer √† Tetris pour NES. Partie 1: analyse du code du jeu</a></li>
<li><a href="../fr420729/index.html">Webinaire ouvert "Naive Bayes Classifier"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>