<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>😅 👩🏾‍🎓 🎉 T2F: un projet de conversion de texte en dessin facial avec deep learning 🏂🏿 📍 👨‍🔧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Le code du projet est disponible dans le référentiel. 

 Présentation 
 Quand j'ai lu les descriptions de l'apparence des personnages dans les livres,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>T2F: un projet de conversion de texte en dessin facial avec deep learning</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/420709/"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/703/0df/5ae7030df8270466b01b81aad0ace49f.jpg"><br><br>  <i>Le code du projet est disponible dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">référentiel.</a></i> <br><br><h2>  Présentation </h2><br>  Quand j'ai lu les descriptions de l'apparence des personnages dans les livres, j'ai toujours été intéressé par leur apparence dans la vie.  Il est tout à fait possible d'imaginer une personne dans son ensemble, mais la description des détails les plus visibles est une tâche difficile, et les résultats varient d'une personne à l'autre.  Plusieurs fois, je ne pouvais rien imaginer d'autre qu'un visage très flou du personnage jusqu'à la fin du travail.  Ce n'est que lorsque le livre est transformé en film que le visage flou se remplit de détails.  Par exemple, je ne pourrais jamais imaginer à quoi ressemble le visage de Rachel dans le livre " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Girl on the Train</a> ".  Mais quand le film est sorti, j'ai pu faire correspondre le visage d'Emily Blunt avec le personnage de Rachel.  Certes, les personnes impliquées dans la sélection des acteurs prennent beaucoup de temps pour représenter correctement les personnages du script. <br><a name="habracut"></a><br>  Ce problème m'a inspiré et m'a motivé à trouver une solution.  Après cela, j'ai commencé à étudier la littérature sur l'apprentissage profond à la recherche de quelque chose de similaire.  Heureusement, il y a eu pas mal d'études sur la synthèse d'images à partir de texte.  Voici certains de ceux sur lesquels j'ai bâti: <br><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1605.05396</a> «Synthèse de textes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">opposés</a> génératifs à la synthèse d'images» </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1612.03242</a> «StackGAN: Synthèse de texte en image photo-réaliste avec des réseaux contradictoires génératifs empilés» </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">arxiv.org/abs/1710.10916</a> «StackGAN ++: synthèse d'images réaliste avec des réseaux contradictoires génératifs empilés» </li></ul><br>  [les <i>projets utilisent des réseaux contradictoires génératifs, GSS (Generative adversarial network, GAN) / env.</i>  <i>perev.</i>  ] <br><br>  Après avoir étudié la littérature, j'ai choisi une architecture qui a été simplifiée par rapport à StackGAN ++ et qui résout assez bien mon problème.  Dans les sections suivantes, je vais expliquer comment j'ai résolu ce problème et partager les résultats préliminaires.  Je décrirai également certains des détails de la programmation et de la formation sur lesquels j'ai passé beaucoup de temps. <br><br><h2>  Analyse des données </h2><br>  Sans aucun doute, l'aspect le plus important du travail est les données utilisées pour former le modèle.  Comme l'a dit le professeur Andrew Eun dans ses cours deeplearning.ai: «Dans le domaine de l'apprentissage automatique, ce n'est pas celui qui a le meilleur algorithme, mais celui qui a les meilleures données.»  C'est ainsi qu'a commencé ma recherche d'un ensemble de données sur les visages avec de bonnes, riches et diverses descriptions textuelles.  Je suis tombé sur différents ensembles de données - soit des visages, soit des visages avec des noms, soit des visages avec une description de la couleur et de la forme des yeux.  Mais il n'y en avait pas dont j'avais besoin.  Ma dernière option était d'utiliser <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">un premier projet</a> - générer une description des données structurelles dans un langage naturel.  Mais une telle option ajouterait du bruit supplémentaire à un ensemble de données déjà assez bruyant. <br><br>  Le temps a passé et à un moment donné, un nouveau projet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Face2Text est apparu</a> .  Il s'agissait d'une collection d'une base de données de descriptions textuelles détaillées de personnes.  Je remercie les auteurs du projet pour l'ensemble de données fourni. <br><br>  L'ensemble de données contenait des descriptions textuelles de 400 images sélectionnées au hasard dans la base de données LFW (faces étiquetées).  Les descriptions ont été nettoyées pour éliminer les caractéristiques ambiguës et mineures.  Certaines descriptions contenaient non seulement des informations sur les visages, mais également des conclusions tirées sur la base des images - par exemple, «la personne sur la photo est probablement un criminel».  Tous ces facteurs, ainsi que la petite taille de l'ensemble de données, ont conduit au fait que mon projet jusqu'à présent ne démontre que des preuves de l'opérabilité de l'architecture.  Par la suite, ce modèle peut être adapté à un ensemble de données plus grand et plus diversifié. <br><br><h2>  L'architecture </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c70/ea1/6de/c70ea16de2bbfb674e618fa556cfc9ff.jpg"><br><br>  L'architecture du projet T2F combine deux architectures stackGAN pour l'encodage de texte incrémenté conditionnellement, et ProGAN ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">croissance GSS progressive</a> ) pour la synthèse d'images de visage.  L'architecture stackgan ++ originale utilisait plusieurs GSS avec différentes résolutions spatiales, et j'ai décidé que c'était une approche trop sérieuse pour toute tâche de distribution de correspondance.  Mais ProGAN n'utilise qu'un seul GSS, progressivement entraîné à des résolutions toujours plus détaillées.  J'ai décidé de combiner ces deux approches. <br><br>  Il y a une explication du flux de données: les descriptions de texte sont encodées dans le vecteur final par intégration dans le réseau LSTM (Embedding) (psy_t) (voir schéma).  Ensuite, l'incorporation est transmise via le bloc d'augmentation de conditionnement (une couche linéaire) pour obtenir la partie texte du vecteur propre (en utilisant la technique de reparamétrisation VAE) pour le GSS en entrée.  La deuxième partie du vecteur propre est le bruit gaussien aléatoire.  Le vecteur propre résultant est envoyé au générateur GSS, et l'incorporation est envoyée à la dernière couche discriminante pour une distribution conditionnelle de la correspondance.  La formation des processus GSS se déroule exactement comme dans l'article sur ProGAN - en couches, avec une augmentation de la résolution spatiale.  Une nouvelle couche est introduite à l'aide de la technique de fondu pour éviter d'effacer les résultats d'apprentissage précédents. <br><br><h2>  Mise en œuvre et autres détails </h2><br>  L'application a été écrite en python en utilisant le framework PyTorch.  J'avais l'habitude de travailler avec des packages tensorflow et keras, mais maintenant je voulais essayer PyTorch.  J'ai aimé utiliser le débogueur python intégré pour travailler avec l'architecture réseau - tout cela grâce à la stratégie d'exécution précoce.  Tensorflow a également récemment activé le mode d'exécution impatient.  Cependant, je ne veux pas juger quel cadre est le meilleur, je veux juste souligner que le code de ce projet a été écrit en utilisant PyTorch. <br><br>  Plusieurs parties du projet me semblent réutilisables, notamment ProGAN.  Par conséquent, j'ai écrit un code séparé pour eux en tant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">qu'extension du</a> module PyTorch, et il peut également être utilisé sur d'autres ensembles de données.  Il suffit d'indiquer la profondeur et la taille des caractéristiques de l'ESG.  GSS peut être formé progressivement pour tout ensemble de données. <br><br><h2>  Détails de la formation </h2><br>  J'ai formé plusieurs versions du réseau en utilisant différents hyperparamètres.  Les détails du travail sont les suivants: <br><br><ol><li>  Le discriminateur n'a pas d'opérations par lot ou par couche, donc la perte de WGAN-GP peut croître de façon explosive.  J'ai utilisé une pénalité de dérive avec lambda égale à 0,001. </li><li>  Pour contrôler votre propre diversité, obtenue à partir du texte encodé, il est nécessaire d'utiliser la distance Kullback - Leibler dans les pertes du générateur. </li><li>  Pour que les images résultantes correspondent mieux à la distribution de texte entrante, il est préférable d'utiliser la version WGAN du discriminateur (Matching-Aware) correspondant. </li><li>  Le temps de fondu pour les niveaux supérieurs doit dépasser le temps de fondu pour les niveaux inférieurs.  J'ai utilisé 85% comme valeur de fondu lors de l'entraînement. </li><li>  J'ai trouvé que les exemples de résolution supérieure (32 x 32 et 64 x 64) produisent plus de bruit de fond que les exemples de résolution inférieure.  Je pense que cela est dû au manque de données. </li><li>  Pendant un entraînement progressif, il est préférable de passer plus de temps sur des résolutions plus basses et de réduire le temps passé à travailler avec des résolutions plus élevées. </li></ol><br>  La vidéo montre le laps de temps du générateur.  La vidéo est compilée à partir d'images de différentes résolutions spatiales obtenues lors de la formation du GSS. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NO_l87rPDb8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2>  Conclusion </h2><br>  Selon les résultats préliminaires, on peut juger que le projet T2F est réalisable et a des applications intéressantes.  Supposons qu'il puisse être utilisé pour composer des photobots.  Ou pour les cas où il faut booster l'imagination.  Je continuerai à travailler sur la mise à l'échelle de ce projet sur des ensembles de données tels que Flicker8K, les légendes Coco, etc. <br><br>  La croissance progressive de l'ESG est une technologie phénoménale pour une formation GSS plus rapide et plus stable.  Il peut être combiné avec diverses technologies modernes mentionnées dans d'autres articles.  GSS peut être utilisé dans différents domaines de MO. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr420709/">https://habr.com/ru/post/fr420709/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr420697/index.html">Le thème éternel avec PHP et MySQL</a></li>
<li><a href="../fr420701/index.html">HSBI invite à une soirée conférence sur la conception de jeux le 29 août</a></li>
<li><a href="../fr420703/index.html">Synopsis du livre «Négociations sans défaite. Méthode Harvard</a></li>
<li><a href="../fr420705/index.html">8 idées profondes de la tribu des mentors de Tim Ferris</a></li>
<li><a href="../fr420707/index.html">La startup JITX utilise l'IA pour automatiser le développement de cartes de circuits imprimés complexes</a></li>
<li><a href="../fr420711/index.html">Moscou Data Science Major: annonce et inscription</a></li>
<li><a href="../fr420713/index.html">Comment Chuck Hull a inventé l'impression 3D</a></li>
<li><a href="../fr420715/index.html">La dure vérité sur la gravité de l'apprentissage</a></li>
<li><a href="../fr420725/index.html">Comment j'ai appris à l'IA à jouer à Tetris pour NES. Partie 1: analyse du code du jeu</a></li>
<li><a href="../fr420729/index.html">Webinaire ouvert "Naive Bayes Classifier"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>