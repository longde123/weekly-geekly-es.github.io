<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîñ üôèüèæ üóëÔ∏è Codage d'effacement Glusterfs +: quand vous en avez besoin, pas cher et fiable üßôüèª üë®üèª‚Äçüç≥ üÜí</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Peu de gens ont un Glaster en Russie, et toute exp√©rience est int√©ressante. Nous l'avons grand et industriel et, √† en juger par la discussion dans le ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Codage d'effacement Glusterfs +: quand vous en avez besoin, pas cher et fiable</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/croccloudteam/blog/417475/">  Peu de gens ont un Glaster en Russie, et toute exp√©rience est int√©ressante.  Nous l'avons grand et industriel et, √† en juger par la discussion dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dernier post</a> , en demande.  J'ai parl√© du tout d√©but de l'exp√©rience de la migration des sauvegardes du stockage d'entreprise vers Glusterfs. <br><br>  Ce n'est pas assez hardcore.  Nous ne nous sommes pas arr√™t√©s et avons d√©cid√© de collecter quelque chose de plus s√©rieux.  Par cons√©quent, nous parlerons ici de choses telles que le codage d'effacement, le sharding, le r√©√©quilibrage et sa limitation, les tests de r√©sistance, etc. <br><br><img src="https://habrastorage.org/webt/8t/ol/2d/8tol2dsnki7fr_jfcvhxwldsxdk.jpeg"><br><br><ul><li>  Plus de th√©orie du volume / sous-volume </li><li>  pi√®ce de rechange chaude </li><li>  gu√©rir / gu√©rir complet / r√©√©quilibrer </li><li>  Conclusions apr√®s le red√©marrage de 3 n≈ìuds (ne faites jamais cela) </li><li>  Comment l'enregistrement √† diff√©rentes vitesses √† partir de diff√©rentes machines virtuelles et l'activation / la d√©sactivation des fragments affectent-ils la charge du sous-volume? </li><li>  r√©√©quilibrage apr√®s le d√©part du disque </li><li>  r√©√©quilibrage rapide </li></ul><br><a name="habracut"></a><h3>  Que vouliez-vous </h3><br>  <b>La t√¢che est simple:</b> collecter un magasin bon march√© mais fiable.  Moins cher que possible, fiable - de sorte qu'il ne serait pas effrayant de stocker nos propres fichiers √† vendre.  Au revoir.  Ensuite, apr√®s de longs tests et sauvegardes sur un autre syst√®me de stockage - √©galement client. <br><br>  <b>Application (IO s√©quentielle)</b> : <br><br>  - Sauvegardes <br>  - Tester les infrastructures <br>  - Testez le stockage des fichiers multim√©dias lourds. <br>  Nous y sommes. <br>  - Fichier de bataille et infrastructure de test s√©rieuse <br>  - Stockage des donn√©es importantes. <br><br>  Comme la derni√®re fois, la principale exigence est la vitesse du r√©seau entre les instances de Glaster.  10G au d√©but, c'est bien. <br><br><h3>  Th√©orie: qu'est-ce que le volume dispers√©? </h3><br>  Le volume dispers√© est bas√© sur la technologie de codage d'effacement (EC), qui fournit une protection assez efficace contre les pannes de disque ou de serveur.  C'est comme RAID 5 ou 6, mais pas vraiment.  Il stocke le fragment cod√© du fichier pour chaque brique de telle mani√®re que seul un sous-ensemble des fragments stock√©s dans les briks restants est requis pour restaurer le fichier.  Le nombre de briques indisponibles sans perte d'acc√®s aux donn√©es est configur√© par l'administrateur lors de la cr√©ation du volume. <br><br><img src="https://habrastorage.org/webt/rt/br/t1/rtbrt12s-0oyc9avxlsp32qzsus.png"><br><br><h3>  Qu'est-ce qu'un sous-volume? </h3><br>  L'essence du sous-volume dans la terminologie de GlusterFS se manifeste avec les volumes distribu√©s.  En effacement distribu√©-dispers√©, le codage ne fonctionnera que dans le cadre du subwoofer.  Et dans le cas, par exemple, avec des donn√©es r√©pliqu√©es distribu√©es seront r√©pliqu√©es dans le cadre du subwoofer. <br>  Chacun d'eux est distribu√© sur des serveurs diff√©rents, ce qui leur permet de perdre librement ou de se synchroniser.  Sur la figure, les serveurs (physiques) sont marqu√©s en vert, les sous-loups sont en pointill√©s.  Chacun d'eux est pr√©sent√© comme un disque (volume) au serveur d'applications: <br><br><img src="https://habrastorage.org/webt/w-/fp/dj/w-fpdjqguwigusvhtprq_6su3z8.png"><br><br>  Il a √©t√© d√©cid√© que la configuration 4 + 2 distribu√©e-dispers√©e sur 6 n≈ìuds semble assez fiable, nous pouvons perdre 2 serveurs ou 2 disques dans chaque caisson de basses, tout en continuant d'avoir acc√®s aux donn√©es. <br><br>  Nous avions √† notre disposition 6 anciens DELL PowerEdge R510 avec 12 emplacements de disque et 48 disques SATA de 48x2 To.  En principe, s'il existe un serveur avec 12 emplacements de disque et ayant jusqu'√† 12 To de disques sur le march√©, nous pouvons collecter jusqu'√† 576 To d'espace de stockage utilisable.  Mais n'oubliez pas que m√™me si les tailles maximales de disques durs continuent de cro√Ætre d'ann√©e en ann√©e, leurs performances restent immobiles et une reconstruction d'un disque de 10 √† 12 To peut vous prendre une semaine. <br><br><img src="https://habrastorage.org/webt/xo/ud/6f/xoud6fl7sdtknj7vrbn_5fgslpu.png"><br><br>  <b>Cr√©ation de volume:</b> <br>  Une description d√©taill√©e de la fa√ßon de pr√©parer des briques, vous pouvez lire dans mon <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">post pr√©c√©dent</a> <br><br><pre><code class="bash hljs">gluster volume create freezer disperse-data 4 redundancy 2 transport tcp \ $(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..7} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> {sl051s,sl052s,sl053s,sl064s,sl075s,sl078s}:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/freezer ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>)</code> </pre> <br>  Nous cr√©ons, mais nous ne sommes pas press√©s de lancer et de monter, car nous devons encore appliquer plusieurs param√®tres importants. <br><br>  <b>Ce que nous avons obtenu:</b> <br><br><img src="https://habrastorage.org/webt/8j/il/e9/8jile9swj-qws3hgdjo3gtht3oo.png"><br><br>  Tout semble tout √† fait normal, mais il y a une mise en garde. <br><br>  <b>Il consiste √† enregistrer un tel volume sur les briques:</b> <br>  Les fichiers sont plac√©s un par un dans les sous-loups et ne sont pas r√©partis uniform√©ment entre eux.Par cons√©quent, t√¥t ou tard, nous rencontrerons sa taille, et non la taille de tout le volume.  La taille de fichier maximale que nous pouvons mettre dans ce r√©f√©rentiel est la taille utilisable du subwoofer moins l'espace d√©j√† occup√© dessus.  Dans mon cas, c'est &lt;8 To. <br><br>  <b>Que faire?</b>  <b>Comment √™tre?</b> <br>  Ce probl√®me est r√©solu par le partitionnement ou le volume de bande, mais, comme la pratique l'a montr√©, la bande fonctionne tr√®s mal. <br><br>  Par cons√©quent, nous allons essayer de partager. <br><br>  <b>Qu'est-ce que le sharding, en d√©tail</b> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ici</a> . <br><br>  <b>Qu'est-ce que le sharding, en bref</b> : <br>  Chaque fichier que vous placez dans un volume sera divis√© en parties (fragments), qui sont dispos√©es de mani√®re relativement uniforme en sous-loups.  La taille du fragment est sp√©cifi√©e par l'administrateur, la valeur standard est de 4 Mo. <br><br>  <b>Activez le partitionnement apr√®s avoir cr√©√© un volume, mais avant qu'il ne commence</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard on</code> </pre> <br>  <b>Nous d√©finissons la taille du fragment (ce qui est optimal? Les mecs de oVirt recommandent 512 Mo)</b> : <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> freezer features.shard-block-size 512MB</code> </pre> <br>  Empiriquement, il s'av√®re que la taille r√©elle du fragment dans les briques lors de l'utilisation du volume dispers√© 4 + 2 est √©gale √† shard-block-size / 4, dans notre cas 512M / 4 = 128M. <br><br>  Chaque √©clat selon la logique de codage d'effacement est d√©compos√© selon les briques dans le cadre du sous-monde avec ces pi√®ces: 4 * 128M + 2 * 128M <br><br>  <b>Dessinez les cas de d√©faillance qui survivent avec cette configuration:</b> <br>  Dans cette configuration, nous pouvons survivre √† la chute de 2 n≈ìuds ou de 2 disques dans le m√™me sous-volume. <br><br>  Pour les tests, nous avons d√©cid√© de glisser le stockage r√©sultant sous notre cloud et d'ex√©cuter fio √† partir de machines virtuelles. <br><br>  Nous activons l'enregistrement s√©quentiel √† partir de 15 machines virtuelles et proc√©dons comme suit. <br><br>  <b>Red√©marrage du 1er n≈ìud:</b> <br>  17:09 <br>  Il ne semble pas critique (~ 5 secondes d'indisponibilit√© par le param√®tre ping.timeout). <br><br>  17:19 <br>  Lanc√© gu√©rir compl√®tement. <br>  Le nombre d'entr√©es de soins ne fait qu'augmenter, probablement en raison du niveau √©lev√© d'√©criture dans le cluster. <br><br>  17:32 <br>  Il a √©t√© d√©cid√© de d√©sactiver l'enregistrement √† partir de la machine virtuelle. <br>  Le nombre d'entr√©es de soins a commenc√© √† diminuer. <br><br>  17:50 <br>  gu√©rir fait. <br><br>  <b>Red√©marrez 2 n≈ìuds:</b> <br><br>  <i>Les m√™mes r√©sultats sont observ√©s qu'avec le 1er n≈ìud.</i> <br><br>  <b>Red√©marrez 3 n≈ìuds:</b> <br>  <i>Le point d'extr√©mit√© de transport √©mis par le point de montage n'est pas connect√©, les machines virtuelles ont re√ßu une erreur io.</i> <i><br></i>  <i>Apr√®s avoir allum√© les n≈ìuds, le Glaster s'est r√©tabli, sans interf√©rence de notre c√¥t√©, et le processus de traitement a commenc√©.</i> <br><br>  Mais 4 machines virtuelles sur 15 n'ont pas pu augmenter.  J'ai vu des erreurs sur l'hyperviseur: <br><br><pre> <code class="bash hljs">2018.04.27 13:21:32.719 ( volumes.py:0029): I: Attaching volume vol-BA3A1BE1 (/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1) with attach <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> generic... 2018.04.27 13:21:32.721 ( qmp.py:0166): D: Querying QEMU: __com.redhat_drive_add({<span class="hljs-string"><span class="hljs-string">'file'</span></span>: u<span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_rd'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'media'</span></span>: <span class="hljs-string"><span class="hljs-string">'disk'</span></span>, <span class="hljs-string"><span class="hljs-string">'format'</span></span>: <span class="hljs-string"><span class="hljs-string">'qcow2'</span></span>, <span class="hljs-string"><span class="hljs-string">'cache'</span></span>: <span class="hljs-string"><span class="hljs-string">'none'</span></span>, <span class="hljs-string"><span class="hljs-string">'detect-zeroes'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>, <span class="hljs-string"><span class="hljs-string">'id'</span></span>: <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span>, <span class="hljs-string"><span class="hljs-string">'iops_wr'</span></span>: 400, <span class="hljs-string"><span class="hljs-string">'discard'</span></span>: <span class="hljs-string"><span class="hljs-string">'unmap'</span></span>})... 2018.04.27 13:21:32.784 ( instance.py:0298): E: Failed to attach volume vol-BA3A1BE1 to the instance: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized Traceback (most recent call last): File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/ic/instance.py"</span></span>, line 292, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> emulation_started c2.qemu.volumes.attach(controller.qemu(), device) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/volumes.py"</span></span>, line 36, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> attach c2.qemu.query(qemu, drive_meth, drive_args) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/_init_.py"</span></span>, line 247, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query <span class="hljs-built_in"><span class="hljs-built_in">return</span></span> c2.qemu.qmp.query(qemu.pending_messages, qemu.qmp_socket, <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>, args, suppress_logging) File <span class="hljs-string"><span class="hljs-string">"/usr/lib64/python2.7/site-packages/c2/qemu/qmp.py"</span></span>, line 194, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query message[<span class="hljs-string"><span class="hljs-string">"error"</span></span>].get(<span class="hljs-string"><span class="hljs-string">"desc"</span></span>, <span class="hljs-string"><span class="hljs-string">"Unknown error"</span></span>) QmpError: Device <span class="hljs-string"><span class="hljs-string">'qdev_1k7EzY85TIWm6-gTBorE3Q'</span></span> could not be initialized qemu-img: Could not open <span class="hljs-string"><span class="hljs-string">'/GLU/volumes/33/33e3bc8c-b53e-4230-b9be-b120079c0ce1'</span></span>: Could not <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> determining its format: Input/output error</code> </pre><br>  <b>Difficile paiement 3 n≈ìuds avec partitionnement d√©sactiv√©</b> <br><br><pre> <code class="bash hljs">Transport endpoint is not connected (107) /GLU/volumes/e0/e0bf9a42-8915-48f7-b509-2f6dd3f17549: ERROR: cannot <span class="hljs-built_in"><span class="hljs-built_in">read</span></span> (Input/output error)</code> </pre> <br>  Nous perdons √©galement des donn√©es, il n'est pas possible de restaurer. <br><br>  <b>Remboursez doucement 3 n≈ìuds avec sharding, y aura-t-il une corruption de donn√©es?</b> <br>  Il y en a, mais beaucoup moins (co√Øncidence?), J'ai perdu 3 des 30 disques. <br><br>  <b>Conclusions:</b> <br><br><ol><li>  La gu√©rison de ces fichiers se bloque sans fin, le r√©√©quilibrage n'aide pas.  Nous concluons que les fichiers sur lesquels l'enregistrement actif √©tait en cours lorsque le 3e n≈ìud a √©t√© d√©sactiv√© sont perdus √† jamais. </li><li>  Ne rechargez jamais plus de 2 n≈ìuds dans une configuration 4 + 2 en production! </li><li>  Comment ne pas perdre de donn√©es si vous voulez vraiment red√©marrer 3 n≈ìuds ou plus?  P Arr√™tez l'enregistrement au point de montage et / ou arr√™tez le volume. </li><li>  Les n≈ìuds ou les briques doivent √™tre remplac√©s d√®s que possible.  Pour cela, il est hautement souhaitable d'avoir, par exemple, 1 √† 2 briques de rechange √† chaud dans chaque n≈ìud pour un remplacement rapide.  Et un n≈ìud de rechange suppl√©mentaire avec des briques en cas de vidage de n≈ìud. </li></ol><br><img src="https://habrastorage.org/webt/-m/rj/ti/-mrjtikde2imxdydeka4w-hvjjk.png"><br><br>  Il est √©galement tr√®s important de tester les cas de remplacement de lecteur <br><br>  <b>D√©parts de briks (disques):</b> <b><br></b>  <b>17:20</b> <br>  Nous sortons une brique: <br><br><pre> <code class="bash hljs">/dev/sdh 1.9T 598G 1.3T 33% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6</code> </pre> <br>  <b>17:22</b> <br><pre> <code class="bash hljs">gluster volume replace-brick freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick_spare_1/freezer sl051s:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/freezer commit force</code> </pre> <br>  Vous pouvez voir un tel rabattement au moment du remplacement de la brique (enregistrement √† partir d'une source): <br><br><img src="https://habrastorage.org/webt/jk/96/ns/jk96ns2kzhy0radczfpxlfpodso.png"><br><br>  Le processus de remplacement est assez long, avec un petit niveau d'enregistrement par cluster et des param√®tres par d√©faut de 1 To, il faut environ une journ√©e pour r√©cup√©rer. <br><br>  <b>Param√®tres r√©glables pour le traitement:</b> <br><br><pre> <code class="bash hljs">gluster volume <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> cluster.background-self-heal-count 20 <span class="hljs-comment"><span class="hljs-comment"># Default Value: 8 # Description: This specifies the number of per client self-heal jobs that can perform parallel heals in the background. gluster volume set cluster.heal-timeout 500 # Default Value: 600 # Description: time interval for checking the need to self-heal in self-heal-daemon gluster volume set cluster.self-heal-window-size 2 # Default Value: 1 # Description: Maximum number blocks per file for which self-heal process would be applied simultaneously. gluster volume set cluster.data-self-heal-algorithm diff # Default Value: (null) # Description: Select between "full", "diff". The "full" algorithm copies the entire file from source to # sink. The "diff" algorithm copies to sink only those blocks whose checksums don't match with those of # source. If no option is configured the option is chosen dynamically as follows: If the file does not exist # on one of the sinks or empty file exists or if the source file size is about the same as page size the # entire file will be read and written ie "full" algo, otherwise "diff" algo is chosen. gluster volume set cluster.self-heal-readdir-size 2KB # Default Value: 1KB # Description: readdirp size for performing entry self-heal</span></span></code> </pre> <br>  <i>Option: disperse.background-heals</i> <i><br></i>  <i>Valeur par d√©faut: 8</i> <i><br></i>  <i>Description: Cette option peut √™tre utilis√©e pour contr√¥ler le nombre de soins parall√®les</i> <i><br><br></i>  <i>Option: disperse.heal-wait-qlength</i> <i><br></i>  <i>Valeur par d√©faut: 128</i> <i><br></i>  <i>Description: cette option peut √™tre utilis√©e pour contr√¥ler le nombre de soins pouvant attendre</i> <i><br><br></i>  <i>Option: disperse.shd-max-threads</i> <i><br></i>  <i>Valeur par d√©faut: 1</i> <i><br></i>  <i>Description: Nombre maximum de soins parall√®les que SHD peut effectuer par brique locale.</i>  <i>Cela peut r√©duire consid√©rablement les temps de gu√©rison, mais peut √©galement √©craser vos briques si vous ne disposez pas du mat√©riel de stockage pour le prendre en charge.</i> <i><br><br></i>  <i>Option: disperse.shd-wait-qlength</i> <i><br></i>  <i>Valeur par d√©faut: 1024</i> <i><br></i>  <i>Description: cette option peut √™tre utilis√©e pour contr√¥ler le nombre de soins pouvant attendre en SHD par sous-volume</i> <i><br><br></i>  <i>Option: disperse.cpu-extensions</i> <i><br></i>  <i>Valeur par d√©faut: auto</i> <i><br></i>  <i>Description: forcer les extensions cpu √† √™tre utilis√©es pour acc√©l√©rer les calculs du champ galois.</i> <i><br><br></i>  <i>Option: disperse.self-heal-window-size</i> <i><br></i>  <i>Valeur par d√©faut: 1</i> <i><br></i>  <i>Description: Nombre maximum de blocs (128 Ko) par fichier pour lequel le processus d'auto-gu√©rison serait appliqu√© simultan√©ment.</i> <br><br>  Stood: <br><br><pre> <code class="bash hljs">disperse.shd-max-threads: 6 disperse.self-heal-window-size: 4 cluster.self-heal-readdir-size: 2KB cluster.data-self-heal-algorithm: diff cluster.self-heal-window-size: 2 cluster.heal-timeout: 500 cluster.background-self-heal-count: 20 cluster.disperse-self-heal-daemon: <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> disperse.background-heals: 18</code> </pre> <br>  Avec de nouveaux param√®tres, 1 To de donn√©es a √©t√© compl√©t√© en 8 heures (3 fois plus vite!) <br><br>  <b>Le moment d√©sagr√©able est que le r√©sultat est un brik plus grand qu'il ne l'√©tait</b> <br><br>  <b>√©tait:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdd 1.9T 645G 1.2T 35% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2</code> </pre> <br>  <b>est devenu:</b> <pre> <code class="bash hljs">Filesystem Size Used Avail Use% Mounted on /dev/sdj 1.9T 1019G 843G 55% /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/hot_spare_brick_0</code> </pre> <br>  Il faut comprendre.  La chose gonfle probablement des disques minces.  Avec le remplacement ult√©rieur de la brique augment√©e, la taille est rest√©e la m√™me. <br><br>  <b>R√©√©quilibrage:</b> <br>  <i>Apr√®s avoir d√©velopp√© ou r√©duit (sans migrer les donn√©es) un volume (√† l'aide des commandes add-brick et remove-brick respectivement), vous devez r√©√©quilibrer les donn√©es entre les serveurs.</i>  <i>Dans un volume non r√©pliqu√©, toutes les briques doivent √™tre en place pour effectuer l'op√©ration de remplacement des briques (option de d√©marrage).</i>  <i>Dans un volume r√©pliqu√©, au moins une des briques de la r√©plique doit √™tre en place.</i> <br><br>  <b>Fa√ßonner le r√©√©quilibrage:</b> <br><br>  <i>Option: cluster.rebal-throttle</i> <i><br></i>  <i>Valeur par d√©faut: normal</i> <i><br></i>  <i>Description: d√©finit le nombre maximal de migrations de fichiers parall√®les autoris√©es sur un n≈ìud pendant l'op√©ration de r√©√©quilibrage.</i>  <i>La valeur par d√©faut est normale et autorise un maximum de [($ (unit√©s de traitement) - 4) / 2), 2] fichiers √† b</i> <i><br></i>  <i>Nous avons migr√© √† la fois.</i>  <i>Lazy autorise la migration d'un seul fichier √† la fois et l'agressif autorise un maximum de [($ (unit√©s de traitement) - 4) / 2), 4]</i> <br><br>  <i>Option: migration cluster.lock</i> <i><br></i>  <i>Valeur par d√©faut: off</i> <i><br></i>  <i>Description: si elle est activ√©e, cette fonction fera migrer les verrous posix associ√©s √† un fichier lors du r√©√©quilibrage</i> <br><br>  <i>Option: r√©√©quilibrage pond√©r√© en cluster</i> <i><br></i>  <i>Valeur par d√©faut: on</i> <i><br></i>  <i>Description: Une fois activ√©, les fichiers seront attribu√©s aux briques avec une probabilit√© proportionnelle √† leur taille.</i>  <i>Sinon, toutes les briques auront la m√™me probabilit√© (comportement h√©rit√©).</i> <br><br>  <b>Comparaison de l'√©criture, puis lecture des m√™mes param√®tres fio (r√©sultats plus d√©taill√©s des tests de performances - en PM):</b> <br><br><pre> <code class="bash hljs">fio --fallocate=keep --ioengine=libaio --direct=1 --buffered=0 --iodepth=1 --bs=64k --name=<span class="hljs-built_in"><span class="hljs-built_in">test</span></span> --rw=write/<span class="hljs-built_in"><span class="hljs-built_in">read</span></span> --filename=/dev/vdb --runtime=6000</code> </pre><br><img src="https://habrastorage.org/webt/fw/up/j0/fwupj0gj9m6vn25bepslaaox01e.png"><br><br><img src="https://habrastorage.org/webt/xi/yf/pd/xiyfpdsecqbfc52fudoz4nwklty.png"><br><br><img src="https://habrastorage.org/webt/nh/xp/es/nhxpestbf-gfjkcwogumbbqabc4.jpeg"><br><br>  <b>Si c'est int√©ressant, comparez la vitesse de rsync au trafic vers les n≈ìuds Glaster:</b> <br><br><img src="https://habrastorage.org/webt/6i/cz/ox/6iczoxword1qaauuhkwm3vfkk-q.png"><br><br><img src="https://habrastorage.org/webt/42/ka/eq/42kaeqkdbcuzhq8rwdrqybgkc5u.png"><br><br>  <i>On peut voir qu'environ 170 Mo / s / trafic √† 110 Mo / s / charge utile.</i>  <i>Il s'av√®re que cela repr√©sente 33% du trafic suppl√©mentaire, ainsi que 1/3 de la redondance du codage d'effacement.</i> <br><br>  <b>La consommation de m√©moire c√¥t√© serveur avec et sans charge ne change pas:</b> <br><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><img src="https://habrastorage.org/webt/nz/sz/zc/nzszzcu0eqrck4gpmhvazhm8x7i.png"><br><br>  <b>La charge sur les h√¥tes du cluster avec la charge maximale sur le volume:</b> <br><br><img src="https://habrastorage.org/webt/vb/u3/3c/vbu33cgi-rmgjn7c2w1guz5ps3c.png"></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417475/">https://habr.com/ru/post/fr417475/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es436928/index.html">WebRTC: a√∫n m√°s f√°cil (EasyRTC)</a></li>
<li><a href="../es436934/index.html">Matem√°ticas ingenuas: el motor de Mendocino y el teorema de Earnshaw</a></li>
<li><a href="../es436936/index.html">Dep√≥sito nuclear japon√©s en Primorye o el sitio de disposici√≥n de los submarinos nucleares de la Flota del Pac√≠fico</a></li>
<li><a href="../es436938/index.html">Guix es el sistema operativo m√°s avanzado.</a></li>
<li><a href="../fr417473/index.html">Stockage s√©curis√© avec DRBD9 et Proxmox (Partie 1: NFS)</a></li>
<li><a href="../fr417477/index.html">Bureau chaud</a></li>
<li><a href="../fr417479/index.html">Concat√©nation plus rapide des cha√Ænes de bricolage dans Go</a></li>
<li><a href="../fr417481/index.html">√Ä propos des g√©n√©rateurs dans JavaScript ES6 et pourquoi il est facultatif de les √©tudier</a></li>
<li><a href="../fr417483/index.html">Comparaison des frameworks JS: React, Vue et Hyperapp</a></li>
<li><a href="../fr417485/index.html">[signet] Aide-m√©moire de l'administrateur syst√®me pour les outils r√©seau Linux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>