<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üïù ü¶ã üïµüèæ AI tradujo la actividad cerebral al habla üñïüèº üë©‚Äçüë¶ üç¥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Muchas personas paralizadas que no pueden hablar tienen se√±ales de lo que quieren decir ocultas en sus cerebros. Y nadie podr√≠a descifrar estas se√±ale...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI tradujo la actividad cerebral al habla</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435904/"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2da/ae7/5ef/2daae75ef2ff8a8c4af731c7d5c8eeb3.jpg" alt="imagen"></div><br>  Muchas personas paralizadas que no pueden hablar tienen se√±ales de lo que quieren decir ocultas en sus cerebros.  Y nadie podr√≠a descifrar estas se√±ales.  Pero recientemente, tres equipos de investigaci√≥n han progresado en la traducci√≥n de datos de electrodos colocados en el cerebro de manera quir√∫rgica en lenguaje sintetizado por computadora. <br><br>  Utilizando modelos construidos en redes neuronales, reconstruyeron palabras e incluso oraciones completas, que, en algunos casos, eran bastante inteligibles para el oyente humano promedio. <br><a name="habracut"></a><br>  Ninguno de los intentos descritos en las preimpresiones de trabajo en bioRxiv para recrear el habla de los pensamientos ha llevado al √©xito.  En cambio, los investigadores observaron la actividad de varias regiones del cerebro del paciente mientras le√≠an en voz alta, o le√≠an a s√≠ mismos pero a√∫n mov√≠an los labios, hablaban el texto internamente o escuchaban las notas. <br><br>  "Mostrar que el discurso reconstruido es bastante comprensible es realmente emocionante".  Dice Stephanie Martin, una neuroingeniera de la Universidad de Ginebra en Suiza, que participa en este proyecto. <br><br>  Las personas que han perdido la capacidad de hablar despu√©s de un derrame cerebral, o como resultado de una enfermedad, pueden usar sus ojos u otros movimientos peque√±os para controlar el cursor o seleccionar letras en la pantalla (el cosm√≥logo Stephen Hawking se tens√≥ la mejilla para activar el interruptor instalado en sus gafas).  Pero si la interfaz cerebro-computadora puede reproducir directamente el habla de los pacientes, esto ampliar√° en gran medida sus capacidades: dar√° control sobre la tonalidad y le permitir√° participar en conversaciones r√°pidas y continuas. <br><br>  "Estamos tratando de desarrollar un esquema ... de neuronas que se activan en diferentes momentos y hacer una conclusi√≥n sobre c√≥mo suena el discurso", dice Nima Mesgarani, ingeniera de la Universidad de Columbia.  "Convertir uno en otro no es tan sencillo". <br><br>  La forma en que estas se√±ales de las neuronas se convierten en habla var√≠a de persona a persona, por lo tanto, los modelos de computadora deben entrenarse por separado para cada individuo.  Y lo mejor de todo, resulta que los modelos aprenden de datos extremadamente precisos, cuyo recibo requiere la apertura del cr√°neo. <br><br>  Los investigadores pueden obtener esta oportunidad en un caso muy raro.  Una de ellas es cuando se extrae a un paciente de un tumor cerebral.  Los cirujanos usan las lecturas de sensores que leen se√±ales el√©ctricas directamente del cerebro para localizar y evitar √°reas del habla y motoras.  Otro ejemplo es cuando se implantan electrodos en un paciente con epilepsia durante varios d√≠as para localizar la fuente de las convulsiones antes de realizar la cirug√≠a. <br><br>  "Tenemos un m√°ximo de 20, a veces 30 minutos, para recopilar datos", dice Stephanie Martin.  "Estamos muy, muy limitados en el tiempo". <br><br>  Los mejores resultados fueron logrados por los equipos "alimentando" los datos obtenidos del registro de la actividad cerebral en redes neuronales artificiales.  Como resultado (ed. Labels), las redes recibieron un discurso que el paciente dijo en voz alta o escuch√≥. <br><br>  El equipo de Nima Mesgarani se bas√≥ en datos de cinco pacientes diferentes con epilepsia.  Sus redes neuronales fueron entrenadas en grabaciones de la corteza auditiva del cerebro de las personas (que est√° activo tanto durante el propio discurso como mientras escucha el de otra persona), que en ese momento estaban reproduciendo registros de varias historias y doblando una secuencia de n√∫meros del 0 al 9. Luego, un modelo de computadora sintetiz√≥ el habla pronunciando la misma secuencia de n√∫meros y un grupo de control de personas pudo reconocer el 75% de estos datos. <br><br><div class="spoiler">  <b class="spoiler_title">Discurso generado por computadora obtenido de los datos de actividad cerebral de un paciente mientras escucha n√∫meros</b> <div class="spoiler_text">  <a href="">www.sciencemag.org/sites/default/files/audio/Mesgarani-1.mp3</a> <br></div></div><br>  Otro equipo, dirigido por Tanja Schultz de la Universidad de Bremen en Alemania, utiliz√≥ datos de 6 personas sometidas a cirug√≠a para extirpar tumores cerebrales.  Su discurso fue grabado en un micr√≥fono mientras le√≠an palabras monosil√°bicas en voz alta.  Al mismo tiempo, los electrodos colocados en su cerebro capturaron la actividad de las √°reas de planificaci√≥n y las √°reas motoras, enviando comandos a la ruta de voz para pronunciar palabras. <br><br>  Los ingenieros Miguel Angrick y Christian Herff, de la Universidad de Maastricht, entrenaron una red neuronal que combinaba los datos le√≠dos utilizando los electrodos con las grabaciones de audio resultantes, y luego reconstruyeron las palabras y frases para el modelo no mostrado previamente de los conjuntos de datos le√≠dos.  De acuerdo con estos datos, el modelo sintetiz√≥ el habla, aproximadamente el 40% de los cuales result√≥ ser comprensible para los humanos. <br><br><div class="spoiler">  <b class="spoiler_title">Grabaci√≥n de voz generada por computadora basada en datos de electrodos</b> <div class="spoiler_text">  <a href="">www.sciencemag.org/sites/default/files/audio/Herff-1.mp3</a> <br></div></div><br>  Y finalmente, el neurocirujano Edward Chang y su equipo de la Universidad de California en San Francisco reconstruyeron oraciones completas sobre la actividad del centro del habla, le√≠das por electrodos en 6 pacientes con epilepsia, en el momento en que le√≠an en voz alta.  Los investigadores realizaron una prueba en l√≠nea en la que 166 personas escucharon una de las oraciones generadas por el modelo de computadora y luego tuvieron que elegir entre las 10 opciones propuestas la que, en su opini√≥n, se ley√≥.  Algunas oraciones se identificaron correctamente en m√°s del 80% de los casos.  Pero los investigadores no se detuvieron all√≠ y forzaron al modelo a recrear el discurso de una persona de acuerdo con los datos sobre la actividad cerebral obtenidos mientras le√≠a palabras para s√≠ mismo, pero movi√≥ los labios en ese momento, como si "los pronunciara internamente". <br><br>  "Este es un resultado muy importante", dice Christian Herff, "estamos un paso m√°s cerca de las pr√≥tesis de lenguaje". <br><br>  "Sin embargo, lo que realmente esperamos es c√≥mo estos m√©todos se mostrar√°n cuando el paciente no pueda hablar en absoluto".  - Responde a Stephanie Ri√®s, neurocient√≠fica de la Universidad de San Diego en California.  ‚ÄúLas se√±ales del cerebro, mientras una persona se lee a s√≠ misma o escucha a los dem√°s, son diferentes de las que aparecen mientras lee en voz alta o en comunicaci√≥n en vivo.  Sin un sonido externo con el que se pueda comparar la actividad cerebral, ser√° muy dif√≠cil para los modelos inform√°ticos predecir d√≥nde comienza y d√≥nde termina el habla interna ". <br><br>  "Decodificar el discurso imaginario dar√° un gran paso adelante", dice Gerwin Schalk, neurocient√≠fico del Centro Nacional de Neurotecnolog√≠a Adaptativa del Departamento de Salud del Estado de Nueva York.  "Y ahora no est√° completamente claro c√≥mo lograr esto". <br><br>  Seg√∫n Herff, uno de los m√©todos puede ser la retroalimentaci√≥n que el paciente dar√° a un modelo de computadora que reproducir√° el habla en tiempo real a medida que la persona pronuncia mentalmente las palabras.  Con una cantidad suficiente de entrenamiento tanto para el paciente como para la IA, el cerebro y la computadora pueden encontrarse en alg√∫n punto intermedio. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es435904/">https://habr.com/ru/post/es435904/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es435894/index.html">Clases privadas Escondi√©ndose en php</a></li>
<li><a href="../es435896/index.html">Uso de DiagnosticSource en .NET Core: teor√≠a</a></li>
<li><a href="../es435898/index.html">En qu√© pensar en una entrevista NALSD</a></li>
<li><a href="../es435900/index.html">Encapsularlo</a></li>
<li><a href="../es435902/index.html">No puede simplemente tomar y escribir SELECCIONAR si el proveedor no lo permite ... pero nosotros escribiremos</a></li>
<li><a href="../es435906/index.html">Marcapasos Cluster Storage + DRBD (Dual primario) + ctdb</a></li>
<li><a href="../es435908/index.html">Web asc√©tica: prototipo de mercados de pulgas en go and js</a></li>
<li><a href="../es435910/index.html">¬øPor qu√© BSD perdi√≥ la batalla con GNU / Linux?</a></li>
<li><a href="../es435912/index.html">Los principales problemas del desarrollo de interfaces modernas.</a></li>
<li><a href="../es435914/index.html">Empaquetado de aplicaciones principales de ASP.NET con Docker</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>