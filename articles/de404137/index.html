<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüè´ üçô üïµüèª Der Philosoph f√ºr k√ºnstliche Intelligenz Eliezer Yudkowsky √ºber Singularit√§t, Bayesianisches Gehirn und Goblins in einem Kabinett üë©‚Äçüë©‚Äçüëß‚Äçüë¶ üóÇÔ∏è üë∏üèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eliezer Shlomo Yudkovsky ist ein amerikanischer Spezialist f√ºr k√ºnstliche Intelligenz, der die Probleme der technologischen Singularit√§t untersucht un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Der Philosoph f√ºr k√ºnstliche Intelligenz Eliezer Yudkowsky √ºber Singularit√§t, Bayesianisches Gehirn und Goblins in einem Kabinett</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/404137/"><img src="https://habrastorage.org/getpro/geektimes/post_images/152/af6/3ff/152af63ff689233ebebadde98fed2580.png" alt="Bild"><br><br>  <i>Eliezer Shlomo Yudkovsky ist ein amerikanischer Spezialist f√ºr k√ºnstliche Intelligenz, der die Probleme der technologischen Singularit√§t untersucht und sich f√ºr die Schaffung einer freundlichen KI einsetzt.</i>  <i>In nicht-akademischen Kreisen ist er besser bekannt als Autor der Fanfiction Harry Potter und Methods of Rationality unter der Schirmherrschaft von Less Wrong.</i> <br><br>  Ich war immer wieder begeistert von klugen Leuten, die an Dinge glauben, die mir absurd erscheinen.  Zum Beispiel glaubt der Genetiker und Direktor der National Institutes of Health, Francis Collins, dass Jesus von den Toten auferstanden ist.  KI-Theoretiker Eliezer Yudkowsky glaubt, dass Autos ... Aber besser, ich werde ihm selbst das Wort erteilen.  2008 habe ich ihn auf Bloggingheads.tv interviewt, aber es ist nichts Gutes dabei herausgekommen, weil ich beschlossen habe, dass er ein Anh√§nger von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ray Kurzweils</a> Singularit√§tsguru ist.  Aber Yudkovsky folgte niemandem und ging nie aufs College.  Er ist ein hartn√§ckiger und origineller Theoretiker der Intelligenz, sowohl menschlich als auch k√ºnstlich.  Seine Arbeit (zum Beispiel ein Aufsatz, der mir geholfen hat, Bayes 'Theoreme zu verstehen oder die Illusion des Verstehens zu vermitteln) strahlt die Arroganz des Autodidakten aus, dessen scharfe Kanten nicht durch eine formale Ausbildung poliert wurden - aber dies ist Teil seines Charmes.  Auch wenn es dich nervt, ist Yudkovsky lustig, frisch, provokativ.  Einzelheiten zu seiner Biografie finden Sie auf seiner pers√∂nlichen Website oder auf der Website des Instituts f√ºr das Studium der maschinellen Intelligenz, an dem er teilgenommen hat.  Und lesen Sie dieses Interview mit einem Bonus in Form von Kommentaren seiner Frau Briena. <br><a name="habracut"></a><br>  <b>Horgan</b> : Wenn Sie auf einer Party gefragt werden, was Sie tun, was antworten Sie? <br><br>  <b>Yudkovsky</b> : <b>Kommt</b> auf die Veranstaltung an.  ‚ÄûIch bin Spezialist f√ºr Entscheidungstheorie‚Äú oder ‚ÄûMitbegr√ºnder des Instituts f√ºr das Studium der Maschinenintelligenz‚Äú oder, wenn es sich um eine andere Partei handelt, √ºber meine Kunstwerke. <br><br>  <b>X:</b> Was ist dein Lieblings-KI-Film und warum? <br><br>  <b>Yu: Die</b> KI in Filmen ist schrecklich Standard.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ex Machina ist</a> einer Ausnahme von dieser Regel so nahe gekommen, wie es zu erwarten ist. <br><br>  <b>X:</b> Wird das College-Dienstprogramm √ºberbewertet? <br><br>  <b>Yu:</b> Ich w√§re √ºberrascht, wenn seine N√ºtzlichkeit angesichts der sozialen Anforderungen f√ºr die Beendigung untersch√§tzt w√ºrde.  Soweit ich wei√ü, gibt es keinen Grund, √ñkonomen nicht zu glauben, die glauben, dass das College zu einem ‚Äûprestigetr√§chtigen Produkt‚Äú geworden ist und dass Versuche, die Studentenkredite zu erh√∂hen, einfach die Kosten des Colleges und die Schuldenlast der Studenten erh√∂ht haben. <br><br>  <b>X:</b> Warum schreibst du Fiktionsgeschichten? <br><br>  <b>Yu:</b> Wenn Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wondermark-</a> Comics <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">umformulieren</a> : "Zuerst habe ich versucht, es nicht zu tun, aber es hat nicht funktioniert." <br><br>  Dar√ºber hinaus vermittelt seri√∂se Literatur Wissen, w√§hrend Fiktion Erfahrung vermittelt.  Wenn Sie die Beweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Bayes-Formel</a> verstehen wollen, kann ich Diagramme verwenden.  Wenn Sie f√ºhlen m√∂chten, wie es ist, Bayes-Logik zu verwenden, muss ich eine Geschichte schreiben, in der der Charakter es macht. <br><br>  <b>X:</b> Bist du in irgendeiner Weise religi√∂s? <br><br>  <b>Yu:</b> Nein.  Wenn Sie einen Fehler machen, m√ºssen Sie die Versuchung vermeiden, in der Verteidigung zu bleiben, und versuchen, einen Standpunkt zu finden, von dem aus Sie zumindest ein wenig Recht haben.  Es ist viel kl√ºger zu sagen: ‚ÄûOh‚Äú, zuzugeben, dass Sie nicht einmal ein bisschen Recht hatten, eine bittere Pille ganz zu schlucken und weiterzuleben.  So sollte sich die Menschheit auf die Religion beziehen. <br><br>  <b>X:</b> Wenn Sie der ‚ÄûK√∂nig der Welt‚Äú w√ºrden, was w√ºrde ganz oben auf Ihrer To-Do-Liste stehen? <br><br>  <b>Yu:</b> Ich habe einmal geschrieben: ‚ÄûEin Test f√ºr einen Libert√§ren funktioniert so: Stellen Sie sich vor, Sie haben Macht erlangt;  Was werden Sie zuerst denken - √ºber Gesetze, die Sie akzeptieren, oder √ºber Gesetze, die Sie aufheben? ‚Äú  Ich bin nicht 100% libert√§r, weil nicht alle meine Wunschliste in der Aufhebung von Gesetzen und der Lockerung von Beschr√§nkungen zum Ausdruck kommen.  Aber ich stelle mir vor, wie ich versuchen w√ºrde, eine Welt zu schaffen, in der ein Arbeitsloser Ihnen eine Fahrt zur Arbeit anbieten k√∂nnte, 5 Dollar f√ºr eine 20-min√ºtige Fahrt bekommt und ihm dadurch nichts Schlimmes passieren w√ºrde.  Er m√ºsste nicht die Arbeitslosenversicherung verlieren, eine Gesch√§ftserlaubnis registrieren, die Krankenversicherung verlieren, sich einer Pr√ºfung unterziehen, einen Anwalt bitten, zu best√§tigen, dass seine Arbeit den Vorschriften der Arbeitsschutzbeh√∂rde entspricht usw.  Er h√§tte nur 5 Dollar hinzugef√ºgt. <br><br>  Ich w√ºrde versuchen, in einen Zustand zur√ºckzukehren, in dem die Einstellung eines Mitarbeiters so einfach w√§re wie im Jahr 1900.  Vielleicht gibt es jetzt einen Sinn in bestimmten Sicherheitsma√ünahmen, aber ich w√ºrde versuchen, eine solche Sicherheit zu schaffen, die eine Person nicht zur√ºckh√§lt und keine Papiere als Ergebnis einer einfachen R√ºckkehr einer Person in die Wirtschaft produziert. <br><br>  Ich w√ºrde versuchen, alles zu tun, wor√ºber kluge √ñkonomen seit langem schreien und was kein Staat tut.  Ersetzen Sie Investitions- und Einkommenssteuern durch Verbrauchs- und Immobiliensteuern.  Ersetzen Sie den Mindestlohn durch negative Lohnsteuern.  Festlegung einer Politik zur Ausrichtung des nominalen BIP f√ºr Zentralbanken und Einstellung der Unterst√ºtzung von Strukturen, die "zu gro√ü sind, um pleite zu gehen".  Fordern Sie, dass die unterlegene Partei w√§hrend eines Patentstreits Anwaltskosten zahlt [ <i>nach dem sogenannten</i>  <i>Englische Regel - im Gegensatz zu amerikanischen Gesetzen, nach denen jede Partei ihre eigenen Kosten tr√§gt - ca.</i>  <i>perev.</i>  ] und geben Sie die Copyright-Dauer auf 28 Jahre zur√ºck.  Entfernen Sie Hindernisse beim Hausbau.  Kopieren Sie die Krankenversicherung aus Singapur.  E-Government in Estland.  Ersetzen Sie Aussch√ºsse und komplexe Entscheidungsprozesse durch bestimmte Personen, die √∂ffentlich dokumentierte Entscheidungen treffen und daf√ºr verantwortlich sind.  F√ºhren Sie kontrollierte Experimente mit verschiedenen Optionen f√ºr die Verwaltung von L√§ndern durch und ber√ºcksichtigen Sie deren Ergebnisse.  Ich kann stundenlang auf die Liste gehen. <br><br>  All dies mag in zweihundert Millionen Jahren keine Rolle spielen.  Aber die nominalen Verm√∂genswerte, die sich aus dem wirtschaftlichen Aufschwung ergeben, k√∂nnen gute Arbeit leisten, w√§hrend ich versuche herauszufinden, was wir mit k√ºnstlicher Intelligenz tun werden.  Das Offensichtliche ist ein Manhattan-Projekt irgendwo auf der Insel, dessen Bezahlung auf dem Wettbewerb zwischen den gr√∂√üten Hedgefonds basiert, bei denen Menschen das Problem der allgemeinen k√ºnstlichen Intelligenz untersuchen k√∂nnen, ohne die Ergebnisse ihrer Arbeit zu ver√∂ffentlichen, was automatisch das Ende der Welt bringt.  Und wenn wir nicht akzeptieren, dass ich magische F√§higkeiten oder einen grundlegend irreversiblen Modus habe, sehe ich nicht, wie ein Gesetz, das ich akzeptieren w√ºrde, die Ann√§herung der KI auf einem Planeten, auf dem Computer allgegenw√§rtig sind, ziemlich stark verz√∂gern w√ºrde. <br><br>  Aber all dies kann immer noch als unm√∂gliches Gedankenexperiment angesehen werden, und im wirklichen Leben ist die Wahrscheinlichkeit eines solchen Experiments Null. <br><br>  <b>X:</b> Was ist so gut am Bayes-Theorem? <br><br>  <b>Yu:</b> Nun, zum Beispiel ist sie sehr tief.  Daher ist eine solche Frage schwer kurz zu beantworten. <br><br>  Ich k√∂nnte antworten, dass der Satz von Bayes als zweiter Hauptsatz der Thermodynamik f√ºr die Erkenntnis bezeichnet werden kann.  Wenn Sie zu dem Schluss kommen, dass die Wahrscheinlichkeit einer Annahme 99% betr√§gt, sei es das Vorhandensein von Milch im Supermarkt oder die anthropogene Ursache der globalen Erw√§rmung, dann haben Sie eine Kombination aus hinreichend guten a priori-Wahrscheinlichkeiten und ziemlich zuverl√§ssigen Beweisen.  Dies ist keine gesetzliche Anforderung, sondern ein Gesetz.  So wie ein Auto nicht fahren kann, ohne die Entropie zu zerstreuen, kann man sich kein gutes Bild von der Welt machen, ohne einen Prozess durchzuf√ºhren, bei dem irgendwo im Inneren eine Bayes'sche Struktur existiert, selbst wenn die Wahrscheinlichkeiten nicht direkt im Prozess verwendet werden. <br><br>  Pers√∂nlich denke ich, dass die Hauptsache, die Bayes uns anbieten kann, die Existenz von Regeln und eisernen Gesetzen ist, die bestimmen, ob die Denkweise die Realit√§t kennzeichnet.  Mormonen wird gesagt, dass sie die Wahrheit des Buches Mormon durch ein brennendes Gef√ºhl im Herzen erkennen.  Akzeptieren Sie konservativ die a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">priori Wahrscheinlichkeit des</a> Buches Mormon als eine von einer Milliarde.  Dann bewerten wir die Wahrscheinlichkeit, dass das Buch Mormon nicht wahr ist, und jemand versp√ºrte ein brennendes Gef√ºhl im Herzen, nachdem er ihm gesagt hatte, dass dies zu erwarten sei.  Wenn Sie die Bayes-Formel verstehen, werden wir sofort feststellen, dass die geringe Wahrscheinlichkeit des Beweises nicht mit der geringen Wahrscheinlichkeit der Hypothese vereinbar ist, die sie mit ihrer Hilfe zu beweisen versuchen.  Sie m√ºssen sich nicht einmal bestimmte Zahlen einfallen lassen, um zu verstehen, dass sie nicht konvergieren. Wie Philip Tetlock in seiner Studie √ºber ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Superpredictors</a> ‚Äú feststellte, kannten sie h√§ufig die Bayes-Formel, gaben jedoch selten bestimmte Zahlen an.  In gewissem Sinne ist es schwieriger, Sie zu t√§uschen, wenn Sie verstehen, dass es eine Art Mathematik gibt, mit der Sie die St√§rke des Beweises genau bestimmen und verstehen k√∂nnen, ob es ausreicht, die geringe Wahrscheinlichkeit der Hypothese zu √ºberwinden.  Sie k√∂nnen nicht einfach etwas erfinden und daran glauben, weil es so nicht funktioniert. <br><br>  <b>X:</b> Beeindruckt Sie die Bayes'sche Gehirnhypothese? <br><br>  <b>Yu:</b> Ich denke, dass einige Leute, die √ºber dieses Thema streiten, √ºber verschiedene Dinge sprechen.  Zu fragen, ob das Gehirn ein Bayes'scher Algorithmus ist, ist wie zu fragen, ob der Honda Accord <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von einer Carnot-W√§rmekraftmaschine angetrieben wird</a> .  Wenn eine Person sagt: ‚ÄûJedes Auto ist ein thermodynamischer Prozess, der Kraftstoff ben√∂tigt und Streuw√§rme abf√ºhrt‚Äú, und eine andere Person h√∂rt: ‚ÄûWenn Sie ein Carnot-Zyklusdiagramm erstellen und dessen Mechanik zeigen, muss sie zustimmen, dass es wie das Innere eines Honda Accord aussieht "Dann ist eine hitzige Debatte unvermeidlich. <br><br>  Einige Leute werden sehr gl√ºcklich sein, wenn sie den Verbrennungsmotor √∂ffnen, die Zylinder darin finden und sagen: "Ich bin sicher, dass sie W√§rme in Druck umwandeln und dabei helfen, das Auto vorw√§rts zu bewegen!"  Und sie werden Recht haben, aber andere Leute werden sagen: ‚ÄûSie konzentrieren sich auf die einzige Komponente eines viel gr√∂√üeren Satzes von Autoteilen.  Der Katalysator ist ebenfalls sehr wichtig und steht nicht in Ihren Carnot-Zyklusdiagrammen.  Und manchmal funktioniert eine Klimaanlage f√ºr uns, genau das Gegenteil davon, wie die W√§rmekraftmaschine nach Ihren Worten funktioniert. ‚Äú <br><br>  Ich denke nicht, dass es √ºberraschend w√§re, wenn ich sage, dass Leute, die auf Sie herabreden: ‚ÄûSie sind mit modernen Autos eindeutig nicht vertraut;  Sie ben√∂tigen eine ganze Reihe verschiedener Methoden, um einen Motor zu bauen, wie Kerzen und Katalysatoren, und nicht nur Ihre thermodynamischen Prozesse. ‚ÄúSie vermissen eine wichtige Abstraktionsebene. <br><br>  Aber wenn Sie wissen m√∂chten, ob das Gehirn buchst√§blich als Bayes'sch betrachtet werden kann und nicht als Ger√§t, das kognitive Arbeit leistet und dessen Natur wir mit Bayes'schen Methoden verstehen k√∂nnen, dann kann ich Ihre Frage beantworten: "Nein, nat√ºrlich."  Es mag mehrere Bayes'sche "Zylinder" in diesem "Motor" geben, aber sehr viel dort wird so seltsam aussehen wie Sicherheitsgurte und Klimaanlage.  Diese Erg√§nzungen √§ndern jedoch nichts an der Tatsache, dass zur korrekten Identifizierung eines Apfels anhand sensorischer Beweise etwas getan werden muss, das im Wesentlichen als Ergebnis einer Induktion interpretiert werden kann, die das Konzept eines Apfels verstehen kann und auf der Grundlage von Beweisen aktualisiert wird, die √Ñpfel von Nicht√§pfeln unterscheiden. <br><br>  <b>X:</b> Kann man zu rational sein? <br><br>  <b>Yu:</b> Du kannst in die sogenannte  "Das Tal der schlechten Rationalit√§t."  Wenn Sie zuvor in mehreren Dingen irrational waren und sich gegenseitig ausbalancierten, k√∂nnen Sie, wenn Sie rational werden, schlechter werden als zuvor.  Je rationaler Sie werden, desto schlimmer k√∂nnen Sie werden, wenn Sie die falsche Richtung f√ºr die Anwendung Ihrer F√§higkeiten w√§hlen. <br><br>  Aber ich w√ºrde nicht empfehlen, sich zu sehr um eine solche Gelegenheit zu k√ºmmern.  Meiner Meinung nach sind Leute, die dar√ºber sprechen, wie klug irrational sein kann, Idioten.  Es ist schwierig, eine realistische, nicht weit hergeholte Lebenssituation zu finden, in der Sie sich entscheiden k√∂nnen, irrational zu sein, und deren Ergebnis Ihnen noch unbekannt ist.  Im wirklichen Leben ist es besser, sich die Wahrheit zu sagen und nicht schlau zu sein. <br><br>  Es ist m√∂glich, dass der ideale Vertreter des Bayes'schen Denkens mit einem interessanten und unterhaltsamen Leben unvereinbar ist.  Dies ist jedoch eindeutig kein so gro√ües Problem wie unsere Tendenz zur Selbstzerst√∂rung. <br><br>  <b>X:</b> Inwiefern unterscheidet sich Ihre Sichtweise zur Singularit√§t von der von Kurzweil? <br><br>  <b>Yu:</b> <br>  ‚Ä¢ Ich glaube nicht, dass Moores Gesetz auf KI angewendet werden kann.  KI ist ein Softwareproblem. <br>  ‚Ä¢ Ich glaube nicht, dass der erste √ºbermenschliche Intellekt aus der Verschmelzung von Maschinen mit Menschen hervorgeht.  Hundert Jahre sind seit dem Aufkommen der Autos vergangen, und wir versuchen gerade, ein Exoskelett f√ºr ein Pferd herzustellen, und ein gew√∂hnliches Auto ist immer noch schneller. <br>  ‚Ä¢ Ich glaube nicht, dass die erste starke KI auf Algorithmen aus der Neurobiologie basieren wird, so wie Flugzeuge nicht auf V√∂geln basierten. <br>  ‚Ä¢ Ich denke nicht, dass die Fusion von Nano-, Info- und Biotechnologien m√∂glich, unvermeidlich, genau definiert oder notwendig ist. <br>  ‚Ä¢ Ich denke, dass es von 1930 bis 1970 mehr Ver√§nderungen gab als von 1970 bis 2010. <br>  ‚Ä¢ Ich denke, dass in Industriel√§ndern die Produktivit√§t stagniert. <br>  ‚Ä¢ Ich denke, die Extrapolation von Moores Gesetz auf den technologischen Fortschritt, die angeblich alles vorhersagt, was nach dem Aufkommen der KI kl√ºger als Menschen sein wird, ist eine sehr seltsame Sache.  Eine intelligentere KI ruiniert alle Ihre Grafiken. <br>  ‚Ä¢ Einige Analysten wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Illka ‚Äã‚ÄãTuomi</a> glauben, dass Moores Gesetz Anfang der 2000er Jahre gebrochen hat.  Ich bin mir nicht sicher, ob ich etwas dagegen haben kann. <br>  ‚Ä¢ Die einzige technologische Schwelle, die mich interessiert, ist, wo KI die F√§higkeit zur Verbesserung gewinnt.  Wir haben keinen Zeitplan, der diese Schwelle erreicht, und es ist unklar, wie er aussehen wird (obwohl er das Niveau einer Person nicht wesentlich √ºberschreiten sollte, da eine Person die Informatik versteht), so dass seine Offensive nicht vorhergesagt werden kann. <br>  ‚Ä¢ Ich denke nicht, dass das Ergebnis eines solchen Fortschritts standardm√§√üig gut sein wird.  Ich denke, es kann gut gemacht werden, aber es muss ernsthaft daran gearbeitet werden, und Kennzahlen sind daran nicht interessiert.  Den Menschen zu sagen, dass wir uns auf einem nat√ºrlichen Weg zu gro√üartigen und wundervollen Zeiten befinden, wird eine L√ºge sein. <br>  ‚Ä¢ Ich denke, dass ‚ÄûSingularit√§t‚Äú zu einem Kofferwort mit zu vielen inkompatiblen Bedeutungen und Details geworden ist, deshalb habe ich es nicht mehr verwendet. <br><br>  <b>X:</b> Werden Sie wahrscheinlich ein ultra-intelligenter Cyborg? <br><br>  <b>Yu:</b> Das Gesetz der Konjunktion von Wahrscheinlichkeiten besagt, dass P (A &amp; B) &lt;= P (A) ist.  Die Wahrscheinlichkeit des gleichzeitigen Auftretens der Ereignisse A und B ist geringer als die Wahrscheinlichkeit des Auftretens eines Ereignisses A. In Experimenten, in denen Menschen glauben, dass P (A &amp; B)&gt; P (A) f√ºr zwei Ereignisse A und B ist, tritt beispielsweise ein ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konjunktionsfehler</a> ‚Äú auf. 1982 wiesen Experten des Internationalen Kongresses f√ºr Vorhersagen dem Ereignis ‚ÄûRussland dringt in Polen ein und die diplomatischen Beziehungen zur UdSSR brechen zusammen‚Äú eine gr√∂√üere Wahrscheinlichkeit zu als der Wahrscheinlichkeit eines separaten Ereignisses ‚ÄûZusammenbruch der diplomatischen Beziehungen zur UdSSR‚Äú, das von einer anderen Gruppe ernannt wurde.  Ebenso hat eine andere Gruppe dem Ereignis ‚ÄûEin Erdbeben in Kalifornien f√ºhrt zu einer √úberschwemmung, die zu Tausenden von Opfern f√ºhrt‚Äú eine gr√∂√üere Wahrscheinlichkeit zugewiesen als eine andere - die Wahrscheinlichkeit des Ereignisses ‚ÄûIrgendwo in Nordamerika gibt es eine √úberschwemmung mit Tausenden von Opfern‚Äú.  Das Hinzuf√ºgen zus√§tzlicher Details zur Geschichte macht es zwar weniger wahrscheinlich, aber glaubw√ºrdiger.  Wenn ich diese Tatsache verstehe, ist es f√ºr mich wie eine ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eselbr√ºcke</a> ‚Äú f√ºr ernsthaften Futurismus - der Unterschied zwischen der Tatsache, dass Sie jede einzelne Annahme sorgf√§ltig abw√§gen und herausfinden, ob Sie diese Klarstellung unabh√§ngig von allen anderen unterst√ºtzen k√∂nnen und einfach eine wunderbare komponieren und eine lebendige Geschichte. <br><br>  Dies ist alles, was ich im Zusammenhang mit der Beantwortung der Frage sage: ‚ÄûWarum f√ºgen Sie dem eine solche Verfeinerung wie einen Cyborg hinzu?  Ich m√∂chte kein Cyborg sein. ‚Äú  Es ist notwendig, zus√§tzliche Details zu den Aussagen sorgf√§ltig zu weben. <br><br>  <b>X:</b> Hast du eine Chance auf Unsterblichkeit? <br><br>  <b>Yu:</b> Buchst√§blich?  W√∂rtliche Unsterblichkeit ist schwer zu erreichen.  Um viel l√§nger als ein paar Billionen Jahre zu leben, m√ºssen Sie das erwartete Schicksal eines expandierenden Universums √ºberdenken.  Um l√§nger als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Googolpleks</a> zu leben, ist es notwendig, dass wir einen Fehler in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bezug auf</a> die Grundlagen physikalischer Gesetze machen und nicht nur in Details. <br><br>  Selbst wenn sich einige der ungew√∂hnlichen √úberlegungen als wahr herausstellen und unser Universum Tochteruniversen erzeugen kann, wird dies uns keine Unsterblichkeit geben.  Um viel mehr Jahre mit Googleplex zu leben und sich nicht zu wiederholen, ben√∂tigen Sie Computer mit mehr Elementen als Google, und eine solche Maschine passt nicht in die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hubble-Sph√§re</a> . <br><br>  Und Googolpleks ist nicht unendlich.  Um Martin Gardner zu paraphrasieren: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grahams Zahl</a> ist immer noch recht klein, da die meisten endg√ºltigen Zahlen viel gr√∂√üer sind als er.  Wenn Sie umgehauen werden m√∂chten, lesen Sie √ºber <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schnell wachsende Hierarchien</a> , und die Unendlichkeit wird noch l√§nger sein.  Nur sehr seltsame und be√§ngstigende anthropische Theorien erm√∂glichen es Ihnen, lange genug zu leben, um einen Stopp bei der am l√§ngsten laufenden Turing-Maschine mit Hunderten von Staaten zu beobachten. <br><br>  Ich glaube jedoch nicht, dass ich aus emotionaler Sicht lange genug leben m√∂chte, um die hundertste Zahl im Spiel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jagd nach einem biberharten Arbeiter</a> " zu sehen.  Ich kann mich irgendwie in mich hineinversetzen, der in hundert Jahren gelebt hat.  Diese Zukunft werde ich in weiteren hundert Jahren selbst in die Zukunft einf√ºhlen k√∂nnen.  Und vielleicht gibt es irgendwo in dieser Sequenz jemanden, der die Aussicht hat, seine Existenz zu beenden, und er kann dar√ºber sehr traurig sein.  Aber ich bin mir nicht sicher, ob ich mir diese Person vorstellen kann.  ‚ÄûIch m√∂chte noch einen Tag leben.  Morgen werde ich auch noch einen Tag leben wollen.  Deshalb m√∂chte ich f√ºr immer leben, was durch die Induktion positiver Ganzzahlen bewiesen wird. "  Sogar mein Wunsch nach einem langen Leben in einem physikalisch m√∂glichen Universum ist eine Abstraktion, die durch Induktion erzeugt wird.  Ich kann mich in einer Billion Jahren nicht vorstellen. <br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich habe die Singularit√§t als eine fl√ºchtige und pseudowissenschaftliche Fantasie beschrieben, die uns vom Klimawandel, Kriegen, Ungleichheit und anderen ernsten Problemen ablenkt. Warum irre ich mich? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Weil Sie versuchen, empirische Fakten durch Psychoanalyse vorherzusagen. Es wird niemals funktionieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen wir an, wir erleben das Aufkommen der KI, die klug genug ist, um die KI genauso zu verbessern wie die Menschen. Er kann sich anpassen, programmieren, neue Algorithmen erfinden. Zu verbessern. Was wird als n√§chstes passieren - es wird intelligenter, sieht noch mehr Verbesserungsm√∂glichkeiten und erreicht schnell ein sehr hohes Niveau? Oder wird nichts Besonderes passieren?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es kann vorkommen, dass (A) die Selbstverbesserung eines bestimmten Deltas die KI so intelligent macht, dass sie zur√ºckblicken und eine neue potenzielle Verbesserung der Gr√∂√üe des k * -Deltas finden kann, wobei k&gt; 1 ist, und dies wird viele Male wiederholt, um zu einer schnellen Selbstverbesserung zu f√ºhren Niveau der Superintelligenz. Was </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die "Explosion der Intelligenz" nannte. Oder (B), k ist kleiner als eins oder alle diese Verbesserungen sind klein und f√ºhren nicht zum Auftreten von Superintelligenz, oder Superintelligenz ist im Allgemeinen unm√∂glich, und anstelle einer Explosion wird es einen Zilch geben. Was ist wahr, A oder B? Wenn Sie eine KI eines bestimmten Niveaus erstellen und er dies versucht, geschieht etwas in der empirischen realen Welt, und dieses Ereignis wird durch Fakten bestimmt, die sich auf die Landschaft der Algorithmen und erreichbare Verbesserungen beziehen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuverl√§ssige Informationen √ºber dieses Ereignis k√∂nnen nicht aus der Psychoanalyse von Menschen erhalten werden. Es ist, als w√ºrde man versuchen, ein Auto ohne Kraftstoff zu starten - das sagt uns der Satz von Bayes. Einige Menschen werden immer Eskapisten sein, unabh√§ngig von den tats√§chlichen Werten versteckter Variablen in der Informatik. Daher kann die Beobachtung einiger Eskapisten nicht als strenger Beweis bezeichnet werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist ein Missverst√§ndnis √ºber die Natur der Rationalit√§t - dass es rational ist zu glauben, dass ‚ÄûGoblins in Schr√§nken nicht existieren‚Äú, weil der Glaube an Goblins aus einem Schrank dumm, unreif, veraltet ist und nur Idioten daran glauben. Das wahre Prinzip der Rationalit√§t besteht darin, im Schrank nachzusehen. In jenen Universen, in denen Goblins in Schr√§nken leben, werden Sie an Goblins glauben, und in Universen, in denen Goblins nicht in Schr√§nken sind, werden Sie nicht an sie glauben.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist schwierig, aber im Prinzip ist es m√∂glich, durch die angelehnte T√ºr zu schauen und zu fragen: "Was w√§re anders im Universum, wenn es nicht m√∂glich w√§re, ein gutes Einkommen aus kognitiven Investitionen zu erzielen, dh eine KI, die versucht, sich selbst zu verbessern, w√ºrde nicht mit einer Explosion enden, sondern mit einem Zilch?" Welche anderen Tatsachen w√§ren f√ºr ein solches Universum charakteristisch? ‚Äú </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt Leute, die behaupten, dass KI nur auf das Niveau einer Person angehoben werden kann, da wir selbst Menschen sind und wir es nicht h√∂her anheben k√∂nnen. Es scheint mir, dass wenn unser Universum so ist, wir einen R√ºckgang der Einnahmen aus Investitionen in Hardware und Software f√ºr Computerschach beobachten sollten, der das Niveau einer Person √ºbersteigt - was tats√§chlich nicht der Fall ist. Au√üerdem w√§re nat√ºrliche Selektion dann nicht in der Lage, eine Person zu erschaffen, und Einsteins Mutter h√§tte eine erstaunliche Physikerin usw. sein sollen.</font></font> usw. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt Leute, die argumentieren, dass je komplexer der Algorithmus ist, desto mehr Anpassungen erforderlich sind und dass unsere Intelligenz als eine Art Einschr√§nkung f√ºr diesen Prozess dient. Dies stimmt jedoch nicht mit den anthropologischen Aufzeichnungen der menschlichen Intelligenz √ºberein; Investitionen in Gehirn-Tuning und Mutationen bieten verbesserte kognitive F√§higkeiten. Wir wissen es, weil die Genetik uns sagt, dass Mutationen mit einer kleinen statistischen Antwort w√§hrend der Evolution nicht behoben werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und Hominiden brauchten kein exponentiell gr√∂√üeres Gehirn als Schimpansen. Und der Kopf von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John von Neumann war</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nicht exponentiell gr√∂√üer als der Kopf eines Durchschnittsmenschen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aus rein praktischer Sicht √ºbertragen menschliche Axone Informationen mit einer Geschwindigkeit, die eine Million Mal geringer ist als die Lichtgeschwindigkeit, und selbst unter dem Gesichtspunkt der W√§rmeableitung verbraucht jede synaptische Operation eine Million Mal mehr als die minimale W√§rmeableitung einer irreversiblen bin√§ren Operation bei 300 Kelvin und so weiter. Warum sollten wir annehmen, dass Gehirnsoftware n√§her am Optimum liegt als Eisen? Das Privileg der menschlichen Intelligenz besteht darin, dass es sich um die kleinste Intelligenzstufe handelt, mit der ein Computer erstellt werden kann. Wenn es m√∂glich w√§re, einen Computer mit einer niedrigeren Intelligenzstufe zu erstellen, w√ºrden wir dies auf einer niedrigeren Stufe diskutieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist jedoch kein einfaches Argument, und f√ºr eine detaillierte Beschreibung sende ich Menschen zu einem meiner alten Werke, "Mikro√∂konomie der Explosion der Intelligenz", das leider immer noch als beste Informationsquelle dient. Aber genau diese Fragen m√ºssen gestellt werden, um anhand der verf√ºgbaren Beweise zu diskutieren, ob es zu einer KI-Explosion kommen wird, bei der eine gewisse Verbesserung der kognitiven F√§higkeiten, die in die Selbstoptimierung investiert werden, diese Verbesserung √ºbersteigt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was die M√∂glichkeiten und ihre Preise betrifft: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen sich eine Welt ohne eine Explosion von Intelligenz und ohne Superintelligenz vorstellen. Oder eine Welt, in der die Tricks, mit denen Experten f√ºr maschinelles Lernen Super-KI kontrollieren, zur Kontrolle von Menschen und des √ºbermenschlichen Regimes geeignet sind. Oder eine Welt, in der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">moralischer Internalismus</font></a><font style="vertical-align: inherit;"> funktioniert</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, also sind alle ziemlich fortgeschrittenen AIs gut. In solchen Welten braucht niemand die ganze Arbeit und die Sorgen des Instituts f√ºr maschinelles Lernen. Und mehrere Moskitonetze wurden verschwendet, und es war besser, sie dem Fonds zur Bek√§mpfung der Malaria zu geben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie k√∂nnen sich auch eine Welt vorstellen, in der Sie mit Malaria k√§mpfen, die Kohlenstoffemissionen bek√§mpfen und auf dem richtigen Niveau halten oder Geoengineering-L√∂sungen verwenden, um bereits gemachte Fehler zu neutralisieren. Und all dies erweist sich als nutzlos, weil die Zivilisation das Problem der KI-Moral nicht l√∂sen kann - und alle Kinder, die mit Hilfe von Netzen vor Malaria gerettet wurden, nur so aufwachsen, dass Nanomaschinen sie im Traum t√∂ten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, dass Menschen, die versuchen, sich f√ºr eine vern√ºnftige Wohlt√§tigkeit einzusetzen, zustimmen werden, dass wir in keiner dieser Welten leben m√∂chten. Die Frage ist nur, welche wahrscheinlicher ist. Das zentrale Prinzip der Rationalit√§t besteht darin, den Glauben an Goblins nicht abzulehnen, weil er dumm und nicht prestigetr√§chtig ist, und nicht an Goblins zu glauben, weil er gesund und sch√∂n ist. Das zentrale Prinzip der Rationalit√§t ist, welche beobachtbaren Zeichen und logischen Schlussfolgerungen uns helfen, eine dieser beiden Welten zu w√§hlen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, die erste Welt ist unwahrscheinlich und die zweite wahrscheinlich. Ich verstehe, dass der Versuch, andere davon zu √ºberzeugen, versucht, gegen den Fluss des Glaubens an die ewige Normalit√§t zu schwimmen. Der Glaube, dass nur unsere kurzfristige Zivilisation, die seit mehreren Jahrzehnten existiert, und nur unsere Spezies, die nur einen Moment auf evolution√§rer und geologischer Ebene existiert, Sinn machen und f√ºr immer existieren m√ºssen. Und obwohl ich glaube, dass die erste Welt nur ein optimistischer Traum ist, denke ich nicht, dass wir das Problem ignorieren m√ºssen, √ºber das wir in Zukunft in Panik geraten werden. Die Mission des Instituts ist es, heute Forschung zu betreiben, die nach Angaben von Menschen, die nach 30 Jahren leben, vor 30 Jahren h√§tte beginnen sollen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Glaubt Ihre Frau Brijena an Singularit√§t?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brijena: Wenn mich jemand fragen w√ºrde, ob ich an eine Singularit√§t glaube, w√ºrde ich eine Augenbraue hochziehen und fragen, ob er an automatische Lastwagen glaubt. Das ist eine seltsame Frage. Ich wei√ü nicht, wie die erste Flotte automatischer LKWs aussehen wird oder wie lange sie brauchen werden, um das vorhandene Frachttransportsystem zu ersetzen. Und ich bin nicht der Meinung, dass ich an Roboter-LKWs glaube. Ich gehe zuversichtlich davon aus, dass unbemannte Transporte moderne Transporte durch die Beteiligung von Menschen ersetzen werden, denn wir gehen in diese Richtung, wenn nichts wirklich Seltsames passiert. Aus dem gleichen Grund sage ich zuversichtlich eine Explosion der Intelligenz voraus. In anderen Bedeutungen des Wortes "Singularit√§t" bin ich mir nicht so sicher. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Brijena gab ihre Antwort, ohne meine Antworten zu sehen. Es ist nur so, dass wir gut zueinander passen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ist es m√∂glich, Superintelligenz zu schaffen, ohne zu verstehen, wie das Gehirn funktioniert? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In dem gleichen Sinne, wie man Flugzeuge bauen kann, ohne zu verstehen, wie ein Vogel fliegt. Sie m√ºssen kein Experte f√ºr V√∂gel sein, aber gleichzeitig ben√∂tigen Sie viel Wissen, um ein Flugzeug zu bauen, das Sie im Prinzip bereits verstehen k√∂nnen, wie grob ein Vogel aus der Luft schwebt oder abst√∂√üt. Deshalb schreibe ich √ºber menschliche Rationalit√§t - wenn Sie in der Frage der maschinellen Intelligenz weit genug gehen, k√∂nnen Sie nicht anders, als sich einige Ideen dar√ºber auszudenken, wie Menschen denken. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Was k√∂nnte Superintelligenz wollen? Werden sie so etwas wie sexuelles Verlangen haben? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Denken Sie an einen riesigen Raum von M√∂glichkeiten, an eine riesige mehrdimensionale Kugel. Dies ist ein Raum von Geistestypen, eine Reihe aller m√∂glichen kognitiven Algorithmen. Stellen Sie sich vor, irgendwo am unteren Rand der Kugel befindet sich ein winziger Punkt, der alle Menschen kennzeichnet, die jemals gelebt haben. Dies ist ein winziger Punkt, da alle Menschen ungef√§hr das gleiche Gehirn haben, mit Kortex, Kleinhirn, Thalamus usw. Einige Menschen sind nicht wie andere, daher kann es sich um einen stacheligen Punkt handeln, aber die Spitzen haben den gleichen Ma√üstab wie der Punkt selbst. Unabh√§ngig von Ihrer Neuroatypizit√§t arbeiten Sie nicht an einem anderen kortikalen Algorithmus.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zu fragen, was Superintelligenz will, ist die falsche Frage. Superintelligenzen sind kein seltsamer Stamm von Menschen, die auf der anderen Seite des Flusses leben und exotische Br√§uche haben. KI ist einfach der Name des gesamten Raums der M√∂glichkeiten au√üerhalb eines winzigen menschlichen Punktes. Mit ausreichendem Wissen k√∂nnen Sie in diesen Raum der M√∂glichkeiten aufsteigen und eine KI mit W√ºnschen herausholen, die von der Wunschliste in der menschlichen Sprache beschrieben werden kann, aber nicht, weil dies die nat√ºrliche Wunschliste dieser exotischen √úbermenschen sein wird, sondern weil Sie einen Teil vom Raum der Geistestypen isoliert haben .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Bezug auf sexuelle W√ºnsche - wenn Sie genau wissen, was Sie tun, haben Sie das Hauptproblem des Aufbaus von KI gel√∂st, indem Sie bestimmte Dinge stabil wollen, w√§hrend es sich verbessert, wenn Sie das Hauptproblem gel√∂st haben, n√ºtzliche Funktionen der KI auf Aufgaben zu lenken, die einer Person t√§uschend einfach erscheinen, und Ein noch komplizierteres Problem ist die Konstruktion von KI unter Verwendung einer bestimmten Art von Architektur, in der Dinge wie ‚Äûsexuelle W√ºnsche‚Äú und ‚ÄûGl√ºck durch Sex‚Äú eine Rolle spielen. Dann k√∂nnen Sie KI m√∂glicherweise dazu bringen, Menschen zu betrachten, die modellieren Sei ihr Wunsch, extrahiere diesen Teil von ihnen in Bezug auf das sexuelle Verlangen und lass die KI es erfahren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nat√ºrlich k√∂nnen Sie mit guten Kenntnissen der organischen Biologie und Aerodynamik auch Flugzeuge bauen, die sich mit V√∂geln paaren k√∂nnen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber ich denke nicht, dass die Gebr√ºder Wright solche Dinge zu Beginn ihrer Aktivit√§ten h√§tten tun sollen. Das w√ºrde keinen Sinn ergeben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es erscheint vern√ºnftiger, das Problem zu l√∂sen, in den Raum des Geistes einzudringen und daraus eine KI zu extrahieren, die uns nicht in freie Atome zerlegen will. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich m√∂chte denken, dass besonders kluge Kreaturen Gewaltfreiheit bekennen, weil sie verstehen, dass Gewalt dumm ist. Bin ich naiv </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich denke schon. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">David hume</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich w√ºrde Ihnen sagen, dass Sie einen typischen Fehler machen, indem Sie das Pr√§dikat ‚ÄûDummheit‚Äú auf die Werte oder offiziellen Operationen einer Person anwenden. Aktionen, Entscheidungen, Regeln k√∂nnen dumm sein, wenn Sie Vorlieben f√ºr den Endzustand der Welt haben. Wenn Sie eine Person mit Meta-Pr√§ferenzen sind, die Sie nicht vollst√§ndig berechnet haben, haben Sie m√∂glicherweise eine Plattform, auf die Sie sich verlassen und Spekulationen √ºber Objektpr√§ferenzen als "dumm" bezeichnen k√∂nnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Staple Maximizer [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gedankenexperiment, das zeigt, wie KI, die ohne b√∂swillige Absicht hergestellt wurde, der Menschheit schaden kann - ca.</font></font></i>  <i>perev.</i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] macht keinen Rechenfehler und w√§hlt die F√§lle aus denen aus, in denen die maximale Anzahl von Heftklammern erhalten wird. Es befindet sich nicht in Ihrer Pr√§ferenzplattform, wenn Sie fehlerhafte Aktionen ausw√§hlen, und es befindet sich nicht in Ihrer Meta-Pr√§ferenzplattform, wenn Sie f√§lschlicherweise Pr√§ferenzen ausw√§hlen. Er berechnet die Antwort auf eine andere Frage und nicht auf die, die Sie sich stellen, die Frage "Was soll ich tun?" Der Heftklammermaximierer f√ºhrt einfach die Aktion aus, die zu den meisten Heftklammern f√ºhrt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein fatales Szenario ist, wenn die KI Sie weder liebt noch hasst, weil Sie aus Atomen bestehen, mit denen sie etwas anderes erschaffen kann. Spieltheorie und Probleme der Zusammenarbeit im </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gefangenendilemma</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manifestieren sich nicht in allen m√∂glichen F√§llen. Sie werden beispielsweise nicht angezeigt, wenn ein bestimmtes Motiv so viel st√§rker ist als Sie, dass es Sie zu Atomen machen kann, wenn Sie auf die Schaltfl√§chen "Zusammenarbeiten" oder "√Ñndern" klicken m√∂chten. Und wenn wir diese Schwelle √ºberschreiten, haben Sie entweder das Problem gel√∂st, etwas zu schaffen, das Ihnen nicht schaden will, oder Sie haben bereits verloren. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wird Superintelligenz </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">das schwierige Problem des Bewusstseins l√∂sen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ja, und im R√ºckblick wird uns die Antwort sch√§ndlich einfach erscheinen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Werden Superintelligenzen einen freien Willen haben? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ja, aber sie werden nicht die Illusion eines freien Willens haben. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wie sieht deine Utopie aus? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich werde deine Leser zu meinen leiten. "</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sequenzen der Unterhaltungstheorie,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "da ich es noch nicht geschafft habe, eine Geschichte zu schreiben, deren Handlung in einer unterhaltsam-theoretischen optimalen Welt stattfindet.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de404137/">https://habr.com/ru/post/de404137/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de404125/index.html">Der Countdown ist abgelaufen: Noch 7 Tage bis zum Polybius ICO</a></li>
<li><a href="../de404127/index.html">Der erste Start des Electron LV war teilweise erfolgreich</a></li>
<li><a href="../de404129/index.html">Happy Geeks Day (ja, er ist heute)</a></li>
<li><a href="../de404133/index.html">Bis Ende dieses Jahres will Google einen 49-Qubit-Quantencomputer in Betrieb nehmen</a></li>
<li><a href="../de404135/index.html">Google sammelt und analysiert Offline-K√§ufe von Android Pay-Nutzern</a></li>
<li><a href="../de404139/index.html">Bitcoin in Russland: Steuern (ein paar einfache Fragen)</a></li>
<li><a href="../de404141/index.html">Unlauterer Wettbewerb beim Anbieter</a></li>
<li><a href="../de404143/index.html">Tiny Nuggets: Ein R√ºckblick auf die russischen Registrare TrendVision Split and Tube</a></li>
<li><a href="../de404147/index.html">Sound, du bist nur "Weltraum": Campfire Audio Andromeda Kopfh√∂rer</a></li>
<li><a href="../de404149/index.html">Test des wasserdichten Leseger√§ts der neuen Generation PocketBook 641 Aqua 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>