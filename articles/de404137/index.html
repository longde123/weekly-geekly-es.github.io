<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👨🏼‍🏫 🍙 🕵🏻 Der Philosoph für künstliche Intelligenz Eliezer Yudkowsky über Singularität, Bayesianisches Gehirn und Goblins in einem Kabinett 👩‍👩‍👧‍👦 🗂️ 👸🏽</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Eliezer Shlomo Yudkovsky ist ein amerikanischer Spezialist für künstliche Intelligenz, der die Probleme der technologischen Singularität untersucht un...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Der Philosoph für künstliche Intelligenz Eliezer Yudkowsky über Singularität, Bayesianisches Gehirn und Goblins in einem Kabinett</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/404137/"><img src="https://habrastorage.org/getpro/geektimes/post_images/152/af6/3ff/152af63ff689233ebebadde98fed2580.png" alt="Bild"><br><br>  <i>Eliezer Shlomo Yudkovsky ist ein amerikanischer Spezialist für künstliche Intelligenz, der die Probleme der technologischen Singularität untersucht und sich für die Schaffung einer freundlichen KI einsetzt.</i>  <i>In nicht-akademischen Kreisen ist er besser bekannt als Autor der Fanfiction Harry Potter und Methods of Rationality unter der Schirmherrschaft von Less Wrong.</i> <br><br>  Ich war immer wieder begeistert von klugen Leuten, die an Dinge glauben, die mir absurd erscheinen.  Zum Beispiel glaubt der Genetiker und Direktor der National Institutes of Health, Francis Collins, dass Jesus von den Toten auferstanden ist.  KI-Theoretiker Eliezer Yudkowsky glaubt, dass Autos ... Aber besser, ich werde ihm selbst das Wort erteilen.  2008 habe ich ihn auf Bloggingheads.tv interviewt, aber es ist nichts Gutes dabei herausgekommen, weil ich beschlossen habe, dass er ein Anhänger von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ray Kurzweils</a> Singularitätsguru ist.  Aber Yudkovsky folgte niemandem und ging nie aufs College.  Er ist ein hartnäckiger und origineller Theoretiker der Intelligenz, sowohl menschlich als auch künstlich.  Seine Arbeit (zum Beispiel ein Aufsatz, der mir geholfen hat, Bayes 'Theoreme zu verstehen oder die Illusion des Verstehens zu vermitteln) strahlt die Arroganz des Autodidakten aus, dessen scharfe Kanten nicht durch eine formale Ausbildung poliert wurden - aber dies ist Teil seines Charmes.  Auch wenn es dich nervt, ist Yudkovsky lustig, frisch, provokativ.  Einzelheiten zu seiner Biografie finden Sie auf seiner persönlichen Website oder auf der Website des Instituts für das Studium der maschinellen Intelligenz, an dem er teilgenommen hat.  Und lesen Sie dieses Interview mit einem Bonus in Form von Kommentaren seiner Frau Briena. <br><a name="habracut"></a><br>  <b>Horgan</b> : Wenn Sie auf einer Party gefragt werden, was Sie tun, was antworten Sie? <br><br>  <b>Yudkovsky</b> : <b>Kommt</b> auf die Veranstaltung an.  „Ich bin Spezialist für Entscheidungstheorie“ oder „Mitbegründer des Instituts für das Studium der Maschinenintelligenz“ oder, wenn es sich um eine andere Partei handelt, über meine Kunstwerke. <br><br>  <b>X:</b> Was ist dein Lieblings-KI-Film und warum? <br><br>  <b>Yu: Die</b> KI in Filmen ist schrecklich Standard.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ex Machina ist</a> einer Ausnahme von dieser Regel so nahe gekommen, wie es zu erwarten ist. <br><br>  <b>X:</b> Wird das College-Dienstprogramm überbewertet? <br><br>  <b>Yu:</b> Ich wäre überrascht, wenn seine Nützlichkeit angesichts der sozialen Anforderungen für die Beendigung unterschätzt würde.  Soweit ich weiß, gibt es keinen Grund, Ökonomen nicht zu glauben, die glauben, dass das College zu einem „prestigeträchtigen Produkt“ geworden ist und dass Versuche, die Studentenkredite zu erhöhen, einfach die Kosten des Colleges und die Schuldenlast der Studenten erhöht haben. <br><br>  <b>X:</b> Warum schreibst du Fiktionsgeschichten? <br><br>  <b>Yu:</b> Wenn Sie die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Wondermark-</a> Comics <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">umformulieren</a> : "Zuerst habe ich versucht, es nicht zu tun, aber es hat nicht funktioniert." <br><br>  Darüber hinaus vermittelt seriöse Literatur Wissen, während Fiktion Erfahrung vermittelt.  Wenn Sie die Beweise <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">der Bayes-Formel</a> verstehen wollen, kann ich Diagramme verwenden.  Wenn Sie fühlen möchten, wie es ist, Bayes-Logik zu verwenden, muss ich eine Geschichte schreiben, in der der Charakter es macht. <br><br>  <b>X:</b> Bist du in irgendeiner Weise religiös? <br><br>  <b>Yu:</b> Nein.  Wenn Sie einen Fehler machen, müssen Sie die Versuchung vermeiden, in der Verteidigung zu bleiben, und versuchen, einen Standpunkt zu finden, von dem aus Sie zumindest ein wenig Recht haben.  Es ist viel klüger zu sagen: „Oh“, zuzugeben, dass Sie nicht einmal ein bisschen Recht hatten, eine bittere Pille ganz zu schlucken und weiterzuleben.  So sollte sich die Menschheit auf die Religion beziehen. <br><br>  <b>X:</b> Wenn Sie der „König der Welt“ würden, was würde ganz oben auf Ihrer To-Do-Liste stehen? <br><br>  <b>Yu:</b> Ich habe einmal geschrieben: „Ein Test für einen Libertären funktioniert so: Stellen Sie sich vor, Sie haben Macht erlangt;  Was werden Sie zuerst denken - über Gesetze, die Sie akzeptieren, oder über Gesetze, die Sie aufheben? “  Ich bin nicht 100% libertär, weil nicht alle meine Wunschliste in der Aufhebung von Gesetzen und der Lockerung von Beschränkungen zum Ausdruck kommen.  Aber ich stelle mir vor, wie ich versuchen würde, eine Welt zu schaffen, in der ein Arbeitsloser Ihnen eine Fahrt zur Arbeit anbieten könnte, 5 Dollar für eine 20-minütige Fahrt bekommt und ihm dadurch nichts Schlimmes passieren würde.  Er müsste nicht die Arbeitslosenversicherung verlieren, eine Geschäftserlaubnis registrieren, die Krankenversicherung verlieren, sich einer Prüfung unterziehen, einen Anwalt bitten, zu bestätigen, dass seine Arbeit den Vorschriften der Arbeitsschutzbehörde entspricht usw.  Er hätte nur 5 Dollar hinzugefügt. <br><br>  Ich würde versuchen, in einen Zustand zurückzukehren, in dem die Einstellung eines Mitarbeiters so einfach wäre wie im Jahr 1900.  Vielleicht gibt es jetzt einen Sinn in bestimmten Sicherheitsmaßnahmen, aber ich würde versuchen, eine solche Sicherheit zu schaffen, die eine Person nicht zurückhält und keine Papiere als Ergebnis einer einfachen Rückkehr einer Person in die Wirtschaft produziert. <br><br>  Ich würde versuchen, alles zu tun, worüber kluge Ökonomen seit langem schreien und was kein Staat tut.  Ersetzen Sie Investitions- und Einkommenssteuern durch Verbrauchs- und Immobiliensteuern.  Ersetzen Sie den Mindestlohn durch negative Lohnsteuern.  Festlegung einer Politik zur Ausrichtung des nominalen BIP für Zentralbanken und Einstellung der Unterstützung von Strukturen, die "zu groß sind, um pleite zu gehen".  Fordern Sie, dass die unterlegene Partei während eines Patentstreits Anwaltskosten zahlt [ <i>nach dem sogenannten</i>  <i>Englische Regel - im Gegensatz zu amerikanischen Gesetzen, nach denen jede Partei ihre eigenen Kosten trägt - ca.</i>  <i>perev.</i>  ] und geben Sie die Copyright-Dauer auf 28 Jahre zurück.  Entfernen Sie Hindernisse beim Hausbau.  Kopieren Sie die Krankenversicherung aus Singapur.  E-Government in Estland.  Ersetzen Sie Ausschüsse und komplexe Entscheidungsprozesse durch bestimmte Personen, die öffentlich dokumentierte Entscheidungen treffen und dafür verantwortlich sind.  Führen Sie kontrollierte Experimente mit verschiedenen Optionen für die Verwaltung von Ländern durch und berücksichtigen Sie deren Ergebnisse.  Ich kann stundenlang auf die Liste gehen. <br><br>  All dies mag in zweihundert Millionen Jahren keine Rolle spielen.  Aber die nominalen Vermögenswerte, die sich aus dem wirtschaftlichen Aufschwung ergeben, können gute Arbeit leisten, während ich versuche herauszufinden, was wir mit künstlicher Intelligenz tun werden.  Das Offensichtliche ist ein Manhattan-Projekt irgendwo auf der Insel, dessen Bezahlung auf dem Wettbewerb zwischen den größten Hedgefonds basiert, bei denen Menschen das Problem der allgemeinen künstlichen Intelligenz untersuchen können, ohne die Ergebnisse ihrer Arbeit zu veröffentlichen, was automatisch das Ende der Welt bringt.  Und wenn wir nicht akzeptieren, dass ich magische Fähigkeiten oder einen grundlegend irreversiblen Modus habe, sehe ich nicht, wie ein Gesetz, das ich akzeptieren würde, die Annäherung der KI auf einem Planeten, auf dem Computer allgegenwärtig sind, ziemlich stark verzögern würde. <br><br>  Aber all dies kann immer noch als unmögliches Gedankenexperiment angesehen werden, und im wirklichen Leben ist die Wahrscheinlichkeit eines solchen Experiments Null. <br><br>  <b>X:</b> Was ist so gut am Bayes-Theorem? <br><br>  <b>Yu:</b> Nun, zum Beispiel ist sie sehr tief.  Daher ist eine solche Frage schwer kurz zu beantworten. <br><br>  Ich könnte antworten, dass der Satz von Bayes als zweiter Hauptsatz der Thermodynamik für die Erkenntnis bezeichnet werden kann.  Wenn Sie zu dem Schluss kommen, dass die Wahrscheinlichkeit einer Annahme 99% beträgt, sei es das Vorhandensein von Milch im Supermarkt oder die anthropogene Ursache der globalen Erwärmung, dann haben Sie eine Kombination aus hinreichend guten a priori-Wahrscheinlichkeiten und ziemlich zuverlässigen Beweisen.  Dies ist keine gesetzliche Anforderung, sondern ein Gesetz.  So wie ein Auto nicht fahren kann, ohne die Entropie zu zerstreuen, kann man sich kein gutes Bild von der Welt machen, ohne einen Prozess durchzuführen, bei dem irgendwo im Inneren eine Bayes'sche Struktur existiert, selbst wenn die Wahrscheinlichkeiten nicht direkt im Prozess verwendet werden. <br><br>  Persönlich denke ich, dass die Hauptsache, die Bayes uns anbieten kann, die Existenz von Regeln und eisernen Gesetzen ist, die bestimmen, ob die Denkweise die Realität kennzeichnet.  Mormonen wird gesagt, dass sie die Wahrheit des Buches Mormon durch ein brennendes Gefühl im Herzen erkennen.  Akzeptieren Sie konservativ die a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">priori Wahrscheinlichkeit des</a> Buches Mormon als eine von einer Milliarde.  Dann bewerten wir die Wahrscheinlichkeit, dass das Buch Mormon nicht wahr ist, und jemand verspürte ein brennendes Gefühl im Herzen, nachdem er ihm gesagt hatte, dass dies zu erwarten sei.  Wenn Sie die Bayes-Formel verstehen, werden wir sofort feststellen, dass die geringe Wahrscheinlichkeit des Beweises nicht mit der geringen Wahrscheinlichkeit der Hypothese vereinbar ist, die sie mit ihrer Hilfe zu beweisen versuchen.  Sie müssen sich nicht einmal bestimmte Zahlen einfallen lassen, um zu verstehen, dass sie nicht konvergieren. Wie Philip Tetlock in seiner Studie über „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Superpredictors</a> “ feststellte, kannten sie häufig die Bayes-Formel, gaben jedoch selten bestimmte Zahlen an.  In gewissem Sinne ist es schwieriger, Sie zu täuschen, wenn Sie verstehen, dass es eine Art Mathematik gibt, mit der Sie die Stärke des Beweises genau bestimmen und verstehen können, ob es ausreicht, die geringe Wahrscheinlichkeit der Hypothese zu überwinden.  Sie können nicht einfach etwas erfinden und daran glauben, weil es so nicht funktioniert. <br><br>  <b>X:</b> Beeindruckt Sie die Bayes'sche Gehirnhypothese? <br><br>  <b>Yu:</b> Ich denke, dass einige Leute, die über dieses Thema streiten, über verschiedene Dinge sprechen.  Zu fragen, ob das Gehirn ein Bayes'scher Algorithmus ist, ist wie zu fragen, ob der Honda Accord <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von einer Carnot-Wärmekraftmaschine angetrieben wird</a> .  Wenn eine Person sagt: „Jedes Auto ist ein thermodynamischer Prozess, der Kraftstoff benötigt und Streuwärme abführt“, und eine andere Person hört: „Wenn Sie ein Carnot-Zyklusdiagramm erstellen und dessen Mechanik zeigen, muss sie zustimmen, dass es wie das Innere eines Honda Accord aussieht "Dann ist eine hitzige Debatte unvermeidlich. <br><br>  Einige Leute werden sehr glücklich sein, wenn sie den Verbrennungsmotor öffnen, die Zylinder darin finden und sagen: "Ich bin sicher, dass sie Wärme in Druck umwandeln und dabei helfen, das Auto vorwärts zu bewegen!"  Und sie werden Recht haben, aber andere Leute werden sagen: „Sie konzentrieren sich auf die einzige Komponente eines viel größeren Satzes von Autoteilen.  Der Katalysator ist ebenfalls sehr wichtig und steht nicht in Ihren Carnot-Zyklusdiagrammen.  Und manchmal funktioniert eine Klimaanlage für uns, genau das Gegenteil davon, wie die Wärmekraftmaschine nach Ihren Worten funktioniert. “ <br><br>  Ich denke nicht, dass es überraschend wäre, wenn ich sage, dass Leute, die auf Sie herabreden: „Sie sind mit modernen Autos eindeutig nicht vertraut;  Sie benötigen eine ganze Reihe verschiedener Methoden, um einen Motor zu bauen, wie Kerzen und Katalysatoren, und nicht nur Ihre thermodynamischen Prozesse. “Sie vermissen eine wichtige Abstraktionsebene. <br><br>  Aber wenn Sie wissen möchten, ob das Gehirn buchstäblich als Bayes'sch betrachtet werden kann und nicht als Gerät, das kognitive Arbeit leistet und dessen Natur wir mit Bayes'schen Methoden verstehen können, dann kann ich Ihre Frage beantworten: "Nein, natürlich."  Es mag mehrere Bayes'sche "Zylinder" in diesem "Motor" geben, aber sehr viel dort wird so seltsam aussehen wie Sicherheitsgurte und Klimaanlage.  Diese Ergänzungen ändern jedoch nichts an der Tatsache, dass zur korrekten Identifizierung eines Apfels anhand sensorischer Beweise etwas getan werden muss, das im Wesentlichen als Ergebnis einer Induktion interpretiert werden kann, die das Konzept eines Apfels verstehen kann und auf der Grundlage von Beweisen aktualisiert wird, die Äpfel von Nichtäpfeln unterscheiden. <br><br>  <b>X:</b> Kann man zu rational sein? <br><br>  <b>Yu:</b> Du kannst in die sogenannte  "Das Tal der schlechten Rationalität."  Wenn Sie zuvor in mehreren Dingen irrational waren und sich gegenseitig ausbalancierten, können Sie, wenn Sie rational werden, schlechter werden als zuvor.  Je rationaler Sie werden, desto schlimmer können Sie werden, wenn Sie die falsche Richtung für die Anwendung Ihrer Fähigkeiten wählen. <br><br>  Aber ich würde nicht empfehlen, sich zu sehr um eine solche Gelegenheit zu kümmern.  Meiner Meinung nach sind Leute, die darüber sprechen, wie klug irrational sein kann, Idioten.  Es ist schwierig, eine realistische, nicht weit hergeholte Lebenssituation zu finden, in der Sie sich entscheiden können, irrational zu sein, und deren Ergebnis Ihnen noch unbekannt ist.  Im wirklichen Leben ist es besser, sich die Wahrheit zu sagen und nicht schlau zu sein. <br><br>  Es ist möglich, dass der ideale Vertreter des Bayes'schen Denkens mit einem interessanten und unterhaltsamen Leben unvereinbar ist.  Dies ist jedoch eindeutig kein so großes Problem wie unsere Tendenz zur Selbstzerstörung. <br><br>  <b>X:</b> Inwiefern unterscheidet sich Ihre Sichtweise zur Singularität von der von Kurzweil? <br><br>  <b>Yu:</b> <br>  • Ich glaube nicht, dass Moores Gesetz auf KI angewendet werden kann.  KI ist ein Softwareproblem. <br>  • Ich glaube nicht, dass der erste übermenschliche Intellekt aus der Verschmelzung von Maschinen mit Menschen hervorgeht.  Hundert Jahre sind seit dem Aufkommen der Autos vergangen, und wir versuchen gerade, ein Exoskelett für ein Pferd herzustellen, und ein gewöhnliches Auto ist immer noch schneller. <br>  • Ich glaube nicht, dass die erste starke KI auf Algorithmen aus der Neurobiologie basieren wird, so wie Flugzeuge nicht auf Vögeln basierten. <br>  • Ich denke nicht, dass die Fusion von Nano-, Info- und Biotechnologien möglich, unvermeidlich, genau definiert oder notwendig ist. <br>  • Ich denke, dass es von 1930 bis 1970 mehr Veränderungen gab als von 1970 bis 2010. <br>  • Ich denke, dass in Industrieländern die Produktivität stagniert. <br>  • Ich denke, die Extrapolation von Moores Gesetz auf den technologischen Fortschritt, die angeblich alles vorhersagt, was nach dem Aufkommen der KI klüger als Menschen sein wird, ist eine sehr seltsame Sache.  Eine intelligentere KI ruiniert alle Ihre Grafiken. <br>  • Einige Analysten wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Illka ​​Tuomi</a> glauben, dass Moores Gesetz Anfang der 2000er Jahre gebrochen hat.  Ich bin mir nicht sicher, ob ich etwas dagegen haben kann. <br>  • Die einzige technologische Schwelle, die mich interessiert, ist, wo KI die Fähigkeit zur Verbesserung gewinnt.  Wir haben keinen Zeitplan, der diese Schwelle erreicht, und es ist unklar, wie er aussehen wird (obwohl er das Niveau einer Person nicht wesentlich überschreiten sollte, da eine Person die Informatik versteht), so dass seine Offensive nicht vorhergesagt werden kann. <br>  • Ich denke nicht, dass das Ergebnis eines solchen Fortschritts standardmäßig gut sein wird.  Ich denke, es kann gut gemacht werden, aber es muss ernsthaft daran gearbeitet werden, und Kennzahlen sind daran nicht interessiert.  Den Menschen zu sagen, dass wir uns auf einem natürlichen Weg zu großartigen und wundervollen Zeiten befinden, wird eine Lüge sein. <br>  • Ich denke, dass „Singularität“ zu einem Kofferwort mit zu vielen inkompatiblen Bedeutungen und Details geworden ist, deshalb habe ich es nicht mehr verwendet. <br><br>  <b>X:</b> Werden Sie wahrscheinlich ein ultra-intelligenter Cyborg? <br><br>  <b>Yu:</b> Das Gesetz der Konjunktion von Wahrscheinlichkeiten besagt, dass P (A &amp; B) &lt;= P (A) ist.  Die Wahrscheinlichkeit des gleichzeitigen Auftretens der Ereignisse A und B ist geringer als die Wahrscheinlichkeit des Auftretens eines Ereignisses A. In Experimenten, in denen Menschen glauben, dass P (A &amp; B)&gt; P (A) für zwei Ereignisse A und B ist, tritt beispielsweise ein „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Konjunktionsfehler</a> “ auf. 1982 wiesen Experten des Internationalen Kongresses für Vorhersagen dem Ereignis „Russland dringt in Polen ein und die diplomatischen Beziehungen zur UdSSR brechen zusammen“ eine größere Wahrscheinlichkeit zu als der Wahrscheinlichkeit eines separaten Ereignisses „Zusammenbruch der diplomatischen Beziehungen zur UdSSR“, das von einer anderen Gruppe ernannt wurde.  Ebenso hat eine andere Gruppe dem Ereignis „Ein Erdbeben in Kalifornien führt zu einer Überschwemmung, die zu Tausenden von Opfern führt“ eine größere Wahrscheinlichkeit zugewiesen als eine andere - die Wahrscheinlichkeit des Ereignisses „Irgendwo in Nordamerika gibt es eine Überschwemmung mit Tausenden von Opfern“.  Das Hinzufügen zusätzlicher Details zur Geschichte macht es zwar weniger wahrscheinlich, aber glaubwürdiger.  Wenn ich diese Tatsache verstehe, ist es für mich wie eine „ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Eselbrücke</a> “ für ernsthaften Futurismus - der Unterschied zwischen der Tatsache, dass Sie jede einzelne Annahme sorgfältig abwägen und herausfinden, ob Sie diese Klarstellung unabhängig von allen anderen unterstützen können und einfach eine wunderbare komponieren und eine lebendige Geschichte. <br><br>  Dies ist alles, was ich im Zusammenhang mit der Beantwortung der Frage sage: „Warum fügen Sie dem eine solche Verfeinerung wie einen Cyborg hinzu?  Ich möchte kein Cyborg sein. “  Es ist notwendig, zusätzliche Details zu den Aussagen sorgfältig zu weben. <br><br>  <b>X:</b> Hast du eine Chance auf Unsterblichkeit? <br><br>  <b>Yu:</b> Buchstäblich?  Wörtliche Unsterblichkeit ist schwer zu erreichen.  Um viel länger als ein paar Billionen Jahre zu leben, müssen Sie das erwartete Schicksal eines expandierenden Universums überdenken.  Um länger als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Googolpleks</a> zu leben, ist es notwendig, dass wir einen Fehler in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Bezug auf</a> die Grundlagen physikalischer Gesetze machen und nicht nur in Details. <br><br>  Selbst wenn sich einige der ungewöhnlichen Überlegungen als wahr herausstellen und unser Universum Tochteruniversen erzeugen kann, wird dies uns keine Unsterblichkeit geben.  Um viel mehr Jahre mit Googleplex zu leben und sich nicht zu wiederholen, benötigen Sie Computer mit mehr Elementen als Google, und eine solche Maschine passt nicht in die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hubble-Sphäre</a> . <br><br>  Und Googolpleks ist nicht unendlich.  Um Martin Gardner zu paraphrasieren: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Grahams Zahl</a> ist immer noch recht klein, da die meisten endgültigen Zahlen viel größer sind als er.  Wenn Sie umgehauen werden möchten, lesen Sie über <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schnell wachsende Hierarchien</a> , und die Unendlichkeit wird noch länger sein.  Nur sehr seltsame und beängstigende anthropische Theorien ermöglichen es Ihnen, lange genug zu leben, um einen Stopp bei der am längsten laufenden Turing-Maschine mit Hunderten von Staaten zu beobachten. <br><br>  Ich glaube jedoch nicht, dass ich aus emotionaler Sicht lange genug leben möchte, um die hundertste Zahl im Spiel " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Jagd nach einem biberharten Arbeiter</a> " zu sehen.  Ich kann mich irgendwie in mich hineinversetzen, der in hundert Jahren gelebt hat.  Diese Zukunft werde ich in weiteren hundert Jahren selbst in die Zukunft einfühlen können.  Und vielleicht gibt es irgendwo in dieser Sequenz jemanden, der die Aussicht hat, seine Existenz zu beenden, und er kann darüber sehr traurig sein.  Aber ich bin mir nicht sicher, ob ich mir diese Person vorstellen kann.  „Ich möchte noch einen Tag leben.  Morgen werde ich auch noch einen Tag leben wollen.  Deshalb möchte ich für immer leben, was durch die Induktion positiver Ganzzahlen bewiesen wird. "  Sogar mein Wunsch nach einem langen Leben in einem physikalisch möglichen Universum ist eine Abstraktion, die durch Induktion erzeugt wird.  Ich kann mich in einer Billion Jahren nicht vorstellen. <br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich habe die Singularität als eine flüchtige und pseudowissenschaftliche Fantasie beschrieben, die uns vom Klimawandel, Kriegen, Ungleichheit und anderen ernsten Problemen ablenkt. Warum irre ich mich? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Weil Sie versuchen, empirische Fakten durch Psychoanalyse vorherzusagen. Es wird niemals funktionieren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nehmen wir an, wir erleben das Aufkommen der KI, die klug genug ist, um die KI genauso zu verbessern wie die Menschen. Er kann sich anpassen, programmieren, neue Algorithmen erfinden. Zu verbessern. Was wird als nächstes passieren - es wird intelligenter, sieht noch mehr Verbesserungsmöglichkeiten und erreicht schnell ein sehr hohes Niveau? Oder wird nichts Besonderes passieren?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es kann vorkommen, dass (A) die Selbstverbesserung eines bestimmten Deltas die KI so intelligent macht, dass sie zurückblicken und eine neue potenzielle Verbesserung der Größe des k * -Deltas finden kann, wobei k&gt; 1 ist, und dies wird viele Male wiederholt, um zu einer schnellen Selbstverbesserung zu führen Niveau der Superintelligenz. Was </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Irving John Goode</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> die "Explosion der Intelligenz" nannte. Oder (B), k ist kleiner als eins oder alle diese Verbesserungen sind klein und führen nicht zum Auftreten von Superintelligenz, oder Superintelligenz ist im Allgemeinen unmöglich, und anstelle einer Explosion wird es einen Zilch geben. Was ist wahr, A oder B? Wenn Sie eine KI eines bestimmten Niveaus erstellen und er dies versucht, geschieht etwas in der empirischen realen Welt, und dieses Ereignis wird durch Fakten bestimmt, die sich auf die Landschaft der Algorithmen und erreichbare Verbesserungen beziehen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuverlässige Informationen über dieses Ereignis können nicht aus der Psychoanalyse von Menschen erhalten werden. Es ist, als würde man versuchen, ein Auto ohne Kraftstoff zu starten - das sagt uns der Satz von Bayes. Einige Menschen werden immer Eskapisten sein, unabhängig von den tatsächlichen Werten versteckter Variablen in der Informatik. Daher kann die Beobachtung einiger Eskapisten nicht als strenger Beweis bezeichnet werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist ein Missverständnis über die Natur der Rationalität - dass es rational ist zu glauben, dass „Goblins in Schränken nicht existieren“, weil der Glaube an Goblins aus einem Schrank dumm, unreif, veraltet ist und nur Idioten daran glauben. Das wahre Prinzip der Rationalität besteht darin, im Schrank nachzusehen. In jenen Universen, in denen Goblins in Schränken leben, werden Sie an Goblins glauben, und in Universen, in denen Goblins nicht in Schränken sind, werden Sie nicht an sie glauben.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es ist schwierig, aber im Prinzip ist es möglich, durch die angelehnte Tür zu schauen und zu fragen: "Was wäre anders im Universum, wenn es nicht möglich wäre, ein gutes Einkommen aus kognitiven Investitionen zu erzielen, dh eine KI, die versucht, sich selbst zu verbessern, würde nicht mit einer Explosion enden, sondern mit einem Zilch?" Welche anderen Tatsachen wären für ein solches Universum charakteristisch? “ </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt Leute, die behaupten, dass KI nur auf das Niveau einer Person angehoben werden kann, da wir selbst Menschen sind und wir es nicht höher anheben können. Es scheint mir, dass wenn unser Universum so ist, wir einen Rückgang der Einnahmen aus Investitionen in Hardware und Software für Computerschach beobachten sollten, der das Niveau einer Person übersteigt - was tatsächlich nicht der Fall ist. Außerdem wäre natürliche Selektion dann nicht in der Lage, eine Person zu erschaffen, und Einsteins Mutter hätte eine erstaunliche Physikerin usw. sein sollen.</font></font> usw. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es gibt Leute, die argumentieren, dass je komplexer der Algorithmus ist, desto mehr Anpassungen erforderlich sind und dass unsere Intelligenz als eine Art Einschränkung für diesen Prozess dient. Dies stimmt jedoch nicht mit den anthropologischen Aufzeichnungen der menschlichen Intelligenz überein; Investitionen in Gehirn-Tuning und Mutationen bieten verbesserte kognitive Fähigkeiten. Wir wissen es, weil die Genetik uns sagt, dass Mutationen mit einer kleinen statistischen Antwort während der Evolution nicht behoben werden. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und Hominiden brauchten kein exponentiell größeres Gehirn als Schimpansen. Und der Kopf von </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">John von Neumann war</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nicht exponentiell größer als der Kopf eines Durchschnittsmenschen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aus rein praktischer Sicht übertragen menschliche Axone Informationen mit einer Geschwindigkeit, die eine Million Mal geringer ist als die Lichtgeschwindigkeit, und selbst unter dem Gesichtspunkt der Wärmeableitung verbraucht jede synaptische Operation eine Million Mal mehr als die minimale Wärmeableitung einer irreversiblen binären Operation bei 300 Kelvin und so weiter. Warum sollten wir annehmen, dass Gehirnsoftware näher am Optimum liegt als Eisen? Das Privileg der menschlichen Intelligenz besteht darin, dass es sich um die kleinste Intelligenzstufe handelt, mit der ein Computer erstellt werden kann. Wenn es möglich wäre, einen Computer mit einer niedrigeren Intelligenzstufe zu erstellen, würden wir dies auf einer niedrigeren Stufe diskutieren.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dies ist jedoch kein einfaches Argument, und für eine detaillierte Beschreibung sende ich Menschen zu einem meiner alten Werke, "Mikroökonomie der Explosion der Intelligenz", das leider immer noch als beste Informationsquelle dient. Aber genau diese Fragen müssen gestellt werden, um anhand der verfügbaren Beweise zu diskutieren, ob es zu einer KI-Explosion kommen wird, bei der eine gewisse Verbesserung der kognitiven Fähigkeiten, die in die Selbstoptimierung investiert werden, diese Verbesserung übersteigt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Was die Möglichkeiten und ihre Preise betrifft: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie können sich eine Welt ohne eine Explosion von Intelligenz und ohne Superintelligenz vorstellen. Oder eine Welt, in der die Tricks, mit denen Experten für maschinelles Lernen Super-KI kontrollieren, zur Kontrolle von Menschen und des übermenschlichen Regimes geeignet sind. Oder eine Welt, in der </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">moralischer Internalismus</font></a><font style="vertical-align: inherit;"> funktioniert</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, also sind alle ziemlich fortgeschrittenen AIs gut. In solchen Welten braucht niemand die ganze Arbeit und die Sorgen des Instituts für maschinelles Lernen. Und mehrere Moskitonetze wurden verschwendet, und es war besser, sie dem Fonds zur Bekämpfung der Malaria zu geben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sie können sich auch eine Welt vorstellen, in der Sie mit Malaria kämpfen, die Kohlenstoffemissionen bekämpfen und auf dem richtigen Niveau halten oder Geoengineering-Lösungen verwenden, um bereits gemachte Fehler zu neutralisieren. Und all dies erweist sich als nutzlos, weil die Zivilisation das Problem der KI-Moral nicht lösen kann - und alle Kinder, die mit Hilfe von Netzen vor Malaria gerettet wurden, nur so aufwachsen, dass Nanomaschinen sie im Traum töten.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, dass Menschen, die versuchen, sich für eine vernünftige Wohltätigkeit einzusetzen, zustimmen werden, dass wir in keiner dieser Welten leben möchten. Die Frage ist nur, welche wahrscheinlicher ist. Das zentrale Prinzip der Rationalität besteht darin, den Glauben an Goblins nicht abzulehnen, weil er dumm und nicht prestigeträchtig ist, und nicht an Goblins zu glauben, weil er gesund und schön ist. Das zentrale Prinzip der Rationalität ist, welche beobachtbaren Zeichen und logischen Schlussfolgerungen uns helfen, eine dieser beiden Welten zu wählen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich denke, die erste Welt ist unwahrscheinlich und die zweite wahrscheinlich. Ich verstehe, dass der Versuch, andere davon zu überzeugen, versucht, gegen den Fluss des Glaubens an die ewige Normalität zu schwimmen. Der Glaube, dass nur unsere kurzfristige Zivilisation, die seit mehreren Jahrzehnten existiert, und nur unsere Spezies, die nur einen Moment auf evolutionärer und geologischer Ebene existiert, Sinn machen und für immer existieren müssen. Und obwohl ich glaube, dass die erste Welt nur ein optimistischer Traum ist, denke ich nicht, dass wir das Problem ignorieren müssen, über das wir in Zukunft in Panik geraten werden. Die Mission des Instituts ist es, heute Forschung zu betreiben, die nach Angaben von Menschen, die nach 30 Jahren leben, vor 30 Jahren hätte beginnen sollen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Glaubt Ihre Frau Brijena an Singularität?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Brijena: Wenn mich jemand fragen würde, ob ich an eine Singularität glaube, würde ich eine Augenbraue hochziehen und fragen, ob er an automatische Lastwagen glaubt. Das ist eine seltsame Frage. Ich weiß nicht, wie die erste Flotte automatischer LKWs aussehen wird oder wie lange sie brauchen werden, um das vorhandene Frachttransportsystem zu ersetzen. Und ich bin nicht der Meinung, dass ich an Roboter-LKWs glaube. Ich gehe zuversichtlich davon aus, dass unbemannte Transporte moderne Transporte durch die Beteiligung von Menschen ersetzen werden, denn wir gehen in diese Richtung, wenn nichts wirklich Seltsames passiert. Aus dem gleichen Grund sage ich zuversichtlich eine Explosion der Intelligenz voraus. In anderen Bedeutungen des Wortes "Singularität" bin ich mir nicht so sicher. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Brijena gab ihre Antwort, ohne meine Antworten zu sehen. Es ist nur so, dass wir gut zueinander passen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ist es möglich, Superintelligenz zu schaffen, ohne zu verstehen, wie das Gehirn funktioniert? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In dem gleichen Sinne, wie man Flugzeuge bauen kann, ohne zu verstehen, wie ein Vogel fliegt. Sie müssen kein Experte für Vögel sein, aber gleichzeitig benötigen Sie viel Wissen, um ein Flugzeug zu bauen, das Sie im Prinzip bereits verstehen können, wie grob ein Vogel aus der Luft schwebt oder abstößt. Deshalb schreibe ich über menschliche Rationalität - wenn Sie in der Frage der maschinellen Intelligenz weit genug gehen, können Sie nicht anders, als sich einige Ideen darüber auszudenken, wie Menschen denken. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Was könnte Superintelligenz wollen? Werden sie so etwas wie sexuelles Verlangen haben? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Denken Sie an einen riesigen Raum von Möglichkeiten, an eine riesige mehrdimensionale Kugel. Dies ist ein Raum von Geistestypen, eine Reihe aller möglichen kognitiven Algorithmen. Stellen Sie sich vor, irgendwo am unteren Rand der Kugel befindet sich ein winziger Punkt, der alle Menschen kennzeichnet, die jemals gelebt haben. Dies ist ein winziger Punkt, da alle Menschen ungefähr das gleiche Gehirn haben, mit Kortex, Kleinhirn, Thalamus usw. Einige Menschen sind nicht wie andere, daher kann es sich um einen stacheligen Punkt handeln, aber die Spitzen haben den gleichen Maßstab wie der Punkt selbst. Unabhängig von Ihrer Neuroatypizität arbeiten Sie nicht an einem anderen kortikalen Algorithmus.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zu fragen, was Superintelligenz will, ist die falsche Frage. Superintelligenzen sind kein seltsamer Stamm von Menschen, die auf der anderen Seite des Flusses leben und exotische Bräuche haben. KI ist einfach der Name des gesamten Raums der Möglichkeiten außerhalb eines winzigen menschlichen Punktes. Mit ausreichendem Wissen können Sie in diesen Raum der Möglichkeiten aufsteigen und eine KI mit Wünschen herausholen, die von der Wunschliste in der menschlichen Sprache beschrieben werden kann, aber nicht, weil dies die natürliche Wunschliste dieser exotischen Übermenschen sein wird, sondern weil Sie einen Teil vom Raum der Geistestypen isoliert haben .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In Bezug auf sexuelle Wünsche - wenn Sie genau wissen, was Sie tun, haben Sie das Hauptproblem des Aufbaus von KI gelöst, indem Sie bestimmte Dinge stabil wollen, während es sich verbessert, wenn Sie das Hauptproblem gelöst haben, nützliche Funktionen der KI auf Aufgaben zu lenken, die einer Person täuschend einfach erscheinen, und Ein noch komplizierteres Problem ist die Konstruktion von KI unter Verwendung einer bestimmten Art von Architektur, in der Dinge wie „sexuelle Wünsche“ und „Glück durch Sex“ eine Rolle spielen. Dann können Sie KI möglicherweise dazu bringen, Menschen zu betrachten, die modellieren Sei ihr Wunsch, extrahiere diesen Teil von ihnen in Bezug auf das sexuelle Verlangen und lass die KI es erfahren. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Natürlich können Sie mit guten Kenntnissen der organischen Biologie und Aerodynamik auch Flugzeuge bauen, die sich mit Vögeln paaren können.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aber ich denke nicht, dass die Gebrüder Wright solche Dinge zu Beginn ihrer Aktivitäten hätten tun sollen. Das würde keinen Sinn ergeben. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Es erscheint vernünftiger, das Problem zu lösen, in den Raum des Geistes einzudringen und daraus eine KI zu extrahieren, die uns nicht in freie Atome zerlegen will. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich möchte denken, dass besonders kluge Kreaturen Gewaltfreiheit bekennen, weil sie verstehen, dass Gewalt dumm ist. Bin ich naiv </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich denke schon. </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">David hume</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich würde Ihnen sagen, dass Sie einen typischen Fehler machen, indem Sie das Prädikat „Dummheit“ auf die Werte oder offiziellen Operationen einer Person anwenden. Aktionen, Entscheidungen, Regeln können dumm sein, wenn Sie Vorlieben für den Endzustand der Welt haben. Wenn Sie eine Person mit Meta-Präferenzen sind, die Sie nicht vollständig berechnet haben, haben Sie möglicherweise eine Plattform, auf die Sie sich verlassen und Spekulationen über Objektpräferenzen als "dumm" bezeichnen können. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Staple Maximizer [ </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gedankenexperiment, das zeigt, wie KI, die ohne böswillige Absicht hergestellt wurde, der Menschheit schaden kann - ca.</font></font></i>  <i>perev.</i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">] macht keinen Rechenfehler und wählt die Fälle aus denen aus, in denen die maximale Anzahl von Heftklammern erhalten wird. Es befindet sich nicht in Ihrer Präferenzplattform, wenn Sie fehlerhafte Aktionen auswählen, und es befindet sich nicht in Ihrer Meta-Präferenzplattform, wenn Sie fälschlicherweise Präferenzen auswählen. Er berechnet die Antwort auf eine andere Frage und nicht auf die, die Sie sich stellen, die Frage "Was soll ich tun?" Der Heftklammermaximierer führt einfach die Aktion aus, die zu den meisten Heftklammern führt. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein fatales Szenario ist, wenn die KI Sie weder liebt noch hasst, weil Sie aus Atomen bestehen, mit denen sie etwas anderes erschaffen kann. Spieltheorie und Probleme der Zusammenarbeit im </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gefangenendilemma</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">manifestieren sich nicht in allen möglichen Fällen. Sie werden beispielsweise nicht angezeigt, wenn ein bestimmtes Motiv so viel stärker ist als Sie, dass es Sie zu Atomen machen kann, wenn Sie auf die Schaltflächen "Zusammenarbeiten" oder "Ändern" klicken möchten. Und wenn wir diese Schwelle überschreiten, haben Sie entweder das Problem gelöst, etwas zu schaffen, das Ihnen nicht schaden will, oder Sie haben bereits verloren. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wird Superintelligenz </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">das schwierige Problem des Bewusstseins lösen</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ja, und im Rückblick wird uns die Antwort schändlich einfach erscheinen. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Werden Superintelligenzen einen freien Willen haben? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ja, aber sie werden nicht die Illusion eines freien Willens haben. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">X:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Wie sieht deine Utopie aus? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yu:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Ich werde deine Leser zu meinen leiten. "</font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sequenzen der Unterhaltungstheorie,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> "da ich es noch nicht geschafft habe, eine Geschichte zu schreiben, deren Handlung in einer unterhaltsam-theoretischen optimalen Welt stattfindet.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de404137/">https://habr.com/ru/post/de404137/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de404125/index.html">Der Countdown ist abgelaufen: Noch 7 Tage bis zum Polybius ICO</a></li>
<li><a href="../de404127/index.html">Der erste Start des Electron LV war teilweise erfolgreich</a></li>
<li><a href="../de404129/index.html">Happy Geeks Day (ja, er ist heute)</a></li>
<li><a href="../de404133/index.html">Bis Ende dieses Jahres will Google einen 49-Qubit-Quantencomputer in Betrieb nehmen</a></li>
<li><a href="../de404135/index.html">Google sammelt und analysiert Offline-Käufe von Android Pay-Nutzern</a></li>
<li><a href="../de404139/index.html">Bitcoin in Russland: Steuern (ein paar einfache Fragen)</a></li>
<li><a href="../de404141/index.html">Unlauterer Wettbewerb beim Anbieter</a></li>
<li><a href="../de404143/index.html">Tiny Nuggets: Ein Rückblick auf die russischen Registrare TrendVision Split and Tube</a></li>
<li><a href="../de404147/index.html">Sound, du bist nur "Weltraum": Campfire Audio Andromeda Kopfhörer</a></li>
<li><a href="../de404149/index.html">Test des wasserdichten Lesegeräts der neuen Generation PocketBook 641 Aqua 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>