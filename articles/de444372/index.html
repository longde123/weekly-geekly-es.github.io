<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üñáÔ∏è üë©üèª üåó Maschinenorientierung √ºber gro√üe Entfernungen durch verst√§rktes Lernen ‚úíÔ∏è üë®üèø‚Äçüé§ üíà</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Allein in den Vereinigten Staaten gibt es 3 Millionen Menschen mit Behinderungen, die ihre H√§user nicht verlassen k√∂nnen. Hilfsroboter, die automatisc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Maschinenorientierung √ºber gro√üe Entfernungen durch verst√§rktes Lernen</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/444372/">  Allein in den Vereinigten Staaten gibt es 3 Millionen Menschen mit Behinderungen, die ihre H√§user nicht verlassen k√∂nnen.  Hilfsroboter, die automatisch √ºber gro√üe Entfernungen navigieren k√∂nnen, k√∂nnen diese Menschen unabh√§ngiger machen, indem sie ihnen Lebensmittel, Medikamente und Pakete bringen.  Studien zeigen, dass Deep Learning mit Verst√§rkung (OP) gut geeignet ist, um rohe Eingabedaten und Aktionen zu vergleichen, z. B. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">um Objekte</a> zu <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">erfassen</a> oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Roboter zu bewegen. In der</a> Regel fehlt den OP- <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Agenten jedoch</a> das Verst√§ndnis f√ºr gro√üe physische R√§ume, die f√ºr eine sichere Orientierung auf Ferngespr√§che erforderlich sind Entfernungen ohne menschliche Hilfe und Anpassung an eine neue Umgebung. <br><a name="habracut"></a><br>  In drei k√ºrzlich erschienenen Arbeiten, ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Orientierungslauftraining von Grund auf mit AOP</a> ‚Äú, ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PRM-RL: Implementierung von Roboter-Orientierungslauf √ºber gro√üe Entfernungen mithilfe einer Kombination aus Verst√§rkungslernen und musterbasierter Planung</a> ‚Äú und ‚Äû <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Langstrecken-Orientierungslauf mit PRM-RL</a> ‚Äú, haben wir Wir untersuchen autonome Roboter, die sich leicht an eine neue Umgebung anpassen lassen und Deep OP mit langfristiger Planung kombinieren.  Wir bringen lokalen Planern bei, wie sie die grundlegenden Aktionen ausf√ºhren, die zur Orientierung erforderlich sind, und wie sie kurze Strecken ohne Kollisionen mit sich bewegenden Objekten zur√ºcklegen.  Lokale Planer f√ºhren verrauschte Umgebungsbeobachtungen mit Sensoren wie eindimensionalen Lidaren durch, die den Abstand zu einem Hindernis bereitstellen und lineare und Winkelgeschwindigkeiten zur Steuerung des Roboters bereitstellen.  Wir schulen den lokalen Planer in Simulationen mithilfe des automatischen Verst√§rkungslernens (AOP), einer Methode, die die Suche nach Belohnungen f√ºr das OP und die Architektur des neuronalen Netzwerks automatisiert.  Trotz der begrenzten Reichweite von 10 bis 15 m passen sich lokale Planer sowohl f√ºr den Einsatz in echten Robotern als auch f√ºr neue, bisher unbekannte Umgebungen gut an.  Auf diese Weise k√∂nnen Sie sie als Bausteine ‚Äã‚Äãf√ºr die Ausrichtung auf gro√üen R√§umen verwenden.  Dann erstellen wir eine Stra√üenkarte, ein Diagramm, in dem die Knoten separate Abschnitte sind, und die Kanten verbinden die Knoten nur dann, wenn lokale Planer, die echte Roboter mit lauten Sensoren und Steuerungen gut imitieren, zwischen ihnen wechseln k√∂nnen. <br><br><h2>  Automatisches Verst√§rkungslernen (AOP) </h2><br>  In <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">unserer ersten Arbeit</a> schulen wir einen lokalen Planer in einer kleinen statischen Umgebung.  Beim Lernen mit dem Standard-Deep-OP-Algorithmus, beispielsweise dem Deep Deterministic Gradient ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">DDPG</a> ), gibt es jedoch mehrere Hindernisse.  Zum Beispiel ist das eigentliche Ziel der lokalen Planer, ein bestimmtes Ziel zu erreichen, wodurch sie seltene Belohnungen erhalten.  In der Praxis m√ºssen die Forscher viel Zeit f√ºr die schrittweise Implementierung des Algorithmus und die manuelle Anpassung der Auszeichnungen aufwenden.  Die Forscher m√ºssen auch Entscheidungen √ºber die Architektur neuronaler Netze treffen, ohne klare, erfolgreiche Rezepte zu haben.  Schlie√ülich lernen Algorithmen wie DDPG instabil und zeigen oft <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">katastrophale Vergesslichkeit</a> . <br><br>  Um diese Hindernisse zu √ºberwinden, haben wir tiefes Lernen mit Verst√§rkung automatisiert.  AOP ist ein evolution√§rer automatischer Wrapper um ein tiefes OP, der Belohnungen und neuronale Netzwerkarchitektur durch <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">umfassende Hyperparameteroptimierung</a> sucht.  Es funktioniert in zwei Schritten: der Suche nach Belohnungen und der Suche nach Architektur.  W√§hrend der Suche nach Belohnungen trainiert AOP gleichzeitig die Population der DDPG-Agenten √ºber mehrere Generationen hinweg, und jeder hat seine eigene leicht modifizierte Belohnungsfunktion, die f√ºr die wahre Aufgabe des lokalen Planers optimiert ist: das Erreichen des Endpunkts des Pfades.  Am Ende der Belohnungssuchphase w√§hlen wir eine aus, die die Agenten am h√§ufigsten zum Ziel f√ºhrt.  In der Suchphase der neuronalen Netzwerkarchitektur wiederholen wir diesen Vorgang f√ºr dieses Rennen unter Verwendung der ausgew√§hlten Auszeichnung und Anpassen der Netzwerkschichten, um die kumulative Auszeichnung zu optimieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP mit der Suche nach Auszeichnung und Architektur des neuronalen Netzes</i> <br><br>  Dieser schrittweise Prozess macht AOP jedoch hinsichtlich der Anzahl der Proben unwirksam.  F√ºr ein AOP-Training mit 10 Generationen von 100 Wirkstoffen sind 5 Milliarden Proben erforderlich, was 32 Studienjahren entspricht!  Der Vorteil ist, dass nach AOP der manuelle Lernprozess automatisiert wird und DDPG kein katastrophales Vergessen hat.  Am wichtigsten ist, dass die Qualit√§t der endg√ºltigen Richtlinien h√∂her ist - sie sind best√§ndig gegen St√∂rungen durch Sensor, Laufwerk und Lokalisierung und lassen sich gut auf neue Umgebungen √ºbertragen.  Unsere beste Strategie ist 26% erfolgreicher als andere Orientierungsmethoden an unseren Teststandorten. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Rot - AOP-Erfolge auf kurzen Strecken (bis zu 10 m) in mehreren bisher unbekannten Geb√§uden.</i>  <i>Vergleich mit manuell trainiertem DDPG (dunkelrot), k√ºnstlichen Potentialfeldern (blau), dynamischem Fenster (blau) und Verhaltensklonen (gr√ºn).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Die lokale AOP-Scheduler-Richtlinie funktioniert gut mit Robotern in realen unstrukturierten Umgebungen</i> <br><br>  Und obwohl diese Politiker nur lokal orientiert sind, sind sie widerstandsf√§hig gegen sich bewegende Hindernisse und werden von echten Robotern in unstrukturierten Umgebungen gut vertragen.  Und obwohl sie in Simulationen mit statischen Objekten geschult wurden, bew√§ltigen sie effektiv bewegte Objekte.  Der n√§chste Schritt besteht darin, AOP-Richtlinien mit einer stichprobenbasierten Planung zu kombinieren, um ihren Arbeitsbereich zu erweitern und ihnen das Navigieren √ºber gro√üe Entfernungen beizubringen. <br><br><h2>  Fernorientierung mit PRM-RL </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Musterbasierte Planer</a> arbeiten mit gro√üer Ausrichtung und n√§hern sich den Roboterbewegungen an.  Beispielsweise erstellt ein Roboter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">probabilistische Roadmaps</a> (PRMs), indem er √úbergangspfade zwischen Abschnitten zeichnet.  In unserer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">zweiten Arbeit</a> , die auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ICRA 2018-</a> Konferenz ausgezeichnet wurde, kombinieren wir PRM mit manuell abgestimmten lokalen OP-Schedulern (ohne AOP), um Roboter lokal zu trainieren und sie dann an andere Umgebungen anzupassen. <br><br>  Zun√§chst trainieren wir f√ºr jeden Roboter die lokale Scheduler-Richtlinie in einer verallgemeinerten Simulation.  Anschlie√üend erstellen wir unter Ber√ºcksichtigung dieser Richtlinie ein PRM, das sogenannte PRM-RL, basierend auf einer Karte der Umgebung, in der es verwendet wird.  Dieselbe Karte kann f√ºr jeden Roboter verwendet werden, den wir im Geb√§ude verwenden m√∂chten. <br><br>  Um ein PRM-RL zu erstellen, kombinieren wir Knoten aus Samples nur, wenn der lokale OP-Scheduler zuverl√§ssig und wiederholt zwischen ihnen wechseln kann.  Dies erfolgt in einer Monte-Carlo-Simulation.  Die resultierende Karte passt sich den F√§higkeiten und der Geometrie eines bestimmten Roboters an.  Karten f√ºr Roboter mit derselben Geometrie, aber unterschiedlichen Sensoren und Antrieben haben unterschiedliche Konnektivit√§t.  Da sich der Agent um die Ecke drehen kann, k√∂nnen auch Knoten aktiviert werden, die sich nicht in direkter Sichtlinie befinden.  Es ist jedoch weniger wahrscheinlich, dass Knoten, die an W√§nde und Hindernisse angrenzen, aufgrund von Sensorrauschen in die Karte aufgenommen werden.  Zur Laufzeit bewegt sich der OP-Agent √ºber die Karte von einem Abschnitt zum anderen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>F√ºr jedes zuf√§llig ausgew√§hlte Knotenpaar wird eine Karte mit drei Monte-Carlo-Simulationen erstellt</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>Die gr√∂√üte Karte war 288 x 163 m gro√ü und enthielt fast 700.000 Kanten.</i>  <i>300 Arbeiter sammelten es f√ºr 4 Tage, nachdem sie 1,1 Milliarden Kollisionskontrollen durchgef√ºhrt hatten.</i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Die dritte Arbeit</a> bietet mehrere Verbesserungen gegen√ºber dem urspr√ºnglichen PRM-RL.  Erstens ersetzen wir das manuell abgestimmte DDPG durch lokale AOP-Scheduler, wodurch sich die Ausrichtung √ºber gro√üe Entfernungen verbessert.  Zweitens werden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Karten zur gleichzeitigen Lokalisierung und Markierung</a> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SLAM</a> ) hinzugef√ºgt, die Roboter zur Laufzeit als Quelle f√ºr die Erstellung von Roadmaps verwenden.  SLAM-Karten sind Rauschen ausgesetzt, und dies schlie√üt die ‚ÄûL√ºcke zwischen Simulator und Realit√§t‚Äú, ein bekanntes Problem in der Robotik, aufgrund dessen sich in Simulationen geschulte Agenten in der realen Welt viel schlechter verhalten.  Unser Erfolgsniveau in der Simulation stimmt mit dem Erfolgsniveau realer Roboter √ºberein.  Und schlie√ülich haben wir verteilte Geb√§udekarten hinzugef√ºgt, damit wir sehr gro√üe Karten mit bis zu 700.000 Knoten erstellen k√∂nnen. <br><br>  Wir haben diese Methode mit Hilfe unseres AOP-Agenten evaluiert, der Karten basierend auf Zeichnungen von Geb√§uden erstellt hat, die die Trainingsumgebung um das 200-fache √ºberschritten haben, einschlie√ülich nur Rippen, die in 90% der F√§lle in 20 Versuchen erfolgreich abgeschlossen wurden.  Wir haben PRM-RL mit verschiedenen Methoden in Entfernungen von bis zu 100 m verglichen, was die Reichweite des lokalen Planers deutlich √ºberstieg.  PRM-RL erzielte aufgrund der korrekten Verbindung der Knoten, die f√ºr die F√§higkeiten des Roboters geeignet ist, 2-3-mal h√§ufiger Erfolge als herk√∂mmliche Methoden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>Erfolgsquote beim Umzug von 100 m in verschiedenen Geb√§uden.</i>  <i>Blau - lokaler AOP-Scheduler, erster Job;</i>  <i>rot - original PRM;</i>  <i>gelb - k√ºnstliche Potentialfelder;</i>  <i>Gr√ºn ist der zweite Job;</i>  <i>rot - der dritte Job, PRM mit AOP.</i> <br><br>  Wir haben PRM-RL an vielen echten Robotern in vielen Geb√§uden getestet.  Unten finden Sie eine der Testsuiten.  Der Roboter bewegt sich fast √ºberall zuverl√§ssig, mit Ausnahme der unordentlichsten Stellen und Bereiche, die √ºber die SLAM-Karte hinausgehen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Fazit </h2><br>  Maschinenorientierung kann die Unabh√§ngigkeit von Menschen mit eingeschr√§nkter Mobilit√§t ernsthaft erh√∂hen.  Dies kann erreicht werden, indem autonome Roboter entwickelt werden, die sich leicht an die Umgebung anpassen lassen, und die Methoden, die f√ºr die Implementierung in der neuen Umgebung verf√ºgbar sind, basierend auf vorhandenen Informationen.  Dies kann erreicht werden, indem das grundlegende Orientierungstraining f√ºr kurze Strecken mit AOP automatisiert und die erworbenen F√§higkeiten zusammen mit SLAM-Karten verwendet werden, um Roadmaps zu erstellen.  Roadmaps bestehen aus Knoten, die durch Rippen verbunden sind, auf denen sich Roboter zuverl√§ssig bewegen k√∂nnen.  Als Ergebnis wird eine Roboterverhaltensrichtlinie entwickelt, die nach einem Training in verschiedenen Umgebungen verwendet werden kann und Roadmaps erstellt, die speziell f√ºr einen bestimmten Roboter angepasst wurden. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de444372/">https://habr.com/ru/post/de444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de444362/index.html">Unity und Havok arbeiten an einer neuen Physik-Engine</a></li>
<li><a href="../de444364/index.html">24 Stunden Rostspiel: Erfahrung in der pers√∂nlichen Entwicklung</a></li>
<li><a href="../de444366/index.html">Seminar "Anforderungen an die Informationssicherheit: Wie Unternehmen mit ihnen leben k√∂nnen"</a></li>
<li><a href="../de444368/index.html">Wir haben das Mikrofon gerade auf einem 3D-Drucker im Labor gedruckt - und dann wird es eine vollst√§ndige Science-Fiction geben</a></li>
<li><a href="../de444370/index.html">Was kann das Mini PCI-e-Format?</a></li>
<li><a href="../de444374/index.html">Hipster-Effekt: Warum Nonkonformisten oft gleich aussehen</a></li>
<li><a href="../de444376/index.html">Die Wirtschaft der Aufmerksamkeit ist fast tot</a></li>
<li><a href="../de444378/index.html">USPACE - Single Space f√ºr bemannte und unbemannte Flugzeuge</a></li>
<li><a href="../de444382/index.html">So besuchen Sie die Korea University mit dem Network File System</a></li>
<li><a href="../de444384/index.html">Buch "Angewandte Textdatenanalyse in Python"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>