<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>👂🏽 🎣 🚣🏾 Erstellen einer Datenflussvorlage zum Streamen von Daten von Pub / Sub zu BigQuery basierend auf GCP mit dem Apache Beam SDK und Python 🤛🏻 👆🏽 👏🏾</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Moment beschäftige ich mich mit dem Streaming (und Konvertieren) von Daten. In einigen Kreisen 
 ein solcher Prozess ist als ETL bekannt , d.h. Ext...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen einer Datenflussvorlage zum Streamen von Daten von Pub / Sub zu BigQuery basierend auf GCP mit dem Apache Beam SDK und Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441892/"><p><img src="https://habrastorage.org/webt/is/qf/7j/isqf7j7patfl6lgirlqfrfbz-k8.png" alt="Bild"></p><br><p>  Im Moment beschäftige ich mich mit dem Streaming (und Konvertieren) von Daten.  In einigen Kreisen <br>  ein solcher Prozess ist als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ETL bekannt</a> , d.h.  Extrahieren, Konvertieren und Laden von Informationen. </p><br><p>  Der gesamte Prozess umfasst die Teilnahme der folgenden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Cloud Platform-</a> Dienste: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pub / Sub-</a> Service für Echtzeit-Daten-Streaming </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenfluss</a> - ein Dienst zum Konvertieren von Daten (can <br>  sowohl in Echtzeit als auch im Batch-Modus arbeiten) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BigQuery</a> - ein Dienst zum Speichern von Daten in Form von Tabellen <br>  (unterstützt SQL) </li></ul><a name="habracut"></a><br><h5 id="0-tekuschee-polozhenie-del">  0. Aktueller Status </h5><br><p>  Im Moment gibt es eine funktionierende Version des Streamings für die oben genannten Dienste, jedoch in <br>  Als Vorlage wird eine der <a href="">Standardvorlagen verwendet</a> . </p><br><p> Das Problem besteht darin, dass diese Vorlage eine 1: 1-Datenübertragung bereitstellt, d. H.  auf <br>  Am Eingang zu Pub / Sub haben wir eine Zeichenfolge im JSON-Format. Am Ausgang haben wir eine BigQuery-Tabelle mit Feldern. <br>  die den Schlüsseln der Objekte auf der obersten Ebene des Eingabe-JSON entsprechen. </p><br><h5 id="1-postanovka-zadachi">  1. Erklärung des Problems </h5><br><p> Erstellen Sie eine Datenflussvorlage, mit der Sie eine oder mehrere Tabellen an der Ausgabe abrufen können <br>  gemäß den gegebenen Bedingungen.  Zum Beispiel möchten wir für jede eine eigene Tabelle erstellen <br>  Werte eines bestimmten Eingabe-JSON-Schlüssels.  Es ist notwendig, die Tatsache zu berücksichtigen, dass einige <br>  Eingabe-JSON-Objekte können verschachtelten JSON als Wert enthalten, d. H.  ist notwendig <br>  Sie können BigQuery-Tabellen mit Feldern vom Typ <code>RECORD</code> zum Speichern <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschachtelter</a> Tabellen erstellen <br>  Daten. </p><br><h5 id="2-podgotovka-k-resheniyu">  2. Vorbereitung auf die Entscheidung </h5><br><p>  Verwenden Sie zum Erstellen einer Datenflussvorlage das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Beam</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SDK</a> , das wiederum <br>  unterstützt Java und Python als Programmiersprache.  Das muss ich sagen <br>  Es wird nur die Version Python 2.7.x unterstützt, was mich etwas überrascht hat.  Darüber hinaus Unterstützung <br>  Java ist etwas breiter, weil  Für Python sind beispielsweise einige Funktionen nicht verfügbar und mehr <br>  Eine bescheidene Liste integrierter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anschlüsse</a> .  Übrigens können Sie Ihre eigenen Konnektoren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schreiben</a> . </p><br><p>  Aufgrund der Tatsache, dass ich mit Java nicht vertraut bin, habe ich Python verwendet. </p><br><p>  Bevor Sie mit dem Erstellen einer Vorlage beginnen, müssen Sie über Folgendes verfügen: </p><br><ol><li>  Geben Sie das JSON-Format ein und es sollte sich nicht rechtzeitig ändern </li><li>  Schema oder Schemata von BigQuery-Tabellen, in die Daten gestreamt werden </li><li>  Die Anzahl der Tabellen, in die der Ausgabedatenstrom gestreamt wird </li></ol><br><p>  Beachten Sie, dass diese Parameter nach dem Erstellen einer Vorlage und dem Starten des darauf basierenden Datenflussjobs verwendet werden können <br>  Ändern Sie dies nur, indem Sie eine neue Vorlage erstellen. </p><br><p>  Lassen Sie uns ein paar Worte zu diesen Einschränkungen sagen.  Sie alle kommen von der Tatsache, dass es keine Möglichkeit gibt <br>  Erstellen Sie eine dynamische Vorlage, die eine beliebige Zeichenfolge als Eingabe verwenden kann, und analysieren Sie sie <br>  nach interner Logik und füllen Sie dann dynamisch erstellte Tabellen dynamisch mit <br>  von der Schaltung erstellt.  Es ist sehr wahrscheinlich, dass diese Möglichkeit besteht, jedoch innerhalb der Daten <br>  Es ist mir nicht gelungen, ein solches System umzusetzen.  Soweit ich das Ganze verstehe <br>  Die Pipeline wird erstellt, bevor sie zur Laufzeit ausgeführt wird. Daher gibt es keine Möglichkeit, sie zu ändern <br>  fliegen.  Vielleicht teilt jemand seine Entscheidung. </p><br><h5 id="3-reshenie">  3. Entscheidung </h5><br><p>  Für ein vollständigeres Verständnis des Prozesses lohnt es sich, ein Diagramm der sogenannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pipeline zu erstellen</a> <br>  aus der Apache Beam-Dokumentation. </p><br><p><img src="https://habrastorage.org/webt/yq/yi/3z/yqyi3zjiwpmjf4i6qp7x4znqv2c.png" alt="Bild"></p><br><p>  In unserem Fall (wir werden die Unterteilung in mehrere Tabellen verwenden): </p><br><ul><li>  Eingabedaten stammen von PubSub im Datenflussjob </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformation</a> Nr. 1 - Daten werden von einer Zeichenfolge in ein Python-Wörterbuch konvertiert. Wir erhalten eine Ausgabe <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PCollection</a> # 1 </li><li>  Transformation Nr. 2 - Die Daten werden zur weiteren Trennung gemäß separaten Tabellen in markiert <br>  Die Ausgabe ist PCollection # 2 (eigentlich ein PCollection-Tupel). </li><li>  Transformation Nr. 3 - Daten aus PCollection Nr. 2 werden mithilfe von Schemata in Tabellen geschrieben <br>  Tabellen </li></ul><br><p>  Beim Schreiben meiner eigenen Vorlage habe ich mich aktiv von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen</a> Beispielen inspirieren lassen. </p><br><div class="spoiler">  <b class="spoiler_title">Vorlagencode mit Kommentaren (linke Kommentare auf die gleiche Weise wie bei früheren Autoren):</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding=utf-8 from __future__ import absolute_import import logging import json import os import apache_beam as beam from apache_beam.pvalue import TaggedOutput from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions from apache_beam.options.pipeline_options import StandardOptions from apache_beam.io.gcp.bigquery import parse_table_schema_from_json #  GCP  gcp_project = '' #  Pub/Sub  topic_name = '' # Pub/Sub    'projects/_GCP_/topics/_' input_topic = 'projects/%s/topics/%s' % (gcp_project, topic_name) #  BigQuery  bq_dataset = 'segment_eu_test' #       schema_dir = './' class TransformToBigQuery(beam.DoFn): #          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #   yield body class TagDataWithReqType(beam.DoFn): #      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element def run(): #       _.json   schema_dir,  #         ()  schema_dct = {} for schema_file in os.listdir(schema_dir): filename_list = schema_file.split('.') if filename_list[-1] == 'json': with open('%s/%s' % (schema_dir, schema_file)) as f: schema_json = f.read() schema_dct[filename_list[0]] = json.dumps({'fields': json.loads(schema_json)}) # We use the save_main_session option because one or more DoFn's in this # workflow rely on global context (eg, a module imported at module level). pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = True pipeline_options.view_as(StandardOptions).streaming = True # Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic) # Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery()) # Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()). with_outputs(*schema_dct.keys(), main='default') # Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), ) # Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), ) result = p.run() result.wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) logger = logging.getLogger(__name__) run()</span></span></code> </pre> </div></div><br><p>  Jetzt werden wir den Code durchgehen und Erklärungen geben, aber zuerst ist es erwähnenswert, dass der Hauptcode <br>  Die Schwierigkeit beim Schreiben dieser Vorlage besteht darin, in Bezug auf den "Datenstrom" zu denken, und <br>  keine bestimmte Nachricht.  Es ist auch wichtig zu verstehen, dass Pub / Sub mit Nachrichten und arbeitet <br>  Von ihnen erhalten wir Informationen zum Markieren des Streams. </p><br><pre> <code class="python hljs">pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> pipeline_options.view_as(StandardOptions).streaming = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span></code> </pre> <br><p>  Weil  Der Apache Beam Pub / Sub IO-Anschluss wird nur im erforderlichen Streaming-Modus verwendet <br>  Fügen Sie PipelineOptions () hinzu (obwohl die Optionen tatsächlich nicht verwendet werden). Andernfalls erstellen Sie eine Vorlage <br>  fällt mit der Ausnahme.  Es muss über die Optionen zum Starten der Vorlage gesagt werden.  Sie können sein <br>  statische und sogenannte "Laufzeit".  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist ein</a> Link zur Dokumentation zu diesem Thema.  Mit den Optionen können Sie eine Vorlage erstellen, ohne zuvor Parameter anzugeben, diese jedoch beim Starten des Datenflussjobs über die Vorlage übergeben. Ich konnte sie jedoch immer noch nicht implementieren, wahrscheinlich weil dieser Connector <code>RuntimeValueProvider</code> nicht unterstützt. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic)</span></span></code> </pre> <br><p>  Alles ist aus dem Kommentar klar, wir lesen den Thread aus dem Thema.  Es lohnt sich hinzuzufügen, dass Sie den Stream nehmen können <br>  sowohl aus dem Thema als auch aus dem Abonnement (Abonnement).  Wenn das Thema als Eingabe angegeben ist, dann <br>  Ein temporäres Abonnement für dieses Thema wird automatisch erstellt.  Die Syntax ist auch hübsch <br>  klar, Eingabedatenstrom <code>beam.io.ReadFromPubSub(input_topic)</code> an unsere gesendet <br>  Pipeline <code>p</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery())</span></span></code> </pre> <br><p>  Hier findet die Transformation Nr. 1 statt und unsere Eingabe wird von einer Python-Zeichenfolge in konvertiert <br>  Python-Diktat, und in der Ausgabe erhalten wir PCollection # 1.  <code>&gt;&gt;</code> erscheint in der Syntax.  Ein <br>  Tatsächlich ist der Text in Anführungszeichen der Name des Streams (muss eindeutig sein) sowie ein Kommentar. <br>  Dies wird dem Block im Diagramm in der GCP Dataflow-Weboberfläche hinzugefügt.  Lassen Sie uns genauer betrachten <br>  überschriebene Klasse <code>TransformToBigQuery</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TransformToBigQuery</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #  ,      python dict yield body</span></span></code> </pre> <br><p>  Die Elementvariable enthält eine Nachricht aus dem PubSub-Abonnement.  Wie von gesehen <br>  Code, in unserem Fall sollte es gültiger JSON sein.  Im Klassenzimmer muss sein <br>  Die <code>process</code> wird neu definiert, in der die erforderlichen Transformationen durchgeführt werden sollen <br>  Eingangsleitung, um einen Ausgang zu erhalten, der der Schaltung entspricht <br>  die Tabelle, in die diese Daten geladen werden.  Weil  Unser Fluss in diesem Fall ist <br>  kontinuierlich, <code>unbounded</code> in Bezug auf Apache Beam, müssen Sie es mit zurückgeben <br>  <code>yield</code> , nicht <code>return</code> , wie beim endgültigen Datenstrom.  Im Falle eines endgültigen Ablaufs können Sie <br>  (und notwendig) zusätzlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>windowing</code></a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>triggers</code></a> konfigurieren </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()).with_outputs(*schema_dct.keys(), main='default')</span></span></code> </pre> <br><p>  Dieser Code leitet PCollection # 1 zu Transform # 2, wo das Tagging stattfinden wird <br>  (Trennung) des Datenstroms.  In der Variablen <code>schema_dct</code> in diesem Fall einem Wörterbuch, in dem der Schlüssel der Name der <code>schema_dct</code> ohne die Erweiterung ist, ist dies das Tag und der Wert der gültige JSON des Schemas <br>  BigQuery-Tabellen für dieses Tag.  Es ist zu beachten, dass das Schema genau an übertragen werden sollte <br>  view <code>{'fields': }</code> wobei <code></code> das Schema der BigQuery-Tabelle in JSON-Form ist (Sie können <br>  Export von der Weboberfläche). </p><br><p>  <code>main='default'</code> ist der Name des Thread-Tags, zu dem sie gehen <br>  Alle Nachrichten, für die keine Tagging-Bedingungen gelten.  Betrachten Sie die Klasse <br>  <code>TagDataWithReqType</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TagDataWithReqType</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element</span></span></code> </pre> <br><p>  Wie Sie sehen, wird auch hier die <code>process</code> überschrieben.  Die Variable <code>types</code> enthält Namen <br>  Tags und sie müssen die Nummer und den Namen mit der Nummer und den Namen der Wörterbuchschlüssel übereinstimmen <br>  <code>schema_dct</code> .  Obwohl die <code>process</code> Argumente akzeptieren kann, habe ich nie <br>  Ich konnte sie bestehen.  Ich habe den Grund noch nicht herausgefunden. </p><br><p>  Bei der Ausgabe erhalten wir ein Tupel von Threads in der Anzahl der Tags, nämlich der Anzahl unserer <br>  vordefinierte Tags + Standard-Thread, der nicht markiert werden konnte. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), )</span></span></code> </pre> <br><p>  Transform # ... (tatsächlich ist es nicht im Diagramm, dies ist ein "Zweig") - wir schreiben den Standard-Stream <br>  zur Standardtabelle. </p><br><p>  <code>tagged_stream.default</code> - Ein Stream mit dem <code>default</code> Tag wird verwendet. Die alternative Syntax lautet <code>tagged_stream['default']</code> </p><br><p>  <code>schema=parse_table_schema_from_json(schema_dct.get('default'))</code> - hier wird das Schema definiert <br>  Tabellen.  Beachten Sie, dass die Datei default.json das gültige BigQuery-Tabellenschema enthält <br>  muss sich im aktuellen <code>schema_dir = './'</code> . </p><br><p>  Der Stream wird in eine Tabelle namens <code>default</code> . </p><br><p>  Wenn eine Tabelle mit diesem Namen (im angegebenen Datensatz dieses Projekts) nicht vorhanden ist, ist sie vorhanden <br>  wird dank der Standardeinstellung automatisch aus dem Schema erstellt <br> <code>create_disposition=BigQueryDisposition.CREATE_IF_NEEDED</code> </p> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), )</span></span></code> </pre> <br><p>  Transform # 3, alles sollte für diejenigen klar sein, die den Artikel von Anfang an lesen und besitzen <br>  Python-Syntax.  Wir trennen das Stream-Tupel durch eine Schleife und schreiben jeden Stream in eine eigene Tabelle mit <br>  sein Schema.  Es sei daran erinnert, dass der <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> eindeutig sein muss - <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> . </p><br><p>  Jetzt sollte klar sein, wie das funktioniert und Sie können eine Vorlage erstellen.  Dafür brauchst du <br>  Führen Sie es in der Konsole aus (vergessen Sie nicht, venv zu aktivieren, falls verfügbar) oder in der IDE: </p><br><pre> <code class="bash hljs">python _.py / --runner DataflowRunner / --project dreamdata-test / --staging_location gs://STORAGE_NAME/STAGING_DIR / --temp_location gs://STORAGE_NAME/TEMP_DIR / --template_location gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> </pre> <br><p>  Gleichzeitig sollte der Zugriff auf das Google-Konto beispielsweise durch Export organisiert werden <br>  die Umgebungsvariable <code>GOOGLE_APPLICATION_CREDENTIALS</code> oder eine andere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Methode</a> . </p><br><p>  Ein paar Worte zu <code>--runner</code> .  In diesem Fall sagt <code>DataflowRunner</code> , dass dieser Code <br>  wird als Vorlage für den Datenflussjob ausgeführt.  Es ist weiterhin möglich anzugeben <br>  <code>DirectRunner</code> wird standardmäßig verwendet, wenn keine Option vorhanden ist - <code>--runner</code> und Code <br>  funktioniert als Datenflussjob, jedoch lokal, was für das Debuggen sehr praktisch ist. </p><br><p>  Wenn keine Fehler aufgetreten sind, <code>gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> <br>  Vorlage erstellt.  Es ist erwähnenswert, dass in <code>gs://STORAGE_NAME/STAGING_DIR</code> auch geschrieben wird <br>  Servicedateien, die für den erfolgreichen Betrieb des auf Basis von erstellten Datafow-Jobs erforderlich sind <br>  Vorlage und Sie müssen sie nicht löschen. </p><br><p>  Als Nächstes müssen Sie einen Datenflussjob mithilfe dieser Vorlage manuell oder nach Belieben erstellen <br>  auf andere Weise (z. B. CI). </p><br><h5 id="4-vyvody">  4. Schlussfolgerungen </h5><br><p>  So ist es uns gelungen, den Stream von PubSub nach BigQuery mit zu streamen <br>  notwendige Datentransformationen zum Zwecke der weiteren Speicherung, Transformation und <br>  Datennutzung. </p><br><h2 id="osnovnye-ssylki">  Hauptlinks </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Beam SDK</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwjHvcGT5svgAhV7wsQBHSDWDEoQFjAAegQIABAC%26url%3D">Google Datenfluss</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Bigquery</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">James Moore Artikel über Medium</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Codebeispiele für Apache Beam</a> </li></ul><br><p>  In diesem Artikel sind Ungenauigkeiten und sogar Fehler möglich, ich werde für das Konstruktive dankbar sein <br>  Kritik.  Am Ende möchte ich hinzufügen, dass hier tatsächlich nicht alle verwendet werden <br>  Funktionen des Apache Beam SDK, aber das war nicht das Ziel. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441892/">https://habr.com/ru/post/de441892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441878/index.html">Du wie du willst, aber ich habe es getan</a></li>
<li><a href="../de441882/index.html">VMware NSX für die Kleinsten. Teil 3. DHCP konfigurieren</a></li>
<li><a href="../de441886/index.html">In den letzten 12 Jahren habe ich nie einen Lebenslauf gezeigt</a></li>
<li><a href="../de441888/index.html">SIP von Megaphone zu Hause Rate</a></li>
<li><a href="../de441890/index.html">Alles, was Sie über iOS App Extensions wissen müssen</a></li>
<li><a href="../de441896/index.html">Lernen Sie kontroverse Taktiken, Techniken und allgemeines Wissen (ATT @ CK). Unternehmenstaktik. Teil 9</a></li>
<li><a href="../de441898/index.html">Sketch + Node.js: Generieren von Symbolen für viele Plattformen und Marken</a></li>
<li><a href="../de441900/index.html">Satya Nadella sprach über die Zusammenarbeit mit dem Pentagon</a></li>
<li><a href="../de441902/index.html">Wie Technologie neue Realitäten schafft</a></li>
<li><a href="../de441904/index.html">Installieren eines IPS-Displays auf dem Thinkpad T430S</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>