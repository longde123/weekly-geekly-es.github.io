<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üëÇüèΩ üé£ üö£üèæ Erstellen einer Datenflussvorlage zum Streamen von Daten von Pub / Sub zu BigQuery basierend auf GCP mit dem Apache Beam SDK und Python ü§õüèª üëÜüèΩ üëèüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Moment besch√§ftige ich mich mit dem Streaming (und Konvertieren) von Daten. In einigen Kreisen 
 ein solcher Prozess ist als ETL bekannt , d.h. Ext...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Erstellen einer Datenflussvorlage zum Streamen von Daten von Pub / Sub zu BigQuery basierend auf GCP mit dem Apache Beam SDK und Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/441892/"><p><img src="https://habrastorage.org/webt/is/qf/7j/isqf7j7patfl6lgirlqfrfbz-k8.png" alt="Bild"></p><br><p>  Im Moment besch√§ftige ich mich mit dem Streaming (und Konvertieren) von Daten.  In einigen Kreisen <br>  ein solcher Prozess ist als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ETL bekannt</a> , d.h.  Extrahieren, Konvertieren und Laden von Informationen. </p><br><p>  Der gesamte Prozess umfasst die Teilnahme der folgenden <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Cloud Platform-</a> Dienste: </p><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pub / Sub-</a> Service f√ºr Echtzeit-Daten-Streaming </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenfluss</a> - ein Dienst zum Konvertieren von Daten (can <br>  sowohl in Echtzeit als auch im Batch-Modus arbeiten) </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">BigQuery</a> - ein Dienst zum Speichern von Daten in Form von Tabellen <br>  (unterst√ºtzt SQL) </li></ul><a name="habracut"></a><br><h5 id="0-tekuschee-polozhenie-del">  0. Aktueller Status </h5><br><p>  Im Moment gibt es eine funktionierende Version des Streamings f√ºr die oben genannten Dienste, jedoch in <br>  Als Vorlage wird eine der <a href="">Standardvorlagen verwendet</a> . </p><br><p> Das Problem besteht darin, dass diese Vorlage eine 1: 1-Daten√ºbertragung bereitstellt, d. H.  auf <br>  Am Eingang zu Pub / Sub haben wir eine Zeichenfolge im JSON-Format. Am Ausgang haben wir eine BigQuery-Tabelle mit Feldern. <br>  die den Schl√ºsseln der Objekte auf der obersten Ebene des Eingabe-JSON entsprechen. </p><br><h5 id="1-postanovka-zadachi">  1. Erkl√§rung des Problems </h5><br><p> Erstellen Sie eine Datenflussvorlage, mit der Sie eine oder mehrere Tabellen an der Ausgabe abrufen k√∂nnen <br>  gem√§√ü den gegebenen Bedingungen.  Zum Beispiel m√∂chten wir f√ºr jede eine eigene Tabelle erstellen <br>  Werte eines bestimmten Eingabe-JSON-Schl√ºssels.  Es ist notwendig, die Tatsache zu ber√ºcksichtigen, dass einige <br>  Eingabe-JSON-Objekte k√∂nnen verschachtelten JSON als Wert enthalten, d. H.  ist notwendig <br>  Sie k√∂nnen BigQuery-Tabellen mit Feldern vom Typ <code>RECORD</code> zum Speichern <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verschachtelter</a> Tabellen erstellen <br>  Daten. </p><br><h5 id="2-podgotovka-k-resheniyu">  2. Vorbereitung auf die Entscheidung </h5><br><p>  Verwenden Sie zum Erstellen einer Datenflussvorlage das <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Beam</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">SDK</a> , das wiederum <br>  unterst√ºtzt Java und Python als Programmiersprache.  Das muss ich sagen <br>  Es wird nur die Version Python 2.7.x unterst√ºtzt, was mich etwas √ºberrascht hat.  Dar√ºber hinaus Unterst√ºtzung <br>  Java ist etwas breiter, weil  F√ºr Python sind beispielsweise einige Funktionen nicht verf√ºgbar und mehr <br>  Eine bescheidene Liste integrierter <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Anschl√ºsse</a> .  √úbrigens k√∂nnen Sie Ihre eigenen Konnektoren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">schreiben</a> . </p><br><p>  Aufgrund der Tatsache, dass ich mit Java nicht vertraut bin, habe ich Python verwendet. </p><br><p>  Bevor Sie mit dem Erstellen einer Vorlage beginnen, m√ºssen Sie √ºber Folgendes verf√ºgen: </p><br><ol><li>  Geben Sie das JSON-Format ein und es sollte sich nicht rechtzeitig √§ndern </li><li>  Schema oder Schemata von BigQuery-Tabellen, in die Daten gestreamt werden </li><li>  Die Anzahl der Tabellen, in die der Ausgabedatenstrom gestreamt wird </li></ol><br><p>  Beachten Sie, dass diese Parameter nach dem Erstellen einer Vorlage und dem Starten des darauf basierenden Datenflussjobs verwendet werden k√∂nnen <br>  √Ñndern Sie dies nur, indem Sie eine neue Vorlage erstellen. </p><br><p>  Lassen Sie uns ein paar Worte zu diesen Einschr√§nkungen sagen.  Sie alle kommen von der Tatsache, dass es keine M√∂glichkeit gibt <br>  Erstellen Sie eine dynamische Vorlage, die eine beliebige Zeichenfolge als Eingabe verwenden kann, und analysieren Sie sie <br>  nach interner Logik und f√ºllen Sie dann dynamisch erstellte Tabellen dynamisch mit <br>  von der Schaltung erstellt.  Es ist sehr wahrscheinlich, dass diese M√∂glichkeit besteht, jedoch innerhalb der Daten <br>  Es ist mir nicht gelungen, ein solches System umzusetzen.  Soweit ich das Ganze verstehe <br>  Die Pipeline wird erstellt, bevor sie zur Laufzeit ausgef√ºhrt wird. Daher gibt es keine M√∂glichkeit, sie zu √§ndern <br>  fliegen.  Vielleicht teilt jemand seine Entscheidung. </p><br><h5 id="3-reshenie">  3. Entscheidung </h5><br><p>  F√ºr ein vollst√§ndigeres Verst√§ndnis des Prozesses lohnt es sich, ein Diagramm der sogenannten <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Pipeline zu erstellen</a> <br>  aus der Apache Beam-Dokumentation. </p><br><p><img src="https://habrastorage.org/webt/yq/yi/3z/yqyi3zjiwpmjf4i6qp7x4znqv2c.png" alt="Bild"></p><br><p>  In unserem Fall (wir werden die Unterteilung in mehrere Tabellen verwenden): </p><br><ul><li>  Eingabedaten stammen von PubSub im Datenflussjob </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Transformation</a> Nr. 1 - Daten werden von einer Zeichenfolge in ein Python-W√∂rterbuch konvertiert. Wir erhalten eine Ausgabe <br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">PCollection</a> # 1 </li><li>  Transformation Nr. 2 - Die Daten werden zur weiteren Trennung gem√§√ü separaten Tabellen in markiert <br>  Die Ausgabe ist PCollection # 2 (eigentlich ein PCollection-Tupel). </li><li>  Transformation Nr. 3 - Daten aus PCollection Nr. 2 werden mithilfe von Schemata in Tabellen geschrieben <br>  Tabellen </li></ul><br><p>  Beim Schreiben meiner eigenen Vorlage habe ich mich aktiv von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen</a> Beispielen inspirieren lassen. </p><br><div class="spoiler">  <b class="spoiler_title">Vorlagencode mit Kommentaren (linke Kommentare auf die gleiche Weise wie bei fr√ºheren Autoren):</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding=utf-8 from __future__ import absolute_import import logging import json import os import apache_beam as beam from apache_beam.pvalue import TaggedOutput from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions from apache_beam.options.pipeline_options import StandardOptions from apache_beam.io.gcp.bigquery import parse_table_schema_from_json #  GCP  gcp_project = '' #  Pub/Sub  topic_name = '' # Pub/Sub    'projects/_GCP_/topics/_' input_topic = 'projects/%s/topics/%s' % (gcp_project, topic_name) #  BigQuery  bq_dataset = 'segment_eu_test' #       schema_dir = './' class TransformToBigQuery(beam.DoFn): #          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #   yield body class TagDataWithReqType(beam.DoFn): #      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element def run(): #       _.json   schema_dir,  #         ()  schema_dct = {} for schema_file in os.listdir(schema_dir): filename_list = schema_file.split('.') if filename_list[-1] == 'json': with open('%s/%s' % (schema_dir, schema_file)) as f: schema_json = f.read() schema_dct[filename_list[0]] = json.dumps({'fields': json.loads(schema_json)}) # We use the save_main_session option because one or more DoFn's in this # workflow rely on global context (eg, a module imported at module level). pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = True pipeline_options.view_as(StandardOptions).streaming = True # Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic) # Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery()) # Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()). with_outputs(*schema_dct.keys(), main='default') # Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), ) # Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), ) result = p.run() result.wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) logger = logging.getLogger(__name__) run()</span></span></code> </pre> </div></div><br><p>  Jetzt werden wir den Code durchgehen und Erkl√§rungen geben, aber zuerst ist es erw√§hnenswert, dass der Hauptcode <br>  Die Schwierigkeit beim Schreiben dieser Vorlage besteht darin, in Bezug auf den "Datenstrom" zu denken, und <br>  keine bestimmte Nachricht.  Es ist auch wichtig zu verstehen, dass Pub / Sub mit Nachrichten und arbeitet <br>  Von ihnen erhalten wir Informationen zum Markieren des Streams. </p><br><pre> <code class="python hljs">pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> pipeline_options.view_as(StandardOptions).streaming = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span></code> </pre> <br><p>  Weil  Der Apache Beam Pub / Sub IO-Anschluss wird nur im erforderlichen Streaming-Modus verwendet <br>  F√ºgen Sie PipelineOptions () hinzu (obwohl die Optionen tats√§chlich nicht verwendet werden). Andernfalls erstellen Sie eine Vorlage <br>  f√§llt mit der Ausnahme.  Es muss √ºber die Optionen zum Starten der Vorlage gesagt werden.  Sie k√∂nnen sein <br>  statische und sogenannte "Laufzeit".  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier ist ein</a> Link zur Dokumentation zu diesem Thema.  Mit den Optionen k√∂nnen Sie eine Vorlage erstellen, ohne zuvor Parameter anzugeben, diese jedoch beim Starten des Datenflussjobs √ºber die Vorlage √ºbergeben. Ich konnte sie jedoch immer noch nicht implementieren, wahrscheinlich weil dieser Connector <code>RuntimeValueProvider</code> nicht unterst√ºtzt. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic)</span></span></code> </pre> <br><p>  Alles ist aus dem Kommentar klar, wir lesen den Thread aus dem Thema.  Es lohnt sich hinzuzuf√ºgen, dass Sie den Stream nehmen k√∂nnen <br>  sowohl aus dem Thema als auch aus dem Abonnement (Abonnement).  Wenn das Thema als Eingabe angegeben ist, dann <br>  Ein tempor√§res Abonnement f√ºr dieses Thema wird automatisch erstellt.  Die Syntax ist auch h√ºbsch <br>  klar, Eingabedatenstrom <code>beam.io.ReadFromPubSub(input_topic)</code> an unsere gesendet <br>  Pipeline <code>p</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery())</span></span></code> </pre> <br><p>  Hier findet die Transformation Nr. 1 statt und unsere Eingabe wird von einer Python-Zeichenfolge in konvertiert <br>  Python-Diktat, und in der Ausgabe erhalten wir PCollection # 1.  <code>&gt;&gt;</code> erscheint in der Syntax.  Ein <br>  Tats√§chlich ist der Text in Anf√ºhrungszeichen der Name des Streams (muss eindeutig sein) sowie ein Kommentar. <br>  Dies wird dem Block im Diagramm in der GCP Dataflow-Weboberfl√§che hinzugef√ºgt.  Lassen Sie uns genauer betrachten <br>  √ºberschriebene Klasse <code>TransformToBigQuery</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TransformToBigQuery</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #  ,      python dict yield body</span></span></code> </pre> <br><p>  Die Elementvariable enth√§lt eine Nachricht aus dem PubSub-Abonnement.  Wie von gesehen <br>  Code, in unserem Fall sollte es g√ºltiger JSON sein.  Im Klassenzimmer muss sein <br>  Die <code>process</code> wird neu definiert, in der die erforderlichen Transformationen durchgef√ºhrt werden sollen <br>  Eingangsleitung, um einen Ausgang zu erhalten, der der Schaltung entspricht <br>  die Tabelle, in die diese Daten geladen werden.  Weil  Unser Fluss in diesem Fall ist <br>  kontinuierlich, <code>unbounded</code> in Bezug auf Apache Beam, m√ºssen Sie es mit zur√ºckgeben <br>  <code>yield</code> , nicht <code>return</code> , wie beim endg√ºltigen Datenstrom.  Im Falle eines endg√ºltigen Ablaufs k√∂nnen Sie <br>  (und notwendig) zus√§tzlich <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>windowing</code></a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><code>triggers</code></a> konfigurieren </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()).with_outputs(*schema_dct.keys(), main='default')</span></span></code> </pre> <br><p>  Dieser Code leitet PCollection # 1 zu Transform # 2, wo das Tagging stattfinden wird <br>  (Trennung) des Datenstroms.  In der Variablen <code>schema_dct</code> in diesem Fall einem W√∂rterbuch, in dem der Schl√ºssel der Name der <code>schema_dct</code> ohne die Erweiterung ist, ist dies das Tag und der Wert der g√ºltige JSON des Schemas <br>  BigQuery-Tabellen f√ºr dieses Tag.  Es ist zu beachten, dass das Schema genau an √ºbertragen werden sollte <br>  view <code>{'fields': }</code> wobei <code></code> das Schema der BigQuery-Tabelle in JSON-Form ist (Sie k√∂nnen <br>  Export von der Weboberfl√§che). </p><br><p>  <code>main='default'</code> ist der Name des Thread-Tags, zu dem sie gehen <br>  Alle Nachrichten, f√ºr die keine Tagging-Bedingungen gelten.  Betrachten Sie die Klasse <br>  <code>TagDataWithReqType</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TagDataWithReqType</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element</span></span></code> </pre> <br><p>  Wie Sie sehen, wird auch hier die <code>process</code> √ºberschrieben.  Die Variable <code>types</code> enth√§lt Namen <br>  Tags und sie m√ºssen die Nummer und den Namen mit der Nummer und den Namen der W√∂rterbuchschl√ºssel √ºbereinstimmen <br>  <code>schema_dct</code> .  Obwohl die <code>process</code> Argumente akzeptieren kann, habe ich nie <br>  Ich konnte sie bestehen.  Ich habe den Grund noch nicht herausgefunden. </p><br><p>  Bei der Ausgabe erhalten wir ein Tupel von Threads in der Anzahl der Tags, n√§mlich der Anzahl unserer <br>  vordefinierte Tags + Standard-Thread, der nicht markiert werden konnte. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), )</span></span></code> </pre> <br><p>  Transform # ... (tats√§chlich ist es nicht im Diagramm, dies ist ein "Zweig") - wir schreiben den Standard-Stream <br>  zur Standardtabelle. </p><br><p>  <code>tagged_stream.default</code> - Ein Stream mit dem <code>default</code> Tag wird verwendet. Die alternative Syntax lautet <code>tagged_stream['default']</code> </p><br><p>  <code>schema=parse_table_schema_from_json(schema_dct.get('default'))</code> - hier wird das Schema definiert <br>  Tabellen.  Beachten Sie, dass die Datei default.json das g√ºltige BigQuery-Tabellenschema enth√§lt <br>  muss sich im aktuellen <code>schema_dir = './'</code> . </p><br><p>  Der Stream wird in eine Tabelle namens <code>default</code> . </p><br><p>  Wenn eine Tabelle mit diesem Namen (im angegebenen Datensatz dieses Projekts) nicht vorhanden ist, ist sie vorhanden <br>  wird dank der Standardeinstellung automatisch aus dem Schema erstellt <br> <code>create_disposition=BigQueryDisposition.CREATE_IF_NEEDED</code> </p> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), )</span></span></code> </pre> <br><p>  Transform # 3, alles sollte f√ºr diejenigen klar sein, die den Artikel von Anfang an lesen und besitzen <br>  Python-Syntax.  Wir trennen das Stream-Tupel durch eine Schleife und schreiben jeden Stream in eine eigene Tabelle mit <br>  sein Schema.  Es sei daran erinnert, dass der <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> eindeutig sein muss - <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> . </p><br><p>  Jetzt sollte klar sein, wie das funktioniert und Sie k√∂nnen eine Vorlage erstellen.  Daf√ºr brauchst du <br>  F√ºhren Sie es in der Konsole aus (vergessen Sie nicht, venv zu aktivieren, falls verf√ºgbar) oder in der IDE: </p><br><pre> <code class="bash hljs">python _.py / --runner DataflowRunner / --project dreamdata-test / --staging_location gs://STORAGE_NAME/STAGING_DIR / --temp_location gs://STORAGE_NAME/TEMP_DIR / --template_location gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> </pre> <br><p>  Gleichzeitig sollte der Zugriff auf das Google-Konto beispielsweise durch Export organisiert werden <br>  die Umgebungsvariable <code>GOOGLE_APPLICATION_CREDENTIALS</code> oder eine andere <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Methode</a> . </p><br><p>  Ein paar Worte zu <code>--runner</code> .  In diesem Fall sagt <code>DataflowRunner</code> , dass dieser Code <br>  wird als Vorlage f√ºr den Datenflussjob ausgef√ºhrt.  Es ist weiterhin m√∂glich anzugeben <br>  <code>DirectRunner</code> wird standardm√§√üig verwendet, wenn keine Option vorhanden ist - <code>--runner</code> und Code <br>  funktioniert als Datenflussjob, jedoch lokal, was f√ºr das Debuggen sehr praktisch ist. </p><br><p>  Wenn keine Fehler aufgetreten sind, <code>gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> <br>  Vorlage erstellt.  Es ist erw√§hnenswert, dass in <code>gs://STORAGE_NAME/STAGING_DIR</code> auch geschrieben wird <br>  Servicedateien, die f√ºr den erfolgreichen Betrieb des auf Basis von erstellten Datafow-Jobs erforderlich sind <br>  Vorlage und Sie m√ºssen sie nicht l√∂schen. </p><br><p>  Als N√§chstes m√ºssen Sie einen Datenflussjob mithilfe dieser Vorlage manuell oder nach Belieben erstellen <br>  auf andere Weise (z. B. CI). </p><br><h5 id="4-vyvody">  4. Schlussfolgerungen </h5><br><p>  So ist es uns gelungen, den Stream von PubSub nach BigQuery mit zu streamen <br>  notwendige Datentransformationen zum Zwecke der weiteren Speicherung, Transformation und <br>  Datennutzung. </p><br><h2 id="osnovnye-ssylki">  Hauptlinks </h2><br><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Apache Beam SDK</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=https://www.google.com/url%3Fsa%3Dt%26rct%3Dj%26q%3D%26esrc%3Ds%26source%3Dweb%26cd%3D1%26cad%3Drja%26uact%3D8%26ved%3D2ahUKEwjHvcGT5svgAhV7wsQBHSDWDEoQFjAAegQIABAC%26url%3D">Google Datenfluss</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Bigquery</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">James Moore Artikel √ºber Medium</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Python-Codebeispiele f√ºr Apache Beam</a> </li></ul><br><p>  In diesem Artikel sind Ungenauigkeiten und sogar Fehler m√∂glich, ich werde f√ºr das Konstruktive dankbar sein <br>  Kritik.  Am Ende m√∂chte ich hinzuf√ºgen, dass hier tats√§chlich nicht alle verwendet werden <br>  Funktionen des Apache Beam SDK, aber das war nicht das Ziel. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de441892/">https://habr.com/ru/post/de441892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de441878/index.html">Du wie du willst, aber ich habe es getan</a></li>
<li><a href="../de441882/index.html">VMware NSX f√ºr die Kleinsten. Teil 3. DHCP konfigurieren</a></li>
<li><a href="../de441886/index.html">In den letzten 12 Jahren habe ich nie einen Lebenslauf gezeigt</a></li>
<li><a href="../de441888/index.html">SIP von Megaphone zu Hause Rate</a></li>
<li><a href="../de441890/index.html">Alles, was Sie √ºber iOS App Extensions wissen m√ºssen</a></li>
<li><a href="../de441896/index.html">Lernen Sie kontroverse Taktiken, Techniken und allgemeines Wissen (ATT @ CK). Unternehmenstaktik. Teil 9</a></li>
<li><a href="../de441898/index.html">Sketch + Node.js: Generieren von Symbolen f√ºr viele Plattformen und Marken</a></li>
<li><a href="../de441900/index.html">Satya Nadella sprach √ºber die Zusammenarbeit mit dem Pentagon</a></li>
<li><a href="../de441902/index.html">Wie Technologie neue Realit√§ten schafft</a></li>
<li><a href="../de441904/index.html">Installieren eines IPS-Displays auf dem Thinkpad T430S</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>