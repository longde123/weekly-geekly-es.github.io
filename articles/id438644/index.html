<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍄 🧑🏼‍🤝‍🧑🏻 🔩 Keamanan algoritma pembelajaran mesin. Melindungi dan Menguji Model Menggunakan Python 🍆 🤶🏿 🏸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Pada artikel sebelumnya, kita berbicara tentang masalah pembelajaran mesin seperti contoh permusuhan dan beberapa jenis serangan yang memungkinkan mer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Keamanan algoritma pembelajaran mesin. Melindungi dan Menguji Model Menggunakan Python</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/dsec/blog/438644/"><p><img src="https://habrastorage.org/webt/wo/o_/u2/woo_u2i8ll_fqrqvt3o-typrlue.jpeg" alt="gambar"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pada artikel sebelumnya,</a> kita berbicara tentang masalah pembelajaran mesin seperti contoh permusuhan dan beberapa jenis serangan yang memungkinkan mereka dihasilkan.  Artikel ini akan fokus pada algoritma perlindungan dari efek semacam ini dan rekomendasi untuk model pengujian. </p><a name="habracut"></a><br><h2 id="zaschita">  Perlindungan </h2><br><p>  Pertama-tama, mari kita jelaskan satu hal - tidak mungkin untuk sepenuhnya mempertahankan diri dari efek seperti itu, dan ini sangat wajar.  Memang, jika kita menyelesaikan masalah contoh permusuhan sepenuhnya, maka kita akan secara bersamaan memecahkan masalah membangun hyperplane yang ideal, yang, tentu saja, tidak dapat dilakukan tanpa kumpulan data umum. </p><br><p>  Ada dua tahap untuk mempertahankan model pembelajaran mesin: </p><br><p>  <strong>Belajar</strong> - Kami mengajarkan algoritme kami untuk merespons dengan benar contoh-contoh permusuhan </p><br><p>  <strong>Operasi</strong> - kami mencoba mendeteksi contoh permusuhan selama fase operasi model. </p><br><p>  Layak <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">disebutkan</a> bahwa Anda dapat bekerja dengan metode perlindungan yang disajikan dalam artikel ini menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Adversarial Robustness Toolbox</a> dari IBM. </p><br><h3 id="adversarial-training">  Pelatihan permusuhan </h3><br><p><img src="https://habrastorage.org/webt/4w/t_/lm/4wt_lmm-cbcdye9rabryki0jj70.png" alt="gambar"></p><br><p> Jika Anda bertanya kepada seseorang yang baru saja mengenal masalah permusuhan dengan contoh-contoh, pertanyaan: "Bagaimana melindungi diri Anda dari efek ini?"  Pendekatan ini segera diusulkan dalam artikel yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sifatnya menarik dari jaringan saraf</a> kembali pada tahun 2013.  Di artikel inilah masalah ini pertama kali dijelaskan dan serangan L-BFGS, yang memungkinkan menerima contoh-contoh permusuhan. </p><br><p>  Metode ini sangat sederhana.  Kami membuat contoh Adversarial menggunakan berbagai jenis serangan dan menambahkannya ke pelatihan yang ditetapkan pada setiap iterasi, sehingga meningkatkan "resistensi" model Adversarial terhadap contoh-contoh. </p><br><p>  Kerugian dari metode ini cukup jelas: pada setiap iterasi pelatihan, untuk masing-masing contoh, kita dapat menghasilkan sejumlah besar contoh, masing-masing, dan waktu untuk memodelkan pelatihan meningkat berkali-kali. </p><br><p>  Anda dapat menerapkan metode ini menggunakan perpustakaan ART-IBM sebagai berikut. </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.adversarial_trainer <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> AdversarialTrainer trainer = AdversarialTrainer(model, attacks) trainer.fit(x_train, y_train)</code> </pre> <br><h3 id="gaussian-data-augmentation">  Augmentasi Data Gaussian </h3><br><p><img src="https://habrastorage.org/webt/jf/9d/ko/jf9dkoia9fom1rtqkvgwe-7aulo.png" alt="gambar"></p><br><p>  Metode berikut, yang dijelaskan dalam artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Efisien Pertahanan Terhadap Serangan Adversarial</a> , menggunakan logika yang serupa: ia juga menyarankan menambahkan objek tambahan ke set pelatihan, tetapi tidak seperti Pelatihan Adversarial, objek ini bukan contoh Adversarial, tetapi objek set latihan yang sedikit bising (Gaussian digunakan sebagai noise kebisingan, maka nama metode ini).  Dan, memang, ini tampak sangat logis, karena masalah utama dari model justru kekebalannya yang buruk. </p><br><p>  Metode ini menunjukkan hasil yang mirip dengan Pelatihan Adversarial, sambil menghabiskan lebih sedikit waktu untuk menghasilkan objek untuk pelatihan. </p><br><p>  Anda dapat menerapkan metode ini menggunakan kelas GaussianAugmentation di ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.gaussian_augmentation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GaussianAugmentation GDA = GaussianAugmentation() new_x = GDA(x_train)</code> </pre> <br><h3 id="label-smoothing">  Label smoothing </h3><br><p>  Metode Label Smoothing sangat sederhana untuk diimplementasikan, namun demikian membawa banyak arti probabilistik.  Kami tidak akan membahas perincian interpretasi probabilistik dari metode ini, Anda dapat menemukannya di artikel asli <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Memikirkan Kembali Arsitektur Inception for Computer Vision</a> .  Namun, untuk membuatnya lebih singkat, Label Smoothing adalah jenis tambahan regularisasi model dalam masalah klasifikasi, yang membuatnya lebih tahan terhadap kebisingan. </p><br><p>  Bahkan, metode ini menghaluskan label kelas.  Membuatnya, katakanlah, bukan 1, tetapi 0,9.  Dengan demikian, model pelatihan didenda karena "kepercayaan" yang jauh lebih besar pada label untuk objek tertentu. </p><br><p>  Aplikasi metode ini dalam Python dapat dilihat di bawah ini. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.label_smoothing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabelSmoothing LS = LabelSmoothing() new_x, new_y = LS(train_x, train_y)</code> </pre> <br><h3 id="bounded-relu">  Relu terikat </h3><br><p><img src="https://habrastorage.org/webt/dw/mw/sz/dwmwszowk1t9l6byacxcscvmvh4.png" alt="gambar"></p><br><p>  Ketika kita berbicara tentang serangan, banyak yang dapat memperhatikan bahwa beberapa serangan (JSMA, OnePixel) bergantung pada seberapa kuat gradien pada satu titik atau lainnya pada gambar input.  Metode sederhana dan "murah" (dalam hal biaya komputasi dan waktu) dari Bounded ReLU sedang mencoba untuk mengatasi masalah ini. </p><br><p>  Inti dari metode ini adalah sebagai berikut.  Mari kita ganti fungsi aktivasi ReLU di jaringan saraf dengan yang sama, yang dibatasi tidak hanya dari bawah, tetapi juga dari atas, sehingga memuluskan peta gradien, dan pada titik-titik tertentu tidak akan mungkin untuk mendapatkan percikan, yang tidak akan memungkinkan Anda membodohi algoritma dengan mengubah satu piksel gambar. </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"></script></p><br>  \ begin {persamaan *} f (x) = <br>  \ begin {cases} <br>  0, x &lt;0 <br>  \\ <br>  x, 0 \ leq x \ leq t <br>  \\ <br>  t, x&gt; t <br>  \ end {cases} <br>  \ end {persamaan *} <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot; />" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0" height="0.25ex" viewBox="0 -53.9 0 107.7" role="img" focusable="false" style="vertical-align: -0.125ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"></script></p><br><p>  Metode ini juga telah dijelaskan dalam artikel <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pertahanan Efisien Terhadap Serangan Musuh</a> </p><br><h3 id="postroenie-ansambley-modeley">  Ensembles Model Bangunan </h3><br><p><img src="https://habrastorage.org/webt/cq/2i/pg/cq2ipgeru_vavdrnk-usoydtlxw.png" alt="gambar"><br>  Tidak sulit untuk menipu satu model yang terlatih.  Untuk menipu dua model sekaligus dengan satu objek bahkan lebih sulit.  Dan jika ada N model seperti itu?  Di sinilah metode ensemble model didasarkan.  Kami hanya membangun N model yang berbeda dan menggabungkan hasilnya menjadi satu jawaban.  Jika model juga diwakili oleh algoritma yang berbeda, maka sangat sulit untuk menipu sistem seperti itu, tetapi sangat sulit! </p><br><p>  Sangat wajar bahwa implementasi ansambel model adalah pendekatan arsitektur murni, mengajukan banyak pertanyaan (Model dasar apa yang harus diambil? Bagaimana cara mengagregasi output model dasar? Apakah ada hubungan antara model? Dan seterusnya).  Karena alasan ini, pendekatan ini tidak diterapkan dalam ART-IBM </p><br><h3 id="feature-squeezing">  Meremas fitur </h3><br><p><img src="https://habrastorage.org/webt/sn/wy/wp/snwywpuqqae7pun4njrlmluseeg.png" alt="gambar"><br>  Metode ini, dijelaskan dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Pemerasan Fitur: Mendeteksi Contoh Adversarial di Deep Neural Networks</a> , bekerja selama fase operasional model.  Ini memungkinkan Anda untuk mendeteksi contoh permusuhan. </p><br><p>  Gagasan di balik metode ini adalah sebagai berikut: jika Anda melatih model pada data yang sama, tetapi dengan rasio kompresi yang berbeda, hasil pekerjaan mereka akan tetap serupa.  Pada saat yang sama, contoh Adversarial, yang berfungsi pada jaringan sumber, kemungkinan besar akan gagal pada jaringan tambahan.  Dengan demikian, setelah mempertimbangkan perbedaan berpasangan antara output dari jaringan saraf awal dan yang tambahan, memilih maksimum dari mereka dan membandingkannya dengan ambang yang dipilih sebelumnya, kita dapat menyatakan bahwa objek input adalah Adversarial atau benar-benar valid. </p><br><p>  Berikut ini adalah metode untuk mendapatkan objek yang dikompresi menggunakan ART-IBM </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> art.defences.feature_squeezing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FeatureSqueezing FS = FeatureSqueezing() new_x = FS(train_x)</code> </pre> <br><p>  Kami akan mengakhiri dengan metode perlindungan.  Tetapi akan salah untuk tidak memahami satu poin penting.  Jika penyerang tidak memiliki akses ke input dan output model, ia tidak akan mengerti bagaimana data mentah diproses di dalam sistem Anda sebelum memasukkan model.  Kemudian dan hanya kemudian semua serangannya akan dikurangi menjadi secara acak menyortir nilai input, yang secara alami tidak mungkin mengarah pada hasil yang diinginkan. </p><br><h2 id="testirovanie">  Pengujian </h2><br><p>  Sekarang mari kita bicara tentang algoritma pengujian untuk melawan contoh permusuhan.  Di sini, pertama-tama, perlu dipahami bagaimana kita akan menguji model kita.  Jika kami berasumsi bahwa penyerang dapat memperoleh akses penuh ke seluruh model, maka perlu untuk menguji model kami menggunakan metode serangan WhiteBox. <br><img src="https://habrastorage.org/webt/vj/pm/-w/vjpm-wuwle8c5sngov5ksw5iahq.png" alt="gambar"></p><br><p>  Dalam kasus lain, kami berasumsi bahwa penyerang tidak akan pernah mendapatkan akses ke "bagian dalam" model kami, namun, ia akan dapat, meskipun secara tidak langsung, untuk mempengaruhi data input dan melihat hasil dari model.  Maka Anda harus menerapkan metode serangan BlackBox. <br><img src="https://habrastorage.org/webt/xc/h9/wo/xch9wo0pweqhlf33pzgrgdiihqm.png" alt="gambar"></p><br><p>  Algoritma pengujian umum dapat dijelaskan dengan contoh berikut: </p><br><p><img src="https://habrastorage.org/webt/1d/p_/lx/1dp_lxdocm0zkd2ssmbg_fmnvba.jpeg" alt="gambar"></p><br><p>  Biarkan ada jaringan saraf terlatih yang ditulis dalam TensorFlow (TF NN).  Kami dengan ahli mengklaim bahwa jaringan kami dapat jatuh ke tangan penyerang dengan menembus sistem di mana model berada.  Dalam hal ini, kita perlu melakukan serangan WhiteBox.  Untuk melakukan ini, kami mendefinisikan kumpulan serangan dan kerangka kerja (FoolBox - FB, CleverHans - CH, Adboxer robustness toolbox - ART) yang memungkinkan serangan ini diimplementasikan.  Kemudian, dengan menghitung berapa banyak serangan yang berhasil, kami menghitung Tingkat Succes (SR).  Jika SR cocok untuk kita, kita menyelesaikan pengujian, jika tidak kita menggunakan salah satu metode perlindungan, misalnya, diterapkan dalam ART-IBM.  Kemudian kami melakukan serangan dan mempertimbangkan SR.  Kami melakukan operasi ini secara siklis, sampai SR cocok untuk kami. </p><br><h2 id="vyvody">  Kesimpulan </h2><br><p>  Saya ingin mengakhiri di sini dengan informasi umum tentang serangan, pertahanan, dan model pembelajaran mesin pengujian.  Merangkum dua artikel, kita dapat menyimpulkan yang berikut: </p><br><ol><li>  Jangan percaya pembelajaran mesin sebagai semacam keajaiban yang bisa menyelesaikan semua masalah Anda. </li><li>  Saat menerapkan algoritme pembelajaran mesin dalam tugas Anda, pikirkan tentang seberapa tahan algoritma ini terhadap ancaman seperti contoh Adversarial. </li><li>  Anda dapat melindungi algoritma baik dari sisi pembelajaran mesin, dan dari sisi sistem di mana model ini dioperasikan. </li><li>  Uji model Anda, terutama dalam kasus di mana hasil model secara langsung mempengaruhi keputusan </li><li>  Perpustakaan seperti FoolBox, CleverHans, ART-IBM menyediakan antarmuka yang nyaman untuk menyerang dan mempertahankan model pembelajaran mesin. </li></ol><br><p>  Juga dalam artikel ini saya ingin merangkum karya dengan perpustakaan FoolBox, CleverHans dan ART-IBM: </p><br><p>  FoolBox adalah perpustakaan sederhana dan dapat dipahami untuk menyerang jaringan saraf, mendukung banyak kerangka kerja yang berbeda. </p><br><p>  CleverHans adalah pustaka yang memungkinkan Anda untuk melakukan serangan dengan mengubah banyak parameter serangan, sedikit lebih rumit dari FoolBox, mendukung lebih sedikit kerangka kerja. </p><br><p>  ART-IBM adalah satu-satunya perpustakaan di atas yang memungkinkan Anda untuk bekerja dengan metode keamanan, sejauh ini hanya mendukung TensorFlow dan Keras, tetapi sedang berkembang lebih cepat daripada yang lain. </p><br><p>  Di sini patut dikatakan bahwa ada <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">perpustakaan</a> lain untuk bekerja dengan contoh-contoh permusuhan dari Baidu, tetapi, sayangnya, hanya cocok untuk orang-orang yang berbicara bahasa Cina. </p><br><p>  Pada artikel selanjutnya tentang topik ini, kami akan menganalisis bagian dari tugas yang diusulkan untuk diselesaikan selama ZeroNights HackQuest 2018 dengan menipu jaringan saraf tipikal menggunakan perpustakaan FoolBox. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id438644/">https://habr.com/ru/post/id438644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id438634/index.html">Di kantor, terlalu panas atau terlalu dingin: apakah ada cara yang lebih baik untuk menyesuaikan suhu?</a></li>
<li><a href="../id438636/index.html">Menanam fungsi yang cacat di Go</a></li>
<li><a href="../id438638/index.html">Kami menganalisis protokol pesan pager POCSAG, bagian 2</a></li>
<li><a href="../id438640/index.html">Mata uang elektronik terbuka berkecepatan tinggi</a></li>
<li><a href="../id438642/index.html">Dasar-dasar pemrograman reaktif menggunakan RxJS</a></li>
<li><a href="../id438646/index.html">Tentang membuat gambar stereo anggaran dengan jari (stereogram, anaglyph, stereoscope)</a></li>
<li><a href="../id438648/index.html">Perbandingan sistem BI (Tableau, Power BI, Oracle, Qlik)</a></li>
<li><a href="../id438650/index.html">Roket 9M729. Beberapa kata tentang "pelanggar" Perjanjian INF</a></li>
<li><a href="../id438652/index.html">Portabelisasi IDA</a></li>
<li><a href="../id438654/index.html">OpenSceneGraph: Integrasi dengan Qt Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>