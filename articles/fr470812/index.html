<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤴 🐛 🍓 Cartouche Tarantool: Sharding Lua Backend en trois lignes 👴🏿 🎤 ♉️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans Mail.ru Group, nous avons Tarantool, un serveur d'applications basé sur Lua et une base de données unie. C'est rapide et élégant, mais les ressou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cartouche Tarantool: Sharding Lua Backend en trois lignes</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/470812/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/tg/6f/71/tg6f71dbjoo5m5qm5ebndfjxjdu.jpeg"></div><br>  Dans Mail.ru Group, nous avons Tarantool, un serveur d'applications basé sur Lua et une base de données unie.  C'est rapide et élégant, mais les ressources d'un seul serveur sont toujours limitées.  La mise à l'échelle verticale n'est pas non plus la panacée.  C'est pourquoi Tarantool a quelques outils pour la mise à l'échelle horizontale, ou le module vshard <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">[1]</a> .  Il vous permet de répartir les données sur plusieurs serveurs, mais vous devrez les bricoler pendant un certain temps pour les configurer et renforcer la logique métier. <br><br>  Bonne nouvelle: nous avons obtenu notre part de bosses (par exemple, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">[2]</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">[3]</a> ) et avons créé un autre framework, ce qui simplifie considérablement la solution à ce problème. <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool Cartridge</a> est le nouveau cadre de développement de systèmes distribués complexes.  Il vous permet de vous concentrer sur l'écriture de la logique métier au lieu de résoudre les problèmes d'infrastructure.  Sous la coupe, je vais vous dire comment ce cadre fonctionne et comment il pourrait aider à écrire des services distribués. <br><a name="habracut"></a><br><h2>  Alors quel est exactement le problème? </h2><br>  Nous avons Tarantool et vshard - que voulons-nous de plus? <br><br>  Premièrement, c'est une question de commodité.  Vshard est configuré dans les tables Lua.  Mais pour qu'un système distribué de plusieurs processus Tarantool fonctionne correctement, la configuration doit être la même partout.  Personne ne voudrait le faire manuellement, donc toutes sortes de scripts, d'Ansible et de systèmes de déploiement sont utilisés. <br><br>  La cartouche elle-même gère la configuration vshard sur la base de <i>sa propre configuration distribuée</i> .  En fait, il s'agit d'un simple fichier YAML, et sa copie est stockée sur chaque instance de Tarantool.  En d'autres termes, le framework surveille sa configuration pour qu'il en soit de même partout. <br><br>  Deuxièmement, c'est encore une question de commodité.  La configuration Vshard n'est pas liée au développement de la logique métier et ne distrait qu'un développeur de son travail.  Lorsque nous discutons de l'architecture d'un projet, la question concerne très probablement des composants séparés et leur interaction.  Il est encore trop tôt pour penser à déployer un cluster pour 3 centres de données. <br><br>  Nous avons résolu ces problèmes à maintes reprises, et à un moment donné, nous avons réussi à développer une approche afin de simplifier le travail avec l'application tout au long de son cycle de vie: création, développement, test, CI / CD, maintenance. <br><br>  La cartouche introduit le concept de rôles pour chaque processus Tarantool.  Les rôles permettent au développeur de se concentrer sur l'écriture de code.  Tous les rôles disponibles dans le projet peuvent être exécutés sur la seule instance de Tarantool, ce qui serait suffisant pour les tests. <br><br>  Caractéristiques principales de la cartouche Tarantool: <br><br><ul><li>  orchestration automatisée des clusters; <br></li><li>  fonctionnalité d'application étendue avec de nouveaux rôles; <br></li><li>  modèle d'application pour le développement et le déploiement; <br></li><li>  sharding automatique intégré; <br></li><li>  intégration avec le framework Luatest; <br></li><li>  gestion de cluster à l'aide de WebUI et API; <br></li><li>  outils de packaging et de déploiement. <br></li></ul><br><h2>  Bonjour tout le monde! </h2><br>  J'ai hâte de vous montrer le cadre lui-même, alors sauvegardons l'histoire de l'architecture pour plus tard et commençons par une tâche facile.  En supposant que Tarantool est déjà installé, tout ce que nous avons à faire est <br><br><pre><code class="plaintext hljs">$ tarantoolctl rocks install cartridge-cli $ export PATH=$PWD/.rocks/bin/:$PATH</code> </pre> <br>  En conséquence, les utilitaires de ligne de commande sont installés, ce qui vous permet de créer votre première application à partir du modèle: <br><br><pre> <code class="plaintext hljs">$ cartridge create --name myapp</code> </pre> <br>  Et voici ce que nous obtenons: <br><br><pre> <code class="plaintext hljs">myapp/ ├── .git/ ├── .gitignore ├── app/roles/custom.lua ├── deps.sh ├── init.lua ├── myapp-scm-1.rockspec ├── test │ ├── helper │ │ ├── integration.lua │ │ └── unit.lua │ ├── helper.lua │ ├── integration/api_test.lua │ └── unit/sample_test.lua └── tmp/</code> </pre> <br>  Ceci est un référentiel git avec un "Hello, World!" Prêt à l'emploi  application.  Essayons de l'exécuter après avoir installé les dépendances (y compris le framework lui-même): <br><br><pre> <code class="plaintext hljs">$ tarantoolctl rocks make $ ./init.lua --http-port 8080</code> </pre> <br>  Nous avons lancé un nœud de notre future application fragmentée.  Si vous êtes curieux, vous pouvez immédiatement ouvrir l'interface Web, qui fonctionne sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">localhost</a> : 8080, utiliser une souris pour configurer un cluster à un nœud et profiter du résultat, mais ne vous excitez pas trop tôt.  L'application ne sait pas comment faire quoi que ce soit d'utile pour l'instant, je vais donc vous parler du déploiement plus tard, et maintenant il est temps d'écrire du code. <br><br><h2>  Développer des applications </h2><br>  Imaginez que nous concevions un système qui devrait recevoir des données, les enregistrer et créer un rapport une fois par jour. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc6/dba/8a2/dc6dba8a2a2ff31693cff9a825c44b7f.png"></div><br>  Nous dessinons donc un diagramme avec trois composants: passerelle, stockage et ordonnanceur.  Continuons à travailler sur l'architecture.  Puisque nous utilisons vshard comme stockage, ajoutons vshard-router et vshard-storage au diagramme.  Ni la passerelle ni le planificateur n'accéderont directement au stockage - un routeur est explicitement créé pour cette tâche. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/151/eb0/ac5/151eb0ac5b9d8a1a6217e0992b502089.png"></div><br>  Ce diagramme semble abstrait car les composants ne reflètent toujours pas ce que nous allons créer dans le projet.  Nous devrons voir comment ce projet correspond au vrai Tarantool, nous regroupons donc nos composants par processus. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fdc/781/a46/fdc781a46e819a67232bfcd308605b5d.png"></div><br>  Il n'est pas très judicieux de conserver vshard-router et gateway sur des instances distinctes.  Pourquoi irions-nous à nouveau sur le réseau, si cela est déjà la responsabilité du routeur?  Ils doivent s'exécuter dans le même processus, c'est-à-dire que la passerelle et vshard.router.cfg doivent être initialisés dans le même processus et interagir localement. <br><br>  Pendant la phase de conception, il était pratique de travailler avec trois composants, mais en tant que développeur, je ne veux pas penser à lancer trois instances de Tarantool lors de l'écriture de code.  J'ai besoin d'exécuter les tests et de vérifier que j'ai correctement écrit le code de la passerelle.  Ou je veux peut-être montrer une nouvelle fonctionnalité à mes collègues.  Pourquoi devrais-je prendre des problèmes avec le déploiement de trois instances?  Ainsi, le concept de rôles est né.  Un rôle est un module Lua régulier, et la cartouche gère son cycle de vie.  Dans cet exemple, il y en a quatre: passerelle, routeur, stockage et planificateur.  Un autre projet peut avoir plus de rôles.  Tous les rôles peuvent être lancés en un seul processus, et ce serait suffisant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1e1/49c/95a/1e149c95aed1d5dc7cf73e298198c719.png"></div><br>  Et lorsque le problème concerne le déploiement vers la mise en production ou la production, nous attribuons un ensemble de rôles distinct à chaque processus Tarantool en fonction des capacités matérielles sous-jacentes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1f9/b59/1d7/1f9b591d794e4b459b863f0da01f042b.png"></div><br><h2>  Gestion de la topologie </h2><br>  Nous devons également stocker des informations sur les rôles en cours d'exécution quelque part.  Et "quelque part" signifie la configuration distribuée susmentionnée.  La chose la plus importante ici est la topologie de cluster.  Vous pouvez voir ici 3 groupes de réplication de 5 processus Tarantool: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eb6/118/a0f/eb6118a0f7c2381ce82491bd65272ec5.png"></div><br>  Nous ne voulons pas perdre les données, nous traitons donc les informations sur les processus en cours avec soin.  La cartouche surveille la configuration à l'aide d'une validation en deux phases.  Dès que nous voulons mettre à jour la configuration, il vérifie d'abord si les instances sont disponibles et prêtes à accepter la nouvelle configuration.  Après cela, la configuration est appliquée dans la deuxième phase.  Ainsi, même si une instance est temporairement indisponible, alors rien ne peut mal tourner.  La configuration ne sera tout simplement pas appliquée et vous verrez une erreur à l'avance. <br><br>  La section topologie a également un paramètre aussi important que le leader de chaque groupe de réplication.  Il s'agit généralement de l'instance qui accepte les écritures.  Les autres sont le plus souvent en lecture seule, bien qu'il puisse y avoir des exceptions.  Parfois, les développeurs courageux n'ont pas peur des conflits et peuvent écrire des données sur plusieurs répliques en même temps.  Néanmoins, certaines opérations ne doivent pas être effectuées deux fois.  C'est pourquoi nous avons un leader. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f96/b85/7f3/f96b857f346a756f27c05651b85a9886.png"></div><br><h2>  Cycle de vie des rôles </h2><br>  Pour qu'une architecture de projet contienne des rôles abstraits, le cadre doit en quelque sorte être capable de les gérer.  Naturellement, les rôles sont gérés sans redémarrer le processus Tarantool.  Il existe quatre rappels conçus pour la gestion des rôles.  La cartouche elle-même les appelle en fonction des informations de la configuration distribuée, appliquant ainsi la configuration aux rôles spécifiques. <br><br><pre> <code class="plaintext hljs">function init() function validate_config() function apply_config() function stop()</code> </pre> <br>  Chaque rôle a une fonction <code>init</code> .  Il est appelé une fois: soit lorsque le rôle est activé, soit lorsque Tarantool redémarre.  Ici, il est pratique, par exemple, d'initialiser box.space.create, ou le planificateur peut exécuter une fibre d'arrière-plan qui terminerait la tâche à intervalles réguliers. <br><br>  La fonction <code>init</code> seule peut ne pas suffire.  La cartouche permet aux rôles d'accéder à la configuration distribuée utilisée pour stocker la topologie.  Dans la même configuration, nous pouvons déclarer une nouvelle section et y stocker une partie de la configuration métier.  Dans mon exemple, cela pourrait être un schéma de données ou des paramètres de planification pour le rôle de planificateur. <br><br>  Le cluster appelle <code>validate_config</code> et <code>apply_config</code> chaque fois que la configuration distribuée change.  Lorsqu'une configuration est appliquée dans une validation en deux phases, le cluster vérifie que chaque rôle sur chaque serveur est prêt à accepter cette nouvelle configuration et, si nécessaire, signale une erreur à l'utilisateur.  Lorsque tout le monde est d'accord avec la configuration, <code>apply_config</code> est appelé. <br><br>  Les rôles prennent également en charge une méthode d' <code>stop</code> pour nettoyer les ordures.  Si nous disons que l'ordonnanceur n'est pas nécessaire sur ce serveur, il peut arrêter les fibres qu'il a commencé à utiliser <code>init</code> . <br><br>  Les rôles peuvent interagir les uns avec les autres.  Nous sommes habitués à écrire des appels de fonction Lua, mais le processus peut ne pas avoir le rôle nécessaire.  Pour faciliter l'accès au réseau, nous utilisons un module auxiliaire appelé rpc (appel de procédure distante), qui est construit sur la base du module standard Tarantool net.box.  Cela peut être utile, par exemple, si votre passerelle souhaite demander directement au planificateur d'effectuer la tâche maintenant, plutôt qu'en une journée. <br><br>  Un autre point important est d'assurer la tolérance aux pannes.  La cartouche utilise le protocole SWIM <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">[4]</a> pour surveiller la santé.  En bref, les processus échangent des «rumeurs» via UDP, c'est-à-dire que chaque processus informe ses voisins des dernières nouvelles, et ils répondent.  S'il n'y a soudainement pas de réponse, Tarantool soupçonne que quelque chose ne va pas, et après un certain temps, il déclare la mort et envoie ce message à tout le monde. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/748/1ff/6f2/7481ff6f235c9833ac93d4aa310e2911.png"></div><br>  Sur la base de ce protocole, Cartridge organise le basculement automatique.  Chaque processus surveille son environnement et si le leader cesse soudainement de répondre, la réplique pourrait revendiquer son rôle et Cartridge configurerait les rôles en cours d'exécution en conséquence. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/49a/925/53b/49a92553be1edd2834c373b14cf17431.png"></div><br>  Vous devez être prudent ici car des allers-retours fréquents peuvent entraîner des conflits de données lors de la réplication.  Le basculement automatique ne doit certainement pas être activé de manière aléatoire.  Vous devez avoir une idée claire de ce qui se passe et être sûr que la réplication ne plantera pas lorsque le leader récupère et retrouve sa couronne. <br><br>  D'après tout ce qui a été dit, les rôles peuvent sembler similaires aux microservices.  Dans un sens, ils ne sont que des modules dans les processus Tarantool, et il existe plusieurs différences fondamentales.  Tout d'abord, tous les rôles de projet doivent vivre dans la même base de code.  Et tous les processus Tarantool devraient s'exécuter à partir de la même base de code, afin qu'il n'y ait pas de surprise, comme lorsque nous essayons d'initialiser le planificateur, mais qu'il n'y a tout simplement pas de planificateur.  De plus, nous ne devons pas autoriser de différences dans les versions de code car le comportement du système est compliqué à prévoir et à déboguer dans une telle situation. <br><br>  Contrairement à Docker, nous ne pouvons pas simplement prendre une "image" d'un rôle, le transférer sur une autre machine et l'exécuter là-bas.  Nos rôles ne sont pas aussi isolés que les conteneurs Docker.  De plus, nous ne pouvons pas exécuter deux rôles identiques sur la même instance.  Le rôle est là ou non;  dans un sens, c'est un singleton.  Et troisièmement, les rôles devraient être les mêmes au sein de l'ensemble du groupe de réplication, car sinon, cela aurait l'air ridicule: les données sont les mêmes, mais le comportement est différent. <br><br><h2>  Outils de déploiement </h2><br>  J'ai promis de vous montrer comment la cartouche pourrait aider à déployer des applications.  Pour vous faciliter la vie, le framework crée des packages RPM: <br><br><pre> <code class="plaintext hljs">$ cartridge pack rpm myapp # will create ./myapp-0.1.0-1.rpm $ sudo yum install ./myapp-0.1.0-1.rpm</code> </pre> <br>  Le package installé contient presque tout ce dont vous avez besoin: à la fois l'application et les dépendances Lua installées.  Tarantool arrive également sur le serveur en tant que dépendance de package RPM, et notre service est prêt à être lancé.  Tout cela se fait en utilisant systemd, mais d'abord, nous devons faire une configuration, au moins spécifier l'URI de chaque processus.  Trois suffiraient pour notre exemple. <br><br><pre> <code class="plaintext hljs">$ sudo tee /etc/tarantool/conf.d/demo.yml &lt;&lt;CONFIG myapp.router: {"advertise_uri": "localhost:3301", "http_port": 8080} myapp.storage_A: {"advertise_uri": "localhost:3302", "http_enabled": False} myapp.storage_B: {"advertise_uri": "localhost:3303", "http_enabled": False} CONFIG</code> </pre> <br>  Il y a un aspect intéressant à prendre en compte: au lieu de ne spécifier que le port du protocole binaire, nous spécifions l'adresse publique de l'ensemble du processus, y compris le nom d'hôte.  Nous le faisons parce que les nœuds de cluster doivent savoir comment se connecter les uns aux autres.  Ce serait une mauvaise idée d'utiliser l'adresse 0.0.0.0 comme advertise_uri, car ce devrait être une adresse IP externe, plutôt qu'une liaison de socket.  Rien ne fonctionne sans cela, donc Cartridge ne laisserait tout simplement pas le nœud avec le mauvais advertise_uri démarrer. <br><br>  Maintenant que la configuration est prête, nous pouvons démarrer les processus.  Étant donné qu'une unité systemd régulière ne permet pas de démarrer plusieurs processus, les unités dites instanciées installent les applications sur la cartouche: <br><br><pre> <code class="plaintext hljs">$ sudo systemctl start myapp@router $ sudo systemctl start myapp@storage_A $ sudo systemctl start myapp@storage_B</code> </pre> <br>  Nous avons spécifié le port HTTP pour l'interface Web de la cartouche dans la configuration: 8080. Allons là-bas et jetons un œil: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c4d/c41/8fc/c4dc418fca8a18c3f3dd2c4430bd1eaa.png"></div><br>  Nous pouvons voir que les processus ne sont pas encore configurés, bien qu'ils soient déjà en cours d'exécution.  La cartouche ne sait pas encore comment la réplication doit être effectuée et ne peut pas décider seule, elle attend donc nos actions.  Nous n'avons pas beaucoup de choix: la vie d'un nouveau cluster commence par la configuration du premier nœud.  Ensuite, nous ajoutons d'autres nœuds au cluster, nous leur attribuons des rôles et le déploiement peut être considéré comme terminé avec succès. <br><br>  Versons-nous un verre et relaxons après une longue semaine de travail.  L'application est prête à l'emploi. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95e/78c/fc5/95e78cfc50600fc8b3acf86d3f553242.png"></div><br><h2>  Résultats </h2><br>  Et les résultats?  Veuillez tester, utiliser, laisser des commentaires et créer des tickets sur Github. <br><br><h2>  Les références </h2><br>  [1] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tarantool »2.2» Référence »Référence Rocks» Module vshard</a> <br>  [2] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment nous avons mis en œuvre le cœur de métier d'investissement d'Alfa-Bank basé sur Tarantool</a> <br>  [3] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Architecture de facturation de nouvelle génération: transition vers Tarantool</a> <br>  [4] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SWIM - Cluster Building Protocol</a> <br>  [5] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub - tarantool / cartouche-cli</a> <br>  [6] <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">GitHub - tarantool / cartouche</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr470812/">https://habr.com/ru/post/fr470812/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr470800/index.html">P - anticipation, ainsi que le programme préliminaire DUMP de Kazan. Voir les rapports qui ont passé la sélection du hachoir à viande</a></li>
<li><a href="../fr470802/index.html">Sauvegarde, partie 6: comparaison des outils de sauvegarde</a></li>
<li><a href="../fr470804/index.html">Alice Award: 5 idées du gagnant</a></li>
<li><a href="../fr470806/index.html">Problème de test fondamental</a></li>
<li><a href="../fr470808/index.html">Astra Linux "Eagle" Common Edition: existe-t-il la vie après Windows</a></li>
<li><a href="../fr470814/index.html">Matériaux d'Android-Mitap: outils d'autotests | Yandex Mapkit 3 | conception d'applications | Interface utilisateur pilotée par le serveur</a></li>
<li><a href="../fr470816/index.html">Russie - TPA - Biélorussie: ici le troisième n'est pas du tout superflu</a></li>
<li><a href="../fr470818/index.html">Bug Hunt, Blind-XSS et Fox Tricks</a></li>
<li><a href="../fr470820/index.html">Un test rapide de dizaines d'hypothèses: comment sortir de la routine et discuter dans une autre ville</a></li>
<li><a href="../fr470822/index.html">Surveillance de la température dans l'entreprise</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>