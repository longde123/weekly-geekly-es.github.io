<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚õ≥Ô∏è üññüèø üå∫ 3 histoires de crash de Kubernetes en production: anti-affinit√©, arr√™t gracieux, webhook üö® üôéüèø üï¥üèª</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Remarque perev. : Nous pr√©sentons une mini-s√©lection de post-mortem sur les probl√®mes mortels auxquels les ing√©nieurs de diff√©rentes entreprises ont √©...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>3 histoires de crash de Kubernetes en production: anti-affinit√©, arr√™t gracieux, webhook</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/flant/blog/475026/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/y7/_c/ra/y7_cracjhm0ke_mtazml1fhzprk.jpeg"></div><br>  <i><b>Remarque</b></i>  <i><b>perev.</b></i>  <i>: Nous pr√©sentons une mini-s√©lection de post-mortem sur les probl√®mes mortels auxquels les ing√©nieurs de diff√©rentes entreprises ont √©t√© confront√©s lors de l'exploitation de l'infrastructure bas√©e sur Kubernetes.</i>  <i>Chaque note parle du probl√®me lui-m√™me, de ses causes et de ses cons√©quences, et, bien s√ªr, d'une solution qui permet d'√©viter des situations similaires √† l'avenir.</i> <i><br><br></i>  <i>Comme vous le savez, apprendre de l'exp√©rience de quelqu'un d'autre est moins cher, et donc - laissez ces histoires vous aider √† vous pr√©parer √† d'√©ventuelles surprises.</i>  <i>Soit dit en passant, une large s√©lection de liens vers ces ¬´histoires d'√©checs¬ª est r√©guli√®rement publi√©e sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce site</a> (selon les donn√©es de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce r√©f√©rentiel Git</a> ).</i> <a name="habracut"></a><br><br><h2>  N ¬∞ 1.  Comment la panique du noyau a bloqu√© un site </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">clair de lune</a> .</i> <br><br>  Entre le 18 et le 22 janvier, le site Web Moonlight et l'API ont connu des dysfonctionnements intermittents.  Tout a commenc√© avec des erreurs API al√©atoires et s'est termin√© par un arr√™t complet.  Les probl√®mes ont √©t√© r√©solus et l'application est revenue √† la normale. <br><br><h3>  Informations g√©n√©rales </h3><br>  Moonlight utilise un logiciel appel√© Kubernetes.  Kubernetes ex√©cute des applications sur des groupes de serveurs.  Ces serveurs sont appel√©s n≈ìuds.  Les copies de l'application ex√©cut√©e sur le n≈ìud sont appel√©es pods.  Kubernetes a un planificateur qui d√©termine dynamiquement quels pods sur quels n≈ìuds devraient fonctionner. <br><br><h3>  Chronologie </h3><br>  Vendredi, les premi√®res erreurs √©taient li√©es √† des probl√®mes de connexion √† la base de donn√©es Redis.  L'API Moonlight utilise Redis pour v√©rifier les sessions pour chaque demande authentifi√©e.  Notre outil de surveillance Kubernetes a signal√© que certains n≈ìuds et pods ne r√©pondaient pas.  Dans le m√™me temps, Google Cloud a signal√© un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">dysfonctionnement des services r√©seau</a> , et nous avons d√©cid√© qu'ils √©taient √† l'origine de nos probl√®mes. <br><br>  Comme le trafic du week-end a diminu√©, les erreurs ont sembl√© √™tre r√©solues dans leur masse.  Cependant, mardi matin, le site de Moonlight est tomb√© et le trafic externe n'a pas du tout atteint le cluster.  Nous avons trouv√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">une autre personne sur Twitter</a> avec des sympt√¥mes similaires et avons d√©cid√© que l'h√©bergement Google avait une panne de r√©seau.  Nous avons contact√© l'assistance Google Cloud, qui a rapidement signal√© le probl√®me √† l'√©quipe d'assistance technique. <br><br>  L'√©quipe d'assistance technique de Google a r√©v√©l√© une certaine tendance dans le comportement des n≈ìuds de notre cluster Kubernetes.  La charge CPU des n≈ìuds individuels a atteint 100%, apr√®s quoi la panique du noyau s'est produite dans la machine virtuelle et elle s'est bloqu√©e. <br><br><h3>  Raisons </h3><br>  Le cycle √† l'origine de l'√©chec √©tait le suivant: <br><br><ul><li>  Le planificateur Kubernetes a h√©berg√© plusieurs pods avec une consommation √©lev√©e de CPU sur le m√™me n≈ìud. </li><li>  Les pods ont consomm√© toutes les ressources CPU du n≈ìud. </li><li>  Vint ensuite la panique du noyau, qui entra√Æna une p√©riode d'indisponibilit√© pendant laquelle le n≈ìud ne r√©pondit pas au planificateur. </li><li>  Le planificateur a d√©plac√© tous les pods tomb√©s vers un nouveau noeud, et le processus a √©t√© r√©p√©t√©, exacerbant la situation g√©n√©rale. </li></ul><br>  Initialement, l'erreur s'est produite dans le pod Redis, mais au final tous les pods qui fonctionnent avec le trafic sont tomb√©s, ce qui a conduit √† un arr√™t complet.  Des retards exponentiels pendant la replanification ont entra√Æn√© de plus longues p√©riodes d'indisponibilit√©. <br><br><h3>  Solution </h3><br>  Nous avons pu restaurer le site en ajoutant <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">des r√®gles anti-affinit√©</a> √† tous les d√©ploiements majeurs.  Ils distribuent automatiquement les modules sur les n≈ìuds, augmentant la tol√©rance aux pannes et les performances. <br><br>  Kubernetes lui-m√™me est con√ßu comme un syst√®me h√¥te tol√©rant aux pannes.  Moonlight utilise trois n≈ìuds sur des serveurs diff√©rents pour plus de stabilit√©, et nous ex√©cutons trois copies de chaque application qui dessert le trafic.  L'id√©e est d'avoir une copie sur chaque n≈ìud.  Dans ce cas, m√™me une d√©faillance de deux n≈ìuds n'entra√Ænera pas de temps d'arr√™t.  Cependant, Kubernetes a parfois plac√© les trois modules avec le site sur le m√™me n≈ìud, cr√©ant ainsi un goulot d'√©tranglement dans le syst√®me.  Dans le m√™me temps, d'autres applications exigeant de la puissance CPU (√† savoir le rendu c√¥t√© serveur) se sont retrouv√©es sur le m√™me n≈ìud, et non sur un autre. <br><br>  Un cluster Kubernetes correctement configur√© et fonctionnant correctement est n√©cessaire pour faire face √† de longues p√©riodes de charge CPU √©lev√©e et placer les pods de mani√®re √† maximiser l'utilisation des ressources disponibles.  Nous continuons de travailler avec la prise en charge de Google Cloud pour identifier et r√©soudre la cause premi√®re de la panique du noyau sur les serveurs. <br><br><h3>  Conclusion </h3><br>  Les r√®gles anti-affinit√© vous permettent de rendre les applications qui fonctionnent avec le trafic externe plus tol√©rantes aux pannes.  Si vous avez un service similaire chez Kubernetes, pensez √† les ajouter. <br><br>  Nous continuons de travailler avec les gars de Google pour trouver et √©liminer la cause des d√©faillances du noyau du syst√®me d'exploitation sur les n≈ìuds. <br><br><h2>  N ¬∞ 2.  Le ¬´sale¬ª secret de Kubernetes et Ingress endpoint </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Phil Pearl de Ravelin</a> .</i> <br><br><h3>  L'√©l√©gance est surfaite </h3><br>  Chez Ravelin, nous avons migr√© vers Kubernetes (sur GKE).  Le processus a √©t√© couronn√© de succ√®s.  Nos budgets de perturbation des pods sont aussi complets que jamais, les √©tats sont vraiment majestueux <i>(un jeu de mots difficile √† traduire: "nos ensembles avec √©tat sont tr√®s majestueux" - environ la traduction)</i> , et le remplacement coulissant des n≈ìuds fonctionne comme une horloge. <br><br>  La derni√®re pi√®ce du puzzle consiste √† d√©placer la couche API des anciennes machines virtuelles vers le cluster Kubernetes.  Pour ce faire, nous devons configurer Ingress afin que l'API soit accessible depuis le monde ext√©rieur. <br><br>  Au d√©but, la t√¢che semblait simple.  Nous d√©finissons simplement le contr√¥leur Ingress, modifions Terraform pour obtenir un certain nombre d'adresses IP, et Google s'occupe de presque tout le reste.  Et tout cela fonctionnera comme par magie.  Classe! <br><br>  Cependant, au fil du temps, ils ont commenc√© √† remarquer que les tests d'int√©gration recevaient p√©riodiquement des erreurs 502. De l√†, notre voyage a commenc√©.  Cependant, je vous ferai gagner du temps et irai directement aux conclusions. <br><br><h3>  Arr√™t progressif </h3><br>  Tout le monde parle d'un arr√™t progressif ("gracieux", arr√™t progressif).  Mais vous ne devriez vraiment pas compter sur lui √† Kubernetes.  Ou du moins, cela ne devrait pas √™tre l'arr√™t gracieux que vous avez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=https://golang.org/pkg/net/">absorb√© avec le lait de votre m√®re</a> .  Dans le monde Kubernetes, ce niveau ¬´d'√©l√©gance¬ª est inutile et menace de graves probl√®mes. <br><br><h3>  Monde parfait </h3><br>  Voici comment, dans la vue majoritaire, le pod est supprim√© du service ou de l'√©quilibreur de charge dans Kubernetes: <br><br><ol><li>  Le contr√¥leur de r√©plication d√©cide de supprimer le pod. </li><li>  Le module de noeud final est supprim√© du service ou de l'√©quilibreur de charge.  Le nouveau trafic vers le pod n'arrive plus. </li><li>  Un crochet de pr√©-arr√™t est appel√© ou le pod re√ßoit un signal SIGTERM. </li><li>  Le pod "gracieusement" est d√©connect√©.  Il cesse d'accepter les connexions entrantes. </li><li>  La d√©connexion "gracieuse" est termin√©e et le pod est d√©truit une fois toutes ses connexions existantes arr√™t√©es ou termin√©es. </li></ol><br>  Malheureusement, la r√©alit√© est compl√®tement diff√©rente. <br><br><h3>  Monde r√©el </h3><br>  La plupart de la documentation laisse entendre que tout se passe un peu diff√©remment, mais ils n'√©crivent explicitement √† ce sujet nulle part.  Le probl√®me principal est que l'√©tape 3 ne suit pas l'√©tape 2. Elles se produisent simultan√©ment.  Dans les services ordinaires, la suppression des points d'extr√©mit√© est si rapide que la probabilit√© de rencontrer des probl√®mes est extr√™mement faible.  Cependant, avec Ingresss, tout est diff√©rent: ils r√©agissent g√©n√©ralement beaucoup plus lentement, donc le probl√®me devient √©vident.  Le pod peut obtenir SIGTERM bien avant que les changements de points de terminaison n'entrent dans Ingress. <br><br>  En cons√©quence, un arr√™t gracieux n'est pas du tout ce qui est requis d'un pod.  Il recevra de nouvelles connexions et devra continuer √† les traiter, sinon les clients commenceront √† recevoir les 500e erreurs et toute l'histoire merveilleuse des d√©ploiements et de la mise √† l'√©chelle simples commencera √† s'effondrer. <br><br>  Voici ce qui se passe r√©ellement: <br><br><ol><li>  Le contr√¥leur de r√©plication d√©cide de supprimer le pod. </li><li>  Le module de noeud final est supprim√© du service ou de l'√©quilibreur de charge.  Dans le cas d'Ingresss, cela peut prendre un certain temps et le nouveau trafic continuera de circuler dans le pod. </li><li>  Un crochet de pr√©-arr√™t est appel√© ou le pod re√ßoit un signal SIGTERM. </li><li>  Dans une large mesure, le pod doit ignorer cela, continuer √† fonctionner et maintenir de nouvelles connexions.  Si possible, il devrait laisser entendre aux clients que ce serait bien de passer √† un autre endroit.  Par exemple, dans le cas de HTTP, il peut envoyer <code>Connection: close</code> dans les en-t√™tes de r√©ponse. </li><li>  Le pod ne sort que lorsque la p√©riode d'attente ¬´√©l√©gante¬ª expire et qu'il est tu√© par SIGKILL. </li><li>  Assurez-vous que cette p√©riode est plus longue que le temps n√©cessaire pour reprogrammer l'√©quilibreur de charge. </li></ol><br>  S'il s'agit d'un code tiers et que vous ne pouvez pas modifier son comportement, la meilleure chose √† faire est d'ajouter un crochet de pr√©-arr√™t qui dormira juste pendant une p√©riode ¬´√©l√©gante¬ª, de sorte que le pod continuera de fonctionner comme si de rien arriv√©. <br><br><h2>  Num√©ro 3.  Comment un simple webhook a provoqu√© une panne de cluster </h2><br>  <i>Original: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Jetstack</a> .</i> <br><br>  Jetstack propose √† ses clients des plateformes multi-locataires sur Kubernetes.  Parfois, il existe des exigences particuli√®res que nous ne pouvons pas satisfaire avec la configuration standard de Kubernetes.  Pour les impl√©menter, nous avons r√©cemment commenc√© √† utiliser l' <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Open Policy Agent</a> <i>(nous avons √©crit plus en d√©tail sur le projet dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">cette revue</a> - environ Transl.)</i> En tant que contr√¥leur d'acc√®s pour la mise en ≈ìuvre de politiques sp√©ciales. <br><br>  Cet article d√©crit l'√©chec provoqu√© par une mauvaise configuration de cette int√©gration. <br><br><h3>  Incident </h3><br>  Nous √©tions engag√©s dans la mise √† jour de l'assistant pour le cluster de d√©veloppement, dans lequel diverses √©quipes ont test√© leurs applications pendant la journ√©e de travail.  Il s'agissait d'un cluster r√©gional dans la zone europe-ouest1 sur le moteur Google Kubernetes (GKE). <br><br>  Les commandes ont √©t√© averties qu'une mise √† jour √©tait en cours, sans aucun temps d'arr√™t pr√©vu.  Plus t√¥t dans la journ√©e, nous avons d√©j√† effectu√© une mise √† jour similaire vers un autre environnement de pr√©-production. <br><br>  Nous avons commenc√© la mise √† niveau en utilisant notre pipeline GKE Terraform.  La mise √† jour de l'assistant ne s'est pas termin√©e avant l'expiration du d√©lai d'expiration Terraform, que nous avons d√©fini pour 20 minutes.  Ce fut le premier r√©veil que quelque chose s'est mal pass√©, bien que dans la console GKE le cluster soit toujours r√©pertori√© comme ¬´mise √† niveau¬ª. <br><br>  Le red√©marrage du pipeline a conduit √† l'erreur suivante <br><br><pre> <code class="bash hljs">google_container_cluster.cluster: Error waiting <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> updating GKE master version: All cluster resources were brought up, but the cluster API is reporting that: component <span class="hljs-string"><span class="hljs-string">"kube-apiserver"</span></span> from endpoint <span class="hljs-string"><span class="hljs-string">"gke-..."</span></span> is unhealthy</code> </pre> <br>  Cette fois, la connexion avec le serveur API a commenc√© √† √™tre interrompue p√©riodiquement et les √©quipes n'ont pas pu d√©ployer leurs applications. <br><br>  Pendant que nous essayions de comprendre ce qui se passait, tous les n≈ìuds ont commenc√© √† √™tre d√©truits et recr√©√©s dans un cycle sans fin.  Cela a conduit √† un d√©ni de service aveugle pour tous nos clients. <br><br><h3>  Nous √©tablissons la cause profonde de l'√©chec </h3><br>  Avec l'assistance Google, nous avons pu d√©terminer la s√©quence des √©v√©nements qui ont conduit √† l'√©chec: <br><br><ol><li>  GKE a termin√© la mise √† niveau sur une instance de l'assistant et a commenc√© √† accepter tout le trafic vers le serveur API sur celui-ci lors de la mise √† jour des assistants suivants. </li><li>  Lors de la mise √† niveau de la deuxi√®me instance de l'assistant, le serveur API n'a pas pu ex√©cuter <a href="">PostStartHook</a> pour <a href="">enregistrer l'autorit√© de certification.</a> </li><li>  Pendant l'ex√©cution de ce hook, le serveur API a essay√© de mettre √† jour ConfigMap appel√© <code>extension-apiserver-authentication</code> dans <code>kube-system</code> .  Cela n'a pas √©t√© possible car le backend pour le webhook de v√©rification de l'Open Policy Agent (OPA) que nous avons configur√© n'a pas r√©pondu. </li><li>  Pour que l'assistant r√©ussisse un contr√¥le d'int√©grit√©, cette op√©ration doit aboutir.  Comme cela ne s'est pas produit, le deuxi√®me ma√Ætre est entr√© dans le cycle d'urgence et a arr√™t√© la mise √† jour. </li></ol><br>  Le r√©sultat a √©t√© des plantages d'API p√©riodiques, en raison desquels les kubelets n'ont pas pu signaler la sant√© du n≈ìud.  √Ä son tour, cela a conduit au fait que le m√©canisme de restauration automatique des n≈ìuds GKE <i>(r√©paration</i> automatique des n≈ìuds <i>) a</i> commenc√© √† red√©marrer les n≈ìuds.  Cette fonctionnalit√© est d√©crite en d√©tail dans la <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">documentation</a> : <br><br><blockquote>  <i>Un √©tat malsain peut signifier: dans un d√©lai donn√© (environ 10 minutes), le n≈ìud ne donne aucun √©tat.</i> </blockquote><br><h3>  Solution </h3><br>  Lorsque nous avons d√©couvert que la ressource <code>ValidatingAdmissionWebhook</code> provoquait un acc√®s intermittent au serveur API, nous l'avons supprim√© et restaur√© le cluster pour fonctionner. <br><br>  Depuis lors, le <code>ValidatingAdmissionWebhook</code> pour OPA a √©t√© configur√© pour surveiller uniquement les espaces de noms o√π la strat√©gie est applicable et auxquels les √©quipes de d√©veloppement ont acc√®s.  Nous avons √©galement limit√© le webhook √† <code>Ingress</code> and <code>Service</code> , les seuls avec lesquels notre politique fonctionne. <br><br>  Depuis le premier d√©ploiement de l'OPA, la documentation a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">√©t√© mise</a> √† <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">jour</a> pour refl√©ter ce changement. <br><br>  Nous avons √©galement ajout√© un test de vivacit√© pour nous assurer que l'OPA red√©marre en cas d'indisponibilit√© (et apport√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">les modifications appropri√©es</a> √† la documentation). <br><br>  Nous avons √©galement envisag√© de d√©sactiver <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le m√©canisme de</a> r√©cup√©ration automatique pour les n≈ìuds GKE, mais nous avons tout de m√™me d√©cid√© d'abandonner cette id√©e. <br><br><h3>  R√©sum√© </h3><br>  Si nous activions les alertes de temps de r√©ponse du serveur API, nous serions initialement en mesure de constater son augmentation globale pour toutes les demandes <code>CREATE</code> et <code>UPDATE</code> apr√®s le d√©ploiement du webhook pour OPA. <br><br>  Cela souligne l'importance de mettre en place des tests pour toutes les charges de travail.  Avec le recul, nous pouvons dire que le d√©ploiement de l'OPA √©tait si trompeusement simple que nous ne nous sommes m√™me pas impliqu√©s dans le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">diagramme de Helm</a> (bien qu'il le devrait).  Le tableau fait un certain nombre d'ajustements au-del√† des param√®tres de base d√©crits dans le manuel, y compris le param√®tre livenessProbe pour les conteneurs avec un contr√¥leur d'admission. <br><br>  Nous n'avons pas √©t√© les premiers √† rencontrer ce probl√®me: le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">probl√®me en amont</a> reste ouvert.  La fonctionnalit√© dans ce domaine peut clairement √™tre am√©lior√©e (et nous ferons un suivi √† ce sujet). <br><br><h2>  PS du traducteur </h2><br>  Lisez aussi dans notre blog: <br><br><ul><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Comment les priorit√©s des pods chez Kubernetes ont caus√© des temps d'arr√™t chez Grafana Labs</a> ;" </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">De la vie avec Kubernetes: comment les Espagnols ne se sont pas plaints du serveur HTTP</a> ¬ª; </li><li>  ¬´ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">6 bugs syst√®me divertissants dans le fonctionnement de Kubernetes [et leur solution]</a> ¬ª; </li><li>  " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">6 histoires pratiques de notre vie quotidienne SRE</a> ." </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr475026/">https://habr.com/ru/post/fr475026/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr475016/index.html">QA mitap √† Redmadrobot le 22 novembre</a></li>
<li><a href="../fr475018/index.html">Modification des colonnes Radiotehnika S-30</a></li>
<li><a href="../fr475020/index.html">Comment la technologie moderne remplace progressivement les tours d'incendie</a></li>
<li><a href="../fr475022/index.html">Schizophr√©nie architecturale Facebook Balance</a></li>
<li><a href="../fr475024/index.html">La course √† pied est un sport id√©al pour un travailleur √† distance. Partie 1: le chemin vers la premi√®re course d'une centaine de kilom√®tres</a></li>
<li><a href="../fr475028/index.html">Observations sur l'application du ML en affaires sur les actions ≈ΩijemeIT</a></li>
<li><a href="../fr475032/index.html">Gartner Hype Cycle 2019: d√©briefing</a></li>
<li><a href="../fr475034/index.html">Graphique dans le navigateur pour Arduino et STM32</a></li>
<li><a href="../fr475036/index.html">Migration de Cassandra vers Kubernetes: fonctionnalit√©s et solutions</a></li>
<li><a href="../fr475038/index.html">La premi√®re s√©rie de "Math√©matiques appliqu√©es et informatique" au HSE de Saint-P√©tersbourg: qui sont-ils et comment travailler avec eux?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>