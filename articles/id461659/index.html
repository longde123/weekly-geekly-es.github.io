<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🏴󠁧󠁢󠁥󠁮󠁧󠁿 🥔 👲 Neural Networks dan Deep Learning, Bab 4: Bukti Visual bahwa Neural Networks Dapat Menghitung Semua Fungsi 🕍 👩🏼‍✈️ 🎸</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dalam bab ini, saya memberikan penjelasan sederhana dan sebagian besar visual dari teorema universalitas. Untuk mengikuti materi dalam bab ini, Anda t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neural Networks dan Deep Learning, Bab 4: Bukti Visual bahwa Neural Networks Dapat Menghitung Semua Fungsi</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  Dalam bab ini, saya memberikan penjelasan sederhana dan sebagian besar visual dari teorema universalitas.  Untuk mengikuti materi dalam bab ini, Anda tidak harus membaca yang sebelumnya.  Ini disusun sebagai esai independen.  Jika Anda memiliki pemahaman paling dasar tentang NS, Anda harus dapat memahami penjelasannya. <br><br><div class="spoiler">  <b class="spoiler_title">Isi</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 1: menggunakan jaringan saraf untuk mengenali nomor tulisan tangan</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 2: cara kerja algoritma backpropagation</a> </li><li>  Bab 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1: meningkatkan metode pelatihan jaringan saraf</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 2: Mengapa regularisasi membantu mengurangi pelatihan ulang?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 3: bagaimana memilih hyperparameters jaringan saraf?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 4: bukti visual bahwa jaringan saraf mampu menghitung fungsi apa pun</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bab 5: mengapa jaringan saraf yang dalam begitu sulit untuk dilatih?</a> </li><li>  Bab 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 1: Pembelajaran Mendalam</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Bagian 2: kemajuan terbaru dalam pengenalan gambar</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Kata penutup: apakah ada algoritma sederhana untuk menciptakan kecerdasan?</a> </li></ul></div></div><br>  Salah satu fakta paling menakjubkan tentang jaringan saraf adalah mereka dapat menghitung fungsi apa pun.  Artinya, katakanlah seseorang memberi Anda semacam fungsi kompleks dan berliku f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  Dan terlepas dari fungsi ini, ada jaminan jaringan saraf sehingga untuk setiap input x nilai f (x) (atau beberapa perkiraan dekat dengan itu) akan menjadi output dari jaringan ini, yaitu: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Ini berfungsi bahkan jika itu adalah fungsi dari banyak variabel f ​​= f (x <sub>1</sub> , ..., x <sub>m</sub> ), dan dengan banyak nilai.  Sebagai contoh, di sini adalah jaringan yang menghitung suatu fungsi dengan m = 3 input dan n = 2 output: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Hasil ini menunjukkan bahwa jaringan saraf memiliki universalitas tertentu.  Tidak peduli apa fungsi yang ingin kita hitung, kita tahu bahwa ada jaringan saraf yang bisa melakukan ini. <br><br>  Selain itu, teorema universalitas berlaku bahkan jika kita membatasi jaringan pada satu lapisan antara neuron yang masuk dan keluar - yang disebut  dalam satu lapisan tersembunyi.  Jadi, bahkan jaringan dengan arsitektur yang sangat sederhana pun bisa sangat kuat. <br><br>  Teorema universalitas dikenal oleh orang-orang yang menggunakan jaringan saraf.  Tetapi meskipun demikian, pemahaman tentang fakta ini tidak begitu luas.  Dan sebagian besar penjelasan untuk ini terlalu rumit secara teknis.  Sebagai contoh, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">salah satu makalah pertama yang</a> membuktikan hasil ini menggunakan <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">teorema Hahn - Banach</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">teorema representasi Riesz</a> , dan beberapa analisis Fourier.  Jika Anda seorang ahli matematika, mudah bagi Anda untuk memahami bukti ini, tetapi bagi kebanyakan orang itu tidak mudah.  Sayang sekali, karena alasan dasar universalitas itu sederhana dan indah. <br><br>  Dalam bab ini, saya memberikan penjelasan sederhana dan sebagian besar visual dari teorema universalitas.  Kami akan melangkah langkah demi langkah melalui ide-ide yang melatarbelakanginya.  Anda akan mengerti mengapa jaringan saraf benar-benar dapat menghitung fungsi apa pun.  Anda akan memahami beberapa batasan dari hasil ini.  Dan Anda akan memahami bagaimana hasilnya dikaitkan dengan NS yang mendalam. <br><br>  Untuk mengikuti materi dalam bab ini, Anda tidak harus membaca yang sebelumnya.  Ini disusun sebagai esai independen.  Jika Anda memiliki pemahaman paling dasar tentang NS, Anda harus dapat memahami penjelasannya.  Namun saya terkadang akan memberikan tautan ke materi sebelumnya untuk membantu mengisi kesenjangan pengetahuan. <br><br>  Teorema universalitas sering ditemukan dalam ilmu komputer, jadi terkadang kita bahkan lupa betapa menakjubkannya mereka.  Tapi ada baiknya mengingatkan diri sendiri: kemampuan untuk menghitung fungsi sewenang-wenang benar-benar luar biasa.  Hampir setiap proses yang dapat Anda bayangkan dapat direduksi menjadi penghitungan suatu fungsi.  Pertimbangkan tugas menemukan nama komposisi musik berdasarkan pada bagian singkat.  Ini dapat dianggap sebagai penghitungan fungsi.  Atau pertimbangkan tugas menerjemahkan teks berbahasa Mandarin ke bahasa Inggris.  Dan ini dapat dianggap sebagai penghitungan fungsi (pada kenyataannya, banyak fungsi, karena ada banyak opsi yang dapat diterima untuk menerjemahkan satu teks).  Atau pertimbangkan tugas menghasilkan deskripsi plot film dan kualitas akting berdasarkan file mp4.  Ini juga dapat dianggap sebagai penghitungan fungsi tertentu (pernyataan yang dibuat tentang opsi terjemahan teks juga benar di sini).  Universalitas berarti bahwa, pada prinsipnya, NS dapat melakukan semua tugas ini, dan banyak lainnya. <br><br>  Tentu saja, hanya dari fakta bahwa kita tahu bahwa ada NS yang mampu, misalnya, menerjemahkan dari Bahasa Mandarin ke Bahasa Inggris, tidak berarti kita memiliki teknik yang baik untuk membuat atau bahkan mengenali jaringan semacam itu.  Pembatasan ini juga berlaku untuk teorema universalitas tradisional untuk model seperti skema Boolean.  Tetapi, seperti yang telah kita lihat dalam buku ini, NS memiliki algoritma yang kuat untuk fungsi pembelajaran.  Kombinasi dari algoritma pembelajaran dan fleksibilitas adalah campuran yang menarik.  Sejauh ini, dalam buku ini, kami telah berkonsentrasi pada algoritma pelatihan.  Dalam bab ini, kita akan fokus pada keserbagunaan dan apa artinya. <br><br><h2>  Dua trik </h2><br>  Sebelum menjelaskan mengapa teorema universalitas itu benar, saya ingin menyebutkan dua trik yang terkandung dalam pernyataan informal “jaringan saraf dapat menghitung fungsi apa pun”. <br><br>  Pertama, ini tidak berarti bahwa jaringan dapat digunakan untuk menghitung fungsi apa pun secara akurat.  Kita hanya bisa mendapatkan perkiraan sebaik yang kita butuhkan.  Dengan meningkatkan jumlah neuron tersembunyi, kami meningkatkan aproksimasi.  Sebagai contoh, saya sebelumnya menggambarkan jaringan yang menghitung fungsi tertentu f (x) menggunakan tiga neuron tersembunyi.  Untuk sebagian besar fungsi, menggunakan tiga neuron, hanya perkiraan berkualitas rendah yang dapat diperoleh.  Dengan meningkatkan jumlah neuron tersembunyi (katakanlah, hingga lima), kita biasanya bisa mendapatkan perkiraan yang lebih baik: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  Dan untuk memperbaiki situasi dengan meningkatkan jumlah neuron tersembunyi lebih lanjut. <br><br>  Untuk memperjelas pernyataan ini, katakanlah kita diberi fungsi f (x), yang ingin kita hitung dengan akurasi yang diperlukan ε&gt; 0.  Ada jaminan bahwa ketika menggunakan jumlah neuron tersembunyi yang cukup, kita selalu dapat menemukan NS yang outputnya g (x) memenuhi persamaan | g (x) −f (x) | &lt;ε untuk setiap x.  Dengan kata lain, perkiraan akan dicapai dengan akurasi yang diinginkan untuk setiap nilai input yang memungkinkan. <br><br>  Tangkapan kedua adalah bahwa fungsi yang dapat diperkirakan dengan metode yang dijelaskan adalah milik kelas kontinu.  Jika fungsi terputus, yaitu, ia membuat lompatan tajam tiba-tiba, maka dalam kasus umum tidak mungkin untuk mendekati dengan bantuan NS.  Dan ini tidak mengejutkan, karena NS kami menghitung fungsi input data yang berkelanjutan.  Namun, bahkan jika fungsi yang benar-benar perlu kita hitung adalah diskontinyu, perkiraannya seringkali cukup kontinu.  Jika demikian, maka kita dapat menggunakan NS.  Dalam praktiknya, batasan ini biasanya tidak penting. <br><br>  Akibatnya, pernyataan yang lebih akurat dari teorema universalitas adalah bahwa NS dengan satu lapisan tersembunyi dapat digunakan untuk memperkirakan setiap fungsi kontinu dengan akurasi yang diinginkan.  Dalam bab ini, kami membuktikan versi yang sedikit kurang keras dari teorema ini, menggunakan dua lapisan tersembunyi, bukan satu.  Dalam tugas, saya akan menjelaskan secara singkat bagaimana penjelasan ini dapat disesuaikan, dengan perubahan kecil, menjadi bukti yang hanya menggunakan satu lapisan tersembunyi. <br><br><h2>  Fleksibilitas dengan satu input dan satu nilai output </h2><br>  Untuk memahami mengapa teorema universalitas itu benar, kita mulai dengan memahami cara membuat fungsi perkiraan NS hanya dengan satu input dan satu nilai output: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Ternyata inilah esensi dari tugas universalitas.  Setelah kami memahami kasus khusus ini, akan sangat mudah untuk memperluasnya ke fungsi dengan banyak nilai input dan output. <br><br>  Untuk membuat pemahaman tentang bagaimana membangun jaringan untuk menghitung f, kita mulai dengan jaringan yang mengandung lapisan tersembunyi tunggal dengan dua neuron tersembunyi, dan dengan lapisan keluaran yang mengandung satu neuron keluaran: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Untuk membayangkan bagaimana komponen jaringan bekerja, kami fokus pada neuron tersembunyi atas.  Dalam diagram di <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel asli,</a> Anda dapat secara interaktif mengubah berat dengan mouse dengan mengklik "w" dan segera melihat bagaimana fungsi yang dihitung oleh perubahan neuron tersembunyi atas: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Seperti yang kita pelajari sebelumnya dalam buku ini, neuron tersembunyi menghitung σ (wx + b), di mana σ (z) ≡ 1 / (1 + e <sup>−z</sup> ) adalah <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">sigmoid</a> .  Sejauh ini, kami cukup sering menggunakan bentuk aljabar ini.  Namun, untuk membuktikan universalitas, akan lebih baik jika kita mengabaikan aljabar ini sepenuhnya, dan sebaliknya memanipulasi dan mengamati bentuk pada grafik.  Ini tidak hanya akan membantu Anda lebih merasakan apa yang terjadi, tetapi juga memberi kami bukti universalitas yang berlaku untuk fungsi aktivasi lainnya selain sigmoid. <br><br>  Sebenarnya, pendekatan visual yang saya pilih secara tradisional tidak dianggap sebagai bukti.  Tetapi saya percaya bahwa pendekatan visual memberikan lebih banyak wawasan tentang kebenaran hasil akhir daripada bukti tradisional.  Dan, tentu saja, pemahaman seperti itu adalah tujuan nyata dari buktinya.  Dalam bukti yang saya usulkan, celah kadang-kadang akan muncul;  Saya akan memberikan bukti visual yang masuk akal, tetapi tidak selalu ketat.  Jika ini mengganggu Anda, maka anggap tugas Anda untuk mengisi kekosongan ini.  Namun, jangan lupa tujuan utama: untuk memahami mengapa teorema universalitas itu benar. <br><br>  Untuk memulai dengan bukti ini, klik pada offset b pada diagram asli dan seret ke kanan untuk memperbesarnya.  Anda akan melihat bahwa dengan peningkatan offset, grafik bergerak ke kiri, tetapi tidak berubah bentuk. <br><br>  Kemudian seret ke kiri untuk mengurangi offset.  Anda akan melihat bahwa grafik bergerak ke kanan tanpa mengubah bentuk. <br><br>  Kurangi berat badan menjadi 2-3.  Anda akan melihat bahwa ketika berat berkurang, kurva meluruskan.  Agar kurva tidak lari dari grafik, Anda mungkin harus memperbaiki offset. <br><br>  Akhirnya, tambah bobot ke nilai yang lebih besar dari 100. Kurva akan menjadi lebih curam, dan akhirnya mendekati langkah.  Coba sesuaikan offset sehingga sudutnya berada di wilayah titik x = 0,3.  Video di bawah ini menunjukkan apa yang harus terjadi: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Kami dapat sangat menyederhanakan analisis kami dengan meningkatkan bobot sehingga output benar-benar merupakan perkiraan yang baik dari fungsi langkah.  Di bawah ini saya membuat output neuron tersembunyi atas untuk berat w = 999.  Ini adalah gambar statis: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Menggunakan fungsi langkah sedikit lebih mudah daripada dengan sigmoid khas.  Alasannya adalah bahwa kontribusi dari semua neuron tersembunyi ditambahkan di lapisan output.  Jumlah dari sekelompok fungsi langkah mudah untuk dianalisis, tetapi lebih sulit untuk berbicara tentang apa yang terjadi ketika sekelompok kurva ditambahkan dalam bentuk sigmoid.  Oleh karena itu, akan jauh lebih mudah untuk mengasumsikan bahwa neuron tersembunyi kita menghasilkan fungsi bertahap.  Lebih tepatnya, kami melakukan ini dengan memperbaiki bobot w pada beberapa nilai yang sangat besar, dan kemudian menetapkan posisi langkah melalui offset.  Tentu saja, bekerja dengan output sebagai fungsi langkah adalah perkiraan, tetapi sangat bagus, dan sejauh ini kami akan memperlakukan fungsi tersebut sebagai fungsi langkah yang sebenarnya.  Nanti, saya akan kembali untuk membahas efek penyimpangan dari perkiraan ini. <br><br>  Berapa nilai x adalah langkahnya?  Dengan kata lain, bagaimana posisi langkah tergantung pada berat dan perpindahan? <br><br>  Untuk menjawab pertanyaan, cobalah mengubah bobot dan mengimbangi di bagan interaktif.  Bisakah Anda mengerti bagaimana posisi langkah tergantung pada w dan b?  Dengan sedikit berlatih, Anda dapat meyakinkan diri sendiri bahwa posisinya proporsional dengan b dan berbanding terbalik dengan w. <br><br>  Bahkan, langkahnya adalah pada s = −b / w, seperti yang akan terlihat jika kita menyesuaikan berat dan perpindahan ke nilai berikut: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Kehidupan kita akan sangat disederhanakan jika kita menggambarkan neuron tersembunyi dengan parameter tunggal, s, yaitu, dengan posisi langkah, s = −b / w.  Dalam diagram interaktif berikut, Anda dapat dengan mudah mengubah s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Seperti disebutkan di atas, kami secara khusus menetapkan bobot w pada input ke nilai yang sangat besar - cukup besar sehingga fungsi langkah menjadi perkiraan yang baik.  Dan kita dapat dengan mudah mengubah neuron yang diparameterisasi dengan cara ini kembali ke bentuk biasanya dengan memilih bias b = −ws. <br><br>  Sejauh ini, kami hanya berkonsentrasi pada output dari neuron tersembunyi superior.  Mari kita lihat perilaku seluruh jaringan.  Misalkan neuron tersembunyi menghitung fungsi langkah yang didefinisikan oleh parameter langkah s <sub>1</sub> (neuron atas) dan s <sub>2</sub> (neuron bawah).  Bobot output masing-masing adalah w <sub>1</sub> dan w <sub>2</sub> .  Inilah jaringan kami: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  Di sebelah kanan adalah grafik dari output tertimbang w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 dari</sub> lapisan tersembunyi.  Di sini a <sub>1</sub> dan <sub>2</sub> adalah output dari neuron tersembunyi atas dan bawah, masing-masing.  Mereka dilambangkan dengan "a", karena mereka sering disebut aktivasi neuron. <br><br>  By the way, kami mencatat bahwa output dari seluruh jaringan adalah σ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), di mana b adalah bias dari neuron output.  Ini, jelas, tidak sama dengan output tertimbang dari lapisan tersembunyi, grafik yang sedang kita bangun.  Tetapi untuk saat ini, kami akan berkonsentrasi pada output seimbang dari lapisan tersembunyi, dan hanya kemudian berpikir tentang bagaimana hubungannya dengan output dari seluruh jaringan. <br><br>  Cobalah untuk menambah dan mengurangi langkah s <sub>1 dari</sub> neuron tersembunyi atas pada diagram interaktif <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">di artikel asli</a> .  Lihat bagaimana ini mengubah output tertimbang dari lapisan tersembunyi.  Sangat berguna untuk memahami apa yang terjadi ketika s <sub>1</sub> melebihi s <sub>2</sub> .  Anda akan melihat bahwa grafik dalam kasus-kasus ini berubah bentuk, ketika kita bergerak dari situasi di mana neuron tersembunyi atas diaktifkan terlebih dahulu ke situasi di mana neuron tersembunyi bawah diaktifkan terlebih dahulu. <br><br>  Demikian pula, cobalah memanipulasi langkah <sub>2 dari</sub> neuron tersembunyi yang lebih rendah dan lihat bagaimana ini mengubah output keseluruhan dari neuron tersembunyi. <br><br>  Cobalah untuk mengurangi dan meningkatkan bobot keluaran.  Perhatikan bagaimana skala ini kontribusi dari neuron tersembunyi yang sesuai.  Apa yang terjadi jika salah satu bobot sama dengan 0? <br><br>  Terakhir, coba atur w <sub>1</sub> ke 0.8 dan w <sub>2</sub> ke -0.8.  Hasilnya adalah fungsi "tonjolan", dengan awal pada s <sub>1</sub> , akhir pada s <sub>2</sub> , dan ketinggian 0,8.  Misalnya, output tertimbang mungkin terlihat seperti ini: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Tentu saja, tonjolan dapat ditingkatkan ke ketinggian berapa pun.  Mari kita gunakan satu parameter, h, yang menunjukkan tinggi.  Juga, untuk kesederhanaan, saya akan menghilangkan notasi "s <sub>1</sub> = ..." dan "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Coba tambah dan kurangi nilai h untuk melihat bagaimana ketinggian penonjolan berubah.  Cobalah untuk membuat h negatif.  Coba ubah titik langkah untuk mengamati bagaimana ini mengubah bentuk tonjolan. <br><br>  Anda akan melihat bahwa kami menggunakan neuron kami tidak hanya sebagai grafis primitif, tetapi juga sebagai unit yang lebih akrab bagi programmer - sesuatu seperti instruksi if-then-else dalam pemrograman: <br><br>  jika input&gt; = mulai dari langkah: <br>  tambahkan 1 ke output tertimbang <br>  lain: <br>  tambahkan 0 ke output tertimbang <br><br>  Sebagian besar saya akan menempel pada notasi grafis.  Namun, kadang-kadang akan berguna bagi Anda untuk beralih ke tampilan if-then-else dan merenungkan apa yang terjadi dalam istilah-istilah ini. <br><br>  Kita dapat menggunakan trik penonjolan kita dengan menempelkan dua bagian neuron tersembunyi bersama di jaringan yang sama: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Di sini saya menjatuhkan bobot dengan hanya menuliskan nilai h untuk setiap pasangan neuron tersembunyi.  Coba mainkan dengan kedua nilai h dan lihat bagaimana ia mengubah grafik.  Pindahkan tab, ubah titik langkah. <br><br>  Dalam kasus yang lebih umum, ide ini dapat digunakan untuk mendapatkan jumlah puncak yang diinginkan dari ketinggian berapa pun.  Secara khusus, kita dapat membagi interval [0,1] menjadi sejumlah besar subintervals (N), dan menggunakan pasangan N neuron tersembunyi untuk mendapatkan puncak dari ketinggian yang diinginkan.  Mari kita lihat bagaimana ini bekerja untuk N = 5.  Ini sudah cukup banyak neuron, jadi saya presentasi sedikit lebih sempit.  Maaf untuk diagram yang rumit - Saya bisa menyembunyikan kerumitan di balik abstraksi tambahan, tetapi bagi saya nilainya sedikit siksaan dengan kerumitan agar lebih merasakan bagaimana kerja jaringan saraf. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Anda tahu, kami memiliki lima pasang neuron tersembunyi.  Poin dari langkah-langkah pasangan yang sesuai terletak di 0,1 / 5, lalu 1 / 5,2 / 5, dan seterusnya, hingga 4 / 5,5 / 5.  Nilai-nilai ini adalah tetap - kami mendapatkan lima tonjolan dengan lebar yang sama pada grafik. <br><br>  Setiap pasangan neuron memiliki nilai h yang terkait dengannya.  Ingat bahwa koneksi neuron output memiliki bobot h dan –h.  Dalam artikel asli pada bagan, Anda dapat mengklik nilai h dan memindahkannya ke kiri-kanan.  Dengan perubahan ketinggian, jadwal juga berubah.  Dengan mengubah bobot keluaran, kami membangun fungsi akhir! <br><br>  Pada diagram, Anda masih dapat mengklik grafik, dan seret ketinggian langkah ke atas atau ke bawah.  Ketika Anda mengubah ketinggiannya, Anda akan melihat bagaimana ketinggian h yang terkait berubah.  Output bobot + h dan –h berubah sesuai.  Dengan kata lain, kami secara langsung memanipulasi fungsi yang grafiknya diperlihatkan di sebelah kanan dan melihat perubahan ini dalam nilai h di sebelah kiri.  Anda juga dapat menahan tombol mouse di salah satu tonjolan, lalu seret mouse ke kiri atau kanan, dan tonjolan akan menyesuaikan dengan ketinggian saat ini. <br><br>  Saatnya menyelesaikan pekerjaan. <br><br>  Ingat fungsi yang saya gambar di awal bab ini: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Lalu saya tidak menyebutkan ini, tetapi sebenarnya ini terlihat seperti ini: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhizbo6dyHFauBSdTVTZkDsiRmg1Zg#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>15</mn><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&nbsp;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mn>50</mn><mi>x</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>113</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Ini dibangun untuk nilai x dari 0 hingga 1, dan nilai sepanjang sumbu y bervariasi dari 0 hingga 1. <br><br>  Jelas, fungsi ini tidak trivial.  Dan Anda harus mencari cara menghitungnya menggunakan jaringan saraf. <br><br>  Dalam jaringan saraf kami di atas, kami menganalisis kombinasi berbobot ∑jwj dari output neuron tersembunyi.  Kami tahu cara mendapatkan kontrol yang signifikan atas nilai ini.  Tetapi, seperti yang saya sebutkan sebelumnya, nilai ini tidak sama dengan output jaringan.  Output dari jaringan adalah σ (∑ <sub>jw</sub> <sub>j</sub> a <sub>j</sub> + b), di mana b adalah perpindahan dari neuron output.  Bisakah kita mendapatkan kontrol langsung atas output jaringan? <br><br>  Solusinya adalah mengembangkan jaringan saraf di mana output tertimbang dari lapisan tersembunyi diberikan oleh persamaan σ <sup>−1</sup> ⋅f (x), di mana σ <sup>−1</sup> adalah fungsi kebalikan dari σ.  Artinya, kami ingin output tertimbang dari lapisan tersembunyi menjadi seperti ini: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jika ini berhasil, maka output dari seluruh jaringan akan menjadi perkiraan yang baik untuk f (x) (saya mengatur offset neuron output ke 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maka tugas Anda adalah mengembangkan NS yang mendekati fungsi objektif yang ditunjukkan di atas. Untuk lebih memahami apa yang terjadi, saya sarankan Anda menyelesaikan masalah ini dua kali. Untuk pertama kalinya dalam </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artikel asli,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> klik pada grafik, dan langsung sesuaikan ketinggian dari berbagai tonjolan. Ini akan sangat mudah bagi Anda untuk mendapatkan perkiraan yang baik untuk fungsi tujuan. Tingkat perkiraan diperkirakan oleh deviasi rata-rata, perbedaan antara fungsi tujuan dan fungsi yang dihitung jaringan. Tugas Anda adalah membawa deviasi rata-rata ke nilai minimum. Tugas dianggap selesai ketika simpangan rata-rata tidak melebihi 0.40.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Setelah mencapai kesuksesan, tekan tombolReset, yang mengubah tab secara acak. Kedua kalinya, jangan menyentuh grafik, tetapi ubah nilai h di sisi kiri diagram, mencoba membawa deviasi rata-rata ke nilai 0,40 atau kurang. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jadi, Anda telah menemukan semua elemen yang diperlukan jaringan untuk menghitung fungsi f (x)! Perkiraannya ternyata kasar, tetapi kita dapat dengan mudah meningkatkan hasilnya dengan hanya meningkatkan jumlah pasangan neuron tersembunyi, yang akan meningkatkan jumlah tonjolan. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secara khusus, mudah untuk mengubah semua data yang ditemukan kembali ke tampilan standar dengan parameterisasi yang digunakan untuk NS. Biarkan saya dengan cepat mengingatkan Anda bagaimana ini bekerja. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pada lapisan pertama, semua bobot memiliki nilai konstanta yang besar, misalnya, w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perpindahan neuron tersembunyi dihitung melalui b = −ws. Jadi, misalnya, untuk neuron tersembunyi kedua, s = 0,2 berubah menjadi b = 0001000 × 0,2 = −200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lapisan terakhir dari skala ditentukan oleh nilai-nilai h. Jadi, misalnya, nilai yang Anda pilih untuk h pertama, h = -0.2, berarti bahwa bobot keluaran dari dua neuron tersembunyi atas masing-masing adalah -0,2 dan 0,2. Dan seterusnya, untuk seluruh lapisan bobot keluaran. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Akhirnya, offset neuron output adalah 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dan hanya itu: kami mendapat deskripsi lengkap NS, yang menghitung fungsi tujuan awal dengan baik. Dan kami memahami bagaimana meningkatkan kualitas perkiraan dengan meningkatkan jumlah neuron tersembunyi. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selain itu, dalam fungsi tujuan awal kami f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0.3sin (15x) + 0.05cos (50x) tidak ada yang istimewa. </font><font style="vertical-align: inherit;">Prosedur serupa dapat digunakan untuk fungsi kontinu pada interval dari [0,1] ke [0,1]. </font><font style="vertical-align: inherit;">Bahkan, kami menggunakan NS single-layer kami untuk membangun tabel pencarian untuk suatu fungsi. </font><font style="vertical-align: inherit;">Dan kita dapat mengambil ide ini sebagai dasar untuk mendapatkan bukti universalitas yang menyeluruh.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fungsi banyak parameter </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami memperluas hasil kami ke kasus satu set variabel input. Kedengarannya rumit, tetapi semua ide yang kita butuhkan sudah dapat dipahami untuk kasus ini dengan hanya dua variabel yang masuk. Oleh karena itu, kami mempertimbangkan kasus dengan dua variabel yang masuk. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mari kita mulai dengan melihat apa yang terjadi ketika neuron memiliki dua input: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami memiliki input x dan y, dengan bobot yang sesuai w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> dan mengimbangi b dari neuron. Kami mengatur bobot w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> hingga 0 dan bermain dengan yang pertama, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , dan mengimbangi b untuk melihat bagaimana mereka memengaruhi output neuron: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seperti yang Anda lihat, dengan w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0, input y tidak memengaruhi output neuron. Semuanya terjadi seolah-olah x adalah satu-satunya input.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dengan ini, menurut Anda apa yang akan terjadi ketika kami menambah bobot w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ke w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 dan w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> meninggalkan 0? Jika ini tidak segera jelas bagi Anda, pikirkan sedikit tentang masalah ini. Kemudian tonton video berikut, yang menunjukkan apa yang akan terjadi:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Seperti sebelumnya, dengan peningkatan bobot input, output mendekati bentuk langkah. Perbedaannya adalah bahwa fungsi langkah kami sekarang terletak di tiga dimensi. Seperti sebelumnya, kita dapat memindahkan lokasi langkah dengan mengubah offset. Sudutnya berada pada titik s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ≡ - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mari kita ulang diagram sehingga parameter adalah lokasi langkah: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami menganggap bahwa bobot input x sangat penting - Saya menggunakan w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 - dan bobot w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. Angka pada neuron adalah posisi langkah, dan x di atasnya mengingatkan kita bahwa kita memindahkan langkah di sepanjang sumbu x. Secara alami, sangat mungkin untuk mendapatkan fungsi langkah di sepanjang sumbu y, membuat bobot masuk untuk y besar (misalnya, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), dan bobot untuk x adalah 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Angka pada neuron, sekali lagi, menunjukkan posisi langkah, dan y di atasnya mengingatkan kita bahwa kita memindahkan langkah di sepanjang sumbu y. Saya bisa langsung menunjuk bobot untuk x dan y, tetapi saya tidak melakukannya, karena itu akan mengotori bagan. Tetapi perlu diingat bahwa penanda y menunjukkan bahwa berat untuk y adalah besar dan untuk x adalah 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kita dapat menggunakan fungsi langkah yang baru saja kita rancang untuk menghitung fungsi penonjolan tiga dimensi. Untuk melakukan ini, kita mengambil dua neuron, yang masing-masing akan menghitung fungsi langkah di sepanjang sumbu x. Kemudian kita menggabungkan fungsi langkah ini dengan bobot h dan –h, di mana h adalah tinggi penonjolan yang diinginkan. Semua ini dapat dilihat pada diagram berikut:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coba ubah nilai h. Lihat bagaimana kaitannya dengan bobot jaringan. Dan bagaimana dia mengubah ketinggian fungsi tonjolan di sebelah kanan. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coba juga untuk mengubah titik langkah, nilai yang diatur ke 0,30 di neuron tersembunyi atas. Lihat bagaimana itu mengubah bentuk tonjolan. Apa yang terjadi jika Anda memindahkannya melampaui titik 0,70 yang terkait dengan neuron tersembunyi yang lebih rendah? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Kami belajar bagaimana membangun fungsi tonjolan di sepanjang sumbu x. Secara alami, kita dapat dengan mudah membuat fungsi tonjolan di sepanjang sumbu y, menggunakan dua fungsi langkah di sepanjang sumbu y. Ingatlah bahwa kita dapat melakukan ini dengan membuat bobot besar pada input y, dan mengatur bobot 0 pada input x. Jadi, apa yang terjadi:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Itu terlihat hampir identik dengan jaringan sebelumnya! Satu-satunya perubahan yang terlihat adalah penanda y kecil pada neuron tersembunyi. Mereka mengingatkan kita bahwa mereka menghasilkan fungsi langkah untuk y, dan bukan untuk x, sehingga bobot pada input y sangat besar, dan pada input x itu adalah nol, dan bukan sebaliknya. Seperti sebelumnya, saya memutuskan untuk tidak menampilkannya secara langsung, agar tidak mengacaukan gambar. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mari kita lihat apa yang terjadi jika kita menambahkan dua fungsi tonjolan, satu di sepanjang sumbu x, yang lain di sepanjang sumbu y, keduanya tingginya h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Untuk menyederhanakan diagram koneksi dengan bobot nol, saya hilangkan. Sejauh ini, saya telah meninggalkan penanda x dan y kecil pada neuron tersembunyi untuk mengingat ke arah mana fungsi tonjolan dihitung. Nanti kita akan menolak mereka, karena mereka tersirat oleh variabel yang masuk.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Coba ubah parameter h. </font><font style="vertical-align: inherit;">Seperti yang Anda lihat, karena ini, bobot output berubah, serta bobot dari kedua fungsi penonjolan, x dan y. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Apa yang kami buat sedikit mirip dengan "fungsi menara": </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jika kami dapat membuat fungsi menara seperti itu, kami dapat menggunakannya untuk memperkirakan fungsi sewenang-wenang dengan hanya menambahkan menara berbagai ketinggian di tempat yang berbeda: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tentu saja, kami belum mencapai pembuatan fungsi menara yang sewenang-wenang. </font><font style="vertical-align: inherit;">Sejauh ini, kami telah membangun sesuatu seperti menara pusat tinggi 2 jam dengan dataran tinggi h di sekitarnya. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tapi kita bisa membuat fungsi menara. </font><font style="vertical-align: inherit;">Ingatlah bahwa kami sebelumnya menunjukkan bagaimana neuron dapat digunakan untuk mengimplementasikan pernyataan if-then-else:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Itu adalah neuron satu input.  Dan kita perlu menerapkan ide yang mirip dengan output gabungan dari neuron tersembunyi: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Jika kita memilih ambang kanan - misalnya, 3 jam / 2, terjepit di antara ketinggian dataran tinggi dan ketinggian menara pusat - kita dapat menghancurkan dataran tinggi itu hingga nol, dan hanya menyisakan satu menara. <br><br>  Bayangkan bagaimana melakukan ini?  Coba bereksperimen dengan jaringan berikut.  Sekarang kita sedang merencanakan keluaran dari seluruh jaringan, dan bukan hanya keluaran tertimbang dari lapisan tersembunyi.  Ini berarti bahwa kami menambahkan istilah offset ke output tertimbang dari lapisan tersembunyi, dan menerapkan sigmoid tersebut.  Bisakah Anda menemukan nilai untuk h dan b yang Anda dapatkan menara?  Jika Anda terjebak pada titik ini, berikut adalah dua tips: (1) agar neuron yang keluar menunjukkan perilaku if-then-else, kita perlu bobot yang masuk (semua jam atau –j) menjadi besar;  (2) nilai b menentukan skala ambang if-then-else. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Dengan parameter default, output mirip dengan versi rata dari diagram sebelumnya, dengan menara dan dataran tinggi.  Untuk mendapatkan perilaku yang diinginkan, Anda perlu meningkatkan nilai h.  Ini akan memberi kita perilaku ambang jika-maka-lain.  Kedua, untuk mengatur ambang dengan benar, seseorang harus memilih b ≈ −3h / 2. <br><br>  Begini tampilannya untuk h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Browser Anda tidak mendukung video HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Bahkan untuk nilai h yang relatif sederhana, kami mendapatkan fungsi menara yang bagus.  Dan, tentu saja, kita bisa mendapatkan hasil yang indah secara sewenang-wenang dengan meningkatkan h lebih jauh dan menjaga bias pada tingkat b = h3j / 2. <br><br>  Mari kita coba merekatkan dua jaringan bersama untuk menghitung dua fungsi menara yang berbeda.  Untuk memperjelas peran masing-masing dari dua subnet, saya menempatkannya dalam persegi panjang yang terpisah: masing-masing menghitung fungsi menara menggunakan teknik yang dijelaskan di atas.  Grafik di sebelah kanan menunjukkan keluaran tertimbang dari lapisan tersembunyi kedua, yaitu, kombinasi tertimbang dari fungsi menara. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  Secara khusus, dapat dilihat bahwa dengan mengubah bobot pada lapisan terakhir, Anda dapat mengubah ketinggian menara keluaran. <br><br>  Gagasan yang sama memungkinkan Anda menghitung menara sebanyak yang Anda suka.  Kita bisa membuat mereka semena-mena tipis dan tinggi.  Sebagai hasilnya, kami menjamin bahwa keluaran tertimbang dari lapisan tersembunyi kedua mendekati fungsi apa pun yang diinginkan dari dua variabel: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  Secara khusus, dengan membuat output tertimbang dari lapisan tersembunyi kedua mendekati σ <sup>−1</sup> ⋅f dengan baik, kami menjamin bahwa output jaringan kami akan menjadi perkiraan yang baik dari fungsi yang diinginkan f. <br><br>  Bagaimana dengan fungsi banyak variabel? <br><br>  Mari kita coba mengambil tiga variabel, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  Dapatkah jaringan berikut digunakan untuk menghitung fungsi menara dalam empat dimensi? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Di sini x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> menunjukkan input jaringan.  s <sub>1</sub> , t <sub>1,</sub> dan seterusnya - titik langkah untuk neuron - yaitu, semua bobot pada lapisan pertama besar, dan offset ditetapkan sehingga titik langkah adalah s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... Bobot pada lapisan kedua berganti, + h, −h, di mana h adalah angka yang sangat besar.  Output offset adalah −5j / 2. <br><br>  Jaringan menghitung fungsi sama dengan 1 dalam tiga kondisi: x <sub>1</sub> adalah antara s <sub>1</sub> dan t <sub>1</sub> ;  x <sub>2</sub> adalah antara s <sub>2</sub> dan t <sub>2</sub> ;  x <sub>3</sub> berada di antara s <sub>3</sub> dan t <sub>3</sub> .  Jaringan adalah 0 di semua tempat lain.  Ini adalah menara di mana 1 adalah bagian kecil dari ruang masuk, dan 0 adalah segalanya. <br><br>  Merekatkan banyak jaringan seperti itu, kita bisa mendapatkan menara sebanyak yang kita suka, dan memperkirakan fungsi sewenang-wenang dari tiga variabel.  Gagasan yang sama bekerja dalam dimensi m.  Hanya offset keluaran (+m + 1/2) h yang diubah untuk menekan dengan benar nilai yang diinginkan dan menghapus dataran tinggi. <br><br>  Nah, sekarang kita tahu cara menggunakan NS untuk memperkirakan fungsi sebenarnya dari banyak variabel.  Bagaimana dengan fungsi vektor f (x <sub>1</sub> , ..., x <sub>m</sub> ) ∈ R <sup>n</sup> ?  Tentu saja, fungsi seperti itu dapat dianggap hanya sebagai n fungsi nyata yang terpisah f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ), dan seterusnya.  Dan kemudian kita hanya merekatkan semua jaringan bersama.  Jadi mudah untuk mengetahuinya. <br><br><h3>  Tantangan </h3><br><ul><li>  Kami melihat bagaimana menggunakan jaringan saraf dengan dua lapisan tersembunyi untuk memperkirakan fungsi sewenang-wenang.  Bisakah Anda membuktikan bahwa ini mungkin dengan satu lapisan tersembunyi?  Petunjuk - coba bekerja dengan hanya dua variabel keluaran, dan tunjukkan bahwa: (a) dimungkinkan untuk mendapatkan fungsi langkah-langkah tidak hanya sepanjang sumbu x atau y, tetapi juga dalam arah sewenang-wenang;  (b) menjumlahkan banyak konstruksi dari langkah (a), adalah mungkin untuk memperkirakan fungsi putaran daripada menara persegi panjang;  © menggunakan menara bundar, dimungkinkan untuk memperkirakan fungsi yang berubah-ubah.  Langkah © akan lebih mudah dilakukan dengan menggunakan materi yang disajikan dalam bab ini sedikit di bawah. </li></ul><br><h2>  Melampaui neuron sigmoid </h2><br>  Kami telah membuktikan bahwa jaringan neuron sigmoid dapat menghitung fungsi apa pun.  Ingat bahwa dalam neuron sigmoid, input x <sub>1</sub> , x <sub>2</sub> , ... berubah pada output menjadi σ (jw <sub>j</sub> x <sub>j j</sub> + b), di mana w <sub>j</sub> adalah bobot, b adalah bias, dan σ adalah sigmoid. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  Bagaimana jika kita melihat tipe neuron lain menggunakan fungsi aktivasi yang berbeda, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Yaitu, kita mengasumsikan bahwa jika sebuah neuron memiliki x <sub>1</sub> , x <sub>2</sub> , ... bobot w <sub>1</sub> , w <sub>2</sub> , ... dan bias b, maka s (jw <sub>j</sub> x <sub>j</sub> + b) akan menjadi output. <br><br>  Kita dapat menggunakan fungsi aktivasi ini untuk melangkah, seperti dalam kasus sigmoid.  Coba (dalam <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">artikel asli</a> ) pada diagram untuk mengangkat beban, katakanlah, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Seperti dalam kasus sigmoid, karena ini, fungsi aktivasi dikompresi, dan sebagai hasilnya berubah menjadi perkiraan yang sangat baik dari fungsi langkah.  Coba ubah offset, dan Anda akan melihat bahwa kami dapat mengubah lokasi langkah menjadi apa pun.  Karena itu, kita dapat menggunakan semua trik yang sama seperti sebelumnya untuk menghitung fungsi yang diinginkan. <br><br>  Properti apa yang harus dimiliki s (z) agar ini berfungsi?  Kita perlu berasumsi bahwa s (z) didefinisikan dengan baik sebagai z → −∞ dan z → ∞.  Batasan ini adalah dua nilai yang diterima oleh fungsi langkah kami.  Kita juga perlu mengasumsikan bahwa batasan ini berbeda.  Jika mereka tidak berbeda, langkah-langkahnya tidak akan berhasil, hanya akan ada jadwal yang datar!  Tetapi jika fungsi aktivasi s (z) memenuhi sifat-sifat ini, neuron yang didasarkan padanya secara universal cocok untuk perhitungan. <br><br><h3>  Tugasnya </h3><br><ul><li>  Sebelumnya dalam buku ini, kami bertemu dengan jenis neuron yang berbeda - neuron linear yang diluruskan, atau unit linear yang diperbaiki, ReLU.  Jelaskan mengapa neuron semacam itu tidak memenuhi kondisi yang diperlukan untuk universalitas.  Temukan bukti keserbagunaan yang menunjukkan bahwa ReLU secara universal cocok untuk komputasi. </li><li>  Misalkan kita sedang mempertimbangkan neuron linier, dengan fungsi aktivasi s (z) = z.  Jelaskan mengapa neuron linier tidak memenuhi kondisi universalitas.  Tunjukkan bahwa neuron semacam itu tidak dapat digunakan untuk komputasi universal. </li></ul><br><h2>  Perbaiki fungsi langkah </h2><br>  Untuk saat ini, kami mengasumsikan bahwa neuron kami menghasilkan fungsi langkah yang akurat.  Ini adalah perkiraan yang baik, tetapi hanya perkiraan.  Bahkan, ada celah kegagalan yang sempit, yang ditunjukkan pada grafik berikut, di mana fungsi-fungsinya tidak berperilaku sama sekali seperti fungsi langkah: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  Dalam periode kegagalan ini, penjelasan saya tentang universalitas tidak berfungsi. <br><br>  Kegagalan tidak begitu menakutkan.  Dengan mengatur bobot input yang cukup besar, kita dapat membuat kesenjangan ini kecil secara sewenang-wenang.  Kita bisa membuatnya lebih kecil dari pada grafik, tidak terlihat oleh mata.  Jadi mungkin kita tidak perlu khawatir tentang masalah ini. <br><br>  Namun demikian, saya ingin memiliki beberapa cara untuk menyelesaikannya. <br><br>  Ternyata itu mudah dipecahkan.  Mari kita lihat solusi ini untuk menghitung fungsi NS dengan hanya satu input dan output.  Gagasan yang sama akan bekerja untuk menyelesaikan masalah dengan sejumlah besar input dan output. <br><br>  Secara khusus, misalkan kita ingin jaringan kita menghitung beberapa fungsi f.  Seperti sebelumnya, kami mencoba melakukan ini dengan mendesain jaringan sehingga keluaran tertimbang dari lapisan neuron yang tersembunyi adalah σ <sup>−1</sup> ⋅f (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Jika kita melakukan ini menggunakan teknik yang dijelaskan di atas, kita akan memaksa neuron yang tersembunyi untuk menghasilkan urutan fungsi tonjolan: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  Tentu saja, saya melebih-lebihkan ukuran interval kegagalan, sehingga lebih mudah dilihat.  Harus jelas bahwa jika kita menjumlahkan semua fungsi tonjolan ini, kita mendapatkan perkiraan yang cukup baik dari σ <sup>−1</sup> ⋅f (x) di mana-mana kecuali untuk interval kegagalan. <br><br>  Tetapi anggaplah bahwa alih-alih menggunakan perkiraan yang baru saja dijelaskan, kita menggunakan satu set neuron tersembunyi untuk menghitung perkiraan setengah dari fungsi tujuan awal kita, yaitu, σ <sup>−1</sup> ⋅f (x) / 2.  Tentu saja, itu akan terlihat seperti versi skala dari grafik terbaru: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  Dan anggaplah kita membuat satu set neuron tersembunyi menghitung perkiraan untuk σ <sup>−1</sup> ⋅f (x) / 2, namun, pada dasarnya tonjolan akan digeser setengah lebar: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Sekarang kita memiliki dua perkiraan berbeda untuk σ - 1⋅f (x) / 2.  Jika kita menjumlahkan dua perkiraan ini, kita memperoleh perkiraan umum untuk σ - 1⋅f (x).  Perkiraan umum ini masih akan memiliki ketidakakuratan dalam interval kecil.  Tetapi masalahnya akan lebih kecil dari sebelumnya - karena titik-titik yang jatuh ke dalam interval kegagalan dari aproksimasi pertama tidak akan jatuh ke dalam interval kegagalan aproksimasi kedua.  Oleh karena itu, perkiraan dalam interval ini akan menjadi sekitar 2 kali lebih baik. <br><br>  Kita dapat memperbaiki situasi dengan menambahkan sejumlah besar, M, dari tumpang tindih perkiraan fungsi σ - 1⋅f (x) / M.  Jika semua interval kegagalannya cukup sempit, arus apa pun hanya ada di salah satunya.  Jika Anda menggunakan cukup banyak perkiraan M yang tumpang tindih, hasilnya adalah perkiraan umum yang sangat baik. <br><br><h2>  Kesimpulan </h2><br>  Penjelasan tentang universalitas yang dibahas di sini pasti tidak bisa disebut deskripsi praktis tentang bagaimana cara menghitung fungsi menggunakan jaringan saraf!  Dalam hal ini, ini lebih seperti bukti fleksibilitas gerbang logika NAND dan banyak lagi.  Oleh karena itu, saya pada dasarnya mencoba membuat desain ini jelas dan mudah diikuti tanpa mengoptimalkan detailnya.  Namun, mencoba mengoptimalkan desain ini bisa menjadi latihan yang menarik dan instruktif untuk Anda. <br><br>  Meskipun hasil yang diperoleh tidak dapat langsung digunakan untuk membuat NS, ini penting karena menghilangkan pertanyaan tentang komputabilitas fungsi tertentu menggunakan NS.  Jawaban untuk pertanyaan seperti itu akan selalu positif.  Oleh karena itu, sudah benar untuk menanyakan apakah ada fungsi yang dapat dihitung, tetapi apa cara yang benar untuk menghitungnya. <br><br>  Desain universal kami hanya menggunakan dua lapisan tersembunyi untuk menghitung fungsi arbitrer.  Seperti yang telah kita diskusikan, adalah mungkin untuk mendapatkan hasil yang sama dengan satu lapisan tersembunyi.  Dengan ini, Anda mungkin bertanya-tanya mengapa kami membutuhkan jaringan yang dalam, yaitu jaringan dengan sejumlah besar lapisan tersembunyi.  Tidak bisakah kita hanya mengganti jaringan ini dengan yang dangkal yang memiliki satu lapisan tersembunyi? <br><br>  Meskipun pada prinsipnya dimungkinkan, ada alasan praktis yang baik untuk menggunakan jaringan saraf yang dalam.  Seperti dijelaskan dalam Bab 1, NS mendalam memiliki struktur hierarkis yang memungkinkan mereka beradaptasi dengan baik untuk mempelajari pengetahuan hierarkis, yang berguna untuk memecahkan masalah nyata.  Lebih khusus lagi, ketika memecahkan masalah seperti pengenalan pola, akan berguna untuk menggunakan sistem yang tidak hanya memahami piksel individu, tetapi juga konsep yang semakin kompleks: dari batas ke bentuk geometris sederhana, dan seterusnya, ke adegan kompleks yang melibatkan beberapa objek.  Dalam bab-bab selanjutnya kita akan melihat bukti yang mendukung fakta bahwa NS mendalam akan lebih mampu mengatasi studi hierarki pengetahuan seperti itu daripada yang dangkal.  Ringkasnya: universalitas memberi tahu kita bahwa NS dapat menghitung fungsi apa pun;  bukti empiris menunjukkan bahwa NS mendalam lebih baik disesuaikan dengan studi fungsi yang berguna untuk memecahkan banyak masalah dunia nyata. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id461659/">https://habr.com/ru/post/id461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id461649/index.html">Bagaimana saya menyelamatkan dunia</a></li>
<li><a href="../id461651/index.html">Frontend Weekly Digest (22 - 28 Juli 2019)</a></li>
<li><a href="../id461653/index.html">Software Defined Radio - bagaimana cara kerjanya? Bagian 10</a></li>
<li><a href="../id461655/index.html">Intisari bahan-bahan segar dari dunia front-end untuk minggu terakhir No. 373 (22 - 28 Juli 2019)</a></li>
<li><a href="../id461657/index.html">Membeli Topi Merah: Akan Membantu Pertempuran Raksasa Biru Untuk Kepemimpinan Awan Hibrida</a></li>
<li><a href="../id461661/index.html">Panduan Pengembangan Berbasis Komponen</a></li>
<li><a href="../id461663/index.html">Kisah bagaimana Linux membawa Windows</a></li>
<li><a href="../id461665/index.html">Zen2. Evolusi platform AM4 pada contoh Ryzen 7 3700x</a></li>
<li><a href="../id461669/index.html">PHP Digest No. 161 (15 - 29 Juli 2019)</a></li>
<li><a href="../id461673/index.html">8 tips untuk programmer pemula atau retrospeksi karir saya</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>