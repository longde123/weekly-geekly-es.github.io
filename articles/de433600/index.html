<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∞üèæ üôÜüèø üöΩ Pixel 3 lernt, wie man die Tiefe von Fotos bestimmt ü§Ø üèë üëäüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Im Hochformat auf Pixel-Smartphones k√∂nnen Sie professionell aussehende Fotos aufnehmen, die die Aufmerksamkeit auf das Motiv lenken und den Hintergru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Pixel 3 lernt, wie man die Tiefe von Fotos bestimmt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/433600/"> Im Hochformat auf Pixel-Smartphones k√∂nnen Sie professionell aussehende Fotos aufnehmen, die die Aufmerksamkeit auf das Motiv lenken und den Hintergrund verwischen.  Letztes Jahr haben wir beschrieben, wie wir die Tiefe mit einer einzigen Kamera und einem Phasendetektions-Autofokus (Phase-Detection-Autofokus, PDAF) berechnen, der auch als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dual-Pixel-Autofokus bezeichnet wird</a> .  Dieser Prozess verwendete einen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">traditionellen Stereo-Algorithmus</a> ohne Training.  In diesem Jahr haben wir bei Pixel 3 maschinelles Lernen eingef√ºhrt, um die Tiefenbewertung zu verbessern und im Hochformat noch bessere Ergebnisse zu erzielen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/505/531/899/505531899e63adc78fbd74d94f1c3a3a.gif"><br>  <i>Links: Das in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">HDR + aufgenommene</a> Originalbild.</i>  <i>Auf der rechten Seite sehen Sie einen Vergleich der Aufnahmeergebnisse im Hochformat mit der Tiefe des herk√∂mmlichen Stereo- und maschinellen Lernens.</i>  <i>Lernergebnisse f√ºhren zu weniger Fehlern.</i>  <i>Im traditionellen Stereo-Ergebnis wird die Tiefe vieler horizontaler Linien hinter dem Mann f√§lschlicherweise gleich der Tiefe des Mannes selbst gesch√§tzt, wodurch sie scharf bleiben.</i> <br><a name="habracut"></a><br><h2>  Ein kurzer Ausflug in das vorherige Material </h2><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Letztes Jahr haben</a> wir beschrieben, dass der Portr√§tmodus ein neuronales Netzwerk verwendet, um Pixel, die zu den Bildern von Personen geh√∂ren, und ein Hintergrundbild zu trennen, und diese zweistufige Maske mit Tiefeninformationen erg√§nzt, die von PDAF-Pixeln abgeleitet sind.  All dies wurde getan, um je nach Tiefe eine Unsch√§rfe zu erzielen, die der einer professionellen Kamera nahe kommt. <br><br>  Zur Arbeit macht der PDAF zwei leicht unterschiedliche Aufnahmen der Szene.  Wenn Sie zwischen Bildern wechseln, k√∂nnen Sie sehen, dass sich die Person nicht bewegt und der Hintergrund sich horizontal bewegt - dieser Effekt wird als <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Parallaxe bezeichnet</a> .  Da die Parallaxe eine Funktion des Abstands eines Punkts von der Kamera und des Abstands zwischen zwei Blickwinkeln ist, k√∂nnen wir die Tiefe bestimmen, indem wir jeden Punkt in einem Bild mit dem entsprechenden Punkt in einem anderen vergleichen. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e15/41c/044/e1541c044baa5454ddee71ef45c9a96c.gif"><br>  <i>Die PDAF-Bilder links und in der Mitte sehen √§hnlich aus, aber die Parallaxe ist im vergr√∂√üerten Fragment rechts zu sehen.</i>  <i>Am einfachsten ist es an der runden Struktur im Zentrum der Vergr√∂√üerung zu erkennen.</i> <br><br>  Das Auffinden solcher Entsprechungen in PDAF-Bildern (diese Methode wird als Stereotiefe bezeichnet) ist jedoch eine √§u√üerst schwierige Aufgabe, da sich die Punkte zwischen den Fotos sehr schwach bewegen.  Dar√ºber hinaus leiden alle Stereotechnologien unter Blendenproblemen.  Wenn Sie die Szene durch eine kleine Blende betrachten, ist es nicht m√∂glich, die Entsprechung von Punkten f√ºr Linien parallel zur Stereo-Grundlinie zu finden, dh die Linie, die die beiden Kameras verbindet.  Mit anderen Worten, wenn Sie die horizontalen Linien auf dem dargestellten Foto (oder die vertikalen Linien in Bildern mit Hochformat) untersuchen, sehen alle Verschiebungen in einem Bild relativ zu einem anderen ungef√§hr gleich aus.  Im Portr√§tmodus des letzten Jahres k√∂nnten all diese Faktoren zu Fehlern bei der Bestimmung der Tiefe und des Auftretens unangenehmer Artefakte f√ºhren. <br><br><h2>  Verbesserung der Tiefenbewertung </h2><br>  Im Pixel 3-Portr√§tmodus beheben wir diese Fehler, indem wir die Parallaxe von Stereofotos nur als einen von vielen Hinweisen in den Bildern betrachten.  Beispielsweise scheinen Punkte, die weit von der Fokusebene entfernt sind, weniger scharf zu sein, und dies ist ein Hinweis auf die defokussierte Tiefe.  Selbst wenn Sie ein Bild auf einem Flachbildschirm anzeigen, k√∂nnen Sie die Entfernung zu Objekten leicht absch√§tzen, da wir die ungef√§hre Gr√∂√üe allt√§glicher Objekte kennen (dh Sie k√∂nnen die Anzahl der Pixel, die das Gesicht einer Person darstellen, verwenden, um abzusch√§tzen, wie weit es sich befindet).  Dies wird ein semantischer Hinweis sein. <br><br>  Die manuelle Entwicklung eines Algorithmus, der diese Spitzen kombiniert, ist √§u√üerst schwierig. Mit MO k√∂nnen wir dies jedoch tun und gleichzeitig die Leistung der PDAF-Parallaxenspitzen verbessern.  Insbesondere trainieren wir ein in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow</a> geschriebenes <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Faltungsnetzwerk</a> , das Pixel von PDAF als Eingabe empf√§ngt und lernt, die Tiefe vorherzusagen.  Diese neue, verbesserte Methode zur Sch√§tzung der Tiefe basierend auf MO wird im Pixel 3-Portr√§tmodus verwendet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7db/0ff/5e3/7db0ff5e3063425f703aaf2b3f30fd77.png"><br>  <i>Unser Faltungs-Neuronales Netzwerk empf√§ngt PDAF-Bilder und liefert eine Tiefenkarte.</i>  <i>Das Netzwerk verwendet eine Encoder-Decoder-Architektur mit zus√§tzlichen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sprungverbindungen</a> und Restbl√∂cken.</i> <br><br><h2>  Neuronales Netzwerktraining </h2><br>  Um das Netzwerk zu trainieren, ben√∂tigen wir viele PDAF-Bilder und entsprechende hochwertige Tiefenkarten.  Und da wir Tiefenvorhersagen ben√∂tigen, um im Hochformat n√ºtzlich zu sein, m√ºssen die Trainingsdaten den Fotos √§hneln, die Benutzer mit Smartphones aufnehmen. <br><br>  Zu diesem Zweck haben wir ein spezielles Frankenfon-Ger√§t entwickelt, bei dem wir f√ºnf Pixel 3-Telefone kombiniert und eine WiFi-Verbindung zwischen ihnen hergestellt haben, sodass wir gleichzeitig Fotos von allen Telefonen aufnehmen konnten (mit einem Unterschied von nicht mehr als 2 ms).  Mit diesem Ger√§t haben wir hochwertige Tiefenkarten basierend auf Fotos berechnet, wobei sowohl Bewegung als auch Stereo aus mehreren Winkeln verwendet wurden. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/995/827/05b/99582705b2c447fa6faf95c5c114d20f.gif"><br>  <i>Links: Ein Ger√§t zum Sammeln von Trainingsdaten.</i>  <i>In der Mitte: Ein Beispiel f√ºr den Wechsel zwischen f√ºnf Fotos.</i>  <i>Die Kamerasynchronisation erm√∂glicht die Berechnung der Tiefe in dynamischen Szenen.</i>  <i>Rechts: Gesamttiefe.</i>  <i>Punkte mit geringer Sicherheit, bei denen der Vergleich von Pixeln in verschiedenen Fotos aufgrund der Schw√§che der Texturen ungewiss war, sind schwarz gestrichen und werden im Training nicht verwendet.</i> <br><br>  Die mit diesem Ger√§t erhaltenen Daten waren aus folgenden Gr√ºnden ideal f√ºr das Training des Netzwerks: <br><br><ul><li>  F√ºnf Gesichtspunkte garantieren das Vorhandensein einer Parallaxe in mehrere Richtungen, was uns vor dem Problem der Blende bewahrt. </li><li>  Die Position der Kameras stellt sicher, dass jeder Punkt im Bild in mindestens zwei Fotos wiederholt wird, wodurch die Anzahl der Punkte verringert wird, die nicht √ºbereinstimmen k√∂nnen. </li><li>  Die Basislinie, dh der Abstand zwischen den Kameras, ist gr√∂√üer als der des PDAF, was eine genauere Sch√§tzung der Tiefe garantiert. </li><li>  Die Kamerasynchronisation erm√∂glicht die Berechnung der Tiefe in dynamischen Szenen. </li><li>  Die Portabilit√§t des Ger√§ts garantiert die M√∂glichkeit, Fotos in der Natur aufzunehmen und Fotos zu simulieren, die Benutzer mit Smartphones aufnehmen. </li></ul><br>  Trotz der Idealit√§t der mit diesem Ger√§t erhaltenen Daten ist es immer noch √§u√üerst schwierig, die absolute Tiefe von Szenenobjekten vorherzusagen - jedes gegebene PDAF-Paar kann verschiedenen Tiefenkarten entsprechen (alles h√§ngt von den Eigenschaften der Objektive, der Brennweite usw. ab).  Um all dies zu ber√ºcksichtigen, sch√§tzen wir die relative Tiefe der Szenenobjekte, was ausreicht, um im Hochformat zufriedenstellende Ergebnisse zu erzielen. <br><br><h2>  Wir kombinieren das alles </h2><br>  Das Sch√§tzen der Tiefe mithilfe von MOs auf Pixel 3 sollte schnell funktionieren, damit Benutzer nicht zu lange auf Hochformatergebnisse warten m√ºssen.  Um jedoch gute Tiefensch√§tzungen mit kleiner Defokussierung und Parallaxe zu erhalten, m√ºssen Sie die neuronalen Netze des Fotos in voller Aufl√∂sung speisen.  Um schnelle Ergebnisse zu gew√§hrleisten, verwenden wir <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">TensorFlow Lite</a> , eine plattform√ºbergreifende L√∂sung zum Starten von MO-Modellen auf Mobilger√§ten und eingebetteten Ger√§ten sowie eine leistungsstarke Pixel 3-GPU, mit der Sie die Tiefe ungew√∂hnlich gro√üer Eingabedaten schnell berechnen k√∂nnen.  Anschlie√üend kombinieren wir die erhaltenen Tiefensch√§tzungen mit Masken aus unserem neuronalen Netzwerk, das Personen unterscheidet, um die sch√∂nsten Ergebnisse der Aufnahme im Hochformat zu erzielen. <br><br><h2>  Probieren Sie es selbst aus </h2><br>  In der Google Camera App Version 6.1 und h√∂her sind unsere Tiefenkarten in Bilder im Hochformat eingebettet.  Dies bedeutet, dass wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google Fotos-Tiefeneditor verwenden k√∂nnen</a> , um den Grad der Unsch√§rfe und den Fokuspunkt nach dem Aufnehmen eines Bildes zu √§ndern.  Sie k√∂nnen auch Programme von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Drittanbietern</a> verwenden, um Tiefenkarten aus JPEG zu extrahieren und sie selbst zu studieren.  Sie k√∂nnen auch ein Album <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">aus dem Link</a> entnehmen, das Karten mit relativer Tiefe und entsprechende Bilder im Hochformat zeigt, um den traditionellen Stereo- und MO-Ansatz zu vergleichen. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de433600/">https://habr.com/ru/post/de433600/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de433586/index.html">Wie wir den Hackathon nicht gewonnen haben</a></li>
<li><a href="../de433588/index.html">Erstaunliche Leistung paralleler C ++ 17-Algorithmen. Mythos oder Wirklichkeit?</a></li>
<li><a href="../de433592/index.html">Information: Yandex.Phone</a></li>
<li><a href="../de433596/index.html">Magellans Fehler: Puffer√ºberlauf oder Expedition um die Welt mit SQLite FTS</a></li>
<li><a href="../de433598/index.html">Wie LLVM die Funktion optimiert</a></li>
<li><a href="../de433602/index.html">Mathematische Einfachheit kann der Geschwindigkeit der Evolution zugrunde liegen.</a></li>
<li><a href="../de433604/index.html">Komfortable Arbeit mit Android Studio</a></li>
<li><a href="../de433606/index.html">SIEM-Tiefen: Out-of-Box-Korrelationen. Teil 3.2. Ereignisnormalisierungsmethode</a></li>
<li><a href="../de433608/index.html">Das Auto der Zukunft. Bildschirme statt Autoglas?</a></li>
<li><a href="../de433610/index.html">Notizen eines Phytochemikers. Persimmon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>