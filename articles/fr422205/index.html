<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>â° ğŸ’Š ğŸ‘¨â€ğŸ« Richard Hamming: Chapitre 13. ThÃ©orie de l'information ğŸ§”ğŸ½ âš¾ï¸ ğŸ¤¹ğŸ½</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nous l'avons fait! 

 Â«Le but de ce cours est de vous prÃ©parer Ã  votre avenir technique.Â» 
 Salut, Habr. Vous vous souvenez de l'article gÃ©nial Â«Vous ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Richard Hamming: Chapitre 13. ThÃ©orie de l'information</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/422205/">  Nous l'avons fait! <br><br><blockquote>  Â«Le but de ce cours est de vous prÃ©parer Ã  votre avenir technique.Â» </blockquote><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/6ff/9ea/d676ff9eadd2a38b0948de76bbf27fd4.jpg" alt="image" align="right">  Salut, Habr.  Vous vous souvenez de l'article gÃ©nial <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Vous et votre travailÂ»</a> (+219, 2588 signets, 429k lectures)? <br><br>  Hamming (oui, oui, les <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">codes Hamming Ã </a> vÃ©rification automatique et Ã  correction automatique) a un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">livre</a> entier Ã©crit sur la base de ses confÃ©rences.  Nous le traduisons, parce que l'homme parle d'affaires. <br><br>  Ce livre n'est pas seulement sur l'informatique, c'est un livre sur le style de pensÃ©e des gens incroyablement cool.  <i>Â«Ce n'est pas seulement une charge de pensÃ©e positive;</i>  <i>il dÃ©crit les conditions qui augmentent les chances de faire du bon travail. Â»</i> <br><br>  <i>Merci pour la traduction Ã  Andrei Pakhomov.</i> <br><br>  La thÃ©orie de l'information a Ã©tÃ© dÃ©veloppÃ©e par C.E.Shannon Ã  la fin des annÃ©es 40.  La direction des Bell Labs a insistÃ© pour l'appeler Â«thÃ©orie de la communicationÂ», car  c'est un nom beaucoup plus prÃ©cis.  Pour des raisons Ã©videntes, le nom "ThÃ©orie de l'information" a un impact beaucoup plus important sur le public, alors Shannon l'a choisi, et c'est ce que nous savons Ã  ce jour.  Le nom lui-mÃªme suggÃ¨re que la thÃ©orie traite de l'information, ce qui la rend importante, car nous pÃ©nÃ©trons plus profondÃ©ment dans l'Ã¨re de l'information.  Dans ce chapitre, je vais aborder quelques conclusions de base de cette thÃ©orie, je donnerai des preuves non pas strictes, mais plutÃ´t intuitives de certaines des dispositions distinctes de cette thÃ©orie, afin que vous compreniez ce qu'est la "thÃ©orie de l'information", oÃ¹ vous pouvez l'appliquer et oÃ¹ pas . <br><a name="habracut"></a><br>  Tout d'abord, qu'est-ce que Â«l'informationÂ»?  Shannon identifie les informations avec incertitude.  Il a choisi le logarithme nÃ©gatif de la probabilitÃ© d'un Ã©vÃ©nement comme mesure quantitative des informations que vous recevez lorsqu'un Ã©vÃ©nement se produit avec une probabilitÃ© p.  Par exemple, si je vous dis que le temps Ã  Los Angeles est brumeux, alors p est proche de 1, ce qui dans l'ensemble ne nous donne pas beaucoup d'informations.  Mais si je dis qu'il pleut Ã  Monterey en juin, il y aura de l'incertitude dans ce message, et il contiendra plus d'informations.  Un Ã©vÃ©nement fiable ne contient aucune information, car log 1 = 0. <br><br>  ArrÃªtons-nous lÃ -dessus plus en dÃ©tail.  Shannon estimait qu'une mesure quantitative de l'information devrait Ãªtre une fonction continue de la probabilitÃ© d'un Ã©vÃ©nement p, et pour les Ã©vÃ©nements indÃ©pendants, elle devrait Ãªtre additive - la quantitÃ© d'informations obtenues Ã  la suite de deux Ã©vÃ©nements indÃ©pendants devrait Ãªtre Ã©gale Ã  la quantitÃ© d'informations obtenues Ã  la suite d'un Ã©vÃ©nement conjoint.  Par exemple, le rÃ©sultat d'un lancer de dÃ©s et de piÃ¨ces est gÃ©nÃ©ralement considÃ©rÃ© comme un Ã©vÃ©nement indÃ©pendant.  Traduisons ce qui prÃ©cÃ¨de dans le langage des mathÃ©matiques.  Si I (p) est la quantitÃ© d'informations contenues dans l'Ã©vÃ©nement avec probabilitÃ© p, alors pour un Ã©vÃ©nement conjoint composÃ© de deux Ã©vÃ©nements indÃ©pendants x avec probabilitÃ© p <sub>1</sub> et y avec probabilitÃ© p <sub>2,</sub> nous obtenons <br><br><img src="https://habrastorage.org/webt/yi/hc/2d/yihc2dbw1d5_rlv2rptbyqs1mww.jpeg" alt="image"><br>  <i>(Ã©vÃ©nements indÃ©pendants x et y)</i> <br><br>  Il s'agit de l'Ã©quation fonctionnelle de Cauchy, vraie pour tous les p <sub>1</sub> et p2.  Pour rÃ©soudre cette Ã©quation fonctionnelle, supposons que <br><br>  p <sub>1</sub> = p <sub>2</sub> = p, <br><br>  Ã§a donne <br><br><img src="https://habrastorage.org/webt/dk/rj/gf/dkrjgfn-xucnz-twcyhq3l6xefw.jpeg" alt="image"><br><br>  Si p <sub>1</sub> = p <sup>2</sup> et p <sub>2</sub> = p, alors <br><br><img src="https://habrastorage.org/webt/ux/-s/cj/ux-scjh2wymlpjj-hpnsnzrgsly.jpeg" alt="image"><br><br>  etc.  En Ã©tendant ce processus en utilisant la mÃ©thode standard pour les exponentielles pour tous les nombres rationnels m / n, ce qui suit est vrai <br><br><img src="https://habrastorage.org/webt/gt/_t/ro/gt_trop25xscgj5m3c1-k0nxiwe.jpeg" alt="image"><br><br>  De la continuitÃ© supposÃ©e de la mesure d'information, il s'ensuit que la fonction logarithmique est la seule solution continue Ã  l'Ã©quation fonctionnelle de Cauchy. <br><br>  Dans la thÃ©orie de l'information, il est habituel de prendre la base du logarithme de 2, donc le choix binaire contient exactement 1 bit d'information.  Par consÃ©quent, les informations sont mesurÃ©es par la formule <br><br><img src="https://habrastorage.org/webt/wx/ix/ow/wxixowgi6tookkpcslvwgmu6bpg.jpeg" alt="image"><br><br>  ArrÃªtons-nous et voyons ce qui s'est passÃ© ci-dessus.  Tout d'abord, nous n'avons pas donnÃ© de dÃ©finition Ã  la notion d 'Â«informationÂ», nous avons juste dÃ©fini une formule pour sa mesure quantitative. <br><br>  DeuxiÃ¨mement, cette mesure dÃ©pend de l'incertitude, et bien qu'elle soit suffisamment adaptÃ©e aux machines - par exemple, les systÃ¨mes tÃ©lÃ©phoniques, la radio, la tÃ©lÃ©vision, les ordinateurs, etc. - elle ne reflÃ¨te pas une attitude humaine normale Ã  l'Ã©gard de l'information. <br><br>  TroisiÃ¨mement, il s'agit d'une mesure relative, elle dÃ©pend de l'Ã©tat actuel de vos connaissances.  Si vous regardez le flux de Â«nombres alÃ©atoiresÂ» du gÃ©nÃ©rateur de nombres alÃ©atoires, vous supposez que chaque nombre suivant est indÃ©fini, mais si vous connaissez la formule de calcul des Â«nombres alÃ©atoiresÂ», le nombre suivant sera connu et, par consÃ©quent, ne contiendra pas informations. <br><br>  Ainsi, la dÃ©finition donnÃ©e par Shannon pour l'information convient dans de nombreux cas aux machines, mais ne semble pas correspondre Ã  la comprÃ©hension humaine du mot.  Pour cette raison, la Â«thÃ©orie de l'informationÂ» devrait Ãªtre appelÃ©e Â«thÃ©orie de la communicationÂ».  Cependant, il est trop tard pour changer les dÃ©finitions (grÃ¢ce Ã  quoi la thÃ©orie a gagnÃ© sa popularitÃ© initiale, et qui font encore penser que cette thÃ©orie traite de Â«l'informationÂ»), donc nous devons les accepter, mais vous devez comprendre clairement Ã  quel point la dÃ©finition des informations donnÃ©e par Shannon est loin de son bon sens.  Les informations de Shannon portent sur quelque chose de complÃ¨tement diffÃ©rent, Ã  savoir l'incertitude. <br><br>  C'est Ã  cela que vous devez penser lorsque vous proposez une terminologie.  Dans quelle mesure la dÃ©finition proposÃ©e, par exemple, la dÃ©finition donnÃ©e par Shannon, avec votre idÃ©e originale, et en quoi est-elle diffÃ©rente?  Il n'y a presque pas de terme qui reflÃ©terait fidÃ¨lement votre vision antÃ©rieure du concept, mais en fin de compte, c'est la terminologie utilisÃ©e qui reflÃ¨te le sens du concept, donc formaliser quelque chose Ã  travers des dÃ©finitions claires fait toujours du bruit. <br><br>  ConsidÃ©rons un systÃ¨me dont l'alphabet est composÃ© de symboles q avec des probabilitÃ©s pi.  Dans ce cas, la <i>quantitÃ© moyenne d'informations</i> dans le systÃ¨me (sa valeur attendue) est: <br><br><img src="https://habrastorage.org/webt/wj/ss/83/wjss83z5fnynnyurbcftgokak6e.jpeg" alt="image"><br><br>  C'est ce qu'on appelle l'entropie du systÃ¨me de distribution de probabilitÃ© {pi}.  Nous utilisons le terme Â«entropieÂ» car la mÃªme forme mathÃ©matique se pose en thermodynamique et en mÃ©canique statistique.  C'est pourquoi le terme Â«entropieÂ» crÃ©e autour de lui une aura d'importance qui, finalement, n'est pas justifiÃ©e.  La mÃªme forme mathÃ©matique de notation n'implique pas la mÃªme interprÃ©tation des caractÃ¨res! <br><br>  L'entropie de la distribution de probabilitÃ© joue un rÃ´le majeur dans la thÃ©orie du codage.  L'inÃ©galitÃ© de Gibbs pour deux distributions de probabilitÃ© diffÃ©rentes pi et qi est l'une des consÃ©quences importantes de cette thÃ©orie.  Nous devons donc prouver que <br><br><img src="https://habrastorage.org/webt/jx/1v/c-/jx1vc-ac5t4kzyxhpplqfb20bmu.jpeg" alt="image"><br><br>  La preuve est basÃ©e sur un graphique Ã©vident, fig.  13.I, qui montre que <br><br><img src="https://habrastorage.org/webt/gn/_o/i1/gn_oi1vqrmafw6k5v0g043jpz30.jpeg" alt="image"><br><br>  et l'Ã©galitÃ© n'est atteinte que pour x = 1. Nous appliquons l'inÃ©galitÃ© Ã  chaque somme de la somme du cÃ´tÃ© gauche: <br><br><img src="https://habrastorage.org/webt/rg/wl/5o/rgwl5owcdte29pj1ycvixfgdogo.jpeg" alt="image"><br><br>  Si l'alphabet du systÃ¨me de communication se compose de q caractÃ¨res, puis en prenant la probabilitÃ© de transmission de chaque caractÃ¨re qi = 1 / q et en substituant q, nous obtenons de l'inÃ©galitÃ© de Gibbs <br><br><img src="https://habrastorage.org/webt/qo/dz/gv/qodzgviyhmwnovomxokegxao54e.jpeg" alt="image"><br><br><img src="https://habrastorage.org/webt/8n/2m/y9/8n2my9t_5vd7vktnvdezxi0p1q4.jpeg" alt="image"><br><br>  <i>Figure 13.I</i> <br><br>  Cela suggÃ¨re que si la probabilitÃ© de transmettre tous les q caractÃ¨res est la mÃªme et Ã©gale Ã  - 1 / q, alors l'entropie maximale est ln q, sinon l'inÃ©galitÃ© tient. <br><br>  Dans le cas d'un code uniquement dÃ©codÃ©, nous avons l'inÃ©galitÃ© Kraft <br><br><img src="https://habrastorage.org/webt/0-/8k/5l/0-8k5lsd16wlmzgwmmzvxgvlbr4.jpeg" alt="image"><br><br>  Maintenant, si nous dÃ©finissons des pseudo-probabilitÃ©s <br><br><img src="https://habrastorage.org/webt/s9/qc/fv/s9qcfv1ywfj7_zkfvwm4jna8cqw.jpeg" alt="image"><br><br>  oÃ¹ bien sÃ»r <img src="https://habrastorage.org/webt/is/_u/vw/is_uvwjifihdmybm68w_ayyfk-q.jpeg" alt="image">  = 1, qui dÃ©coule de l'inÃ©galitÃ© de Gibbs, <br><br><img src="https://habrastorage.org/webt/my/hy/s0/myhys0cvxd6kgzu7f8r10vtnlym.jpeg" alt="image"><br><br>  et appliquer une algÃ¨bre (rappelez-vous que K â‰¤ 1, afin que nous puissions omettre le terme logarithmique, et Ã©ventuellement renforcer l'inÃ©galitÃ© plus tard), nous obtenons <br><br><img src="https://habrastorage.org/webt/v0/i4/yo/v0i4yoxhwj8grndqupxwbo4zppw.jpeg" alt="image"><br><br>  oÃ¹ L est la longueur moyenne du code. <br><br>  Ainsi, l'entropie est la limite minimale pour tout code de caractÃ¨re avec un mot de code moyen L. C'est le thÃ©orÃ¨me de Shannon pour un canal sans interfÃ©rence. <br><br>  Nous considÃ©rons maintenant le thÃ©orÃ¨me principal sur les limites des systÃ¨mes de communication dans lesquels l'information est transmise sous la forme d'un flux de bits indÃ©pendants et du bruit est prÃ©sent.  On suppose que la probabilitÃ© de transmission correcte d'un bit est P&gt; 1/2, et la probabilitÃ© que la valeur du bit soit inversÃ©e pendant la transmission (une erreur se produit) est Q = 1 - P. Pour plus de commoditÃ©, nous supposons que les erreurs sont indÃ©pendantes et la probabilitÃ© d'erreur est la mÃªme pour chaque envoyÃ© bits - c'est-Ã -dire qu'il y a du Â«bruit blancÂ» dans le canal de communication. <br><br>  La faÃ§on dont nous avons un long flux de n bits codÃ©s dans un seul message est une extension Ã  n dimensions d'un code Ã  un bit.  Nous dÃ©terminerons la valeur de n plus tard.  ConsidÃ©rons un message composÃ© de n bits comme un point dans un espace Ã  n dimensions.  Comme nous avons un espace Ã  n dimensions - et pour simplifier, nous supposons que chaque message a la mÃªme probabilitÃ© d'occurrence - il y a M messages possibles (M sera Ã©galement dÃ©terminÃ© plus tard), par consÃ©quent, la probabilitÃ© de tout message envoyÃ© est Ã©gale Ã  <br><br><img src="https://habrastorage.org/webt/vs/gj/e5/vsgje5adnr3222jgs5qqlii1hga.jpeg" alt="image"><br><br><img src="https://habrastorage.org/webt/s8/mu/br/s8mubr4blaju8evwtbsgqxjvqye.jpeg" alt="image"><br>  (expÃ©diteur) <br>  <i>Graphique 13.II</i> <br><br>  Ensuite, considÃ©rez l'idÃ©e de la bande passante du canal.  Sans entrer dans les dÃ©tails, la capacitÃ© du canal est dÃ©finie comme la quantitÃ© maximale d'informations pouvant Ãªtre transmise de maniÃ¨re fiable sur le canal de communication, en tenant compte de l'utilisation du codage le plus efficace.  Il n'y a aucun argument selon lequel plus d'informations peuvent Ãªtre transmises par le canal de communication que sa capacitÃ©.  Cela peut Ãªtre prouvÃ© pour un canal symÃ©trique binaire (que nous utilisons dans notre cas).  La capacitÃ© du canal pour l'envoi au niveau du bit est dÃ©finie comme <br><br><img src="https://habrastorage.org/webt/pi/ek/76/piek76b-zyqfhs7k2qxzhleky9y.jpeg" alt="image"><br><br>  oÃ¹, comme prÃ©cÃ©demment, P est la probabilitÃ© d'absence d'erreur dans un bit envoyÃ©.  Lors de l'envoi de n bits indÃ©pendants, la capacitÃ© du canal est dÃ©terminÃ©e comme <br><br><img src="https://habrastorage.org/webt/nv/7x/67/nv7x67ybwwcisk8pxntmligj3ky.jpeg" alt="image"><br><br>  Si nous sommes proches de la bande passante du canal, alors nous devrions envoyer presque un tel volume d'informations pour chacun des caractÃ¨res ai, i = 1, ..., M. Ã‰tant donnÃ© que la probabilitÃ© d'occurrence de chaque caractÃ¨re ai est 1 / M, nous obtenons <br><br><img src="https://habrastorage.org/webt/ne/mf/om/nemfom_mjugxea6infrm7asm4la.jpeg" alt="image"><br><br>  lorsque nous envoyons l'un des M messages Ã©quiprobables ai, nous avons <br><br><img src="https://habrastorage.org/webt/mt/c-/si/mtc-siba4teyqt0flsyzqs3-eja.jpeg" alt="image"><br><br>  Lors de l'envoi de n bits, nous nous attendons Ã  ce que des erreurs nQ se produisent.  En pratique, pour un message composÃ© de n bits, nous aurons environ nQ erreurs dans le message reÃ§u.  Pour n grand, variation relative (variation = largeur de distribution,) <br>  la distribution du nombre d'erreurs sera plus Ã©troite avec l'augmentation de n. <br><br>  Donc, du cÃ´tÃ© de l'Ã©metteur, je prends le message ai pour envoyer et dessiner une sphÃ¨re autour d'elle avec un rayon <br><br><img src="https://habrastorage.org/webt/de/j8/h9/dej8h9wxzm0ktckfoksyksxdcm0.jpeg" alt="image"><br><br>  qui est lÃ©gÃ¨rement plus grand d'un montant Ã©gal Ã  e2 que le nombre d'erreurs Q attendu (figure 13.II).  Si n est suffisamment grand, il existe alors une probabilitÃ© arbitrairement faible d'apparition du point de message bj du cÃ´tÃ© rÃ©cepteur, qui va au-delÃ  de cette sphÃ¨re.  Nous allons dessiner la situation, telle que je la vois du point de vue de l'Ã©metteur: nous avons tous les rayons du message transmis ai au message reÃ§u bj avec une probabilitÃ© d'erreur Ã©gale (ou presque Ã©gale) Ã  la distribution normale, atteignant un maximum en nQ.  Pour tout e2 donnÃ©, il y a n si grand que la probabilitÃ© que le point rÃ©sultant bj, dÃ©passant ma sphÃ¨re, soit aussi petit que vous le souhaitez. <br><br>  ConsidÃ©rez maintenant la mÃªme situation de votre part (Fig. 13.III).  Du cÃ´tÃ© rÃ©cepteur, il y a une sphÃ¨re S (r) de mÃªme rayon r autour du point reÃ§u bj dans l'espace Ã  n dimensions, de sorte que si le message reÃ§u bj est Ã  l'intÃ©rieur de ma sphÃ¨re, alors le message ai que j'ai envoyÃ© est Ã  l'intÃ©rieur de votre sphÃ¨re. <br><br>  Comment une erreur peut-elle se produire?  Une erreur peut se produire dans les cas dÃ©crits dans le tableau ci-dessous: <br><br><img src="https://habrastorage.org/webt/ap/jj/ce/apjjcepa80evnhjxgc_swmf62z4.jpeg" alt="image"><br><br>  <i>Figure 13.III</i> <br><br><img src="https://habrastorage.org/webt/qt/m3/v8/qtm3v8viaw9cw291jxn11t9ya0i.jpeg" alt="image"><br><br>  Ici, nous voyons que si dans la sphÃ¨re construite autour du point reÃ§u, il y a au moins un point correspondant Ã  un Ã©ventuel message non codÃ© envoyÃ©, alors une erreur s'est produite lors de la transmission, car vous ne pouvez pas dÃ©terminer lequel de ces messages a Ã©tÃ© transmis.  Le message envoyÃ© ne contient pas d'erreurs uniquement si le point qui lui correspond est dans la sphÃ¨re, et il n'y a pas d'autres points possibles dans ce code qui se trouvent dans la mÃªme sphÃ¨re. <br><br>  Nous avons une Ã©quation mathÃ©matique pour la probabilitÃ© d'une erreur Re si ai a Ã©tÃ© envoyÃ© <br><br><img src="https://habrastorage.org/webt/uu/fr/fo/uufrfozpmxs0asmf4sjlinmk9rk.jpeg" alt="image"><br><br>  On peut jeter le premier facteur dans le second terme, en le prenant comme 1. Ainsi, on obtient l'inÃ©galitÃ© <br><br><img src="https://habrastorage.org/webt/oh/qt/_9/ohqt_97ig6afppntwqmz7ifd-fs.jpeg" alt="image"><br><br>  De toute Ã©vidence, <br><br><img src="https://habrastorage.org/webt/ac/hx/ty/achxtyvmg-aq2yirdswhnsm4olk.jpeg" alt="image"><br><br>  donc <br><br><img src="https://habrastorage.org/webt/ym/27/5p/ym275poqvhr2e8jyuajtui9a4ky.jpeg" alt="image"><br><br>  prÃ©senter une nouvelle demande au dernier membre Ã  droite <br><br><img src="https://habrastorage.org/webt/hd/4v/9g/hd4v9gnlnbiyjcjv-grtzvsslsg.jpeg" alt="image"><br><br>  Si n est pris assez grand, le premier terme peut Ãªtre pris arbitrairement petit, disons infÃ©rieur Ã  un certain nombre d.  Nous avons donc <br><br><img src="https://habrastorage.org/webt/lt/ey/hf/lteyhffjmrrzy8pjblxrsigpkzu.jpeg" alt="image"><br><br>  Voyons maintenant comment vous pouvez crÃ©er un code de remplacement simple pour coder M messages composÃ©s de n bits.  N'ayant aucune idÃ©e de comment construire le code (les codes de correction d'erreur n'ont pas encore Ã©tÃ© inventÃ©s), Shannon a choisi le codage alÃ©atoire.  Lancez une piÃ¨ce pour chacun des n bits du message et rÃ©pÃ©tez le processus pour M messages.  Tout ce que vous avez Ã  faire est de lancer des piÃ¨ces nM, donc c'est possible <br><br><img src="https://habrastorage.org/webt/ii/ke/ss/iikessacb5q-xgdhr5btks6otss.jpeg" alt="image"><br><br>  dictionnaires de code ayant la mÃªme probabilitÃ© de Â½nM.  Bien sÃ»r, le processus alÃ©atoire de crÃ©ation d'un livre de codes signifie qu'il existe une probabilitÃ© de doublons, ainsi que des points de code, qui seront proches les uns des autres et, par consÃ©quent, seront une source d'erreurs probables.  Il est nÃ©cessaire de prouver que si cela ne se produit pas avec une probabilitÃ© supÃ©rieure Ã  tout petit niveau d'erreur sÃ©lectionnÃ©, alors le n donnÃ© est suffisamment grand. <br>  Le point dÃ©cisif est que Shannon a fait la moyenne de tous les livres de codes possibles pour trouver l'erreur moyenne!  Nous utiliserons le symbole Av [.] Pour dÃ©signer la moyenne sur l'ensemble de tous les dictionnaires de code alÃ©atoire possibles.  Une moyenne sur la constante d, bien sÃ»r, donne une constante, car pour faire la moyenne chaque terme coÃ¯ncide avec tout autre terme de la somme <br><br><img src="https://habrastorage.org/webt/-j/gh/_z/-jgh_zkjbnuzjdj39bz03euss_g.jpeg" alt="image"><br><br>  qui peut Ãªtre augmentÃ© (M - 1 passe Ã  M) <br><br><img src="https://habrastorage.org/webt/ma/3u/wz/ma3uwzaee8iaohanrz5qsgvspnq.jpeg" alt="image"><br><br>  Pour tout message particulier, lors de la moyenne de tous les livres de codes, le codage passe par toutes les valeurs possibles, de sorte que la probabilitÃ© moyenne qu'un point se trouve dans une sphÃ¨re est le rapport du volume de la sphÃ¨re au volume total de l'espace.  L'Ã©tendue de la sphÃ¨re <br><br><img src="https://habrastorage.org/webt/ry/pu/qb/rypuqbtnwvzsd4ei3_-6sxsot3c.jpeg" alt="image"><br><br>  oÃ¹ s = Q + e2 &lt;1/2 et ns doit Ãªtre un entier. <br><br>  Le dernier terme Ã  droite est le plus grand de cette somme.  PremiÃ¨rement, nous estimons sa valeur par la formule de Stirling pour les factorielles.  Ensuite, nous regardons le coefficient de rÃ©duction du terme devant lui, notons que ce coefficient augmente en se dÃ©plaÃ§ant vers la gauche, et donc nous pouvons: (1) limiter la valeur de la somme Ã  la somme de la progression gÃ©omÃ©trique avec ce coefficient initial, (2) Ã©tendre la progression gÃ©omÃ©trique de ns membres Ã  nombre infini de termes, (3) calculer la somme de la progression gÃ©omÃ©trique infinie (algÃ¨bre standard, rien de significatif) et enfin obtenir la valeur limite (pour un n suffisamment grand): <br><br><img src="https://habrastorage.org/webt/6f/jo/lr/6fjolr9tmeljs-eglmaxup0yrli.jpeg" alt="image"><br><br>  Remarquez comment l'entropie H (s) est apparue dans l'identitÃ© binomiale.  Il est Ã  noter que l'expansion dans la sÃ©rie de Taylor H (s) = H (Q + e2) donne une estimation obtenue en prenant en compte uniquement la dÃ©rivÃ©e premiÃ¨re et en ignorant toutes les autres.  Collectons maintenant l'expression finale: <br><br><img src="https://habrastorage.org/webt/9h/-u/wd/9h-uwd1ukrxttavrdy91ifezgpw.jpeg" alt="image"><br><br>  oÃ¹ <br><br><img src="https://habrastorage.org/webt/bm/h4/bb/bmh4bbgfnl2h9g0-omxft9b8lbe.jpeg" alt="image"><br><br>  Il suffit de choisir e2 pour que e3 &lt;e1, puis le dernier terme soit arbitrairement petit, pour n suffisamment grand.  Par consÃ©quent, l'erreur PE moyenne peut Ãªtre obtenue arbitrairement petite avec une capacitÃ© de canal arbitrairement proche de C. <br>  Si la moyenne de tous les codes a une erreur suffisamment faible, alors au moins un code doit Ãªtre appropriÃ©, par consÃ©quent, au moins un systÃ¨me de codage appropriÃ© existe.  C'est un rÃ©sultat important obtenu par Shannon - Â«ThÃ©orÃ¨me de Shannon pour un canal avec interfÃ©renceÂ», mÃªme s'il convient de noter qu'il l'a prouvÃ© pour un cas beaucoup plus gÃ©nÃ©ral que pour un simple canal symÃ©trique binaire que j'ai utilisÃ©.  Pour le cas gÃ©nÃ©ral, les calculs mathÃ©matiques sont beaucoup plus compliquÃ©s, mais les idÃ©es ne sont pas si diffÃ©rentes, donc trÃ¨s souvent, en utilisant l'exemple d'un cas spÃ©cial, on peut rÃ©vÃ©ler la vraie signification du thÃ©orÃ¨me. <br><br>  Critiquons le rÃ©sultat.  Nous avons rÃ©pÃ©tÃ© Ã  plusieurs reprises: "Pour un n suffisamment grand".  Mais quelle est la taille de n?  TrÃ¨s, trÃ¨s grand, si vous voulez vraiment Ãªtre simultanÃ©ment proche de la bande passante du canal et Ãªtre sÃ»r du bon transfert de donnÃ©es!  Tellement gros qu'en fait, vous devrez attendre trÃ¨s longtemps pour accumuler un message Ã  partir de tant de bits afin de le coder plus tard.  Dans le mÃªme temps, la taille du dictionnaire de code alÃ©atoire sera Ã©norme (aprÃ¨s tout, un tel dictionnaire ne peut pas Ãªtre reprÃ©sentÃ© sous une forme plus courte que la liste complÃ¨te de tous les Mn bits, alors que n et M sont trÃ¨s grands)! <br><br>  Les codes de correction d'erreurs Ã©vitent d'attendre un trÃ¨s long message, avec son encodage et son dÃ©codage ultÃ©rieurs via de trÃ¨s grands livres de codes, car ils Ã©vitent les livres de codes en soi et utilisent Ã  la place des calculs conventionnels.  Dans une thÃ©orie simple, ces codes perdent gÃ©nÃ©ralement leur capacitÃ© Ã  approcher la capacitÃ© du canal et maintiennent en mÃªme temps un taux d'erreur assez faible, mais lorsque le code corrige un grand nombre d'erreurs, ils donnent de bons rÃ©sultats.  En d'autres termes, si vous disposez d'une certaine capacitÃ© de canal pour la correction d'erreurs, vous devez utiliser l'option de correction d'erreurs la plupart du temps, c'est-Ã -dire que dans chaque message envoyÃ©, un grand nombre d'erreurs doit Ãªtre corrigÃ©, sinon vous perdez cette capacitÃ© en vain. <br><br>  De plus, le thÃ©orÃ¨me prouvÃ© ci-dessus n'est toujours pas dÃ©nuÃ© de sens!  Il montre que les systÃ¨mes de transmission efficaces devraient utiliser des schÃ©mas de codage sophistiquÃ©s pour les trÃ¨s longues chaÃ®nes de bits.  Un exemple est les satellites qui ont volÃ© en dehors de la planÃ¨te extÃ©rieure;  Ã  mesure qu'ils s'Ã©loignent de la Terre et du Soleil, ils sont obligÃ©s de corriger de plus en plus d'erreurs dans le bloc de donnÃ©es: certains satellites utilisent des batteries solaires, qui fournissent environ 5 watts, d'autres utilisent des sources d'Ã©nergie atomique qui fournissent Ã  peu prÃ¨s la mÃªme puissance.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">La faible puissance de la source d'alimentation, la petite taille des plaques Ã©mettrices et la taille limitÃ©e des plaques rÃ©ceptrices sur Terre, la distance Ã©norme que le signal doit parcourir - tout cela nÃ©cessite l'utilisation de codes avec un niveau Ã©levÃ© de correction d'erreur pour construire un systÃ¨me de communication efficace.</font></font><br><br>   n- ,      .  ,  ,         , â€”  ,         ,    ,       .   ,         , nQ,        .   ,    ,      .  ,   ,       ,    .       n-  ,     M      .   ,    ,                 .      ,  â€”   ,              ,      . <br><br>      ,    ,          .         , ,   ,       ,      . ,        ,  ,      ,      .     ,   ,      -   ,         . <br><br>    .  ,    ,     ,      ,      ,      .  , ,   , ,   ,   ; ,    ,           .  ,      ,     . <br><br>       IQ,     ,    ,       .  , ,  ,   .   ,      ,   ,          ,   Â«Â»    (    ).    ,      ,    ,      .         ?   ,    ,      ?    !   ,        ,   . <br><br>  ,       ,    ,      ,   ,        .  ,     ,     ,     ,  .             ,    ,     .    ,       ,   - . <br><br>      ,       .   ,   ,     ,    !      ,   . <br><br> <i> ...</i> <br><br> <i>    ,     â€”       magisterludi2016@yandex.ru</i> <br><br> ,         â€” <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«The Dream Machine:   Â»</a> ) <br><br> <b> </b> ,    <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> ,     </a> . ( <i>  10 ,  20  </i> ) <br><br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text">  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">PrÃ©face</a> <br><ol><li>  Introduction Ã  l'art de faire des sciences et du gÃ©nie: apprendre Ã  apprendre (28 mars 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Traduction: Chapitre 1</a> </li><li>  Â«Fondements de la rÃ©volution numÃ©rique (discrÃ¨te)Â» (30 mars 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 2. Principes fondamentaux de la rÃ©volution numÃ©rique (discrÃ¨te)</a> </li><li>  Â«Histoire des ordinateurs - MatÃ©rielÂ» (31 mars 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 3.</a> Histoire des ordinateurs - MatÃ©riel </li><li>  Â«Histoire des ordinateurs - logicielsÂ» (4 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 4. Histoire des ordinateurs - logiciels</a> </li><li>  Histoire des ordinateurs - Applications (6 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 5. Histoire des ordinateurs - Application pratique</a> </li><li>  Â«Intelligence artificielle - Partie IÂ» (7 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 6. Intelligence artificielle - 1</a> </li><li>  Â«Intelligence artificielle - Partie IIÂ» (11 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 7. Intelligence artificielle - II</a> </li><li>  Â«Intelligence artificielle IIIÂ» (13 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 8. Intelligence artificielle-III</a> </li><li>  Â«Espace N-dimensionnelÂ» (14 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 9. Espace N-dimensionnel</a> </li><li>  Â«ThÃ©orie du codage - La reprÃ©sentation de l'information, partie IÂ» (18 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 10. ThÃ©orie du codage - I</a> </li><li>  Â«ThÃ©orie du codage - La reprÃ©sentation de l'information, partie IIÂ» (20 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 11. ThÃ©orie du codage - II</a> </li><li>  Â«Codes de correction d'erreursÂ» (21 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 12. Codes de correction d'erreurs</a> </li><li> Â«Information TheoryÂ» (April 25, 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="> 13.  </a> </li><li>  Filtres numÃ©riques, partie I (27 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 14. Filtres numÃ©riques - 1</a> </li><li>  Filtres numÃ©riques, partie II (28 avril 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 15. Filtres numÃ©riques - 2</a> </li><li>  Filtres numÃ©riques, partie III (2 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 16. Filtres numÃ©riques - 3</a> </li><li>  Filtres numÃ©riques, partie IV (4 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 17. Filtres numÃ©riques - IV</a> </li><li>  Â«Simulation, partie IÂ» (5 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 18. ModÃ©lisation - I</a> </li><li>  Â«Simulation, Partie IIÂ» (9 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 19. ModÃ©lisation - II</a> </li><li>  Â«Simulation, Partie IIIÂ» (11 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 20. ModÃ©lisation - III</a> </li><li>  Fibre optique (12 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 21. Fibre optique</a> </li><li>  Enseignement assistÃ© par ordinateur (16 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 22. Apprentissage</a> assistÃ© par ordinateur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">(CAI)</a> </li><li>  MathÃ©matiques (18 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 23. MathÃ©matiques</a> </li><li>  MÃ©canique quantique (19 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 24. MÃ©canique quantique</a> </li><li>  CrÃ©ativitÃ© (23 mai 1995).  Traduction: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 25. CrÃ©ativitÃ©</a> </li><li>  Â«ExpertsÂ» (25 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 26. Experts</a> </li><li>  Â«DonnÃ©es non fiablesÂ» (26 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 27. DonnÃ©es invalides</a> </li><li>  IngÃ©nierie des systÃ¨mes (30 mai 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 28. IngÃ©nierie des systÃ¨mes</a> </li><li>  Â«Vous obtenez ce que vous mesurezÂ» (1er juin 1995) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Chapitre 29.</a> Vous obtenez ce que vous mesurez </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Â«Comment savons-nous ce que nous savonsÂ»</a> (2 juin 1995) <i>traduit en tranches de 10 minutes</i> </li><li>  Hamming, Â«Vous et vos recherchesÂ» (6 juin 1995).  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Traduction: vous et votre travail</a> </li></ol><br>     ,     â€”       magisterludi2016@yandex.ru <br><br></div></div></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr422205/">https://habr.com/ru/post/fr422205/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr422195/index.html">L'utilisation d'ACS dans les mines</a></li>
<li><a href="../fr422197/index.html">Nous disons un mot sur le relais</a></li>
<li><a href="../fr422199/index.html">Security Week 33: par qui le moniteur oscille-t-il?</a></li>
<li><a href="../fr422201/index.html">Chine, laissez-moi radier?</a></li>
<li><a href="../fr422203/index.html">Clicker DIY</a></li>
<li><a href="../fr422207/index.html">Monstres aprÃ¨s les vacances: AMD Threadripper 2990WX 32-Core et 2950X 16-Core (partie 4)</a></li>
<li><a href="../fr422209/index.html">Monstres aprÃ¨s les vacances: AMD Threadripper 2990WX 32-Core et 2950X 16-Core (partie 5)</a></li>
<li><a href="../fr422211/index.html">Belle structure de composants dans le cloud Microsoft Azure</a></li>
<li><a href="../fr422213/index.html">Un jour sans JavaScript: qu'est-ce qui pourrait mal tourner?</a></li>
<li><a href="../fr422217/index.html">Pas vraiment sÃ©rieux concernant l'hÃ©bergement ou comment vÃ©rifier l'adÃ©quation de l'hÃ´te</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>