<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë™ üßëüèΩ‚Äçü§ù‚Äçüßëüèº üëçüèæ Escalado de bases de datos en sistemas altamente cargados üå∫ üö≤ üë®üèø‚Äçüç≥</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="En la √∫ltima reuni√≥n interna de Pyrus, hablamos sobre el almacenamiento distribuido moderno, y Maxim Nalsky, CEO y fundador de Pyrus, comparti√≥ su pri...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Escalado de bases de datos en sistemas altamente cargados</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/440306/"> En la √∫ltima reuni√≥n interna de Pyrus, hablamos sobre el almacenamiento distribuido moderno, y Maxim Nalsky, CEO y fundador de Pyrus, comparti√≥ su primera impresi√≥n de FoundationDB.  En este art√≠culo, hablamos sobre los matices t√©cnicos que enfrenta al elegir una tecnolog√≠a para escalar el almacenamiento de datos estructurados. <br><br>  Cuando el servicio no est√° disponible para los usuarios durante alg√∫n tiempo, es muy desagradable, pero a√∫n no mortal.  Pero perder los datos del cliente es absolutamente inaceptable.  Por lo tanto, evaluamos escrupulosamente cualquier tecnolog√≠a para almacenar datos de dos a tres docenas de par√°metros. <a name="habracut"></a>  Algunos de ellos dictan la carga actual en el servicio. <br><br><img src="https://habrastorage.org/webt/1c/p2/gm/1cp2gmh6pjentlkkocm1k2msgay.png">  <font color="#777777">Carga actual.</font>  <font color="#777777">Seleccionamos tecnolog√≠a teniendo en cuenta el crecimiento de estos indicadores.</font> <br><br><h2>  Arquitectura del servidor del cliente </h2><br>  El modelo cl√°sico de cliente-servidor es el ejemplo m√°s simple de un sistema distribuido.  Un servidor es un punto de sincronizaci√≥n; permite que varios clientes hagan algo juntos de manera coordinada. <br><br><img src="https://habrastorage.org/webt/jq/hy/ee/jqhyeeqxfebgrvhzvkwsrz58jhy.png">  <font color="#777777">Un esquema muy simplificado de interacci√≥n cliente-servidor.</font> <br><br>  ¬øQu√© es poco confiable en la arquitectura cliente-servidor?  Obviamente, el servidor puede fallar.  Y cuando el servidor falla, todos los clientes no pueden trabajar.  Para evitar esto, a la gente se le ocurri√≥ una conexi√≥n maestro-esclavo (que ahora es <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">pol√≠ticamente correcta llamada l√≠der-seguidor</a> ).  La conclusi√≥n es que hay dos servidores, todos los clientes se comunican con el principal, y en el segundo todos los datos simplemente se replican. <br><br><img src="https://habrastorage.org/webt/tk/bo/vu/tkbovuwxni03qjfpkx7m5iwmfie.png">  <font color="#777777">Arquitectura cliente-servidor con replicaci√≥n de datos a seguidores.</font> <br><br>  Est√° claro que este es un sistema m√°s confiable: si el servidor principal falla, entonces una copia de todos los datos est√° en el seguidor y se puede generar r√°pidamente. <br><br>  Es importante comprender c√≥mo funciona la replicaci√≥n.  Si es sincr√≥nico, la transacci√≥n debe almacenarse simult√°neamente en el l√≠der y en el seguidor, y esto puede ser lento.  Si la replicaci√≥n es as√≠ncrona, puede perder algunos de los datos despu√©s de una conmutaci√≥n por error. <br><br>  ¬øY qu√© pasar√° si el l√≠der cae de noche cuando todos est√°n durmiendo?  Hay datos sobre el seguidor, pero nadie le dijo que ahora es un l√≠der y que los clientes no se conectan con √©l.  Bien, dotemos al seguidor con la l√≥gica de que comienza a considerarse lo m√°s importante cuando se pierde la conexi√≥n con el l√≠der.  Entonces podemos obtener f√°cilmente un cerebro dividido, un conflicto cuando la conexi√≥n entre el l√≠der y el seguidor se rompe, y ambos piensan que son los principales.  Esto realmente sucede en muchos sistemas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">como RabbitMQ</a> , la tecnolog√≠a de colas m√°s popular de la actualidad. <br><br>  Para resolver estos problemas, organice la conmutaci√≥n por error autom√°tica: agregue un tercer servidor (testigo, testigo).  Asegura que solo tengamos un l√≠der.  Y si el l√≠der se cae, el seguidor se enciende autom√°ticamente con un tiempo de inactividad m√≠nimo, que puede reducirse a unos segundos.  Por supuesto, los clientes en este esquema deben conocer de antemano las direcciones del l√≠der y seguidor e implementar la l√≥gica de reconexi√≥n autom√°tica entre ellos. <br><br><img src="https://habrastorage.org/webt/n1/ve/aq/n1veaqz67xiw-ozn0cnsudwx7cw.png">  <font color="#777777">El testigo garantiza que solo hay un l√≠der.</font>  <font color="#777777">Si el l√≠der se cae, el seguidor se enciende autom√°ticamente.</font> <br><br>  Tal sistema ahora funciona con nosotros.  Hay una base de datos principal, una base de datos de repuesto, hay un testigo y s√≠, a veces venimos por la ma√±ana y vemos que el cambio ocurri√≥ por la noche. <br><br>  Pero este esquema tambi√©n tiene inconvenientes.  Imagine que est√° instalando service packs o actualizando el sistema operativo en un servidor l√≠der.  Antes de eso, cambiaste manualmente la carga en el seguidor y luego ... ¬°se cae!  Desastre, su servicio no est√° disponible.  ¬øQu√© hacer para protegerte de esto?  Agregue un tercer servidor de respaldo, otro seguidor.  El tres es una especie de n√∫mero m√°gico.  Si desea que el sistema funcione de manera confiable, dos servidores no son suficientes, necesita tres.  Uno para mantenimiento, el segundo cae, el tercero permanece. <br><br><img src="https://habrastorage.org/webt/57/et/ao/57etao8c03-skbgh4_evsna3vwu.png">  <font color="#777777">El tercer servidor proporciona un funcionamiento confiable si los dos primeros no est√°n disponibles.</font> <br><br>  Para resumir, la redundancia debe ser igual a dos.  Una redundancia de uno no es suficiente.  Por esta raz√≥n, en las matrices de discos, las personas comenzaron a usar el esquema RAID6 en lugar de RAID5, sobreviviendo la ca√≠da de dos discos a la vez. <br><br><h2>  Transacciones </h2><br>  Son bien conocidos cuatro requisitos b√°sicos de transacci√≥n: atomicidad, consistencia, aislamiento y durabilidad (Atomicidad, Consistencia, Aislamiento, Durabilidad - ACID). <br><br>  Cuando hablamos de bases de datos distribuidas, queremos decir que los datos deben ser escalados.  La lectura se escala muy bien: miles de transacciones pueden leer datos en paralelo sin ning√∫n problema.  Pero cuando otras transacciones escriben datos al mismo tiempo que la lectura, son posibles varios efectos indeseables.  Es muy f√°cil obtener una situaci√≥n en la que una transacci√≥n leer√° diferentes valores de los mismos registros.  Aqu√≠ hay algunos ejemplos. <br><br>  <b>Lecturas sucias.</b>  En la primera transacci√≥n, enviamos la misma solicitud dos veces: tome todos los usuarios cuyo ID = 1. Si la segunda transacci√≥n cambia esta l√≠nea y luego revierte, la base de datos no ver√° ning√∫n cambio por un lado, pero por otro la primera transacci√≥n leer√° diferentes valores de edad para Joe. <br><br><img src="https://habrastorage.org/webt/qj/zl/d6/qjzld6frtj0ogu9lpfoss5wva2k.png"><br><br>  <b>Lecturas no repetibles.</b>  Otro caso es si la transacci√≥n de escritura se complet√≥ con √©xito y la transacci√≥n de lectura recibi√≥ datos diferentes durante la ejecuci√≥n de la misma solicitud. <br><br><img src="https://habrastorage.org/webt/85/wd/ug/85wdugy_ypmk0cfxpxl1hy5pf-s.png"><br><br>  En el primer caso, el cliente ley√≥ datos que generalmente estaban ausentes en la base de datos.  En el segundo caso, el cliente lee ambas veces los datos de la base de datos, pero son diferentes, aunque la lectura se produce dentro de la misma transacci√≥n. <br><br>  <b>Las lecturas fantasmas son</b> cuando volvemos a leer un rango dentro de la misma transacci√≥n y obtenemos un conjunto diferente de l√≠neas.  En alg√∫n lugar en el medio, otra transacci√≥n ingres√≥ e insert√≥ o elimin√≥ registros. <br><br><img src="https://habrastorage.org/webt/k4/65/yq/k465yqehsgbjd0ertvg7poziaie.png"><br><br>  Para evitar estos efectos indeseables, los DBMS modernos implementan mecanismos de bloqueo (una transacci√≥n restringe el acceso a los datos con los que est√° trabajando actualmente para otras transacciones) o el control de versiones multiversion, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MVCC</a> (una transacci√≥n nunca cambia los datos grabados previamente y siempre crea una nueva versi√≥n). <br><br>  El est√°ndar ANSI / ISO SQL define 4 niveles de aislamiento para transacciones que afectan su grado de bloqueo mutuo.  Cuanto mayor sea el nivel de aislamiento, menos efectos indeseables.  El precio de esto es ralentizar la aplicaci√≥n (ya que las transacciones a menudo esperan desbloquear los datos que necesitan) y aumentar la probabilidad de puntos muertos. <br><br><img src="https://habrastorage.org/webt/0-/iy/4h/0-iy4hwem9b4-mkupvpetxgc9so.png"><br><br>  Lo m√°s divertido para un programador de aplicaciones es el nivel Serializable: no hay efectos indeseables y toda la complejidad de garantizar la integridad de los datos se transfiere al DBMS. <br><br>  Pensemos en la implementaci√≥n ingenua del nivel Serializable: con cada transacci√≥n, simplemente bloqueamos a todos los dem√°s.  Te√≥ricamente, cada transacci√≥n de escritura puede realizarse en 50 ¬µs (el tiempo de una operaci√≥n de escritura en discos SSD modernos).  Y queremos guardar datos en tres m√°quinas, ¬ørecuerdas?  Si est√°n en el mismo centro de datos, la grabaci√≥n durar√° de 1 a 3 ms.  Y si, por confiabilidad, se encuentran en diferentes ciudades, la grabaci√≥n puede tomar f√°cilmente 10-12 ms (el tiempo de viaje de un paquete de red de Mosc√∫ a San Petersburgo y viceversa).  Es decir, con una implementaci√≥n ingenua del nivel Serializable mediante grabaci√≥n secuencial, no podemos realizar m√°s de 100 transacciones por segundo.  ¬°Mientras que un SSD separado le permite realizar aproximadamente 20,000 operaciones de escritura por segundo! <br><br>  Conclusi√≥n: las transacciones de escritura deben realizarse en paralelo, y para escalarlas, necesita un buen mecanismo de resoluci√≥n de conflictos. <br><br><h2>  Sharding </h2><br>  ¬øQu√© hacer cuando los datos dejan de llegar a un servidor?  Hay dos mecanismos de zoom est√°ndar: <br><br><ul><li>  <b>Vertical</b> cuando solo agregamos memoria y discos a este servidor.  Esto tiene sus l√≠mites, en t√©rminos de la cantidad de n√∫cleos por procesador, la cantidad de procesadores y la cantidad de memoria. <br></li><li>  <b>Horizontal,</b> cuando usamos muchas m√°quinas y distribuimos datos entre ellas.  Los conjuntos de tales m√°quinas se denominan grupos.  Para colocar los datos en un cl√∫ster, deben estar fragmentados, es decir, para cada registro, determinar en qu√© servidor se ubicar√°. <br></li></ul><br>  Una clave de particionamiento es un par√°metro mediante el cual los datos se distribuyen entre servidores, por ejemplo, un identificador de cliente u organizaci√≥n. <br><br>  Imagine que necesita registrar datos sobre todos los habitantes de la Tierra en un grupo.  Como clave de fragmento, puede tomar, por ejemplo, el a√±o de nacimiento de la persona.  Entonces ser√°n suficientes 116 servidores (y cada a√±o ser√° necesario agregar un nuevo servidor).  O puede tomar como clave el pa√≠s donde vive la persona, luego necesitar√° aproximadamente 250 servidores.  A√∫n as√≠, la primera opci√≥n es preferible, porque la fecha de nacimiento de la persona no cambia y nunca necesitar√° transferir datos sobre √©l entre los servidores. <br><br><img src="https://habrastorage.org/webt/zr/8h/sz/zr8hszqcct-xf5q07gcow3cf7lu.png"><br><br>  En Pyrus, puede tomar una organizaci√≥n como clave de fragmentaci√≥n.  Pero son muy diferentes en tama√±o: hay un enorme Sovcombank (m√°s de 15 mil usuarios) y miles de peque√±as empresas.  Cuando asigna a una organizaci√≥n un servidor espec√≠fico, no sabe de antemano c√≥mo crecer√°.  Si la organizaci√≥n es grande y usa el servicio de manera activa, tarde o temprano sus datos dejar√°n de colocarse en un servidor y tendr√° que volver a compartirlos.  Y esto no es f√°cil si los datos son terabytes.  Imag√≠nese: un sistema cargado, las transacciones contin√∫an cada segundo, y en estas condiciones necesita mover datos de un lugar a otro.  No puede detener el sistema, ese volumen puede bombearse durante varias horas y los clientes comerciales no sobrevivir√°n a un tiempo de inactividad tan largo. <br><br>  Como clave de fragmentaci√≥n, es mejor elegir datos que rara vez cambian.  Sin embargo, lejos de ser siempre una tarea aplicada, esto es f√°cil de hacer. <br><br><h2>  Consenso en el cluster </h2><br>  Cuando hay muchas m√°quinas en el cl√∫ster y algunas pierden contacto con las otras, ¬øc√≥mo decidir qui√©n almacena la √∫ltima versi√≥n de los datos?  Simplemente asignar un servidor testigo no es suficiente, porque tambi√©n puede perder contacto con todo el cl√∫ster.  Adem√°s, en una situaci√≥n de cerebro dividido, varias m√°quinas pueden registrar diferentes versiones de los mismos datos, y usted necesita determinar de alguna manera cu√°l es la m√°s relevante.  Para resolver este problema, a las personas se les ocurrieron algoritmos de consenso.  Permiten que varias m√°quinas id√©nticas lleguen a un solo resultado en cualquier tema mediante votaci√≥n.  En 1989, se public√≥ el primer algoritmo de este tipo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Paxos</a> , y en 2014, los muchachos de Stanford idearon una <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">balsa</a> m√°s simple para implementar.  Estrictamente hablando, para que un grupo de servidores (2N + 1) llegue a un consenso, es suficiente que al mismo tiempo no tenga m√°s de N fallas.  Para sobrevivir a 2 fallas, el cl√∫ster debe tener al menos 5 servidores. <br><br><h2>  Escala relacional de DBMS </h2><br>  La mayor√≠a de las bases de datos a las que los desarrolladores est√°n acostumbrados a trabajar con soporte de √°lgebra relacional.  Los datos se almacenan en tablas y, en ocasiones, debe unir los datos de diferentes tablas mediante la operaci√≥n JOIN.  Considere un ejemplo de una base de datos y una consulta simple. <br><br><img src="https://habrastorage.org/webt/hv/fo/d-/hvfod-guiaz4-52ji4epjhizpaq.png"><br><br>  Suponga que A.id es una clave primaria con un √≠ndice agrupado.  Luego, el optimizador crear√° un plan que probablemente primero seleccionar√° los registros necesarios de la tabla A y luego tomar√° los enlaces apropiados a los registros de la tabla B desde un √≠ndice adecuado (A, B). El tiempo de ejecuci√≥n de esta consulta aumenta logar√≠tmicamente a partir del n√∫mero de registros en las tablas. <br><br>  Ahora imagine que los datos se distribuyen a trav√©s de cuatro servidores en el cl√∫ster y que necesita ejecutar la misma consulta: <br><br><img src="https://habrastorage.org/webt/kr/yw/45/kryw45daubuflf-r14zqswson_o.png"><br><br>  Si el DBMS no quiere ver todos los registros de todo el cl√∫ster, entonces probablemente intentar√° encontrar registros con A.id igual a 128, 129 o 130 y buscar los registros apropiados para ellos en la tabla B. Pero si A.id no es una clave de fragmento, entonces el DBMS por adelantado no se puede saber en qu√© servidor se encuentran los datos de la tabla A. De todos modos, tendr√° que ponerse en contacto con todos los servidores para averiguar si hay registros A.id adecuados para nuestra condici√≥n.  Luego, cada servidor puede hacer una UNI√ìN dentro de s√≠ mismo, pero esto no es suficiente.  Usted ve, necesitamos el registro en el nodo 2 en la muestra, pero no hay registro con A.id = 128?  Si los nodos 1 y 2 se unir√°n independientemente, entonces el resultado de la consulta estar√° incompleto; no recibiremos parte de los datos. <br><br>  Por lo tanto, para cumplir con esta solicitud, cada servidor debe recurrir a todos los dem√°s.  El tiempo de ejecuci√≥n crece de forma cuadr√°tica con el n√∫mero de servidores.  (Tiene suerte si puede fragmentar todas las tablas con la misma clave, entonces no necesita rastrear todos los servidores. Sin embargo, en la pr√°ctica esto no es realista: siempre habr√° consultas en las que la recuperaci√≥n no se base en la clave de fragmentaci√≥n). <br><br>  Por lo tanto, las operaciones de JOIN escalan fundamentalmente deficientemente y este es un problema fundamental del enfoque relacional. <br><br><h2>  Enfoque NoSQL </h2><br>  Las dificultades para escalar los DBMS cl√°sicos han llevado a las personas a crear bases de datos NoSQL que no tienen operaciones JOIN.  No se une, no hay problema.  Pero no hay propiedades ACID, pero no mencionaron esto en los materiales de marketing.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Artesanos</a> r√°pidamente <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">encontrados</a> que prueban la fuerza de varios sistemas distribuidos y <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">publican los resultados p√∫blicamente</a> .  Result√≥ que hay escenarios en los que el <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cl√∫ster Redis pierde el 45% de los datos almacenados, el</a> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">cl√∫ster RabbitMQ - 35% de los mensajes</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">MongoDB - 9% de los registros</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Cassandra - hasta el 5%</a> .  Y estamos hablando <b>de la p√©rdida despu√©s de que el cl√∫ster inform√≥ al cliente sobre el guardado exitoso.</b>  Por lo general, espera un mayor nivel de confiabilidad de la tecnolog√≠a elegida. <br><br>  Google ha desarrollado la base de datos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">Spanner</a> , que opera globalmente en todo el mundo.  Spanner garantiza propiedades ACID, serializaci√≥n y m√°s.  Tienen relojes at√≥micos en los centros de datos que proporcionan una hora precisa, y esto le permite crear un orden global de transacciones sin tener que reenviar paquetes de red entre continentes.  La idea de Spanner es que es mejor para los programadores abordar los problemas de rendimiento que surgen con una gran cantidad de transacciones que las muletas en torno a la falta de transacciones.  Sin embargo, Spanner es una tecnolog√≠a cerrada, no le conviene si por alguna raz√≥n no desea depender de un proveedor. <br><br>  Los nativos de Google desarrollaron un an√°logo de c√≥digo abierto de Spanner y lo llamaron CockroachDB ("cucaracha" en ingl√©s "cucaracha", que deber√≠a simbolizar la capacidad de supervivencia de la base de datos).  Sobre Habr√© <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">ya escribi√≥</a> sobre la falta de disponibilidad del producto para la producci√≥n, porque el cl√∫ster estaba perdiendo datos.  Decidimos revisar la nueva versi√≥n 2.0 y llegamos a una conclusi√≥n similar.  No perdimos los datos, pero algunas de las consultas m√°s simples se ejecutaron sin raz√≥n. <br><br><hr><br>  Como resultado, hoy existen bases de datos relacionales que se escalan bien solo verticalmente, lo cual es costoso.  Y hay soluciones NoSQL sin transacciones y sin garant√≠as ACID (si desea ACID, escriba muletas). <br><br>  ¬øC√≥mo hacer aplicaciones de misi√≥n cr√≠tica en las que los datos no caben en un servidor?  Aparecen nuevas soluciones en el mercado, y sobre una de ellas, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=es&amp;u=">FoundationDB</a> , le contaremos m√°s en el pr√≥ximo art√≠culo. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/440306/">https://habr.com/ru/post/440306/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../440296/index.html">6 aplicaciones para el IoT industrial</a></li>
<li><a href="../440298/index.html">Aplicaci√≥n de San Valent√≠n en Libgdx</a></li>
<li><a href="../440300/index.html">10 comandos de consola para ayudar a debatir el c√≥digo JavaScript como un PRO</a></li>
<li><a href="../440302/index.html">CRM: costo de √©xito, costo de error, costo de propiedad</a></li>
<li><a href="../440304/index.html">Interrupciones de dispositivos externos en un sistema x86. Parte 3. Configuraci√≥n del enrutamiento de interrupci√≥n en el conjunto de chips utilizando el ejemplo coreboot</a></li>
<li><a href="../440308/index.html">Divide y conquista, o escribe lentamente - lee r√°pidamente</a></li>
<li><a href="../440310/index.html">C√≥mo ense√±arle a una m√°quina a comprender facturas y extraer datos de ellas</a></li>
<li><a href="../440312/index.html">Hackquest 2018. Resultados y rese√±as. D√≠a 4-7</a></li>
<li><a href="../440314/index.html">Candidato de lanzamiento de JDK 12: Shenandoah, G1, JMH, Arm64. Los insectos en Swing contraatacan</a></li>
<li><a href="../440316/index.html">Distribuci√≥n uniforme de puntos en un tri√°ngulo.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>