<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üàπ üßòüèø üôÜüèΩ √úberblick √ºber die Sprachsynthese-Technologie üë®üèø‚Äçü§ù‚Äçüë®üèº ü§ûüèø üîÆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hallo allerseits! Mein Name ist Vlad und ich arbeite als Datenwissenschaftler im Tinkoff-Team f√ºr Sprachtechnologien, die in unserem Sprachassistenten...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>√úberblick √ºber die Sprachsynthese-Technologie</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/tinkoff/blog/474782/"><p>  Hallo allerseits!  Mein Name ist Vlad und ich arbeite als Datenwissenschaftler im Tinkoff-Team f√ºr Sprachtechnologien, die in unserem Sprachassistenten Oleg verwendet werden. </p><br><p>  In diesem Artikel m√∂chte ich einen kurzen √úberblick √ºber die in der Branche verwendeten Sprachsynthesetechnologien geben und die Erfahrungen unseres Teams beim Aufbau unserer eigenen Synthese-Engine teilen. </p><br><p><img src="https://habrastorage.org/webt/fc/3j/vs/fc3jvsr59z_90ojbvjushekmmm4.png" alt="Bild"></p><a name="habracut"></a><br><h3 id="sintez-rechi">  Sprachsynthese </h3><br><p>  Sprachsynthese ist die Erzeugung von Klang basierend auf Text.  Dieses Problem wird heute durch zwei Ans√§tze gel√∂st: </p><br><ul><li>  Einheitenauswahl [1] oder ein verketteter Ansatz.  Es basiert auf dem Verkleben von Audiofragmenten.  Seit den sp√§ten 90er Jahren gilt es seit langem als De-facto-Standard f√ºr die Entwicklung von Sprachsynthese-Engines.  Zum Beispiel k√∂nnte in Siri [2] eine Stimme gefunden werden, die mit der Ger√§teauswahlmethode erklingt. </li><li>  Parametrische Sprachsynthese [3], deren Kern darin besteht, ein Wahrscheinlichkeitsmodell zu erstellen, das die akustischen Eigenschaften eines Audiosignals f√ºr einen bestimmten Text vorhersagt. </li></ul><br><p>  Die Sprache von Einheitenauswahlmodellen ist von hoher Qualit√§t, geringer Variabilit√§t und erfordert eine gro√üe Datenmenge f√ºr das Training.  W√§hrend f√ºr das Training parametrischer Modelle eine viel geringere Datenmenge ben√∂tigt wird, erzeugen sie unterschiedlichere Intonationen, litten jedoch bis vor kurzem unter einer im Vergleich zum Ansatz der Ger√§teauswahl insgesamt eher schlechten Klangqualit√§t. </p><br><p>  Mit der Entwicklung von Deep-Learning-Technologien haben parametrische Synthesemodelle jedoch ein signifikantes Wachstum bei allen Qualit√§tsmetriken erzielt und k√∂nnen Sprache erzeugen, die praktisch nicht von menschlicher Sprache zu unterscheiden ist. </p><br><h3 id="metriki-kachestva">  Qualit√§tsmetriken </h3><br><p> Bevor Sie dar√ºber sprechen, welche Sprachsynthesemodelle besser sind, m√ºssen Sie die Qualit√§tsmetriken bestimmen, anhand derer die Algorithmen verglichen werden. </p><br><p>  Da derselbe Text auf unendlich viele Arten gelesen werden kann, gibt es a priori keinen richtigen Weg, um eine bestimmte Phrase auszusprechen.  Daher sind die Metriken f√ºr die Qualit√§t der Sprachsynthese oft subjektiv und h√§ngen von der Wahrnehmung des H√∂rers ab. </p><br><p>  Die Standardmetrik ist der MOS (Mean Opinion Score), eine durchschnittliche Bewertung der Nat√ºrlichkeit von Sprache, die von Bewertern f√ºr synthetisierte Audiodaten auf einer Skala von 1 bis 5 angegeben wird. Eine bedeutet v√∂llig unplausiblen Klang und f√ºnf bedeutet Sprache, die nicht vom Menschen zu unterscheiden ist.  Bei echten Personen liegen die Aufzeichnungen normalerweise bei etwa 4,5, und ein Wert √ºber 4 wird als ziemlich hoch angesehen. </p><br><h3 id="kak-rabotaet-sintez-rechi">  Wie Sprachsynthese funktioniert </h3><br><p>  Der erste Schritt zum Aufbau eines Sprachsynthesesystems ist das Sammeln von Daten f√ºr das Training.  In der Regel handelt es sich dabei um hochwertige Audioaufnahmen, auf denen der Ansager speziell ausgew√§hlte Phrasen liest.  Die ungef√§hre Gr√∂√üe des Datensatzes, der f√ºr Modelle zur Auswahl von Trainingseinheiten ben√∂tigt wird, betr√§gt 10 bis 20 Stunden reine Sprache [2], w√§hrend f√ºr Methoden zur Parametrisierung des neuronalen Netzwerks die obere Sch√§tzung etwa 25 Stunden betr√§gt [4, 5]. </p><br><p>  Wir diskutieren beide Synthesetechnologien. </p><br><h3 id="unit-selection">  Ger√§teauswahl </h3><br><p><img src="https://habrastorage.org/webt/9-/r7/dm/9-r7dmw2tieg5ypyjbt-lddxddc.png" alt="Bild"></p><br><p>  Typischerweise kann die aufgezeichnete Sprache des Sprechers nicht alle m√∂glichen F√§lle abdecken, in denen die Synthese verwendet wird.  Daher besteht der Kern der Methode darin, die gesamte Audiobasis in kleine Fragmente, sogenannte Units, zu teilen, die dann mit minimaler Nachbearbeitung zusammengeklebt werden.  Einheiten sind in der Regel minimale akustische Spracheinheiten, z. B. Telefone oder Diphons [2]. <br>  Der gesamte Generierungsprozess besteht aus zwei Phasen: dem NLP-Frontend, das f√ºr das Extrahieren der sprachlichen Darstellung des Texts verantwortlich ist, und dem Backend, das die Einheitsstraffunktion f√ºr die angegebenen sprachlichen Merkmale berechnet.  Das NLP-Frontend enth√§lt: </p><br><ol><li>  Die Aufgabe des Normalisierens des Textes besteht darin, alle Nichtbuchstaben (Zahlen, Prozentzeichen, W√§hrungen usw.) in ihre verbale Darstellung zu √ºbersetzen.  Zum Beispiel sollte "5%" in "5%" umgewandelt werden. </li><li>  Extrahieren von Sprachmerkmalen aus einem normalisierten Text: Phonemdarstellung, Stress, Wortarten usw. </li></ol><br><p>  In der Regel wird das NLP-Frontend mithilfe manuell vorgeschriebener Regeln f√ºr eine bestimmte Sprache implementiert. In letzter Zeit besteht jedoch eine zunehmende Tendenz zur Verwendung von Modellen f√ºr maschinelles Lernen [7]. </p><br><p>  Die vom Back-End-Subsystem gesch√§tzte Strafe ist die Summe der Zielkosten oder der Entsprechung der akustischen Darstellung der Einheit f√ºr ein bestimmtes Phonem und der Verkettungskosten, dh der Angemessenheit der Verbindung zweier benachbarter Einheiten.  Zur Bewertung der Feinfunktionen kann man die Regeln oder das bereits trainierte akustische Modell der parametrischen Synthese verwenden [2].  Die Auswahl der aus Sicht der oben definierten Strafen optimalsten Abfolge von Einheiten erfolgt mit dem Viterbi-Algorithmus [1]. </p><br><p>  Ungef√§hre Werte der Auswahlmodelle f√ºr MOS-Einheiten f√ºr die englische Sprache: 3,7-4,1 [2, 4, 5]. </p><br><p>  Vorteile des Einheitenselektionsansatzes: </p><br><ul><li>  Der nat√ºrliche Klang. </li><li>  High-Speed-Generierung. </li><li>  Geringe Modellgr√∂√üe - Dies erm√∂glicht es Ihnen, die Synthese direkt auf Ihrem Mobilger√§t zu verwenden. </li></ul><br><p>  Nachteile: </p><br><ul><li>  Die synthetisierte Sprache ist eint√∂nig, enth√§lt keine Emotionen. </li><li>  Charakteristische Klebegegenst√§nde. </li><li>  Es erfordert eine ausreichend gro√üe Trainingsbasis f√ºr Audiodaten, um alle Arten von Kontexten abzudecken. </li><li>  Im Prinzip kann kein Ton erzeugt werden, der nicht im Trainingssatz enthalten ist. </li></ul><br><h3 id="parametricheskiy-sintez-rechi">  Parametrische Sprachsynthese </h3><br><p>  Der parametrische Ansatz basiert auf der Idee, ein probabilistisches Modell zu erstellen, das die Verteilung der akustischen Merkmale eines bestimmten Textes sch√§tzt. <br>  Der Prozess der Sprachgenerierung in der parametrischen Synthese kann in vier Stufen unterteilt werden: </p><br><ol><li>  Das NLP-Frontend ist die gleiche Phase der Datenvorverarbeitung wie beim Unit-Selection-Ansatz, was zu einer Vielzahl kontextsensitiver sprachlicher Funktionen f√ºhrt. </li><li>  Durationsmodell zur Vorhersage der Phonemdauer. </li><li>  Ein akustisches Modell, das die Verteilung der akustischen Merkmale auf die sprachlichen wiederherstellt.  Akustische Merkmale umfassen Grundfrequenzwerte, spektrale Darstellung des Signals und so weiter. </li><li>  Ein Vocoder, der akustische Merkmale in eine Schallwelle √ºbersetzt. </li></ol><br><p>  F√ºr Trainingsdauer- und Akustikmodelle k√∂nnen versteckte Markov-Modelle [3], tiefe neuronale Netze oder deren wiederkehrende Variet√§ten [6] verwendet werden.  Ein herk√∂mmlicher Vocoder ist ein Algorithmus, der auf dem Quellfiltermodell [3] basiert und davon ausgeht, dass Sprache das Ergebnis der Anwendung eines linearen Rauschfilters auf das urspr√ºngliche Signal ist. <br>  Die allgemeine Sprachqualit√§t klassischer parametrischer Methoden ist aufgrund der Vielzahl unabh√§ngiger Annahmen √ºber die Struktur des Schallerzeugungsprozesses recht gering. </p><br><p>  Mit dem Aufkommen von Deep-Learning-Technologien ist es jedoch m√∂glich geworden, End-to-End-Modelle zu trainieren, die akustische Zeichen direkt per Buchstabe vorhersagen.  Beispielsweise geben die neuronalen Netze Tacotron [4] und Tacotron 2 [5] eine Folge von Buchstaben ein und geben das Kreidespektrogramm unter Verwendung des Algorithmus seq2seq [8] zur√ºck.  Somit werden die Schritte 1 bis 3 des klassischen Ansatzes durch ein einzelnes neuronales Netzwerk ersetzt.  Das folgende Diagramm zeigt die Architektur des Tacotron 2-Netzwerks, mit dem eine recht hohe Klangqualit√§t erzielt wird. </p><br><p><img src="https://habrastorage.org/webt/tv/8l/pc/tv8lpchvxw75yr3msdbhjaqscwc.jpeg" alt="Bild"></p><br><p>  Ein weiterer Faktor f√ºr eine signifikante Steigerung der Qualit√§t von synthetisierter Sprache war die Verwendung von Vocodern f√ºr neuronale Netze anstelle von Algorithmen f√ºr die digitale Signalverarbeitung. </p><br><p>  Der erste derartige Vocoder war das neuronale WaveNet-Netzwerk [9], das nacheinander schrittweise die Amplitude der Schallwelle vorhersagte. </p><br><p>  Aufgrund der Verwendung einer gro√üen Anzahl von Faltungsschichten mit L√ºcken, um mehr Kontext zu erfassen und Verbindungen in der Netzwerkarchitektur zu √ºberspringen, konnte eine Verbesserung des MOS um etwa 10% im Vergleich zu Einheitenauswahlmodellen erzielt werden.  Das folgende Diagramm zeigt die Architektur des WaveNet-Netzwerks. </p><br><p><img src="https://habrastorage.org/webt/lg/ei/df/lgeidfeylr_yu-u-ucmdsxi7xki.png" alt="Bild"></p><br><p>  Der Hauptnachteil von WaveNet ist die niedrige Geschwindigkeit, die mit einer seriellen Signalabtastschaltung verbunden ist.  Dieses Problem kann entweder durch technische Optimierung f√ºr eine bestimmte Eisenarchitektur oder durch Ersetzen des Stichprobenplans durch ein schnelleres gel√∂st werden. <br>  Beide Ans√§tze wurden in der Branche erfolgreich umgesetzt.  Die erste ist bei Tinkoff.ru und als Teil des zweiten Ansatzes f√ºhrte Google 2017 das Parallel WaveNet [10] -Netzwerk ein, dessen Errungenschaften im Google-Assistenten verwendet werden. </p><br><p>  Ungef√§hre MOS-Werte f√ºr neuronale Netzwerkmethoden: 4.4‚Äì4.5 [5, 11], dh synthetisierte Sprache unterscheidet sich praktisch nicht von menschlicher Sprache. </p><br><p>  Vorteile der parametrischen Synthese: </p><br><ul><li>  Nat√ºrlicher und weicher Klang bei Verwendung des End-to-End-Ansatzes. </li><li>  Gr√∂√üere Vielfalt in der Intonation. </li><li>  Verwenden Sie weniger Daten als Modelle zur Einheitenauswahl. </li></ul><br><p>  Nachteile: </p><br><ul><li>  Niedrige Geschwindigkeit im Vergleich zur Ger√§teauswahl. </li><li>  Gro√üe rechnerische Komplexit√§t. </li></ul><br><h3 id="kak-rabotaet-sintez-rechi-v-tinkoff">  So funktioniert die Tinkoff-Sprachsynthese </h3><br><p>  Wie aus dem Aufsatz hervorgeht, sind Methoden der parametrischen Sprachsynthese, die auf neuronalen Netzen basieren, dem Ansatz der Einheitenauswahl derzeit in ihrer Qualit√§t erheblich √ºberlegen und viel einfacher zu entwickeln.  Um unsere eigene Synthesemaschine zu bauen, haben wir sie verwendet. <br>  F√ºr Trainingsmodelle wurden ca. 25 Stunden reine Sprache eines professionellen Sprechers verwendet.  Lesetexte wurden speziell ausgew√§hlt, um die Phonetik der Umgangssprache m√∂glichst vollst√§ndig abzudecken.  Um die Intonationssynthese abwechslungsreicher zu gestalten, haben wir den Ansager au√üerdem gebeten, je nach Kontext Texte mit einem Ausdruck zu lesen. </p><br><p>  Die Architektur unserer L√∂sung sieht konzeptionell folgenderma√üen aus: </p><br><ul><li>  NLP-Frontend, das die Textnormalisierung des neuronalen Netzwerks und ein Modell zum Platzieren von Pausen und Spannungen umfasst. </li><li>  Tacotron 2 akzeptiert Buchstaben als Eingabe. </li><li>  Autoregressives WaveNet, das in Echtzeit auf der CPU arbeitet. </li></ul><br><p>  Dank dieser Architektur generiert unsere Engine Ausdruckssprache in hoher Qualit√§t in Echtzeit, erfordert keine Erstellung eines Phonemw√∂rterbuchs und erm√∂glicht die Steuerung von Belastungen in einzelnen W√∂rtern.  Beispiele f√ºr synthetisiertes Audio k√∂nnen durch Klicken auf den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Link angeh√∂rt werden</a> . </p><br><h3 id="ssylki">  Referenzen: </h3><br><p>  [1] AJ Hunt, AW Black.  Einheitenauswahl in einem verketteten Sprachsynthesesystem unter Verwendung einer gro√üen Sprachdatenbank, ICASSP, 1996. <br>  [2] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N. Huddleston, M. Hunt, J. Li, M. Neeracher, K. Prahallad, T. Raitio R. Rasipuram, G. Townsend, B. Williamson, D. Winarsky, Z. Wu, H. Zhang.  Siri On-Device Deep Learning-gesteuerte Ger√§teauswahl Text-to-Speech-System, Interspeech, 2017. <br>  [3] H. Zen, K. Tokuda, AW Black.  Statistische parametrische Sprachsynthese, Speech Communication, Vol. 3, No.  51, nein.  11, pp.  1039-1064, 2009. <br>  [4] Yuxuan Wang, RJ Skerry-Ryan, G√§nsebl√ºmchen Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous .  Tacotron: Auf dem Weg zur Ende-zu-Ende-Sprachsynthese. <br>  [5] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rif A. Saurous, Yannis Agiomyrgiannakis, Yonghui Wu.  Nat√ºrliche TTS-Synthese durch Konditionierung von WaveNet anhand von Mel-Spektrogramm-Vorhersagen. <br>  [6] Heiga Zen, Andrew Senior, Mike Schuster.  Statistische parametrische Sprachsynthese mit tiefen neuronalen Netzen. <br>  [7] Hao Zhang, Richard Sproat, Axel H. Ng, Felix Stahlberg, Xiaochang Peng, Kyle Gorman und Brian Roark.  Neuronale Modelle der Textnormalisierung f√ºr Sprachanwendungen. <br>  [8] Ilya Sutskever, Oriol Vinyals, Quoc V. Le.  Sequenz zu Sequenz Lernen mit neuronalen Netzen. <br>  [9] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior und Koray Kavukcuoglu.  WaveNet: Ein generatives Modell f√ºr Raw Audio. <br>  [10] Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman , Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters, Dan Belov, Demis Hassabis.  Parallel WaveNet: Schnelle High-Fidelity-Sprachsynthese. <br>  [11] Wei Ping Kainan Peng Jitong Chen.  ClariNet: Parallele Wellenerzeugung in End-to-End-Text-to-Speech. <br>  [12] Dario Rethage, Jordi Pons und Xavier Serra.  Ein Wavenet zum Entrauschen von Sprache. </p></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de474782/">https://habr.com/ru/post/de474782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de474762/index.html">Seminar: Hybride IT-L√∂sungen f√ºr Unternehmen. 14. November Moskau</a></li>
<li><a href="../de474768/index.html">Open Broadcast der Haupthalle HighLoad ++ 2019</a></li>
<li><a href="../de474770/index.html">So f√ºhren wir Abrechnungsregressionstests in SAP HCM durch</a></li>
<li><a href="../de474772/index.html">Ein Startup, das in 21 Tagen mit AI ein Heilmittel entwickelt hat</a></li>
<li><a href="../de474776/index.html">Allgemeine Theorie und Arch√§ologie der Virtualisierung x86</a></li>
<li><a href="../de474784/index.html">Arcade Stick Story</a></li>
<li><a href="../de474788/index.html">Organisation von Routen in Laravel</a></li>
<li><a href="../de474790/index.html">Verhandlungsgeschichten</a></li>
<li><a href="../de474792/index.html">6. bis 8. Dezember - Rosbank Tech.Madness Hackathon</a></li>
<li><a href="../de474796/index.html">Was ist das Internet der Dinge und wie k√∂nnen Unternehmen damit mehr verdienen?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>