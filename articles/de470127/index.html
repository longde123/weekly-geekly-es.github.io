<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèª‚Äç‚öñÔ∏è üê† üè® Komponist mit langem Kurzzeitged√§chtnis üíì üîΩ ü•Ç</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Musik automatisch komponieren 

 Fast unmittelbar nachdem ich Programmieren gelernt hatte, wollte ich eine Software erstellen, die Musik komponieren k...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Komponist mit langem Kurzzeitged√§chtnis</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/470127/"><h2>  Musik automatisch komponieren </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e91/f1e/b09/e91f1eb09e53ad458aaaeca5650b59aa.jpg" width="500"></div><br>  Fast unmittelbar nachdem ich Programmieren gelernt hatte, wollte ich eine Software erstellen, die Musik komponieren kann. <br><br>  Einige Jahre lang habe ich primitive Versuche unternommen, automatisch Musik f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Visions of Chaos</a> zu komponieren.  Grunds√§tzlich wurden einfache mathematische Formeln oder genetische Mutationen zuf√§lliger Notenfolgen verwendet.  Nachdem ich k√ºrzlich bescheidene Erfolge bei der Untersuchung und Anwendung von TensorFlow und neuronalen Netzen zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Suche nach zellularen Automaten</a> erzielt hatte, beschloss ich, neuronale Netze zum Erstellen von Musik zu verwenden. <br><br><h2>  Wie funktioniert es? </h2><br>  Der Komponist unterrichtet ein neuronales Netzwerk mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">langem</a> Kurzzeitged√§chtnis (LSTM).  LSTM-Netzwerke eignen sich gut zur Vorhersage der n√§chsten Datensequenzen.  Lesen Sie hier mehr √ºber LSTM. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0db/ce9/120/0dbce9120efb949af9aff64a8177c02f.png" title="LSTM - Klicken Sie hier f√ºr die Originalgr√∂√üe" width="500"></div><br>  Ein LSTM-Netzwerk empf√§ngt verschiedene Notenfolgen (in diesem Fall handelt es sich um einkanalige MIDI-Dateien).  Nach einer ausreichenden Ausbildung erh√§lt sie die M√∂glichkeit, Musik zu erstellen, die den Unterrichtsmaterialien √§hnelt. <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3f1/b32/98d/3f1b3298dededa70f51a95d42727aaf0.png" title="LSTM - Klicken Sie hier f√ºr die Originalgr√∂√üe" width="500"></div><br>  LSTM-Interna m√∂gen einsch√ºchternd wirken, aber die Verwendung von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">TensorFlow</a> und / oder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Keras</a> vereinfacht die Erstellung und das Experimentieren von LSTM erheblich. <br><br><h2>  Quellmusik f√ºr das Modelltraining </h2><br>  F√ºr solch einfache LSTM-Netzwerke reicht es f√ºr uns aus, dass die Quellkompositionen ein einzelner Midi-Kanal sind.  Hervorragend geeignet sind Midi-Dateien von Solo bis Klavier.  Ich fand Midi-Dateien mit Klaviersoli auf der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Classical Piano Midi-Seite</a> und in den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Dateien</a> und trainierte damit meine Modelle. <br><br>  Ich habe die Musik verschiedener Komponisten in separate Ordner gelegt.  Dank dessen kann der Benutzer Bach ausw√§hlen, auf die Schaltfl√§che Komponieren klicken und ein Lied erhalten, das (hoffentlich) wie Bach sein wird. <br><br><h2>  LSTM-Modell </h2><br>  Das Modell, auf dessen Grundlage ich den Code geschrieben habe, hat <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">dieses Beispiel des</a> Autors <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Sigur√∞ur Sk√∫li Sigurgeirsson ausgew√§hlt</a> , √ºber den er <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">hier</a> ausf√ºhrlicher schreibt. <br><br>  Ich habe das Skript lstm.py ausgef√ºhrt und nach 15 Stunden das Training abgeschlossen.  Als ich Predict.py ausf√ºhrte, um die MIDI-Dateien zu generieren, war ich entt√§uscht, weil sie aus einer sich wiederholenden Notiz bestanden.  Wenn ich das Training zweimal wiederholte, bekam ich die gleichen Ergebnisse. <br><br>  Quellmodell <br><br><pre><code class="python hljs">model = Sequential() model.add(CuDNNLSTM(<span class="hljs-number"><span class="hljs-number">512</span></span>,input_shape=(network_input.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], network_input.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]),return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(CuDNNLSTM(<span class="hljs-number"><span class="hljs-number">512</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(CuDNNLSTM(<span class="hljs-number"><span class="hljs-number">512</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">256</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)) model.add(Dense(n_vocab)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'rmsprop'</span></span>,metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>])</code> </pre> <br>  Nachdem ich dem Skript eine Grafikausgabe hinzugef√ºgt hatte, sah ich, warum mein Modell nicht funktionierte.  Die Genauigkeit ist im Laufe der Zeit nicht so gewachsen, wie es sollte.  Unten im Beitrag finden Sie gute Grafiken, die zeigen, wie das Arbeitsmodell aussehen soll. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/28c/2ef/19d/28c2ef19dee58e8235a240b536162ceb.jpg" title="Klicken Sie f√ºr Originalgr√∂√üe" width="500"></div><br>  Ich hatte keine Ahnung, warum es passiert ist.  aber dieses Modell aufgegeben und begann, die Einstellungen anzupassen. <br><br><pre> <code class="python hljs">model = Sequential() model.add(CuDNNLSTM(<span class="hljs-number"><span class="hljs-number">512</span></span>, input_shape=(network_input.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>], network_input.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]), return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) model.add(BatchNormalization()) model.add(CuDNNLSTM(<span class="hljs-number"><span class="hljs-number">256</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) model.add(BatchNormalization()) model.add(Dense(<span class="hljs-number"><span class="hljs-number">128</span></span>, activation=<span class="hljs-string"><span class="hljs-string">"relu"</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.2</span></span>)) model.add(BatchNormalization()) model.add(Dense(n_vocab)) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>,metrics=[<span class="hljs-string"><span class="hljs-string">"accuracy"</span></span>])</code> </pre> <br>  Es ist kompakter und hat weniger LSTM-Schichten.  Ich habe auch BatchNormalization hinzugef√ºgt und es im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">sentdex-Video gesehen</a> .  H√∂chstwahrscheinlich gibt es bessere Modelle, aber dieses hat in all meinen Trainingseinheiten ziemlich gut funktioniert. <br><br>  Beachten Sie, dass ich in beiden Modellen LSTM durch CuDNNLSTM ersetzt habe.  So habe ich dank der Verwendung von Cuda ein viel schnelleres LSTM-Training erreicht.  Wenn Sie keine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">GPU mit Cuda-Unterst√ºtzung haben</a> , m√ºssen Sie LSTM verwenden.  Vielen Dank an <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">sendtex</a> f√ºr diesen Tipp.  Das Erlernen neuer Modelle und das Erstellen von Midi-Dateien mit CuDNNLSTM ist etwa f√ºnfmal schneller. <br><br><h2>  Wie lange sollte das Modell trainiert werden? </h2><br>  Die √Ñhnlichkeit der Ergebnisse mit der Originalmusik h√§ngt von der Dauer des Trainings des Modells (der Anzahl der Epochen) ab.  Wenn es zu wenige Epochen gibt, enth√§lt das resultierende Ergebnis zu viele sich wiederholende Noten.  Wenn es zu viele Epochen gibt, wird das Modell umgeschult und einfach die Originalmusik kopiert. <br><br>  Aber woher wei√üt du, wie viele Epochen zu stoppen sind? <br><br>  Eine einfache L√∂sung besteht darin, einen R√ºckruf hinzuzuf√ºgen, in dem das Modell und das Genauigkeits- / Verlustdiagramm alle 50 Epochen eines Trainingslaufs in 500 Epochen gespeichert werden.  Dank dessen erhalten Sie nach Abschluss des Trainings Modelle und Grafiken in Schritten von 50 Epochen, die zeigen, wie das Training verl√§uft. <br><br>  Hier sind die Ergebnisse der Grafiken eines Laufs mit Speichern alle 50 Epochen, kombiniert in einem animierten GIF. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e93/357/716/e933577163ece7cc7e679edec71d0b90.gif"></div><br>  Dies sind die Grafiken, die wir sehen m√∂chten.  Die Verluste sollten sinken und niedrig bleiben.  Die Genauigkeit sollte steigen und nahe bei 100% bleiben. <br><br>  Es ist erforderlich, ein Modell mit der Anzahl der Epochen zu verwenden, die dem Zeitpunkt entspricht, an dem die Graphen zum ersten Mal an ihre Grenzen stie√üen.  F√ºr die oben gezeigte Grafik sind es 150 Epochen.  Wenn Sie √§ltere Modelle verwenden, werden diese umgeschult und f√ºhren h√∂chstwahrscheinlich zu einem einfachen Kopieren des Quellmaterials. <br><br>  Das diesen Spalten entsprechende Modell wurde auf Midi-Dateien der Kategorie Anthems trainiert, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">von hier stammen</a> . <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  MIDI-Daten in einem Modell mit 150 Epochen ausgeben. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Midi-Ausgabe in einem 100-Epochen-Modell. <br><br>  Selbst ein Modell mit 100 Epochen kann die Quelle zu genau kopieren.  Dies kann auf eine relativ kleine Auswahl von MIDI-Dateien f√ºr das Training zur√ºckzuf√ºhren sein.  Mit mehr Notizen ist das Lernen besser. <br><br><h2>  Wenn das Lernen schlecht wird </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/166/c17/97c/166c1797ceb0baa6894a982d55ce12ed.jpg" title="Klicken Sie f√ºr Originalgr√∂√üe" width="500"></div><br>  Das Bild oben zeigt ein Beispiel daf√ºr, was w√§hrend des Trainings passieren kann und was passiert.  Die Verluste werden reduziert und die Genauigkeit wie √ºblich erh√∂ht, aber pl√∂tzlich werden sie verr√ºckt.  In diesem Stadium kann es sich auch lohnen, anzuhalten.  Das Modell wird (zumindest nach meiner Erfahrung) nicht mehr richtig lernen.  In diesem Fall ist das gespeicherte Modell mit 100 Epochen immer noch zu zuf√§llig, und mit 150 Epochen ist der Moment des Modellfehlers bereits vergangen.  Jetzt bin ich alle 25 Epochen gerettet, um genau den idealen Moment des Modells mit dem besten Training zu finden, noch bevor sie umschult und abst√ºrzt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/486/65a/eea/48665aeea577e093ad0d944a4b4a3a35.jpg" title="Klicken Sie f√ºr Originalgr√∂√üe" width="500"></div><br>  Ein weiteres Beispiel f√ºr Lernfehler.  Dieses Modell wurde auf Midi-Dateien trainiert, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">von hier stammen</a> .  In diesem Fall hielt sie sich etwas l√§nger als 200 Epochen.  Bei Verwendung eines Modells mit 200 Epochen wird in Midi das folgende Ergebnis erzielt. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ohne die Erstellung von Diagrammen w√ºrden wir nie wissen, ob das Modell Probleme hat und wann sie auftreten, und k√∂nnten auch kein gutes Modell erhalten, ohne von vorne zu beginnen. <br><br><h2>  Andere Beispiele </h2><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell mit 75 Epochen, das auf den Kompositionen von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Chopin</a> basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus der 50er-√Ñra, das auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Midi-Dateien f√ºr Weihnachtskompositionen</a> basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein 100-Epochen-Modell basierend auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Midi-Dateien f√ºr Weihnachtskompositionen</a> .  Aber sind sie wirklich "Weihnachten"? <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus 300 Epochen, das auf Bach-Midi-Dateien basiert, die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">von hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">von hier stammen</a> . <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus 200 Epochen, das auf Balakirevs einziger Midi-Datei basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Modell mit 200 Epochen, basierend auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Debussy-</a> Kompositionen. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus 175 Jahren, das auf Mozarts Kompositionen basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell mit 100 Epochen basierend auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Schubert-</a> Kompositionen. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus der 200er-√Ñra, das auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Schumann-</a> Kompositionen basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus der 200er-√Ñra, das auf <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Tschaikowskys</a> Kompositionen basiert. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell mit 175 Epochen basierend auf Volksliedern. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Modell mit 100 Epochen basierend auf Schlafliedern. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein 100-√Ñra-Modell basierend auf Hochzeitsmusik. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/b21/d24/c00/b21d24c00c73a214714352d2c3b1f900.png" title="Klicken Sie hier, um Midi anzuh√∂ren" width="26"></a> <br><br>  Ein Modell aus 200 Epochen, das auf meinen eigenen Midi-Dateien basiert, die aus meinen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">YouTube-Video-</a> Soundtracks stammen.  Es kann ein bisschen umgeschult werden, weil es im Grunde Kopien meiner kurzen Ein- und Zweitakt-Midi-Dateien generiert. <br><br><h2>  Scores </h2><br>  Sobald Sie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Ihre</a> Midi-Dateien erhalten haben, k√∂nnen Sie sie mit Online-Tools wie <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">SolMiRe</a> in Partituren konvertieren.  Unten ist die Punktzahl der oben dargestellten Midi-Softology-Datei mit 200 Epochen aufgef√ºhrt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1e4/7fe/2cc/1e47fe2cc3cada4e27671bfe27397226.jpg" title="Klicken Sie f√ºr Originalgr√∂√üe" width="500"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ccd/2ff/dff/ccd2ffdff1344ef7ad97a210dba6b17e.jpg" title="Klicken Sie f√ºr Originalgr√∂√üe" width="500"></div><br><h2>  Wo kann ich den Komponisten testen? </h2><br>  LSTM Composer ist jetzt in <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">Visions of Chaos enthalten</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/29c/abd/6ac/29cabd6acecf1baf6fd9f99e531d1f9e.jpg" width="500"></div><br>  W√§hlen Sie einen Stil aus der Dropdown-Liste aus und klicken Sie auf Verfassen.  Wenn Sie das erforderliche Minimum an Python und TensorFlow installiert haben (siehe Anweisungen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=" rel="noopener">hier</a> ), erhalten Sie in wenigen Sekunden (wenn Sie eine schnelle GPU haben) eine neue maschinengeschriebene MIDI-Datei, die Sie anh√∂ren und f√ºr andere Zwecke verwenden k√∂nnen.  Kein Urheberrecht, keine Lizenzgeb√ºhren.  Wenn Ihnen die Ergebnisse nicht gefallen, k√∂nnen Sie erneut auf "Verfassen" klicken. Nach einigen Sekunden ist eine neue Komposition fertig. <br><br>  Die Ergebnisse k√∂nnen noch nicht als vollwertige Kompositionen betrachtet werden, aber sie enthalten interessante kleine Notenfolgen, mit denen ich in Zukunft Musik machen werde.  In dieser Hinsicht kann der LSTM-Komponist eine gute Inspirationsquelle f√ºr neue Kompositionen sein. <br><br><h2>  Python-Quelle </h2><br>  Unten finden Sie den Python-Skriptcode, den ich f√ºr das LSTM-Training und die Prognose verwendet habe.  Damit diese Skripte funktionieren, muss Visions of Chaos nicht installiert werden, und das Lernen und Generieren von MIDI funktioniert √ºber die Befehlszeile. <br><br>  Hier ist das Trainingsskript <code>lstm_music_train.py</code> <br><br><div class="spoiler">  <b class="spoiler_title">lstm_music_train.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># based on code from https://github.com/Skuldur/Classical-Piano-Composer # to use this script pass in; # 1. the directory with midi files # 2. the directory you want your models to be saved to # 3. the model filename prefix # 4. how many total epochs you want to train for # eg python -W ignore "C:\\LSTM Composer\\lstm_music_train.py" "C:\\LSTM Composer\\Bach\\" "C:\\LSTM Composer\\" "Bach" 500 import os import tensorflow as tf # ignore all info and warning messages os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) import glob import pickle import numpy import sys import keras import matplotlib.pyplot as plt from music21 import converter, instrument, note, chord from datetime import datetime from keras.models import Sequential from keras.layers.normalization import BatchNormalization from keras.layers import Dense from keras.layers import Dropout from keras.layers import CuDNNLSTM from keras.layers import Activation from keras.utils import np_utils from keras.callbacks import TensorBoard from shutil import copyfile # name of midi file directory, model directory, model file prefix, and epochs mididirectory = str(sys.argv[1]) modeldirectory = str(sys.argv[2]) modelfileprefix = str(sys.argv[3]) modelepochs = int(sys.argv[4]) notesfile = modeldirectory + modelfileprefix + '.notes' # callback to save model and plot stats every 25 epochs class CustomSaver(keras.callbacks.Callback): def __init__(self): self.epoch = 0 # This function is called when the training begins def on_train_begin(self, logs={}): # Initialize the lists for holding the logs, losses and accuracies self.losses = [] self.acc = [] self.logs = [] def on_epoch_end(self, epoch, logs={}): # Append the logs, losses and accuracies to the lists self.logs.append(logs) self.losses.append(logs.get('loss')) self.acc.append(logs.get('acc')*100) # save model and plt every 50 epochs if (epoch+1) % 25 == 0: sys.stdout.write("\nAuto-saving model and plot after {} epochs to ".format(epoch+1)+"\n"+modeldirectory + modelfileprefix + "_" + str(epoch+1).zfill(3) + ".model\n"+modeldirectory + modelfileprefix + "_" + str(epoch+1).zfill(3) + ".png\n\n") sys.stdout.flush() self.model.save(modeldirectory + modelfileprefix + '_' + str(epoch+1).zfill(3) + '.model') copyfile(notesfile,modeldirectory + modelfileprefix + '_' + str(epoch+1).zfill(3) + '.notes'); N = numpy.arange(0, len(self.losses)) # Plot train loss, train acc, val loss and val acc against epochs passed plt.figure() plt.subplots_adjust(hspace=0.7) plt.subplot(2, 1, 1) # plot loss values plt.plot(N, self.losses, label = "train_loss") plt.title("Loss [Epoch {}]".format(epoch+1)) plt.xlabel('Epoch') plt.ylabel('Loss') plt.subplot(2, 1, 2) # plot accuracy values plt.plot(N, self.acc, label = "train_acc") plt.title("Accuracy % [Epoch {}]".format(epoch+1)) plt.xlabel("Epoch") plt.ylabel("Accuracy %") plt.savefig(modeldirectory + modelfileprefix + '_' + str(epoch+1).zfill(3) + '.png') plt.close() # train the neural network def train_network(): sys.stdout.write("Reading midi files...\n\n") sys.stdout.flush() notes = get_notes() # get amount of pitch names n_vocab = len(set(notes)) sys.stdout.write("\nPreparing note sequences...\n") sys.stdout.flush() network_input, network_output = prepare_sequences(notes, n_vocab) sys.stdout.write("\nCreating CuDNNLSTM neural network model...\n") sys.stdout.flush() model = create_network(network_input, n_vocab) sys.stdout.write("\nTraining CuDNNLSTM neural network model...\n\n") sys.stdout.flush() train(model, network_input, network_output) # get all the notes and chords from the midi files def get_notes(): # remove existing data file if it exists if os.path.isfile(notesfile): os.remove(notesfile) notes = [] for file in glob.glob("{}/*.mid".format(mididirectory)): midi = converter.parse(file) sys.stdout.write("Parsing %s ...\n" % file) sys.stdout.flush() notes_to_parse = None try: # file has instrument parts s2 = instrument.partitionByInstrument(midi) notes_to_parse = s2.parts[0].recurse() except: # file has notes in a flat structure notes_to_parse = midi.flat.notes for element in notes_to_parse: if isinstance(element, note.Note): notes.append(str(element.pitch)) elif isinstance(element, chord.Chord): notes.append('.'.join(str(n) for n in element.normalOrder)) with open(notesfile,'wb') as filepath: pickle.dump(notes, filepath) return notes # prepare the sequences used by the neural network def prepare_sequences(notes, n_vocab): sequence_length = 100 # get all pitch names pitchnames = sorted(set(item for item in notes)) # create a dictionary to map pitches to integers note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) network_input = [] network_output = [] # create input sequences and the corresponding outputs for i in range(0, len(notes) - sequence_length, 1): sequence_in = notes[i:i + sequence_length] # needs to take into account if notes in midi file are less than required 100 ( mod ? ) sequence_out = notes[i + sequence_length] # needs to take into account if notes in midi file are less than required 100 ( mod ? ) network_input.append([note_to_int[char] for char in sequence_in]) network_output.append(note_to_int[sequence_out]) n_patterns = len(network_input) # reshape the input into a format compatible with CuDNNLSTM layers network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1)) # normalize input network_input = network_input / float(n_vocab) network_output = np_utils.to_categorical(network_output) return (network_input, network_output) # create the structure of the neural network def create_network(network_input, n_vocab): ''' """ create the structure of the neural network """ model = Sequential() model.add(CuDNNLSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True)) model.add(Dropout(0.3)) model.add(CuDNNLSTM(512, return_sequences=True)) model.add(Dropout(0.3)) model.add(CuDNNLSTM(512)) model.add(Dense(256)) model.add(Dropout(0.3)) model.add(Dense(n_vocab)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=["accuracy"]) ''' model = Sequential() model.add(CuDNNLSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True)) model.add(Dropout(0.2)) model.add(BatchNormalization()) model.add(CuDNNLSTM(256)) model.add(Dropout(0.2)) model.add(BatchNormalization()) model.add(Dense(128, activation="relu")) model.add(Dropout(0.2)) model.add(BatchNormalization()) model.add(Dense(n_vocab)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=["accuracy"]) return model # train the neural network def train(model, network_input, network_output): # saver = CustomSaver() # history = model.fit(network_input, network_output, epochs=modelepochs, batch_size=50, callbacks=[tensorboard]) history = model.fit(network_input, network_output, epochs=modelepochs, batch_size=50, callbacks=[CustomSaver()]) # evaluate the model print("\nModel evaluation at the end of training") train_acc = model.evaluate(network_input, network_output, verbose=0) print(model.metrics_names) print(train_acc) # save trained model model.save(modeldirectory + modelfileprefix + '_' + str(modelepochs) + '.model') # delete temp notes file os.remove(notesfile) if __name__ == '__main__': train_network()</span></span></code> </pre> </div></div><br>  Und hier ist das Generierungsskript midi <code>lstm_music_predict.py</code> : <br><br><div class="spoiler">  <b class="spoiler_title">lstm_music_predict.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># based on code from https://github.com/Skuldur/Classical-Piano-Composer # to use this script pass in; # 1. path to notes file # 2. path to model # 3. path to midi output # eg python -W ignore "C:\\LSTM Composer\\lstm_music_predict.py" "C:\\LSTM Composer\\Bach.notes" "C:\\LSTM Composer\\Bach.model" "C:\\LSTM Composer\\Bach.mid" # ignore all info and warning messages import os os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' import tensorflow as tf tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) import pickle import numpy import sys import keras.models from music21 import instrument, note, stream, chord from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Activation # name of weights filename notesfile = str(sys.argv[1]) modelfile = str(sys.argv[2]) midifile = str(sys.argv[3]) # generates a piano midi file def generate(): sys.stdout.write("Loading notes data file...\n\n") sys.stdout.flush() #load the notes used to train the model with open(notesfile, 'rb') as filepath: notes = pickle.load(filepath) sys.stdout.write("Getting pitch names...\n\n") sys.stdout.flush() # Get all pitch names pitchnames = sorted(set(item for item in notes)) # Get all pitch names n_vocab = len(set(notes)) sys.stdout.write("Preparing sequences...\n\n") sys.stdout.flush() network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab) sys.stdout.write("Loading LSTM neural network model...\n\n") sys.stdout.flush() model = create_network(normalized_input, n_vocab) sys.stdout.write("Generating note sequence...\n\n") sys.stdout.flush() prediction_output = generate_notes(model, network_input, pitchnames, n_vocab) sys.stdout.write("\nCreating MIDI file...\n\n") sys.stdout.flush() create_midi(prediction_output) # prepare the sequences used by the neural network def prepare_sequences(notes, pitchnames, n_vocab): # map between notes and integers and back note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) sequence_length = 100 network_input = [] output = [] for i in range(0, len(notes) - sequence_length, 1): sequence_in = notes[i:i + sequence_length] sequence_out = notes[i + sequence_length] network_input.append([note_to_int[char] for char in sequence_in]) output.append(note_to_int[sequence_out]) n_patterns = len(network_input) # reshape the input into a format compatible with LSTM layers normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1)) # normalize input normalized_input = normalized_input / float(n_vocab) return (network_input, normalized_input) # create the structure of the neural network def create_network(network_input, n_vocab): model = keras.models.load_model(modelfile) return model # generate notes from the neural network based on a sequence of notes def generate_notes(model, network_input, pitchnames, n_vocab): # pick a random sequence from the input as a starting point for the prediction start = numpy.random.randint(0, len(network_input)-1) int_to_note = dict((number, note) for number, note in enumerate(pitchnames)) pattern = network_input[start] prediction_output = [] # generate 500 notes for note_index in range(500): prediction_input = numpy.reshape(pattern, (1, len(pattern), 1)) prediction_input = prediction_input / float(n_vocab) prediction = model.predict(prediction_input, verbose=0) index = numpy.argmax(prediction) result = int_to_note[index] prediction_output.append(result) pattern.append(index) pattern = pattern[1:len(pattern)] if (note_index + 1) % 50 == 0: sys.stdout.write("{} out of 500 notes generated\n".format(note_index+1)) sys.stdout.flush() return prediction_output # convert the output from the prediction to notes and create a midi file from the notes def create_midi(prediction_output): offset = 0 output_notes = [] # create note and chord objects based on the values generated by the model for pattern in prediction_output: # pattern is a chord if ('.' in pattern) or pattern.isdigit(): notes_in_chord = pattern.split('.') notes = [] for current_note in notes_in_chord: new_note = note.Note(int(current_note)) new_note.storedInstrument = instrument.Piano() notes.append(new_note) new_chord = chord.Chord(notes) new_chord.offset = offset output_notes.append(new_chord) # pattern is a note else: new_note = note.Note(pattern) new_note.offset = offset new_note.storedInstrument = instrument.Piano() output_notes.append(new_note) # increase offset each iteration so that notes do not stack offset += 0.5 midi_stream = stream.Stream(output_notes) midi_stream.write('midi', fp=midifile) if __name__ == '__main__': generate()</span></span></code> </pre> </div></div><br><h2>  Modelldateigr√∂√üen </h2><br>  Der Nachteil der Einbeziehung neuronaler Netze in Visions of Chaos ist die Gr√∂√üe der Dateien.  Wenn die Generierung des Modells schneller w√§re, w√ºrde ich einfach eine Schaltfl√§che hinzuf√ºgen, damit der Endbenutzer die Modelle selbst trainieren kann.  Da einige der Schulungen f√ºr viele Modelle mehrere Tage dauern k√∂nnen, ist dies nicht besonders praktisch.  Es schien mir, dass es besser ist, das gesamte Training und Testen selbst durchzuf√ºhren und nur die besten Arbeitsmodelle hinzuzuf√ºgen.  Dies bedeutet auch, dass der Endbenutzer nur einen Knopf dr√ºcken muss und trainierte Modelle musikalische Kompositionen erstellen. <br><br>  Jedes der Modelle hat eine Gr√∂√üe von 22 Megabyte.  Unter den Bedingungen des modernen Internets ist dies nicht so sehr, aber im Laufe der Jahre der Entwicklung hat Visions of Chaos allm√§hlich an Gr√∂√üe zugenommen und ist erst k√ºrzlich pl√∂tzlich von 70 auf 91 MB gestiegen (aufgrund des Suchmodells f√ºr zellulare Automaten).  Daher habe ich bisher nur ein Modell zum Hauptinstallationsprogramm von Visions of Chaos hinzugef√ºgt.  F√ºr Benutzer, die mehr wollen, habe ich einen Link zu weiteren 1 GB Modellen gepostet.  Sie k√∂nnen auch das obige Skript verwenden, um ihre eigenen Modelle basierend auf ihren MIDI-Dateien zu erstellen. <br><br><h2>  Was weiter? </h2><br>  In dieser Phase ist der LSTM-Komponist das einfachste Beispiel f√ºr die Verwendung neuronaler Netze zum Komponieren von Musik. <br><br>  Ich habe bereits andere Musikkomponisten in neuronalen Netzen gefunden, mit denen ich in Zukunft experimentieren werde. Sie k√∂nnen also erwarten, dass es in Visions of Chaos neue M√∂glichkeiten gibt, automatisch Musik zu komponieren. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de470127/">https://habr.com/ru/post/de470127/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de470113/index.html">Wie die In-Memory-Technologie Business Intelligence ver√§ndert hat</a></li>
<li><a href="../de470117/index.html">Vorbereitungen f√ºr das Kombinieren</a></li>
<li><a href="../de470121/index.html">Firmenprogrammierschulen oder Einstieg in die IT</a></li>
<li><a href="../de470123/index.html">Yandex.Geldfinanzfalle</a></li>
<li><a href="../de470125/index.html">Beurteilen Sie den Code eines anderen nicht streng</a></li>
<li><a href="../de470129/index.html">Deklarative Speicherverwaltung</a></li>
<li><a href="../de470133/index.html">So sammeln Sie mit Prometheus Metriken, die nicht durch Zeitangaben verzerrt sind</a></li>
<li><a href="../de470135/index.html">Eine interaktive Webanwendung ohne Programmierung? Einfach! Mavo in deinen Armen</a></li>
<li><a href="../de470139/index.html">2 Life Hacks: Alternativen zur klassischen Suche in Microsoft SQL Server</a></li>
<li><a href="../de470145/index.html">‚ÄûVorsicht, FAS!‚Äú: Warum ist ein milit√§risches Ticket in der Werbung gef√§hrlich, warum ist es wichtig, Mathe zu kennen und ob immer die blo√üe Wahrheit ben√∂tigt wird</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>