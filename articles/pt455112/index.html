<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üßò ‚úèÔ∏è ‚ùå Bitrix24: ‚ÄúLevantado rapidamente n√£o √© considerado como tendo ca√≠do‚Äù üëû üë©üèΩ‚Äçüíº ‚è±Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At√© o momento, o servi√ßo Bitrix24 n√£o possui centenas de gigabits de tr√°fego, n√£o h√° uma grande frota de servidores (embora existam, √© claro, muitos e...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Bitrix24: ‚ÄúLevantado rapidamente n√£o √© considerado como tendo ca√≠do‚Äù</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/itsumma/blog/455112/">  At√© o momento, o servi√ßo Bitrix24 n√£o possui centenas de gigabits de tr√°fego, n√£o h√° uma grande frota de servidores (embora existam, √© claro, muitos existentes).  Mas para muitos clientes, √© a principal ferramenta para trabalhar na empresa, √© um aplicativo essencial para os neg√≥cios.  Portanto, caindo - bem, de jeito nenhum.  Mas e se a queda acontecesse, mas o servi√ßo "se rebelasse" t√£o rapidamente que ningu√©m notasse nada?  E como voc√™ consegue implementar o failover sem perder a qualidade do trabalho e o n√∫mero de clientes?  Alexander Demidov, diretor de servi√ßos em nuvem Bitrix24, contou ao nosso blog sobre como o sistema de backup evoluiu ao longo dos 7 anos de exist√™ncia do produto. <br><br><img src="https://habrastorage.org/webt/xp/iz/9_/xpiz9_ri1tdppyelfb8nu9tzeg8.jpeg"><br><a name="habracut"></a><br>  ‚ÄúNa forma de SaaS, lan√ßamos o Bitrix24 h√° 7 anos.  A principal dificuldade, provavelmente, foi a seguinte: antes de ser lan√ßado em p√∫blico na forma de SaaS, este produto existia simplesmente no formato de uma solu√ß√£o in a box.  Os clientes compraram de n√≥s, os colocaram em seus servidores, criaram um portal corporativo - uma solu√ß√£o comum para comunica√ß√£o com funcion√°rios, armazenamento de arquivos, gerenciamento de tarefas, CRM, s√≥ isso.  E at√© 2012, decidimos que quer√≠amos lan√ß√°-lo como um SaaS, administrando-o n√≥s mesmos, fornecendo toler√¢ncia a falhas e confiabilidade.  Adquirimos experi√™ncia no processo, porque at√© ent√£o simplesmente n√£o possu√≠amos - √©ramos apenas fabricantes de software, n√£o fornecedores de servi√ßos. <br><br>  Ao iniciar o servi√ßo, entendemos que o mais importante √© garantir a toler√¢ncia a falhas, a confiabilidade e a disponibilidade constante do servi√ßo, porque se voc√™ tem um site regular simples, uma loja, por exemplo, e ele caiu de voc√™ e fica uma hora - apenas voc√™ sofre, voc√™ perde pedidos , voc√™ perde clientes, mas para o seu cliente - para ele, isso n√£o √© muito cr√≠tico.  Ele ficou chateado, √© claro, mas foi comprar em outro site.  E se esse √© um aplicativo ao qual todos trabalham dentro da empresa, comunica√ß√µes e decis√µes, o mais importante √© conquistar a confian√ßa dos usu√°rios, ou seja, n√£o decepcion√°-los e n√£o cair.  Porque todo o trabalho pode melhorar se algo dentro n√£o funcionar. <br><br><h4>  Bitrix.24 como SaaS </h4><br>  O primeiro prot√≥tipo que montamos um ano antes do lan√ßamento p√∫blico, em 2011.  Reunidos em cerca de uma semana, pareciam retorcidos - ele estava mesmo trabalhando.  Ou seja, foi poss√≠vel entrar no formul√°rio, inserir o nome do portal ali, um novo portal se desenrolando, uma base de usu√°rios sendo configurada.  Examinamos, avaliamos o produto em princ√≠pio, desligamos e finalizamos um ano depois.  Como t√≠nhamos uma grande tarefa: n√£o quer√≠amos criar duas bases de c√≥digo diferentes, n√£o quer√≠amos oferecer suporte a um produto em caixa separado, solu√ß√µes em nuvem separadamente - quer√≠amos fazer tudo isso dentro do mesmo c√≥digo. <br><br><img src="https://habrastorage.org/webt/lw/zu/fe/lwzufe1w3lchcx7iqtwwh04gqfu.jpeg"><br><br>  Uma aplica√ß√£o web t√≠pica da √©poca √© um servidor no qual algum c√≥digo php est√° sendo executado, a base do mysql, os arquivos est√£o sendo baixados, os documentos e as fotos s√£o colocados no pai do upload - bem, tudo funciona.  Infelizmente, √© imposs√≠vel executar um servi√ßo da Web criticamente sustent√°vel nisso.  O cache distribu√≠do n√£o √© suportado l√°, a replica√ß√£o de banco de dados n√£o √© suportada. <br><br>  Formulamos os requisitos: essa capacidade de estar localizado em diferentes locais, para suportar a replica√ß√£o, idealmente para estar localizado em diferentes data centers distribu√≠dos geograficamente.  Separe a l√≥gica do produto e, de fato, o armazenamento de dados.  Dinamicamente ser capaz de escalar de acordo com a carga, geralmente faz a est√°tica.  A partir dessas considera√ß√µes, de fato, havia requisitos para o produto, que acabamos de desenvolver durante o ano.  Durante esse per√≠odo, em uma plataforma que acabou sendo unificada - para solu√ß√µes in a box, para nosso pr√≥prio servi√ßo - apoiamos o que precis√°vamos.  Suporte para replica√ß√£o mysql no n√≠vel do produto em si: ou seja, o desenvolvedor que escreve o c√≥digo n√£o pensa em como suas solicita√ß√µes ser√£o distribu√≠das, ele usa nossa API e podemos distribuir corretamente solicita√ß√µes de grava√ß√£o e leitura entre mestres e escravos. <br><br>  Fornecemos suporte em n√≠vel de produto para v√°rios armazenamentos de objetos na nuvem: armazenamento do google, amazon s3, - plus, suporte para pilha aberta r√°pida.  Portanto, era conveniente tanto para n√≥s como um servi√ßo quanto para desenvolvedores que trabalham com uma solu√ß√£o in a box: se eles apenas usarem nossa API para o trabalho, n√£o pensam onde o arquivo ser√° salvo, localmente no sistema de arquivos ou no armazenamento de arquivo de objeto . <br><br>  Como resultado, decidimos imediatamente que reservar√≠amos no n√≠vel de um data center inteiro.  Em 2012, lan√ßamos completamente o Amazon AWS, porque j√° t√≠nhamos experi√™ncia com essa plataforma - nosso pr√≥prio site estava hospedado l√°.  Ficamos atra√≠dos pelo fato de que em cada regi√£o da Amaz√¥nia existem v√°rias zonas de acesso - de fato (em sua terminologia) v√°rios datacenters mais ou menos independentes entre si e que nos permitem reservar no n√≠vel de um datacenter inteiro: se repentinamente falha, os bancos de dados mestre-mestre s√£o replicados, os servidores de aplicativos da web s√£o reservados e a est√°tica √© movida para o armazenamento de objeto s3.  A carga √© equilibrada - naquele momento, o cotovelo da Amaz√¥nia, mas um pouco mais tarde chegamos a nossos pr√≥prios balanceadores, porque precis√°vamos de uma l√≥gica mais complexa. <br><br><h4>  O que eles queriam, eles conseguiram ... </h4><br>  Todas as coisas b√°sicas que quer√≠amos fornecer - a toler√¢ncia a falhas dos pr√≥prios servidores, aplicativos da web, bancos de dados - tudo funcionou bem.  O cen√°rio mais simples: se alguns aplicativos da Web falharem, tudo ser√° simples - eles ser√£o desligados da balan√ßa. <br><br><img src="https://habrastorage.org/webt/0y/vn/7h/0yvn7ho3syy9rpwkz8wo7govryu.jpeg"><br><br>  O balanceador de m√°quinas (na √©poca era um cotovelo da Amaz√¥nia) que causou um acidente na pr√≥pria m√°quina, marcado como n√£o √≠ntegro, desativou a distribui√ß√£o de carga neles.  O dimensionamento autom√°tico da Amaz√¥nia funcionou: quando a carga aumentou, novos carros foram adicionados ao grupo de dimensionamento autom√°tico, a carga foi distribu√≠da para carros novos - tudo estava bem.  Com nossos balanceadores, a l√≥gica √© praticamente a mesma: se algo acontecer com o servidor de aplicativos, removeremos solicita√ß√µes dele, jogaremos essas m√°quinas fora, iniciaremos novas e continuaremos trabalhando.  O esquema para todos esses anos mudou um pouco, mas continua funcionando: √© simples, compreens√≠vel e n√£o h√° dificuldades com isso. <br><br>  Trabalhamos em todo o mundo, o pico de carga dos clientes √© completamente diferente e, de uma maneira boa, devemos ser capazes de executar determinados trabalhos de manuten√ß√£o com qualquer componente do sistema a qualquer momento - invisivelmente para os clientes.  Portanto, temos a oportunidade de desligar o banco de dados do trabalho, redistribuindo a carga no segundo data center. <br><br>  Como tudo isso funciona?  - Mudamos o tr√°fego para um datacenter em funcionamento - se for um acidente no datacenter, ent√£o completamente, se √© o trabalho planejado com qualquer base, ent√£o fazemos parte do tr√°fego que atende esses clientes, mudamos para o segundo datacenter e para replica√ß√£o.  Se voc√™ precisar de novas m√°quinas para aplicativos da Web, √† medida que a carga no segundo data center aumentar, elas ser√£o iniciadas automaticamente.  Terminamos o trabalho, a replica√ß√£o √© restaurada e retornamos toda a carga.  Se precisarmos espelhar algum trabalho no segundo controlador de dom√≠nio, por exemplo, instalar atualiza√ß√µes do sistema ou alterar as configura√ß√µes no segundo banco de dados, em geral, repetimos a mesma coisa, da outra maneira.  E se isso for um acidente, fazemos tudo de maneira trivial: no sistema de monitoramento, usamos o mecanismo de manipuladores de eventos.  Se v√°rias verifica√ß√µes funcionarem para n√≥s e o status for cr√≠tico, esse manipulador ser√° iniciado, um manipulador que pode executar essa ou aquela l√≥gica.  Para cada banco de dados, registramos qual servidor possui failover e onde voc√™ precisa trocar o tr√°fego, se n√£o estiver dispon√≠vel.  N√≥s - como ele se desenvolveu historicamente - usamos de uma forma ou de outra nagios ou qualquer um de seus garfos.  Em princ√≠pio, mecanismos semelhantes existem em quase qualquer sistema de monitoramento; ainda n√£o estamos usando algo mais complicado, mas talvez um dia o usemos.  Agora, o monitoramento √© acionado pela inacessibilidade e tem a capacidade de mudar alguma coisa. <br><br><h4>  Reservamos tudo? </h4><br>  Temos muitos clientes dos EUA, muitos clientes da Europa, muitos clientes mais pr√≥ximos do leste - Jap√£o, Cingapura e assim por diante.  Claro, uma enorme propor√ß√£o de clientes na R√∫ssia.  Ou seja, o trabalho est√° longe de estar em uma regi√£o.  Os usu√°rios desejam uma resposta r√°pida, h√° requisitos para a observa√ß√£o de v√°rias leis locais e, em cada regi√£o, reservamos dois data centers, al√©m de alguns servi√ßos adicionais que, novamente, s√£o convenientes para serem colocados em uma regi√£o - para clientes que est√£o nessa regi√£o. trabalho regional.  Manipuladores REST, servidores de autoriza√ß√£o, eles s√£o menos cr√≠ticos para o cliente como um todo. Voc√™ pode alternar entre eles com um pequeno atraso aceit√°vel, mas n√£o deseja inventar bicicletas, como monitor√°-las e o que fazer com elas.  Portanto, ao m√°ximo, estamos tentando usar as solu√ß√µes existentes, e n√£o desenvolver alguma compet√™ncia em produtos adicionais.  E em algum lugar, usamos trivialmente a comuta√ß√£o no n√≠vel de DNS e determinamos a vitalidade do servi√ßo com o mesmo DNS.  A Amazon possui um servi√ßo Route 53, mas n√£o √© apenas um DNS para o qual voc√™ pode gravar tudo, √© muito mais flex√≠vel e conveniente.  Por meio dele, voc√™ pode criar servi√ßos de distribui√ß√£o geogr√°fica com geolocaliza√ß√µes, quando us√°-lo para determinar de onde o cliente veio e fornecer a eles certos registros - com ele, voc√™ pode construir arquiteturas de failover.  As mesmas verifica√ß√µes de sa√∫de s√£o configuradas no pr√≥prio Route 53, voc√™ especifica pontos de extremidade monitorados, define m√©tricas e especifica quais protocolos determinam a vitalidade do servi√ßo - tcp, http, https;  defina a frequ√™ncia das verifica√ß√µes que determinam se o servi√ßo est√° ativo ou n√£o.  E no pr√≥prio DNS voc√™ prescreve o que ser√° prim√°rio, o que ser√° secund√°rio, para onde mudar se a verifica√ß√£o de sa√∫de dentro da rota 53 for acionada. Tudo isso pode ser feito com outras ferramentas, mas o que √© mais conveniente - uma vez configurado e depois n√£o pensamos em como fazemos verifica√ß√µes, como trocamos: tudo funciona por si s√≥. <br><br>  <b>O primeiro ‚Äúmas‚Äù</b> : como e como reservar a pr√≥pria rota 53?  Isso acontece se algo acontecer com ele?  Felizmente, nunca pisamos neste rake, mas novamente, na minha frente, terei uma hist√≥ria de por que pensamos que ainda precisamos reservar.  Aqui colocamos a palha com anteced√™ncia.  V√°rias vezes ao dia, descarregamos completamente todas as zonas que temos na rota 53.  A API da Amazon permite envi√°-los com seguran√ßa para JSON, e criamos v√°rios servidores redundantes onde os convertemos, carregamos na forma de configura√ß√µes e, grosso modo, temos uma configura√ß√£o de backup.  Nesse caso, podemos implant√°-lo manualmente rapidamente, n√£o perderemos os dados das configura√ß√µes de DNS. <br><br>  <b>O segundo ‚Äúmas‚Äù</b> : o que n√£o est√° reservado nesta figura?  O balanceador ele mesmo!  Nossa distribui√ß√£o de clientes por regi√£o √© muito simples.  Temos dom√≠nios bitrix24.ru, bitrix24.com, .de - agora existem 13 dom√≠nios diferentes que funcionam em zonas muito diferentes.  Chegamos ao seguinte: cada regi√£o tem seus pr√≥prios balanceadores.  √â mais conveniente distribuir por regi√£o, dependendo de onde est√° o pico de carga na rede.  Se houver uma falha no n√≠vel de qualquer balanceador, ele ser√° simplesmente desativado e removido do DNS.  Se ocorrer um problema com um grupo de balanceadores, eles ser√£o reservados em outros sites, e a altern√¢ncia entre eles ser√° feita usando a mesma rota53, porque, devido a um ttl curto, a altern√¢ncia ocorre por no m√°ximo 2, 3, 5 minutos. <br><br>  <b>O terceiro ‚Äúmas‚Äù</b> : o que ainda n√£o foi reservado?  S3, certo.  Colocando os arquivos armazenados pelos usu√°rios no s3, acreditamos sinceramente que era uma armadura e n√£o havia necessidade de reservar nada l√°.  Mas a hist√≥ria mostra o que acontece de maneira diferente.  Em geral, a Amazon descreve o S3 como um servi√ßo fundamental, porque o pr√≥prio Amazon usa o S3 para armazenar imagens de m√°quinas, configura√ß√µes, imagens AMI, instant√¢neos ... E se o s3 travar, como aconteceu nesses 7 anos, quanto bitrix24 estamos usando, √© seguido por um f√£ puxa um monte de tudo - inacessibilidade ao iniciar m√°quinas virtuais, mau funcionamento da API e assim por diante. <br><br>  E o S3 pode cair - aconteceu uma vez.  Portanto, chegamos ao seguinte esquema: alguns anos atr√°s, n√£o havia armazenamento p√∫blico s√©rio de objetos na R√∫ssia, e est√°vamos considerando a op√ß√£o de fazer algo nosso ... Felizmente, n√£o come√ßamos a faz√™-lo, porque investig√°vamos aquele exame que n√£o realizamos. possuir, e provavelmente teria feito isso.  Agora o Mail.ru possui armazenamentos compat√≠veis com s3, o Yandex e v√°rios provedores ainda o possuem.  Como resultado, chegamos √† conclus√£o de que queremos ter, primeiro, um backup e, em segundo lugar, a capacidade de trabalhar com c√≥pias locais.  Para uma regi√£o russa espec√≠fica, usamos o servi√ßo Mail.ru Hotbox, que √© compat√≠vel com API s3.  N√£o precisamos de modifica√ß√µes s√©rias no c√≥digo dentro do aplicativo e fizemos o seguinte mecanismo: no s3 existem gatilhos que trabalham na cria√ß√£o / exclus√£o de objetos, a Amazon possui um servi√ßo como o Lambda - este √© um c√≥digo em execu√ß√£o sem servidor que ser√° executado apenas quando certos gatilhos s√£o acionados. <br><br><img src="https://habrastorage.org/webt/yv/y5/cp/yvy5cp7xsn82ryeocsgos7ns7ss.jpeg"><br><br>  Fizemos isso de maneira muito simples: se o nosso gatilho for acionado, executaremos o c√≥digo que copiar√° o objeto no reposit√≥rio Mail.ru.  Para come√ßar a trabalhar totalmente com c√≥pias locais de dados, tamb√©m precisamos de sincroniza√ß√£o reversa, para que os clientes localizados no segmento russo possam trabalhar com o armazenamento mais pr√≥ximo deles.  O Mail est√° prestes a concluir os gatilhos em seu reposit√≥rio - ser√° poss√≠vel executar a sincroniza√ß√£o reversa j√° no n√≠vel da infraestrutura, mas por enquanto estamos fazendo isso no n√≠vel do nosso pr√≥prio c√≥digo.  Se percebermos que o cliente colocou algum tipo de arquivo, no n√≠vel do c√≥digo, colocamos o evento na fila, processamos e fazemos a replica√ß√£o reversa.  Por que isso √© ruim: se tivermos algum tipo de trabalho com nossos objetos fora do nosso produto, ou seja, por alguns meios externos, n√£o levaremos isso em considera√ß√£o.  Portanto, esperamos at√© o final, quando os gatilhos aparecerem no n√≠vel de armazenamento, para que, independentemente de onde executemos o c√≥digo, o objeto que veio at√© n√≥s seja copiado da outra maneira. <br><br>  No n√≠vel do c√≥digo, para cada cliente, os dois reposit√≥rios s√£o registrados: um √© considerado o principal e o outro √© o backup.  Se tudo estiver bem, trabalhamos com o armazenamento mais pr√≥ximo de n√≥s: nossos clientes que est√£o na Amazon, trabalham com o S3 e aqueles que trabalham na R√∫ssia, trabalham com o Hotbox.  Se a caixa de sele√ß√£o funcionar, o failover deve se conectar a n√≥s e trocaremos os clientes para outro armazenamento.  Podemos definir esse sinalizador independentemente por regi√£o e altern√°-los.  Na pr√°tica, ainda n√£o o usamos, mas imaginamos esse mecanismo e achamos que algum dia precisaremos e usaremos essa mesma op√ß√£o.  Uma vez que j√° aconteceu. <br><br><h4>  Ah, e sua Amaz√¥nia escapou ... </h4><br>  Este m√™s de abril √© o anivers√°rio do in√≠cio dos bloqueios do Telegram na R√∫ssia.  O provedor mais afetado que veio com isso √© a Amazon.  E, infelizmente, as empresas russas que trabalharam em todo o mundo sofreram mais. <br><br>  Se a empresa √© global e a R√∫ssia, por ser um segmento muito pequeno, 3-5% - bem, de uma forma ou de outra, voc√™ pode do√°-las. <br><br>  Se esta √© uma empresa puramente russa - tenho certeza de que voc√™ precisa localiz√°-la localmente - bem, √© justo que os pr√≥prios usu√°rios sejam convenientes, confort√°veis, haver√° menos riscos. <br><br>  E se esta √© uma empresa que trabalha globalmente e possui quotas de clientes aproximadamente iguais da R√∫ssia e de algum lugar do mundo?  A conectividade dos segmentos √© importante e eles devem trabalhar entre si de qualquer maneira. <br><br>  No final de mar√ßo de 2018, a Roskomnadzor enviou uma carta aos maiores operadores afirmando que planejava bloquear v√°rios milh√µes de ip da Amazon para bloquear ... o Zello messenger.  Gra√ßas a esses mesmos fornecedores, eles vazaram a carta com sucesso para todos, e havia um entendimento de que a conectividade com a Amazon poderia desmoronar.  Era sexta-feira, corremos em p√¢nico para os colegas do servers.ru, com as palavras: "Amigos, precisamos de v√°rios servidores que n√£o estar√£o na R√∫ssia, n√£o na Amaz√¥nia, mas, por exemplo, em algum lugar de Amsterd√£". para poder, de alguma forma, colocar nosso pr√≥prio VPN e proxy l√° para alguns pontos de extremidade que n√£o podemos influenciar, por exemplo, pontos finais do mesmo s3 - n√£o podemos tentar aumentar um novo servi√ßo e obter outro IP, voc√™ ainda precisa chegar l√°.  Em alguns dias, configuramos esses servidores, aumentamos e, em geral, preparamos o in√≠cio dos bloqueios.  √â curioso que o ILV, olhando para o hype e o p√¢nico elevado, tenha dito: "N√£o, n√£o vamos bloquear nada agora".  (Mas isso √© exatamente at√© o momento em que eles come√ßaram a bloquear os telegramas.) Depois de configurar as op√ß√µes de desvio e perceber que eles n√£o entraram na fechadura, n√≥s, no entanto, n√£o desmontamos tudo.  Ent√£o, apenas no caso. <br><br><img src="https://habrastorage.org/webt/la/o1/c1/lao1c1iqnqljjdv76qb1s9hg-qo.jpeg"><br><br>  E em 2019, ainda vivemos nas condi√ß√µes de bloqueios.  Eu olhei ontem √† noite: cerca de um milh√£o de ip continuam bloqueados.  √â verdade que a Amazon quase completamente desbloqueada, no pico atingiu 20 milh√µes de endere√ßos ... Em geral, a realidade √© que conectividade, boa conectividade - pode n√£o ser.  De repente.  Pode n√£o ser por raz√µes t√©cnicas - inc√™ndios, escavadeiras, tudo isso.  Ou, como vimos, n√£o totalmente t√©cnico.  Portanto, algu√©m grande e grande, com seu pr√≥prio AS-kami, provavelmente pode orient√°-lo de outras maneiras - a conex√£o direta e outras coisas j√° est√£o no n√≠vel l2.  Mas, em uma vers√£o simples, assim como n√≥s ou ainda menor, voc√™ pode, no caso de ter, redund√¢ncia no n√≠vel de servidores criados em outro local, configurado com anteced√™ncia vpn, proxy, com a capacidade de alternar rapidamente configura√ß√µes nos segmentos com conectividade cr√≠tica .  Isso foi √∫til para n√≥s mais de uma vez, quando os bloqueios da Amazon come√ßaram, deixamos o tr√°fego do S3 na pior das hip√≥teses, mas gradualmente tudo deu errado. <br><br><h4>  E como reservar ... todo o provedor? </h4><br>  Agora n√£o temos cen√°rio em caso de falha de toda a Amaz√¥nia.  Temos um cen√°rio semelhante para a R√∫ssia.  N√≥s, na R√∫ssia, est√°vamos hospedados por um provedor, de quem escolhemos ter v√°rios sites.  E h√° um ano, encontramos um problema: mesmo sendo dois data centers, j√° pode haver problemas no n√≠vel da configura√ß√£o de rede do provedor que afetar√£o os dois data centers de qualquer maneira.  E podemos obter inacessibilidade nos dois sites.  Claro, foi isso que aconteceu.  Finalmente redefinimos a arquitetura interna.  N√£o mudou muito, mas para a R√∫ssia agora temos dois sites, que n√£o s√£o um provedor, mas dois diferentes.  Se um deles falhar, podemos mudar para outro. <br><br>  Hipoteticamente, estamos considerando que a Amazon se reserve no n√≠vel de outro fornecedor;  talvez Google, talvez outra pessoa ... Mas at√© agora observamos na pr√°tica que, se a Amazon travar no mesmo n√≠vel de zona de disponibilidade, travamentos no n√≠vel de uma regi√£o inteira s√£o bastante raros.  Portanto, teoricamente, temos a ideia de que, talvez, fa√ßamos uma reserva ‚ÄúAmaz√¥nia n√£o √© Amaz√¥nia‚Äù, mas na pr√°tica isso ainda n√£o existe. <br><br><h4>  Algumas palavras sobre automa√ß√£o </h4><br>  Voc√™ sempre precisa de automa√ß√£o?  √â apropriado recuperar o efeito Dunning-Krueger.  No eixo x, nosso conhecimento e experi√™ncia, que estamos obtendo, e no eixo y - confian√ßa em nossas a√ß√µes.  A princ√≠pio, n√£o sabemos nada e n√£o temos certeza.  Ent√£o, sabemos um pouco e nos tornamos mega-confiantes - esse √© o chamado "pico da estupidez", bem ilustrado pelo quadro "dem√™ncia e coragem".  Al√©m disso, j√° aprendemos um pouco e estamos prontos para a batalha.  Ent√£o pisamos em um rake mega s√©rio, ca√≠mos em um vale de desespero quando parecemos saber alguma coisa, mas na verdade n√£o sabemos muito.  Ent√£o, √† medida que voc√™ ganha experi√™ncia, nos tornamos mais confiantes. <br><br><img src="https://habrastorage.org/webt/ha/-x/uy/ha-xuyf7ttz6wyojkkh5idgrli4.jpeg"><br><br>  Nossa l√≥gica sobre v√°rias mudan√ßas automaticamente para um ou outro acidente √© muito bem descrita por este gr√°fico.   ‚Äî    ,     .   ,       , ,  .      -:    false positive,    - , , -,    . ,     - ‚Äî     .     ,       .       ,    .  Mas!     ,        ,  ,  , ,   ,     ‚Ä¶ <br><br><h4>  Conclus√£o </h4><br>  7      , ,  - , ‚Äî  -,  ,    ,   ,   ‚Äî   ‚Äî .    - ,     ,   ,   .      ‚Äî         ,    ,           ‚Äî    .     ,  -        ‚Äî    s3,     ,   .         ,      ,  - - .     .     ,      , ‚Äî  :  ,      ‚Äî            ? , -           ,      ,     -   ¬´,   ¬ª. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um compromisso razo√°vel entre perfeccionismo e for√ßas reais, tempo, dinheiro que voc√™ pode gastar no esquema que acabar√° tendo. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Este texto √© uma vers√£o suplementar e ampliada do relat√≥rio de Alexander Demidov na confer√™ncia </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Uptime day 4</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt455112/">https://habr.com/ru/post/pt455112/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt455100/index.html">Como organizar o caf√© no escrit√≥rio</a></li>
<li><a href="../pt455102/index.html">Conectando interfaces seriais por IP</a></li>
<li><a href="../pt455104/index.html">BotAuth - login e registro usando bots</a></li>
<li><a href="../pt455106/index.html">Desenvolvimento de software. 2019 Tend√™ncias</a></li>
<li><a href="../pt455110/index.html">A felicidade dos funcion√°rios depende de tarefas interessantes? Tell Badoo, SKB Kontur, Dodo Pizza, Staply e Jogos Alternativos</a></li>
<li><a href="../pt455114/index.html">Implementa√ß√£o de tipo inteiro no CPython</a></li>
<li><a href="../pt455118/index.html">Para o futuro com a integra√ß√£o de servi√ßos Jenkins e Oracle APEX</a></li>
<li><a href="../pt455120/index.html">Wi-fi n√£o √© para todos. Como autorizar estrangeiros na rede por lei?</a></li>
<li><a href="../pt455122/index.html">Alternativa mais r√°pida para reflex√£o de Java</a></li>
<li><a href="../pt455126/index.html">Roupas inteligentes do futuro: existe potencial?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>