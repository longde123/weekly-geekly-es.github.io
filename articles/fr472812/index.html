<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ‘µğŸ¿ ğŸ³ ğŸ’ Le serveur Â«s'Ã©teintÂ» si le test de fumÃ©e du centre de donnÃ©es Â«prend feuÂ»? âœï¸ ğŸ© ğŸ‘©ğŸ¼â€ğŸ“</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Que ressentiriez-vous si, un beau jour d'Ã©tÃ©, un centre de donnÃ©es avec votre Ã©quipement ressemblait Ã  Ã§a? 



 Bonjour Ã  tous! Je m'appelle Dmitry Sa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Le serveur Â«s'Ã©teintÂ» si le test de fumÃ©e du centre de donnÃ©es Â«prend feuÂ»?</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/472812/">  Que ressentiriez-vous si, un beau jour d'Ã©tÃ©, un centre de donnÃ©es avec votre Ã©quipement ressemblait Ã  Ã§a? <br><br><img src="https://habrastorage.org/webt/b4/5i/by/b45ibyn7ljfqivfhmjhqb-hyjmq.jpeg"><br><br>  Bonjour Ã  tous!  Je m'appelle Dmitry Samsonov, je travaille en tant qu'administrateur systÃ¨me de premier plan chez <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Odnoklassniki</a> .  La photo montre l'un des quatre centres de donnÃ©es oÃ¹ l'Ã©quipement qui sert notre projet est installÃ©.  DerriÃ¨re ces murs, il y a environ 4 000 unitÃ©s d'Ã©quipements: serveurs, systÃ¨me de stockage de donnÃ©es, Ã©quipements rÃ©seau, etc.  - prÃ¨s des â…“ de tous nos Ã©quipements. <br>  La plupart des serveurs sont Linux.  Il existe plusieurs dizaines de serveurs Windows (MS SQL) - notre hÃ©ritage, que nous refusons systÃ©matiquement depuis de nombreuses annÃ©es. <br>  Ainsi, le 5 juin 2019 Ã  14h35, les ingÃ©nieurs de l'un de nos centres de donnÃ©es ont signalÃ© une alarme incendie. <br><a name="habracut"></a><br><h4>  DÃ©ni </h4><br>  14h45.  Les incidents enfumÃ©s mineurs dans les centres de donnÃ©es sont plus courants qu'ils ne le semblent.  Les indicateurs Ã  l'intÃ©rieur des halls Ã©taient normaux, donc notre premiÃ¨re rÃ©action a Ã©tÃ© relativement calme: nous avons introduit une interdiction de travailler avec la production, c'est-Ã -dire de tout changement de configuration, de dÃ©ployer de nouvelles versions, etc., Ã  l'exception des travaux liÃ©s Ã  la rÃ©paration de quelque chose. <br><br><h4>  La colÃ¨re </h4><br>  Avez-vous dÃ©jÃ  essayÃ© de savoir exactement auprÃ¨s des pompiers oÃ¹ il y avait un incendie sur le toit ou de monter vous-mÃªme sur un toit en feu pour Ã©valuer la situation?  Quel sera le degrÃ© de confiance dans les informations reÃ§ues de cinq personnes? <br><br>  14h50.  <b>Des informations ont Ã©tÃ© reÃ§ues indiquant que l'incendie s'approchait du systÃ¨me de refroidissement</b> .  Mais cela viendra-t-il?  L'administrateur systÃ¨me en service affiche le trafic externe depuis les faÃ§ades de ce centre de donnÃ©es. <br><blockquote>  Ã€ l'heure actuelle, les fronts de tous nos services sont dupliquÃ©s dans trois centres de donnÃ©es, un Ã©quilibrage au niveau DNS est utilisÃ©, ce qui vous permet de supprimer les adresses d'un centre de donnÃ©es du DNS, protÃ©geant ainsi les utilisateurs des problÃ¨mes potentiels d'accÃ¨s aux services.  Si des problÃ¨mes sont dÃ©jÃ  survenus dans le centre de donnÃ©es, il quitte automatiquement la rotation.  Plus de dÃ©tails peuvent Ãªtre trouvÃ©s ici: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Ã‰quilibrage de charge et tolÃ©rance aux pannes Ã  Odnoklassniki.</a> </blockquote><br>  L'incendie ne nous a pas encore affectÃ© de quelque faÃ§on que ce soit - ni les utilisateurs ni l'Ã©quipement n'ont Ã©tÃ© affectÃ©s.  Est-ce un accident?  La premiÃ¨re section du document Â«Plan d'action contre les accidentsÂ» dÃ©finit le concept d 'Â«accidentÂ», et la section se termine comme suit: <br>  <b>Â« <u>En cas de doute, accident ou pas, c'est un accident!</u></b>  <b>"</b> <br><br>  14:53.  Un coordinateur accident est nommÃ©. <br><blockquote>  Un coordinateur est une personne qui contrÃ´le la communication entre tous les participants, estime l'ampleur de l'accident, utilise le Â«Plan d'action contre les accidentsÂ», attire le personnel nÃ©cessaire, surveille l'achÃ¨vement des rÃ©parations et, surtout, dÃ©lÃ¨gue toutes les tÃ¢ches.  En d'autres termes, c'est la personne qui gÃ¨re l'ensemble du processus d'Ã©limination de l'accident. </blockquote><br><h4>  NÃ©gociation </h4><br>  15:01.  Nous commenÃ§ons Ã  dÃ©connecter les serveurs qui ne sont pas liÃ©s Ã  la production. <br>  15h03.  DÃ©sactivez correctement tous les services rÃ©servÃ©s. <br>  Cela inclut non seulement les fronts (auxquels les utilisateurs ne se connectent plus Ã  ce stade) et leurs services auxiliaires (logique mÃ©tier, caches, etc.), mais Ã©galement diverses bases de donnÃ©es avec facteur de rÃ©plication 2 ou plus ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Cassandra</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">stockage de donnÃ©es binaires</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">stockage Ã  froid</a> , <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">NewSQL</a> , etc.). <br>  15h06.  <b>Des informations ont Ã©tÃ© reÃ§ues selon lesquelles un incendie menace l'un des halls du centre de donnÃ©es.</b>  Nous n'avons pas d'Ã©quipement dans ce hall, mais le fait que le feu puisse se propager du toit aux halls change considÃ©rablement l'image de ce qui se passe. <br>  (Plus tard, il s'est avÃ©rÃ© qu'il n'y avait aucune menace physique pour le hall, car il Ã©tait hermÃ©tiquement isolÃ© du toit. La menace Ã©tait uniquement pour le systÃ¨me de refroidissement de ce hall.) <br>  15h07.  Nous permettons l'exÃ©cution de commandes sur des serveurs en mode accÃ©lÃ©rÃ© sans vÃ©rifications supplÃ©mentaires ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">sans notre calculatrice prÃ©fÃ©rÃ©e</a> ). <br>  15h08.  La tempÃ©rature dans les chambres est dans les limites normales. <br>  15:12.  <b>Une augmentation de la tempÃ©rature dans les halls a Ã©tÃ© enregistrÃ©e.</b> <br>  15:13.  Plus de la moitiÃ© des serveurs du centre de donnÃ©es sont hors tension.  Nous continuons. <br>  15:16.  Il a Ã©tÃ© dÃ©cidÃ© d'Ã©teindre tout l'Ã©quipement. <br>  15 h 21  Nous commenÃ§ons Ã  couper l'alimentation des serveurs sans Ã©tat sans arrÃªter correctement l'application et les systÃ¨mes d'exploitation. <br>  15:23.  Un groupe de responsables de MS SQL est isolÃ© (il y en a peu, la dÃ©pendance des services Ã  leur Ã©gard n'est pas grande, mais la procÃ©dure de rÃ©cupÃ©ration prend plus de temps et est plus compliquÃ©e que, par exemple, Cassandra). <br><br><h4>  La dÃ©pression </h4><br>  15h25.  <b>Des informations ont Ã©tÃ© reÃ§ues sur la coupure du courant dans quatre des 16 chambres (n Â° 6, 7, 8, 9).</b>  Dans les 7e et 8e salles se trouvent nos Ã©quipements.  Il n'y a plus d'informations sur nos deux chambres (n Â° 1 et 3). <br>  Habituellement, pendant les incendies, l'alimentation est immÃ©diatement coupÃ©e, mais dans ce cas, grÃ¢ce au travail coordonnÃ© des pompiers et du personnel technique du centre de donnÃ©es, elle a Ã©tÃ© coupÃ©e pas partout et pas tout de suite, mais au besoin. <br>  (Plus tard, il s'est avÃ©rÃ© que l'alimentation dans les halls 8 et 9 ne s'Ã©tait pas coupÃ©e.) <br>  15:28.  Nous commenÃ§ons Ã  dÃ©ployer des bases de donnÃ©es MS SQL Ã  partir de sauvegardes dans d'autres centres de donnÃ©es. <br>  Combien de temps cela prendra-t-il?  La bande passante rÃ©seau est-elle suffisante pour l'ensemble de l'itinÃ©raire? <br>  15:37.  <b>VerrouillÃ© certaines parties du rÃ©seau.</b> <br>  Le rÃ©seau de gestion et de production est physiquement isolÃ© l'un de l'autre.  Si le rÃ©seau de production est disponible, vous pouvez vous rendre sur le serveur, arrÃªter l'application et dÃ©sactiver le systÃ¨me d'exploitation.  S'il n'est pas disponible, vous pouvez passer par IPMI, arrÃªter l'application et dÃ©sactiver le systÃ¨me d'exploitation.  S'il n'y a pas de rÃ©seau, vous ne pouvez rien faire.  "Merci casquette!" Vous penserez. <br>  "Quoi qu'il en soit, il y a beaucoup de confusion", pourrait-on aussi penser. <br>  Le fait est que les serveurs mÃªme sans feu gÃ©nÃ¨rent une Ã©norme quantitÃ© de chaleur.  Plus prÃ©cisÃ©ment, quand il y a du refroidissement, ils gÃ©nÃ¨rent de la chaleur, et quand il n'y en a pas, ils crÃ©ent un enfer infernal, qui au mieux fera fondre une partie de l'Ã©quipement et Ã©teindra l'autre, et au pire ... cela provoquera un incendie Ã  l'intÃ©rieur du hall, qui dÃ©truira presque certainement tout. <br><br><img src="https://habrastorage.org/webt/fp/m6/zg/fpm6zg2uwewoewqfwgocfbkamky.jpeg"><br><br>  15:39.  Nous corrigeons les problÃ¨mes avec la base de donnÃ©es conf. <br><blockquote>  La base de donnÃ©es conf est le backend du service du mÃªme nom, qui est utilisÃ© par toutes les applications de production pour modifier rapidement les paramÃ¨tres.  Sans cette base, nous ne pouvons pas contrÃ´ler le portail, mais le portail lui-mÃªme peut fonctionner. </blockquote><br>  15:41.  Les capteurs de tempÃ©rature sur l'Ã©quipement du rÃ©seau central enregistrent des lectures proches du maximum autorisÃ©.  Il s'agit d'un boÃ®tier qui occupe un rack entier et garantit le fonctionnement de tous les rÃ©seaux Ã  l'intÃ©rieur du centre de donnÃ©es. <br><br><img src="https://habrastorage.org/webt/hk/pu/ju/hkpujukttnjdf651rnnhpof0qxw.jpeg"><br><br>  15:42.  Le suivi des problÃ¨mes et le wiki ne sont pas disponibles, passez en mode veille. <br>  Ce n'est pas de la production, mais en cas d'accident, la disponibilitÃ© de toute base de connaissances peut Ãªtre critique. <br>  15h50.  L'un des systÃ¨mes de surveillance s'est dÃ©connectÃ©. <br>  Il y en a plusieurs et ils sont responsables de divers aspects du travail des services.  Certains d'entre eux sont configurÃ©s pour fonctionner de maniÃ¨re autonome Ã  l'intÃ©rieur de chaque centre de donnÃ©es (c'est-Ã -dire qu'ils ne surveillent que leur centre de donnÃ©es), tandis que d'autres sont constituÃ©s de composants distribuÃ©s qui survivent de maniÃ¨re transparente Ã  la perte de tout centre de donnÃ©es. <br>  Dans ce cas, le <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">systÃ¨me de dÃ©tection des anomalies dans les indicateurs de logique mÃ©tier</a> qui fonctionne en mode maÃ®tre-veille a cessÃ© de fonctionner.  Passez en veille. <br><br><h4>  Acceptation </h4><br>  15:51.  GrÃ¢ce Ã  IPMI, tous les serveurs Ã  l'exception de MS SQL ont Ã©tÃ© Ã©teints sans un arrÃªt correct. <br>  ÃŠtes-vous prÃªt pour la gestion de serveurs de masse via IPMI si nÃ©cessaire? <br><br><hr>  <i>Le moment mÃªme oÃ¹ le sauvetage de l'Ã©quipement dans le centre de donnÃ©es Ã  ce stade est terminÃ©.</i>  <i>Tout ce qui pouvait Ãªtre fait a Ã©tÃ© fait.</i>  <i>Certains collÃ¨gues peuvent se dÃ©tendre.</i> <hr><br>  16:13.  <b>Il y avait des informations selon lesquelles des tubes de frÃ©on des climatiseurs ont Ã©clatÃ© sur le toit - cela retardera le lancement du centre de donnÃ©es aprÃ¨s avoir Ã©liminÃ© l'incendie.</b> <br>  16:19.  Selon les donnÃ©es reÃ§ues du personnel technique du centre de donnÃ©es, l'augmentation de la tempÃ©rature dans les halls s'est arrÃªtÃ©e. <br>  17:10.  RestaurÃ© le travail de la base de donnÃ©es conf.  Nous pouvons maintenant modifier les paramÃ¨tres de l'application. <br>  Pourquoi est-il si important que tout soit tolÃ©rant aux pannes et fonctionne mÃªme sans un seul centre de donnÃ©es? <br>  PremiÃ¨rement, tout n'est pas tolÃ©rant aux pannes.  Il existe divers services secondaires qui ne survivent pas assez bien Ã  la panne du centre de donnÃ©es, et il existe des bases en mode maÃ®tre-veille.  La capacitÃ© de gÃ©rer les paramÃ¨tres vous permet de faire tout le nÃ©cessaire pour minimiser l'impact des consÃ©quences de l'accident sur les utilisateurs mÃªme dans des conditions difficiles. <br>  DeuxiÃ¨mement, il est devenu clair que dans les prochaines heures, le centre de donnÃ©es ne se rÃ©tablira pas complÃ¨tement, il Ã©tait donc nÃ©cessaire de prendre des mesures pour que l'indisponibilitÃ© Ã  long terme des rÃ©pliques n'entraÃ®ne pas de problÃ¨mes supplÃ©mentaires tels que des dÃ©passements de disque dans les centres de donnÃ©es restants. <br>  17:29.  Le temps de la pizza!  Nous avons des gens qui travaillent, pas des robots. <br><br><img src="https://habrastorage.org/webt/3x/ft/hl/3xfthlpehimklv9yd_oua6phnis.png"><br><br><h4>  RÃ©habilitation </h4><br>  18:02.  Dans les chambres n Â° 8 (les nÃ´tres), 9, 10 et 11, la tempÃ©rature s'est stabilisÃ©e.  Dans l'un de ceux qui restent dÃ©connectÃ©s (n Â° 7), notre Ã©quipement est situÃ©, et la tempÃ©rature y continue d'augmenter. <br>  18:31.  Ils ont donnÃ© le feu vert pour lancer des Ã©quipements dans les halls nos 1 et 3 - l'incendie n'a pas affectÃ© ces halls. <br><br><hr>  <i>Actuellement, des serveurs sont lancÃ©s dans les halls n Â° 1, 3, 8, Ã  commencer par les plus critiques.</i>  <i>VÃ©rifie le bon fonctionnement de tous les services en cours d'exÃ©cution.</i>  <i>Il y a encore des problÃ¨mes avec la chambre 7.</i> <hr><br><br>  18:44.  Le personnel technique du centre de donnÃ©es a constatÃ© que dans le hall numÃ©ro 7 (oÃ¹ se trouvent uniquement nos Ã©quipements), de nombreux serveurs ne sont pas Ã©teints.  Selon nos donnÃ©es, 26 serveurs y restent.  AprÃ¨s une nouvelle vÃ©rification, nous trouvons 58 serveurs. <br>  20:18.  Le personnel technique du centre de donnÃ©es souffle de l'air dans la piÃ¨ce sans climatisation par des conduits mobiles posÃ©s dans les couloirs. <br>  23:08.  Ils ont laissÃ© le premier administrateur rentrer chez lui.  Quelqu'un a besoin de dormir la nuit pour continuer Ã  travailler demain.  Ensuite, nous publions une autre partie des administrateurs et des dÃ©veloppeurs. <br>  02:56.  Nous avons lancÃ© tout ce qui pouvait Ãªtre lancÃ©.  Nous effectuons une grande vÃ©rification de tous les services avec des autotests. <br><br><img src="https://habrastorage.org/webt/90/pq/ii/90pqii3vxt6p5hzomjdymkebp3a.jpeg"><br><br>  3 h 02  Air conditionnÃ© dans le dernier, 7Ã¨me hall restaurÃ©. <br>  03:36.  Ils ont fait tourner les fronts du centre de donnÃ©es dans le DNS.  A partir de ce moment, le trafic des utilisateurs commence Ã  arriver. <br>  Nous dissolvons la plupart de l'Ã©quipe des administrateurs Ã  domicile.  Mais nous laissons quelques personnes. <br><blockquote>  Petite FAQ: <br>  Q: Que s'est-il passÃ© de 18 h 31 Ã  02 h 56? <br>  R: Suite au Â«Plan d'action contre les accidentsÂ», nous lanÃ§ons tous les services, en commenÃ§ant par les plus importants.  Dans le mÃªme temps, le coordinateur du chat fournit un service Ã  un administrateur gratuit, qui vÃ©rifie si le systÃ¨me d'exploitation et l'application ont dÃ©marrÃ©, s'il y a des erreurs ou si les indicateurs sont normaux.  Une fois le lancement terminÃ©, il informe le chat qu'il est libre et reÃ§oit un nouveau service du coordinateur. <br>  Le processus est en outre inhibÃ© par le fer dÃ©faillant.  MÃªme si l'arrÃªt du systÃ¨me d'exploitation et l'arrÃªt des serveurs Ã©taient corrects, certains serveurs ne reviennent pas en raison de disques, de mÃ©moire ou de chÃ¢ssis soudainement dÃ©faillants.  Avec une perte de puissance, le taux de dÃ©faillance augmente. <br>  Q: Pourquoi ne pouvez-vous pas tout dÃ©marrer en mÃªme temps, puis rÃ©parer ce qui sort de la surveillance? <br>  R: Tout doit Ãªtre fait progressivement, car il existe des dÃ©pendances entre les services.  Et tout doit Ãªtre vÃ©rifiÃ© immÃ©diatement, sans attendre la surveillance - car il vaut mieux traiter les problÃ¨mes tout de suite, pas attendre leur aggravation. </blockquote><br>  7 h 40  Le dernier administrateur (coordinateur) s'est couchÃ©.  Le travail du premier jour est terminÃ©. <br>  8 h 09  Les premiers dÃ©veloppeurs, ingÃ©nieurs des centres de donnÃ©es et administrateurs (dont le nouveau coordinateur) ont commencÃ© les travaux de restauration. <br>  09:37.  Nous avons commencÃ© Ã  Ã©lever le hall numÃ©ro 7 (le dernier). <br>  Dans le mÃªme temps, nous continuons de restaurer ce que nous n'avons pas fini dans d'autres salles: remplacer les disques / mÃ©moire / serveurs, rÃ©parer tout ce qui est en feu dans la surveillance, inverser le changement de rÃ´le dans les circuits maÃ®tre-veille et d'autres petites choses, qui sont nÃ©anmoins beaucoup. <br>  17:08.  Autorisez tous les travaux rÃ©guliers avec la production. <br>  21:45.  Le travail du deuxiÃ¨me jour est terminÃ©. <br>  09h45.  Aujourd'hui c'est vendredi.  Il y a encore pas mal de problÃ¨mes mineurs dans la surveillance.  Avant le week-end, tout le monde veut se dÃ©tendre.  Nous continuons Ã  rÃ©parer massivement tout ce qui est possible.  Les tÃ¢ches d'administration rÃ©guliÃ¨res qui pourraient Ãªtre reportÃ©es sont reportÃ©es.  Le coordinateur est nouveau. <br>  15h40.  Soudainement, la moitiÃ© de la pile principale d'Ã©quipement rÃ©seau dans le centre de donnÃ©es OTHER a redÃ©marrÃ©.  Suppression des fronts de la rotation pour minimiser les risques.  Il n'y a aucun effet pour les utilisateurs.  Plus tard, il s'est avÃ©rÃ© que c'Ã©tait un mauvais chÃ¢ssis.  Le coordinateur travaille Ã  rÃ©parer deux plantages Ã  la fois. <br>  17:17.  Le rÃ©seau d'un autre centre de donnÃ©es est restaurÃ©, tout est vÃ©rifiÃ©.  Le centre de donnÃ©es est en rotation. <br>  18:29.  Le travail du troisiÃ¨me jour et, en gÃ©nÃ©ral, la rÃ©cupÃ©ration de l'accident est terminÃ©. <br><br><h4>  Postface </h4><br>  04/04/2013, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">le jour de la 404Ã¨me erreur</a> , Odnoklassniki a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">survÃ©cu Ã  un accident majeur</a> - pendant trois jours, le portail a Ã©tÃ© totalement ou partiellement indisponible.  Tout au long de cette pÃ©riode, plus de 100 personnes de diffÃ©rentes villes, de diffÃ©rentes entreprises (merci beaucoup!) Ã€ distance et directement dans les centres de donnÃ©es, ont rÃ©parÃ© manuellement et automatiquement des milliers de serveurs. <br>  Nous avons tirÃ© des conclusions.  Pour Ã©viter que cela ne se reproduise, nous avons effectuÃ© et continuons de mener un travail considÃ©rable Ã  ce jour. <br><br>  Quelles sont les principales diffÃ©rences entre l'accident actuel et le 404? <br><br><ul><li>  Nous avons un Â«plan d'action d'urgenceÂ».  Une fois par trimestre, nous menons des exercices - nous jouons sur une urgence, qu'un groupe d'administrateurs (tous tour Ã  tour) doit Ã©liminer en utilisant le Â«Plan d'action d'urgenceÂ».  Des administrateurs systÃ¨me de premier plan travaillent Ã  tour de rÃ´le sur le rÃ´le de coordinateur. </li><li>  Trimestriellement en mode test, isolez les centres de donnÃ©es (tous Ã  tour de rÃ´le) sur le LAN et le WAN, ce qui vous permet d'identifier les goulots d'Ã©tranglement en temps opportun. </li><li>  Moins de mauvais disques, car nous avons resserrÃ© les normes: moins d'heures de fonctionnement, des seuils plus stricts pour SMART, </li><li>  BerkeleyDB complÃ¨tement abandonnÃ© - une ancienne base de donnÃ©es instable qui nÃ©cessitait beaucoup de temps pour rÃ©cupÃ©rer aprÃ¨s un redÃ©marrage du serveur. </li><li>  RÃ©duisez le nombre de serveurs avec MS SQL et rÃ©duisez la dÃ©pendance vis-Ã -vis des autres. </li><li>  Nous avons notre <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">propre cloud - un cloud</a> , oÃ¹ nous migrons activement tous les services depuis deux ans.  Le cloud simplifie considÃ©rablement tout le cycle de travail avec l'application et, en cas d'accident, fournit des outils uniques tels que: <br><ul><li>  arrÃªter correctement toutes les applications en un seul clic; </li><li>  migration simple des applications Ã  partir de serveurs dÃ©faillants; </li><li>  classement automatique (par ordre de prioritÃ© des services) de lancement d'un datacenter complet. </li></ul></li></ul><br>  L'accident dÃ©crit dans cet article est devenu le plus important depuis le jour 404.  Bien sÃ»r, tout ne s'est pas bien passÃ©.  Par exemple, pendant l'indisponibilitÃ© du graveur de centre de donnÃ©es dans un autre centre de donnÃ©es, un disque est tombÃ© en panne sur l'un des serveurs, c'est-Ã -dire qu'une seule des trois rÃ©pliques du cluster Cassandra Ã©tait disponible, raison pour laquelle 4,2% des utilisateurs d'applications mobiles n'ont pas pu se connecter .  Dans le mÃªme temps, les utilisateurs dÃ©jÃ  connectÃ©s ont continuÃ© Ã  travailler.  Au total, plus de 30 problÃ¨mes ont Ã©tÃ© identifiÃ©s selon les rÃ©sultats de l'accident - des bogues banaux aux dÃ©fauts de l'architecture des services. <br><br>  Mais la principale diffÃ©rence entre l'accident actuel et le 404e est que, mÃªme si nous avons Ã©liminÃ© les consÃ©quences de l'incendie, les utilisateurs ont toujours correspondu et passÃ© des appels vidÃ©o Ã  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Tamtam</a> , jouÃ© Ã  des jeux, Ã©coutÃ© de la musique, se sont fait des cadeaux, regardÃ© des vidÃ©os, des Ã©missions de tÃ©lÃ©vision et des chaÃ®nes de tÃ©lÃ©vision dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OK</a> , et Ã©galement diffusÃ© sur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">OK Live</a> . <br><br>  Comment se passent vos accidents? </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr472812/">https://habr.com/ru/post/fr472812/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr472792/index.html">Ordinateur basÃ© sur les vannes NOR: Ã  l'intÃ©rieur de l'ordinateur de commande embarquÃ© Apollo</a></li>
<li><a href="../fr472796/index.html">OUI flanche FAANG * ou [guide pratique] sur la recherche d'emploi aux USA / Europe pour spÃ©cialiste informatique</a></li>
<li><a href="../fr472798/index.html">Cartes Yandex pour l'application Taxi</a></li>
<li><a href="../fr472802/index.html">MIRO est une plate-forme de robot intÃ©rieure ouverte. Partie 2 - Conception de robots</a></li>
<li><a href="../fr472810/index.html">Ã€ l'administrateur systÃ¨me dÃ©butant: comment mettre de l'ordre dans le chaos</a></li>
<li><a href="../fr472814/index.html">Ma premiÃ¨re machine virtuelle: comment ne pas salir</a></li>
<li><a href="../fr472816/index.html">Motifs Ã©lÃ©gants en JavaScript moderne (cycle d'Ã©quipe Bill Sourour)</a></li>
<li><a href="../fr472818/index.html">Mouvement collectif: comment les scientifiques ont Ã©tudiÃ© les bouchons de liÃ¨ge</a></li>
<li><a href="../fr472822/index.html">Quand l'AcadÃ©mie russe des sciences est impuissante</a></li>
<li><a href="../fr472826/index.html">Microinteractions et leur utilisation dans les interfaces utilisateur</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>