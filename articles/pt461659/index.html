<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçæ üë©üèΩ‚Äçü§ù‚Äçüë®üèº üì§ Redes neurais e aprendizado profundo, cap√≠tulo 4: Prova visual de que as redes neurais podem calcular qualquer fun√ß√£o üë®üèº‚Äçüç≥ üë©üèø‚Äçü§ù‚Äçüë©üèΩ ‚õ¥Ô∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Neste cap√≠tulo, dou uma explica√ß√£o simples e principalmente visual do teorema da universalidade. Para seguir o material deste cap√≠tulo, voc√™ n√£o preci...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Redes neurais e aprendizado profundo, cap√≠tulo 4: Prova visual de que as redes neurais podem calcular qualquer fun√ß√£o</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/461659/">  Neste cap√≠tulo, dou uma explica√ß√£o simples e principalmente visual do teorema da universalidade.  Para seguir o material deste cap√≠tulo, voc√™ n√£o precisa ler os anteriores.  Est√° estruturado como um ensaio independente.  Se voc√™ possui o entendimento mais b√°sico do NS, deve poder entender as explica√ß√µes. <br><br><div class="spoiler">  <b class="spoiler_title">Conte√∫do</b> <div class="spoiler_text"><ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 1: usando redes neurais para reconhecer n√∫meros manuscritos</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 2: como o algoritmo de retropropaga√ß√£o funciona</a> </li><li>  Cap√≠tulo 3: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: aprimorando o m√©todo de treinamento de redes neurais</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: Por que a regulariza√ß√£o ajuda a reduzir a reciclagem?</a> <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 3: como escolher hiperpar√¢metros de redes neurais?</a> <br></li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 4: prova visual de que as redes neurais s√£o capazes de computar qualquer fun√ß√£o</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Cap√≠tulo 5: por que as redes neurais profundas s√£o t√£o dif√≠ceis de treinar?</a> </li><li>  Cap√≠tulo 6: <ul><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 1: Aprendizado Profundo</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Parte 2: progresso recente no reconhecimento de imagens</a> </li></ul></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Posf√°cio: existe um algoritmo simples para criar intelig√™ncia?</a> </li></ul></div></div><br>  Um dos fatos mais surpreendentes sobre as redes neurais √© que elas podem calcular qualquer fun√ß√£o.  Ou seja, digamos que algu√©m lhe d√™ algum tipo de fun√ß√£o complexa e sinuosa f (x): <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><a name="habracut"></a><br>  E, independentemente dessa fun√ß√£o, √© garantida uma rede neural que, para qualquer entrada x, o valor f (x) (ou alguma aproxima√ß√£o pr√≥xima a ela) ser√° a sa√≠da dessa rede, ou seja: <br><br><img src="https://habrastorage.org/webt/eb/ud/zc/ebudzc72xyiytdvkf4ulx-u7onq.png"><br><br>  Isso funciona mesmo que seja uma fun√ß√£o de muitas vari√°veis ‚Äã‚Äãf = f (x <sub>1</sub> , ..., x <sub>m</sub> ) e com muitos valores.  Por exemplo, aqui est√° uma rede que calcula uma fun√ß√£o com m = 3 entradas en = 2 sa√≠das: <br><br><img src="https://habrastorage.org/webt/1g/01/i7/1g01i7vpnwo-mlm1r2brhd9al9m.png"><br><br>  Esse resultado sugere que as redes neurais t√™m uma certa universalidade.  Independentemente da fun√ß√£o que queremos calcular, sabemos que existe uma rede neural que pode fazer isso. <br><br>  Al√©m disso, o teorema da universalidade se mant√©m mesmo se restringirmos a rede a uma √∫nica camada entre os neur√¥nios de entrada e sa√≠da - os chamados  em uma camada oculta.  Assim, mesmo redes com uma arquitetura muito simples podem ser extremamente poderosas. <br><br>  O teorema da universalidade √© bem conhecido pelas pessoas que usam redes neurais.  Mas, embora seja assim, uma compreens√£o desse fato n√£o √© t√£o difundida.  E a maioria das explica√ß√µes para isso √© tecnicamente complexa demais.  Por exemplo, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">um dos primeiros trabalhos que</a> comprova esse resultado usou o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">teorema de Hahn - Banach</a> , o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">teorema de representa√ß√£o de Riesz</a> e algumas an√°lises de Fourier.  Se voc√™ √© matem√°tico, √© f√°cil entender essas evid√™ncias, mas para a maioria das pessoas n√£o √© t√£o f√°cil.  √â uma pena, porque as raz√µes b√°sicas da universalidade s√£o simples e bonitas. <br><br>  Neste cap√≠tulo, dou uma explica√ß√£o simples e principalmente visual do teorema da universalidade.  Vamos seguir passo a passo as id√©ias subjacentes.  Voc√™ entender√° por que as redes neurais podem realmente calcular qualquer fun√ß√£o.  Voc√™ entender√° algumas das limita√ß√µes deste resultado.  E voc√™ entender√° como o resultado est√° associado ao NS profundo. <br><br>  Para seguir o material deste cap√≠tulo, voc√™ n√£o precisa ler os anteriores.  Est√° estruturado como um ensaio independente.  Se voc√™ possui o entendimento mais b√°sico do NS, deve poder entender as explica√ß√µes.  √Äs vezes, por√©m, fornecerei links para material anterior para ajudar a preencher as lacunas de conhecimento. <br><br>  Os teoremas da universalidade s√£o freq√ºentemente encontrados na ci√™ncia da computa√ß√£o, ent√£o √†s vezes at√© esquecemos o qu√£o incr√≠vel eles s√£o.  Mas vale lembrar: a capacidade de calcular qualquer fun√ß√£o arbitr√°ria √© realmente incr√≠vel.  Quase todo processo que voc√™ pode imaginar pode ser reduzido ao c√°lculo de uma fun√ß√£o.  Considere a tarefa de encontrar o nome de uma composi√ß√£o musical com base em uma breve passagem.  Isso pode ser considerado um c√°lculo de fun√ß√£o.  Ou considere a tarefa de traduzir um texto chin√™s para o ingl√™s.  E isso pode ser considerado um c√°lculo de fun√ß√£o (de fato, muitas fun√ß√µes, pois existem muitas op√ß√µes aceit√°veis ‚Äã‚Äãpara traduzir um √∫nico texto).  Ou considere a tarefa de gerar uma descri√ß√£o do enredo do filme e a qualidade da atua√ß√£o com base no arquivo mp4.  Isso tamb√©m pode ser considerado como o c√°lculo de uma determinada fun√ß√£o (a observa√ß√£o feita sobre as op√ß√µes de tradu√ß√£o de texto tamb√©m est√° correta aqui).  Universalidade significa que, em princ√≠pio, os NSs podem executar todas essas tarefas e muitas outras. <br><br>  Obviamente, apenas pelo fato de sabermos que existem NSs capazes de, por exemplo, traduzir do chin√™s para o ingl√™s, n√£o se segue que temos boas t√©cnicas para criar ou mesmo reconhecer uma rede desse tipo.  Essa restri√ß√£o tamb√©m se aplica aos teoremas tradicionais da universalidade para modelos como esquemas booleanos.  Mas, como j√° vimos neste livro, o NS possui algoritmos poderosos para fun√ß√µes de aprendizado.  A combina√ß√£o de algoritmos de aprendizagem e versatilidade √© uma mistura atraente.  At√© agora, no livro, nos concentramos em algoritmos de treinamento.  Neste cap√≠tulo, focaremos na versatilidade e no que isso significa. <br><br><h2>  Dois truques </h2><br>  Antes de explicar por que o teorema da universalidade √© verdadeiro, quero mencionar dois truques contidos na declara√ß√£o informal "uma rede neural pode calcular qualquer fun√ß√£o". <br><br>  Primeiro, isso n√£o significa que a rede possa ser usada para calcular com precis√£o qualquer fun√ß√£o.  S√≥ podemos obter uma aproxima√ß√£o t√£o boa quanto precisamos.  Ao aumentar o n√∫mero de neur√¥nios ocultos, melhoramos a aproxima√ß√£o.  Por exemplo, eu ilustrei anteriormente uma rede que computa uma determinada fun√ß√£o f (x) usando tr√™s neur√¥nios ocultos.  Para a maioria das fun√ß√µes, usando tr√™s neur√¥nios, apenas uma aproxima√ß√£o de baixa qualidade pode ser obtida.  Ao aumentar o n√∫mero de neur√¥nios ocultos (digamos, at√© cinco), geralmente podemos obter uma aproxima√ß√£o melhorada: <br><br><img src="https://habrastorage.org/webt/x2/nt/zw/x2ntzw4ykxb450nexszfsd-qz08.png"><br><br>  E para melhorar a situa√ß√£o aumentando ainda mais o n√∫mero de neur√¥nios ocultos. <br><br>  Para esclarecer esta afirma√ß√£o, digamos que recebemos uma fun√ß√£o f (x), que queremos calcular com a precis√£o necess√°ria Œµ&gt; 0.  H√° uma garantia de que, ao usar um n√∫mero suficiente de neur√¥nios ocultos, sempre podemos encontrar um NS cuja sa√≠da g (x) satisfa√ßa a equa√ß√£o | g (x) ‚àíf (x) | &lt;Œµ para qualquer x.  Em outras palavras, a aproxima√ß√£o ser√° alcan√ßada com a precis√£o desejada para qualquer valor de entrada poss√≠vel. <br><br>  O segundo problema √© que as fun√ß√µes que podem ser aproximadas pelo m√©todo descrito pertencem a uma classe cont√≠nua.  Se a fun√ß√£o for interrompida, ou seja, ocorrer saltos bruscos repentinos, no caso geral, ser√° imposs√≠vel aproximar com a ajuda do NS.  E isso n√£o √© surpreendente, j√° que nossos NSs calculam fun√ß√µes cont√≠nuas de dados de entrada.  No entanto, mesmo que a fun√ß√£o que realmente precisamos calcular seja descont√≠nua, a aproxima√ß√£o geralmente √© bastante cont√≠nua.  Se sim, ent√£o podemos usar o NS.  Na pr√°tica, essa limita√ß√£o geralmente n√£o √© importante. <br><br>  Como resultado, uma declara√ß√£o mais precisa do teorema da universalidade ser√° que NS com uma camada oculta pode ser usado para aproximar qualquer fun√ß√£o cont√≠nua com a precis√£o desejada.  Neste cap√≠tulo, provamos uma vers√£o um pouco menos rigorosa desse teorema, usando duas camadas ocultas em vez de uma.  Nas tarefas, descreverei brevemente como essa explica√ß√£o pode ser adaptada, com pequenas altera√ß√µes, a uma prova que usa apenas uma camada oculta. <br><br><h2>  Versatilidade com uma entrada e um valor de sa√≠da </h2><br>  Para entender por que o teorema da universalidade √© verdadeiro, come√ßamos entendendo como criar uma fun√ß√£o de aproxima√ß√£o NS com apenas uma entrada e um valor de sa√≠da: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Acontece que essa √© a ess√™ncia da tarefa da universalidade.  Depois que entendermos esse caso especial, ser√° bastante f√°cil estend√™-lo para fun√ß√µes com muitos valores de entrada e sa√≠da. <br><br>  Para criar um entendimento de como construir uma rede para contar f, come√ßamos com uma rede contendo uma √∫nica camada oculta com dois neur√¥nios ocultos e com uma camada de sa√≠da contendo um neur√¥nio de sa√≠da: <br><br><img src="https://habrastorage.org/webt/b7/cz/ql/b7czqllzyyxbpzeq7gs3h6a2338.png"><br><br>  Para imaginar como os componentes de rede funcionam, nos concentramos no neur√¥nio oculto superior.  No diagrama do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo original,</a> voc√™ pode alterar interativamente o peso com o mouse, clicando em "w" e ver imediatamente como a fun√ß√£o calculada pelo neur√¥nio oculto superior muda: <br><br><img src="https://habrastorage.org/webt/mh/r0/dz/mhr0dzpmf_zop4a3bwi2of04qtu.png"><br><br>  Como aprendemos anteriormente no livro, um neur√¥nio oculto conta œÉ (wx + b), onde œÉ (z) ‚â° 1 / (1 + e <sup>-z</sup> ) √© um <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">sigm√≥ide</a> .  At√© agora, usamos essa forma alg√©brica com bastante frequ√™ncia.  No entanto, para provar a universalidade, seria melhor ignorarmos completamente essa √°lgebra e, em vez disso, manipularmos e observarmos a forma no gr√°fico.  Isso n√£o apenas o ajudar√° a sentir melhor o que est√° acontecendo, mas tamb√©m nos fornecer√° uma prova de universalidade aplic√°vel a outras fun√ß√µes de ativa√ß√£o al√©m do sigm√≥ide. <br><br>  A rigor, a abordagem visual que escolhi n√£o √© tradicionalmente considerada evid√™ncia.  Mas acredito que a abordagem visual fornece mais informa√ß√µes sobre a verdade do resultado final do que as provas tradicionais.  E, √© claro, esse entendimento √© o real objetivo da prova.  Nas evid√™ncias que proponho, as lacunas ocasionalmente aparecer√£o;  Darei evid√™ncias visuais razo√°veis, mas nem sempre rigorosas.  Se isso o incomoda, considere sua tarefa preencher essas lacunas.  No entanto, n√£o perca de vista o objetivo principal: entender por que o teorema da universalidade √© verdadeiro. <br><br>  Para come√ßar com essa prova, clique no deslocamento b no diagrama original e arraste para a direita para aument√°-lo.  Voc√™ ver√° que, com um aumento no deslocamento, o gr√°fico se move para a esquerda, mas n√£o muda de forma. <br><br>  Em seguida, arraste-o para a esquerda para reduzir o deslocamento.  Voc√™ ver√° que o gr√°fico est√° se movendo para a direita sem alterar a forma. <br><br>  Reduza o peso para 2-3.  Voc√™ ver√° que, √† medida que o peso diminui, a curva se endireita.  Para que a curva n√£o fuja do gr√°fico, talvez seja necess√°rio corrigir o deslocamento. <br><br>  Por fim, aumente o peso para valores maiores que 100. A curva se tornar√° mais √≠ngreme e, eventualmente, se aproximar√° da etapa.  Tente ajustar o deslocamento para que seu √¢ngulo fique na regi√£o do ponto x = 0,3.  O v√≠deo abaixo mostra o que deve acontecer: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/create_step_function.mp4" type="video/mp4"></video></div></div></div><br><br>  Podemos simplificar bastante nossa an√°lise aumentando o peso, para que o resultado seja realmente uma boa aproxima√ß√£o da fun√ß√£o step.  Abaixo, constru√≠ a sa√≠da do neur√¥nio oculto superior para o peso w = 999.  Esta √© uma imagem est√°tica: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/94c/24e/8a8/94c24e8a8a262c06c102b97bef033e99.jpg"><br><br>  Usar fun√ß√µes de passo √© um pouco mais f√°cil do que com o sigm√≥ide t√≠pico.  A raz√£o √© que as contribui√ß√µes de todos os neur√¥nios ocultos s√£o adicionadas na camada de sa√≠da.  A soma de v√°rias fun√ß√µes de etapa √© f√°cil de analisar, mas √© mais dif√≠cil falar sobre o que acontece quando v√°rias curvas s√£o adicionadas na forma de um sigm√≥ide.  Portanto, ser√° muito mais simples supor que nossos neur√¥nios ocultos produzam fun√ß√µes graduais.  Mais precisamente, fazemos isso fixando o peso w em um valor muito grande e atribuindo a posi√ß√£o da etapa atrav√©s do deslocamento.  Obviamente, trabalhar com uma sa√≠da como uma fun√ß√£o step √© uma aproxima√ß√£o, mas √© muito bom, e at√© agora trataremos a fun√ß√£o como uma verdadeira fun√ß√£o step.  Mais tarde, voltarei a discutir o efeito dos desvios dessa aproxima√ß√£o. <br><br>  Qual o valor de x √© o passo?  Em outras palavras, como a posi√ß√£o do degrau depende do peso e do deslocamento? <br><br>  Para responder √† pergunta, tente alterar o peso e o deslocamento no gr√°fico interativo.  Voc√™ consegue entender como a posi√ß√£o do passo depende de eb?  Ao praticar um pouco, voc√™ pode se convencer de que sua posi√ß√£o √© proporcional a be inversamente proporcional a w. <br><br>  De fato, o passo √© s = ‚àíb / w, como ser√° visto se ajustarmos o peso e o deslocamento com os seguintes valores: <br><br><img src="https://habrastorage.org/webt/ee/d9/zo/eed9zodaxp8ot33ip8y8j-sdcqg.png"><br><br>  Nossas vidas ser√£o bastante simplificadas se descrevermos os neur√¥nios ocultos com um √∫nico par√¢metro, s, isto √©, pela posi√ß√£o do passo, s = ‚àíb / w.  No diagrama interativo a seguir, voc√™ pode simplesmente alterar s: <br><br><img src="https://habrastorage.org/webt/uy/g6/9h/uyg69hokiufnmt7zwwmysjof5uc.png"><br><br>  Como observado acima, atribu√≠mos um peso w na entrada a um valor muito grande - suficientemente grande para que a fun√ß√£o step se torne uma boa aproxima√ß√£o.  E podemos facilmente transformar o neur√¥nio parametrizado dessa maneira de volta √† sua forma usual, escolhendo o vi√©s b = -ws. <br><br>  At√© agora, concentramo-nos apenas na produ√ß√£o do neur√¥nio oculto superior.  Vejamos o comportamento de toda a rede.  Suponha que os neur√¥nios ocultos calculem as fun√ß√µes do passo definidas pelos par√¢metros dos passos s <sub>1</sub> (neur√¥nio superior) es <sub>2</sub> (neur√¥nio inferior).  Seus respectivos pesos de sa√≠da s√£o w <sub>1</sub> e w <sub>2</sub> .  Aqui est√° a nossa rede: <br><br><img src="https://habrastorage.org/webt/6u/ot/ns/6uotnsmlecwfh8iaqz1eb5p5tjo.png"><br><br>  √Ä direita, est√° um gr√°fico da sa√≠da ponderada w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2 da</sub> camada oculta.  Aqui <sub>1</sub> e <sub>2</sub> s√£o as sa√≠das dos neur√¥nios ocultos superior e inferior, respectivamente.  Eles s√£o indicados por "a", como costumam ser chamados de ativa√ß√µes neuronais. <br><br>  A prop√≥sito, notamos que a sa√≠da de toda a rede √© œÉ (w <sub>1</sub> a <sub>1</sub> + w <sub>2</sub> a <sub>2</sub> + b), onde b √© o vi√©s do neur√¥nio de sa√≠da.  Obviamente, isso n√£o √© o mesmo que a sa√≠da ponderada da camada oculta, cujo gr√°fico estamos construindo.  Mas, por enquanto, vamos nos concentrar na sa√≠da equilibrada da camada oculta e s√≥ mais tarde pensar em como ela se relaciona com a sa√≠da de toda a rede. <br><br>  Tente aumentar e diminuir a etapa <sub>1 do</sub> neur√¥nio oculto superior no diagrama interativo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no artigo original</a> .  Veja como isso altera a sa√≠da ponderada da camada oculta.  √â especialmente √∫til entender o que acontece quando s <sub>1</sub> excede s <sub>2</sub> .  Voc√™ ver√° que o gr√°fico nesses casos muda de forma, √† medida que passamos de uma situa√ß√£o na qual o neur√¥nio oculto superior √© ativado primeiro para uma situa√ß√£o na qual o neur√¥nio oculto inferior √© ativado primeiro. <br><br>  Da mesma forma, tente manipular a etapa s <sub>2 do</sub> neur√¥nio oculto inferior e veja como isso altera a produ√ß√£o geral dos neur√¥nios ocultos. <br><br>  Tente reduzir e aumentar os pesos de sa√≠da.  Observe como isso aumenta a contribui√ß√£o dos neur√¥nios ocultos correspondentes.  O que acontece se um dos pesos for igual a 0? <br><br>  Por fim, tente definir w <sub>1</sub> a 0,8 e w <sub>2</sub> a -0,8.  O resultado √© uma fun√ß√£o de ‚Äúprotrus√£o‚Äù, com in√≠cio em s <sub>1</sub> , final em s <sub>2</sub> e altura de 0,8.  Por exemplo, uma sa√≠da ponderada pode ser assim: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/9be/e75/84d9bee755d8a57bbdb3aed20d07da78.jpg"><br><br>  Obviamente, a protrus√£o pode ser dimensionada para qualquer altura.  Vamos usar um par√¢metro, h, denotando altura.  Al√©m disso, por simplicidade, vou me livrar da nota√ß√£o "s <sub>1</sub> = ..." e "w <sub>1</sub> = ...". <br><br><img src="https://habrastorage.org/webt/io/by/7l/ioby7lcd1whqowsw0ak9j1td16q.png"><br><br>  Tente aumentar e diminuir o valor h para ver como a altura da sali√™ncia muda.  Tente fazer h negativo.  Tente alterar os pontos das etapas para observar como isso muda a forma da sali√™ncia. <br><br>  Voc√™ ver√° que usamos nossos neur√¥nios n√£o apenas como primitivas gr√°ficas, mas tamb√©m como unidades mais familiares aos programadores - algo como uma instru√ß√£o if-then-else na programa√ß√£o: <br><br>  se input&gt; = in√≠cio da etapa: <br>  adicione 1 √† sa√≠da ponderada <br>  mais: <br>  adicione 0 √† sa√≠da ponderada <br><br>  Na maior parte, vou me ater √† nota√ß√£o gr√°fica.  No entanto, √†s vezes ser√° √∫til mudar para a exibi√ß√£o "se-ent√£o-outro" e refletir sobre o que est√° acontecendo nesses termos. <br><br>  Podemos usar nosso truque de protrus√£o colando duas partes de neur√¥nios ocultos na mesma rede: <br><br><img src="https://habrastorage.org/webt/4w/4p/pz/4w4ppzryydmyz3f3dglgzcwisfm.png"><br><br>  Aqui eu larguei os pesos simplesmente anotando os valores de h para cada par de neur√¥nios ocultos.  Tente jogar com ambos os valores de h e veja como isso muda o gr√°fico.  Mova as guias, alterando os pontos das etapas. <br><br>  Em um caso mais geral, essa ideia pode ser usada para obter qualquer n√∫mero desejado de picos de qualquer altura.  Em particular, podemos dividir o intervalo [0,1] em um grande n√∫mero de (N) subintervalos e usar N pares de neur√¥nios ocultos para obter picos de qualquer altura desejada.  Vamos ver como isso funciona para N = 5.  Isso j√° √© um monte de neur√¥nios, ent√£o eu sou uma apresenta√ß√£o um pouco mais restrita.  Desculpe pelo diagrama complexo - eu poderia esconder a complexidade por tr√°s de abstra√ß√µes adicionais, mas parece-me que vale um pouco de tormento com complexidade, a fim de sentir melhor como as redes neurais funcionam. <br><br><img src="https://habrastorage.org/webt/do/2t/x-/do2tx-fp-h-w83rnboapp8w-o98.png"><br><br>  Veja bem, temos cinco pares de neur√¥nios ocultos.  Os pontos das etapas dos pares correspondentes est√£o localizados em 0,1 / 5, depois em 1 / 5,2 / 5 e assim por diante, at√© 4 / 5,5 / 5.  Esses valores s√£o fixos - obtemos cinco sali√™ncias de igual largura no gr√°fico. <br><br>  Cada par de neur√¥nios tem um valor h associado a ele.  Lembre-se de que as conex√µes dos neur√¥nios de sa√≠da t√™m pesos heh.  No artigo original no gr√°fico, voc√™ pode clicar nos valores de h e mov√™-los para a esquerda-direita.  Com uma mudan√ßa de altura, a programa√ß√£o tamb√©m muda.  Alterando os pesos de sa√≠da, constru√≠mos a fun√ß√£o final! <br><br>  No diagrama, voc√™ ainda pode clicar no gr√°fico e arrastar a altura das etapas para cima ou para baixo.  Quando voc√™ altera sua altura, v√™ como a altura do h correspondente √© alterada.  Os pesos de sa√≠da + he ‚Äìh mudam de acordo.  Em outras palavras, manipulamos diretamente uma fun√ß√£o cujo gr√°fico √© mostrado √† direita e vemos essas altera√ß√µes nos valores de h √† esquerda.  Voc√™ tamb√©m pode manter pressionado o bot√£o do mouse em uma das sali√™ncias e arrastar o mouse para a esquerda ou direita, e as sali√™ncias ser√£o ajustadas √† altura atual. <br><br>  √â hora de fazer o trabalho. <br><br>  Lembre-se da fun√ß√£o que eu desenhei no in√≠cio do cap√≠tulo: <br><br><img src="https://habrastorage.org/webt/yi/ot/sl/yiotslaplbwh6savahfpzfwdyum.png"><br><br>  Ent√£o eu n√£o mencionei isso, mas na verdade √© assim: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy=&quot;false&quot;>(</mo><mn>15</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy=&quot;false&quot;>(</mo><mn>50</mn><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>113</mn></mrow></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="60.204ex" height="3.021ex" viewBox="0 -987.6 25921 1300.8" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30" x="3236" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2C" x="3736" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-32" x="4181" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2B" x="4904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30" x="5905" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2C" x="6405" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-34" x="6850" y="0"></use><g transform="translate(7351,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-32" x="809" y="583"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2B" x="8599" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30" x="9600" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2C" x="10101" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-33" x="10546" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-78" x="11046" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-73" x="11869" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-69" x="12338" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-6E" x="12684" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-28" x="13284" y="0"></use><g transform="translate(13674,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-78" x="14675" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-29" x="15247" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2B" x="15859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30" x="16860" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-2C" x="17360" y="0"></use><g transform="translate(17805,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-35" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-63" x="19056" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-6F" x="19490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-73" x="19975" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-28" x="20445" y="0"></use><g transform="translate(20834,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-78" x="21835" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-29" x="22408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-74" x="23047" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-61" x="23409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMATHI-67" x="23938" y="0"></use><g transform="translate(24419,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-31" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=https://habr.com/ru/post/461659/&amp;usg=ALkJrhj1i4KOa4v0cvXxy7mi7CA6if7dDw#MJMAIN-33" x="1001" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo>,</mo><mn>2</mn><mo>+</mo><mn>0</mn><mo>,</mo><mn>4</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>0</mn><mo>,</mo><mn>3</mn><mi>x</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mn>15</mn><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mn>0</mn><mo>,</mo><mn>05</mn><mtext>&nbsp;</mtext><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mn>50</mn><mi>x</mi><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>t</mi><mi>a</mi><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mn>113</mn></mrow></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> f (x) = 0,2 + 0,4 x ^ 2 + 0,3x \ sin (15 x) + 0,05 \ cos (50 x) \ tag {113} </script></p><br><br>  Ele √© constru√≠do para valores x de 0 a 1, e os valores ao longo do eixo y variam de 0 a 1. <br><br>  Obviamente, essa fun√ß√£o n√£o √© trivial.  E voc√™ precisa descobrir como calcul√°-lo usando redes neurais. <br><br>  Nas redes neurais acima, analisamos uma combina√ß√£o ponderada - <sub>j</sub> w <sub>j</sub> a <sub>j</sub> da produ√ß√£o de neur√¥nios ocultos.  Sabemos como obter controle significativo sobre esse valor.  Mas, como observei anteriormente, esse valor n√£o √© igual √† sa√≠da da rede.  A sa√≠da da rede √© œÉ (w <sub>j</sub> w <sub>j</sub> a <sub>j</sub> + b), onde b √© o deslocamento do neur√¥nio de sa√≠da.  Podemos obter controle diretamente sobre a sa√≠da da rede? <br><br>  A solu√ß√£o √© desenvolver uma rede neural na qual a sa√≠da ponderada da camada oculta seja dada pela equa√ß√£o œÉ <sup>‚àí1</sup> ‚ãÖ f (x), onde œÉ <sup>‚àí1</sup> √© a fun√ß√£o inversa de œÉ.  Ou seja, queremos que a sa√≠da ponderada da camada oculta seja assim: <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Se isso der certo, a sa√≠da de toda a rede ser√° uma boa aproxima√ß√£o de f (x) (defino o deslocamento do neur√¥nio de sa√≠da como 0). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ent√£o sua tarefa √© desenvolver um NS que se aproxime da fun√ß√£o objetivo mostrada acima. Para entender melhor o que est√° acontecendo, recomendo que voc√™ solucione esse problema duas vezes. Pela primeira vez no </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">artigo original,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> clique no gr√°fico e ajuste diretamente as alturas das diferentes sali√™ncias. Ser√° bastante f√°cil voc√™ obter uma boa aproxima√ß√£o √† fun√ß√£o objetivo. O grau de aproxima√ß√£o √© estimado pelo desvio m√©dio, a diferen√ßa entre a fun√ß√£o objetivo e a fun√ß√£o que a rede calcula. Sua tarefa √© trazer o desvio m√©dio para um valor m√≠nimo. A tarefa √© considerada conclu√≠da quando o desvio m√©dio n√£o excede 0,40.</font></font><br><br><img src="https://habrastorage.org/webt/jb/qh/j8/jbqhj8kul1dtc6o-nyrm0qxhh_0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Depois de obter sucesso, pressione o bot√£o Redefinir, que altera as guias aleatoriamente. Na segunda vez, n√£o toque no gr√°fico, mas altere os valores h no lado esquerdo do diagrama, tentando elevar o desvio m√©dio para um valor de 0,40 ou menos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E assim, voc√™ encontrou todos os elementos necess√°rios para a rede calcular aproximadamente a fun√ß√£o f (x)! A aproxima√ß√£o acabou sendo dif√≠cil, mas podemos melhorar facilmente o resultado simplesmente aumentando o n√∫mero de pares de neur√¥nios ocultos, o que aumentar√° o n√∫mero de sali√™ncias. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Em particular, √© f√°cil transformar todos os dados encontrados de volta na visualiza√ß√£o padr√£o com a parametriza√ß√£o usada para o NS. Deixe-me lembr√°-lo rapidamente de como isso funciona. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Na primeira camada, todos os pesos t√™m um grande valor constante, por exemplo, w = 1000.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Os deslocamentos dos neur√¥nios ocultos s√£o calculados atrav√©s de b = ‚àís. Assim, por exemplo, para o segundo neur√¥nio oculto, s = 0,2 se transforma em b = -1000 √ó 0,2 = ‚àí200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A √∫ltima camada da escala √© determinada pelos valores de h. Assim, por exemplo, o valor escolhido para o primeiro h, h = -0,2, significa que os pesos de sa√≠da dos dois neur√¥nios ocultos superiores s√£o -0,2 e 0,2, respectivamente. E assim por diante, para toda a camada de pesos de sa√≠da. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finalmente, o deslocamento do neur√¥nio de sa√≠da √© 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">E √© isso: temos uma descri√ß√£o completa do NS, que calcula bem a fun√ß√£o objetivo inicial. E entendemos como melhorar a qualidade da aproxima√ß√£o, melhorando o n√∫mero de neur√¥nios ocultos. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Al√©m disso, em nossa fun√ß√£o objetivo original f (x) = 0,2 + 0,4x </font></font><sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sup><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ 0,3sin (15x) + 0,05cos (50x) n√£o √© nada de especial. </font><font style="vertical-align: inherit;">Um procedimento semelhante pode ser usado para qualquer fun√ß√£o cont√≠nua nos intervalos de [0,1] a [0,1]. </font><font style="vertical-align: inherit;">De fato, usamos nosso NS de camada √∫nica para criar uma tabela de pesquisa para uma fun√ß√£o. </font><font style="vertical-align: inherit;">E podemos tomar essa id√©ia como base para obter uma prova generalizada da universalidade.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fun√ß√£o de muitos par√¢metros </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Estendemos nossos resultados ao caso de um conjunto de vari√°veis ‚Äã‚Äãde entrada. Parece complicado, mas todas as id√©ias que precisamos j√° podem ser entendidas para o caso com apenas duas vari√°veis ‚Äã‚Äãrecebidas. Portanto, consideramos o caso com duas vari√°veis ‚Äã‚Äãrecebidas. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos come√ßar examinando o que acontece quando um neur√¥nio tem duas entradas: </font></font><br><br><img src="https://habrastorage.org/webt/k5/cm/a9/k5cma9i-bgfwxnp2ao9h1wiwfz0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Temos entradas x e y, com os pesos correspondentes w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> e deslocamento b do neur√¥nio. Definimos o peso de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> como 0 e brincamos com o primeiro, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , e deslocamos b para ver como eles afetam a sa√≠da do neur√¥nio: </font></font><br><br><img src="https://habrastorage.org/webt/bl/71/6p/bl716pdfanpkwighwk2dc6m20ey.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como voc√™ pode ver, com w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0, a entrada y n√£o afeta a sa√≠da do neur√¥nio. Tudo acontece como se x fosse a √∫nica entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Diante disso, o que voc√™ acha que acontecer√° quando aumentarmos o peso de w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 100 e w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> deixar 0? Se isso n√£o estiver claro imediatamente, pense um pouco sobre esse problema. Assista ao v√≠deo a seguir, que mostra o que vai acontecer:</font></font><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/step_3d.mp4" type="video/mp4"></video></div></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Como antes, com um aumento no peso de entrada, a sa√≠da se aproxima da forma da etapa. A diferen√ßa √© que nossa fun√ß√£o de etapa agora est√° localizada em tr√™s dimens√µes. Como antes, podemos mover a localiza√ß√£o das etapas alterando o deslocamento. O √¢ngulo estar√° no ponto s </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">x</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚â° - b / w1. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos refazer o diagrama para que o par√¢metro seja o local da etapa: </font></font><br><br><img src="https://habrastorage.org/webt/aw/qs/59/awqs59ahvnac-1i9piafzg2jbpi.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Assumimos que o peso de entrada de x seja de grande import√¢ncia - usei w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 1000 - e o peso w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0. O n√∫mero no neur√¥nio √© a posi√ß√£o do passo, e o x acima nos lembra que movemos o passo ao longo do eixo x. Naturalmente, √© bem poss√≠vel obter uma fun√ß√£o de passo ao longo do eixo y, aumentando o peso recebido por y (por exemplo, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">= 1000), e o peso de x √© 0, w </font></font><sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></sub><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> = 0: </font></font><br><br><img src="https://habrastorage.org/webt/xg/zw/y0/xgzwy0jgsj5q1gl3oqgygzqv0nq.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">O n√∫mero no neur√¥nio indica novamente a posi√ß√£o do passo, e y acima dele nos lembra que movemos o passo ao longo do eixo y. Eu poderia designar diretamente os pesos para x e y, mas n√£o o fiz, porque isso iria desarrumar o gr√°fico. Mas lembre-se de que o marcador y indica que o peso de y √© grande e de x √© 0. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Podemos usar as fun√ß√µes de etapa que acabamos de projetar para calcular a fun√ß√£o de protrus√£o tridimensional. Para fazer isso, usamos dois neur√¥nios, cada um dos quais calcular√° uma fun√ß√£o de etapa ao longo do eixo x. Em seguida, combinamos essas fun√ß√µes de etapa com os pesos heh, onde h √© a altura de protrus√£o desejada. Tudo isso pode ser visto no diagrama a seguir:</font></font><br><br><img src="https://habrastorage.org/webt/5s/qn/wk/5sqnwkfmm7_uzs3jchov90ylyz8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tente alterar o valor de h. Veja como isso se relaciona com os pesos da rede. E como ela altera a altura da fun√ß√£o de protrus√£o √† direita. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tente tamb√©m alterar o ponto da etapa, cujo valor √© definido como 0,30 no neur√¥nio oculto superior. Veja como ele muda a forma da sali√™ncia. O que acontece se voc√™ o mover al√©m do ponto 0,70 associado ao neur√¥nio oculto inferior? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Aprendemos como construir a fun√ß√£o de protrus√£o ao longo do eixo x. Naturalmente, podemos facilmente fazer a fun√ß√£o de protrus√£o ao longo do eixo y, usando duas fun√ß√µes de passo ao longo do eixo y. Lembre-se de que podemos fazer isso fazendo grandes pesos na entrada ye definindo o peso 0 na entrada x. E ent√£o, o que acontece:</font></font><br><br><img src="https://habrastorage.org/webt/ic/uu/fq/icuufqisf9gjv8zccnkg0f0bevc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Parece quase id√™ntico √† rede anterior! A √∫nica mudan√ßa vis√≠vel s√£o pequenos marcadores nos neur√¥nios ocultos. Eles nos lembram que produzem fun√ß√µes de passo para y, e n√£o para x, portanto, o peso na entrada y √© muito grande, e na entrada x √© zero, e n√£o vice-versa. Como antes, decidi n√£o mostr√°-lo diretamente, para n√£o confundir a imagem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vamos ver o que acontece se adicionarmos duas fun√ß√µes de protrus√£o, uma ao longo do eixo x, a outra ao longo do eixo y, ambas de altura h: </font></font><br><br><img src="https://habrastorage.org/webt/7f/u7/fc/7fu7fcn8xnl5r4zffk3tpunuedg.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Para simplificar o diagrama de conex√£o com peso zero, eu o omiti. At√© agora, deixei pequenos marcadores x e y em neur√¥nios ocultos para lembrar em quais dire√ß√µes as fun√ß√µes de protrus√£o s√£o calculadas. Mais tarde, n√≥s os recusaremos, pois eles est√£o impl√≠citos na vari√°vel de entrada.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tente alterar o par√¢metro h. </font><font style="vertical-align: inherit;">Como voc√™ pode ver, por causa disso, os pesos de sa√≠da mudam, assim como os pesos de ambas as fun√ß√µes de protrus√£o, xey. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nossa </font></font><br><br><img src="https://habrastorage.org/webt/ad/le/ww/adlewwyzmc3zhrk-fm9a9yvx6zo.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cria√ß√£o √© um </font><font style="vertical-align: inherit;">pouco como uma "fun√ß√£o de torre": </font><font style="vertical-align: inherit;">se podemos criar essas fun√ß√µes de torre, podemos us√°-las para aproximar fun√ß√µes arbitr√°rias simplesmente adicionando torres de v√°rias alturas em lugares diferentes: </font></font><br><br><img src="https://habrastorage.org/webt/u1/lv/xv/u1lvxvdmfi4xxsqgpjabiwofr2k.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">√© claro, ainda n√£o alcan√ßamos a cria√ß√£o de uma fun√ß√£o de torre arbitr√°ria. </font><font style="vertical-align: inherit;">At√© agora, constru√≠mos algo como uma torre central de altura 2h, com um plat√¥ de altura h ao seu redor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mas podemos fazer uma torre funcionar. </font><font style="vertical-align: inherit;">Lembre-se de que mostramos anteriormente como os neur√¥nios podem ser usados ‚Äã‚Äãpara implementar a instru√ß√£o if-then-else:</font></font><br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>  &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Era um neur√¥nio de uma entrada.  E precisamos aplicar uma id√©ia semelhante √† produ√ß√£o combinada de neur√¥nios ocultos: <br><br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>     &gt;= :  <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>:  <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  Se escolhermos o limiar certo - por exemplo, 3h / 2, espremido entre a altura do plat√¥ e a altura da torre central - podemos esmagar o plat√¥ at√© zero e deixar apenas uma torre. <br><br>  Imagine como fazer isso?  Tente experimentar a seguinte rede.  Agora, estamos plotando a sa√≠da de toda a rede, e n√£o apenas a sa√≠da ponderada da camada oculta.  Isso significa que adicionamos o termo de deslocamento √† sa√≠da ponderada da camada oculta e aplicamos o sigm√≥ide.  Voc√™ pode encontrar os valores de he para os quais voc√™ obt√©m uma torre?  Se voc√™ ficar preso nesse momento, aqui est√£o duas dicas: (1) para o neur√¥nio de sa√≠da mostrar um comportamento se-ent√£o-outro, precisamos que os pesos recebidos (todos h ou ‚Äìh) sejam grandes;  (2) o valor de b determina a escala do limiar se-ent√£o-outro. <br><br><img src="https://habrastorage.org/webt/ys/k-/1u/ysk-1uvu-jo68ikk5rqu274u7wc.png"><br><br>  Com par√¢metros padr√£o, a sa√≠da √© semelhante a uma vers√£o nivelada do diagrama anterior, com uma torre e um plat√¥.  Para obter o comportamento desejado, voc√™ precisa aumentar o valor de h.  Isso nos dar√° o comportamento do limite do if-then-else.  Em segundo lugar, para definir corretamente o limite, √© necess√°rio escolher b ‚àí3h / 2. <br><br>  Aqui est√° o que parece para h = 10: <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Seu navegador n√£o suporta v√≠deo HTML5. <source src="http://neuralnetworksanddeeplearning.com/movies/tower_construction.mp4" type="video/mp4"></video></div></div></div><br>  Mesmo para valores relativamente modestos de h, obtemos uma boa fun√ß√£o de torre.  E, √© claro, podemos obter um resultado arbitrariamente bonito aumentando h ainda mais e mantendo o vi√©s no n√≠vel b = ‚àí3h / 2. <br><br>  Vamos tentar colar duas redes para contar duas fun√ß√µes diferentes da torre.  Para esclarecer os respectivos pap√©is das duas sub-redes, coloquei-os em ret√¢ngulos separados: cada um deles calcula a fun√ß√£o da torre usando a t√©cnica descrita acima.  O gr√°fico √† direita mostra a sa√≠da ponderada da segunda camada oculta, ou seja, a combina√ß√£o ponderada das fun√ß√µes da torre. <br><br><img src="https://habrastorage.org/webt/8-/cl/ke/8-clkebo6vphf1-0_jgoaf3exts.png"><br><br>  Em particular, pode-se observar que, alterando o peso na √∫ltima camada, √© poss√≠vel alterar a altura das torres de sa√≠da. <br><br>  A mesma id√©ia permite que voc√™ calcule quantas torres quiser.  Podemos torn√°-los arbitrariamente magros e altos.  Como resultado, garantimos que a sa√≠da ponderada da segunda camada oculta se aproxime de qualquer fun√ß√£o desejada de duas vari√°veis: <br><br><img src="https://habrastorage.org/webt/ig/0u/5z/ig0u5zbzifftdfq4ww4y9a4r7du.png"><br><br>  Em particular, fazendo com que a sa√≠da ponderada da segunda camada oculta se aproxime œÉ <sup>‚àí1</sup> ‚ãÖ de po√ßo, garantimos que a sa√≠da da nossa rede seja uma boa aproxima√ß√£o da fun√ß√£o desejada f. <br><br>  E as fun√ß√µes de muitas vari√°veis? <br><br>  Vamos tentar pegar tr√™s vari√°veis, x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> .  A rede a seguir pode ser usada para calcular a fun√ß√£o da torre em quatro dimens√µes? <br><br><img src="https://habrastorage.org/webt/wg/ki/kn/wgkiknicnzeoaept0d-cemw0sd0.png"><br><br>  Aqui x <sub>1</sub> , x <sub>2</sub> , x <sub>3</sub> denotam a entrada de rede.  s <sub>1</sub> , t <sub>1</sub> e assim por diante - os pontos de refer√™ncia para os neur√¥nios - ou seja, todos os pesos na primeira camada s√£o grandes e as compensa√ß√µes s√£o atribu√≠das para que os pontos de refer√™ncia sejam s <sub>1</sub> , t <sub>1</sub> , s <sub>2</sub> , ... Os pesos na segunda camada se alternam, + h, ‚àíh, onde h √© um n√∫mero muito grande.  O deslocamento da sa√≠da √© de -5h / 2. <br><br>  A rede calcula uma fun√ß√£o igual a 1 em tr√™s condi√ß√µes: x <sub>1</sub> est√° entre s <sub>1</sub> e t <sub>1</sub> ;  x2 est√° entre s2 e t2;  x <sub>3</sub> est√° entre s <sub>3</sub> e t <sub>3</sub> .  A rede √© 0 em todos os outros lugares.  Esta √© uma torre na qual 1 √© uma pequena por√ß√£o do espa√ßo de entrada e 0 √© todo o resto. <br><br>  Ao colar muitas dessas redes, podemos obter quantas torres quisermos e aproximar uma fun√ß√£o arbitr√°ria de tr√™s vari√°veis.  A mesma id√©ia funciona em m dimens√µes.  Somente o deslocamento da sa√≠da (‚àím + 1/2) h √© alterado para comprimir adequadamente os valores desejados e remover o plat√¥. <br><br>  Bem, agora sabemos como usar o NS para aproximar a fun√ß√£o real de muitas vari√°veis.  E as fun√ß√µes vetoriais f (x <sub>1</sub> , ..., x <sub>m</sub> ) ‚àà R <sup>n</sup> ?  Obviamente, essa fun√ß√£o pode ser considerada simplesmente como n fun√ß√µes reais separadas f1 (x <sub>1</sub> , ..., x <sub>m</sub> ), f2 (x <sub>1</sub> , ..., x <sub>m</sub> ) e assim por diante.  E ent√£o apenas colamos todas as redes juntas.  Portanto, √© f√°cil descobrir isso. <br><br><h3>  Desafio </h3><br><ul><li>  Vimos como usar redes neurais com duas camadas ocultas para aproximar uma fun√ß√£o arbitr√°ria.  Voc√™ pode provar que isso √© poss√≠vel com uma camada oculta?  Dica - tente trabalhar com apenas duas vari√°veis ‚Äã‚Äãde sa√≠da e mostre que: (a) √© poss√≠vel obter as fun√ß√µes das etapas n√£o apenas ao longo dos eixos x ou y, mas tamb√©m em uma dire√ß√£o arbitr√°ria;  (b) somando muitas constru√ß√µes da etapa (a), √© poss√≠vel aproximar a fun√ß√£o de uma torre redonda e n√£o retangular;  ¬© usando torres redondas, √© poss√≠vel aproximar uma fun√ß√£o arbitr√°ria.  A etapa ¬© ser√° mais f√°cil de usar usando o material apresentado neste cap√≠tulo, um pouco abaixo. </li></ul><br><h2>  Indo al√©m dos neur√¥nios sigm√≥ides </h2><br>  Provamos que uma rede de neur√¥nios sigm√≥ides pode calcular qualquer fun√ß√£o.  Lembre-se de que em um neur√¥nio sigm√≥ide, as entradas x <sub>1</sub> , x <sub>2</sub> , ... se transformam na sa√≠da em œÉ (w <sub>j</sub> w <sub>j</sub> x <sub>j j</sub> + b), onde w <sub>j</sub> s√£o os pesos, b √© o vi√©s e œÉ √© o sigm√≥ide. <br><br><img src="https://habrastorage.org/webt/0h/ut/93/0hut93wneejtjvxvxiwnfwpmo40.png"><br><br>  E se olharmos para outro tipo de neur√¥nio usando uma fun√ß√£o de ativa√ß√£o diferente, s (z): <br><br><img src="https://habrastorage.org/webt/ua/0-/it/ua0-itpxz-uwkpnptxfsvszqabg.png"><br><br>  Ou seja, assumimos que se um neur√¥nio tem x <sub>1</sub> , x <sub>2</sub> , ... pesos w <sub>1</sub> , w <sub>2</sub> , ... e vi√©s b, ent√£o s (‚àë <sub>j</sub> w <sub>j</sub> x <sub>j</sub> + b) ser√° emitido. <br><br>  N√≥s podemos usar esta fun√ß√£o de ativa√ß√£o para sermos pisados, assim como no caso do sigm√≥ide.  Tente (no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">artigo original</a> ) no diagrama elevar o peso para, digamos, w = 100: <br><br><img src="https://habrastorage.org/webt/vz/-v/mu/vz-vmulc79w1g7xxom_btmnfyow.png"><br><br><img src="https://habrastorage.org/webt/nf/aq/iz/nfaqiznhl2klhebfc33iiy6htrg.png"><br><br>  Como no caso do sigm√≥ide, por causa disso, a fun√ß√£o de ativa√ß√£o √© compactada e, como resultado, se transforma em uma aproxima√ß√£o muito boa da fun√ß√£o de passo.  Tente alterar o deslocamento, e voc√™ ver√° que podemos alterar a localiza√ß√£o da etapa para qualquer.  Portanto, podemos usar todos os mesmos truques de antes para calcular qualquer fun√ß√£o desejada. <br><br>  Quais propriedades s (z) devem ter para que isso funcione?  Precisamos assumir que s (z) est√° bem definido como z ‚Üí ‚àí e z ‚Üí ‚àû.  Esses limites s√£o dois valores aceitos pela nossa fun√ß√£o step.  Tamb√©m precisamos assumir que esses limites s√£o diferentes.  Se eles n√£o diferissem, as etapas n√£o funcionariam; simplesmente haveria um hor√°rio fixo!  Mas se a fun√ß√£o de ativa√ß√£o s (z) satisfaz essas propriedades, os neur√¥nios baseados nela s√£o universalmente adequados para c√°lculos. <br><br><h3>  As tarefas </h3><br><ul><li>  No in√≠cio do livro, encontramos um tipo diferente de neur√¥nio - um neur√¥nio linear endireitado ou uma unidade linear retificada, ReLU.  Explique por que esses neur√¥nios n√£o satisfazem as condi√ß√µes necess√°rias para a universalidade.  Encontre evid√™ncias de versatilidade, mostrando que as ReLUs s√£o universalmente adequadas para computa√ß√£o. </li><li>  Suponha que estamos considerando neur√¥nios lineares, com a fun√ß√£o de ativa√ß√£o s (z) = z.  Explique por que os neur√¥nios lineares n√£o satisfazem as condi√ß√µes da universalidade.  Mostre que esses neur√¥nios n√£o podem ser usados ‚Äã‚Äãpara computa√ß√£o universal. </li></ul><br><h2>  Fun√ß√£o de corre√ß√£o de etapa </h2><br>  Por enquanto, assumimos que nossos neur√¥nios produzem fun√ß√µes precisas de passos.  Esta √© uma boa aproxima√ß√£o, mas apenas uma aproxima√ß√£o.  De fato, h√° uma lacuna estreita de falha, mostrada no gr√°fico a seguir, onde as fun√ß√µes n√£o se comportam como uma fun√ß√£o de etapa: <br><br><img src="https://habrastorage.org/webt/mr/0t/ng/mr0tng4l1giob-gsuhyo_oh_vk0.png"><br><br>  Nesse per√≠odo de fracasso, minha explica√ß√£o da universalidade n√£o funciona. <br><br>  O fracasso n√£o √© t√£o assustador.  Ao definir pesos de entrada suficientemente grandes, podemos reduzir esses espa√ßos arbitrariamente pequenos.  Podemos torn√°-los muito menores do que no gr√°fico, invis√≠veis aos olhos.  Talvez n√£o tenhamos que nos preocupar com esse problema. <br><br>  No entanto, eu gostaria de ter uma maneira de resolv√™-lo. <br><br>  Acontece que √© f√°cil de resolver.  Vamos analisar esta solu√ß√£o para calcular as fun√ß√µes do NS com apenas uma entrada e sa√≠da.  As mesmas id√©ias trabalhar√£o para resolver o problema com um grande n√∫mero de entradas e sa√≠das. <br><br>  Em particular, suponha que queremos que nossa rede calcule alguma fun√ß√£o f.  Como antes, tentamos fazer isso projetando a rede para que a sa√≠da ponderada da camada oculta de neur√¥nios seja œÉ <sup>‚àí1</sup> ‚ãÖ f (x): <br><br><img src="https://habrastorage.org/webt/sk/bu/bw/skbubwnwkrrpukeblqe9a1qo8cw.png"><br><br>  Se fizermos isso usando a t√©cnica descrita acima, for√ßaremos os neur√¥nios ocultos a produzir uma sequ√™ncia de fun√ß√µes de protrus√£o: <br><br><img src="https://habrastorage.org/webt/71/uc/x_/71ucx_26mzx0_isj6dlob9wdacq.png"><br><br>  √â claro que exagerei o tamanho dos intervalos de falha, para que fosse mais f√°cil ver.  Deve ficar claro que, se somarmos todas essas fun√ß√µes das sali√™ncias, obteremos uma aproxima√ß√£o bastante boa de œÉ <sup>‚àí1</sup> ‚ãÖ f (x) em todos os lugares, exceto nos intervalos de falha. <br><br>  Mas suponha que, em vez de usar a aproxima√ß√£o descrita, usamos um conjunto de neur√¥nios ocultos para calcular a aproxima√ß√£o de metade da nossa fun√ß√£o objetivo original, ou seja, œÉ <sup>‚àí1</sup> ‚ãÖ f (x) / 2.  Obviamente, ser√° parecido com uma vers√£o em escala do gr√°fico mais recente: <br><br><img src="https://habrastorage.org/webt/8-/0b/fv/8-0bfvrf5njiwum-w6d8edu4dro.png"><br><br>  E suponha que fa√ßamos mais um conjunto de neur√¥nios ocultos calcular a aproxima√ß√£o de œÉ <sup>‚àí1</sup> ‚ãÖ f (x) / 2, no entanto, em sua base, as sali√™ncias ser√£o deslocadas pela metade de sua largura: <br><br><img src="https://habrastorage.org/webt/dj/cz/7n/djcz7nxhhm98yhiq94uluglvm-m.png"><br><br>  Agora temos duas aproxima√ß√µes diferentes para œÉ - 1‚ãÖf (x) / 2.  Se somarmos essas duas aproxima√ß√µes, obteremos uma aproxima√ß√£o geral para œÉ - 1‚ãÖf (x).  Essa aproxima√ß√£o geral ainda ter√° imprecis√µes em pequenos intervalos.  Mas o problema ser√° menor do que antes - porque os pontos que caem nos intervalos da falha da primeira aproxima√ß√£o n√£o caem nos intervalos da falha da segunda aproxima√ß√£o.  Portanto, a aproxima√ß√£o nesses intervalos ser√° aproximadamente duas vezes melhor. <br><br>  Podemos melhorar a situa√ß√£o adicionando um n√∫mero grande, M, de aproxima√ß√µes sobrepostas da fun√ß√£o œÉ - 1‚ãÖf (x) / M.  Se todos os intervalos de falha forem estreitos o suficiente, qualquer corrente estar√° em apenas um deles.  Se voc√™ usar um n√∫mero suficientemente grande de aproxima√ß√µes sobrepostas de M, o resultado ser√° uma excelente aproxima√ß√£o geral. <br><br><h2>  Conclus√£o </h2><br>  A explica√ß√£o da universalidade discutida aqui definitivamente n√£o pode ser chamada de uma descri√ß√£o pr√°tica de como contar fun√ß√µes usando redes neurais!  Nesse sentido, √© mais uma prova da versatilidade dos port√µes l√≥gicos da NAND e muito mais.  Portanto, eu basicamente tentei tornar esse design claro e f√°cil de seguir, sem otimizar seus detalhes.  No entanto, tentar otimizar esse design pode ser um exerc√≠cio interessante e instrutivo para voc√™. <br><br>  Embora o resultado obtido n√£o possa ser usado diretamente para criar o NS, √© importante porque remove a quest√£o da computabilidade de qualquer fun√ß√£o espec√≠fica usando o NS.  A resposta para essa pergunta sempre ser√° positiva.  Portanto, √© correto perguntar se alguma fun√ß√£o √© comput√°vel, mas qual √© a maneira correta de calcul√°-la. <br><br>  Nosso design universal usa apenas duas camadas ocultas para calcular uma fun√ß√£o arbitr√°ria.  Como discutimos, √© poss√≠vel obter o mesmo resultado com uma √∫nica camada oculta.  Diante disso, voc√™ pode se perguntar por que precisamos de redes profundas, ou seja, redes com um grande n√∫mero de camadas ocultas.  N√£o podemos simplesmente substituir essas redes por redes rasas que possuem uma camada oculta? <br><br>  Embora, em princ√≠pio, seja poss√≠vel, existem boas raz√µes pr√°ticas para o uso de redes neurais profundas.  Conforme descrito no Cap√≠tulo 1, os NSs profundos t√™m uma estrutura hier√°rquica que lhes permite adaptar-se bem ao estudo do conhecimento hier√°rquico, que √© √∫til para resolver problemas reais.  Mais especificamente, ao resolver problemas como o reconhecimento de padr√µes, √© √∫til usar um sistema que compreenda n√£o apenas pixels individuais, mas tamb√©m conceitos cada vez mais complexos: de bordas a formas geom√©tricas simples e al√©m, a cenas complexas envolvendo v√°rios objetos.  Em cap√≠tulos posteriores, veremos evid√™ncias a favor do fato de que NSs profundas ser√£o mais capazes de lidar com o estudo de tais hierarquias de conhecimento do que as rasas.  Resumindo: a universalidade nos diz que o NS pode calcular qualquer fun√ß√£o;  evid√™ncias emp√≠ricas sugerem que NSs profundas s√£o melhor adaptadas ao estudo de fun√ß√µes √∫teis para resolver muitos problemas do mundo real. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt461659/">https://habr.com/ru/post/pt461659/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt461649/index.html">Como vou salvar o mundo</a></li>
<li><a href="../pt461651/index.html">Frontend Weekly Digest (22 a 28 de julho de 2019)</a></li>
<li><a href="../pt461653/index.html">R√°dio definido por software - como funciona? Parte 10</a></li>
<li><a href="../pt461655/index.html">O resumo de materiais frescos do mundo do front-end da √∫ltima semana n ¬∞ 373 (22 a 28 de julho de 2019)</a></li>
<li><a href="../pt461657/index.html">Comprando a Red Hat: Ajudar√° a luta gigante azul pela lideran√ßa em nuvens h√≠bridas</a></li>
<li><a href="../pt461661/index.html">Guia de Desenvolvimento Baseado em Componentes</a></li>
<li><a href="../pt461663/index.html">A hist√≥ria de como o Linux trouxe o Windows</a></li>
<li><a href="../pt461665/index.html">Zen2. A evolu√ß√£o da plataforma AM4 no exemplo da Ryzen 7 3700x</a></li>
<li><a href="../pt461669/index.html">PHP Digest No. 161 (15 a 29 de julho de 2019)</a></li>
<li><a href="../pt461673/index.html">8 dicas para programadores iniciantes ou uma retrospectiva da minha carreira</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>