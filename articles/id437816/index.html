<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧕🏿 👨🏻‍🌾 ⬇️ MPLS ada di mana-mana. Bagaimana infrastruktur jaringan Yandex.Cloud 💇 🌫️ 🦅</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Posting disiapkan oleh: Alexander Virilin xscrew - penulis, kepala layanan infrastruktur jaringan, Leonid Klyuyev - editor 

 Kami terus memperkenalka...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>MPLS ada di mana-mana. Bagaimana infrastruktur jaringan Yandex.Cloud</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/yandex/blog/437816/"> <sup><i>Posting disiapkan oleh: Alexander Virilin <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=" class="user_link">xscrew</a> - penulis, kepala layanan infrastruktur jaringan, Leonid Klyuyev - editor</i></sup> <br><br><img src="https://habrastorage.org/webt/td/uq/e-/tduqe-fvjvebbot1h11mdm0ri9g.png" align="right" width="400">  Kami terus memperkenalkan Anda dengan struktur internal <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Yandex.Cloud</a> .  Hari ini kita akan berbicara tentang jaringan - kami akan memberi tahu Anda bagaimana infrastruktur jaringan bekerja, mengapa menggunakan paradigma MPLS tidak populer untuk pusat data, keputusan rumit apa lagi yang harus kami buat dalam proses membangun jaringan cloud, bagaimana kami mengelolanya, dan jenis pemantauan yang kami gunakan. <br><br>  Jaringan di Cloud terdiri dari tiga lapisan.  Lapisan bawah adalah infrastruktur yang telah disebutkan.  Ini adalah jaringan "besi" fisik di dalam pusat data, antara pusat data dan di tempat-tempat koneksi ke jaringan eksternal.  Jaringan virtual dibangun di atas infrastruktur jaringan, dan layanan jaringan dibangun di atas jaringan virtual.  Struktur ini bukan monolitik: lapisan berpotongan, jaringan virtual dan layanan jaringan berinteraksi langsung dengan infrastruktur jaringan.  Karena jaringan virtual sering disebut overlay, kami biasanya menyebut underlay infrastruktur jaringan. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/lr/c_/kr/lrc_krqlbldrs_spninjomwdekm.png"><br><br>  Sekarang infrastruktur Cloud berbasis di wilayah Tengah Rusia dan mencakup tiga zona akses - yaitu, tiga pusat data independen yang didistribusikan secara geografis.  Independen - independen satu sama lain dalam konteks jaringan, teknik dan sistem kelistrikan, dll. <br><br>  Tentang karakteristik.  Geografi lokasi pusat data sedemikian rupa sehingga waktu pulang-pergi (RTT) waktu pulang-pergi di antara mereka selalu 6-7 ms.  Total kapasitas saluran telah melebihi 10 terabit dan terus tumbuh, karena Yandex memiliki jaringan serat optik di antara zona.  Karena kami tidak menyewakan saluran komunikasi, kami dapat dengan cepat meningkatkan kapasitas strip antara DC: masing-masing menggunakan peralatan multiplexing spektral. <br><br>  Berikut adalah representasi zona yang paling skematis: <br><br><img src="https://habrastorage.org/webt/iw/tz/11/iwtz11yihyj7beyxar1pefzif-4.png"><br><br>  Kenyataannya, pada gilirannya, sedikit berbeda: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/bf/fp/aibffptzbz0jtemoiczeiledcdm.png" width="500"></div><br>  Berikut adalah jaringan tulang punggung Yandex saat ini di wilayah tersebut.  Semua layanan Yandex bekerja di atasnya, bagian dari jaringan digunakan oleh Cloud.  (Ini adalah gambar untuk penggunaan internal, oleh karena itu, informasi layanan sengaja disembunyikan. Namun demikian, dimungkinkan untuk memperkirakan jumlah node dan koneksi.) Keputusan untuk menggunakan jaringan backbone adalah logis: kami tidak dapat menemukan apa pun, tetapi menggunakan kembali infrastruktur saat ini - "menderita" selama bertahun-tahun pembangunan. <br><br>  Apa perbedaan antara gambar pertama dan gambar kedua?  Pertama-tama, zona akses tidak terkait langsung: situs teknis terletak di antara mereka.  Situs tidak mengandung peralatan server - hanya perangkat jaringan untuk memastikan konektivitas ditempatkan pada mereka.  Tempat kehadiran di mana Yandex dan Cloud terhubung dengan dunia luar terhubung ke situs teknis.  Semua titik kehadiran berfungsi untuk seluruh wilayah.  Ngomong-ngomong, penting untuk dicatat bahwa dari sudut pandang akses eksternal dari Internet, semua zona akses Cloud adalah setara.  Dengan kata lain, mereka menyediakan konektivitas yang sama - yaitu, kecepatan dan throughput yang sama, serta latensi yang sama rendahnya. <br><br>  Selain itu, ada peralatan di titik-titik kehadiran, yang - jika ada sumber daya di lokasi dan keinginan untuk memperluas infrastruktur lokal dengan fasilitas cloud - pelanggan dapat terhubung melalui saluran yang dijamin.  Ini dapat dilakukan dengan bantuan mitra atau Anda sendiri. <br><br>  Jaringan inti digunakan oleh Cloud sebagai transportasi MPLS. <br><br><h2>  MPLS </h2><br><img src="https://habrastorage.org/webt/ul/kj/rv/ulkjrvkt7kk2igkl_sbjzivjfxk.png"><br><br>  Multi-label label switching adalah teknologi yang banyak digunakan di industri kami.  Misalnya, ketika sebuah paket ditransfer antara zona akses atau antara zona akses dan Internet, peralatan transit hanya memperhatikan label teratas, “tidak memikirkan” apa yang ada di bawahnya.  Dengan cara ini, MPLS memungkinkan Anda untuk menyembunyikan kompleksitas Cloud dari lapisan transport.  Secara umum, kami di Cloud sangat menyukai MPLS.  Kami bahkan menjadikannya bagian dari level yang lebih rendah dan menggunakannya langsung di pabrik switching di pusat data: <br><br><img src="https://habrastorage.org/webt/k-/iy/hg/k-iyhg3ru8bkto7rqrawug0ivra.png"><br><br>  (Sebenarnya, ada banyak tautan paralel antara sakelar Daun dan Duri.) <br><br><h4>  Mengapa MPLS? </h4><br>  Benar, MPLS sama sekali tidak sering ditemukan di jaringan pusat data.  Seringkali teknologi yang benar-benar berbeda digunakan. <br><br>  Kami menggunakan MPLS karena beberapa alasan.  Pertama, kami merasa nyaman untuk menyatukan teknologi bidang kontrol dan bidang data.  Artinya, alih-alih beberapa protokol di jaringan pusat data, protokol lain di jaringan inti dan persimpangan protokol ini - MPLS tunggal.  Dengan demikian, kami menyatukan tumpukan teknologi dan mengurangi kompleksitas jaringan. <br><br>  Kedua, di Cloud, kami menggunakan berbagai peralatan jaringan, seperti Cloud Gateway dan Network Load Balancer.  Mereka perlu berkomunikasi satu sama lain, mengirim lalu lintas ke Internet dan sebaliknya.  Peralatan jaringan ini dapat diskalakan secara horizontal dengan meningkatnya beban, dan karena Cloud dibangun sesuai dengan model hyperconvergence, mereka dapat diluncurkan secara mutlak di mana saja dari sudut pandang jaringan di pusat data, yaitu, di kumpulan sumber daya bersama. <br><br>  Dengan demikian, peralatan ini dapat mulai di belakang port switch rak mana pun server berada, dan mulai berkomunikasi melalui MPLS dengan seluruh infrastruktur.  Satu-satunya masalah dalam membangun arsitektur seperti itu adalah alarm. <br><br><h2>  Alarm </h2><br>  Tumpukan protokol MPLS klasik cukup kompleks.  Omong-omong, ini adalah salah satu alasan non-proliferasi MPLS di jaringan pusat data. <br><br>  Kami, pada gilirannya, tidak menggunakan IGP (Interior Gateway Protocol), atau LDP (Label Distribution Protocol), atau protokol distribusi label lainnya.  Hanya BGP (Border Gateway Protocol) Label-Unicast yang digunakan.  Setiap alat, yang beroperasi, misalnya, sebagai mesin virtual, membuat sesi BGP sebelum sakelar Leaf-mount rack. <br><br><img src="https://habrastorage.org/webt/z7/-o/03/z7-o03kr2x9-v-yuawoom5xegq4.png"><br><br>  Sesi BGP dibuat di alamat yang sudah diketahui sebelumnya.  Tidak perlu mengkonfigurasi sakelar untuk menjalankan setiap alat.  Semua sakelar sudah dikonfigurasikan sebelumnya dan konsisten. <br><br>  Dalam sesi BGP, setiap alat mengirimkan loopback sendiri dan menerima loopback sisa perangkat yang akan digunakan untuk bertukar lalu lintas.  Contoh perangkat tersebut adalah beberapa jenis reflektor rute, router perbatasan dan peralatan lainnya.  Akibatnya, informasi tentang cara menjangkau satu sama lain muncul di perangkat.  Dari Cloud Gateway melalui sakelar Leaf, sakelar Spine dan jaringan ke router perbatasan, Label Switch Path dibuat.  Switch adalah switch L3 yang berperilaku seperti Label Switch Router dan tidak tahu tentang kerumitan di sekitarnya. <br><br>  MPLS di semua tingkatan jaringan kami, antara lain, telah memungkinkan kami untuk menggunakan konsep Eat your dogfood sendiri. <br><br><h2>  Makanlah makanan anjing Anda sendiri </h2><br>  Dari sudut pandang jaringan, konsep ini menyiratkan bahwa kita hidup dalam infrastruktur yang sama yang kami sediakan untuk pengguna.  Berikut adalah diagram rak di area aksesibilitas: <br><br><img src="https://habrastorage.org/webt/tz/zq/o0/tzzqo08b1pv-whl9mqu9e_xnups.png"><br><br>  Tuan rumah cloud mengambil beban dari pengguna, berisi mesin virtualnya.  Dan secara harfiah, host tetangga di rak dapat membawa beban infrastruktur dari sudut pandang jaringan, termasuk reflektor rute, manajemen, server pemantauan, dll. <br><br>  Mengapa ini dilakukan?  Ada godaan untuk menjalankan reflektor rute dan semua elemen infrastruktur dalam segmen toleran-kesalahan yang terpisah.  Kemudian, jika segmen pengguna rusak di suatu tempat di pusat data, server infrastruktur akan terus mengelola seluruh infrastruktur jaringan.  Tetapi pendekatan ini tampak kejam bagi kami - jika kami tidak mempercayai infrastruktur kami sendiri, lalu bagaimana kami dapat menyediakannya untuk pelanggan kami?  Lagi pula, benar-benar semua Cloud, semua jaringan virtual, pengguna, dan layanan cloud bekerja di atasnya. <br><br>  Karenanya, kami meninggalkan segmen terpisah.  Elemen infrastruktur kami berjalan dalam topologi jaringan dan konektivitas jaringan yang sama.  Secara alami, mereka berjalan dalam tiga tingkat - sama seperti klien kami meluncurkan layanan mereka di Cloud. <br><br><h2>  Pabrik IP / MPLS </h2><br>  Berikut adalah diagram contoh dari salah satu zona ketersediaan: <br><br><img src="https://habrastorage.org/webt/jn/xi/rw/jnxirwsmzj62bwzvldfnrq3n2_4.png"><br><br>  Di setiap zona ketersediaan ada sekitar lima modul, dan di setiap modul sekitar seratus rak.  Sakelar yang dipasang di rak, terhubung ke modul mereka pada level Spine, dan konektivitas antar-modul disediakan melalui jaringan Interkoneksi.  Ini adalah level berikutnya, yang mencakup apa yang disebut sakelar Super-Duri dan Tepi, yang sudah menghubungkan zona akses.  Kami sengaja meninggalkan L2, kami hanya berbicara tentang konektivitas L3 IP / MPLS.  BGP digunakan untuk mendistribusikan informasi routing. <br><br>  Bahkan, ada banyak koneksi paralel daripada di gambar.  Sejumlah besar koneksi ECMP (Equal-cost multi-path) memaksakan persyaratan pemantauan khusus.  Selain itu, ada batas tak terduga, pada pandangan pertama, dalam peralatan - misalnya, jumlah kelompok ECMP. <br><br><h2>  Koneksi server </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xf/ay/jo/xfayjofymnoqjc-1u1otd0axhgm.png"></div><br>  Karena investasi yang kuat, Yandex membangun layanan sedemikian rupa sehingga kegagalan satu server, rak server, modul atau bahkan seluruh pusat data tidak pernah mengarah pada penghentian layanan sepenuhnya.  Jika kita memiliki masalah jaringan apa pun - misalkan sakelar pemasangan dilepas - pengguna eksternal tidak pernah melihat ini. <br><br>  Yandex.Cloud adalah kasus khusus.  Kami tidak dapat menentukan kepada klien bagaimana membangun layanannya sendiri, dan kami memutuskan untuk meratakan kemungkinan titik kegagalan ini.  Oleh karena itu, semua server di Cloud terhubung ke dua sakelar pemasangan di rak. <br><br>  Kami juga tidak menggunakan protokol redundansi di tingkat L2, tetapi segera mulai menggunakan hanya L3 dengan BGP - lagi, karena alasan penyatuan protokol.  Koneksi ini memberikan setiap layanan dengan konektivitas IPv4 dan IPv6: beberapa layanan bekerja melalui IPv4, dan beberapa layanan melalui IPv6. <br><br>  Secara fisik, setiap server terhubung oleh dua antarmuka 25-gigabit.  Ini foto dari pusat data: <br><br><img src="https://habrastorage.org/webt/jz/sr/xj/jzsrxj39equkvhixj3bjoj5kn6a.png"><br><br>  Di sini Anda melihat dua switch rack-mount dengan port 100-gigabit.  Kabel breakout yang berbeda terlihat, membagi port 100-gigabit dari switch menjadi 4 port dengan 25 gigabit per server.  Kami menyebutnya kabel ini "hydra". <br><br><h2>  Manajemen infrastruktur </h2><br>  Infrastruktur jaringan Cloud tidak mengandung solusi manajemen kepemilikan apa pun: semua sistem dapat berupa sumber terbuka dengan penyesuaian untuk Cloud, atau sepenuhnya ditulis sendiri. <br><br><img src="https://habrastorage.org/webt/-g/if/qu/-gifqu8wgzw3xwehpuy5_ejfapc.png"><br><br>  Bagaimana infrastruktur ini dikelola?  Ini bukan yang terlarang di Cloud, tetapi sangat tidak disarankan untuk pergi ke perangkat jaringan dan melakukan penyesuaian apa pun.  Ada keadaan sistem saat ini, dan kita perlu menerapkan perubahan: sampai pada keadaan target baru.  “Jalankan skrip” melalui semua kelenjar, ubah sesuatu dalam konfigurasi - Anda sebaiknya tidak melakukan ini.  Sebagai gantinya, kami membuat perubahan pada templat, ke satu sumber sistem kebenaran, dan melakukan perubahan pada sistem kontrol versi.  Ini sangat mudah, karena Anda selalu dapat melakukan rollback, melihat sejarah, mencari tahu siapa yang bertanggung jawab atas komit, dll. <br><br>  Ketika kami membuat perubahan, konfigurasi dibuat dan kami meluncurkannya ke topologi tes laboratorium.  Dari perspektif jaringan, ini adalah awan kecil yang sepenuhnya mengulangi semua produksi yang ada.  Kami akan segera melihat apakah perubahan yang diinginkan merusak sesuatu: pertama, dengan memonitor, dan kedua, dengan umpan balik dari pengguna internal kami. <br><br>  Jika pemantauan mengatakan bahwa semuanya tenang, maka kami terus meluncurkan - tetapi menerapkan perubahan hanya pada bagian dari topologi (dua atau lebih aksesibilitas "tidak memiliki hak" untuk dipecahkan karena alasan yang sama).  Selain itu, kami terus memantau dengan cermat.  Ini adalah proses yang agak rumit, yang akan kita bicarakan di bawah ini. <br><br>  Setelah memastikan semuanya baik-baik saja, kami menerapkan perubahan pada seluruh produksi.  Kapan saja, Anda dapat memutar kembali dan kembali ke kondisi jaringan sebelumnya, dengan cepat melacak dan memperbaiki masalah. <br><br><h4>  Pemantauan </h4><br>  Kami membutuhkan pemantauan yang berbeda.  Salah satu yang paling dicari adalah memonitor konektivitas end-to-end.  Pada waktu tertentu, setiap server harus dapat berkomunikasi dengan server lain.  Faktanya adalah jika ada masalah di suatu tempat, maka kami ingin mencari tahu di mana sedini mungkin (yaitu, server mana yang memiliki masalah mengakses satu sama lain).  Memastikan konektivitas ujung ke ujung adalah perhatian utama kami. <br><br>  Setiap server mendaftar satu set semua server yang dengannya ia dapat berkomunikasi pada waktu tertentu.  Server mengambil subset acak dari set ini dan mengirimkan paket ICMP, TCP, dan UDP ke semua mesin yang dipilih.  Ini memeriksa apakah ada kerugian pada jaringan, apakah penundaan telah meningkat, dll. Seluruh jaringan dipanggil dalam salah satu zona akses dan di antara mereka.  Hasilnya dikirim ke sistem terpusat yang memvisualisasikannya untuk kami. <br><br>  Beginilah hasilnya ketika semuanya tidak terlalu baik: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yd/-1/bs/yd-1bs7mguqfcy-xhdko674rvu0.png"></div><br>  Di sini Anda dapat melihat segmen jaringan mana yang bermasalah antara (dalam hal ini, A dan B) dan di mana semuanya baik-baik saja (A dan D).  Server tertentu, sakelar yang dipasang di rak, modul, dan seluruh zona ketersediaan dapat ditampilkan di sini.  Jika salah satu di atas menjadi sumber masalah, kita akan melihatnya secara real time. <br><br>  Selain itu, ada pemantauan acara.  Kami memonitor dengan seksama semua koneksi, level sinyal pada transceiver, sesi BGP, dll. Misalkan tiga sesi BGP dibangun dari segmen jaringan, salah satunya terputus di malam hari.  Jika kami mengatur pemantauan sehingga jatuhnya satu sesi BGP tidak penting bagi kami dan dapat menunggu sampai pagi, maka pemantauan tidak akan membangunkan insinyur jaringan.  Tetapi jika sesi kedua dari tiga jatuh, seorang insinyur memanggil secara otomatis. <br><br>  Selain pemantauan End-to-End dan acara, kami menggunakan koleksi log terpusat, analisis real-time dan analisis selanjutnya.  Anda dapat melihat korelasinya, mengidentifikasi masalah dan mencari tahu apa yang terjadi pada peralatan jaringan. <br><br>  Topik pemantauan cukup besar, ada ruang lingkup besar untuk perbaikan.  Saya ingin membawa sistem ke otomatisasi yang lebih besar dan penyembuhan diri yang sejati. <br><br><h2>  Apa selanjutnya </h2><br>  Kami punya banyak rencana.  Penting untuk meningkatkan sistem kontrol, pemantauan, pengalihan IP / pabrik MPLS dan banyak lagi. <br><br>  Kami juga aktif mencari sakelar kotak putih.  Ini adalah perangkat "besi" yang sudah jadi, sebuah sakelar tempat Anda dapat memutar perangkat lunak.  Pertama, jika semuanya dilakukan dengan benar, akan mungkin untuk "memperlakukan" switch dengan cara yang sama seperti ke server, membangun proses CI / CD yang sangat nyaman, secara bertahap meluncurkan konfigurasi, dll. <br><br>  Kedua, jika ada masalah, lebih baik membiarkan sekelompok insinyur dan pengembang yang akan memperbaiki masalah ini daripada menunggu waktu yang lama untuk perbaikan dari vendor. <br><br>  Agar semuanya berjalan lancar, pekerjaan sedang berlangsung dalam dua arah: <br><br><ul><li>  Kami secara signifikan mengurangi kompleksitas pabrik IP / MPLS.  Di satu sisi, tingkat jaringan virtual dan alat otomatisasi dari ini, sebaliknya, menjadi sedikit lebih rumit.  Di sisi lain, jaringan yang mendasari itu sendiri menjadi lebih mudah.  Dengan kata lain, ada "jumlah" kompleksitas tertentu yang tidak dapat diselamatkan.  Ini dapat "dilempar" dari satu level ke level lainnya - misalnya, antara level jaringan atau dari level jaringan ke level aplikasi.  Dan Anda dapat mendistribusikan kompleksitas ini dengan benar, yang kami coba lakukan. </li><li>  Dan tentu saja, kami sedang menyelesaikan serangkaian alat kami untuk mengelola seluruh infrastruktur. </li></ul><br>  Ini semua yang ingin kami bicarakan tentang infrastruktur jaringan kami.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">Berikut ini tautan</a> ke saluran Cloud Telegram dengan berita dan kiat. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id437816/">https://habr.com/ru/post/id437816/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id437806/index.html">EcmaScript 10 - JavaScript Tahun Ini (ES2019)</a></li>
<li><a href="../id437808/index.html">Perf dan flamegraf</a></li>
<li><a href="../id437810/index.html">Realitas perusahaan</a></li>
<li><a href="../id437812/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 dan beta lainnya</a></li>
<li><a href="../id437814/index.html">Xcode 10.2, macOS Mojave 10.14.4, iOS 12.1 dan versi beta lainnya</a></li>
<li><a href="../id437818/index.html">Kami mengajarkan komputer untuk membedakan suara: mengenal kontes DCASE dan merakit klasifikasi audio Anda dalam 30 menit</a></li>
<li><a href="../id437820/index.html">50 nuansa keamanan Drupal</a></li>
<li><a href="../id437824/index.html">Ekstensi 1C universal untuk Google Sheets dan Documents - ambil dan gunakan</a></li>
<li><a href="../id437826/index.html">Bagaimana kami memigrasi database dari Redis dan Riak KV ke PostgreSQL. Bagian 1: proses</a></li>
<li><a href="../id437828/index.html">Buka webinar "SELECT pesanan eksekusi permintaan dan rencana permintaan di MS SQL Server"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>