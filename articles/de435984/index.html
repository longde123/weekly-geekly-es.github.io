<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸ§ ğŸ‘©ğŸ¼â€ğŸ¤â€ğŸ‘¨ğŸ½ ğŸ”— Neuronale Netze und Sprachphilosophie ğŸ˜ ğŸˆ¸ ğŸˆ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Warum Wittgensteins Theorien die Grundlage aller modernen NLP bleiben 

 Die Vektordarstellung von WÃ¶rtern ist vielleicht eine der schÃ¶nsten und roman...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und Sprachphilosophie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435984/"> <font color="gray">Warum Wittgensteins Theorien die Grundlage aller modernen NLP bleiben</font> <br><br>  Die Vektordarstellung von WÃ¶rtern ist vielleicht eine der schÃ¶nsten und romantischsten Ideen in der Geschichte der kÃ¼nstlichen Intelligenz.  Die Philosophie der Sprache ist ein Zweig der Philosophie, der die Beziehung zwischen Sprache und RealitÃ¤t untersucht und wie man Sprache sinnvoll und verstÃ¤ndlich macht.  Eine Vektordarstellung von WÃ¶rtern ist eine sehr spezifische Methode in der modernen Verarbeitung natÃ¼rlicher Sprache (NLP).  In gewissem Sinne ist es ein empirischer Beweis fÃ¼r die Theorien von Ludwig Wittgenstein, einem der wichtigsten Philosophen des letzten Jahrhunderts.  FÃ¼r Wittgenstein ist die Verwendung von WÃ¶rtern ein Schritt in einem sozialen Sprachspiel <i>,</i> das von Community-Mitgliedern gespielt wird, die sich verstehen.  Die Bedeutung eines Wortes hÃ¤ngt nur von seiner NÃ¼tzlichkeit in einem Kontext ab, es entspricht nicht eins zu eins mit einem Objekt aus der realen Welt. <br><br><blockquote>  FÃ¼r eine groÃŸe Klasse von FÃ¤llen, in denen wir das Wort "Bedeutung" verwenden, kann es definiert werden, da die <b>Bedeutung des Wortes seine Verwendung in der Sprache ist</b> . </blockquote><a name="habracut"></a><br>  NatÃ¼rlich ist es sehr schwierig, die genaue Bedeutung eines Wortes zu verstehen.  Es sind viele Aspekte zu berÃ¼cksichtigen: <br><br><ul><li>  auf welches Objekt sich das Wort beziehen kann; </li><li>  Welcher Teil der Sprache ist das? </li><li>  ob es ein idiomatischer Ausdruck ist; </li><li>  alle Bedeutungsschattierungen; </li><li>  usw. </li></ul><br>  All diese Aspekte laufen letztendlich auf eines hinaus: zu wissen, wie man das Wort benutzt. <br><br>  Das Konzept der <i>Bedeutung</i> und warum ein geordneter Satz von Zeichen eine eindeutige Konnotation in der Sprache hat, ist nicht nur eine philosophische Frage, sondern wahrscheinlich auch das grÃ¶ÃŸte Problem, mit dem sich KI-Spezialisten, die mit NLP arbeiten, befassen mÃ¼ssen.  FÃ¼r eine russischsprachige Person ist es ziemlich offensichtlich, dass ein â€Hundâ€œ ein â€Tierâ€œ ist und eher wie eine â€Katzeâ€œ als wie ein â€Delphinâ€œ aussieht, aber diese Aufgabe ist fÃ¼r eine systematische LÃ¶sung alles andere als einfach. <br><br>  Nachdem wir Wittgensteins Theorien leicht korrigiert haben, kÃ¶nnen wir sagen, dass Hunde wie Katzen aussehen, weil sie oft in denselben Kontexten vorkommen: Sie kÃ¶nnen wahrscheinlich Hunde und Katzen finden, die mit den WÃ¶rtern â€Zuhauseâ€œ und â€Gartenâ€œ assoziiert sind, als mit den WÃ¶rtern â€Meerâ€œ. und der "Ozean".  Es ist diese Intuition, die <b>Word2Vec</b> zugrunde <b>liegt</b> , einer der bekanntesten und erfolgreichsten Implementierungen der Vektordarstellung von WÃ¶rtern.  Heutzutage sind Maschinen weit davon entfernt <i>,</i> lange Texte und Passagen wirklich zu <i>verstehen</i> , aber die Vektordarstellung von WÃ¶rtern ist zweifellos die einzige Methode, mit der wir in den letzten zehn Jahren den grÃ¶ÃŸten Schritt in diese Richtung getan haben. <br><br><h1>  Von BoW zu Word2Vec </h1><br>  Bei vielen Computerproblemen besteht das erste Problem darin, die Daten in numerischer Form darzustellen.  WÃ¶rter und SÃ¤tze sind in dieser Form wahrscheinlich am schwierigsten vorstellbar.  In unserem Setup werden <b>D</b> WÃ¶rter aus dem WÃ¶rterbuch ausgewÃ¤hlt, und jedem Wort kann ein numerischer Index <b>i</b> zugewiesen werden. <br><br>  Seit vielen Jahrzehnten wird ein klassischer Ansatz gewÃ¤hlt, um jedes Wort als numerischen D-dimensionalen Vektor aller Nullen mit Ausnahme von Eins an Position i darzustellen.  Betrachten Sie als Beispiel ein WÃ¶rterbuch mit drei WÃ¶rtern: "Hund", "Katze" und "Delphin" (D = 3).  Jedes Wort kann als dreidimensionaler Vektor dargestellt werden: "Hund" entspricht [1,0,0], "Katze" [0,1,0] und "Delphin" offensichtlich [0,0,1].  Das Dokument kann als D-dimensionaler Vektor dargestellt werden, wobei jedes Element die Vorkommen des i-ten Wortes im Dokument zÃ¤hlt.  Dieses Modell heiÃŸt Bag-of-Words (BoW) und wird seit Jahrzehnten verwendet. <br><br>  Trotz des Erfolgs in den 90er Jahren fehlte der BoW die einzig interessante Funktion von WÃ¶rtern: ihre Bedeutung.  Wir wissen, dass zwei sehr unterschiedliche WÃ¶rter Ã¤hnliche Bedeutungen haben kÃ¶nnen, auch wenn sie sich vom Standpunkt der Rechtschreibung vÃ¶llig unterscheiden.  "Katze" und "Hund" sind beide Haustiere, "KÃ¶nig" und "KÃ¶nigin" sind nahe beieinander, "Apfel" und "Zigarette" sind vÃ¶llig unabhÃ¤ngig voneinander.  Wir <i>wissen</i> das, aber im BoW-Modell befinden sich alle diese WÃ¶rter im Vektorraum im gleichen Abstand: 1. <br><br>  Das gleiche Problem gilt fÃ¼r Dokumente: Mit BoW kÃ¶nnen wir schlieÃŸen, dass Dokumente nur dann Ã¤hnlich sind, wenn sie das gleiche Wort eine bestimmte Anzahl von Malen enthalten.  Und hier kommt Word2Vec, das viele philosophische Fragen, die Wittgenstein vor 60 Jahren in seinen <i>Philosophischen Studien</i> erÃ¶rterte, in die Begriffe des maschinellen Lernens einfÃ¼hrt. <br><br>  In einem WÃ¶rterbuch der GrÃ¶ÃŸe D, in dem das Wort durch seinen Index identifiziert wird, besteht das Ziel darin, die N-dimensionale Vektordarstellung jedes Wortes fÃ¼r N &lt;&lt; D zu berechnen.  Idealerweise mÃ¶chten wir, dass es sich um einen dichten Vektor handelt, der einige semantisch spezifische Aspekte der Bedeutung darstellt.  Zum Beispiel mÃ¶chten wir idealerweise, dass "Hund" und "Katze" Ã¤hnliche Darstellungen haben, und "Apfel" und "Zigarette" sind im Vektorraum sehr weit entfernt. <br><br>  Wir mÃ¶chten einige grundlegende algebraische Operationen an Vektoren durchfÃ¼hren, z. B. <code>+âˆ’=</code> .  Ich mÃ¶chte, dass der Abstand zwischen den Vektoren â€Schauspielerâ€œ und â€Schauspielerinâ€œ im Wesentlichen mit dem Abstand zwischen â€Prinzâ€œ und â€Prinzessinâ€œ Ã¼bereinstimmt.  Obwohl diese Ergebnisse ziemlich utopisch sind, zeigen Experimente, dass Word2Vec-Vektoren Eigenschaften aufweisen, die diesen sehr nahe kommen. <br><br>  Word2Vec lernt daraus nicht direkt Ansichten, sondern erhÃ¤lt sie als Nebenprodukt der Klassifizierung ohne Lehrer.  Der durchschnittliche NLP-Wortkorpus-Datensatz besteht aus einer Reihe von SÃ¤tzen.  Jedes Wort in einem Satz erscheint im Kontext der umgebenden WÃ¶rter.  Der Zweck des Klassifikators besteht darin, das Zielwort vorherzusagen, wobei KontextwÃ¶rter als Eingabe betrachtet werden.  FÃ¼r den Satz â€brauner Hund spielt im Gartenâ€œ werden dem Modell die WÃ¶rter [braun, spielt im Garten] als Eingabe zur VerfÃ¼gung gestellt, und sie sollte das Wort â€Hundâ€œ vorhersagen.  Diese Aufgabe wird als Lernen ohne Lehrer betrachtet, da der Korpus nicht mit einer externen Quelle der Wahrheit gekennzeichnet werden muss: Mit einer Reihe von SÃ¤tzen kÃ¶nnen Sie immer automatisch positive und negative Beispiele erstellen.  Wenn wir als positives Beispiel â€brauner Hund, der im Garten spieltâ€œ betrachten, kÃ¶nnen wir viele negative Muster erstellen, wie z. B. â€braune Ebene, die im Garten spieltâ€œ oder â€braune Traube, die im Garten spieltâ€œ, wobei das Zielwort â€Hundâ€œ durch zufÃ¤llige WÃ¶rter aus dem Datensatz ersetzt wird. <br><br>  Und jetzt ist die Anwendung von Wittgensteins Theorien vÃ¶llig klar: Der Kontext ist entscheidend fÃ¼r die Vektordarstellung von WÃ¶rtern, da es wichtig ist, dem Wort in seinen Theorien eine Bedeutung beizumessen.  Wenn zwei WÃ¶rter Ã¤hnliche Bedeutungen haben, haben sie Ã¤hnliche Darstellungen (ein kleiner Abstand im N-dimensionalen Raum), nur weil sie hÃ¤ufig in Ã¤hnlichen Kontexten vorkommen.  Daher haben "Katze" und "Hund" mÃ¶glicherweise enge Vektoren, da sie hÃ¤ufig in denselben Kontexten auftreten. Es ist nÃ¼tzlich, dass das Modell Ã¤hnliche Vektordarstellungen fÃ¼r sie verwendet, da dies das bequemste ist, was sie tun kann. um bessere Ergebnisse bei der Vorhersage von zwei WÃ¶rtern basierend auf ihren Kontexten zu erzielen. <br><br>  Der Originalartikel bietet zwei verschiedene Architekturen: CBOW und Skip-Gramm.  In beiden FÃ¤llen werden verbale Darstellungen zusammen mit einer bestimmten Klassifizierungsaufgabe unterrichtet, um die bestmÃ¶glichen Vektordarstellungen von WÃ¶rtern bereitzustellen, die die Modellleistung maximieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Abbildung 1. Vergleich von CBOW- und Skip-Gramm-Architekturen</font></i> <br><br>  <b>CBOW</b> steht fÃ¼r Continuous Bag of Words und hat die Aufgabe, ein Wort mit Blick auf den Kontext als Eingabe zu erraten.  Die Ein- und AusgÃ¤nge werden als D-dimensionale Vektoren dargestellt, die in einen N-dimensionalen Raum mit gemeinsamen Gewichten projiziert werden.  Wir suchen nur Projektionsgewichte.  TatsÃ¤chlich ist die Vektordarstellung von WÃ¶rtern D Ã— N-Matrizen, wobei jede Zeile ein WÃ¶rterbuchwort darstellt.  Alle KontextwÃ¶rter werden an einer Position projiziert und ihre Vektordarstellungen werden gemittelt.  Daher hat die Wortreihenfolge keinen Einfluss auf das Ergebnis. <br><br>  <b>Skip-Gramm</b> macht dasselbe, aber umgekehrt: Es versucht, KontextwÃ¶rter <b>C</b> vorherzusagen, wobei das Zielwort als Eingabe verwendet wird.  Die Aufgabe, mehrere KontextwÃ¶rter vorherzusagen, kann in eine Reihe unabhÃ¤ngiger binÃ¤rer Klassifizierungsprobleme umformuliert werden, und nun besteht das Ziel darin, das Vorhandensein (oder Fehlen) von KontextwÃ¶rtern vorherzusagen. <br><br>  In der Regel benÃ¶tigt Skip-Gramm mehr Zeit fÃ¼r das Training und liefert hÃ¤ufig etwas bessere Ergebnisse. Wie Ã¼blich haben unterschiedliche Anwendungen unterschiedliche Anforderungen, und es ist schwierig, im Voraus vorherzusagen, welche das beste Ergebnis erzielen.  Trotz der Einfachheit des Konzepts ist das Erlernen dieser Art von Architektur aufgrund der Datenmenge und der Verarbeitungsleistung, die zur Optimierung der Gewichte erforderlich sind, ein wahrer Albtraum.  GlÃ¼cklicherweise finden Sie im Internet einige vorab trainierte Vektordarstellungen von WÃ¶rtern, und Sie kÃ¶nnen den Vektorraum - den interessantesten - mit nur wenigen Zeilen Python-Code untersuchen. <br><br><h1>  MÃ¶gliche Verbesserungen: GloVe und fastText </h1><br>  GegenÃ¼ber dem klassischen Word2Vec wurden in den letzten Jahren viele mÃ¶gliche Verbesserungen vorgeschlagen.  Die beiden interessantesten und am hÃ¤ufigsten verwendeten sind GloVe (Stanford University) und fastText (von Facebook entwickelt).  Sie versuchen, die EinschrÃ¤nkungen des ursprÃ¼nglichen Algorithmus zu identifizieren und zu Ã¼berwinden. <br><br>  In einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">originalen wissenschaftlichen Artikel</a> betonen die Autoren von GloVe, dass das Modelltraining in einem separaten lokalen Kontext die globalen Korpusstatistiken nur unzureichend nutzt.  Der erste Schritt zur Ãœberwindung dieser EinschrÃ¤nkung besteht darin, eine globale Matrix <b>X</b> zu erstellen, in der jedes Element <b>i, j</b> die Anzahl der Verweise auf das Wort <b>j</b> im Kontext des Wortes <b>i zÃ¤hlt</b> .  Die zweite wichtige Idee dieses Dokuments ist das VerstÃ¤ndnis, dass nur Wahrscheinlichkeiten allein nicht ausreichen, um Werte zuverlÃ¤ssig vorherzusagen, und dass auch eine Matrix fÃ¼r das gleichzeitige Auftreten erforderlich ist, aus der bestimmte Aspekte von Werten direkt extrahiert werden kÃ¶nnen. <br><br><blockquote>  Betrachten Sie zwei WÃ¶rter i und j, die von besonderem Interesse sind.  Nehmen wir der VollstÃ¤ndigkeit halber an, dass wir uns fÃ¼r das Konzept eines thermodynamischen Zustands interessieren, fÃ¼r den wir <code>i = </code> und <code>j = </code> .  Die Beziehung dieser WÃ¶rter kann untersucht werden, indem das VerhÃ¤ltnis ihrer Wahrscheinlichkeiten des gemeinsamen Auftretens unter Verwendung verschiedener klingender WÃ¶rter, k, untersucht wird.  FÃ¼r WÃ¶rter k, die sich auf Eis beziehen, aber nicht auf Dampf, sagen wir <code>k = </code> [Feststoff, Materiezustand], erwarten wir, dass das VerhÃ¤ltnis Pik / Pjk grÃ¶ÃŸer sein wird.  In Ã¤hnlicher Weise sollte fÃ¼r die WÃ¶rter k, die mit Dampf verbunden sind, aber nicht mit Eis, sagen wir <code>k = </code> , das VerhÃ¤ltnis klein sein.  FÃ¼r WÃ¶rter wie â€Wasserâ€œ oder â€Modeâ€œ, die entweder gleichermaÃŸen mit Eis und Dampf verwandt sind oder keine Beziehung zu ihnen haben, sollte dieses VerhÃ¤ltnis nahe an der Einheit liegen. </blockquote><br>  Dieses WahrscheinlichkeitsverhÃ¤ltnis wird zum Ausgangspunkt fÃ¼r die Untersuchung der Vektordarstellung von WÃ¶rtern.  Wir wollen Vektoren berechnen kÃ¶nnen, die in Kombination mit einer bestimmten Funktion <b>F</b> dieses VerhÃ¤ltnis im Vektordarstellungsraum konstant halten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Abbildung 2. Die gebrÃ¤uchlichste Formel fÃ¼r die Vektordarstellung von WÃ¶rtern im GloVe-Modell</font></i> <br><br>  Die Funktion F und die AbhÃ¤ngigkeit vom Wort k kÃ¶nnen vereinfacht werden, indem die Exponentiale und festen Offsets ersetzt werden, was die Funktion der Minimierung von Fehlern durch die Methode <b>J</b> der kleinsten Quadrate ergibt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Abbildung 3. Die letzte Funktion zur Berechnung der Vektordarstellung von WÃ¶rtern im GloVe-Modell</font></i> <br><br>  Die <b>f-</b> Funktion ist eine ZÃ¤hlfunktion, die versucht, sehr hÃ¤ufige und seltene Ãœbereinstimmungen nicht zu belasten, wÃ¤hrend <b>bi</b> und <b>bj</b> Offsets sind, um die Symmetrie der Funktion wiederherzustellen.  In den letzten AbsÃ¤tzen des Artikels wird gezeigt, dass sich das Training dieses Modells am Ende nicht sehr vom Training des klassischen Skip-Gramm-Modells unterscheidet, obwohl GloVe in empirischen Tests beiden Word2Vec-Implementierungen Ã¼berlegen ist. <br><br>  Andererseits korrigiert <b>fastText</b> einen vÃ¶llig anderen Nachteil von Word2Vec: Wenn das Modelltraining mit der direkten Codierung eines D-dimensionalen Vektors beginnt, wird die interne Struktur von WÃ¶rtern ignoriert.  Anstatt die Codierung von WÃ¶rtern, die verbale Darstellungen lernen, direkt zu codieren, bietet fastText an, N-Gramm Zeichen zu untersuchen und WÃ¶rter als Summe von N-Gramm-Vektoren darzustellen.  Zum Beispiel wird mit N = 3 das Wort "Blume" als 6 verschiedene 3-Gramm [&lt;fl, flo, low, Debt, wer, er&gt;] plus eine spezielle Sequenz &lt;blume&gt; codiert.  Beachten Sie, wie spitze Klammern verwendet werden, um den Anfang und das Ende eines Wortes anzuzeigen.  Somit wird ein Wort durch seinen Index im WÃ¶rterbuch der WÃ¶rter und die Menge der darin enthaltenen N-Gramme dargestellt, die mit der Hash-Funktion Ganzzahlen zugeordnet werden.  Mit dieser einfachen Verbesserung kÃ¶nnen Sie N-Gramm-Darstellungen zwischen WÃ¶rtern aufteilen und Vektordarstellungen von WÃ¶rtern berechnen, die nicht im Lernfall enthalten waren. <br><br><h1>  Experimente und mÃ¶gliche Anwendungen </h1><br>  Wie bereits erwÃ¤hnt, benÃ¶tigen Sie zur <b>Verwendung</b> dieser Vektordarstellungen nur wenige Zeilen Python-Code.  Ich fÃ¼hrte mehrere Experimente mit dem <a href="">50-dimensionalen GloVe-Modell durch</a> , das auf 6 Milliarden WÃ¶rtern aus Wikipedia-SÃ¤tzen trainiert wurde, sowie mit dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">300-dimensionalen fastText-Modell, das auf Common Crawl trainiert wurde</a> (was 600 Milliarden Token ergab).  Dieser Abschnitt enthÃ¤lt nur Links zu den Ergebnissen beider Experimente, um die Konzepte zu beweisen und ein allgemeines VerstÃ¤ndnis des Themas zu vermitteln. <br><br>  ZunÃ¤chst wollte ich einige grundlegende Ã„hnlichkeiten von WÃ¶rtern Ã¼berprÃ¼fen, die einfachste, aber wichtige Eigenschaft ihrer Vektordarstellung.  Wie erwartet waren die Ã¤hnlichsten WÃ¶rter mit dem Wort "Hund" "Katze" (0,92), "Hunde" (0,85), "Pferd" (0,79), "Welpe" (0,78) und "Haustier" (0,77).  Beachten Sie, dass die Pluralform fast dieselbe Bedeutung hat wie der Singular.  Auch hier ist es fÃ¼r uns ziemlich trivial, das zu sagen, aber fÃ¼r ein Auto ist das Ã¼berhaupt keine Tatsache.  Jetzt Essen: Die Ã¤hnlichsten WÃ¶rter fÃ¼r "Pizza" sind "Sandwich" (0,87), "Sandwiches" (0,86), "Snack" (0,81), "Backwaren" (0,79), "Pommes" (0,79) und "Burger" ( 0,78).  Es ist sinnvoll, die Ergebnisse sind zufriedenstellend und das Modell verhÃ¤lt sich ziemlich gut. <br><br>  Der nÃ¤chste Schritt besteht darin, einige grundlegende Berechnungen im Vektorraum durchzufÃ¼hren und zu Ã¼berprÃ¼fen, wie korrekt das Modell einige wichtige Eigenschaften erhalten hat.  In der Tat ergibt sich als Ergebnis der Berechnung der Vektoren <code>+-</code> das Ergebnis "Schauspielerin" (0,94), und als Ergebnis der Berechnung des <code>+-</code> wird das Wort "KÃ¶nig" (0,86) erhalten.  Wenn der Wert <code>a:b=c:d</code> , sollte das Wort <b>d im</b> Allgemeinen als <code>d=b-a+c</code> .  Auf der nÃ¤chsten Ebene kann man sich nicht vorstellen, wie diese Vektoroperationen Ã¼berhaupt geografische Aspekte beschreiben: Wir wissen, dass Rom die Hauptstadt Italiens ist, da Berlin die Hauptstadt Deutschlands ist, nÃ¤mlich <code>+-= (0.88)</code> und <code>+-= (0.83)</code> . <br><br>  Und jetzt zum lustigen Teil.  Nach der gleichen Idee werden wir versuchen, Konzepte zu addieren und zu subtrahieren.  Was ist zum Beispiel das amerikanische Ã„quivalent von Pizza fÃ¼r Italiener?  <code>+-= (0.60)</code> , dann <code> (0.59)</code> .  Seit ich nach Holland gezogen bin, sage ich immer, dass dieses Land eine Mischung aus drei Dingen ist: ein bisschen amerikanischer Kapitalismus, schwedische KÃ¤lte und LebensqualitÃ¤t und schlieÃŸlich eine Prise neapolitanischer <i>FÃ¼lle</i> .  Indem wir den ursprÃ¼nglichen Satz leicht Ã¤ndern und ein wenig Schweizer PrÃ¤zision entfernen, erhalten wir Holland (0,68) als Ergebnis der <code>++-</code> : ziemlich beeindruckend, um ehrlich zu sein. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Abbildung 4. An alle niederlÃ¤ndischen Leser: Nehmen Sie dies als Kompliment, okay?</font></i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> finden Sie gute praktische Ressourcen, um diese vorab trainierten Vektordarstellungen zu verwenden.  <b>Gensim</b> ist eine einfache und vollstÃ¤ndige Python-Bibliothek mit einigen gebrauchsfertigen algebraischen und Ã¤hnlichen Funktionen.  Diese vorab trainierten Vektordarstellungen kÃ¶nnen auf verschiedene (und nÃ¼tzliche) Arten verwendet werden, um beispielsweise die Leistung von Stimmungsanalysatoren oder Sprachmodellen zu verbessern.  UnabhÃ¤ngig von der Aufgabe wird die Verwendung von N-dimensionalen Vektoren die Effizienz des Modells im Vergleich zur direkten Codierung erheblich verbessern.  NatÃ¼rlich wird das Training in Vektordarstellungen in einem bestimmten Bereich das Ergebnis weiter verbessern, aber dies kann mÃ¶glicherweise Ã¼bermÃ¤ÃŸigen Aufwand und Zeit erfordern. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de435984/">https://habr.com/ru/post/de435984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de435970/index.html">AnfÃ¤ngerhandbuch zur Webserverentwicklung mit Node.js.</a></li>
<li><a href="../de435972/index.html">EinfÃ¼hrung der reaktiven Programmierung im FrÃ¼hjahr</a></li>
<li><a href="../de435974/index.html">Three.js - Steuerelemente fÃ¼r Weltraum oder Planetarium</a></li>
<li><a href="../de435976/index.html">WebAssembly in der Produktion und das â€Minenfeldâ€œ von Smart TV: ein Interview mit Andrei Nagih</a></li>
<li><a href="../de435978/index.html">Problemumgehungen fÃ¼r den biometrischen Schutz</a></li>
<li><a href="../de435986/index.html">Windows reserviert 7 GB fÃ¼r Systemaktualisierungen, um zu vermeiden, dass der Festplattenspeicher knapp wird</a></li>
<li><a href="../de435988/index.html">Eine EinfÃ¼hrung in Anmerkungen vom Typ Python. Fortsetzung</a></li>
<li><a href="../de435990/index.html">Wie mache ich einen Wechsel?</a></li>
<li><a href="../de435992/index.html">Spieler Fallout 76, die an einem geheimen Ort von Entwicklern gefangen werden, werden gesperrt</a></li>
<li><a href="../de435994/index.html">Ist es Karma, Baby, oder warum der Angriff auf drahtlose Netzwerke, der in Vergessenheit geraten sollte, noch am Leben ist?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>