<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🧝 👩🏼‍🤝‍👨🏽 🔗 Neuronale Netze und Sprachphilosophie 😍 🈸 🏈</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Warum Wittgensteins Theorien die Grundlage aller modernen NLP bleiben 

 Die Vektordarstellung von Wörtern ist vielleicht eine der schönsten und roman...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Neuronale Netze und Sprachphilosophie</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/435984/"> <font color="gray">Warum Wittgensteins Theorien die Grundlage aller modernen NLP bleiben</font> <br><br>  Die Vektordarstellung von Wörtern ist vielleicht eine der schönsten und romantischsten Ideen in der Geschichte der künstlichen Intelligenz.  Die Philosophie der Sprache ist ein Zweig der Philosophie, der die Beziehung zwischen Sprache und Realität untersucht und wie man Sprache sinnvoll und verständlich macht.  Eine Vektordarstellung von Wörtern ist eine sehr spezifische Methode in der modernen Verarbeitung natürlicher Sprache (NLP).  In gewissem Sinne ist es ein empirischer Beweis für die Theorien von Ludwig Wittgenstein, einem der wichtigsten Philosophen des letzten Jahrhunderts.  Für Wittgenstein ist die Verwendung von Wörtern ein Schritt in einem sozialen Sprachspiel <i>,</i> das von Community-Mitgliedern gespielt wird, die sich verstehen.  Die Bedeutung eines Wortes hängt nur von seiner Nützlichkeit in einem Kontext ab, es entspricht nicht eins zu eins mit einem Objekt aus der realen Welt. <br><br><blockquote>  Für eine große Klasse von Fällen, in denen wir das Wort "Bedeutung" verwenden, kann es definiert werden, da die <b>Bedeutung des Wortes seine Verwendung in der Sprache ist</b> . </blockquote><a name="habracut"></a><br>  Natürlich ist es sehr schwierig, die genaue Bedeutung eines Wortes zu verstehen.  Es sind viele Aspekte zu berücksichtigen: <br><br><ul><li>  auf welches Objekt sich das Wort beziehen kann; </li><li>  Welcher Teil der Sprache ist das? </li><li>  ob es ein idiomatischer Ausdruck ist; </li><li>  alle Bedeutungsschattierungen; </li><li>  usw. </li></ul><br>  All diese Aspekte laufen letztendlich auf eines hinaus: zu wissen, wie man das Wort benutzt. <br><br>  Das Konzept der <i>Bedeutung</i> und warum ein geordneter Satz von Zeichen eine eindeutige Konnotation in der Sprache hat, ist nicht nur eine philosophische Frage, sondern wahrscheinlich auch das größte Problem, mit dem sich KI-Spezialisten, die mit NLP arbeiten, befassen müssen.  Für eine russischsprachige Person ist es ziemlich offensichtlich, dass ein „Hund“ ein „Tier“ ist und eher wie eine „Katze“ als wie ein „Delphin“ aussieht, aber diese Aufgabe ist für eine systematische Lösung alles andere als einfach. <br><br>  Nachdem wir Wittgensteins Theorien leicht korrigiert haben, können wir sagen, dass Hunde wie Katzen aussehen, weil sie oft in denselben Kontexten vorkommen: Sie können wahrscheinlich Hunde und Katzen finden, die mit den Wörtern „Zuhause“ und „Garten“ assoziiert sind, als mit den Wörtern „Meer“. und der "Ozean".  Es ist diese Intuition, die <b>Word2Vec</b> zugrunde <b>liegt</b> , einer der bekanntesten und erfolgreichsten Implementierungen der Vektordarstellung von Wörtern.  Heutzutage sind Maschinen weit davon entfernt <i>,</i> lange Texte und Passagen wirklich zu <i>verstehen</i> , aber die Vektordarstellung von Wörtern ist zweifellos die einzige Methode, mit der wir in den letzten zehn Jahren den größten Schritt in diese Richtung getan haben. <br><br><h1>  Von BoW zu Word2Vec </h1><br>  Bei vielen Computerproblemen besteht das erste Problem darin, die Daten in numerischer Form darzustellen.  Wörter und Sätze sind in dieser Form wahrscheinlich am schwierigsten vorstellbar.  In unserem Setup werden <b>D</b> Wörter aus dem Wörterbuch ausgewählt, und jedem Wort kann ein numerischer Index <b>i</b> zugewiesen werden. <br><br>  Seit vielen Jahrzehnten wird ein klassischer Ansatz gewählt, um jedes Wort als numerischen D-dimensionalen Vektor aller Nullen mit Ausnahme von Eins an Position i darzustellen.  Betrachten Sie als Beispiel ein Wörterbuch mit drei Wörtern: "Hund", "Katze" und "Delphin" (D = 3).  Jedes Wort kann als dreidimensionaler Vektor dargestellt werden: "Hund" entspricht [1,0,0], "Katze" [0,1,0] und "Delphin" offensichtlich [0,0,1].  Das Dokument kann als D-dimensionaler Vektor dargestellt werden, wobei jedes Element die Vorkommen des i-ten Wortes im Dokument zählt.  Dieses Modell heißt Bag-of-Words (BoW) und wird seit Jahrzehnten verwendet. <br><br>  Trotz des Erfolgs in den 90er Jahren fehlte der BoW die einzig interessante Funktion von Wörtern: ihre Bedeutung.  Wir wissen, dass zwei sehr unterschiedliche Wörter ähnliche Bedeutungen haben können, auch wenn sie sich vom Standpunkt der Rechtschreibung völlig unterscheiden.  "Katze" und "Hund" sind beide Haustiere, "König" und "Königin" sind nahe beieinander, "Apfel" und "Zigarette" sind völlig unabhängig voneinander.  Wir <i>wissen</i> das, aber im BoW-Modell befinden sich alle diese Wörter im Vektorraum im gleichen Abstand: 1. <br><br>  Das gleiche Problem gilt für Dokumente: Mit BoW können wir schließen, dass Dokumente nur dann ähnlich sind, wenn sie das gleiche Wort eine bestimmte Anzahl von Malen enthalten.  Und hier kommt Word2Vec, das viele philosophische Fragen, die Wittgenstein vor 60 Jahren in seinen <i>Philosophischen Studien</i> erörterte, in die Begriffe des maschinellen Lernens einführt. <br><br>  In einem Wörterbuch der Größe D, in dem das Wort durch seinen Index identifiziert wird, besteht das Ziel darin, die N-dimensionale Vektordarstellung jedes Wortes für N &lt;&lt; D zu berechnen.  Idealerweise möchten wir, dass es sich um einen dichten Vektor handelt, der einige semantisch spezifische Aspekte der Bedeutung darstellt.  Zum Beispiel möchten wir idealerweise, dass "Hund" und "Katze" ähnliche Darstellungen haben, und "Apfel" und "Zigarette" sind im Vektorraum sehr weit entfernt. <br><br>  Wir möchten einige grundlegende algebraische Operationen an Vektoren durchführen, z. B. <code>+−=</code> .  Ich möchte, dass der Abstand zwischen den Vektoren „Schauspieler“ und „Schauspielerin“ im Wesentlichen mit dem Abstand zwischen „Prinz“ und „Prinzessin“ übereinstimmt.  Obwohl diese Ergebnisse ziemlich utopisch sind, zeigen Experimente, dass Word2Vec-Vektoren Eigenschaften aufweisen, die diesen sehr nahe kommen. <br><br>  Word2Vec lernt daraus nicht direkt Ansichten, sondern erhält sie als Nebenprodukt der Klassifizierung ohne Lehrer.  Der durchschnittliche NLP-Wortkorpus-Datensatz besteht aus einer Reihe von Sätzen.  Jedes Wort in einem Satz erscheint im Kontext der umgebenden Wörter.  Der Zweck des Klassifikators besteht darin, das Zielwort vorherzusagen, wobei Kontextwörter als Eingabe betrachtet werden.  Für den Satz „brauner Hund spielt im Garten“ werden dem Modell die Wörter [braun, spielt im Garten] als Eingabe zur Verfügung gestellt, und sie sollte das Wort „Hund“ vorhersagen.  Diese Aufgabe wird als Lernen ohne Lehrer betrachtet, da der Korpus nicht mit einer externen Quelle der Wahrheit gekennzeichnet werden muss: Mit einer Reihe von Sätzen können Sie immer automatisch positive und negative Beispiele erstellen.  Wenn wir als positives Beispiel „brauner Hund, der im Garten spielt“ betrachten, können wir viele negative Muster erstellen, wie z. B. „braune Ebene, die im Garten spielt“ oder „braune Traube, die im Garten spielt“, wobei das Zielwort „Hund“ durch zufällige Wörter aus dem Datensatz ersetzt wird. <br><br>  Und jetzt ist die Anwendung von Wittgensteins Theorien völlig klar: Der Kontext ist entscheidend für die Vektordarstellung von Wörtern, da es wichtig ist, dem Wort in seinen Theorien eine Bedeutung beizumessen.  Wenn zwei Wörter ähnliche Bedeutungen haben, haben sie ähnliche Darstellungen (ein kleiner Abstand im N-dimensionalen Raum), nur weil sie häufig in ähnlichen Kontexten vorkommen.  Daher haben "Katze" und "Hund" möglicherweise enge Vektoren, da sie häufig in denselben Kontexten auftreten. Es ist nützlich, dass das Modell ähnliche Vektordarstellungen für sie verwendet, da dies das bequemste ist, was sie tun kann. um bessere Ergebnisse bei der Vorhersage von zwei Wörtern basierend auf ihren Kontexten zu erzielen. <br><br>  Der Originalartikel bietet zwei verschiedene Architekturen: CBOW und Skip-Gramm.  In beiden Fällen werden verbale Darstellungen zusammen mit einer bestimmten Klassifizierungsaufgabe unterrichtet, um die bestmöglichen Vektordarstellungen von Wörtern bereitzustellen, die die Modellleistung maximieren. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b9/e32/ee7/2b9e32ee767ad0c402e214a566d848a0.png"><br>  <i><font color="gray">Abbildung 1. Vergleich von CBOW- und Skip-Gramm-Architekturen</font></i> <br><br>  <b>CBOW</b> steht für Continuous Bag of Words und hat die Aufgabe, ein Wort mit Blick auf den Kontext als Eingabe zu erraten.  Die Ein- und Ausgänge werden als D-dimensionale Vektoren dargestellt, die in einen N-dimensionalen Raum mit gemeinsamen Gewichten projiziert werden.  Wir suchen nur Projektionsgewichte.  Tatsächlich ist die Vektordarstellung von Wörtern D × N-Matrizen, wobei jede Zeile ein Wörterbuchwort darstellt.  Alle Kontextwörter werden an einer Position projiziert und ihre Vektordarstellungen werden gemittelt.  Daher hat die Wortreihenfolge keinen Einfluss auf das Ergebnis. <br><br>  <b>Skip-Gramm</b> macht dasselbe, aber umgekehrt: Es versucht, Kontextwörter <b>C</b> vorherzusagen, wobei das Zielwort als Eingabe verwendet wird.  Die Aufgabe, mehrere Kontextwörter vorherzusagen, kann in eine Reihe unabhängiger binärer Klassifizierungsprobleme umformuliert werden, und nun besteht das Ziel darin, das Vorhandensein (oder Fehlen) von Kontextwörtern vorherzusagen. <br><br>  In der Regel benötigt Skip-Gramm mehr Zeit für das Training und liefert häufig etwas bessere Ergebnisse. Wie üblich haben unterschiedliche Anwendungen unterschiedliche Anforderungen, und es ist schwierig, im Voraus vorherzusagen, welche das beste Ergebnis erzielen.  Trotz der Einfachheit des Konzepts ist das Erlernen dieser Art von Architektur aufgrund der Datenmenge und der Verarbeitungsleistung, die zur Optimierung der Gewichte erforderlich sind, ein wahrer Albtraum.  Glücklicherweise finden Sie im Internet einige vorab trainierte Vektordarstellungen von Wörtern, und Sie können den Vektorraum - den interessantesten - mit nur wenigen Zeilen Python-Code untersuchen. <br><br><h1>  Mögliche Verbesserungen: GloVe und fastText </h1><br>  Gegenüber dem klassischen Word2Vec wurden in den letzten Jahren viele mögliche Verbesserungen vorgeschlagen.  Die beiden interessantesten und am häufigsten verwendeten sind GloVe (Stanford University) und fastText (von Facebook entwickelt).  Sie versuchen, die Einschränkungen des ursprünglichen Algorithmus zu identifizieren und zu überwinden. <br><br>  In einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">originalen wissenschaftlichen Artikel</a> betonen die Autoren von GloVe, dass das Modelltraining in einem separaten lokalen Kontext die globalen Korpusstatistiken nur unzureichend nutzt.  Der erste Schritt zur Überwindung dieser Einschränkung besteht darin, eine globale Matrix <b>X</b> zu erstellen, in der jedes Element <b>i, j</b> die Anzahl der Verweise auf das Wort <b>j</b> im Kontext des Wortes <b>i zählt</b> .  Die zweite wichtige Idee dieses Dokuments ist das Verständnis, dass nur Wahrscheinlichkeiten allein nicht ausreichen, um Werte zuverlässig vorherzusagen, und dass auch eine Matrix für das gleichzeitige Auftreten erforderlich ist, aus der bestimmte Aspekte von Werten direkt extrahiert werden können. <br><br><blockquote>  Betrachten Sie zwei Wörter i und j, die von besonderem Interesse sind.  Nehmen wir der Vollständigkeit halber an, dass wir uns für das Konzept eines thermodynamischen Zustands interessieren, für den wir <code>i = </code> und <code>j = </code> .  Die Beziehung dieser Wörter kann untersucht werden, indem das Verhältnis ihrer Wahrscheinlichkeiten des gemeinsamen Auftretens unter Verwendung verschiedener klingender Wörter, k, untersucht wird.  Für Wörter k, die sich auf Eis beziehen, aber nicht auf Dampf, sagen wir <code>k = </code> [Feststoff, Materiezustand], erwarten wir, dass das Verhältnis Pik / Pjk größer sein wird.  In ähnlicher Weise sollte für die Wörter k, die mit Dampf verbunden sind, aber nicht mit Eis, sagen wir <code>k = </code> , das Verhältnis klein sein.  Für Wörter wie „Wasser“ oder „Mode“, die entweder gleichermaßen mit Eis und Dampf verwandt sind oder keine Beziehung zu ihnen haben, sollte dieses Verhältnis nahe an der Einheit liegen. </blockquote><br>  Dieses Wahrscheinlichkeitsverhältnis wird zum Ausgangspunkt für die Untersuchung der Vektordarstellung von Wörtern.  Wir wollen Vektoren berechnen können, die in Kombination mit einer bestimmten Funktion <b>F</b> dieses Verhältnis im Vektordarstellungsraum konstant halten. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4dc/d5d/137/4dcd5d13763ca26ad997565ec2e6e513.jpg"></div><br>  <i><font color="gray">Abbildung 2. Die gebräuchlichste Formel für die Vektordarstellung von Wörtern im GloVe-Modell</font></i> <br><br>  Die Funktion F und die Abhängigkeit vom Wort k können vereinfacht werden, indem die Exponentiale und festen Offsets ersetzt werden, was die Funktion der Minimierung von Fehlern durch die Methode <b>J</b> der kleinsten Quadrate ergibt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e48/049/738/e48049738a71657b998f5630dac792c4.jpg"></div><br>  <i><font color="gray">Abbildung 3. Die letzte Funktion zur Berechnung der Vektordarstellung von Wörtern im GloVe-Modell</font></i> <br><br>  Die <b>f-</b> Funktion ist eine Zählfunktion, die versucht, sehr häufige und seltene Übereinstimmungen nicht zu belasten, während <b>bi</b> und <b>bj</b> Offsets sind, um die Symmetrie der Funktion wiederherzustellen.  In den letzten Absätzen des Artikels wird gezeigt, dass sich das Training dieses Modells am Ende nicht sehr vom Training des klassischen Skip-Gramm-Modells unterscheidet, obwohl GloVe in empirischen Tests beiden Word2Vec-Implementierungen überlegen ist. <br><br>  Andererseits korrigiert <b>fastText</b> einen völlig anderen Nachteil von Word2Vec: Wenn das Modelltraining mit der direkten Codierung eines D-dimensionalen Vektors beginnt, wird die interne Struktur von Wörtern ignoriert.  Anstatt die Codierung von Wörtern, die verbale Darstellungen lernen, direkt zu codieren, bietet fastText an, N-Gramm Zeichen zu untersuchen und Wörter als Summe von N-Gramm-Vektoren darzustellen.  Zum Beispiel wird mit N = 3 das Wort "Blume" als 6 verschiedene 3-Gramm [&lt;fl, flo, low, Debt, wer, er&gt;] plus eine spezielle Sequenz &lt;blume&gt; codiert.  Beachten Sie, wie spitze Klammern verwendet werden, um den Anfang und das Ende eines Wortes anzuzeigen.  Somit wird ein Wort durch seinen Index im Wörterbuch der Wörter und die Menge der darin enthaltenen N-Gramme dargestellt, die mit der Hash-Funktion Ganzzahlen zugeordnet werden.  Mit dieser einfachen Verbesserung können Sie N-Gramm-Darstellungen zwischen Wörtern aufteilen und Vektordarstellungen von Wörtern berechnen, die nicht im Lernfall enthalten waren. <br><br><h1>  Experimente und mögliche Anwendungen </h1><br>  Wie bereits erwähnt, benötigen Sie zur <b>Verwendung</b> dieser Vektordarstellungen nur wenige Zeilen Python-Code.  Ich führte mehrere Experimente mit dem <a href="">50-dimensionalen GloVe-Modell durch</a> , das auf 6 Milliarden Wörtern aus Wikipedia-Sätzen trainiert wurde, sowie mit dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">300-dimensionalen fastText-Modell, das auf Common Crawl trainiert wurde</a> (was 600 Milliarden Token ergab).  Dieser Abschnitt enthält nur Links zu den Ergebnissen beider Experimente, um die Konzepte zu beweisen und ein allgemeines Verständnis des Themas zu vermitteln. <br><br>  Zunächst wollte ich einige grundlegende Ähnlichkeiten von Wörtern überprüfen, die einfachste, aber wichtige Eigenschaft ihrer Vektordarstellung.  Wie erwartet waren die ähnlichsten Wörter mit dem Wort "Hund" "Katze" (0,92), "Hunde" (0,85), "Pferd" (0,79), "Welpe" (0,78) und "Haustier" (0,77).  Beachten Sie, dass die Pluralform fast dieselbe Bedeutung hat wie der Singular.  Auch hier ist es für uns ziemlich trivial, das zu sagen, aber für ein Auto ist das überhaupt keine Tatsache.  Jetzt Essen: Die ähnlichsten Wörter für "Pizza" sind "Sandwich" (0,87), "Sandwiches" (0,86), "Snack" (0,81), "Backwaren" (0,79), "Pommes" (0,79) und "Burger" ( 0,78).  Es ist sinnvoll, die Ergebnisse sind zufriedenstellend und das Modell verhält sich ziemlich gut. <br><br>  Der nächste Schritt besteht darin, einige grundlegende Berechnungen im Vektorraum durchzuführen und zu überprüfen, wie korrekt das Modell einige wichtige Eigenschaften erhalten hat.  In der Tat ergibt sich als Ergebnis der Berechnung der Vektoren <code>+-</code> das Ergebnis "Schauspielerin" (0,94), und als Ergebnis der Berechnung des <code>+-</code> wird das Wort "König" (0,86) erhalten.  Wenn der Wert <code>a:b=c:d</code> , sollte das Wort <b>d im</b> Allgemeinen als <code>d=b-a+c</code> .  Auf der nächsten Ebene kann man sich nicht vorstellen, wie diese Vektoroperationen überhaupt geografische Aspekte beschreiben: Wir wissen, dass Rom die Hauptstadt Italiens ist, da Berlin die Hauptstadt Deutschlands ist, nämlich <code>+-= (0.88)</code> und <code>+-= (0.83)</code> . <br><br>  Und jetzt zum lustigen Teil.  Nach der gleichen Idee werden wir versuchen, Konzepte zu addieren und zu subtrahieren.  Was ist zum Beispiel das amerikanische Äquivalent von Pizza für Italiener?  <code>+-= (0.60)</code> , dann <code> (0.59)</code> .  Seit ich nach Holland gezogen bin, sage ich immer, dass dieses Land eine Mischung aus drei Dingen ist: ein bisschen amerikanischer Kapitalismus, schwedische Kälte und Lebensqualität und schließlich eine Prise neapolitanischer <i>Fülle</i> .  Indem wir den ursprünglichen Satz leicht ändern und ein wenig Schweizer Präzision entfernen, erhalten wir Holland (0,68) als Ergebnis der <code>++-</code> : ziemlich beeindruckend, um ehrlich zu sein. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fd8/f76/e41/fd8f76e41d78598394d1613652f44f26.png"><br>  <i><font color="gray">Abbildung 4. An alle niederländischen Leser: Nehmen Sie dies als Kompliment, okay?</font></i> <br><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Hier</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> finden Sie gute praktische Ressourcen, um diese vorab trainierten Vektordarstellungen zu verwenden.  <b>Gensim</b> ist eine einfache und vollständige Python-Bibliothek mit einigen gebrauchsfertigen algebraischen und ähnlichen Funktionen.  Diese vorab trainierten Vektordarstellungen können auf verschiedene (und nützliche) Arten verwendet werden, um beispielsweise die Leistung von Stimmungsanalysatoren oder Sprachmodellen zu verbessern.  Unabhängig von der Aufgabe wird die Verwendung von N-dimensionalen Vektoren die Effizienz des Modells im Vergleich zur direkten Codierung erheblich verbessern.  Natürlich wird das Training in Vektordarstellungen in einem bestimmten Bereich das Ergebnis weiter verbessern, aber dies kann möglicherweise übermäßigen Aufwand und Zeit erfordern. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de435984/">https://habr.com/ru/post/de435984/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de435970/index.html">Anfängerhandbuch zur Webserverentwicklung mit Node.js.</a></li>
<li><a href="../de435972/index.html">Einführung der reaktiven Programmierung im Frühjahr</a></li>
<li><a href="../de435974/index.html">Three.js - Steuerelemente für Weltraum oder Planetarium</a></li>
<li><a href="../de435976/index.html">WebAssembly in der Produktion und das „Minenfeld“ von Smart TV: ein Interview mit Andrei Nagih</a></li>
<li><a href="../de435978/index.html">Problemumgehungen für den biometrischen Schutz</a></li>
<li><a href="../de435986/index.html">Windows reserviert 7 GB für Systemaktualisierungen, um zu vermeiden, dass der Festplattenspeicher knapp wird</a></li>
<li><a href="../de435988/index.html">Eine Einführung in Anmerkungen vom Typ Python. Fortsetzung</a></li>
<li><a href="../de435990/index.html">Wie mache ich einen Wechsel?</a></li>
<li><a href="../de435992/index.html">Spieler Fallout 76, die an einem geheimen Ort von Entwicklern gefangen werden, werden gesperrt</a></li>
<li><a href="../de435994/index.html">Ist es Karma, Baby, oder warum der Angriff auf drahtlose Netzwerke, der in Vergessenheit geraten sollte, noch am Leben ist?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>