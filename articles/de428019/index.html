<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§∫ üïµÔ∏è üìé Spielen Sie Mortal Kombat mit TensorFlow.js üë®üèº‚Äçüé® üë®üèø‚ÄçüöÄ ‚õÖÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Als ich mit Verbesserungen f√ºr das Prognosemodell von Guess.js experimentierte, begann ich mich eingehend mit Deep Learning zu befassen: wiederkehrend...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Spielen Sie Mortal Kombat mit TensorFlow.js</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/">  Als ich mit Verbesserungen f√ºr das Prognosemodell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Guess.js</a> experimentierte, begann ich mich eingehend mit Deep Learning zu befassen: wiederkehrende neuronale Netze (RNNs), insbesondere LSTMs, aufgrund ihrer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">‚Äûunangemessenen Wirksamkeit‚Äú</a> in dem Bereich, in dem Guess.js arbeitet.  Gleichzeitig fing ich an, mit Faltungs-Neuronalen Netzen (CNNs) herumzuspielen, die auch h√§ufig f√ºr Zeitreihen verwendet werden.  CNNs werden √ºblicherweise zum Klassifizieren, Erkennen und Erkennen von Bildern verwendet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">Verwalten von <a href="">MK.js</a> mit TensorFlow.js</font></i> <br><br><blockquote>  Der Quellcode f√ºr <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Artikel</a> und <a href="">MK.js</a> befinden sich auf meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> .  Ich habe keinen Trainingsdatensatz ver√∂ffentlicht, aber Sie k√∂nnen Ihren eigenen erstellen und das Modell wie unten beschrieben trainieren! </blockquote><a name="habracut"></a><br>  Nachdem ich mit CNN gespielt hatte, erinnerte ich mich an ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experiment, das</a> ich vor einigen Jahren durchgef√ºhrt hatte, als Browserentwickler die <code>getUserMedia</code> API ver√∂ffentlichten.  Darin diente die Kamera des Benutzers als Controller zum Spielen des kleinen JavaScript-Klons von Mortal Kombat 3. Sie finden dieses Spiel im <a href="">GitHub-Repository</a> .  Als Teil des Experiments habe ich einen grundlegenden Positionierungsalgorithmus implementiert, der das Bild in die folgenden Klassen klassifiziert: <br><br><ul><li>  Linker oder rechter Schlag </li><li>  Linker oder rechter Kick </li><li>  Schritte nach links und rechts </li><li>  Kniebeugen </li><li>  Keine der oben genannten </li></ul><br>  Der Algorithmus ist so einfach, dass ich ihn in wenigen S√§tzen erkl√§ren kann: <br><br><blockquote>  Der Algorithmus fotografiert den Hintergrund.  Sobald der Benutzer im Frame erscheint, berechnet der Algorithmus die Differenz zwischen dem Hintergrund und dem aktuellen Frame mit dem Benutzer.  So wird die Position der Benutzerfigur bestimmt.  Der n√§chste Schritt besteht darin, den K√∂rper des Benutzers in Wei√ü auf Schwarz anzuzeigen.  Danach werden vertikale und horizontale Histogramme erstellt, die die Werte f√ºr jedes Pixel summieren.  Basierend auf dieser Berechnung bestimmt der Algorithmus die aktuelle Position des K√∂rpers. </blockquote><br>  Das Video zeigt, wie das Programm funktioniert.  <a href="">GitHub-</a> Quellcode. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Obwohl der winzige MK-Klon erfolgreich funktioniert hat, ist der Algorithmus alles andere als perfekt.  Ein Rahmen mit Hintergrund ist erforderlich.  F√ºr einen ordnungsgem√§√üen Betrieb muss der Hintergrund w√§hrend der Ausf√ºhrung des Programms dieselbe Farbe haben.  Eine solche Einschr√§nkung bedeutet, dass √Ñnderungen in Licht, Schatten und anderen Dingen st√∂ren und zu einem ungenauen Ergebnis f√ºhren.  Schlie√ülich erkennt der Algorithmus die Aktion nicht;  Er klassifiziert den neuen Rahmen nur als die Position des K√∂rpers aus einem vordefinierten Satz. <br><br>  Dank des Fortschritts in der Web-API, n√§mlich WebGL, habe ich mich entschlossen, durch Anwenden von TensorFlow.js zu dieser Aufgabe zur√ºckzukehren. <br><br><h1>  Einf√ºhrung </h1><br>  In diesem Artikel werde ich meine Erfahrungen bei der Erstellung eines Algorithmus zur Klassifizierung von K√∂rperpositionen mithilfe von TensorFlow.js und MobileNet teilen.  Betrachten Sie die folgenden Themen: <br><br><ul><li>  Sammlung von Trainingsdaten zur Bildklassifizierung </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenerweiterung</a> mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">imgaug</a> </li><li>  Lernen mit MobileNet √ºbertragen </li><li>  Bin√§re Klassifikation und N-Prim√§rklassifikation </li><li>  Trainieren Sie das Bildklassifizierungsmodell von TensorFlow.js in Node.js und verwenden Sie es in einem Browser </li><li>  Ein paar Worte zur Klassifizierung von Aktionen mit LSTM </li></ul><br>  In diesem Artikel reduzieren wir das Problem auf die Bestimmung der Position des K√∂rpers anhand eines Frames, im Gegensatz zum Erkennen von Aktionen anhand einer Folge von Frames.  Wir werden mit einem Lehrer ein Modell f√ºr tiefes Lernen entwickeln, das auf der Grundlage des Bildes von der Webcam des Benutzers die Bewegungen einer Person bestimmt: Tritt, Bein oder nichts davon. <br><br>  Am Ende des Artikels k√∂nnen wir ein Modell f√ºr das Spielen von <a href="">MK.js erstellen</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  Zum besseren Verst√§ndnis des Artikels sollte der Leser mit den grundlegenden Konzepten von Programmierung und JavaScript vertraut sein.  Ein grundlegendes Verst√§ndnis von tiefem Lernen ist ebenfalls n√ºtzlich, aber nicht notwendig. <br><br><h1>  Datenerfassung </h1><br>  Die Genauigkeit des Deep-Learning-Modells h√§ngt stark von der Qualit√§t der Daten ab.  Wir m√ºssen uns bem√ºhen, wie in der Produktion einen umfangreichen Datensatz zu sammeln. <br><br>  Unser Modell sollte Schl√§ge und Tritte erkennen k√∂nnen.  Dies bedeutet, dass wir Bilder von drei Kategorien sammeln m√ºssen: <br><br><ul><li>  Tritte </li><li>  Tritte </li><li>  Andere </li></ul><br>  In diesem Experiment halfen mir zwei Freiwillige ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@lili_vs</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@gsamokovarov</a> ) beim Sammeln von Fotos.  Wir haben 5 QuickTime-Videos auf meinem MacBook Pro aufgenommen, die jeweils 2-4 Kicks und 2-4 Kicks enthalten. <br><br>  Dann extrahieren wir mit ffmpeg einzelne Frames aus den Videos und speichern sie als <code>jpg</code> Bilder: <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  Um den obigen Befehl auszuf√ºhren, m√ºssen Sie zuerst <code>ffmpeg</code> auf dem Computer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">installieren</a> . <br><br>  Wenn wir das Modell trainieren m√∂chten, m√ºssen wir die Eingabedaten und die entsprechenden Ausgabedaten bereitstellen, aber zu diesem Zeitpunkt haben wir nur eine Reihe von Bildern von drei Personen in verschiedenen Posen.  Um die Daten zu strukturieren, m√ºssen Sie Frames in drei Kategorien einteilen: Schl√§ge, Tritte und andere.  F√ºr jede Kategorie wird ein separates Verzeichnis erstellt, in das alle entsprechenden Bilder verschoben werden. <br><br>  Daher sollte es in jedem Verzeichnis ungef√§hr 200 Bilder geben, die den folgenden √§hnlich sind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Bitte beachten Sie, dass das Verzeichnis "Sonstige" viel mehr Bilder enth√§lt, da relativ wenige Bilder Fotos von Schl√§gen und Tritten enthalten und in den verbleibenden Bildern Personen das Video laufen, sich umdrehen oder steuern.  Wenn wir zu viele Bilder einer Klasse haben, laufen wir Gefahr, das Modell zu unterrichten, das auf diese bestimmte Klasse ausgerichtet ist.  In diesem Fall kann das neuronale Netzwerk bei der Klassifizierung eines Bildes mit Auswirkung immer noch die Klasse ‚ÄûAndere‚Äú bestimmen.  Um diese Verzerrung zu verringern, k√∂nnen Sie einige Fotos aus dem Verzeichnis "Andere" entfernen und das Modell mit einer gleichen Anzahl von Bildern aus jeder Kategorie trainieren. <br><br>  Der <code>1.jpg</code> <code>2.jpg</code> weisen wir die Nummern in den Katalognummern von <code>1</code> bis <code>190</code> , sodass das erste Bild <code>1.jpg</code> , das zweite <code>2.jpg</code> usw. ist. <br><br>  Wenn wir das Modell in nur 600 Fotos trainieren, die in derselben Umgebung mit denselben Personen aufgenommen wurden, erreichen wir keine sehr hohe Genauigkeit.  Um das Beste aus unseren Daten herauszuholen, erstellen Sie am besten einige zus√§tzliche Stichproben mithilfe der Datenerweiterung. <br><br><h1>  Datenerweiterung </h1><br>  Datenerweiterung ist eine Technik, die die Anzahl der Datenpunkte erh√∂ht, indem neue Punkte aus einem vorhandenen Satz synthetisiert werden.  In der Regel wird Augmentation verwendet, um die Gr√∂√üe und Vielfalt des Trainingssatzes zu erh√∂hen.  Wir √ºbertragen die Originalbilder in die Pipeline der Transformationen, die neue Bilder erstellen.  Sie k√∂nnen sich den Transformationen nicht zu aggressiv n√§hern: Aus einem Schlag sollten nur andere Handschl√§ge generiert werden. <br><br>  Akzeptable Transformationen sind Rotation, Farbinversion, Unsch√§rfe usw. Es gibt ausgezeichnete Open-Source-Tools zur Datenerweiterung.  Zum Zeitpunkt des Schreibens dieses Artikels in JavaScript gab es nicht allzu viele Optionen, daher habe ich die in Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">imgaug</a> implementierte Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verwendet</a> .  Es verf√ºgt √ºber eine Reihe von Augmentern, die probabilistisch angewendet werden k√∂nnen. <br><br>  Hier ist die Datenerweiterungslogik f√ºr dieses Experiment: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  Dieses Skript verwendet die Hauptmethode mit drei <code>for</code> Schleifen - eine f√ºr jede Bildkategorie.  In jeder Iteration, in jeder der Schleifen, rufen wir die Methode <code>draw_single_sequential_images</code> : Das erste Argument ist der Dateiname, das zweite ist der Pfad, das dritte ist das Verzeichnis, in dem das Ergebnis gespeichert werden soll. <br><br>  Danach lesen wir das Image von der Festplatte und wenden eine Reihe von Transformationen darauf an.  Ich habe die meisten Transformationen im obigen Code-Snippet dokumentiert, daher werden wir sie nicht wiederholen. <br><br>  F√ºr jedes Bild werden 16 weitere Bilder erstellt.  Hier ist ein Beispiel, wie sie aussehen: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Bitte beachten Sie, dass wir im obigen Skript Bilder auf <code>100x56</code> Pixel <code>100x56</code> .  Wir tun dies, um die Datenmenge und dementsprechend die Anzahl der Berechnungen zu reduzieren, die unser Modell w√§hrend des Trainings und der Auswertung durchf√ºhrt. <br><br><h1>  Modellbau </h1><br>  Erstellen Sie jetzt ein Modell f√ºr die Klassifizierung! <br><br>  Da es sich um Bilder handelt, verwenden wir ein Faltungsnetzwerk (CNN).  Es ist bekannt, dass diese Netzwerkarchitektur zur Bilderkennung, Objekterkennung und Klassifizierung geeignet ist. <br><br><h3>  Lerntransfer </h3><br>  Das Bild unten zeigt das beliebte CNN VGG-16, mit dem Bilder klassifiziert werden. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  Das neuronale Netzwerk VGG-16 erkennt 1000 Bildklassen.  Es hat 16 Ebenen (ohne die Pooling- und Ausgabeebenen).  Ein solches mehrschichtiges Netzwerk ist in der Praxis schwer zu trainieren.  Dies erfordert einen gro√üen Datensatz und viele Stunden Schulung. <br><br>  Versteckte Ebenen trainierten CNN erkennen verschiedene Elemente von Bildern aus dem Trainingssatz, beginnend an den R√§ndern, bis hin zu komplexeren Elementen wie Formen, einzelnen Objekten usw.  Ein trainiertes CNN im Stil von VGG-16 zum Erkennen eines gro√üen Satzes von Bildern muss verborgene Ebenen haben, die viele Funktionen aus dem Trainingssatz gelernt haben.  Solche Funktionen sind den meisten Bildern gemeinsam und werden dementsprechend f√ºr verschiedene Aufgaben wiederverwendet. <br><br>  Mit dem Lerntransfer k√∂nnen Sie ein vorhandenes und geschultes Netzwerk wiederverwenden.  Wir k√∂nnen die Ausgabe von jeder der Schichten des vorhandenen Netzwerks nehmen und als Eingabe in das neue neuronale Netzwerk √ºbertragen.  Durch das Unterrichten des neu geschaffenen neuronalen Netzwerks kann im Laufe der Zeit gelernt werden, neue Merkmale einer h√∂heren Ebene zu erkennen und Bilder aus Klassen, die das urspr√ºngliche Modell noch nie zuvor gesehen hatte, korrekt zu klassifizieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  Nehmen Sie f√ºr unsere Zwecke das neuronale <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet-</a> Netzwerk aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Paket @ tensorflow-models / mobilet</a> .  MobileNet ist genauso leistungsf√§hig wie VGG-16, aber viel kleiner, was die direkte Verteilung, dh die Netzwerkausbreitung (Forward Propagation), beschleunigt und die Downloadzeit im Browser verk√ºrzt.  MobileNet wurde anhand des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ILSVRC-2012-CLS-Bildklassifizierungsdatensatzes</a> geschult. <br><br>  Bei der Entwicklung eines Modells mit Lerntransfer haben wir zwei M√∂glichkeiten: <br><br><ol><li>  Die Ausgabe, von welcher Ebene des Quellmodells als Eingabe f√ºr das Zielmodell verwendet werden soll. </li><li>  Wie viele Ebenen aus dem Zielmodell werden wir gegebenenfalls trainieren? </li></ol><br>  Der erste Punkt ist sehr wichtig.  Abh√§ngig von der ausgew√§hlten Ebene erhalten wir Features auf einer niedrigeren oder h√∂heren Abstraktionsebene als Eingabe f√ºr unser neuronales Netzwerk. <br><br>  Wir werden keine Schichten von MobileNet trainieren.  Wir <code>global_average_pooling2d_1</code> Ausgabe von <code>global_average_pooling2d_1</code> und √ºbergeben sie als Eingabe an unser kleines Modell.  Warum habe ich diese bestimmte Ebene gew√§hlt?  Empirisch.  Ich habe einige Tests durchgef√ºhrt, und diese Ebene funktioniert recht gut. <br><br><h3>  Modelldefinition </h3><br>  Die anf√§ngliche Aufgabe bestand darin, das Bild in drei Klassen zu klassifizieren: Hand, Fu√ü und andere Bewegungen.  L√∂sen wir zun√§chst das kleinere Problem: Wir werden feststellen, ob sich im Rahmen ein Handschlag befindet oder nicht.  Dies ist ein typisches Problem der bin√§ren Klassifizierung.  Zu diesem Zweck k√∂nnen wir das folgende Modell definieren: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  Ein solcher Code definiert ein einfaches Modell, eine Schicht mit <code>1024</code> Einheiten und <code>ReLU</code> Aktivierung sowie eine Ausgabeeinheit, die die <code>sigmoid</code> Aktivierungsfunktion durchl√§uft.  Letzteres gibt eine Zahl von <code>0</code> bis <code>1</code> , abh√§ngig von der Wahrscheinlichkeit eines Handschlags in diesem Rahmen. <br><br>  Warum habe ich <code>1024</code> Einheiten f√ºr die zweite Stufe und eine Trainingsgeschwindigkeit von <code>1e-6</code> ?  Nun, ich habe verschiedene Optionen ausprobiert und festgestellt, dass solche Optionen am besten funktionieren.  Die Spear-Methode scheint nicht der beste Ansatz zu sein, aber in hohem Ma√üe funktionieren Hyperparameter-Einstellungen in Deep Learning - basierend auf unserem Verst√§ndnis des Modells verwenden wir die Intuition, um orthogonale Parameter zu aktualisieren und empirisch zu √ºberpr√ºfen, wie das Modell funktioniert. <br><br>  Die <code>compile</code> kompiliert die Ebenen und bereitet das Modell f√ºr das Training und die Bewertung vor.  Hier geben wir bekannt, dass wir den <code>adam</code> Optimierungsalgorithmus verwenden m√∂chten.  Wir erkl√§ren auch, dass wir den Verlust (Verlust) aus der Kreuzentropie berechnen und angeben, dass wir die Genauigkeit des Modells bewerten m√∂chten.  TensorFlow.js berechnet dann die Genauigkeit anhand der folgenden Formel: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  Wenn Sie Schulungen vom urspr√ºnglichen MobileNet-Modell √ºbertragen, m√ºssen Sie diese zuerst herunterladen.  Da es nicht praktikabel ist, unser Modell mit mehr als 3.000 Bildern in einem Browser zu trainieren, verwenden wir Node.js und laden das neuronale Netzwerk aus der Datei. <br><br>  Laden Sie MobileNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> herunter.  Der Katalog enth√§lt die Datei <code>model.json</code> , die die Architektur des Modells enth√§lt - Ebenen, Aktivierungen usw.  Die restlichen Dateien enthalten Modellparameter.  Sie k√∂nnen das Modell mit diesem Code aus einer Datei laden: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Beachten Sie, dass wir in der <code>loadModel</code> Methode eine Funktion zur√ºckgeben, die einen eindimensionalen Tensor als Eingabe akzeptiert und <code>mn.infer(input, Layer)</code> zur√ºckgibt.  Die <code>infer</code> Methode verwendet einen Tensor und eine Ebene als Argumente.  Die Ebene bestimmt, von welcher verborgenen Ebene die Ausgabe erfolgen soll.  Wenn Sie <a href="">model.json</a> √∂ffnen und <code>global_average_pooling2d_1</code> <a href="">global_average_pooling2d_1</a> <code>global_average_pooling2d_1</code> , finden Sie einen solchen Namen auf einer der Ebenen. <br><br>  Jetzt m√ºssen Sie einen Datensatz zum Trainieren des Modells erstellen.  Dazu m√ºssen wir alle Bilder in MobileNet durch die <code>infer</code> Methode f√ºhren und ihnen Beschriftungen zuweisen: <code>1</code> f√ºr Bilder mit Strichen und <code>0</code> f√ºr Bilder ohne Striche: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  Im obigen Code lesen wir zuerst die Dateien in Verzeichnissen mit und ohne Treffer.  Dann bestimmen wir den eindimensionalen Tensor, der die Ausgabeetiketten enth√§lt.  Wenn wir <code>n</code> Bilder mit Strichen und <code>m</code> andere Bilder haben, hat der Tensor <code>n</code> Elemente mit einem Wert von 1 und <code>m</code> Elemente mit einem Wert von 0. <br><br>  In <code>xs</code> <code>infer</code> wir <code>infer</code> Ergebnisse des Aufrufs der <code>infer</code> Methode f√ºr einzelne Bilder zusammen.  Beachten Sie, dass wir f√ºr jedes Bild die <code>readInput</code> Methode aufrufen.  Hier ist seine Implementierung: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> ruft zuerst die Funktion <code>readImage</code> und delegiert anschlie√üend den Aufruf an <code>imageToInput</code> .  Die Funktion <code>readImage</code> liest ein Image von der Festplatte und decodiert dann jpg mit dem Paket <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">jpeg-js</a> aus dem Puffer.  In <code>imageToInput</code> konvertieren wir das Bild in einen dreidimensionalen Tensor. <br><br>  Infolgedessen sollte f√ºr jedes <code>i</code> von <code>0</code> bis <code>TotalImages</code> <code>ys[i]</code> gleich <code>1</code> wenn <code>xs[i]</code> dem Bild mit einem Treffer entspricht, andernfalls <code>0</code> . <br><br><h1>  Modelltraining </h1><br>  Jetzt ist das Modell bereit f√ºr das Training!  Rufen Sie die <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  Die obigen Codeaufrufe <code>fit</code> zu drei Argumenten: <code>xs</code> , ys und das Konfigurationsobjekt.  Im Konfigurationsobjekt legen wir fest, wie viele Epochen das Modell, die Paketgr√∂√üe und der R√ºckruf, den TensorFlow.js nach der Verarbeitung jedes Pakets generiert, trainiert werden. <br><br>  Die Paketgr√∂√üe bestimmt <code>xs</code> und <code>ys</code> f√ºr das Training des Modells in einer √Ñra.  F√ºr jede Epoche w√§hlt TensorFlow.js eine Teilmenge von <code>xs</code> und den entsprechenden Elementen aus <code>ys</code> , f√ºhrt eine direkte Verteilung durch, empf√§ngt die Ausgabe der Schicht mit <code>sigmoid</code> Aktivierung und f√ºhrt dann basierend auf dem Verlust eine Optimierung unter Verwendung des <code>adam</code> Algorithmus durch. <br><br>  Nach dem Starten des Trainingsskripts sehen Sie ein √§hnliches Ergebnis wie das folgende: <br><br><pre>  Kosten: 0,84212, Genauigkeit: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 Verlust = 0,84 Kosten: 0,79740, Genauigkeit: 1,00000
 eta = 0,2 =&gt; --------- acc = 1,00 Verlust = 0,80 Kosten: 0,81533, Genauigkeit: 1,00000
 eta = 0,2 ==&gt; -------- acc = 1,00 Verlust = 0,82 Kosten: 0,64303, Genauigkeit: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 Verlust = 0,64 Kosten: 0,51377, Genauigkeit: 0,00000
 eta = 0,2 ====&gt; ------ acc = 0,00 Verlust = 0,51 Kosten: 0,46473, Genauigkeit: 0,50000
 eta = 0,1 =====&gt; ----- acc = 0,50 Verlust = 0,46 Kosten: 0,50872, Genauigkeit: 0,00000
 eta = 0,1 ======&gt; ---- acc = 0,00 Verlust = 0,51 Kosten: 0,62556, Genauigkeit: 1,00000
 eta = 0,1 =======&gt; --- acc = 1,00 Verlust = 0,63 Kosten: 0,65133, Genauigkeit: 0,50000
 eta = 0,1 ========&gt; - acc = 0,50 Verlust = 0,65 Kosten: 0,63824, Genauigkeit: 0,50000
 eta = 0.0 ===========&gt;
 293 ms 14675us / Schritt - acc = 0,60 Verlust = 0,65
 Epoche 3/50
 Kosten: 0,44661, Genauigkeit: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 Verlust = 0,45 Kosten: 0,78060, Genauigkeit: 1,00000
 eta = 0,3 =&gt; --------- acc = 1,00 Verlust = 0,78 Kosten: 0,79208, Genauigkeit: 1,00000
 eta = 0,3 ==&gt; -------- acc = 1,00 Verlust = 0,79 Kosten: 0,49072, Genauigkeit: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 Verlust = 0,49 Kosten: 0,62232, Genauigkeit: 1,00000
 eta = 0,2 ====&gt; ------ acc = 1,00 Verlust = 0,62 Kosten: 0,82899, Genauigkeit: 1,00000
 eta = 0,2 =====&gt; ----- acc = 1,00 Verlust = 0,83 Kosten: 0,67629, Genauigkeit: 0,50000
 eta = 0,1 ======&gt; ---- acc = 0,50 Verlust = 0,68 Kosten: 0,62621, Genauigkeit: 0,50000
 eta = 0,1 =======&gt; --- acc = 0,50 Verlust = 0,63 Kosten: 0,46077, Genauigkeit: 1,00000
 eta = 0,1 ========&gt; - acc = 1,00 Verlust = 0,46 Kosten: 0,62076, Genauigkeit: 1,00000
 eta = 0.0 ===========&gt;
 304 ms 15221us / Schritt - acc = 0,85 Verlust = 0,63 </pre><br>  Beachten Sie, wie die Genauigkeit mit der Zeit zunimmt und der Verlust abnimmt. <br><br>  In meinem Datensatz zeigte das Modell nach dem Training eine Genauigkeit von 92%.  Beachten Sie, dass die Genauigkeit aufgrund der kleinen Trainingsdaten m√∂glicherweise nicht sehr hoch ist. <br><br><h1>  Ausf√ºhren des Modells in einem Browser </h1><br>  Im vorherigen Abschnitt haben wir das bin√§re Klassifizierungsmodell trainiert.  F√ºhren Sie es jetzt in einem Browser aus und stellen Sie eine Verbindung zu <a href="">MK.js her</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  Der obige Code enth√§lt mehrere Erkl√§rungen: <br><br><ul><li>  <code>video</code> enth√§lt einen Link zum <code>HTML5 video</code> auf der Seite </li><li> <code>Layer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> enth√§lt den Namen des Layers aus MobileNet, von dem die Ausgabe abgerufen und als Eingabe f√ºr unser Modell √ºbergeben werden soll </font></font></li><li> <code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Eine Funktion, die eine Instanz von MobileNet verwendet und eine andere Funktion zur√ºckgibt. </font><font style="vertical-align: inherit;">Die zur√ºckgegebene Funktion akzeptiert Eingaben und gibt die entsprechende Ausgabe von der angegebenen MobileNet-Schicht zur√ºck.</font></font></li><li> <code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gibt das Element an </font></font><code>HTML5 canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, mit dem Frames aus dem Video extrahiert werden</font></font></li><li> <code>scale</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- eine andere </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, mit der einzelne Frames skaliert werden</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach erhalten wir den Videostream von der Kamera des Benutzers und legen ihn als Quelle f√ºr das Element fest </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der n√§chste Schritt besteht darin, einen Graustufenfilter zu implementieren, </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der seinen Inhalt </font><font style="vertical-align: inherit;">akzeptiert </font><font style="vertical-align: inherit;">und konvertiert:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Als n√§chsten Schritt verbinden wir das Modell mit MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im obigen Code laden wir zuerst das oben trainierte Modell und laden dann MobileNet herunter. Wir √ºbergeben MobileNet an die Methode </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, um die Ausgabe der verborgenen Netzwerkschicht zu berechnen. Danach rufen wir die Methode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit zwei Netzwerken als Argumente auf.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der interessanteste Teil beginnt in der Methode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! Zuerst f√ºhren wir ein Intervall aus, in dem jeder </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine anonyme Funktion aufruft. Darin wird das </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Video mit dem aktuellen Frame zuerst dar√ºber gerendert. Dann reduzieren wir die Rahmengr√∂√üe auf </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und wenden einen Graustufenfilter darauf an. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der n√§chste Schritt besteht darin, den Frame an MobileNet zu √ºbertragen, die Ausgabe von der gew√ºnschten verborgenen Ebene abzurufen und als Eingabe f√ºr die Methode </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unseres Modells zu √ºbertragen. Das gibt einen Tensor mit einem Element zur√ºck. Mit erhalten </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wir den Wert vom Tensor und weisen ihn einer Konstanten zu </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schlie√ülich pr√ºfen wir: Wenn die Wahrscheinlichkeit eines Handschlags gr√∂√üer ist </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, rufen wir die </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">globale Objektmethode auf </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. MK.js stellt ein globales Objekt mit drei Methoden bereit:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dass wir verwendet werden k√∂nnen , </font><font style="vertical-align: inherit;">einen der Charaktere zu steuern.</font></font><br><br>  Fertig!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ist das Ergebnis! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tritt- und Armerkennung mit N-Klassifizierung </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im n√§chsten Abschnitt werden wir ein intelligenteres Modell erstellen: ein neuronales Netzwerk, das Schl√§ge, Tritte und andere Bilder erkennt. </font><font style="vertical-align: inherit;">Beginnen wir dieses Mal mit der Vorbereitung des Trainingssets:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nach wie vor lesen wir zuerst die Kataloge mit Bildern von Schl√§gen von Hand, Fu√ü und anderen Bildern. Danach bilden wir im Gegensatz zum letzten Mal das erwartete Ergebnis in Form eines zweidimensionalen Tensors und nicht eindimensional. Wenn wir </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bilder mit einem Stempel, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bilder mit einem Kick und </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> andere Bilder, die Tensor </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wird </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elemente des Wertes </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die Elemente mit dem Wert </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gegenst√§nde mit Wert </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein Vektor von </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elementen, in dem es </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elemente mit einem Wert </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und ein Element mit einem Wert gibt </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nennen wir einen einheitlichen Vektor (One-Hot-Vektor). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach bilden wir den Eingangstensor</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stapeln der Ausgabe jedes Bildes aus MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hier m√ºssen Sie die Modelldefinition aktualisieren:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die einzigen zwei Unterschiede zum Vorg√§ngermodell sind: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Anzahl der Einheiten in der Ausgabeebene </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aktivierungen in der Ausgabeebene </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Es gibt drei Einheiten in der Ausgabeebene, da wir drei verschiedene Kategorien von Bildern haben: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Handschlag </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kick </font></font></li><li>  Andere </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bei diesen drei Einheiten </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wird die </font><font style="vertical-align: inherit;">Aktivierung ausgel√∂st </font><font style="vertical-align: inherit;">, wodurch ihre Parameter in einen Tensor mit drei Werten umgewandelt werden. Warum drei Einheiten f√ºr die Ausgabeschicht? Jede der drei Werte f√ºr drei Klassen k√∂nnen durch zwei Bits dargestellt werden: </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Die Summe der Werte des erstellten Tensors </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ist 1, dh wir erhalten niemals 00, sodass wir keine Bilder einer der Klassen klassifizieren k√∂nnen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nachdem </font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ich </font><font style="vertical-align: inherit;">das Modell im Laufe der </font><font style="vertical-align: inherit;">Zeit </font><font style="vertical-align: inherit;">trainiert hatte </font><font style="vertical-align: inherit;">, erreichte ich eine Genauigkeit von ca. 92%! Das ist nicht schlecht, aber vergessen Sie nicht, dass das Training mit einem kleinen Datensatz durchgef√ºhrt wurde. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der n√§chste Schritt ist das Ausf√ºhren des Modells in einem Browser! Da die Logik dem Ausf√ºhren des Modells f√ºr die bin√§re Klassifizierung sehr √§hnlich ist, sehen Sie sich den letzten Schritt an, in dem die Aktion basierend auf der Ausgabe des Modells ausgew√§hlt wird:</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuerst rufen wir MobileNet mit einem reduzierten Rahmen in Graustufen auf, dann √ºbertragen wir das Ergebnis unseres trainierten Modells. </font><font style="vertical-align: inherit;">Das Modell gibt einen eindimensionalen Tensor zur√ºck, den wir in </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font><font style="vertical-align: inherit;">konvertieren </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Im n√§chsten Schritt wandeln wir </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ein typisiertes Array in ein JavaScript-Array um. </font><font style="vertical-align: inherit;">Dann extrahieren wir die Wahrscheinlichkeiten, dass ein Schuss mit einer Hand, ein Tritt oder nichts auf dem Rahmen vorhanden ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn die Wahrscheinlichkeit des dritten Ergebnisses √ºberschritten wird </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, kehren wir zur√ºck. </font><font style="vertical-align: inherit;">Andernfalls </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">senden wir </font><font style="vertical-align: inherit;">, wenn die Wahrscheinlichkeit eines Tritts h√∂her </font><font style="vertical-align: inherit;">ist, einen Trittbefehl an MK.js. </font><font style="vertical-align: inherit;">Wenn die Wahrscheinlichkeit eines Tritts h√∂her </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und h√∂her ist als die Wahrscheinlichkeit eines Tritts, senden wir die Aktion eines Tritts. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Allgemeinen ist das alles! </font><font style="vertical-align: inherit;">Das Ergebnis ist unten dargestellt:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aktionserkennung </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie einen gro√üen und vielf√§ltigen Datensatz √ºber Personen sammeln, die mit H√§nden und F√º√üen schlagen, k√∂nnen Sie ein Modell erstellen, das sich hervorragend f√ºr einzelne Frames eignet. Aber ist das genug? Was ist, wenn wir noch weiter gehen und zwei verschiedene Arten von Tritten unterscheiden wollen: von einer Kurve und von einem R√ºcken (R√ºcktritt). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie in den folgenden Frames zu sehen ist, sehen beide Striche zu einem bestimmten Zeitpunkt aus einem bestimmten Winkel gleich aus: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie jedoch die Leistung betrachten, sind die Bewegungen v√∂llig unterschiedlich: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie k√∂nnen Sie ein neuronales Netzwerk trainieren, um die Abfolge von Frames und nicht nur einen Frame zu analysieren? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zu diesem Zweck k√∂nnen wir eine andere Klasse neuronaler Netze untersuchen, die als wiederkehrende neuronale Netze (RNNs) bezeichnet werden. Zum Beispiel eignen sich RNNs hervorragend f√ºr die Arbeit mit Zeitreihen:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Natural Language Processing (NLP), wobei jedes Wort vom vorherigen und nachfolgenden abh√§ngt </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vorhersage der n√§chsten Seite basierend auf Ihrem Browserverlauf </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Rahmenerkennung </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Implementierung eines solchen Modells w√ºrde den Rahmen dieses Artikels sprengen. Schauen wir uns jedoch eine Beispielarchitektur an, um eine Vorstellung davon zu erhalten, wie all dies zusammenarbeitet. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Kraft von RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das folgende Diagramm zeigt das Modell der Erkennung von Aktionen: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir nehmen die letzten </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bilder aus dem Video und √ºbertragen sie an CNN. </font><font style="vertical-align: inherit;">Der CNN-Ausgang f√ºr jeden Rahmen wird als Eingangs-RNN √ºbertragen. </font><font style="vertical-align: inherit;">Ein wiederkehrendes neuronales Netzwerk bestimmt die Beziehungen zwischen einzelnen Frames und erkennt, welcher Aktion sie entsprechen.</font></font><br><br><h1>  Fazit </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Artikel haben wir ein Bildklassifizierungsmodell entwickelt. Zu diesem Zweck haben wir einen Datensatz gesammelt: Wir haben Videobilder extrahiert und sie manuell in drei Kategorien unterteilt. Dann wurden die Daten </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">durch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hinzuf√ºgen von Bildern mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">imgaug erweitert</font></a><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach haben wir erkl√§rt, was Lerntransfer ist, und das trainierte </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MobileNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Modell aus dem </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">@ tensorflow-models / mobilet-</font></a><font style="vertical-align: inherit;"> Paket f√ºr unsere Zwecke verwendet </font><font style="vertical-align: inherit;">. Wir haben MobileNet aus einer Datei im Node.js-Prozess geladen und eine zus√§tzliche dichte Schicht trainiert, in der Daten aus der verborgenen MobileNet-Schicht eingespeist wurden. Nach dem Training haben wir eine Genauigkeit von mehr als 90% erreicht! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um dieses Modell in einem Browser zu verwenden, haben wir es zusammen mit MobileNet heruntergeladen und alle 100 ms damit begonnen, Frames von der Webcam des Benutzers zu kategorisieren. Wir haben das Modell mit dem Spiel verbunden</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und verwendete die Modellausgabe, um eines der Zeichen zu steuern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schlie√ülich haben wir uns angesehen, wie das Modell verbessert werden kann, indem es mit einem wiederkehrenden neuronalen Netzwerk kombiniert wird, um Aktionen zu erkennen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich hoffe, Ihnen hat dieses kleine Projekt nicht weniger gefallen als mir! </font><font style="vertical-align: inherit;">‚Äç</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428019/">https://habr.com/ru/post/de428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428003/index.html">Responsive Design: Beibehaltung der Form von Markup-Elementen</a></li>
<li><a href="../de428005/index.html">Drei effektive M√∂glichkeiten, um eine PR-Katastrophe zu versch√§rfen</a></li>
<li><a href="../de428007/index.html">Schon kein tragbarer PC, noch kein Notebook: Laptop TOSHIBA T3100 / 20</a></li>
<li><a href="../de428009/index.html">Equifax: ein Jahr nach dem gr√∂√üten Datenleck</a></li>
<li><a href="../de428011/index.html">Weltraum-Zombielieder</a></li>
<li><a href="../de428021/index.html">Dichtungen gegen das neuronale Netz. Oder w√§hlen Sie ein neuronales Netzwerk aus und f√ºhren Sie es aus, um Objekte auf dem Raspberry Zero zu erkennen</a></li>
<li><a href="../de428023/index.html">Grundlagen der elektrischen Sicherheit bei der Konstruktion elektronischer Ger√§te</a></li>
<li><a href="../de428025/index.html">Anschlie√üen einer Auslagerungsdatei (SWAP) in MAC OS X bei Verwendung einer externen SSD als System</a></li>
<li><a href="../de428027/index.html">Wie ich versucht habe, einen statischen GLSL-Analysator herzustellen (und was schief gelaufen ist)</a></li>
<li><a href="../de428029/index.html">Digitale Veranstaltungen in Moskau vom 29. Oktober bis 4. November</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>