<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🤺 🕵️ 📎 Spielen Sie Mortal Kombat mit TensorFlow.js 👨🏼‍🎨 👨🏿‍🚀 ⛅️</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Als ich mit Verbesserungen für das Prognosemodell von Guess.js experimentierte, begann ich mich eingehend mit Deep Learning zu befassen: wiederkehrend...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Spielen Sie Mortal Kombat mit TensorFlow.js</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/428019/">  Als ich mit Verbesserungen für das Prognosemodell von <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Guess.js</a> experimentierte, begann ich mich eingehend mit Deep Learning zu befassen: wiederkehrende neuronale Netze (RNNs), insbesondere LSTMs, aufgrund ihrer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">„unangemessenen Wirksamkeit“</a> in dem Bereich, in dem Guess.js arbeitet.  Gleichzeitig fing ich an, mit Faltungs-Neuronalen Netzen (CNNs) herumzuspielen, die auch häufig für Zeitreihen verwendet werden.  CNNs werden üblicherweise zum Klassifizieren, Erkennen und Erkennen von Bildern verwendet. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fb/9be/edc/1fb9beedcad00d1c0dcdc7bbef67e6d9.png"><br>  <i><font color="gray">Verwalten von <a href="">MK.js</a> mit TensorFlow.js</font></i> <br><br><blockquote>  Der Quellcode für <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">diesen Artikel</a> und <a href="">MK.js</a> befinden sich auf meinem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">GitHub</a> .  Ich habe keinen Trainingsdatensatz veröffentlicht, aber Sie können Ihren eigenen erstellen und das Modell wie unten beschrieben trainieren! </blockquote><a name="habracut"></a><br>  Nachdem ich mit CNN gespielt hatte, erinnerte ich mich an ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Experiment, das</a> ich vor einigen Jahren durchgeführt hatte, als Browserentwickler die <code>getUserMedia</code> API veröffentlichten.  Darin diente die Kamera des Benutzers als Controller zum Spielen des kleinen JavaScript-Klons von Mortal Kombat 3. Sie finden dieses Spiel im <a href="">GitHub-Repository</a> .  Als Teil des Experiments habe ich einen grundlegenden Positionierungsalgorithmus implementiert, der das Bild in die folgenden Klassen klassifiziert: <br><br><ul><li>  Linker oder rechter Schlag </li><li>  Linker oder rechter Kick </li><li>  Schritte nach links und rechts </li><li>  Kniebeugen </li><li>  Keine der oben genannten </li></ul><br>  Der Algorithmus ist so einfach, dass ich ihn in wenigen Sätzen erklären kann: <br><br><blockquote>  Der Algorithmus fotografiert den Hintergrund.  Sobald der Benutzer im Frame erscheint, berechnet der Algorithmus die Differenz zwischen dem Hintergrund und dem aktuellen Frame mit dem Benutzer.  So wird die Position der Benutzerfigur bestimmt.  Der nächste Schritt besteht darin, den Körper des Benutzers in Weiß auf Schwarz anzuzeigen.  Danach werden vertikale und horizontale Histogramme erstellt, die die Werte für jedes Pixel summieren.  Basierend auf dieser Berechnung bestimmt der Algorithmus die aktuelle Position des Körpers. </blockquote><br>  Das Video zeigt, wie das Programm funktioniert.  <a href="">GitHub-</a> Quellcode. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/0_yfU_iNUYo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Obwohl der winzige MK-Klon erfolgreich funktioniert hat, ist der Algorithmus alles andere als perfekt.  Ein Rahmen mit Hintergrund ist erforderlich.  Für einen ordnungsgemäßen Betrieb muss der Hintergrund während der Ausführung des Programms dieselbe Farbe haben.  Eine solche Einschränkung bedeutet, dass Änderungen in Licht, Schatten und anderen Dingen stören und zu einem ungenauen Ergebnis führen.  Schließlich erkennt der Algorithmus die Aktion nicht;  Er klassifiziert den neuen Rahmen nur als die Position des Körpers aus einem vordefinierten Satz. <br><br>  Dank des Fortschritts in der Web-API, nämlich WebGL, habe ich mich entschlossen, durch Anwenden von TensorFlow.js zu dieser Aufgabe zurückzukehren. <br><br><h1>  Einführung </h1><br>  In diesem Artikel werde ich meine Erfahrungen bei der Erstellung eines Algorithmus zur Klassifizierung von Körperpositionen mithilfe von TensorFlow.js und MobileNet teilen.  Betrachten Sie die folgenden Themen: <br><br><ul><li>  Sammlung von Trainingsdaten zur Bildklassifizierung </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Datenerweiterung</a> mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">imgaug</a> </li><li>  Lernen mit MobileNet übertragen </li><li>  Binäre Klassifikation und N-Primärklassifikation </li><li>  Trainieren Sie das Bildklassifizierungsmodell von TensorFlow.js in Node.js und verwenden Sie es in einem Browser </li><li>  Ein paar Worte zur Klassifizierung von Aktionen mit LSTM </li></ul><br>  In diesem Artikel reduzieren wir das Problem auf die Bestimmung der Position des Körpers anhand eines Frames, im Gegensatz zum Erkennen von Aktionen anhand einer Folge von Frames.  Wir werden mit einem Lehrer ein Modell für tiefes Lernen entwickeln, das auf der Grundlage des Bildes von der Webcam des Benutzers die Bewegungen einer Person bestimmt: Tritt, Bein oder nichts davon. <br><br>  Am Ende des Artikels können wir ein Modell für das Spielen von <a href="">MK.js erstellen</a> : <br><br><img src="https://habrastorage.org/webt/2u/0e/g6/2u0eg6ng2p4kwxosmut1koa751g.gif"><br><br>  Zum besseren Verständnis des Artikels sollte der Leser mit den grundlegenden Konzepten von Programmierung und JavaScript vertraut sein.  Ein grundlegendes Verständnis von tiefem Lernen ist ebenfalls nützlich, aber nicht notwendig. <br><br><h1>  Datenerfassung </h1><br>  Die Genauigkeit des Deep-Learning-Modells hängt stark von der Qualität der Daten ab.  Wir müssen uns bemühen, wie in der Produktion einen umfangreichen Datensatz zu sammeln. <br><br>  Unser Modell sollte Schläge und Tritte erkennen können.  Dies bedeutet, dass wir Bilder von drei Kategorien sammeln müssen: <br><br><ul><li>  Tritte </li><li>  Tritte </li><li>  Andere </li></ul><br>  In diesem Experiment halfen mir zwei Freiwillige ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@lili_vs</a> und <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">@gsamokovarov</a> ) beim Sammeln von Fotos.  Wir haben 5 QuickTime-Videos auf meinem MacBook Pro aufgenommen, die jeweils 2-4 Kicks und 2-4 Kicks enthalten. <br><br>  Dann extrahieren wir mit ffmpeg einzelne Frames aus den Videos und speichern sie als <code>jpg</code> Bilder: <br><br> <code>ffmpeg -i video.mov $filename%03d.jpg</code> <br> <br>  Um den obigen Befehl auszuführen, müssen Sie zuerst <code>ffmpeg</code> auf dem Computer <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">installieren</a> . <br><br>  Wenn wir das Modell trainieren möchten, müssen wir die Eingabedaten und die entsprechenden Ausgabedaten bereitstellen, aber zu diesem Zeitpunkt haben wir nur eine Reihe von Bildern von drei Personen in verschiedenen Posen.  Um die Daten zu strukturieren, müssen Sie Frames in drei Kategorien einteilen: Schläge, Tritte und andere.  Für jede Kategorie wird ein separates Verzeichnis erstellt, in das alle entsprechenden Bilder verschoben werden. <br><br>  Daher sollte es in jedem Verzeichnis ungefähr 200 Bilder geben, die den folgenden ähnlich sind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/798/e9a/908/798e9a9083a1f5dfa5811fbb7de3bcc9.jpg"><br><br>  Bitte beachten Sie, dass das Verzeichnis "Sonstige" viel mehr Bilder enthält, da relativ wenige Bilder Fotos von Schlägen und Tritten enthalten und in den verbleibenden Bildern Personen das Video laufen, sich umdrehen oder steuern.  Wenn wir zu viele Bilder einer Klasse haben, laufen wir Gefahr, das Modell zu unterrichten, das auf diese bestimmte Klasse ausgerichtet ist.  In diesem Fall kann das neuronale Netzwerk bei der Klassifizierung eines Bildes mit Auswirkung immer noch die Klasse „Andere“ bestimmen.  Um diese Verzerrung zu verringern, können Sie einige Fotos aus dem Verzeichnis "Andere" entfernen und das Modell mit einer gleichen Anzahl von Bildern aus jeder Kategorie trainieren. <br><br>  Der <code>1.jpg</code> <code>2.jpg</code> weisen wir die Nummern in den Katalognummern von <code>1</code> bis <code>190</code> , sodass das erste Bild <code>1.jpg</code> , das zweite <code>2.jpg</code> usw. ist. <br><br>  Wenn wir das Modell in nur 600 Fotos trainieren, die in derselben Umgebung mit denselben Personen aufgenommen wurden, erreichen wir keine sehr hohe Genauigkeit.  Um das Beste aus unseren Daten herauszuholen, erstellen Sie am besten einige zusätzliche Stichproben mithilfe der Datenerweiterung. <br><br><h1>  Datenerweiterung </h1><br>  Datenerweiterung ist eine Technik, die die Anzahl der Datenpunkte erhöht, indem neue Punkte aus einem vorhandenen Satz synthetisiert werden.  In der Regel wird Augmentation verwendet, um die Größe und Vielfalt des Trainingssatzes zu erhöhen.  Wir übertragen die Originalbilder in die Pipeline der Transformationen, die neue Bilder erstellen.  Sie können sich den Transformationen nicht zu aggressiv nähern: Aus einem Schlag sollten nur andere Handschläge generiert werden. <br><br>  Akzeptable Transformationen sind Rotation, Farbinversion, Unschärfe usw. Es gibt ausgezeichnete Open-Source-Tools zur Datenerweiterung.  Zum Zeitpunkt des Schreibens dieses Artikels in JavaScript gab es nicht allzu viele Optionen, daher habe ich die in Python - <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">imgaug</a> implementierte Bibliothek <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">verwendet</a> .  Es verfügt über eine Reihe von Augmentern, die probabilistisch angewendet werden können. <br><br>  Hier ist die Datenerweiterungslogik für dieses Experiment: <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) ia.seed(<span class="hljs-number"><span class="hljs-number">44</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"others"</span></span>, <span class="hljs-string"><span class="hljs-string">"others-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"hits"</span></span>, <span class="hljs-string"><span class="hljs-string">"hits-aug"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">191</span></span>): draw_single_sequential_images(str(i), <span class="hljs-string"><span class="hljs-string">"kicks"</span></span>, <span class="hljs-string"><span class="hljs-string">"kicks-aug"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">draw_single_sequential_images</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename, path, aug_path)</span></span></span><span class="hljs-function">:</span></span> image = misc.imresize(ndimage.imread(path + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + filename + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span>), (<span class="hljs-number"><span class="hljs-number">56</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>)) sometimes = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> aug: iaa.Sometimes(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, aug) seq = iaa.Sequential( [ iaa.Fliplr(<span class="hljs-number"><span class="hljs-number">0.5</span></span>), <span class="hljs-comment"><span class="hljs-comment"># horizontally flip 50% of all images # crop images by -5% to 10% of their height/width sometimes(iaa.CropAndPad( percent=(-0.05, 0.1), pad_mode=ia.ALL, pad_cval=(0, 255) )), sometimes(iaa.Affine( scale={"x": (0.8, 1.2), "y": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)}, # translate by -10 to +10 percent (per axis) rotate=(-5, 5), shear=(-5, 5), # shear by -5 to +5 degrees order=[0, 1], # use nearest neighbour or bilinear interpolation (fast) cval=(0, 255), # if mode is constant, use a cval between 0 and 255 mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples) )), iaa.Grayscale(alpha=(0.0, 1.0)), iaa.Invert(0.05, per_channel=False), # invert color channels # execute 0 to 5 of the following (less important) augmenters per image # don't execute all of them, as that would often be way too strong iaa.SomeOf((0, 5), [ iaa.OneOf([ iaa.GaussianBlur((0, 2.0)), # blur images with a sigma between 0 and 2.0 iaa.AverageBlur(k=(2, 5)), # blur image using local means with kernel sizes between 2 and 5 iaa.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 3 and 5 ]), iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value) iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation # either change the brightness of the whole image (sometimes # per channel) or change the brightness of subareas iaa.OneOf([ iaa.Multiply((0.9, 1.1), per_channel=0.5), iaa.FrequencyNoiseAlpha( exponent=(-2, 0), first=iaa.Multiply((0.9, 1.1), per_channel=True), second=iaa.ContrastNormalization((0.9, 1.1)) ) ]), iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast ], random_order=True ) ], random_order=True ) im = np.zeros((16, 56, 100, 3), dtype=np.uint8) for c in range(0, 16): im[c] = image for im in range(len(grid)): misc.imsave(aug_path + "/" + filename + "_" + str(im) + ".jpg", grid[im])</span></span></code> </pre> <br>  Dieses Skript verwendet die Hauptmethode mit drei <code>for</code> Schleifen - eine für jede Bildkategorie.  In jeder Iteration, in jeder der Schleifen, rufen wir die Methode <code>draw_single_sequential_images</code> : Das erste Argument ist der Dateiname, das zweite ist der Pfad, das dritte ist das Verzeichnis, in dem das Ergebnis gespeichert werden soll. <br><br>  Danach lesen wir das Image von der Festplatte und wenden eine Reihe von Transformationen darauf an.  Ich habe die meisten Transformationen im obigen Code-Snippet dokumentiert, daher werden wir sie nicht wiederholen. <br><br>  Für jedes Bild werden 16 weitere Bilder erstellt.  Hier ist ein Beispiel, wie sie aussehen: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/759/ad9/43d/759ad943d7aa07dbccee4a6f26a1d920.jpg"></a> <br><br>  Bitte beachten Sie, dass wir im obigen Skript Bilder auf <code>100x56</code> Pixel <code>100x56</code> .  Wir tun dies, um die Datenmenge und dementsprechend die Anzahl der Berechnungen zu reduzieren, die unser Modell während des Trainings und der Auswertung durchführt. <br><br><h1>  Modellbau </h1><br>  Erstellen Sie jetzt ein Modell für die Klassifizierung! <br><br>  Da es sich um Bilder handelt, verwenden wir ein Faltungsnetzwerk (CNN).  Es ist bekannt, dass diese Netzwerkarchitektur zur Bilderkennung, Objekterkennung und Klassifizierung geeignet ist. <br><br><h3>  Lerntransfer </h3><br>  Das Bild unten zeigt das beliebte CNN VGG-16, mit dem Bilder klassifiziert werden. <br><br><img src="https://habrastorage.org/webt/7t/0u/zk/7t0uzk4kdf4pbesgvlojn5nal18.png"><br><br>  Das neuronale Netzwerk VGG-16 erkennt 1000 Bildklassen.  Es hat 16 Ebenen (ohne die Pooling- und Ausgabeebenen).  Ein solches mehrschichtiges Netzwerk ist in der Praxis schwer zu trainieren.  Dies erfordert einen großen Datensatz und viele Stunden Schulung. <br><br>  Versteckte Ebenen trainierten CNN erkennen verschiedene Elemente von Bildern aus dem Trainingssatz, beginnend an den Rändern, bis hin zu komplexeren Elementen wie Formen, einzelnen Objekten usw.  Ein trainiertes CNN im Stil von VGG-16 zum Erkennen eines großen Satzes von Bildern muss verborgene Ebenen haben, die viele Funktionen aus dem Trainingssatz gelernt haben.  Solche Funktionen sind den meisten Bildern gemeinsam und werden dementsprechend für verschiedene Aufgaben wiederverwendet. <br><br>  Mit dem Lerntransfer können Sie ein vorhandenes und geschultes Netzwerk wiederverwenden.  Wir können die Ausgabe von jeder der Schichten des vorhandenen Netzwerks nehmen und als Eingabe in das neue neuronale Netzwerk übertragen.  Durch das Unterrichten des neu geschaffenen neuronalen Netzwerks kann im Laufe der Zeit gelernt werden, neue Merkmale einer höheren Ebene zu erkennen und Bilder aus Klassen, die das ursprüngliche Modell noch nie zuvor gesehen hatte, korrekt zu klassifizieren. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7n/cc/a7/7ncca7e5ne2ammearn2sqnk4by0.png"></div><br><br>  Nehmen Sie für unsere Zwecke das neuronale <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">MobileNet-</a> Netzwerk aus dem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Paket @ tensorflow-models / mobilet</a> .  MobileNet ist genauso leistungsfähig wie VGG-16, aber viel kleiner, was die direkte Verteilung, dh die Netzwerkausbreitung (Forward Propagation), beschleunigt und die Downloadzeit im Browser verkürzt.  MobileNet wurde anhand des <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ILSVRC-2012-CLS-Bildklassifizierungsdatensatzes</a> geschult. <br><br>  Bei der Entwicklung eines Modells mit Lerntransfer haben wir zwei Möglichkeiten: <br><br><ol><li>  Die Ausgabe, von welcher Ebene des Quellmodells als Eingabe für das Zielmodell verwendet werden soll. </li><li>  Wie viele Ebenen aus dem Zielmodell werden wir gegebenenfalls trainieren? </li></ol><br>  Der erste Punkt ist sehr wichtig.  Abhängig von der ausgewählten Ebene erhalten wir Features auf einer niedrigeren oder höheren Abstraktionsebene als Eingabe für unser neuronales Netzwerk. <br><br>  Wir werden keine Schichten von MobileNet trainieren.  Wir <code>global_average_pooling2d_1</code> Ausgabe von <code>global_average_pooling2d_1</code> und übergeben sie als Eingabe an unser kleines Modell.  Warum habe ich diese bestimmte Ebene gewählt?  Empirisch.  Ich habe einige Tests durchgeführt, und diese Ebene funktioniert recht gut. <br><br><h3>  Modelldefinition </h3><br>  Die anfängliche Aufgabe bestand darin, das Bild in drei Klassen zu klassifizieren: Hand, Fuß und andere Bewegungen.  Lösen wir zunächst das kleinere Problem: Wir werden feststellen, ob sich im Rahmen ein Handschlag befindet oder nicht.  Dies ist ein typisches Problem der binären Klassifizierung.  Zu diesem Zweck können wir das folgende Modell definieren: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@tensorflow/tfjs'</span></span>; const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span> })); model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br>  Ein solcher Code definiert ein einfaches Modell, eine Schicht mit <code>1024</code> Einheiten und <code>ReLU</code> Aktivierung sowie eine Ausgabeeinheit, die die <code>sigmoid</code> Aktivierungsfunktion durchläuft.  Letzteres gibt eine Zahl von <code>0</code> bis <code>1</code> , abhängig von der Wahrscheinlichkeit eines Handschlags in diesem Rahmen. <br><br>  Warum habe ich <code>1024</code> Einheiten für die zweite Stufe und eine Trainingsgeschwindigkeit von <code>1e-6</code> ?  Nun, ich habe verschiedene Optionen ausprobiert und festgestellt, dass solche Optionen am besten funktionieren.  Die Spear-Methode scheint nicht der beste Ansatz zu sein, aber in hohem Maße funktionieren Hyperparameter-Einstellungen in Deep Learning - basierend auf unserem Verständnis des Modells verwenden wir die Intuition, um orthogonale Parameter zu aktualisieren und empirisch zu überprüfen, wie das Modell funktioniert. <br><br>  Die <code>compile</code> kompiliert die Ebenen und bereitet das Modell für das Training und die Bewertung vor.  Hier geben wir bekannt, dass wir den <code>adam</code> Optimierungsalgorithmus verwenden möchten.  Wir erklären auch, dass wir den Verlust (Verlust) aus der Kreuzentropie berechnen und angeben, dass wir die Genauigkeit des Modells bewerten möchten.  TensorFlow.js berechnet dann die Genauigkeit anhand der folgenden Formel: <br><br> <code>Accuracy = (True Positives + True Negatives) / (Positives + Negatives)</code> <br> <br>  Wenn Sie Schulungen vom ursprünglichen MobileNet-Modell übertragen, müssen Sie diese zuerst herunterladen.  Da es nicht praktikabel ist, unser Modell mit mehr als 3.000 Bildern in einem Browser zu trainieren, verwenden wir Node.js und laden das neuronale Netzwerk aus der Datei. <br><br>  Laden Sie MobileNet <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">hier</a> herunter.  Der Katalog enthält die Datei <code>model.json</code> , die die Architektur des Modells enthält - Ebenen, Aktivierungen usw.  Die restlichen Dateien enthalten Modellparameter.  Sie können das Modell mit diesem Code aus einer Datei laden: <br><br><pre> <code class="python hljs">export const loadModel = <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> () =&gt; { const mn = new mobilenet.MobileNet(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); mn.path = `file://PATH/TO/model.json`; <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> mn.load(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (input): tf.Tensor1D =&gt; mn.infer(input, <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>) .reshape([<span class="hljs-number"><span class="hljs-number">1024</span></span>]); };</code> </pre> <br>  Beachten Sie, dass wir in der <code>loadModel</code> Methode eine Funktion zurückgeben, die einen eindimensionalen Tensor als Eingabe akzeptiert und <code>mn.infer(input, Layer)</code> zurückgibt.  Die <code>infer</code> Methode verwendet einen Tensor und eine Ebene als Argumente.  Die Ebene bestimmt, von welcher verborgenen Ebene die Ausgabe erfolgen soll.  Wenn Sie <a href="">model.json</a> öffnen und <code>global_average_pooling2d_1</code> <a href="">global_average_pooling2d_1</a> <code>global_average_pooling2d_1</code> , finden Sie einen solchen Namen auf einer der Ebenen. <br><br>  Jetzt müssen Sie einen Datensatz zum Trainieren des Modells erstellen.  Dazu müssen wir alle Bilder in MobileNet durch die <code>infer</code> Methode führen und ihnen Beschriftungen zuweisen: <code>1</code> für Bilder mit Strichen und <code>0</code> für Bilder ohne Striche: <br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor1d( new Array(punches.length).fill(<span class="hljs-number"><span class="hljs-number">1</span></span>) .concat(new Array(others.length).fill(<span class="hljs-number"><span class="hljs-number">0</span></span>))); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br>  Im obigen Code lesen wir zuerst die Dateien in Verzeichnissen mit und ohne Treffer.  Dann bestimmen wir den eindimensionalen Tensor, der die Ausgabeetiketten enthält.  Wenn wir <code>n</code> Bilder mit Strichen und <code>m</code> andere Bilder haben, hat der Tensor <code>n</code> Elemente mit einem Wert von 1 und <code>m</code> Elemente mit einem Wert von 0. <br><br>  In <code>xs</code> <code>infer</code> wir <code>infer</code> Ergebnisse des Aufrufs der <code>infer</code> Methode für einzelne Bilder zusammen.  Beachten Sie, dass wir für jedes Bild die <code>readInput</code> Methode aufrufen.  Hier ist seine Implementierung: <br><br><pre> <code class="python hljs">export const readInput = img =&gt; imageToInput(readImage(img), TotalChannels); const readImage = path =&gt; jpeg.decode(fs.readFileSync(path), true); const imageToInput = image =&gt; { const values = serializeImage(image); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.tensor3d(values, [image.height, image.width, <span class="hljs-number"><span class="hljs-number">3</span></span>], <span class="hljs-string"><span class="hljs-string">'int32'</span></span>); }; const serializeImage = image =&gt; { const totalPixels = image.width * image.height; const result = new Int32Array(totalPixels * <span class="hljs-number"><span class="hljs-number">3</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; totalPixels; i++) { result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">0</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>]; result[i * <span class="hljs-number"><span class="hljs-number">3</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>] = image.data[i * <span class="hljs-number"><span class="hljs-number">4</span></span> + <span class="hljs-number"><span class="hljs-number">2</span></span>]; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result; };</code> </pre> <br>  <code>readInput</code> ruft zuerst die Funktion <code>readImage</code> und delegiert anschließend den Aufruf an <code>imageToInput</code> .  Die Funktion <code>readImage</code> liest ein Image von der Festplatte und decodiert dann jpg mit dem Paket <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">jpeg-js</a> aus dem Puffer.  In <code>imageToInput</code> konvertieren wir das Bild in einen dreidimensionalen Tensor. <br><br>  Infolgedessen sollte für jedes <code>i</code> von <code>0</code> bis <code>TotalImages</code> <code>ys[i]</code> gleich <code>1</code> wenn <code>xs[i]</code> dem Bild mit einem Treffer entspricht, andernfalls <code>0</code> . <br><br><h1>  Modelltraining </h1><br>  Jetzt ist das Modell bereit für das Training!  Rufen Sie die <code>fit</code> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.fit(xs, ys, { epochs: Epochs, batchSize: parseInt(((punches.length + others.length) * BatchSize).toFixed(<span class="hljs-number"><span class="hljs-number">0</span></span>)), callbacks: { onBatchEnd: <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> (_, logs) =&gt; { console.log(<span class="hljs-string"><span class="hljs-string">'Cost: %s, accuracy: %s'</span></span>, logs.loss.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>), logs.acc.toFixed(<span class="hljs-number"><span class="hljs-number">5</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> tf.nextFrame(); } } });</code> </pre> <br>  Die obigen Codeaufrufe <code>fit</code> zu drei Argumenten: <code>xs</code> , ys und das Konfigurationsobjekt.  Im Konfigurationsobjekt legen wir fest, wie viele Epochen das Modell, die Paketgröße und der Rückruf, den TensorFlow.js nach der Verarbeitung jedes Pakets generiert, trainiert werden. <br><br>  Die Paketgröße bestimmt <code>xs</code> und <code>ys</code> für das Training des Modells in einer Ära.  Für jede Epoche wählt TensorFlow.js eine Teilmenge von <code>xs</code> und den entsprechenden Elementen aus <code>ys</code> , führt eine direkte Verteilung durch, empfängt die Ausgabe der Schicht mit <code>sigmoid</code> Aktivierung und führt dann basierend auf dem Verlust eine Optimierung unter Verwendung des <code>adam</code> Algorithmus durch. <br><br>  Nach dem Starten des Trainingsskripts sehen Sie ein ähnliches Ergebnis wie das folgende: <br><br><pre>  Kosten: 0,84212, Genauigkeit: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 Verlust = 0,84 Kosten: 0,79740, Genauigkeit: 1,00000
 eta = 0,2 =&gt; --------- acc = 1,00 Verlust = 0,80 Kosten: 0,81533, Genauigkeit: 1,00000
 eta = 0,2 ==&gt; -------- acc = 1,00 Verlust = 0,82 Kosten: 0,64303, Genauigkeit: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 Verlust = 0,64 Kosten: 0,51377, Genauigkeit: 0,00000
 eta = 0,2 ====&gt; ------ acc = 0,00 Verlust = 0,51 Kosten: 0,46473, Genauigkeit: 0,50000
 eta = 0,1 =====&gt; ----- acc = 0,50 Verlust = 0,46 Kosten: 0,50872, Genauigkeit: 0,00000
 eta = 0,1 ======&gt; ---- acc = 0,00 Verlust = 0,51 Kosten: 0,62556, Genauigkeit: 1,00000
 eta = 0,1 =======&gt; --- acc = 1,00 Verlust = 0,63 Kosten: 0,65133, Genauigkeit: 0,50000
 eta = 0,1 ========&gt; - acc = 0,50 Verlust = 0,65 Kosten: 0,63824, Genauigkeit: 0,50000
 eta = 0.0 ===========&gt;
 293 ms 14675us / Schritt - acc = 0,60 Verlust = 0,65
 Epoche 3/50
 Kosten: 0,44661, Genauigkeit: 1,00000
 eta = 0,3&gt; ---------- acc = 1,00 Verlust = 0,45 Kosten: 0,78060, Genauigkeit: 1,00000
 eta = 0,3 =&gt; --------- acc = 1,00 Verlust = 0,78 Kosten: 0,79208, Genauigkeit: 1,00000
 eta = 0,3 ==&gt; -------- acc = 1,00 Verlust = 0,79 Kosten: 0,49072, Genauigkeit: 0,50000
 eta = 0,2 ===&gt; ------- acc = 0,50 Verlust = 0,49 Kosten: 0,62232, Genauigkeit: 1,00000
 eta = 0,2 ====&gt; ------ acc = 1,00 Verlust = 0,62 Kosten: 0,82899, Genauigkeit: 1,00000
 eta = 0,2 =====&gt; ----- acc = 1,00 Verlust = 0,83 Kosten: 0,67629, Genauigkeit: 0,50000
 eta = 0,1 ======&gt; ---- acc = 0,50 Verlust = 0,68 Kosten: 0,62621, Genauigkeit: 0,50000
 eta = 0,1 =======&gt; --- acc = 0,50 Verlust = 0,63 Kosten: 0,46077, Genauigkeit: 1,00000
 eta = 0,1 ========&gt; - acc = 1,00 Verlust = 0,46 Kosten: 0,62076, Genauigkeit: 1,00000
 eta = 0.0 ===========&gt;
 304 ms 15221us / Schritt - acc = 0,85 Verlust = 0,63 </pre><br>  Beachten Sie, wie die Genauigkeit mit der Zeit zunimmt und der Verlust abnimmt. <br><br>  In meinem Datensatz zeigte das Modell nach dem Training eine Genauigkeit von 92%.  Beachten Sie, dass die Genauigkeit aufgrund der kleinen Trainingsdaten möglicherweise nicht sehr hoch ist. <br><br><h1>  Ausführen des Modells in einem Browser </h1><br>  Im vorherigen Abschnitt haben wir das binäre Klassifizierungsmodell trainiert.  Führen Sie es jetzt in einem Browser aus und stellen Sie eine Verbindung zu <a href="">MK.js her</a> ! <br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> video = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'cam'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> Layer = <span class="hljs-string"><span class="hljs-string">'global_average_pooling2d_1'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> mobilenetInfer = <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">m</span></span></span><span class="hljs-function"> =&gt;</span></span> (p): tf.Tensor&lt;tf.Rank&gt; =&gt; m.infer(p, Layer); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> canvas = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'canvas'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> scale = <span class="hljs-built_in"><span class="hljs-built_in">document</span></span>.getElementById(<span class="hljs-string"><span class="hljs-string">'crop'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> ImageSize = { <span class="hljs-attr"><span class="hljs-attr">Width</span></span>: <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-attr"><span class="hljs-attr">Height</span></span>: <span class="hljs-number"><span class="hljs-number">56</span></span> }; navigator.mediaDevices .getUserMedia({ <span class="hljs-attr"><span class="hljs-attr">video</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">audio</span></span>: <span class="hljs-literal"><span class="hljs-literal">false</span></span> }) .then(<span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">stream</span></span></span><span class="hljs-function"> =&gt;</span></span> { video.srcObject = stream; });</code> </pre> <br>  Der obige Code enthält mehrere Erklärungen: <br><br><ul><li>  <code>video</code> enthält einen Link zum <code>HTML5 video</code> auf der Seite </li><li> <code>Layer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> enthält den Namen des Layers aus MobileNet, von dem die Ausgabe abgerufen und als Eingabe für unser Modell übergeben werden soll </font></font></li><li> <code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Eine Funktion, die eine Instanz von MobileNet verwendet und eine andere Funktion zurückgibt. </font><font style="vertical-align: inherit;">Die zurückgegebene Funktion akzeptiert Eingaben und gibt die entsprechende Ausgabe von der angegebenen MobileNet-Schicht zurück.</font></font></li><li> <code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gibt das Element an </font></font><code>HTML5 canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, mit dem Frames aus dem Video extrahiert werden</font></font></li><li> <code>scale</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- eine andere </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, mit der einzelne Frames skaliert werden</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach erhalten wir den Videostream von der Kamera des Benutzers und legen ihn als Quelle für das Element fest </font></font><code>video</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der nächste Schritt besteht darin, einen Graustufenfilter zu implementieren, </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">der seinen Inhalt </font><font style="vertical-align: inherit;">akzeptiert </font><font style="vertical-align: inherit;">und konvertiert:</font></font><br><br><pre> <code class="python hljs">const grayscale = (canvas: HTMLCanvasElement) =&gt; { const imageData = canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).getImageData(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.height); const data = imageData.data; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (let i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; data.length; i += <span class="hljs-number"><span class="hljs-number">4</span></span>) { const avg = (data[i] + data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] + data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>]) / <span class="hljs-number"><span class="hljs-number">3</span></span>; data[i] = avg; data[i + <span class="hljs-number"><span class="hljs-number">1</span></span>] = avg; data[i + <span class="hljs-number"><span class="hljs-number">2</span></span>] = avg; } canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).putImageData(imageData, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Als nächsten Schritt verbinden wir das Modell mit MK.js: </font></font><br><br><pre> <code class="python hljs">let mobilenet: (p: any) =&gt; tf.Tensor&lt;tf.Rank&gt;; tf.loadModel(<span class="hljs-string"><span class="hljs-string">'http://localhost:5000/model.json'</span></span>).then(model =&gt; { mobileNet .load() .then((mn: any) =&gt; mobilenet = mobilenetInfer(mn)) .then(startInterval(mobilenet, model)); });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im obigen Code laden wir zuerst das oben trainierte Modell und laden dann MobileNet herunter. Wir übergeben MobileNet an die Methode </font></font><code>mobilenetInfer</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, um die Ausgabe der verborgenen Netzwerkschicht zu berechnen. Danach rufen wir die Methode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">mit zwei Netzwerken als Argumente auf.</font></font><br><br><pre> <code class="python hljs">const startInterval = (mobilenet, model) =&gt; () =&gt; { setInterval(() =&gt; { canvas.getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>).drawImage(video, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>); grayscale(scale .getContext(<span class="hljs-string"><span class="hljs-string">'2d'</span></span>) .drawImage( canvas, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, canvas.width, canvas.width / (ImageSize.Width / ImageSize.Height), <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, ImageSize.Width, ImageSize.Height )); const [punching] = Array.<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>(( model.predict(mobilenet(tf.fromPixels(scale))) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D) .dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> Float32Array); const detect = (window <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punching &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) detect &amp;&amp; detect.onPunch(); }, <span class="hljs-number"><span class="hljs-number">100</span></span>); };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der interessanteste Teil beginnt in der Methode </font></font><code>startInterval</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">! Zuerst führen wir ein Intervall aus, in dem jeder </font></font><code>100ms</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">eine anonyme Funktion aufruft. Darin wird das </font></font><code>canvas</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Video mit dem aktuellen Frame zuerst darüber gerendert. Dann reduzieren wir die Rahmengröße auf </font></font><code>100x56</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und wenden einen Graustufenfilter darauf an. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der nächste Schritt besteht darin, den Frame an MobileNet zu übertragen, die Ausgabe von der gewünschten verborgenen Ebene abzurufen und als Eingabe für die Methode </font></font><code>predict</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unseres Modells zu übertragen. Das gibt einen Tensor mit einem Element zurück. Mit erhalten </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wir den Wert vom Tensor und weisen ihn einer Konstanten zu </font></font><code>punching</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schließlich prüfen wir: Wenn die Wahrscheinlichkeit eines Handschlags größer ist </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, rufen wir die </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">globale Objektmethode auf </font></font><code>Detect</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. MK.js stellt ein globales Objekt mit drei Methoden bereit:</font></font><code>onKick</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>onPunch</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Und </font></font><code>onStand</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dass wir verwendet werden können , </font><font style="vertical-align: inherit;">einen der Charaktere zu steuern.</font></font><br><br>  Fertig!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hier ist das Ergebnis! </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/83e/05c/e0e/83e05ce0e9304865bb6aee072204902b.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Tritt- und Armerkennung mit N-Klassifizierung </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im nächsten Abschnitt werden wir ein intelligenteres Modell erstellen: ein neuronales Netzwerk, das Schläge, Tritte und andere Bilder erkennt. </font><font style="vertical-align: inherit;">Beginnen wir dieses Mal mit der Vorbereitung des Trainingssets:</font></font><br><br><pre> <code class="python hljs">const punches = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Punches) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Punches}/${f}`); const kicks = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Kicks) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Kicks}/${f}`); const others = require(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>) .readdirSync(Others) .filter(f =&gt; f.endsWith(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>)) .map(f =&gt; `${Others}/${f}`); const ys = tf.tensor2d( new Array(punches.length) .fill([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>]) .concat(new Array(kicks.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>])) .concat(new Array(others.length).fill([<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>])), [punches.length + kicks.length + others.length, <span class="hljs-number"><span class="hljs-number">3</span></span>] ); const xs: tf.Tensor2D = tf.stack( punches .map((path: string) =&gt; mobileNet(readInput(path))) .concat(kicks.map((path: string) =&gt; mobileNet(readInput(path)))) .concat(others.map((path: string) =&gt; mobileNet(readInput(path)))) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor2D;</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nach wie vor lesen wir zuerst die Kataloge mit Bildern von Schlägen von Hand, Fuß und anderen Bildern. Danach bilden wir im Gegensatz zum letzten Mal das erwartete Ergebnis in Form eines zweidimensionalen Tensors und nicht eindimensional. Wenn wir </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">n</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bilder mit einem Stempel, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">m</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Bilder mit einem Kick und </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">k</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> andere Bilder, die Tensor </font></font><code>ys</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wird </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elemente des Wertes </font></font><code>[1, 0, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>m</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">die Elemente mit dem Wert </font></font><code>[0, 1, 0]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und </font></font><code>k</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gegenstände mit Wert </font></font><code>[0, 0, 1]</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ein Vektor von </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elementen, in dem es </font></font><code>n - 1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Elemente mit einem Wert </font></font><code>0</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und ein Element mit einem Wert gibt </font></font><code>1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, nennen wir einen einheitlichen Vektor (One-Hot-Vektor). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach bilden wir den Eingangstensor</font></font><code>xs</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stapeln der Ausgabe jedes Bildes aus MobileNet. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Hier müssen Sie die Modelldefinition aktualisieren:</font></font><br><br><pre> <code class="python hljs">const model = tf.sequential(); model.add(tf.layers.inputLayer({ inputShape: [<span class="hljs-number"><span class="hljs-number">1024</span></span>] })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">1024</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'relu'</span></span> })); model.add(tf.layers.dense({ units: <span class="hljs-number"><span class="hljs-number">3</span></span>, activation: <span class="hljs-string"><span class="hljs-string">'softmax'</span></span> })); <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> model.compile({ optimizer: tf.train.adam(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>), loss: tf.losses.sigmoidCrossEntropy, metrics: [<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>] });</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die einzigen zwei Unterschiede zum Vorgängermodell sind: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Anzahl der Einheiten in der Ausgabeebene </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aktivierungen in der Ausgabeebene </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Es gibt drei Einheiten in der Ausgabeebene, da wir drei verschiedene Kategorien von Bildern haben: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Handschlag </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Kick </font></font></li><li>  Andere </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bei diesen drei Einheiten </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wird die </font><font style="vertical-align: inherit;">Aktivierung ausgelöst </font><font style="vertical-align: inherit;">, wodurch ihre Parameter in einen Tensor mit drei Werten umgewandelt werden. Warum drei Einheiten für die Ausgabeschicht? Jede der drei Werte für drei Klassen können durch zwei Bits dargestellt werden: </font></font><code>00</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>01</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>10</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. Die Summe der Werte des erstellten Tensors </font></font><code>softmax</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ist 1, dh wir erhalten niemals 00, sodass wir keine Bilder einer der Klassen klassifizieren können. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Nachdem </font></font><code>500</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ich </font><font style="vertical-align: inherit;">das Modell im Laufe der </font><font style="vertical-align: inherit;">Zeit </font><font style="vertical-align: inherit;">trainiert hatte </font><font style="vertical-align: inherit;">, erreichte ich eine Genauigkeit von ca. 92%! Das ist nicht schlecht, aber vergessen Sie nicht, dass das Training mit einem kleinen Datensatz durchgeführt wurde. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Der nächste Schritt ist das Ausführen des Modells in einem Browser! Da die Logik dem Ausführen des Modells für die binäre Klassifizierung sehr ähnlich ist, sehen Sie sich den letzten Schritt an, in dem die Aktion basierend auf der Ausgabe des Modells ausgewählt wird:</font></font><br><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> [punch, kick, nothing] = <span class="hljs-built_in"><span class="hljs-built_in">Array</span></span>.from((model.predict( mobilenet(tf.fromPixels(scaled)) ) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf.Tensor1D).dataSync() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-built_in"><span class="hljs-built_in">Float32Array</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> detect = (<span class="hljs-built_in"><span class="hljs-built_in">window</span></span> <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> any).Detect; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nothing &gt;= <span class="hljs-number"><span class="hljs-number">0.4</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (kick &gt; punch &amp;&amp; kick &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) { detect.onKick(); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (punch &gt; kick &amp;&amp; punch &gt;= <span class="hljs-number"><span class="hljs-number">0.35</span></span>) detect.onPunch();</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zuerst rufen wir MobileNet mit einem reduzierten Rahmen in Graustufen auf, dann übertragen wir das Ergebnis unseres trainierten Modells. </font><font style="vertical-align: inherit;">Das Modell gibt einen eindimensionalen Tensor zurück, den wir in </font></font><code>Float32Array</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font><font style="vertical-align: inherit;">konvertieren </font></font><code>dataSync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">Im nächsten Schritt wandeln wir </font></font><code>Array.from</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ein typisiertes Array in ein JavaScript-Array um. </font><font style="vertical-align: inherit;">Dann extrahieren wir die Wahrscheinlichkeiten, dass ein Schuss mit einer Hand, ein Tritt oder nichts auf dem Rahmen vorhanden ist. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn die Wahrscheinlichkeit des dritten Ergebnisses überschritten wird </font></font><code>0.4</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, kehren wir zurück. </font><font style="vertical-align: inherit;">Andernfalls </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">senden wir </font><font style="vertical-align: inherit;">, wenn die Wahrscheinlichkeit eines Tritts höher </font><font style="vertical-align: inherit;">ist, einen Trittbefehl an MK.js. </font><font style="vertical-align: inherit;">Wenn die Wahrscheinlichkeit eines Tritts höher </font></font><code>0.32</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">und höher ist als die Wahrscheinlichkeit eines Tritts, senden wir die Aktion eines Tritts. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Im Allgemeinen ist das alles! </font><font style="vertical-align: inherit;">Das Ergebnis ist unten dargestellt:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/168/f71/f3d/168f71f3df8d267bec3e0791d5857c64.gif"><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aktionserkennung </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie einen großen und vielfältigen Datensatz über Personen sammeln, die mit Händen und Füßen schlagen, können Sie ein Modell erstellen, das sich hervorragend für einzelne Frames eignet. Aber ist das genug? Was ist, wenn wir noch weiter gehen und zwei verschiedene Arten von Tritten unterscheiden wollen: von einer Kurve und von einem Rücken (Rücktritt). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie in den folgenden Frames zu sehen ist, sehen beide Striche zu einem bestimmten Zeitpunkt aus einem bestimmten Winkel gleich aus: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/6c1/567/5bf/6c15675bf7b8c238e7ce9d5aaefeea80.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a60/e3c/dba/a60e3cdba0eb3ecbc8730c39bc6c95b2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wenn Sie jedoch die Leistung betrachten, sind die Bewegungen völlig unterschiedlich: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e72/28b/fe8/e7228bfe8cfe9bbe73f9011d94778a7a.gif"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wie können Sie ein neuronales Netzwerk trainieren, um die Abfolge von Frames und nicht nur einen Frame zu analysieren? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Zu diesem Zweck können wir eine andere Klasse neuronaler Netze untersuchen, die als wiederkehrende neuronale Netze (RNNs) bezeichnet werden. Zum Beispiel eignen sich RNNs hervorragend für die Arbeit mit Zeitreihen:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Natural Language Processing (NLP), wobei jedes Wort vom vorherigen und nachfolgenden abhängt </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Vorhersage der nächsten Seite basierend auf Ihrem Browserverlauf </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Rahmenerkennung </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Implementierung eines solchen Modells würde den Rahmen dieses Artikels sprengen. Schauen wir uns jedoch eine Beispielarchitektur an, um eine Vorstellung davon zu erhalten, wie all dies zusammenarbeitet. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die Kraft von RNN </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Das folgende Diagramm zeigt das Modell der Erkennung von Aktionen: </font></font><br><br><img src="https://habrastorage.org/webt/kz/oq/ie/kzoqieod8t9nhs_taapnhpr_y0c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir nehmen die letzten </font></font><code>n</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bilder aus dem Video und übertragen sie an CNN. </font><font style="vertical-align: inherit;">Der CNN-Ausgang für jeden Rahmen wird als Eingangs-RNN übertragen. </font><font style="vertical-align: inherit;">Ein wiederkehrendes neuronales Netzwerk bestimmt die Beziehungen zwischen einzelnen Frames und erkennt, welcher Aktion sie entsprechen.</font></font><br><br><h1>  Fazit </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In diesem Artikel haben wir ein Bildklassifizierungsmodell entwickelt. Zu diesem Zweck haben wir einen Datensatz gesammelt: Wir haben Videobilder extrahiert und sie manuell in drei Kategorien unterteilt. Dann wurden die Daten </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">durch</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Hinzufügen von Bildern mit </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">imgaug erweitert</font></a><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Danach haben wir erklärt, was Lerntransfer ist, und das trainierte </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MobileNet-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Modell aus dem </font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;">@ tensorflow-models / mobilet-</font></a><font style="vertical-align: inherit;"> Paket für unsere Zwecke verwendet </font><font style="vertical-align: inherit;">. Wir haben MobileNet aus einer Datei im Node.js-Prozess geladen und eine zusätzliche dichte Schicht trainiert, in der Daten aus der verborgenen MobileNet-Schicht eingespeist wurden. Nach dem Training haben wir eine Genauigkeit von mehr als 90% erreicht! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Um dieses Modell in einem Browser zu verwenden, haben wir es zusammen mit MobileNet heruntergeladen und alle 100 ms damit begonnen, Frames von der Webcam des Benutzers zu kategorisieren. Wir haben das Modell mit dem Spiel verbunden</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MK.js</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> und verwendete die Modellausgabe, um eines der Zeichen zu steuern. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schließlich haben wir uns angesehen, wie das Modell verbessert werden kann, indem es mit einem wiederkehrenden neuronalen Netzwerk kombiniert wird, um Aktionen zu erkennen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ich hoffe, Ihnen hat dieses kleine Projekt nicht weniger gefallen als mir! </font><font style="vertical-align: inherit;">‍</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de428019/">https://habr.com/ru/post/de428019/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de428003/index.html">Responsive Design: Beibehaltung der Form von Markup-Elementen</a></li>
<li><a href="../de428005/index.html">Drei effektive Möglichkeiten, um eine PR-Katastrophe zu verschärfen</a></li>
<li><a href="../de428007/index.html">Schon kein tragbarer PC, noch kein Notebook: Laptop TOSHIBA T3100 / 20</a></li>
<li><a href="../de428009/index.html">Equifax: ein Jahr nach dem größten Datenleck</a></li>
<li><a href="../de428011/index.html">Weltraum-Zombielieder</a></li>
<li><a href="../de428021/index.html">Dichtungen gegen das neuronale Netz. Oder wählen Sie ein neuronales Netzwerk aus und führen Sie es aus, um Objekte auf dem Raspberry Zero zu erkennen</a></li>
<li><a href="../de428023/index.html">Grundlagen der elektrischen Sicherheit bei der Konstruktion elektronischer Geräte</a></li>
<li><a href="../de428025/index.html">Anschließen einer Auslagerungsdatei (SWAP) in MAC OS X bei Verwendung einer externen SSD als System</a></li>
<li><a href="../de428027/index.html">Wie ich versucht habe, einen statischen GLSL-Analysator herzustellen (und was schief gelaufen ist)</a></li>
<li><a href="../de428029/index.html">Digitale Veranstaltungen in Moskau vom 29. Oktober bis 4. November</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>