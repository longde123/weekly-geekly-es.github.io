<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üî∂ ü§öüèª üê∂ Parsim 25 TB com AWK e R üìÜ üë®‚Äçüë©‚Äçüëß‚Äçüë¶ ü§òüèΩ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Como ler este artigo : Pe√ßo desculpas pelo fato de o texto ter sido t√£o longo e ca√≥tico. Para economizar seu tempo, come√ßo cada cap√≠tulo com a introdu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Parsim 25 TB com AWK e R</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mailru/blog/456392/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/9d/3y/lc/9d3ylcjuqiv6r7vrv6p52apvmne.jpeg"></div><br>  <i><b>Como ler este artigo</b> : Pe√ßo desculpas pelo fato de o texto ter sido t√£o longo e ca√≥tico.</i>  <i>Para economizar seu tempo, come√ßo cada cap√≠tulo com a introdu√ß√£o de "O que aprendi", no qual explico a ess√™ncia do cap√≠tulo em uma ou duas frases.</i> <i><br><br></i>  <i><b>"Apenas mostre a solu√ß√£o!"</b></i>  <i>Se voc√™ quiser apenas ver o que eu cheguei, v√° para o cap√≠tulo "Torne-se mais inventivo", mas acho mais interessante e √∫til ler sobre falhas.</i> <br><br>  Recentemente, fui instru√≠do a configurar um processo para processar um grande volume das seq√º√™ncias de DNA originais (tecnicamente, esse √© um chip SNP).  Era necess√°rio obter rapidamente dados sobre um determinado local gen√©tico (chamado SNP) para modelagem subsequente e outras tarefas.  Com a ajuda do R e do AWK, consegui limpar e organizar os dados de maneira natural, acelerando bastante o processamento de solicita√ß√µes.  Isso n√£o foi f√°cil para mim e exigiu in√∫meras itera√ß√µes.  Este artigo ajudar√° voc√™ a evitar alguns dos meus erros e a demonstrar o que fiz no final. <br><a name="habracut"></a><br>  Primeiro, algumas explica√ß√µes introdut√≥rias. <br><br><h2>  Dados </h2><br>  Nosso Centro de Processamento de Informa√ß√µes Gen√©ticas da Universidade nos forneceu 25 TB de dados TSV.  Dividi-os em 5 pacotes compactados pelo Gzip, cada um contendo cerca de 240 arquivos de quatro gigabytes.  Cada linha continha dados para um SNP de uma pessoa.  No total, foram transmitidos dados sobre ~ 2,5 milh√µes de SNPs e ~ 60 mil pessoas.  Al√©m das informa√ß√µes do SNP, havia numerosas colunas nos arquivos com n√∫meros refletindo v√°rias caracter√≠sticas, como intensidade de leitura, frequ√™ncia de diferentes alelos etc.  Havia cerca de 30 colunas com valores √∫nicos. <br><br><h4>  Finalidade </h4><br>  Como em qualquer projeto de gerenciamento de dados, o mais importante era determinar como os dados seriam usados.  Nesse caso, <b>na maioria das vezes, selecionaremos modelos e fluxos de trabalho para o SNP com base no SNP</b> .  Ou seja, ao mesmo tempo, precisaremos de dados para apenas um SNP.  Eu tive que aprender a extrair todos os registros relacionados a um dos 2,5 milh√µes de SNPs da maneira mais simples poss√≠vel, mais r√°pida e barata. <br><br><h1>  Como n√£o fazer </h1><br>  Vou citar um clich√™ adequado: <br><br><blockquote>  N√£o falhei mil vezes, acabei de descobrir mil maneiras de n√£o analisar um monte de dados em um formato conveniente para consultas. </blockquote><br>
<h2>  Primeira tentativa </h2><br>  <b>O que aprendi</b> : n√£o h√° uma maneira barata de analisar 25 TB por vez. <br><br>  Depois de ouvir o assunto "M√©todos avan√ßados de processamento de Big Data" na Universidade de Vanderbilt, tive certeza de que era um chap√©u.  Talvez demore uma ou duas horas para configurar o servidor Hive para executar todos os dados e informar o resultado.  Como nossos dados s√£o armazenados no AWS S3, usei o servi√ßo <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Athena</a> , que permite aplicar consultas do Hive SQL aos dados do S3.  N√£o √© necess√°rio configurar / aumentar o cluster do Hive e pagar apenas pelos dados que voc√™ est√° procurando. <br><br>  Depois de mostrar a Athena meus dados e seu formato, executei alguns testes com consultas semelhantes: <br><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br>  E rapidamente obteve resultados bem estruturados.  Feito. <br><br>  At√© tentarmos usar os dados no trabalho ... <br><br>  Pediram-me para extrair todas as informa√ß√µes do SNP para testar o modelo.  Eu executei uma consulta: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> intensityData <span class="hljs-keyword"><span class="hljs-keyword">where</span></span> snp = <span class="hljs-string"><span class="hljs-string">'rs123456'</span></span>;</code> </pre> <br>  ... e esperou.  Ap√≥s oito minutos e mais de 4 TB dos dados solicitados, obtive o resultado.  Athena cobra uma taxa pela quantidade de dados encontrada, a US $ 5 por terabyte.  Portanto, essa solicita√ß√£o √∫nica custa US $ 20 e oito minutos de espera.  Para executar o modelo de acordo com todos os dados, foi necess√°rio esperar 38 anos e pagar US $ 50 milh√µes, o que obviamente n√£o nos convinha. <br><br><h2>  Era necess√°rio usar Parquet ... </h2><br>  <b>O que aprendi</b> : Tenha cuidado com o tamanho dos seus arquivos Parquet e com a organiza√ß√£o deles. <br><br>  No come√ßo, tentei corrigir a situa√ß√£o convertendo todos os TSVs em <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">arquivos Parquet</a> .  Eles s√£o convenientes para trabalhar com grandes conjuntos de dados, porque as informa√ß√µes neles s√£o armazenadas em forma de coluna: cada coluna est√° em seu pr√≥prio segmento de mem√≥ria / disco, diferente dos arquivos de texto nos quais as linhas cont√™m elementos de cada coluna.  E se voc√™ precisar encontrar algo, basta ler a coluna necess√°ria.  Al√©m disso, um intervalo de valores √© armazenado em cada arquivo de uma coluna; portanto, se o valor desejado n√£o estiver no intervalo da coluna, o Spark n√£o perder√° tempo digitalizando o arquivo inteiro. <br><br>  Executei uma tarefa simples do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">AWS Glue</a> para converter nossos TSVs em Parquet e soltei novos arquivos no Athena.  Demorou cerca de 5 horas.  Mas quando lancei a solicita√ß√£o, demorou quase o mesmo tempo e um pouco menos de dinheiro para conclu√≠-la.  O fato √© que o Spark, tentando otimizar a tarefa, simplesmente desempacotou um peda√ßo de TSV e o colocou em seu pr√≥prio peda√ßo de parquet.  E como cada peda√ßo era grande o suficiente e continha os registros completos de muitas pessoas, todos os SNPs eram armazenados em cada arquivo; portanto, o Spark precisava abrir todos os arquivos para extrair as informa√ß√µes necess√°rias. <br><br>  Curiosamente, o tipo de compacta√ß√£o padr√£o (e recomendado) no Parquet - snappy - n√£o √© divis√≠vel.  Portanto, cada executor manteve a tarefa de descompactar e baixar o conjunto de dados completo de 3,5 GB. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f42/584/fb3/f42584fb3e65319eef46f117c11525f3.png"><br><h2>  Entendemos o problema </h2><br>  <b>O que aprendi</b> : a classifica√ß√£o √© dif√≠cil, principalmente se os dados forem distribu√≠dos. <br><br>  Pareceu-me que agora eu entendia a ess√™ncia do problema.  Tudo o que eu precisava fazer era classificar os dados por coluna SNP, n√£o por pessoas.  Em seguida, v√°rios SNPs ser√£o armazenados em um bloco de dados separado, e a fun√ß√£o inteligente do Parquet "abrir apenas se o valor estiver no intervalo" se manifestar√° em toda a sua gl√≥ria.  Infelizmente, classificar bilh√µes de linhas espalhadas por um cluster provou ser uma tarefa assustadora. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1105127759318319105"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  A AWS certamente n√£o deseja devolver o dinheiro por causa de "eu sou um estudante distra√≠do".  Depois que comecei a classificar no Amazon Glue, ele funcionou por 2 dias e caiu. <br><br><h2>  E o particionamento? </h2><br>  <b>O que eu aprendi</b> : Parti√ß√µes no Spark devem ser equilibradas. <br><br>  Ent√£o surgiu a id√©ia de particionar os dados nos cromossomos.  Existem 23 deles (e mais alguns, devido ao DNA mitocondrial e √°reas n√£o mapeadas). <br>  Isso permitir√° que voc√™ divida os dados em por√ß√µes menores.  Se voc√™ adicionar apenas uma linha <code>partition_by = "chr"</code> √† fun√ß√£o de exporta√ß√£o Spark no script Glue, os dados dever√£o ser classificados em buckets. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/652/f42/3dc/652f423dc8806401b6638a3cf8c1480b.png"><br>  <i>O genoma consiste em v√°rios fragmentos chamados cromossomos.</i> <br><br>  Infelizmente, isso n√£o funcionou.  Os cromossomos t√™m tamanhos diferentes e, portanto, uma quantidade diferente de informa√ß√µes.  Isso significa que as tarefas que o Spark enviou aos trabalhadores n√£o foram balanceadas e executadas lentamente, porque alguns n√≥s foram conclu√≠dos anteriormente e estavam inativos.  No entanto, as tarefas foram conclu√≠das.  Mas, ao solicitar um SNP, o desequil√≠brio novamente causou problemas.  O custo do processamento de SNPs em cromossomos maiores (ou seja, de onde queremos obter os dados) diminuiu apenas 10 vezes.  Muito, mas n√£o o suficiente. <br><br><h2>  E se voc√™ se dividir em parti√ß√µes ainda menores? </h2><br>  <b>O que eu aprendi</b> : nunca tente fazer 2,5 milh√µes de parti√ß√µes. <br><br>  Decidi dar um passeio e particionei todos os SNP.  Isso garantiu o mesmo tamanho de parti√ß√µes.  <b>RUIM ERA UMA IDEIA</b> .  Aproveitei o Glue e adicionei a linha <code>partition_by = 'snp'</code> inocente.  A tarefa iniciou e come√ßou a executar.  Um dia depois, verifiquei e vi que nada estava escrito no S3 at√© agora; ent√£o, matei a tarefa.  Parece que o Glue estava gravando arquivos intermedi√°rios em um local oculto no S3, e muitos arquivos, talvez alguns milh√µes.  Como resultado, meu erro custou mais de mil d√≥lares e n√£o agradou meu mentor. <br><br><h2>  Particionamento + classifica√ß√£o </h2><br>  <b>O que aprendi</b> : a classifica√ß√£o ainda √© dif√≠cil, assim como a instala√ß√£o do Spark. <br><br>  A √∫ltima tentativa de particionamento foi que particionei os cromossomos e depois classifiquei cada parti√ß√£o.  Em teoria, isso aceleraria cada solicita√ß√£o, porque os dados SNP desejados deveriam estar dentro de v√°rios blocos do Parquet dentro de um determinado intervalo.  Infelizmente, classificar at√© os dados particionados provou ser uma tarefa dif√≠cil.  Como resultado, mudei para o EMR para um cluster personalizado e usei oito inst√¢ncias poderosas (C5.4xl) e Sparklyr para criar um fluxo de trabalho mais flex√≠vel ... <br><br><pre> <code class="scala hljs"># <span class="hljs-type"><span class="hljs-type">Sparklyr</span></span> snippet to partition by chr and sort w/in partition # <span class="hljs-type"><span class="hljs-type">Join</span></span> the raw data <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the snp bins raw_data group_by(chr) %&gt;% arrange(<span class="hljs-type"><span class="hljs-type">Position</span></span>) %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_write_Parquet</span></span>( path = <span class="hljs-type"><span class="hljs-type">DUMP_LOC</span></span>, mode = <span class="hljs-symbol"><span class="hljs-symbol">'overwrit</span></span>e', partition_by = c(<span class="hljs-symbol"><span class="hljs-symbol">'ch</span></span>r') )</code> </pre> <br>  ... no entanto, a tarefa ainda n√£o foi conclu√≠da.  Eu sintonizei de todas as formas: aumentei a aloca√ß√£o de mem√≥ria para cada executor de consulta, usei n√≥s com uma grande quantidade de mem√≥ria, usei vari√°veis ‚Äã‚Äãde transmiss√£o, mas cada vez que isso se transformou em meias medidas e gradualmente os executores come√ßaram a falhar, at√© que tudo parasse. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1128703858610450434"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  Estou ficando mais inventivo </h1><br>  <b>O que aprendi</b> : algumas vezes dados especiais requerem solu√ß√µes especiais. <br><br>  Cada SNP tem um valor de posi√ß√£o.  Este √© o n√∫mero correspondente ao n√∫mero de bases ao longo de seu cromossomo.  Essa √© uma maneira boa e natural de organizar nossos dados.  No come√ßo, eu queria dividir por regi√£o de cada cromossomo.  Por exemplo, posi√ß√µes 1 - 2000, 2001 - 4000, etc.  Mas o problema √© que os SNPs n√£o s√£o distribu√≠dos igualmente entre os cromossomos, e √© por isso que o tamanho dos grupos varia muito. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f46/a8e/17b/f46a8e17b9af8d2ae9777c47017764c6.png"><br><br>  Como resultado, vim para ser dividido em categorias (posi√ß√µes).  De acordo com os dados j√° baixados, solicitei uma lista de SNPs exclusivos, suas posi√ß√µes e cromossomos.  Depois, ele classificou os dados dentro de cada cromossomo e coletou o SNP em grupos (bin) de um determinado tamanho.  Diga 1000 SNP cada.  Isso me deu um relacionamento SNP com um grupo no cromossomo. <br><br>  No final, criei grupos (bin) no SNP 75, explicarei o motivo abaixo. <br><br><pre> <code class="bash hljs">snp_to_bin &lt;- unique_snps %&gt;% group_by(chr) %&gt;% arrange(position) %&gt;% mutate( rank = 1:n() bin = floor(rank/snps_per_bin) ) %&gt;% ungroup()</code> </pre> <br><h2>  Primeira tentativa com Spark </h2><br>  <b>O que aprendi</b> : a integra√ß√£o do Spark √© r√°pida, mas o particionamento ainda √© caro. <br><br>  Eu queria ler esse pequeno quadro de dados (2,5 milh√µes de linhas) no Spark, combin√°-lo com dados brutos e depois particionar pela coluna <code>bin</code> rec√©m-adicionada. <br><br><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment"># Join the raw data with the snp bins data_w_bin &lt;- raw_data %&gt;% left_join(sdf_broadcast(snp_to_bin), by ='snp_name') %&gt;% group_by(chr_bin) %&gt;% arrange(Position) %&gt;% Spark_write_Parquet( path = DUMP_LOC, mode = 'overwrite', partition_by = c('chr_bin') )</span></span></code> </pre> <br>  Como usei <code>sdf_broadcast()</code> , o Spark descobre que deve enviar um quadro de dados para todos os n√≥s.  Isso √© √∫til se os dados forem pequenos e necess√°rios para todas as tarefas.  Caso contr√°rio, o Spark tenta ser inteligente e distribui os dados conforme necess√°rio, o que pode causar freios. <br><br>  E, novamente, minha ideia n√£o funcionou: as tarefas funcionaram por um tempo, conclu√≠ram a fus√£o e, como os executores lan√ßados pelo particionamento, come√ßaram a falhar. <br><br><h2>  Adicionar AWK </h2><br>  <b>O que aprendi</b> : n√£o durma quando o b√°sico lhe ensinar.  Certamente algu√©m j√° resolveu seu problema nos anos 80. <br><br>  At√© o momento, a causa de todas as minhas falhas no Spark foi a confus√£o de dados no cluster.  Talvez a situa√ß√£o possa ser melhorada pelo pr√©-processamento.  Decidi tentar dividir os dados de texto bruto em colunas de cromossomos, ent√£o esperava fornecer ao Spark dados "pr√©-particionados". <br><br>  Pesquisei no StackOverflow como descobrir os valores das colunas e encontrei <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">uma resposta √≥tima.</a>  Usando o AWK, voc√™ pode dividir um arquivo de texto em valores de coluna gravando no script, em vez de enviar os resultados para o <code>stdout</code> . <br><br>  Para testar, escrevi um script Bash.  Baixei um dos TSVs compactados, descompactei-o com o <code>gzip</code> e enviei-o para o <code>awk</code> . <br><br><pre> <code class="bash hljs">gzip -dc path/to/chunk/file.gz | awk -F <span class="hljs-string"><span class="hljs-string">'\t'</span></span> \ <span class="hljs-string"><span class="hljs-string">'{print $1",..."$30"&gt;"chunked/"$chr"_chr"$15".csv"}'</span></span></code> </pre> <br>  Funcionou! <br><br><h2>  Enchimento do n√∫cleo </h2><br>  <b>O que eu aprendi</b> : <code>gnu parallel</code> √© uma coisa m√°gica, todos deveriam us√°-lo. <br><br>  A separa√ß√£o foi bastante lenta e, quando iniciei o <code>htop</code> para testar o uso de uma inst√¢ncia poderosa (e cara) do EC2, verificou-se que eu estava usando apenas um n√∫cleo e aproximadamente 200 MB de mem√≥ria.  Para resolver o problema e n√£o perder muito dinheiro, foi necess√°rio descobrir como paralelizar o trabalho.  Felizmente, na impressionante <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Data Science de</a> Jeron Janssens <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">no</a> livro <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Command Line</a> , encontrei um cap√≠tulo sobre paraleliza√ß√£o.  Com isso, aprendi sobre o <code>gnu parallel</code> , um m√©todo muito flex√≠vel para implementar multithreading no Unix. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/835/7c0/e45/8357c0e45f4162d53ca1c3da0c78444a.png" width="300"></div><br>  Quando iniciei a parti√ß√£o usando um novo processo, tudo estava bem, mas havia um gargalo - o download de objetos S3 para o disco n√£o era muito r√°pido e n√£o era completamente paralelo.  Para consertar isso, eu fiz o seguinte: <br><br><ol><li>  Descobri que √© poss√≠vel implementar a etapa de download do S3 diretamente no pipeline, eliminando completamente o armazenamento intermedi√°rio no disco.  Isso significa que posso evitar a grava√ß√£o de dados brutos no disco e usar um armazenamento ainda menor e, portanto, mais barato na AWS. <br></li><li>  O <code>aws configure set default.s3.max_concurrent_requests 50</code> comandos <code>aws configure set default.s3.max_concurrent_requests 50</code> aumentou muito o n√∫mero de threads que a CLI da AWS usa (h√° 10 por padr√£o). <br></li><li>  Mudei para a inst√¢ncia do EC2 otimizada para a velocidade da rede, com a letra n no nome.  Descobri que a perda de poder de computa√ß√£o ao usar n-inst√¢ncias √© mais do que compensada por um aumento na velocidade de download.  Para a maioria das tarefas, usei o c5n.4xl. <br></li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>pigz</code></a> <code>gzip</code> para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="><code>pigz</code></a> , esta √© uma ferramenta gzip que pode fazer coisas legais para paralelizar a tarefa inigual√°vel de descompactar arquivos (isso ajudou o m√≠nimo). <br></li></ol><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Let S3 use as many threads as it wants aws configure set default.s3.max_concurrent_requests 50 for chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do aws s3 cp s3://$batch_loc$chunk_file - | pigz -dc | parallel --block 100M --pipe \ "awk -F '\t' '{print \$1\",...\"$30\"&gt;\"chunked/{#}_chr\"\$15\".csv\"}'" # Combine all the parallel process chunks to single files ls chunked/ | cut -d '_' -f 2 | sort -u | parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}' # Clean up intermediate data rm chunked/* done</span></span></code> </pre> <br>  Essas etapas s√£o combinadas entre si para que tudo funcione muito rapidamente.  Gra√ßas √† maior velocidade de download e √† rejei√ß√£o da grava√ß√£o em disco, agora eu podia processar um pacote de 5 terabytes em apenas algumas horas. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1129416944233226240"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Este tweet deveria mencionar 'TSV'.  Infelizmente. <br><br><h2>  Usando dados analisados ‚Äã‚Äãnovamente </h2><br>  <b>O que aprendi</b> : O Spark adora dados n√£o compactados e n√£o gosta de combinar parti√ß√µes. <br><br>  Agora, os dados estavam no S3 em um formato descompactado (lido, compartilhado) e semi-ordenado, e eu poderia retornar ao Spark novamente.  Uma surpresa me esperou: novamente n√£o consegui o desejado!  Era muito dif√≠cil dizer ao Spark exatamente como os dados foram particionados.  E mesmo quando eu fiz isso, verificou-se que havia muitas parti√ß√µes (95 mil), e quando reduzi seu n√∫mero a limites coerentes com <code>coalesce</code> , isso arruinou minha parti√ß√£o.  Estou certo de que isso pode ser corrigido, mas em alguns dias de pesquisa n√£o consegui encontrar uma solu√ß√£o.  No final, conclu√≠ todas as tarefas no Spark, apesar de levar algum tempo, e meus arquivos Parquet divididos n√£o eram muito pequenos (~ 200 Kb).  No entanto, os dados eram onde eram necess√°rios. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/43b/236/ae543b236b8d37d4a6794aa63d9ada94.png"><br>  <i>Muito pequeno e diferente, maravilhoso!</i> <br><br><h2>  Testando solicita√ß√µes locais do Spark </h2><br>  <b>O que eu aprendi</b> : o Spark tem muita sobrecarga na solu√ß√£o de problemas simples. <br><br>  Ao baixar os dados em um formato inteligente, pude testar a velocidade.  Configurei um script no R para iniciar o servidor Spark local e carreguei o quadro de dados Spark no reposit√≥rio especificado dos grupos Parquet (bin).  Tentei carregar todos os dados, mas n√£o consegui que o Sparklyr reconhecesse o particionamento. <br><br><pre> <code class="scala hljs">sc &lt;- <span class="hljs-type"><span class="hljs-type">Spark_connect</span></span>(master = <span class="hljs-string"><span class="hljs-string">"local"</span></span>) desired_snp &lt;- <span class="hljs-symbol"><span class="hljs-symbol">'rs3477173</span></span>9' # <span class="hljs-type"><span class="hljs-type">Start</span></span> a timer start_time &lt;- <span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() # <span class="hljs-type"><span class="hljs-type">Load</span></span> the desired bin into <span class="hljs-type"><span class="hljs-type">Spark</span></span> intensity_data &lt;- sc %&gt;% <span class="hljs-type"><span class="hljs-type">Spark_read_Parquet</span></span>( name = <span class="hljs-symbol"><span class="hljs-symbol">'intensity_dat</span></span>a', path = get_snp_location(desired_snp), memory = <span class="hljs-type"><span class="hljs-type">FALSE</span></span> ) # <span class="hljs-type"><span class="hljs-type">Subset</span></span> bin to snp and then collect to local test_subset &lt;- intensity_data %&gt;% filter(<span class="hljs-type"><span class="hljs-type">SNP_Name</span></span> == desired_snp) %&gt;% collect() print(<span class="hljs-type"><span class="hljs-type">Sys</span></span>.time() - start_time)</code> </pre> <br>  A execu√ß√£o levou 29,415 segundos.  Muito melhor, mas n√£o muito bom para testar qualquer coisa em massa.  Al√©m disso, n√£o consegui acelerar o trabalho com o cache, porque quando eu tentava armazenar em cache o quadro de dados na mem√≥ria, o Spark sempre travava, mesmo quando eu alocava mais de 50 GB de mem√≥ria para um conjunto de dados com peso inferior a 15. <br><br><h2>  Voltar ao AWK </h2><br>  <b>O que eu aprendi</b> : matrizes associativas AWK s√£o muito eficientes. <br><br>  Eu entendi que poderia alcan√ßar uma velocidade maior.  Lembrei que no excelente guia AWK de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Bruce Barnett</a> , li sobre um recurso interessante chamado " <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">matrizes associativas</a> ".  Na verdade, esses s√£o pares de valores-chave, que por algum motivo foram chamados de maneira diferente no AWK, e, portanto, de alguma forma n√£o os mencionei particularmente.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Roman Cheplyaka</a> lembrou que o termo "matrizes associativas" √© muito mais antigo que o termo "par de valores-chave".  Mesmo se voc√™ <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pesquisar o valor-chave no Google Ngram</a> , voc√™ n√£o ver√° esse termo l√°, mas encontrar√° matrizes associativas!  Al√©m disso, o par de valores-chave √© mais frequentemente associado aos bancos de dados, portanto, √© muito mais l√≥gico comparar com o hashmap.  Percebi que poderia usar essas matrizes associativas para conectar meus SNPs √† tabela bin e aos dados brutos sem usar o Spark. <br><br>  Para isso, no script AWK, usei o bloco <code>BEGIN</code> .  Este √© um peda√ßo de c√≥digo que √© executado antes da primeira linha de dados ser transferida para o corpo principal do script. <br><br><pre> <code class="cpp hljs">join_data.awk BEGIN { FS=<span class="hljs-string"><span class="hljs-string">","</span></span>; batch_num=substr(chunk,<span class="hljs-number"><span class="hljs-number">7</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>); chunk_id=substr(chunk,<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(getline &lt; <span class="hljs-string"><span class="hljs-string">"snp_to_bin.csv"</span></span>) {bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>] = $<span class="hljs-number"><span class="hljs-number">2</span></span>} } { print $<span class="hljs-number"><span class="hljs-number">0</span></span> &gt; <span class="hljs-string"><span class="hljs-string">"chunked/chr_"</span></span>chr<span class="hljs-string"><span class="hljs-string">"_bin_"</span></span>bin[$<span class="hljs-number"><span class="hljs-number">1</span></span>]<span class="hljs-string"><span class="hljs-string">"_"</span></span>batch_num<span class="hljs-string"><span class="hljs-string">"_"</span></span>chunk_id<span class="hljs-string"><span class="hljs-string">".csv"</span></span> }</code> </pre> <br>  O comando <code>while(getline...)</code> carregou todas as linhas do grupo CSV (bin), defina a primeira coluna (nome do SNP) como a chave para a matriz associativa <code>bin</code> e o segundo valor (group) como valor.  Em seguida, no bloco <code>{</code> <code>}</code> , aplicado a todas as linhas do arquivo principal, cada linha √© enviada ao arquivo de sa√≠da, que recebe um nome exclusivo, dependendo do grupo (bin): <code>..._bin_"bin[$1]"_...</code> <br><br>  As <code>chunk_id</code> e <code>chunk_id</code> correspondiam aos dados fornecidos pelo pipeline, o que evitava o estado de corrida, e cada encadeamento de execu√ß√£o iniciado por <code>parallel</code> gravava em seu pr√≥prio arquivo exclusivo. <br><br>  Como eu espalhei todos os dados brutos em pastas nos cromossomos deixados ap√≥s minha experi√™ncia anterior com o AWK, agora eu poderia escrever outro script Bash para processar no cromossomo de uma vez e fornecer dados particionados mais profundos para o S3. <br><br><pre> <code class="bash hljs">DESIRED_CHR=<span class="hljs-string"><span class="hljs-string">'13'</span></span> <span class="hljs-comment"><span class="hljs-comment"># Download chromosome data from s3 and split into bins aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv' | parallel "echo 'reading {}'; aws s3 cp "$DATA_LOC"{} - | awk -v chr=\""$DESIRED_CHR"\" -v chunk=\"{}\" -f split_on_chr_bin.awk" # Combine all the parallel process chunks to single files and upload to rds using R ls chunked/ | cut -d '_' -f 4 | sort -u | parallel "echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds" rm chunked/*</span></span></code> </pre> <br>  O script tem duas se√ß√µes <code>parallel</code> . <br><br>  A primeira se√ß√£o l√™ dados de todos os arquivos que cont√™m informa√ß√µes sobre o cromossomo desejado e, em seguida, esses dados s√£o distribu√≠dos por fluxos que espalham arquivos nos grupos correspondentes (bin).  Para impedir que condi√ß√µes de corrida ocorram quando v√°rios fluxos s√£o gravados no mesmo arquivo, o AWK transfere os nomes dos arquivos para grava√ß√£o de dados em locais diferentes, por exemplo, <code>chr_10_bin_52_batch_2_aa.csv</code> .  Como resultado, muitos arquivos pequenos s√£o criados no disco (para isso, usei volumes EBS de terabyte). <br><br>  O pipeline da segunda se√ß√£o <code>parallel</code> percorre os grupos (bin) e combina seus arquivos individuais em CSVs comuns com <code>cat</code> , e os envia para exporta√ß√£o. <br><br><h2>  Transmitir para R? </h2><br>  <b>O que eu aprendi</b> : voc√™ pode acessar <code>stdin</code> e <code>stdout</code> partir de um script R e, portanto, us√°-lo no pipeline. <br><br>  No script Bash, voc√™ pode observar esta linha: <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  <code>...cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R...</code>  Ele traduz todos os arquivos de grupo concatenados (bin) no script R abaixo.  <code>{}</code> √© uma t√©cnica <code>parallel</code> especial que insere qualquer dado enviado por ele no fluxo especificado diretamente no pr√≥prio comando.  A op√ß√£o <code>{#}</code> fornece um ID de encadeamento exclusivo e <code>{%}</code> representa o n√∫mero do slot de trabalho (repetido, mas nunca ao mesmo tempo).  Uma lista de todas as op√ß√µes pode ser encontrada na <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">documenta√ß√£o.</a> <br><br><pre> <code class="lisp hljs"><span class="hljs-meta"><span class="hljs-meta">#!/usr/bin/env Rscript library(readr) library(aws.s3) # Read first command line argument data_destination &lt;- commandArgs(trailingOnly = TRUE)[1] data_cols &lt;- list(SNP_Name = 'c', ...) s3saveRDS( read_csv( file("stdin"), col_names = names(data_cols), col_types = data_cols ), object = data_destination )</span></span></code> </pre> <br>  Quando a vari√°vel <code>file("stdin")</code> √© passada para <code>readr::read_csv</code> , os dados traduzidos no script R s√£o carregados no quadro, que s√£o gravados diretamente no S3 como um arquivo <code>aws.s3</code> usando o <code>aws.s3</code> . <br><br>  O RDS √© um pouco como uma vers√£o mais nova do Parquet, sem os detalhes do armazenamento em coluna. <br><br>  Depois de concluir o script Bash, recebi <code>.rds</code> arquivos <code>.rds</code> no S3, o que me permitiu usar tipos eficientes de compacta√ß√£o e <code>.rds</code> . <br><br>  Apesar de usar o freio R, tudo funcionou muito r√°pido.  N√£o √© de surpreender que os fragmentos em R respons√°veis ‚Äã‚Äãpela leitura e grava√ß√£o de dados sejam bem otimizados.  Ap√≥s o teste em um cromossomo de tamanho m√©dio, a tarefa foi conclu√≠da na inst√¢ncia C5n.4xl em cerca de duas horas. <br><br><h2>  Limita√ß√µes do S3 </h2><br>  <b>O que aprendi</b> : gra√ßas √† implementa√ß√£o inteligente de caminhos, o S3 pode processar muitos arquivos. <br><br>  Eu estava preocupado se o S3 poderia lidar com muitos arquivos transferidos para ele.  Eu poderia tornar os nomes dos arquivos significativos, mas como o S3 os procurar√°? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/841/0dc/c34/8410dcc34a563c683dd7602dc66d884a.png"><br> <i>  S3    ,        <code>/</code> . <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u="> FAQ- S3.</a></i> <br><br> , S3            -      .  (bucket)   ,   ‚Äî    . <br><br>          Amazon, ,    ¬´-----¬ª  .    :       get-,       . ,      20 . bin-. ,   ,      (,      ,      ).          . <br><br><h2>    ? </h2><br>   :     ‚Äî     . <br><br>       : ¬´    ?¬ª      ( gzip CSV-   7  )      .     ,  R     Parquet ( Arrow)     Spark.       R,         ,         ,       . <br><br><h2>   </h2><br> <b>  </b> :     ,    . <br><br>       ,      . <br>     EC2  ,                 ( ,  Spark    ).  ,          ,    AWS-      10 . <br><br>      R      . <br><br>   S3 ,       . <br><br><pre> <code class="bash hljs">library(aws.s3) library(tidyverse) chr_sizes &lt;- get_bucket_df( bucket = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, prefix = <span class="hljs-string"><span class="hljs-string">'...'</span></span>, max = Inf ) %&gt;% mutate(Size = as.numeric(Size)) %&gt;% filter(Size != 0) %&gt;% mutate( <span class="hljs-comment"><span class="hljs-comment"># Extract chromosome from the file name chr = str_extract(Key, 'chr.{1,4}\\.csv') %&gt;% str_remove_all('chr|\\.csv') ) %&gt;% group_by(chr) %&gt;% summarise(total_size = sum(Size)/1e+9) # Divide to get value in GB # A tibble: 27 x 2 chr total_size &lt;chr&gt; &lt;dbl&gt; 1 0 163. 2 1 967. 3 10 541. 4 11 611. 5 12 542. 6 13 364. 7 14 375. 8 15 372. 9 16 434. 10 17 443. # ‚Ä¶ with 17 more rows</span></span></code> </pre> <br>    ,    ,   ,     <code>num_jobs</code>  ,       . <br><br><pre> <code class="bash hljs">num_jobs &lt;- 7 <span class="hljs-comment"><span class="hljs-comment"># How big would each job be if perfectly split? job_size &lt;- sum(chr_sizes$total_size)/7 shuffle_job &lt;- function(i){ chr_sizes %&gt;% sample_frac() %&gt;% mutate( cum_size = cumsum(total_size), job_num = ceiling(cum_size/job_size) ) %&gt;% group_by(job_num) %&gt;% summarise( job_chrs = paste(chr, collapse = ','), total_job_size = sum(total_size) ) %&gt;% mutate(sd = sd(total_job_size)) %&gt;% nest(-sd) } shuffle_job(1) # A tibble: 1 x 2 sd data &lt;dbl&gt; &lt;list&gt; 1 153. &lt;tibble [7 √ó 3]&gt;</span></span></code> </pre> <br>      purrr     . <br><br><pre> <code class="bash hljs">1:1000 %&gt;% map_df(shuffle_job) %&gt;% filter(sd == min(sd)) %&gt;% pull(data) %&gt;% pluck(1)</code> </pre> <br>     ,    .       Bash-    <code>for</code> .       10 .    ,             .  ,        . <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> DESIRED_CHR <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">"16"</span></span> <span class="hljs-string"><span class="hljs-string">"9"</span></span> <span class="hljs-string"><span class="hljs-string">"7"</span></span> <span class="hljs-string"><span class="hljs-string">"21"</span></span> <span class="hljs-string"><span class="hljs-string">"MT"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-comment"><span class="hljs-comment"># Code for processing a single chromosome fi</span></span></code> </pre> <br>     : <br><br><pre> <code class="bash hljs">sudo shutdown -h now</code> </pre> <br> ‚Ä¶   !   AWS CLI       <code>user_data</code>   Bash-    .     ,         . <br><br><pre> <code class="bash hljs">aws ec2 run-instances ...\ --tag-specifications <span class="hljs-string"><span class="hljs-string">"ResourceType=instance,Tags=[{Key=Name,Value=&lt;&lt;job_name&gt;&gt;}]"</span></span> \ --user-data file://&lt;&lt;job_script_loc&gt;&gt;</code> </pre> <br><h1> ! </h1><br> <b>  </b> : API        . <br><br> -        .      ,     .     API   .        <code>.rds</code>  Parquet-,       ,    .       R-. <br><br>      ,        ,    <code>get_snp</code> .       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">pkgdown</a> ,        . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a75/afb/f3a/a75afbf3a2c7c8ef5fa2a873f8ba50b9.png"><br><br><h2>   </h2><br> <b>  </b> :     ,   ! <br><br>          SNP      ,     (binning)   .     SNP,          (bin).      ( )    . <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Part of get_snp() ... # Test if our current snp data has the desired snp. already_have_snp &lt;- desired_snp %in% prev_snp_results$snps_in_bin if(!already_have_snp){ # Grab info on the bin of the desired snp snp_results &lt;- get_snp_bin(desired_snp) # Download the snp's bin data snp_results$bin_data &lt;- aws.s3::s3readRDS(object = snp_results$data_loc) } else { # The previous snp data contained the right bin so just use it snp_results &lt;- prev_snp_results } ...</span></span></code> </pre> <br>       ,       .    ,      . , <code>dplyr::filter</code>           ,           ,    . <br><br>  ,   <code>prev_snp_results</code>   <code>snps_in_bin</code> .     SNP   (bin),   ,       .        SNP   (bin)    : <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Get bin-mates snps_in_bin &lt;- my_snp_results$snps_in_bin for(current_snp in snps_in_bin){ my_snp_results &lt;- get_snp(current_snp, my_snp_results) # Do something with results }</span></span></code> </pre> <br><h1>  Resultados </h1><br>    (  )    ,   .  ,           .      . <br><br>       ,       ,     ,     ‚Ä¶ <br><br>   .       .       (  ),  ,   (bin)   ,    SNP     0,1 ,     ,     S3 . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1134151057385369600"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h2>  Conclus√£o </h2><br>   ‚Äî   .   ,     . ,    . ,   ,         ,     .  ,       ,  ,        ,    .  ,       ,    ,        ,      -     . <br><br>     .     ,        ,  ¬´¬ª  ,    .          . <br><br><h3>   : </h3><br><ul><li>      25   ; <br></li><li>      Parquet-   ; <br></li><li>   Spark   ; <br></li><li>      2,5  ; <br></li><li>    ,    Spark; <br></li><li>      ; <br></li><li>   Spark  ,      ; <br></li><li>  ,    ,  -       1980-; <br></li><li> <code>gnu parallel</code> ‚Äî   ,    ; <br></li><li> Spark        ; <br></li><li>  Spark        ; <br></li><li>    AWK  ; <br></li><li>    <code>stdin</code>  <code>stdout</code>  R-,       ; <br></li><li>     S3    ; <br></li><li>     ‚Äî     ; <br></li><li>     ,    ; <br></li><li> API        ; <br></li><li>     ,   ! <br></li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt456392/">https://habr.com/ru/post/pt456392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt456380/index.html">Dia Aberto da Faculdade de Programa√ß√£o em Netologia</a></li>
<li><a href="../pt456382/index.html">Colabora√ß√£o e automa√ß√£o no frontend. O que aprendemos com 13 escolas</a></li>
<li><a href="../pt456384/index.html">Gr√°fico de PVS-Studio de desenvolvimento de habilidades de diagn√≥stico</a></li>
<li><a href="../pt456386/index.html">Bibliotecas abertas para visualiza√ß√£o de conte√∫do de √°udio</a></li>
<li><a href="../pt456388/index.html">Gr√°fico de desenvolvimento de diagn√≥stico no PVS-Studio</a></li>
<li><a href="../pt456394/index.html">Criando a onipresente tela inicial no iOS</a></li>
<li><a href="../pt456398/index.html">Plug-ins Vue-cli, trabalhando com dados complexos e testes baseados em propriedades - an√∫ncio do Panda-Meetup Frontend</a></li>
<li><a href="../pt456400/index.html">Por que competir √© melhor que treinar: nossa experi√™ncia em gamifica√ß√£o de aprendizado</a></li>
<li><a href="../pt456402/index.html">Dentes da Sabedoria: Pull-Pull</a></li>
<li><a href="../pt456404/index.html">Looper - Plugin para Sketch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>