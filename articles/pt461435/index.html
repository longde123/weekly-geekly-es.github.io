<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üòø üçù üë®‚Äçüé® Reconhecimento de emo√ß√µes usando uma rede neural convolucional ‚òÅÔ∏è üöπ üñêüèø</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reconhecer emo√ß√µes sempre foi um desafio emocionante para os cientistas. Recentemente, estou trabalhando em um projeto SER experimental (Reconheciment...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Reconhecimento de emo√ß√µes usando uma rede neural convolucional</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/Voximplant/blog/461435/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  Reconhecer emo√ß√µes sempre foi um desafio emocionante para os cientistas.  Recentemente, estou trabalhando em um projeto SER experimental (Reconhecimento de Emo√ß√£o de Fala) para entender o potencial dessa tecnologia - para isso, selecionei os reposit√≥rios mais populares no <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Github</a> e os fiz a base do meu projeto. <br><br>  Antes de come√ßarmos a entender o projeto, ser√° bom lembrar que tipo de gargalos o SER possui. <br><a name="habracut"></a><br><h2>  Principais obst√°culos </h2><br><ul><li>  as emo√ß√µes s√£o subjetivas, at√© as pessoas as interpretam de maneira diferente.  √â dif√≠cil definir o pr√≥prio conceito de "emo√ß√£o"; </li><li>  comentar sobre o √°udio √© dif√≠cil.  De alguma forma, devemos marcar cada palavra, senten√ßa ou toda a comunica√ß√£o como um todo?  Um conjunto de que tipo de emo√ß√µes usar no reconhecimento? </li><li>  coletar dados tamb√©m n√£o √© f√°cil.  Muitos dados de √°udio podem ser coletados de filmes e not√≠cias.  No entanto, ambas as fontes s√£o "tendenciosas" porque as not√≠cias devem ser neutras e as emo√ß√µes dos atores s√£o interpretadas.  √â dif√≠cil encontrar uma fonte "objetiva" de dados de √°udio. </li><li>  dados de marca√ß√£o requerem grandes recursos humanos e de tempo.  Ao contr√°rio de desenhar quadros em imagens, requer pessoal especialmente treinado para ouvir grava√ß√µes de √°udio inteiras, analis√°-las e fornecer coment√°rios.  E esses coment√°rios devem ser apreciados por <b>muitas</b> outras pessoas, porque as classifica√ß√µes s√£o subjetivas. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Descri√ß√£o do Projeto </h2><br>  Usando uma rede neural convolucional para reconhecer emo√ß√µes em grava√ß√µes de √°udio.  E sim, o propriet√°rio do reposit√≥rio n√£o se referiu a nenhuma fonte. <br><br><h2>  Descri√ß√£o dos dados </h2><br>  Existem dois conjuntos de dados que foram usados ‚Äã‚Äãnos reposit√≥rios RAVDESS e SAVEE. Acabei de adaptar o RAVDESS no meu modelo.  Existem dois tipos de dados no contexto RAVDESS: fala e m√∫sica. <br><br>  Conjunto de dados <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">RAVDESS (o banco de dados audiovisual da Ryerson de fala e m√∫sica emocional)</a> : <br><br><ul><li>  12 atores e 12 atrizes gravaram seus discursos e m√∫sicas em sua performance; </li><li>  o ator 18 n√£o tem m√∫sicas gravadas; </li><li>  emo√ß√µes Nojo (nojo), Neutro (neutro) e Surpresas (surpresa) est√£o ausentes nos dados da "m√∫sica". </li></ul><br>  Reparti√ß√£o das Emo√ß√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Gr√°fico de distribui√ß√£o de emo√ß√µes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Extra√ß√£o de recursos </h3><br>  Quando trabalhamos com tarefas de reconhecimento de fala, os <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Coeficientes Cepstrais (MFCCs)</a> s√£o uma tecnologia avan√ßada, apesar de aparecer nos anos 80. <br><br>  Cita√ß√£o do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Tutorial</a> do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">MFCC</a> : <br><blockquote>  Essa forma determina qual √© o som de sa√≠da.  Se conseguirmos identificar o formul√°rio, ele nos dar√° uma representa√ß√£o precisa do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">fonema</a> tocado.  A forma do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">trato vocal</a> se manifesta em um envelope de curto espectro, e o trabalho da MFCC √© exibir com precis√£o esse envelope. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Forma de onda</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Spectrogram</font> <br><br>  Usamos o MFCC como um recurso de entrada.  Se voc√™ estiver interessado em aprender mais sobre o que √© o MFCC, este <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">tutorial</a> √© para voc√™.  O download de dados e a convers√£o para o formato MFCC pode ser feito facilmente usando o pacote librosa Python. <br><br><h3>  Arquitetura de modelo padr√£o </h3><br>  O autor desenvolveu um modelo CNN usando o pacote Keras, criando 7 camadas - seis camadas Con1D e uma camada de densidade (Densa). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  O autor comentou as camadas 4 e 5 na vers√£o mais recente (18 de setembro de 2018) e o tamanho final do arquivo deste modelo n√£o se encaixa na rede fornecida, portanto, n√£o √© poss√≠vel obter o mesmo resultado com precis√£o - 72%. </blockquote><br>  O modelo √© simplesmente treinado com os par√¢metros <code>batch_size=16</code> e <code>epochs=700</code> , sem nenhuma programa√ß√£o de treinamento, etc. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Aqui a <code>categorical_crossentropy</code> √© uma fun√ß√£o das perdas e a medida da avalia√ß√£o √© a precis√£o. <br><br><h2>  Minha experi√™ncia </h2><br><h3>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">An√°lise explorat√≥ria de dados</a> </h3><br>  No conjunto de dados RAVDESS, cada ator mostra 8 emo√ß√µes, pronunciando e cantando 2 frases, 2 vezes cada.  Como resultado, 4 exemplos de cada emo√ß√£o s√£o obtidos de cada ator, com exce√ß√£o das emo√ß√µes neutras, nojo e surpresa acima mencionados.  Cada √°udio dura aproximadamente 4 segundos; no primeiro e no √∫ltimo segundo, geralmente √© o sil√™ncio. <br><br>  <b>Ofertas t√≠picas</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Observa√ß√£o </h3><br>  Depois de selecionar um conjunto de dados de 1 ator e 1 atriz, e depois ouvir todos os seus registros, percebi que homens e mulheres expressam suas emo√ß√µes de maneiras diferentes.  Por exemplo: <br><br><ul><li>  raiva masculina (irritada) √© apenas mais alta; </li><li>  alegria dos homens (feliz) e frustra√ß√£o (triste) - uma caracter√≠stica dos tons de riso e choro durante o "sil√™ncio"; </li><li>  alegria feminina (feliz), raiva (irritada) e frustra√ß√£o (triste) s√£o mais altas; </li><li>  nojo feminino (nojo) cont√©m o som de v√¥mito. </li></ul><br><h3>  Repeti√ß√£o da experi√™ncia </h3><br>  O autor removeu as classes neutra, repugnante e surpresa para fazer o reconhecimento RAVDESS de 10 classes do conjunto de dados.  Tentando repetir a experi√™ncia do autor, obtive este resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  No entanto, descobri que h√° um vazamento de dados quando o conjunto de dados para valida√ß√£o √© id√™ntico ao conjunto de dados de teste.  Portanto, repeti a separa√ß√£o dos dados, isolando os conjuntos de dados de dois atores e duas atrizes para que eles n√£o fiquem vis√≠veis durante o teste: <br><br><ul><li>  os atores 1 a 20 s√£o usados ‚Äã‚Äãpara conjuntos Train / Valid na propor√ß√£o 8: 2; </li><li>  os atores 21 a 24 s√£o isolados dos testes; </li><li>  Par√¢metros do conjunto de trens: (1248, 216, 1); </li><li>  Par√¢metros de conjunto v√°lidos: (312, 216, 1); </li><li>  Par√¢metros do conjunto de teste: (320, 216, 1) - (isolado). </li></ul><br>  Treinei novamente o modelo e aqui est√° o resultado: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Teste de desempenho </h3><br>  No gr√°fico Bruto v√°lido do trem, fica claro que n√£o h√° converg√™ncia para as 10 classes selecionadas.  Por isso, decidi reduzir a complexidade do modelo e deixar apenas as emo√ß√µes masculinas.  Eu isolei dois atores no conjunto de teste e coloquei o resto no conjunto comboio / v√°lido, na propor√ß√£o de 8: 2.  Isso garante que n√£o haja desequil√≠brio no conjunto de dados.  Depois, treinei os dados masculino e feminino separadamente para realizar o teste. <br><br>  <b>Conjunto de dados masculino</b> <br><br><ul><li>  Conjunto de trem - 640 amostras dos atores 1-10; </li><li>  Conjunto v√°lido - 160 amostras dos atores 1-10; </li><li>  Conjunto de teste - 160 amostras dos atores 11-12. </li></ul><br>  <b>Linha de refer√™ncia: men</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Conjunto de dados feminino</b> <br><br><ul><li>  Conjunto de trem - 608 amostras das atrizes 1-10; </li><li>  Conjunto v√°lido - 152 amostras das atrizes 1-10; </li><li>  Conjunto de teste - 160 amostras das atrizes 11-12. </li></ul><br>  <b>Linha de refer√™ncia: mulheres</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  Como voc√™ pode ver, as matrizes de erro s√£o diferentes. <br><br>  Homens: zangado e feliz s√£o as principais classes previstas no modelo, mas n√£o s√£o iguais. <br><br>  Mulheres: desordem (triste) e alegria (feliz) - basicamente classes previstas no modelo;  raiva e alegria s√£o facilmente confundidas. <br><br>  Lembrando as observa√ß√µes da <b>Intelligence Data Analysis</b> , suspeito que as mulheres Angry e Happy sejam semelhantes ao ponto de confus√£o, porque o modo de express√£o delas √© simplesmente elevar suas vozes. <br><br>  Al√©m disso, estou curioso para simplificar ainda mais o modelo, deixando apenas as classes Positiva, Neutra e Negativa.  Ou apenas positivo e negativo.  Em resumo, agrupei as emo√ß√µes em 2 e 3 classes, respectivamente. <br><br>  <b>2 aulas:</b> <br><br><ul><li>  Positivo: alegria (feliz), calmo (calmo); </li><li>  Negativo: raiva, medo (com medo), frustra√ß√£o (triste). </li></ul><br>  <b>3 classes:</b> <br><br><ul><li>  Positivo: alegria (feliz); </li><li>  Neutro: calmo (calmo), neutro (neutro); </li><li>  Negativo: raiva, medo (com medo), frustra√ß√£o (triste). </li></ul><br>  Antes de iniciar o experimento, configurei a arquitetura do modelo usando dados masculinos, fazendo o reconhecimento em 5 classes. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  Eu adicionei 2 camadas de Conv1D, uma camada de MaxPooling1D e 2 camadas de BarchNormalization;  Eu tamb√©m alterei o valor da desist√™ncia para 0,25.  Por fim, mudei o otimizador para SGD com uma velocidade de aprendizado de 0,0001. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  Para treinar o modelo, apliquei uma redu√ß√£o no "plat√¥ de treinamento" e salvei apenas o melhor modelo com um valor m√≠nimo de <code>val_loss</code> .  E aqui est√£o os resultados para as diferentes classes de destino. <br><br><h2>  Desempenho do novo modelo </h2><br>  <b>Homens, 5 classes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Feminino, 5¬™ S√©rie</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>Masculino, 2¬™ S√©rie</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>Homens, 3 classes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Aumento (aumento) </h2><br>  Quando reforcei a arquitetura do modelo, o otimizador e a velocidade do treinamento, verificou-se que o modelo ainda n√£o converge no modo de treinamento.  Sugeri que esse √© um problema de quantidade de dados, pois temos apenas 800 amostras.  Isso me levou a m√©todos para aumentar o √°udio, no final, eu dobrei os conjuntos de dados.  Vamos dar uma olhada nesses m√©todos. <br><br><h3>  Homens, 5 classes </h3><br>  <b>Incremento din√¢mico</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>Ajuste de inclina√ß√£o</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>Deslocamento</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Adicionando ru√≠do branco</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  √â percept√≠vel que o aumento aumenta muito a precis√£o, em at√© 70 +% no caso geral.  Especialmente no caso da adi√ß√£o de branco, que aumenta a precis√£o para 87,19% - no entanto, a precis√£o do teste e a <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">medida F1</a> caem em mais de 5%.  E ent√£o tive a ideia de combinar v√°rios m√©todos de aumento para obter um melhor resultado. <br><br><h3>  Combinando v√°rios m√©todos </h3><br>  <b>Ru√≠do branco + vi√©s</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Teste de aumento em homens </h2><br><h3>  Masculino, 2¬™ S√©rie </h3><br>  <b>Ru√≠do branco + vi√©s</b> <br><br>  Para todas as amostras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>Ru√≠do branco + vi√©s</b> <br><br>  Somente para amostras positivas, uma vez que o conjunto de 2 classes √© desequilibrado (para amostras negativas). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Passo + ru√≠do branco</b> <br>  Para todas as amostras <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Passo + ru√≠do branco</b> <br><br>  Apenas para amostras positivas <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Conclus√£o </h2><br>  No final, eu pude experimentar apenas um conjunto de dados masculino.  Dividi os dados para evitar desequil√≠brios e, consequentemente, vazamento de dados.  Configurei o modelo para experimentar vozes masculinas, pois queria simplific√°-lo o m√°ximo poss√≠vel para come√ßar.  Tamb√©m conduzi testes usando diferentes m√©todos de aumento;  a adi√ß√£o de ru√≠do branco e vi√©s funcionou bem em dados desequilibrados. <br><br><h2>  Conclus√µes </h2><br><ul><li>  as emo√ß√µes s√£o subjetivas e dif√≠ceis de corrigir; </li><li>  √© necess√°rio determinar com anteced√™ncia quais emo√ß√µes s√£o adequadas para o projeto; </li><li>  Nem sempre confie no conte√∫do do Github, mesmo que ele tenha muitas estrelas; </li><li>  compartilhamento de dados - lembre-se disso; </li><li>  a an√°lise explorat√≥ria de dados sempre d√° uma boa ideia, mas voc√™ precisa ser paciente quando se trata de trabalhar com dados de √°udio; </li><li>  Determine o que voc√™ dar√° √† entrada do seu modelo: uma frase, um registro inteiro ou uma exclama√ß√£o? </li><li>  a falta de dados √© um importante fator de sucesso no SER, no entanto, criar um bom conjunto de dados com emo√ß√µes √© uma tarefa complexa e cara; </li><li>  simplifique seu modelo em caso de falta de dados. </li></ul><br><h2>  Melhoria adicional </h2><br><ul><li>  Usei apenas os primeiros 3 segundos como entrada para reduzir o tamanho total dos dados - o projeto original usou 2,5 segundos.  Eu gostaria de experimentar grava√ß√µes em tamanho real; </li><li>  voc√™ pode pr√©-processar os dados: corte o sil√™ncio, normalize o comprimento preenchendo com zeros, etc; </li><li>  tente redes neurais recorrentes para esta tarefa. </li></ul></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt461435/">https://habr.com/ru/post/pt461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt461421/index.html">Como se proteger contra poss√≠veis perdas ao investir na bolsa: produtos estruturais</a></li>
<li><a href="../pt461423/index.html">11 dicas: como apresentar o trabalho da interface do usu√°rio / UX para "n√£o designers"</a></li>
<li><a href="../pt461425/index.html">Como se tornar gerente de produtos e crescer ainda mais</a></li>
<li><a href="../pt461431/index.html">‚ÄúAma e n√£o gosta‚Äù: DNS sobre HTTPS</a></li>
<li><a href="../pt461433/index.html">Usando o Identity Server 4 no Net Core 3.0</a></li>
<li><a href="../pt461437/index.html">370 l√¢mpadas</a></li>
<li><a href="../pt461439/index.html">Iniciando a biblioteca de componentes React e TypeScript</a></li>
<li><a href="../pt461441/index.html">Relat√≥rios sobre o estado do armazenamento usando R. Computa√ß√£o paralela, gr√°ficos, xlsx, email e tudo isso</a></li>
<li><a href="../pt461443/index.html">P√≥s-an√°lise: o que se sabe sobre o √∫ltimo ataque √† rede de servidores de chaves criptogr√°ficas SKS Keyserver</a></li>
<li><a href="../pt461447/index.html">O √©pico sobre administradores de sistemas como esp√©cies amea√ßadas de extin√ß√£o</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>