<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üç∑ üòÆ üõí Hunderassen-ID: Vollst√§ndige Zyklusentwicklung vom Keras-Programm zur Android-App. auf dem Spielmarkt ‚ÜóÔ∏è üëØ üë•</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Angesichts der j√ºngsten Fortschritte bei neuronalen Netzen im Allgemeinen und der Bilderkennung im Besonderen scheint es, dass die Erstellung einer NN...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Hunderassen-ID: Vollst√§ndige Zyklusentwicklung vom Keras-Programm zur Android-App. auf dem Spielmarkt</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/447732/">  Angesichts der j√ºngsten Fortschritte bei neuronalen Netzen im Allgemeinen und der Bilderkennung im Besonderen scheint es, dass die Erstellung einer NN-basierten Anwendung zur Bilderkennung eine einfache Routineoperation ist.  Nun, bis zu einem gewissen Grad ist es wahr: Wenn Sie sich eine Anwendung der Bilderkennung vorstellen k√∂nnen, dann hat h√∂chstwahrscheinlich schon jemand etwas √Ñhnliches getan.  Alles, was Sie tun m√ºssen, ist es zu googeln und zu wiederholen. <br><br>  Es gibt jedoch noch unz√§hlige kleine Details, die ... nicht unl√∂sbar sind, nein.  Sie nehmen einfach zu viel Zeit in Anspruch, besonders wenn Sie ein Anf√§nger sind.  Was helfen w√ºrde, ist ein Schritt-f√ºr-Schritt-Projekt, das direkt vor Ihnen durchgef√ºhrt wird und von Anfang bis Ende durchgef√ºhrt wird.  Ein Projekt, das keine Anweisungen "Dieser Teil ist offensichtlich, also √ºberspringen wir es" enth√§lt.  Na ja, fast :) <br><br>  In diesem Tutorial werden wir einen Hunderassen-Identifikator durchgehen: Wir werden ein neuronales Netzwerk erstellen und unterrichten, es dann auf Java f√ºr Android portieren und auf Google Play ver√∂ffentlichen. <br><br>  F√ºr diejenigen unter Ihnen, die ein Endergebnis sehen m√∂chten, ist hier der Link zur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog-App</a> bei Google Play. <br><br>  Website mit meiner Robotik: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">robotics.snowcron.com</a> . <br>  Website mit: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">NeuroDog User Guide</a> . <br><br>  Hier ist ein Screenshot des Programms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/186/b91/457/186b914572170b01446ed1d722bce200.png" alt="Bild"><br><br><a name="habracut"></a><br><br><h3>  Ein √úberblick </h3><br><br>  Wir werden Keras: Googles Bibliothek f√ºr die Arbeit mit neuronalen Netzen verwenden.  Es ist auf hohem Niveau, was bedeutet, dass die Lernkurve steil sein wird, definitiv schneller als bei anderen mir bekannten Bibliotheken.  Machen Sie sich damit vertraut: Es sind viele hochwertige Tutorials online. <br><br>  Wir werden CNNs - Convolutional Neural Networks verwenden.  CNNs (und auf ihnen basierende fortgeschrittenere Netzwerke) sind de facto Standard bei der Bilderkennung.  Das richtige Unterrichten kann jedoch zu einer gewaltigen Aufgabe werden: Die Struktur des Netzwerks, die Lernparameter (all diese Lernraten, Impulse, L1 und L2 usw.) sollten sorgf√§ltig angepasst werden, und da die Aufgabe viele Rechenressourcen erfordert, m√ºssen wir kann nicht einfach alle m√∂glichen Kombinationen ausprobieren. <br><br>  Dies ist einer der wenigen Gr√ºnde, warum wir in den meisten F√§llen die Verwendung des "Transferwissens" gegen√ºber dem sogenannten "Vanille" -Ansatz bevorzugen.  Transfer Knowlege verwendet ein neuronales Netzwerk, das von einer anderen Person (z. B. Google) f√ºr eine andere Aufgabe trainiert wurde.  Dann entfernen wir die letzten Schichten davon, f√ºgen eigene Schichten hinzu ... und es wirkt Wunder. <br><br>  Es mag seltsam klingen: Wir haben das Google-Netzwerk genutzt, um Katzen, Blumen und M√∂bel zu erkennen, und jetzt identifiziert es Hunderassen!  Um zu verstehen, wie es funktioniert, werfen wir einen Blick auf die Funktionsweise von Deep Neural Networks, einschlie√ülich solcher, die zur Bilderkennung verwendet werden. <br><br>  Wir geben ihm ein Bild als Eingabe.  Die erste Schicht eines Netzwerks analysiert das Bild auf einfache Muster wie "kurze horizontale Linie", "Bogen" usw.  Die n√§chste Ebene nimmt diese Muster (und wo sie sich auf dem Bild befinden) und erzeugt Muster h√∂herer Ebene, wie "Fell", "Augenwinkel" usw.  Am Ende haben wir ein Puzzle, das zu einer Beschreibung eines Hundes kombiniert werden kann: Fell, zwei Augen, menschliches Bein im Mund und so weiter. <br><br>  Dies alles wurde von einer Reihe vorgefertigter Ebenen erledigt (von Google oder einem anderen gro√üen Player).  Schlie√ülich f√ºgen wir unsere eigenen Schichten hinzu und bringen ihm bei, mit diesen Mustern zu arbeiten, um Hunderassen zu erkennen.  Klingt logisch. <br><br>  Zusammenfassend werden wir in diesem Tutorial "Vanilla" CNN und einige "Transfer Learning" -Netzwerke verschiedener Typen erstellen.  Was "Vanille" betrifft: Ich werde es nur als Beispiel daf√ºr verwenden, wie es gemacht werden kann, aber ich werde es nicht verfeinern, da "vorab trainierte" Netzwerke viel einfacher zu benutzen sind.  Keras wird mit wenigen vorgefertigten Netzwerken geliefert. Ich werde einige Konfigurationen ausw√§hlen und vergleichen. <br><br>  Damit unser neuronales Netzwerk Hunderassen erkennen kann, m√ºssen wir ihm Beispielbilder verschiedener Rassen "zeigen".  Gl√ºcklicherweise wurde f√ºr eine √§hnliche Aufgabe ein <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">gro√üer Datensatz</a> erstellt ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Original hier</a> ).  In diesem Artikel werde ich die <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Version von Kaggle verwenden</a> <br><br>  Dann werde ich den "Gewinner" auf Android portieren.  Das Portieren von Keras NN auf Android ist relativ einfach, und wir werden alle erforderlichen Schritte ausf√ºhren. <br><br>  Dann werden wir es auf Google Play ver√∂ffentlichen.  Wie zu erwarten ist, wird Google nicht kooperieren, sodass nur wenige zus√§tzliche Tricks erforderlich sind.  Zum Beispiel √ºberschreitet unser neuronales Netzwerk die zul√§ssige Gr√∂√üe von Android APK: Wir m√ºssen das Bundle verwenden.  Au√üerdem wird Google unsere App nicht in den Suchergebnissen anzeigen, es sei denn, wir tun bestimmte magische Dinge. <br><br>  Am Ende werden wir eine voll funktionsf√§hige "kommerzielle" (in Anf√ºhrungszeichen, da sie kostenlos, aber marktreif ist) Android NN-f√§hige Anwendung haben. <br><br><h3>  Entwicklungsumgebung </h3><br><br>  Je nach verwendetem Betriebssystem (Ubuntu wird empfohlen), Grafikkarte (oder nicht) usw. gibt es nur wenige unterschiedliche Ans√§tze f√ºr die Keras-Programmierung.  Es ist nichts Falsches daran, die Entwicklungsumgebung auf Ihrem lokalen Computer zu konfigurieren und alle erforderlichen Bibliotheken usw. zu installieren.  Au√üer ... es gibt einen einfacheren Weg. <br><br>  Erstens dauert die Installation und Konfiguration mehrerer Entwicklungstools einige Zeit, und Sie m√ºssen erneut Zeit aufwenden, wenn neue Versionen verf√ºgbar werden.  Zweitens erfordert das Training neuronaler Netze viel Rechenleistung.  Sie k√∂nnen Ihren Computer mithilfe der GPU beschleunigen. Zum Zeitpunkt dieses Schreibens kostet eine Top-GPU f√ºr NN-bezogene Berechnungen 2000 bis 7000 US-Dollar.  Und die Konfiguration braucht auch Zeit. <br><br>  Wir werden also einen anderen Ansatz verwenden.  Google erm√∂glicht es Nutzern, seine GPUs kostenlos f√ºr NN-bezogene Berechnungen zu verwenden. Au√üerdem wurde eine vollst√§ndig konfigurierte Umgebung erstellt.  Alles in allem hei√üt es Google Colab.  Der Dienst gew√§hrt Ihnen Zugriff auf ein Jupiter-Notizbuch, in dem Python, Keras und unz√§hlige zus√§tzliche Bibliotheken bereits installiert sind.  Sie m√ºssen lediglich ein Google-Konto einrichten (ein Google Mail-Konto einrichten und auf alles andere zugreifen). <br><br>  Zum Zeitpunkt dieses Schreibens kann auf Colab <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">√ºber diesen Link</a> zugegriffen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden</a> , es kann sich jedoch √§ndern.  Google einfach "Google Colab". <br><br>  Ein offensichtliches Problem bei Colab ist, dass es sich um einen WEB-Dienst handelt.  Wie werden Sie von dort aus auf IHRE Dateien zugreifen?  Speichern neuronaler Netze nach Abschluss des Trainings, Laden von Daten, die f√ºr Ihre Aufgabe spezifisch sind, und so weiter? <br><br>  Es gibt nur wenige (im Moment dieses Schreibens - drei) verschiedene Ans√§tze;  Wir werden das verwenden, was ich f√ºr das Beste halte: Google Drive. <br><br>  Google Drive ist ein Cloud-Speicher, der fast wie eine Festplatte funktioniert und Google Colab zugeordnet werden kann (siehe Code unten).  Dann arbeiten Sie damit wie mit einer lokalen Festplatte.  Wenn Sie beispielsweise auf Fotos von Hunden aus dem in Colab erstellten neuronalen Netzwerk zugreifen m√∂chten, m√ºssen Sie diese Fotos auf Ihr Google Drive hochladen. Das ist alles. <br><br><h2>  Erstellen und Trainieren des NN </h2><br><br>  Im Folgenden werde ich den Python-Code durchgehen, einen Codeblock aus Jupiter Notebook nach dem anderen.  Sie k√∂nnen diesen Code in Ihr Notizbuch kopieren und ausf√ºhren, da Bl√∂cke unabh√§ngig voneinander ausgef√ºhrt werden k√∂nnen. <br><br><h3>  Initialisierung </h3><br><br>  Lassen Sie uns zun√§chst das Google Drive einbinden.  Nur zwei Codezeilen.  Dieser Code muss nur einmal pro Colab-Sitzung ausgef√ºhrt werden (z. B. einmal pro sechs Arbeitsstunden).  Wenn Sie es das zweite Mal ausf√ºhren, wird es √ºbersprungen, da das Laufwerk bereits bereitgestellt ist. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> drive drive.mount(<span class="hljs-string"><span class="hljs-string">'/content/drive/'</span></span>)</code> </pre> <br><br>  Beim ersten Mal werden Sie aufgefordert, die Montage zu best√§tigen - hier ist nichts kompliziert.  Es sieht so aus: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Go to this URL <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a browser: ... &gt;&gt;&gt; Enter your authorization code: &gt;&gt;&gt; ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ &gt;&gt;&gt; Mounted at /content/drive/</code> </pre><br><br>  Ein h√ºbscher Standard- <i>Include-</i> Abschnitt;  h√∂chstwahrscheinlich sind einige der Includes nicht erforderlich.  Wenn ich verschiedene NN-Konfigurationen testen m√∂chte, m√ºssen Sie einige davon f√ºr einen bestimmten NN-Typ kommentieren / auskommentieren: Zum Beispiel InceptionV3 Typ NN, InceptionV3 auskommentieren und beispielsweise ResNet50 kommentieren.  Oder auch nicht: Sie k√∂nnen diese Includes unkommentiert lassen, es wird mehr Speicher ben√∂tigt, aber das ist alles. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> dt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tqdm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> regularizers <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Sequential <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dense, Dropout, Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flatten, Conv2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MaxPooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, GlobalAveragePooling2D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Callback, EarlyStopping <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ReduceLROnPlateau <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ModelCheckpoint <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> shutil <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.vgg16 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> InceptionV3 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.inception_v3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> inception_v3_preprocessor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.mobilenetv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MobileNetV2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.nasnet <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NASNetMobile</code> </pre><br><br>  Auf Google Drive erstellen wir einen Ordner f√ºr unsere Dateien.  Die zweite Zeile zeigt den Inhalt an: <br><br><pre> <code class="python hljs">working_path = <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data/"</span></span> !ls <span class="hljs-string"><span class="hljs-string">"/content/drive/My Drive/DeepDogBreed/data"</span></span> &gt;&gt;&gt; all_images labels.csv models test train valid</code> </pre><br><br>  Wie Sie sehen k√∂nnen, werden Fotos von Hunden (solche, die aus dem Stanford-Datensatz (siehe oben) in Google Drive kopiert wurden, zun√§chst im Ordner <i>all_images</i> gespeichert. Sp√§ter werden sie in Ordner zum <i>Trainieren,</i> <i>Validieren</i> und <i>Testen</i> kopiert. Wir werden speichern trainierte Modelle im Modellordner. Die Datei labels.csv ist Teil eines Datensatzes und ordnet Bilddateien Hunderassen zu. <br><br>  Es gibt viele Tests, die Sie ausf√ºhren k√∂nnen, um herauszufinden, was Sie haben. Lassen Sie uns nur einen ausf√ºhren: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Is GPU Working? import tensorflow as tf tf.test.gpu_device_name() &gt;&gt;&gt; '/device:GPU:0'</span></span></code> </pre><br><br>  Ok, GPU ist angeschlossen.  Wenn nicht, suchen Sie es in den Jupiter Notebook-Einstellungen und schalten Sie es ein. <br><br>  Jetzt m√ºssen wir einige Konstanten deklarieren, die wir verwenden werden, wie die Gr√∂√üe eines Bildes, das das Neuronale Netz erwarten sollte, und so weiter.  Beachten Sie, dass wir ein 256x256-Bild verwenden, da dieses auf der einen Seite gro√ü genug ist und auf der anderen Seite in den Speicher passt.  Einige Arten von neuronalen Netzen, die wir verwenden werden, erwarten jedoch ein 224x224-Bild.  Kommentieren Sie dazu bei Bedarf die alte Bildgr√∂√üe und kommentieren Sie eine neue aus. <br><br>  Der gleiche Ansatz (Kommentar eins - Kommentar aus dem anderen) gilt f√ºr Namen von Modellen, die wir speichern, einfach weil wir das Ergebnis eines vorherigen Tests nicht √ºberschreiben m√∂chten, wenn wir eine neue Konfiguration versuchen. <br><pre> <code class="python hljs">warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">"ignore"</span></span>) os.environ[<span class="hljs-string"><span class="hljs-string">'TF_CPP_MIN_LOG_LEVEL'</span></span>] = <span class="hljs-string"><span class="hljs-string">'2'</span></span> np.random.seed(<span class="hljs-number"><span class="hljs-number">7</span></span>) start = dt.datetime.now() BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">16</span></span> EPOCHS = <span class="hljs-number"><span class="hljs-number">15</span></span> TESTING_SPLIT=<span class="hljs-number"><span class="hljs-number">0.3</span></span> <span class="hljs-comment"><span class="hljs-comment"># 70/30 % NUM_CLASSES = 120 IMAGE_SIZE = 256 #strModelFileName = "models/ResNet50.h5" # strModelFileName = "models/InceptionV3.h5" strModelFileName = "models/InceptionV3_Sgd.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/MobileNetV2.h5" #IMAGE_SIZE = 224 #strModelFileName = "models/NASNetMobileSgd.h5"</span></span></code> </pre><br><br><h3>  Daten laden </h3><br><br>  Laden <i>wir zun√§chst die</i> Datei <i>label.csv</i> und teilen ihren Inhalt in Schulungs- und Validierungsteile auf.  Beachten Sie, dass es noch keinen Testteil gibt, da ich ein bisschen schummeln werde, um mehr Daten f√ºr das Training zu erhalten. <br><br><pre> <code class="python hljs">labels = pd.read_csv(working_path + <span class="hljs-string"><span class="hljs-string">'labels.csv'</span></span>) print(labels.head()) train_ids, valid_ids = train_test_split(labels, test_size = TESTING_SPLIT) print(len(train_ids), <span class="hljs-string"><span class="hljs-string">'train ids'</span></span>, len(valid_ids), <span class="hljs-string"><span class="hljs-string">'validation ids'</span></span>) print(<span class="hljs-string"><span class="hljs-string">'Total'</span></span>, len(labels), <span class="hljs-string"><span class="hljs-string">'testing images'</span></span>) &gt;&gt;&gt; id breed &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">000</span></span>bec180eb18c7604dcecc8fe0dba07 boston_bull &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">001513</span></span>dfcb2ffafc82cccf4d8bbaba97 dingo &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">001</span></span>cdf01b096e06d78e9e5112d419397 pekinese &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">00214</span></span>f311d5d2247d5dfe4fe24b2303d bluetick &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">0021</span></span>f9ceb3235effd7fcde7f7538ed62 golden_retriever &gt;&gt;&gt; <span class="hljs-number"><span class="hljs-number">7155</span></span> train ids <span class="hljs-number"><span class="hljs-number">3067</span></span> validation ids &gt;&gt;&gt; Total <span class="hljs-number"><span class="hljs-number">10222</span></span> testing images</code> </pre><br><br>  Als n√§chstes m√ºssen wir die tats√§chlichen Bilddateien in die Ordner f√ºr Training / Validierung / Testen kopieren, je nachdem, welche Dateinamen wir √ºbergeben.  Die folgende Funktion kopiert Dateien mit Namen in einen angegebenen Ordner. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">copyFileSet</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(strDirFrom, strDirTo, arrFileNames)</span></span></span><span class="hljs-function">:</span></span> arrBreeds = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) arrFileNames = np.asarray(arrFileNames[<span class="hljs-string"><span class="hljs-string">'id'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo): os.makedirs(strDirTo) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tqdm(range(len(arrFileNames))): strFileNameFrom = strDirFrom + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> strFileNameTo = strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span> + arrFileNames[i] + <span class="hljs-string"><span class="hljs-string">".jpg"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> os.path.exists(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>): os.makedirs(strDirTo + arrBreeds[i] + <span class="hljs-string"><span class="hljs-string">"/"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># As a new breed dir is created, copy 1st file # to "test" under name of that breed if not os.path.exists(working_path + "test/"): os.makedirs(working_path + "test/") strFileNameTo = working_path + "test/" + arrBreeds[i] + ".jpg" shutil.copy(strFileNameFrom, strFileNameTo) shutil.copy(strFileNameFrom, strFileNameTo)</span></span></code> </pre><br><br>  Wie Sie sehen, kopieren wir nur eine Datei f√ºr jede Hunderasse in einen Testordner.  Beim Kopieren von Dateien erstellen wir auch Unterordner - einen Unterordner pro Hunderasse.  Bilder f√ºr jede bestimmte Rasse werden in ihren Unterordner kopiert. <br><br>  Der Grund daf√ºr ist, dass Keras mit einer so organisierten Verzeichnisstruktur arbeiten kann, Bilddateien nach Bedarf l√§dt und Speicherplatz spart.  Es w√§re eine sehr schlechte Idee, alle 15.000 Bilder gleichzeitig in den Speicher zu laden. <br><br>  Das Aufrufen dieser Funktion jedes Mal, wenn wir unseren Code ausf√ºhren, w√§re ein Overkill: Bilder werden bereits kopiert. Warum sollten wir sie erneut kopieren?  Kommentieren Sie es also bei der ersten Verwendung aus: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Move the data in subfolders so we can # use the Keras ImageDataGenerator. # This way we can also later use Keras # Data augmentation features. # --- Uncomment once, to copy files --- #copyFileSet(working_path + "all_images/", # working_path + "train/", train_ids) #copyFileSet(working_path + "all_images/", # working_path + "valid/", valid_ids)</span></span></code> </pre><br><br>  Zus√§tzlich ben√∂tigen wir eine Liste von Hunderassen: <br><br><pre> <code class="python hljs">breeds = np.unique(labels[<span class="hljs-string"><span class="hljs-string">'breed'</span></span>]) map_characters = {} <span class="hljs-comment"><span class="hljs-comment">#{0:'none'} for i in range(len(breeds)): map_characters[i] = breeds[i] print("&lt;item&gt;" + breeds[i] + "&lt;/item&gt;") &gt;&gt;&gt; &lt;item&gt;affenpinscher&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;afghan_hound&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;african_hunting_dog&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;airedale&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;american_staffordshire_terrier&lt;/item&gt; &gt;&gt;&gt; &lt;item&gt;appenzeller&lt;/item&gt;</span></span></code> </pre><br><br><h3>  Bilder verarbeiten </h3><br><br>  Wir werden die Keras-Funktion ImageDataGenerators verwenden.  ImageDataGenerator kann ein Bild verarbeiten, seine Gr√∂√üe √§ndern, es drehen usw.  Es kann auch eine <i>Verarbeitungsfunktion √ºbernehmen</i> , die benutzerdefinierte Bildmanipulationen ausf√ºhrt. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) <span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255... img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. #img = cv2.blur(img,(5,5)) return img_1[0]</span></span></code> </pre><br><br>  Beachten Sie die folgende Zeile: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># or use ImageDataGenerator( rescale=1./255...</span></span></code> </pre><br><br>  Wir k√∂nnen die Normalisierung (Anpassen des 0-255-Bereichs des Bildkanals an 0-1) in ImageDataGenerator selbst durchf√ºhren.  Warum brauchen wir also einen Pr√§prozessor?  Als Beispiel habe ich die (auskommentierte) <i>Unsch√§rfefunktion</i> bereitgestellt: Dies ist eine benutzerdefinierte Bildmanipulation.  Hier k√∂nnen Sie alles von Sch√§rfen bis HDR verwenden. <br><br>  Wir werden zwei verschiedene ImageDataGenerators verwenden, einen f√ºr das Training und einen f√ºr die Validierung.  Der Unterschied besteht darin, dass wir f√ºr das Training Rotationen und Zoomen ben√∂tigen, um Bilder "vielf√§ltiger" zu machen, aber wir brauchen sie nicht f√ºr die Validierung (nicht in dieser Aufgabe). <br><br><pre> <code class="python hljs">train_datagen = ImageDataGenerator( preprocessing_function=preprocess, <span class="hljs-comment"><span class="hljs-comment">#rescale=1./255, # done in preprocess() # randomly rotate images (degrees, 0 to 30) rotation_range=30, # randomly shift images horizontally # (fraction of total width) width_shift_range=0.3, height_shift_range=0.3, # randomly flip images horizontal_flip=True, ,vertical_flip=False, zoom_range=0.3) val_datagen = ImageDataGenerator( preprocessing_function=preprocess) train_gen = train_datagen.flow_from_directory( working_path + "train/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical") val_gen = val_datagen.flow_from_directory( working_path + "valid/", batch_size=BATCH_SIZE, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, class_mode="categorical")</span></span></code> </pre><br><br><h3>  Neuronales Netzwerk erstellen </h3><br><br>  Wie oben erw√§hnt, werden wir einige Arten von neuronalen Netzen erstellen.  Jedes Mal, wenn wir eine andere Funktion verwenden, unterscheiden sich die Include-Bibliotheken und in einigen F√§llen die Bildgr√∂√üen.  Um von einem neuronalen Netzwerktyp zum anderen zu wechseln, m√ºssen Sie den entsprechenden Code kommentieren / auskommentieren. <br><br>  Lassen Sie uns zuerst "Vanille" CNN erstellen.  Es funktioniert schlecht, da ich es nicht optimiert habe, aber es bietet zumindest ein Framework, mit dem Sie ein eigenes Netzwerk erstellen k√∂nnen (im Allgemeinen ist es eine schlechte Idee, da vorab trainierte Netzwerke verf√ºgbar sind). <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelVanilla</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() <span class="hljs-comment"><span class="hljs-comment"># Note the (7, 7) here. This is one of technics # used to reduce memory use by the NN: we scan # the image in a larger steps. # Also note regularizers.l2: this technic is # used to prevent overfitting. The "0.001" here # is an empirical value and can be optimized. model.add(Conv2D(16, (7, 7), padding='same', use_bias=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), kernel_regularizer=regularizers.l2(0.001))) # Note the use of a standard CNN building blocks: # Conv2D - BatchNormalization - Activation # MaxPooling2D - Dropout # The last two are used to avoid overfitting, also, # MaxPooling2D reduces memory use. model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(16, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(128, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(Dropout(0.5)) model.add(Conv2D(256, (3, 3), padding='same', use_bias=False, kernel_regularizer=regularizers.l2(0.01))) model.add(BatchNormalization(axis=3, scale=False)) model.add(Activation("relu")) model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same')) model.add(Dropout(0.5)) # This is the end on "convolutional" part of CNN. # Now we need to transform multidementional # data into one-dim. array for a fully-connected # classifier: model.add(Flatten()) # And two layers of classifier itself (plus an # Activation layer in between): model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) model.add(Activation("relu")) model.add(Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.01))) # We need to compile the resulting network. # Note that there are few parameters we can # try here: the best performing one is uncommented, # the rest is commented out for your reference. #model.compile(optimizer='rmsprop', # loss='categorical_crossentropy', # metrics=['accuracy']) #model.compile( # optimizer=keras.optimizers.RMSprop(lr=0.0005), # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #model.compile(optimizer='adadelta', # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.Adadelta(lr=1.0, # rho=0.95, epsilon=0.01, decay=0.01) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) #opt = keras.optimizers.RMSprop(lr=0.0005, # rho=0.9, epsilon=None, decay=0.0001) #model.compile(optimizer=opt, # loss='categorical_crossentropy', # metrics=['accuracy']) # model.summary() return(model)</span></span></code> </pre><br><br>  Wenn wir mithilfe von <i>Transferlernen ein</i> neuronales Netzwerk erstellen, √§ndert sich das Verfahren: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelMobileNetV2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># First, create the NN and load pre-trained # weights for it ('imagenet') # Note that we are not loading last layers of # the network (include_top=False), as we are # going to add layers of our own: base_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) # Then attach our layers at the end. These are # to build "classifier" that makes sense of # the patterns previous layers provide: x = base_model.output x = Dense(512)(x) x = Activation('relu')(x) x = Dropout(0.5)(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) # Create a model model = Model(inputs=base_model.input, outputs=predictions) # We need to make sure that pre-trained # layers are not changed when we train # our classifier: # Either this: #model.layers[0].trainable = False # or that: for layer in base_model.layers: layer.trainable = False # As always, there are different possible # settings, I tried few and chose the best: # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Das Erstellen anderer Arten von vorab trainierten NNs ist sehr √§hnlich: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelResNet50</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> base_model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>, input_shape=(IMAGE_SIZE, IMAGE_SIZE, <span class="hljs-number"><span class="hljs-number">3</span></span>)) x = base_model.output x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>)(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) predictions = Dense(NUM_CLASSES, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=base_model.input, outputs=predictions) <span class="hljs-comment"><span class="hljs-comment">#model.layers[0].trainable = False # model.compile(loss='categorical_crossentropy', # optimizer='adam', metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Attn: der Gewinner!  Dieser NN zeigte die besten Ergebnisse: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelInceptionV3</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Noch eins: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">createModelNASNetMobile</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># model.layers[0].trainable = False # model.compile(optimizer='sgd', # loss='categorical_crossentropy', # metrics=['accuracy']) base_model = NASNetMobile(weights = 'imagenet', include_top = False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(512, activation='relu')(x) predictions = Dense(NUM_CLASSES, activation='softmax')(x) model = Model(inputs = base_model.input, outputs = predictions) for layer in base_model.layers: layer.trainable = False # model.compile(optimizer='adam', # loss='categorical_crossentropy', # metrics=['accuracy']) model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) #model.summary() return(model)</span></span></code> </pre><br><br>  Verschiedene Arten von NNs werden in verschiedenen Situationen verwendet.  Zus√§tzlich zu Pr√§zisionsproblemen, Gr√∂√üenangelegenheiten (mobiles NN ist f√ºnfmal kleiner als Inception 1) und Geschwindigkeit (wenn wir eine Echtzeitanalyse eines Videostreams ben√∂tigen, m√ºssen wir m√∂glicherweise die Pr√§zision opfern). <br><br><h3>  Training des neuronalen Netzwerks </h3><br><br>  Zun√§chst <i>experimentieren</i> wir, daher m√ºssen wir in der Lage sein, zuvor gespeicherte NNs zu l√∂schen, die wir jedoch nicht mehr ben√∂tigen.  Die folgende Funktion l√∂scht NN, wenn die Datei vorhanden ist: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make sure that previous "best network" is deleted. def deleteSavedNet(best_weights_filepath): if(os.path.isfile(best_weights_filepath)): os.remove(best_weights_filepath) print("deleteSavedNet():File removed") else: print("deleteSavedNet():No file to remove")</span></span></code> </pre><br><br>  Die Art und Weise, wie wir NNs erstellen und l√∂schen, ist unkompliziert.  Zuerst l√∂schen wir.  Wenn Sie jetzt nicht mit dem Zauberstab <i>l√∂schen m√∂chten</i> , denken Sie daran, dass das Jupiter-Notizbuch √ºber eine Funktion zum Ausf√ºhren der Auswahl verf√ºgt. W√§hlen Sie nur das aus, was Sie ben√∂tigen, und f√ºhren Sie es aus. <br><br>  Dann erstellen wir die NN, wenn ihre Datei nicht vorhanden ist, oder <i>laden</i> sie, wenn die Datei vorhanden ist: Nat√ºrlich k√∂nnen wir nicht "delete" aufrufen und dann erwarten, dass die NN vorhanden ist. Um das zuvor gespeicherte Netzwerk zu verwenden, rufen Sie nicht <i>delete auf</i> . <br><br>  Mit anderen Worten, wir k√∂nnen eine neue NN erstellen oder eine vorhandene verwenden, je nachdem, was wir gerade experimentieren.  Ein einfaches Szenario: Wir haben den NN trainiert und sind dann in den Urlaub gefahren.  Google hat uns abgemeldet, daher m√ºssen wir den NN neu laden: Kommentieren Sie den Teil "L√∂schen" aus und kommentieren Sie den Teil "Laden" aus. <br><br><pre> <code class="python hljs">deleteSavedNet(working_path + strModelFileName) <span class="hljs-comment"><span class="hljs-comment">#if not os.path.exists(working_path + "models"): # os.makedirs(working_path + "models") # #if not os.path.exists(working_path + # strModelFileName): # model = createModelResNet50() model = createModelInceptionV3() # model = createModelMobileNetV2() # model = createModelNASNetMobile() #else: # model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  <b>Checkpoints</b> sind beim Unterrichten der NNs sehr wichtig.  Sie k√∂nnen eine Reihe von Funktionen erstellen, die am Ende jeder Trainingsepoche aufgerufen werden sollen. Sie k√∂nnen beispielsweise die NN speichern, wenn bessere Ergebnisse als die zuletzt gespeicherte angezeigt werden. <br><br><pre> <code class="python hljs">checkpoint = ModelCheckpoint(working_path + strModelFileName, monitor=<span class="hljs-string"><span class="hljs-string">'val_acc'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'auto'</span></span>, save_weights_only=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) callbacks_list = [ checkpoint ]</code> </pre><br><br>  Schlie√ülich werden wir unsere NN anhand des Trainingssatzes unterrichten: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Calculate sizes of training and validation sets STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size STEP_SIZE_VALID=val_gen.n//val_gen.batch_size # Set to False if we are experimenting with # some other part of code, use history that # was calculated before (and is still in # memory bDoTraining = True if bDoTraining == True: # model.fit_generator does the actual training # Note the use of generators and callbacks # that were defined earlier history = model.fit_generator(generator=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=val_gen, validation_steps=STEP_SIZE_VALID, epochs=EPOCHS, callbacks=callbacks_list) # --- After fitting, load the best model # This is important as otherwise we'll # have the LAST model loaded, not necessarily # the best one. model.load_weights(working_path + strModelFileName) # --- Presentation part # summarize history for accuracy plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['acc', 'val_acc'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['loss', 'val_loss'], loc='upper left') plt.show() # As grid optimization of NN would take too long, # I did just few tests with different parameters. # Below I keep results, commented out, in the same # code. As you can see, Inception shows the best # results: # Inception: # adam: val_acc 0.79393 # sgd: val_acc 0.80892 # Mobile: # adam: val_acc 0.65290 # sgd: Epoch 00015: val_acc improved from 0.67584 to 0.68469 # sgd-30 epochs: 0.68 # NASNetMobile, adam: val_acc did not improve from 0.78335 # NASNetMobile, sgd: 0.8</span></span></code> </pre><br><br>  Hier sind Genauigkeits- und Verlustdiagramme f√ºr den Gewinner NN: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0e/97d/9cc/f0e97d9ccdc8f8ed9e44ddba02cf1f8d.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/612/e09/8b0/612e098b088979768d1cc66c2f6972bc.png"><br><br>  Wie Sie sehen k√∂nnen, lernt das Netzwerk gut. <br><br><h3>  Testen des neuronalen Netzes </h3><br><br>  Nach Abschluss der Schulungsphase m√ºssen wir Tests durchf√ºhren.  Dazu werden NN Bilder pr√§sentiert, die sie nie gesehen haben.  Wenn Sie sich erinnern, haben wir f√ºr jede Hundeart ein Bild beiseite gelegt. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># --- Test j = 0 # Final cycle performs testing on the entire # testing set. for file_name in os.listdir( working_path + "test/"): img = image.load_img(working_path + "test/" + file_name); img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict_on_batch(img_1) # get 5 best predictions y_pred_ids = y_pred[0].argsort()[-5:][::-1] print(file_name) for i in range(len(y_pred_ids)): print("\n\t" + map_characters[y_pred_ids[i]] + " (" + str(y_pred[0][y_pred_ids[i]]) + ")") print("--------------------\n") j = j + 1</span></span></code> </pre><br><br><h3>  NN nach Java exportieren </h3><br><br>  Zuerst m√ºssen wir den NN laden.  Der Grund daf√ºr ist, dass der Export ein separater Codeblock ist, sodass wir ihn wahrscheinlich separat ausf√ºhren, ohne den NN neu zu trainieren.  Wenn Sie meinen Code verwenden, ist es Ihnen eigentlich egal, aber wenn Sie Ihre eigene Entwicklung durchf√ºhren, w√ºrden Sie versuchen, zu vermeiden, dass <i>dasselbe</i> Netzwerk einmal nach dem anderen umgeschult wird. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Test: load and run model = load_model(working_path + strModelFileName)</span></span></code> </pre><br><br>  Aus dem gleichen Grund - dies ist irgendwie ein separater Codeblock - verwenden wir hier zus√§tzliche Includes.  Nichts hindert uns nat√ºrlich daran, sie nach oben zu bringen: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf</code> </pre><br><br>  Ein kleiner Test, nur um sicherzugehen, dass wir alles richtig geladen haben: <br><br><pre> <code class="python hljs">img = image.load_img(working_path + <span class="hljs-string"><span class="hljs-string">"test/affenpinscher.jpg"</span></span>) <span class="hljs-comment"><span class="hljs-comment">#basset.jpg") img_1 = image.img_to_array(img) img_1 = cv2.resize(img_1, (IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_AREA) img_1 = np.expand_dims(img_1, axis=0) / 255. y_pred = model.predict(img_1) Y_pred_classes = np.argmax(y_pred,axis = 1) # print(y_pred) fig, ax = plt.subplots() ax.imshow(img) ax.axis('off') ax.set_title(map_characters[Y_pred_classes[0]]) plt.show()</span></span></code> </pre><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/032/846/05c03284674e4337a2e5a3ba617634dd.png" alt="Bild"><br><br>  Als n√§chstes m√ºssen wir die Namen der Eingabe- und Ausgabeschichten unseres Netzwerks abrufen (es sei denn, wir haben beim Erstellen des Netzwerks den Parameter "name" verwendet, was wir nicht getan haben). <br><br><pre> <code class="python hljs">model.summary() &gt;&gt;&gt; Layer (type) &gt;&gt;&gt; ====================== &gt;&gt;&gt; input_7 (InputLayer) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; conv2d_283 (Conv2D) &gt;&gt;&gt; ______________________ &gt;&gt;&gt; ... &gt;&gt;&gt; dense_14 (Dense) &gt;&gt;&gt; ====================== &gt;&gt;&gt; Total params: <span class="hljs-number"><span class="hljs-number">22</span></span>,<span class="hljs-number"><span class="hljs-number">913</span></span>,<span class="hljs-number"><span class="hljs-number">432</span></span> &gt;&gt;&gt; Trainable params: <span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">110</span></span>,<span class="hljs-number"><span class="hljs-number">648</span></span> &gt;&gt;&gt; Non-trainable params: <span class="hljs-number"><span class="hljs-number">21</span></span>,<span class="hljs-number"><span class="hljs-number">802</span></span>,<span class="hljs-number"><span class="hljs-number">784</span></span></code> </pre><br><br>  Wir werden sp√§ter beim Importieren des NN in eine Android Java-Anwendung die Namen der Eingabe- und Ausgabeebene verwenden. <br><br>  Wir k√∂nnen auch den folgenden Code verwenden, um diese Informationen zu erhalten: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">print_graph_nodes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> g = tf.GraphDef() g.ParseFromString(open(filename, <span class="hljs-string"><span class="hljs-string">'rb'</span></span>).read()) print() print(filename) print(<span class="hljs-string"><span class="hljs-string">"=======================INPUT==================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'input'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"=======================OUTPUT=================="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'output'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"===================KERAS_LEARNING=============="</span></span>) print([n <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> g.node <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> n.name.find(<span class="hljs-string"><span class="hljs-string">'keras_learning_phase'</span></span>) != <span class="hljs-number"><span class="hljs-number">-1</span></span>]) print(<span class="hljs-string"><span class="hljs-string">"==============================================="</span></span>) print() <span class="hljs-comment"><span class="hljs-comment">#def get_script_path(): # return os.path.dirname(os.path.realpath(sys.argv[0]))</span></span></code> </pre><br><br>  Der erste Ansatz ist jedoch bevorzugt. <br><br>  Die folgende Funktion exportiert Keras Neural Network in das <i>pb-</i> Format, das wir in Android verwenden werden. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">keras_to_tensorflow</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(keras_model, output_dir, model_name,out_prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"output_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, log_tensorboard=True)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> os.path.exists(output_dir) == <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: os.mkdir(output_dir) out_nodes = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(keras_model.outputs)): out_nodes.append(out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) tf.identity(keras_model.output[i], out_prefix + str(i + <span class="hljs-number"><span class="hljs-number">1</span></span>)) sess = K.get_session() <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> graph_util <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.framework graph_io init_graph = sess.graph.as_graph_def() main_graph = graph_util.convert_variables_to_constants( sess, init_graph, out_nodes) graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> log_tensorboard: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.python.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> import_pb_to_tensorboard import_pb_to_tensorboard.import_to_tensorboard( os.path.join(output_dir, model_name), output_dir)</code> </pre><br><br><p>  Verwenden Sie diese Funktionen, um eine Export-NN zu erstellen: <br><br></p><pre> <code class="python hljs">model = load_model(working_path + strModelFileName) keras_to_tensorflow(model, output_dir=working_path + strModelFileName, model_name=working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>) print_graph_nodes(working_path + <span class="hljs-string"><span class="hljs-string">"models/dogs.pb"</span></span>)</code> </pre><br><br>  Die letzte Zeile gibt die Struktur unseres NN wieder. <br><br><h2>  Erstellen einer NN-f√§higen Android-App </h2><br><br>  NN in Android App exportieren.  ist gut formalisiert und sollte keine Schwierigkeiten bereiten.  Es gibt wie √ºblich mehr als eine M√∂glichkeit, dies zu tun.  Wir werden die beliebtesten verwenden (zumindest im Moment). <br><br>  Verwenden Sie zun√§chst Android Studio, um ein neues Projekt zu erstellen.  Wir werden ein wenig Abstriche machen, damit es nur eine einzige Aktivit√§t gibt. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b3/76e/997/6b376e997b34f45359c46923f6613d60.png" alt="Bild"><br><br>  Wie Sie sehen k√∂nnen, haben wir den Ordner "Assets" hinzugef√ºgt und dort unsere Neural Network-Datei kopiert. <br><br><h3>  Gradle-Datei </h3><br><br>  Es gibt einige √Ñnderungen, die wir vornehmen m√ºssen, um die Datei zu gradeln.  Zun√§chst m√ºssen wir die <i>Tensorflow-Android-</i> Bibliothek importieren.  Es wird verwendet, um Tensorflow (und Keras entsprechend) von Java aus zu verarbeiten: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a16/091/fab/a16091fab2166f834827812611142d26.png" alt="Bild"><br><br>  Beachten Sie als zus√§tzliches "schwer zu findendes" Detail die Versionen: <i>versionCode</i> und <i>versionName</i> .  W√§hrend Sie an Ihrer App arbeiten, m√ºssen Sie neue Versionen auf Google Play hochladen.  Ohne die Aktualisierung der Versionen (etwa 1 -&gt; 2 -&gt; 3 ...) k√∂nnen Sie dies nicht tun. <br><br><h3>  Manifest </h3><br><br>  Zuallererst unsere App.  wird "schwer" sein - ein 100-Mb-Neuronales Netzwerk passt problemlos in den Speicher moderner Telefone, aber jedes Mal, wenn der Benutzer ein Bild von Facebook "teilt", ist es definitiv keine gute Idee, eine separate Instanz davon zu √∂ffnen. <br><br>  Wir werden also sicherstellen, dass es nur eine Instanz unserer App gibt: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">activity</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">".MainActivity"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:launchMode</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"singleTask"</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br>  Durch Hinzuf√ºgen von <i>android: launchMode = "singleTask"</i> zu MainActivity weisen wir Android an, eine vorhandene App zu √∂ffnen, anstatt eine andere Instanz zu starten. <br><br>  Dann stellen wir sicher, dass unsere App.  wird in einer Liste von Anwendungen angezeigt, die <i>gemeinsam genutzte</i> Bilder verarbeiten k√∂nnen: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Send action required to display activity in share list --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">action</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.action.SEND"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Make activity default to launch --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">category</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.intent.category.DEFAULT"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-comment"><span class="hljs-comment">&lt;!-- Mime type ie what can be shared with this activity only image and text --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">data</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:mimeType</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"image/*"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">intent-filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre><br><br>  Schlie√ülich m√ºssen wir Funktionen und Berechtigungen anfordern, damit die App auf die erforderlichen Systemfunktionen zugreifen kann: <br><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-feature</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.hardware.camera"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:required</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"true"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">= </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.WRITE_EXTERNAL_STORAGE"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">uses-permission</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">android:name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"android.permission.READ_PHONE_STATE"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">tools:node</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"remove"</span></span></span><span class="hljs-tag"> /&gt;</span></span></code> </pre><br><br>  Wenn Sie mit der Android-Programmierung vertraut sind, sollte dieser Teil keine Fragen aufwerfen. <br><br><h3>  Layout der App. </h3><br><br>  Wir werden zwei Layouts erstellen, eines f√ºr Hochformat und eines f√ºr Querformat.  Hier ist das <a href="">Portrait-Layout</a> . <br><br>  Was wir hier haben: eine gro√üe Ansicht zum Anzeigen eines Bildes, eine ziemlich nervige Liste von Werbespots (angezeigt, wenn die Schaltfl√§che "Bone" gedr√ºckt wird), Schaltfl√§chen "Hilfe", Schaltfl√§chen zum Laden eines Bildes aus Datei / Galerie und von Kamera und Zum Schluss eine (zun√§chst ausgeblendete) Schaltfl√§che "Verarbeiten". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f71/882/81f/f7188281ff581965c20c7e818cb0fd77.png" alt="Bild"><br><br>  In der Aktivit√§t selbst werden wir eine Logik implementieren, die je nach Status der Anwendung Schaltfl√§chen anzeigt / ausblendet und aktiviert / deaktiviert. <br><br><h3>  Hauptaktivit√§t </h3><br><br>  Die Aktivit√§t erweitert eine Standard-Android-Aktivit√§t: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MainActivity</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Activity</span></span></span></span></code> </pre><br><br>  Werfen wir einen Blick auf den Code, der f√ºr NN-Operationen verantwortlich ist. <br><br>  Zun√§chst akzeptiert NN eine Bitmap.  Urspr√ºnglich ist es eine gro√üe Bitmap aus Datei oder Kamera (m_bitmap), dann transformieren wir sie in eine Standard-Bitmap 256x256 (m_bitmapForNn).  Wir halten auch die Bildabmessungen (256) konstant: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmap = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> Bitmap m_bitmapForNn = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> m_nImageSize = <span class="hljs-number"><span class="hljs-number">256</span></span>;</code> </pre><br><br>  Wir m√ºssen dem NN mitteilen, wie die Namen f√ºr die Eingabe- und Ausgabeebene lauten.  Wenn Sie die obige Liste konsultieren, werden Sie feststellen, dass die Namen lauten (in unserem Fall! Ihr Fall kann anders sein!): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String INPUT_NAME = <span class="hljs-string"><span class="hljs-string">"input_7_1"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String OUTPUT_NAME = <span class="hljs-string"><span class="hljs-string">"output_1"</span></span>;</code> </pre><br><br>  Dann deklarieren wir die Variable als TensofFlow-Objekt.  Au√üerdem speichern wir den Pfad zur NN-Datei in den Assets: <br><br><p></p><pre> private TensorFlowInferenceInterface tf;
 private String MODEL_PATH = 
	 "file: ///android_asset/dogs.pb";
</pre><br><br>  Hunderassen, um dem Benutzer eine aussagekr√§ftige Information anstelle von Indizes im Array zu pr√§sentieren: <br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> String[] m_arrBreedsArray;</code> </pre><br><br>  Zun√§chst laden wir eine Bitmap.  NN selbst erwartet jedoch ein Array von RGB-Werten, und seine Ausgabe ist ein Array von Wahrscheinlichkeiten daf√ºr, dass das dargestellte Bild eine bestimmte Rasse ist.  Wir m√ºssen also zwei weitere Arrays hinzuf√ºgen (beachten Sie, dass 120 die Anzahl der Rassen in unserem Trainingsdatensatz ist): <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrPrediction = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">120</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] m_arrInput = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>;</code> </pre><br><br>  Laden Sie die Tensorflow-Inferenzbibliothek <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> { System.loadLibrary(<span class="hljs-string"><span class="hljs-string">"tensorflow_inference"</span></span>); }</code> </pre><br><br>  Da der Betrieb von NN langwierig ist, m√ºssen wir ihn in einem separaten Thread ausf√ºhren. Andernfalls besteht eine gute Chance, dass die System-App aufgerufen wird.  nicht reagieren ‚ÄúWarnung, ganz zu schweigen von der Zerst√∂rung der Benutzererfahrung. <br><br><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PredictionTask</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AsyncTask</span></span></span><span class="hljs-class">&lt;</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Void</span></span></span><span class="hljs-class">&gt; </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onPreExecute</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onPreExecute(); } <span class="hljs-comment"><span class="hljs-comment">// --- @Override protected Void doInBackground(Void... params) { try { # We get RGB values packed in integers # from the Bitmap, then break those # integers into individual triplets m_arrInput = new float[ m_nImageSize * m_nImageSize * 3]; int[] intValues = new int[ m_nImageSize * m_nImageSize]; m_bitmapForNn.getPixels(intValues, 0, m_nImageSize, 0, 0, m_nImageSize, m_nImageSize); for (int i = 0; i &lt; intValues.length; i++) { int val = intValues[i]; m_arrInput[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255f; m_arrInput[i * 3 + 2] = (val &amp; 0xFF) / 255f; } // --- tf = new TensorFlowInferenceInterface( getAssets(), MODEL_PATH); //Pass input into the tensorflow tf.feed(INPUT_NAME, m_arrInput, 1, m_nImageSize, m_nImageSize, 3); //compute predictions tf.run(new String[]{OUTPUT_NAME}, false); //copy output into PREDICTIONS array tf.fetch(OUTPUT_NAME, m_arrPrediction); } catch (Exception e) { e.getMessage(); } return null; } // --- @Override protected void onPostExecute(Void result) { super.onPostExecute(result); // --- enableControls(true); // --- tf = null; m_arrInput = null; # strResult contains 5 lines of text # with most probable dog breeds and # their probabilities m_strResult = ""; # What we do below is sorting the array # by probabilities (using map) # and getting in reverse order) the # first five entries TreeMap&lt;Float, Integer&gt; map = new TreeMap&lt;Float, Integer&gt;( Collections.reverseOrder()); for(int i = 0; i &lt; m_arrPrediction.length; i++) map.put(m_arrPrediction[i], i); int i = 0; for (TreeMap.Entry&lt;Float, Integer&gt; pair : map.entrySet()) { float key = pair.getKey(); int idx = pair.getValue(); String strBreed = m_arrBreedsArray[idx]; m_strResult += strBreed + ": " + String.format("%.6f", key) + "\n"; i++; if (i &gt; 5) break; } m_txtViewBreed.setVisibility(View.VISIBLE); m_txtViewBreed.setText(m_strResult); } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In onCreate () der MainActivity m√ºssen wir den onClickListener f√ºr die Schaltfl√§che "Process" hinzuf√ºgen: </font></font><br><br><pre> <code class="java hljs">m_btn_process.setOnClickListener(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> View.OnClickListener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onClick</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(View v)</span></span></span><span class="hljs-function"> </span></span>{ processImage(); } });</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Das processImage () ruft einfach den Thread auf, den wir oben gesehen haben: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">processImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { enableControls(<span class="hljs-keyword"><span class="hljs-keyword">false</span></span>); <span class="hljs-comment"><span class="hljs-comment">// --- PredictionTask prediction_task = new PredictionTask(); prediction_task.execute(); } catch (Exception e) { e.printStackTrace(); } }</span></span></code> </pre><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zus√§tzliche Details </font></font></h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wir werden in diesem Tutorial nicht auf UI-bezogenen Code eingehen, da dieser trivial ist und definitiv nicht Teil der Aufgabe "NN portieren" ist. </font><font style="vertical-align: inherit;">Es gibt jedoch einige Dinge, die gekl√§rt werden sollten. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Als wir unsere App vorstellten. </font><font style="vertical-align: inherit;">Durch das Starten mehrerer Instanzen haben wir gleichzeitig einen normalen Kontrollfluss verhindert: Wenn Sie ein Bild von Facebook freigeben und dann ein anderes freigeben, wird die Anwendung nicht neu gestartet. </font><font style="vertical-align: inherit;">Dies bedeutet, dass die "traditionelle" Art des Umgangs mit gemeinsam genutzten Daten durch Abfangen in onCreate in unserem Fall nicht ausreicht, da onCreate in einem gerade erstellten Szenario nicht aufgerufen wird. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So k√∂nnen Sie mit der Situation umgehen: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1. Rufen Sie in onCreate of MainActivity die Funktion onSharedIntent auf:</font></font><br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onCreate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( Bundle savedInstanceState)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onCreate(savedInstanceState); .... onSharedIntent(); ....</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> F√ºgen Sie au√üerdem einen Handler f√ºr onNewIntent hinzu: </font></font><br><br><pre> <code class="java hljs"><span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">protected</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onNewIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Intent intent)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">super</span></span>.onNewIntent(intent); setIntent(intent); onSharedIntent(); }</code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Die onSharedIntent-Funktion selbst: </font></font><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onSharedIntent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ Intent receivedIntent = getIntent(); String receivedAction = receivedIntent.getAction(); String receivedType = receivedIntent.getType(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (receivedAction.equals(Intent.ACTION_SEND)) { <span class="hljs-comment"><span class="hljs-comment">// If mime type is equal to image if (receivedType.startsWith("image/")) { m_txtViewBreed.setText(""); m_strResult = ""; Uri receivedUri = receivedIntent.getParcelableExtra( Intent.EXTRA_STREAM); if (receivedUri != null) { try { Bitmap bitmap = MediaStore.Images.Media.getBitmap( this.getContentResolver(), receivedUri); if(bitmap != null) { m_bitmap = bitmap; m_picView.setImageBitmap(m_bitmap); storeBitmap(); enableControls(true); } } catch (Exception e) { e.printStackTrace(); } } } } }</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Jetzt verarbeiten wir entweder das freigegebene Image von onCreate (wenn die App gerade gestartet wurde) oder von onNewIntent, wenn eine Instanz im Speicher gefunden wurde. </font></font><br><br><br><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Viel Gl√ºck! </font><font style="vertical-align: inherit;">Wenn Ihnen dieser Artikel gef√§llt, "m√∂gen" Sie ihn bitte in sozialen Netzwerken. Au√üerdem gibt es auf einer </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;pto=nl&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Website</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> selbst </font><font style="vertical-align: inherit;">soziale Schaltfl√§chen </font><font style="vertical-align: inherit;">.</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de447732/">https://habr.com/ru/post/de447732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de447718/index.html">Alles wird nach Plan verlaufen</a></li>
<li><a href="../de447720/index.html">IoT-Sicherheit. Problem 2. Smart Home</a></li>
<li><a href="../de447724/index.html">Wie intelligente St√§dte entstehen</a></li>
<li><a href="../de447728/index.html">Wir berechnen das Energiebudget einer Funkleitung f√ºr einen Satelliten im CubeSat-Format</a></li>
<li><a href="../de447730/index.html">Die Entwicklung des E-Mail-Marketings: von QWERTYUIOP zu GDPR</a></li>
<li><a href="../de447734/index.html">Warum Front-End die Prinzipien der Benutzeroberfl√§che verstehen sollte</a></li>
<li><a href="../de447736/index.html">Drohnenvideo - ein neuer Trend in sozialen Netzwerken</a></li>
<li><a href="../de447738/index.html">Julian Assange von der britischen Polizei festgenommen</a></li>
<li><a href="../de447742/index.html">Was ist die DevOps-Methodik und wer braucht sie?</a></li>
<li><a href="../de447744/index.html">Elbrus besteigen - Aufkl√§rung im Kampf. Technischer Teil 2. Interrupts, Ausnahmen, Systemtimer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>