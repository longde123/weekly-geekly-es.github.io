<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ü§µ ü§µüèø üíò AI, cours pratique. Architectures modernes de r√©seaux de neurones profonds pour la classification d'images üê© üëß üßöüèæ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dans un article pr√©c√©dent, Overview of Neural Networks for Image Classification , nous nous sommes familiaris√©s avec les concepts de base des r√©seaux ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>AI, cours pratique. Architectures modernes de r√©seaux de neurones profonds pour la classification d'images</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/intel/blog/417809/"><img src="https://habrastorage.org/webt/sz/-n/ep/sz-neph-gqvvjim1l0dreihnxlu.png"><br><br>  Dans un article pr√©c√©dent, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Overview of Neural Networks for Image Classification</a> , nous nous sommes familiaris√©s avec les concepts de base des r√©seaux de neurones convolutifs, ainsi que les id√©es sous-jacentes.  Dans cet article, nous allons examiner quelques architectures de r√©seaux de neurones profonds avec une grande puissance de traitement - comme AlexNet, ZFNet, VGG, GoogLeNet et ResNet - et r√©sumer les principaux avantages de chacune de ces architectures.  La structure de l'article est bas√©e sur une entr√©e de blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Concepts de base des r√©seaux de neurones convolutionnels, partie 3</a> . <br><a name="habracut"></a><br>  Actuellement, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ImageNet</a> Challenge est le principal incitatif sous-jacent au d√©veloppement de syst√®mes de reconnaissance de machines et de classification d'images.  La campagne est un concours pour travailler avec des donn√©es, dans lequel les participants re√ßoivent un grand ensemble de donn√©es (plus d'un million d'images).  La t√¢che du concours est de d√©velopper un algorithme qui vous permet de classer les images requises en objets dans 1000 cat√©gories - comme les chiens, les chats, les voitures et autres - avec un nombre minimum d'erreurs. <br><br>  Selon les r√®gles officielles du concours, les algorithmes doivent fournir une liste de cinq cat√©gories d'objets au maximum par ordre d√©croissant de confiance pour chaque cat√©gorie d'images.  La qualit√© du marquage d'image est √©valu√©e sur la base de l'√©tiquette qui correspond le mieux √† la propri√©t√© de v√©rit√© terrain de l'image.  L'id√©e est de permettre √† l'algorithme d'identifier plusieurs objets dans l'image et de ne pas accumuler de points de p√©nalit√© dans le cas o√π l'un des objets d√©tect√©s √©tait effectivement pr√©sent dans l'image mais n'√©tait pas inclus dans la propri√©t√© de v√©rit√© terrain. <br><br>  Au cours de la premi√®re ann√©e du concours, les participants ont re√ßu des attributs d'image pr√©s√©lectionn√©s pour la formation du mod√®le.  Il peut s'agir, par exemple, de signes de l'algorithme <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">SIFT</a> trait√©s √† l'aide de la quantification vectorielle et adapt√©s √† une utilisation dans la m√©thode du sac de mots ou √† une pr√©sentation sous forme de pyramide spatiale.  Cependant, en 2012, il y a eu une v√©ritable perc√©e dans ce domaine: un groupe de scientifiques de l'Universit√© de Toronto a d√©montr√© qu'un r√©seau neuronal profond peut obtenir des r√©sultats nettement meilleurs par rapport aux mod√®les d'apprentissage automatique traditionnels construits sur la base de vecteurs √† partir de propri√©t√©s d'image pr√©c√©demment s√©lectionn√©es.  Dans les sections suivantes, la premi√®re architecture innovante propos√©e en 2012 sera consid√©r√©e, ainsi que les architectures qui en sont les adeptes jusqu'en 2015. <br><br><img src="https://habrastorage.org/webt/b1/yc/0j/b1yc0jlxh6r5g9xmpvsfocbxmxo.png"><br>  <i>Diagramme des changements dans le nombre d'erreurs (en pourcentage) dans la classification des images ImageNet * pour les cinq principales cat√©gories.</i>  <i>Image tir√©e de la pr√©sentation de Kaiming He, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Deep Residual Learning for Image Recognition</a></i> <br><br><h3>  <font color="#0071c5">Alexnet</font> </h3><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">L'</a> architecture <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">AlexNet a</a> √©t√© propos√©e en 2012 par un groupe de scientifiques (A. Krizhevsky, I. Sutskever et J. Hinton) de l'Universit√© de Toronto.  Il s'agissait d'un travail innovant dans lequel les auteurs ont d'abord utilis√© (√† cette √©poque) des r√©seaux de neurones convolutionnels profonds avec une profondeur totale de huit couches (cinq couches convolutionnelles et trois couches enti√®rement connect√©es). <br><br><img src="https://habrastorage.org/webt/fm/wq/lm/fmwqlmznjszot3yzk1zfejfab6w.png"><br>  <i>Architecture AlexNet</i> <br><br>  L'architecture r√©seau se compose des couches suivantes: <br><br><ul><li>  [Couche de convolution + s√©lection de la valeur maximale + normalisation] x 2 </li><li>  [Couche de convolution] x 3 </li><li>  [Choisir la valeur maximale] </li><li>  [Couche compl√®te] x 3 </li></ul><br>  Un tel sch√©ma peut sembler un peu √©trange, car le processus d'apprentissage a √©t√© divis√© entre les deux GPU en raison de sa grande complexit√© de calcul.  Cette s√©paration du travail entre les GPU n√©cessite une s√©paration manuelle du mod√®le en blocs verticaux qui interagissent les uns avec les autres. <br><br>  L'architecture d'AlexNet a r√©duit le nombre d'erreurs pour les cinq principales cat√©gories √† 16,4% - pr√®s de la moiti√© par rapport aux d√©veloppements avanc√©s pr√©c√©dents!  Dans le cadre de cette architecture a √©galement √©t√© introduite une telle fonction d'activation comme unit√© de rectification lin√©aire ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ReLU</a> ), qui est actuellement la norme de l'industrie.  Voici un bref r√©sum√© des autres fonctionnalit√©s cl√©s de l'architecture AlexNet et de son processus d'apprentissage: <br><br><ul><li>  Augmentation intensive des donn√©es </li><li>  M√©thode d'exclusion </li><li>  Optimisation √† l'aide du moment SGD (voir le guide d'optimisation ¬´Pr√©sentation des algorithmes d'optimisation de descente de gradient¬ª) </li><li>  R√©glage manuel de la vitesse d'apprentissage (r√©duction de 10 de ce coefficient avec stabilisation de la pr√©cision) </li><li>  Le mod√®le final est une collection de sept r√©seaux de neurones convolutifs </li><li>  La formation a √©t√© men√©e sur deux processeurs graphiques NVIDIA * GeForce GTX * 580 avec un total de 3 Go de m√©moire vid√©o sur chacun d'eux. </li></ul><br><h3>  <font color="#0071c5">Zfnet</font> </h3><br>  L'architecture de r√©seau <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ZFNet</a> propos√©e par les chercheurs M. Zeiler et R. Fergus de l'Universit√© de New York est presque identique √† l'architecture AlexNet.  Les seules diff√©rences importantes entre eux sont les suivantes: <br><br><ul><li>  Taille et pas du filtre dans la premi√®re couche convolutive (dans AlexNet, la taille du filtre est 11 √ó 11 et le pas est 4; dans ZFNet - 7 √ó 7 et 2, respectivement) </li><li>  Le nombre de filtres dans des couches convolutives propres (3, 4, 5). </li></ul><br><img src="https://habrastorage.org/webt/gj/ji/wz/gjjiwzynwfzcnsrw3_vzgilojoo.png"><br>  <i>Architecture ZFNet</i> <br><br>  Gr√¢ce √† l'architecture ZFNet, le nombre d'erreurs pour les cinq principales cat√©gories est tomb√© √† 11,4%.  Le r√¥le principal est peut-√™tre jou√© par le r√©glage pr√©cis des hyperparam√®tres (taille et nombre de filtres, taille des paquets, vitesse d'apprentissage, etc.).  Cependant, il est √©galement probable que les id√©es de l'architecture ZFNet soient devenues une contribution tr√®s importante au d√©veloppement de r√©seaux de neurones convolutifs.  Ziller et Fergus ont propos√© un syst√®me de visualisation des noyaux, des poids et une vue cach√©e des images appel√© DeconvNet.  Gr√¢ce √† elle, une meilleure compr√©hension et un d√©veloppement ult√©rieur des r√©seaux de neurones convolutifs sont devenus possibles. <br><br><h3>  <font color="#0071c5">VGG Net</font> </h3><br>  En 2014, K. Simonyan et E. Zisserman de l'Universit√© d'Oxford ont propos√© une architecture appel√©e <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">VGG</a> .  L'id√©e principale et distinctive de cette structure est de <i>garder les filtres aussi simples que possible</i> .  Par cons√©quent, toutes les op√©rations de convolution sont effectu√©es en utilisant un filtre de taille 3 et une √©tape de taille 1, et toutes les op√©rations de sous-√©chantillonnage sont effectu√©es en utilisant un filtre de taille 2 et une √©tape de taille 2. Cependant, ce n'est pas tout.  Parall√®lement √† la simplicit√© des modules convolutifs, le r√©seau s'est consid√©rablement d√©velopp√© en profondeur - il compte d√©sormais 19 couches!  L'id√©e la plus importante, d'abord propos√©e dans ce travail, est d' <i>imposer des couches convolutives sans couches de sous-√©chantillonnage</i> .  L'id√©e sous-jacente est qu'une telle superposition fournit toujours un champ r√©cepteur suffisamment grand (par exemple, trois couches convolutionnelles superpos√©es de taille 3 √ó 3 par pas de 1 ont un champ r√©cepteur similaire √† une couche convolutionnelle de 7 √ó 7), cependant, le nombre de param√®tres est nettement inf√©rieur √† celui des r√©seaux avec de grands filtres (sert de r√©gularisateur).  De plus, il devient possible d'introduire des transformations non lin√©aires suppl√©mentaires. <br><br>  Essentiellement, les auteurs ont d√©montr√© que m√™me avec des blocs de construction tr√®s simples, vous pouvez obtenir des r√©sultats de qualit√© sup√©rieure dans le cadre du concours ImageNet.  Le nombre d'erreurs pour les cinq principales cat√©gories a √©t√© r√©duit √† 7,3%. <br><br><img src="https://habrastorage.org/webt/x7/rk/yj/x7rkyjkvxchmnso5gd56gf83eec.png"><br>  <i>Architecture VGG.</i>  <i>Veuillez noter que le nombre de filtres est inversement proportionnel √† la taille spatiale de l'image.</i> <br><br><h3>  <font color="#0071c5">GoogleNet</font> </h3><br>  Auparavant, tout le d√©veloppement de l'architecture devait simplifier les filtres et augmenter la profondeur du r√©seau.  En 2014, C. Szegedy, avec d'autres participants, a propos√© une approche compl√®tement diff√©rente et a cr√©√© l'architecture la plus complexe de l'√©poque, appel√©e GoogLeNet. <br><br><img src="https://habrastorage.org/webt/dl/rg/c7/dlrgc7gmujh1atkvisnx5onluim.png"><br>  <i>Architecture de GoogLeNet.</i>  <i>Il utilise le module Inception, surlign√© en vert sur la figure;</i>  <i>la construction du r√©seau est bas√©e sur ces modules</i> <br><br>  L'une des principales r√©alisations de ce travail est le soi-disant module Inception, qui est illustr√© dans la figure ci-dessous.  Les r√©seaux d'autres architectures traitent les donn√©es d'entr√©e s√©quentiellement, couche par couche, tout en utilisant le module Inception, <i>les donn√©es d'entr√©e sont trait√©es en parall√®le</i> .  Cela vous permet d'acc√©l√©rer la sortie et de minimiser le <i>nombre total de param√®tres</i> . <br><br><img src="https://habrastorage.org/webt/uo/ny/0t/uony0thmws6rtd5jyloaqxs5swe.png"><br>  <i>Module de cr√©ation.</i>  <i>Notez que le module utilise plusieurs branches parall√®les, qui calculent diff√©rentes propri√©t√©s en fonction des m√™mes donn√©es d'entr√©e, puis combinent les r√©sultats</i> <br><br>  Une autre astuce int√©ressante utilis√©e dans le module Inception est d'utiliser des couches convolutives de taille 1 √ó 1. Cela peut sembler inutile jusqu'√† ce que nous rappelions le fait que le filtre couvre toute la dimension de la profondeur.  Ainsi, une convolution 1 √ó 1 est un moyen simple de r√©duire la dimension d'une carte de propri√©t√©.  Ce type de couches convolutives a √©t√© pr√©sent√© pour la premi√®re fois dans <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Network par</a> M. Lin et al., Une explication compl√®te et compr√©hensible peut √©galement √™tre trouv√©e dans le billet de blog <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Convolution [1 √ó 1] - utilit√© contraire √† l'intuition</a> de A. Prakash. <br><br>  En fin de compte, cette architecture a r√©duit le nombre d'erreurs pour les cinq principales cat√©gories d'un autre demi pour cent - √† une valeur de 6,7 pour cent. <br><br><h3>  <font color="#0071c5">Resnet</font> </h3><br>  En 2015, un groupe de chercheurs (Cuming Hee et autres) de Microsoft Research Asia est venu avec une id√©e qui est actuellement consid√©r√©e par la plupart de la communaut√© comme l'une des √©tapes les plus importantes dans le d√©veloppement du deep learning. <br><br>  L'un des principaux probl√®mes des r√©seaux de neurones profonds est le probl√®me d'un gradient en voie de disparition.  En r√©sum√©, il s'agit d'un probl√®me technique qui se pose lors de l'utilisation de la m√©thode de propagation de l'erreur en retour pour l'algorithme de calcul de gradient.  Lorsque vous travaillez avec une propagation inverse des erreurs, une r√®gle de cha√Æne est utilis√©e.  De plus, si le gradient a une petite valeur √† la fin du r√©seau, il peut prendre une valeur infiniment petite au moment o√π il atteint le d√©but du r√©seau.  Cela peut entra√Æner des probl√®mes de nature compl√®tement diff√©rente, notamment l'impossibilit√© d'apprendre le r√©seau en principe (pour plus d'informations, voir l'entr√©e de blog de R. Kapur <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Le probl√®me d'un gradient de d√©coloration</a> ). <br><br>  Pour r√©soudre ce probl√®me, Caiming Hee et son groupe ont propos√© l'id√©e suivante - permettre au r√©seau d'√©tudier la cartographie r√©siduelle (un √©l√©ment qui devrait √™tre ajout√© √† l'entr√©e) au lieu de l'affichage lui-m√™me.  Techniquement, cela se fait en utilisant la connexion de d√©rivation illustr√©e sur la figure. <br><br><img src="https://habrastorage.org/webt/0r/tc/qs/0rtcqsfuosnmqzprsvgdiho2i_o.png"><br>  <i>Sch√©ma de principe du bloc r√©siduel: les donn√©es d'entr√©e sont transmises via une connexion raccourcie contournant les couches de conversion et ajout√©es au r√©sultat.</i>  <i>Veuillez noter qu'une connexion ¬´identique¬ª n'ajoute pas de param√®tres suppl√©mentaires au r√©seau, donc sa structure n'est pas compliqu√©e</i> <br><br>  Cette id√©e est extr√™mement simple, mais en m√™me temps extr√™mement efficace.  Il r√©sout le probl√®me de la disparition du gradient, lui permettant de se d√©placer sans aucun changement des couches sup√©rieures vers les couches inf√©rieures √† travers des connexions "identiques".  Gr√¢ce √† cette id√©e, vous pouvez former des r√©seaux tr√®s profonds, extr√™mement profonds. <br><br>  Le r√©seau qui a remport√© le D√©fi ImageNet en 2015 contenait 152 couches (les auteurs ont pu former le r√©seau contenant 1001 couches, mais il a produit approximativement le m√™me r√©sultat, alors ils ont cess√© de travailler avec lui).  De plus, cette id√©e a permis de r√©duire litt√©ralement de moiti√© le nombre d'erreurs pour les cinq principales cat√©gories - √† une valeur de 3,6%.  Selon une √©tude de <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ce que j'ai appris en faisant concurrence √† un r√©seau de neurones convolutifs dans le cadre du concours ImageNet par</a> A. Karpathy, la performance humaine pour cette t√¢che est d'environ 5%.  Cela signifie que l'architecture ResNet est capable de d√©passer les r√©sultats humains, au moins dans cette t√¢che de classification d'image. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr417809/">https://habr.com/ru/post/fr417809/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr417793/index.html">Oubliez les m√©gastructures des extraterrestres: de nouvelles observations expliquent le comportement de l'√©toile Tabby seule poussi√®re</a></li>
<li><a href="../fr417795/index.html">Mon obsession des jeux vid√©o √† l'adolescence n'est pas un "trouble du jeu"</a></li>
<li><a href="../fr417797/index.html">4 raisons pour lesquelles les projets de la NASA d√©passent les d√©lais et gonflent le budget</a></li>
<li><a href="../fr417801/index.html">Comment j'ai d√©m√©nag√© en Isra√´l apr√®s avoir bloqu√© Telegram</a></li>
<li><a href="../fr417803/index.html">Traitement photo en lot dans Blender</a></li>
<li><a href="../fr417813/index.html">Zabbix - surveillance des voisins OSPF √† l'aide de pi√®ges SNMPv3, douleur et d√©sespoir</a></li>
<li><a href="../fr417821/index.html">Network Digest: 20 documents d'experts sur les protocoles, les normes et la s√©curit√© de l'information</a></li>
<li><a href="../fr417823/index.html">Nouvelle g√©n√©ration: lancement du premier r√©seau commercial 5G au monde</a></li>
<li><a href="../fr417825/index.html">"Repousser les limites": la gamme 6 GHz sera donn√©e aux besoins du Wi-Fi</a></li>
<li><a href="../fr417827/index.html">Wi-Fi gratuit: un tribunal allemand abolit les peines impos√©es aux caf√©s pour violation des droits d'auteur des clients</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>