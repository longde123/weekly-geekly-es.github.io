<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üçÆ üö£üèª üåÅ Gewichtsunempfindliche neuronale Netze (WANN) üëßüèø üë®üèª‚Äçüç≥ üéã</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Googles neue Arbeit bietet eine Architektur neuronaler Netze, die die angeborenen Instinkte und Reflexe von Lebewesen simulieren kann, gefolgt von Wei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gewichtsunempfindliche neuronale Netze (WANN)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465369/"><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Googles</a> neue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> bietet eine Architektur neuronaler Netze, die die angeborenen Instinkte und Reflexe von Lebewesen simulieren kann, gefolgt von Weiterbildung w√§hrend des gesamten Lebens. </p><br><p>  Au√üerdem wird die Anzahl der Verbindungen innerhalb des Netzwerks erheblich reduziert, wodurch die Geschwindigkeit erh√∂ht wird. </p><a name="habracut"></a><br><p>  K√ºnstliche neuronale Netze sind zwar im Prinzip biologischen √§hnlich, unterscheiden sich jedoch immer noch zu stark von ihnen, um in ihrer reinen Form eine starke KI zu erzeugen.  Zum Beispiel ist es jetzt unm√∂glich, ein Modell einer Person in einem Simulator (oder einer Maus oder sogar einem Insekt) zu erstellen, ihm ein ‚ÄûGehirn‚Äú in Form eines modernen neuronalen Netzwerks zu geben und es zu trainieren.  Es funktioniert einfach nicht. </p><br><p>  Selbst wenn die Unterschiede im Lernmechanismus (zum Beispiel im Gehirn gibt es kein genaues Analogon zum Algorithmus zur Fehlerr√ºck√ºbertragung) und das Fehlen von Zeitskorrelationen mit mehreren Ma√üst√§ben, auf deren Grundlage das biologische Gehirn seine Arbeit aufbaut, verworfen werden, haben k√ºnstliche neuronale Netze mehrere weitere Probleme, die es ihnen nicht erm√∂glichen, ausreichend zu simulieren lebendes Gehirn.  Es ist wahrscheinlich, dass aufgrund dieser inh√§renten Probleme des jetzt verwendeten mathematischen Apparats das Reinforcement Learning, das darauf ausgelegt ist, die Ausbildung von Lebewesen auf der Grundlage der Belohnung so weit wie m√∂glich nachzuahmen, in der Praxis nicht so gut funktioniert, wie wir es uns w√ºnschen.  Obwohl es auf wirklich guten und richtigen Ideen basiert.  Die Entwickler selbst scherzen, dass das Gehirn RNN + A3C ist (d. H. Ein wiederkehrender Netzwerk + Schauspieler-Kritiker-Algorithmus f√ºr sein Training). </p><br><p>  Einer der auff√§lligsten Unterschiede zwischen dem biologischen Gehirn und k√ºnstlichen neuronalen Netzen besteht darin, dass die Struktur des lebenden Gehirns durch Millionen von Jahren Evolution vorkonfiguriert ist.  Obwohl der Neokortex, der f√ºr die h√∂here Nervenaktivit√§t bei S√§ugetieren verantwortlich ist, eine ann√§hernd einheitliche Struktur aufweist, ist die allgemeine Struktur des Gehirns durch Gene klar definiert.  Dar√ºber hinaus haben andere Tiere als S√§ugetiere (V√∂gel, Fische) √ºberhaupt keinen Neokortex, zeigen aber gleichzeitig ein komplexes Verhalten, das mit modernen neuronalen Netzen nicht erreichbar ist.  Eine Person hat auch k√∂rperliche Einschr√§nkungen in der Struktur des Gehirns, die schwer zu erkl√§ren sind.  Beispielsweise betr√§gt die Aufl√∂sung eines Auges ungef√§hr 100 Megapixel (~ 100 Millionen lichtempfindliche St√§bchen und Zapfen), was bedeutet, dass der Videostream von zwei Augen ungef√§hr 200 Megapixel mit einer Frequenz von mindestens 15 Bildern pro Sekunde betragen sollte.  In Wirklichkeit kann der Sehnerv jedoch nicht mehr als 2-3 Megapixel durch sich hindurchtreten.  Und seine Verbindungen richten sich √ºberhaupt nicht auf den n√§chstgelegenen Teil des Gehirns, sondern auf den okzipitalen Teil des visuellen Kortex. </p><br><p>  Ohne die Bedeutung des Neokortex zu beeintr√§chtigen (grob gesagt kann er bei der Geburt als Analogon zuf√§llig initiierter moderner neuronaler Netze angesehen werden), legen die Fakten nahe, dass sogar eine Person eine gro√üe Rolle in einer vorbestimmten Gehirnstruktur spielt.  Wenn zum Beispiel ein Baby nur wenige Minuten alt ist, um seine Zunge zu zeigen, wird es dank Spiegelneuronen auch seine Zunge herausstrecken.  Das gleiche passiert mit dem Lachen der Kinder.  Es ist bekannt, dass Babys von Geburt an mit ausgezeichneter Erkennung menschlicher Gesichter ‚Äûgen√§ht‚Äú wurden.  Noch wichtiger ist jedoch, dass das Nervensystem aller Lebewesen f√ºr ihre Lebensbedingungen optimiert ist.  Das Baby wird stundenlang nicht weinen, wenn es hungrig ist.  Er wird m√ºde werden.  Oder Angst vor etwas und halt die Klappe.  Der Fuchs wird nicht ersch√∂pft sein, bis der Hunger nach unzug√§nglichen Trauben greift.  Sie wird mehrere Versuche machen, entscheiden, dass er bitter ist und gehen.  Und dies ist kein Lernprozess, sondern ein von der Biologie vorgegebenes Verhalten.  Dar√ºber hinaus haben verschiedene Arten unterschiedliche.  Einige Raubtiere eilen sofort nach Beute, w√§hrend andere lange Zeit im Hinterhalt sitzen.  Und sie lernten dies nicht durch Versuch und Irrtum, sondern so ist ihre Biologie, gegeben durch Instinkte.  Ebenso haben viele Tiere von den ersten Lebensminuten an Programme zur Vermeidung von Raubtieren verkabelt, obwohl sie diese physisch noch nicht lernen konnten. </p><br><p>  Theoretisch k√∂nnen moderne Methoden zum Trainieren neuronaler Netze von einem vollst√§ndig verbundenen Netzwerk aus die √Ñhnlichkeit eines solchen vorab trainierten Gehirns erzeugen, unn√∂tige Verbindungen auf Null setzen (tats√§chlich abschneiden) und nur die notwendigen Verbindungen belassen.  Dies erfordert jedoch eine Vielzahl von Beispielen, es ist nicht bekannt, wie sie trainiert werden sollen, und vor allem gibt es derzeit keine guten M√∂glichkeiten, diese "anf√§ngliche" Struktur des Gehirns zu reparieren.  Das anschlie√üende Training √§ndert diese Gewichte und alles wird schlecht. </p><br><p>  Forscher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Google</a> haben diese Frage ebenfalls gestellt.  Ist es m√∂glich, eine anf√§ngliche Gehirnstruktur √§hnlich der biologischen zu erstellen, die bereits f√ºr die L√∂sung des Problems gut optimiert ist, und sie dann nur neu zu trainieren?  Theoretisch wird dies den L√∂sungsraum dramatisch einschr√§nken und es Ihnen erm√∂glichen, neuronale Netze schnell zu trainieren. </p><br><p>  Leider arbeiten vorhandene Algorithmen zur Optimierung der Netzwerkstruktur wie die Neural Architecture Search (NAS) mit ganzen Bl√∂cken.  Nach dem Hinzuf√ºgen oder Entfernen muss das neuronale Netzwerk von Grund auf neu trainiert werden.  Dies ist ein ressourcenintensiver Prozess, der das Problem nicht vollst√§ndig l√∂st. </p><br><p>  Daher schlugen die Forscher eine vereinfachte Version vor, die als "Weight Agnostic Neural Networks" (WANN) bezeichnet wird.  Die Idee ist, alle Gewichte eines neuronalen Netzwerks durch ein "gemeinsames" Gewicht zu ersetzen.  Und im Lernprozess geht es nicht darum, Gewichte zwischen Neuronen auszuw√§hlen, wie in gew√∂hnlichen neuronalen Netzen, sondern um die Struktur des Netzwerks selbst (Anzahl und Position der Neuronen), die mit den gleichen Gewichten die besten Ergebnisse zeigt.  Optimieren Sie es anschlie√üend so, dass das Netzwerk mit allen m√∂glichen Werten dieses Gesamtgewichts gut funktioniert (gemeinsam f√ºr alle Verbindungen zwischen Neuronen!). </p><br><p>  Dies ergibt die Struktur eines neuronalen Netzwerks, das nicht von bestimmten Gewichten abh√§ngt, sondern mit jedem gut funktioniert.  Weil es aufgrund der gesamten Netzwerkstruktur funktioniert.  Dies √§hnelt dem Gehirn eines Tieres, das bei der Geburt noch nicht mit bestimmten Skalen initialisiert wurde, aber aufgrund seiner allgemeinen Struktur bereits eingebettete Instinkte enth√§lt.  Und die anschlie√üende Feinabstimmung der Skalen w√§hrend des Trainings w√§hrend des gesamten Lebens macht dieses neuronale Netzwerk noch besser. </p><br><p>  Ein positiver Nebeneffekt dieses Ansatzes ist eine signifikante Verringerung der Anzahl von Neuronen im Netzwerk (da nur die wichtigsten Verbindungen √ºbrig bleiben), was seine Geschwindigkeit erh√∂ht.  Nachfolgend finden Sie einen Vergleich der Komplexit√§t eines klassischen vollst√§ndig verbundenen neuronalen Netzwerks (links) und eines passenden neuen (rechts). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  Um nach einer solchen Architektur zu suchen, verwendeten die Forscher den Topologie-Suchalgorithmus (NEAT).  Zuerst wird eine Reihe einfacher neuronaler Netze erstellt, und dann wird eine von drei Aktionen ausgef√ºhrt: Ein neues Neuron wird zu der bestehenden Verbindung zwischen zwei Neuronen hinzugef√ºgt, eine neue Verbindung mit zuf√§lligen Neuronen wird zu einem anderen Neuron hinzugef√ºgt oder die Aktivierungsfunktion in den Neuronen √§ndert sich (siehe die folgenden Abbildungen).  Und dann werden im Gegensatz zum klassischen NAS, bei dem nach optimalen Gewichten zwischen Neuronen gesucht wird, hier alle Gewichte mit einer einzigen Zahl initialisiert.  Und es wird eine Optimierung durchgef√ºhrt, um die Netzwerkstruktur zu finden, die in einem weiten Wertebereich dieses einen Gesamtgewichts am besten funktioniert.  Auf diese Weise wird ein Netzwerk erhalten, das nicht vom spezifischen Gewicht zwischen Neuronen abh√§ngt, sondern im gesamten Bereich gut funktioniert (aber alle Gewichte werden immer noch durch eine Zahl initiiert und unterscheiden sich nicht wie in normalen Netzwerken).  Dar√ºber hinaus versuchen sie als zus√§tzliches Ziel f√ºr die Optimierung, die Anzahl der Neuronen im Netzwerk zu minimieren. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/097/a11/a58/097a11a5854d96c62b9af8c862c5f9a6.png"></p><br><p>  Nachfolgend finden Sie eine allgemeine √úbersicht √ºber den Algorithmus. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c24/3ab/418/c243ab4183b3ac197c612382e60b8226.png"></p><br><ol><li>  schafft eine Population einfacher neuronaler Netze </li><li>  Jedes Netzwerk initialisiert alle seine Gewichte mit einer Zahl und f√ºr einen weiten Bereich von Zahlen: w = -2 ... + 2 </li><li>  Die resultierenden Netzwerke werden nach der Qualit√§t der L√∂sung des Problems und nach der Anzahl der Neuronen (nach unten) sortiert. </li><li>  Im Teil der besten Vertreter wird ein Neuron hinzugef√ºgt, eine Verbindung oder die Aktivierungsfunktion in einem Neuron √§ndert sich </li><li>  Diese modifizierten Netzwerke werden als Initiale in Punkt 1) verwendet. </li></ol><br><p>  All dies ist gut, aber Hunderte, wenn nicht Tausende verschiedener Ideen wurden f√ºr neuronale Netze vorgeschlagen.  Funktioniert das in der Praxis?  Ja, das tut es.  Nachfolgend finden Sie ein Beispiel f√ºr das Suchergebnis einer solchen Netzwerkarchitektur f√ºr das klassische Pendelwagenproblem.  Wie aus der Abbildung ersichtlich, funktioniert das neuronale Netzwerk gut mit allen Varianten des Gesamtgewichts (besser mit +1,0, versucht aber auch, das Pendel von -1,5 anzuheben).  Und nachdem dieses einzelne Gewicht optimiert wurde, funktioniert es einwandfrei (Option ‚ÄûFein abgestimmte Gewichte‚Äú in der Abbildung). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c7/faa/ce2/1c7faace27a7ba5298a3d13ab2863f4d.gif"></p><br><p>  In der Regel k√∂nnen Sie dieses einzelne Gesamtgewicht neu trainieren, da die Auswahl der Architektur auf einer begrenzten diskreten Anzahl von Parametern erfolgt (im obigen Beispiel -2, -1,1,2).  Und Sie k√∂nnen einen genaueren optimalen Parameter erhalten, z. B. 1,5.  Und Sie k√∂nnen das beste Gesamtgewicht als Ausgangspunkt f√ºr die Umschulung aller Gewichte verwenden, wie beim klassischen Training neuronaler Netze. </p><br><p>  Dies √§hnelt der Art und Weise, wie Tiere trainiert werden.  Mit Instinkten, die bei der Geburt nahezu optimal sind, und unter Verwendung dieser durch die Gene vorgegebenen Gehirnstruktur als erste trainieren die Tiere im Laufe ihres Lebens ihr Gehirn unter bestimmten √§u√üeren Bedingungen.  Weitere Details in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">k√ºrzlich erschienenen Artikel in der Zeitschrift Nature</a> . </p><br><p>  Unten finden Sie ein Beispiel f√ºr ein Netzwerk, das WANN f√ºr eine pixelbasierte Maschinensteuerungsaufgabe gefunden hat.  Bitte beachten Sie, dass dies eine Fahrt mit dem "blo√üen Instinkt" ist, mit dem gleichen Gesamtgewicht in allen Gelenken, ohne die klassische Feinabstimmung aller Gewichte.  Gleichzeitig ist das neuronale Netzwerk √§u√üerst einfach aufgebaut. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ee0/ef6/da2/ee0ef6da26d15ba297396b2b4443a8a8.png"><img src="https://habrastorage.org/getpro/habr/post_images/9be/018/565/9be01856536220bfb09556265efb118a.gif"></p><br><p>  Forscher schlagen vor, Ensembles aus WANN-Netzwerken als weiteren Anwendungsfall f√ºr WANN zu erstellen.  Das √ºbliche zuf√§llig initialisierte neuronale Netzwerk auf MNIST zeigt also eine Genauigkeit von etwa 10%.  Ein ausgew√§hltes einzelnes neuronales WANN-Netzwerk liefert ungef√§hr 80%, aber ein Ensemble von WANN mit unterschiedlichen Gesamtgewichten zeigt bereits&gt; 90%. </p><br><p>  Infolgedessen ahmt die von Google-Forschern vorgeschlagene Methode zur Suche nach der anf√§nglichen Architektur eines optimalen neuronalen Netzwerks nicht nur das Lernen von Tieren nach (Geburt mit eingebauten optimalen Instinkten und Umschulung w√§hrend des Lebens), sondern vermeidet auch die Simulation des gesamten Tierlebens durch vollwertiges Lernen des gesamten Netzwerks in klassischen evolution√§ren Algorithmen Einfache und schnelle Netzwerke gleichzeitig.  Das reicht aus, um ein wenig zu trainieren, um ein vollst√§ndig optimales neuronales Netzwerk zu erhalten. </p><br><h3 id="ssylki">  Referenzen </h3><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google AI-Blogeintrag</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein interaktiver Artikel, in dem Sie das Gesamtgewicht √§ndern und das Ergebnis √ºberwachen k√∂nnen</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Naturartikel √ºber die Bedeutung eingebetteter Instinkte bei der Geburt</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465369/">https://habr.com/ru/post/de465369/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465359/index.html">Huawei CloudCampus: Hoch Cloud-Service-Infrastruktur</a></li>
<li><a href="../de465361/index.html">Digitale Kreuzung in Kasan: Wie Verkehrssicherheitstechnologien in der Stadt eingef√ºhrt werden</a></li>
<li><a href="../de465363/index.html">Natas Web. Passage der CTF-Plattform zur Ausnutzung von Web-Schwachstellen. Teil 5</a></li>
<li><a href="../de465365/index.html">Windows 10-Setup-Skript</a></li>
<li><a href="../de465367/index.html">Wer hat sich nicht versteckt - ich bin nicht schuld (Geschichte der Geheimhaltung in der Luftfahrt)</a></li>
<li><a href="../de465371/index.html">Berechnung der Nullhypothese, zum Beispiel die Analyse der Geh√§lter ukrainischer Programmierer</a></li>
<li><a href="../de465373/index.html">Kein Test des ASUS ZenBook Pro 15 UX580GE - fast ein Jahr mit fast der Spitze</a></li>
<li><a href="../de465375/index.html">Verkauf von dedizierten Servern in den Niederlanden und in Moskau</a></li>
<li><a href="../de465377/index.html">Mach es selbst Skype</a></li>
<li><a href="../de465379/index.html">Selbstgemachte drahtlose eigenst√§ndige Insulinpumpensteuerung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>