<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🍮 🚣🏻 🌁 Gewichtsunempfindliche neuronale Netze (WANN) 👧🏿 👨🏻‍🍳 🎋</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Googles neue Arbeit bietet eine Architektur neuronaler Netze, die die angeborenen Instinkte und Reflexe von Lebewesen simulieren kann, gefolgt von Wei...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Gewichtsunempfindliche neuronale Netze (WANN)</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/465369/"><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Googles</a> neue <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Arbeit</a> bietet eine Architektur neuronaler Netze, die die angeborenen Instinkte und Reflexe von Lebewesen simulieren kann, gefolgt von Weiterbildung während des gesamten Lebens. </p><br><p>  Außerdem wird die Anzahl der Verbindungen innerhalb des Netzwerks erheblich reduziert, wodurch die Geschwindigkeit erhöht wird. </p><a name="habracut"></a><br><p>  Künstliche neuronale Netze sind zwar im Prinzip biologischen ähnlich, unterscheiden sich jedoch immer noch zu stark von ihnen, um in ihrer reinen Form eine starke KI zu erzeugen.  Zum Beispiel ist es jetzt unmöglich, ein Modell einer Person in einem Simulator (oder einer Maus oder sogar einem Insekt) zu erstellen, ihm ein „Gehirn“ in Form eines modernen neuronalen Netzwerks zu geben und es zu trainieren.  Es funktioniert einfach nicht. </p><br><p>  Selbst wenn die Unterschiede im Lernmechanismus (zum Beispiel im Gehirn gibt es kein genaues Analogon zum Algorithmus zur Fehlerrückübertragung) und das Fehlen von Zeitskorrelationen mit mehreren Maßstäben, auf deren Grundlage das biologische Gehirn seine Arbeit aufbaut, verworfen werden, haben künstliche neuronale Netze mehrere weitere Probleme, die es ihnen nicht ermöglichen, ausreichend zu simulieren lebendes Gehirn.  Es ist wahrscheinlich, dass aufgrund dieser inhärenten Probleme des jetzt verwendeten mathematischen Apparats das Reinforcement Learning, das darauf ausgelegt ist, die Ausbildung von Lebewesen auf der Grundlage der Belohnung so weit wie möglich nachzuahmen, in der Praxis nicht so gut funktioniert, wie wir es uns wünschen.  Obwohl es auf wirklich guten und richtigen Ideen basiert.  Die Entwickler selbst scherzen, dass das Gehirn RNN + A3C ist (d. H. Ein wiederkehrender Netzwerk + Schauspieler-Kritiker-Algorithmus für sein Training). </p><br><p>  Einer der auffälligsten Unterschiede zwischen dem biologischen Gehirn und künstlichen neuronalen Netzen besteht darin, dass die Struktur des lebenden Gehirns durch Millionen von Jahren Evolution vorkonfiguriert ist.  Obwohl der Neokortex, der für die höhere Nervenaktivität bei Säugetieren verantwortlich ist, eine annähernd einheitliche Struktur aufweist, ist die allgemeine Struktur des Gehirns durch Gene klar definiert.  Darüber hinaus haben andere Tiere als Säugetiere (Vögel, Fische) überhaupt keinen Neokortex, zeigen aber gleichzeitig ein komplexes Verhalten, das mit modernen neuronalen Netzen nicht erreichbar ist.  Eine Person hat auch körperliche Einschränkungen in der Struktur des Gehirns, die schwer zu erklären sind.  Beispielsweise beträgt die Auflösung eines Auges ungefähr 100 Megapixel (~ 100 Millionen lichtempfindliche Stäbchen und Zapfen), was bedeutet, dass der Videostream von zwei Augen ungefähr 200 Megapixel mit einer Frequenz von mindestens 15 Bildern pro Sekunde betragen sollte.  In Wirklichkeit kann der Sehnerv jedoch nicht mehr als 2-3 Megapixel durch sich hindurchtreten.  Und seine Verbindungen richten sich überhaupt nicht auf den nächstgelegenen Teil des Gehirns, sondern auf den okzipitalen Teil des visuellen Kortex. </p><br><p>  Ohne die Bedeutung des Neokortex zu beeinträchtigen (grob gesagt kann er bei der Geburt als Analogon zufällig initiierter moderner neuronaler Netze angesehen werden), legen die Fakten nahe, dass sogar eine Person eine große Rolle in einer vorbestimmten Gehirnstruktur spielt.  Wenn zum Beispiel ein Baby nur wenige Minuten alt ist, um seine Zunge zu zeigen, wird es dank Spiegelneuronen auch seine Zunge herausstrecken.  Das gleiche passiert mit dem Lachen der Kinder.  Es ist bekannt, dass Babys von Geburt an mit ausgezeichneter Erkennung menschlicher Gesichter „genäht“ wurden.  Noch wichtiger ist jedoch, dass das Nervensystem aller Lebewesen für ihre Lebensbedingungen optimiert ist.  Das Baby wird stundenlang nicht weinen, wenn es hungrig ist.  Er wird müde werden.  Oder Angst vor etwas und halt die Klappe.  Der Fuchs wird nicht erschöpft sein, bis der Hunger nach unzugänglichen Trauben greift.  Sie wird mehrere Versuche machen, entscheiden, dass er bitter ist und gehen.  Und dies ist kein Lernprozess, sondern ein von der Biologie vorgegebenes Verhalten.  Darüber hinaus haben verschiedene Arten unterschiedliche.  Einige Raubtiere eilen sofort nach Beute, während andere lange Zeit im Hinterhalt sitzen.  Und sie lernten dies nicht durch Versuch und Irrtum, sondern so ist ihre Biologie, gegeben durch Instinkte.  Ebenso haben viele Tiere von den ersten Lebensminuten an Programme zur Vermeidung von Raubtieren verkabelt, obwohl sie diese physisch noch nicht lernen konnten. </p><br><p>  Theoretisch können moderne Methoden zum Trainieren neuronaler Netze von einem vollständig verbundenen Netzwerk aus die Ähnlichkeit eines solchen vorab trainierten Gehirns erzeugen, unnötige Verbindungen auf Null setzen (tatsächlich abschneiden) und nur die notwendigen Verbindungen belassen.  Dies erfordert jedoch eine Vielzahl von Beispielen, es ist nicht bekannt, wie sie trainiert werden sollen, und vor allem gibt es derzeit keine guten Möglichkeiten, diese "anfängliche" Struktur des Gehirns zu reparieren.  Das anschließende Training ändert diese Gewichte und alles wird schlecht. </p><br><p>  Forscher <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">von Google</a> haben diese Frage ebenfalls gestellt.  Ist es möglich, eine anfängliche Gehirnstruktur ähnlich der biologischen zu erstellen, die bereits für die Lösung des Problems gut optimiert ist, und sie dann nur neu zu trainieren?  Theoretisch wird dies den Lösungsraum dramatisch einschränken und es Ihnen ermöglichen, neuronale Netze schnell zu trainieren. </p><br><p>  Leider arbeiten vorhandene Algorithmen zur Optimierung der Netzwerkstruktur wie die Neural Architecture Search (NAS) mit ganzen Blöcken.  Nach dem Hinzufügen oder Entfernen muss das neuronale Netzwerk von Grund auf neu trainiert werden.  Dies ist ein ressourcenintensiver Prozess, der das Problem nicht vollständig löst. </p><br><p>  Daher schlugen die Forscher eine vereinfachte Version vor, die als "Weight Agnostic Neural Networks" (WANN) bezeichnet wird.  Die Idee ist, alle Gewichte eines neuronalen Netzwerks durch ein "gemeinsames" Gewicht zu ersetzen.  Und im Lernprozess geht es nicht darum, Gewichte zwischen Neuronen auszuwählen, wie in gewöhnlichen neuronalen Netzen, sondern um die Struktur des Netzwerks selbst (Anzahl und Position der Neuronen), die mit den gleichen Gewichten die besten Ergebnisse zeigt.  Optimieren Sie es anschließend so, dass das Netzwerk mit allen möglichen Werten dieses Gesamtgewichts gut funktioniert (gemeinsam für alle Verbindungen zwischen Neuronen!). </p><br><p>  Dies ergibt die Struktur eines neuronalen Netzwerks, das nicht von bestimmten Gewichten abhängt, sondern mit jedem gut funktioniert.  Weil es aufgrund der gesamten Netzwerkstruktur funktioniert.  Dies ähnelt dem Gehirn eines Tieres, das bei der Geburt noch nicht mit bestimmten Skalen initialisiert wurde, aber aufgrund seiner allgemeinen Struktur bereits eingebettete Instinkte enthält.  Und die anschließende Feinabstimmung der Skalen während des Trainings während des gesamten Lebens macht dieses neuronale Netzwerk noch besser. </p><br><p>  Ein positiver Nebeneffekt dieses Ansatzes ist eine signifikante Verringerung der Anzahl von Neuronen im Netzwerk (da nur die wichtigsten Verbindungen übrig bleiben), was seine Geschwindigkeit erhöht.  Nachfolgend finden Sie einen Vergleich der Komplexität eines klassischen vollständig verbundenen neuronalen Netzwerks (links) und eines passenden neuen (rechts). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ebf/c30/eca/ebfc30ecaa458eca2fe85d3c43956b47.png"></p><br><p>  Um nach einer solchen Architektur zu suchen, verwendeten die Forscher den Topologie-Suchalgorithmus (NEAT).  Zuerst wird eine Reihe einfacher neuronaler Netze erstellt, und dann wird eine von drei Aktionen ausgeführt: Ein neues Neuron wird zu der bestehenden Verbindung zwischen zwei Neuronen hinzugefügt, eine neue Verbindung mit zufälligen Neuronen wird zu einem anderen Neuron hinzugefügt oder die Aktivierungsfunktion in den Neuronen ändert sich (siehe die folgenden Abbildungen).  Und dann werden im Gegensatz zum klassischen NAS, bei dem nach optimalen Gewichten zwischen Neuronen gesucht wird, hier alle Gewichte mit einer einzigen Zahl initialisiert.  Und es wird eine Optimierung durchgeführt, um die Netzwerkstruktur zu finden, die in einem weiten Wertebereich dieses einen Gesamtgewichts am besten funktioniert.  Auf diese Weise wird ein Netzwerk erhalten, das nicht vom spezifischen Gewicht zwischen Neuronen abhängt, sondern im gesamten Bereich gut funktioniert (aber alle Gewichte werden immer noch durch eine Zahl initiiert und unterscheiden sich nicht wie in normalen Netzwerken).  Darüber hinaus versuchen sie als zusätzliches Ziel für die Optimierung, die Anzahl der Neuronen im Netzwerk zu minimieren. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/097/a11/a58/097a11a5854d96c62b9af8c862c5f9a6.png"></p><br><p>  Nachfolgend finden Sie eine allgemeine Übersicht über den Algorithmus. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c24/3ab/418/c243ab4183b3ac197c612382e60b8226.png"></p><br><ol><li>  schafft eine Population einfacher neuronaler Netze </li><li>  Jedes Netzwerk initialisiert alle seine Gewichte mit einer Zahl und für einen weiten Bereich von Zahlen: w = -2 ... + 2 </li><li>  Die resultierenden Netzwerke werden nach der Qualität der Lösung des Problems und nach der Anzahl der Neuronen (nach unten) sortiert. </li><li>  Im Teil der besten Vertreter wird ein Neuron hinzugefügt, eine Verbindung oder die Aktivierungsfunktion in einem Neuron ändert sich </li><li>  Diese modifizierten Netzwerke werden als Initiale in Punkt 1) verwendet. </li></ol><br><p>  All dies ist gut, aber Hunderte, wenn nicht Tausende verschiedener Ideen wurden für neuronale Netze vorgeschlagen.  Funktioniert das in der Praxis?  Ja, das tut es.  Nachfolgend finden Sie ein Beispiel für das Suchergebnis einer solchen Netzwerkarchitektur für das klassische Pendelwagenproblem.  Wie aus der Abbildung ersichtlich, funktioniert das neuronale Netzwerk gut mit allen Varianten des Gesamtgewichts (besser mit +1,0, versucht aber auch, das Pendel von -1,5 anzuheben).  Und nachdem dieses einzelne Gewicht optimiert wurde, funktioniert es einwandfrei (Option „Fein abgestimmte Gewichte“ in der Abbildung). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c7/faa/ce2/1c7faace27a7ba5298a3d13ab2863f4d.gif"></p><br><p>  In der Regel können Sie dieses einzelne Gesamtgewicht neu trainieren, da die Auswahl der Architektur auf einer begrenzten diskreten Anzahl von Parametern erfolgt (im obigen Beispiel -2, -1,1,2).  Und Sie können einen genaueren optimalen Parameter erhalten, z. B. 1,5.  Und Sie können das beste Gesamtgewicht als Ausgangspunkt für die Umschulung aller Gewichte verwenden, wie beim klassischen Training neuronaler Netze. </p><br><p>  Dies ähnelt der Art und Weise, wie Tiere trainiert werden.  Mit Instinkten, die bei der Geburt nahezu optimal sind, und unter Verwendung dieser durch die Gene vorgegebenen Gehirnstruktur als erste trainieren die Tiere im Laufe ihres Lebens ihr Gehirn unter bestimmten äußeren Bedingungen.  Weitere Details in einem <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">kürzlich erschienenen Artikel in der Zeitschrift Nature</a> . </p><br><p>  Unten finden Sie ein Beispiel für ein Netzwerk, das WANN für eine pixelbasierte Maschinensteuerungsaufgabe gefunden hat.  Bitte beachten Sie, dass dies eine Fahrt mit dem "bloßen Instinkt" ist, mit dem gleichen Gesamtgewicht in allen Gelenken, ohne die klassische Feinabstimmung aller Gewichte.  Gleichzeitig ist das neuronale Netzwerk äußerst einfach aufgebaut. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ee0/ef6/da2/ee0ef6da26d15ba297396b2b4443a8a8.png"><img src="https://habrastorage.org/getpro/habr/post_images/9be/018/565/9be01856536220bfb09556265efb118a.gif"></p><br><p>  Forscher schlagen vor, Ensembles aus WANN-Netzwerken als weiteren Anwendungsfall für WANN zu erstellen.  Das übliche zufällig initialisierte neuronale Netzwerk auf MNIST zeigt also eine Genauigkeit von etwa 10%.  Ein ausgewähltes einzelnes neuronales WANN-Netzwerk liefert ungefähr 80%, aber ein Ensemble von WANN mit unterschiedlichen Gesamtgewichten zeigt bereits&gt; 90%. </p><br><p>  Infolgedessen ahmt die von Google-Forschern vorgeschlagene Methode zur Suche nach der anfänglichen Architektur eines optimalen neuronalen Netzwerks nicht nur das Lernen von Tieren nach (Geburt mit eingebauten optimalen Instinkten und Umschulung während des Lebens), sondern vermeidet auch die Simulation des gesamten Tierlebens durch vollwertiges Lernen des gesamten Netzwerks in klassischen evolutionären Algorithmen Einfache und schnelle Netzwerke gleichzeitig.  Das reicht aus, um ein wenig zu trainieren, um ein vollständig optimales neuronales Netzwerk zu erhalten. </p><br><h3 id="ssylki">  Referenzen </h3><br><ol><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Google AI-Blogeintrag</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Ein interaktiver Artikel, in dem Sie das Gesamtgewicht ändern und das Ergebnis überwachen können</a> </li><li>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Naturartikel über die Bedeutung eingebetteter Instinkte bei der Geburt</a> </li></ol></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de465369/">https://habr.com/ru/post/de465369/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de465359/index.html">Huawei CloudCampus: Hoch Cloud-Service-Infrastruktur</a></li>
<li><a href="../de465361/index.html">Digitale Kreuzung in Kasan: Wie Verkehrssicherheitstechnologien in der Stadt eingeführt werden</a></li>
<li><a href="../de465363/index.html">Natas Web. Passage der CTF-Plattform zur Ausnutzung von Web-Schwachstellen. Teil 5</a></li>
<li><a href="../de465365/index.html">Windows 10-Setup-Skript</a></li>
<li><a href="../de465367/index.html">Wer hat sich nicht versteckt - ich bin nicht schuld (Geschichte der Geheimhaltung in der Luftfahrt)</a></li>
<li><a href="../de465371/index.html">Berechnung der Nullhypothese, zum Beispiel die Analyse der Gehälter ukrainischer Programmierer</a></li>
<li><a href="../de465373/index.html">Kein Test des ASUS ZenBook Pro 15 UX580GE - fast ein Jahr mit fast der Spitze</a></li>
<li><a href="../de465375/index.html">Verkauf von dedizierten Servern in den Niederlanden und in Moskau</a></li>
<li><a href="../de465377/index.html">Mach es selbst Skype</a></li>
<li><a href="../de465379/index.html">Selbstgemachte drahtlose eigenständige Insulinpumpensteuerung</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>