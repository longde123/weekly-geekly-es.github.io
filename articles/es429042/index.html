<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üîò ü§òüèΩ üè£ Estamos probando SharxBase, una plataforma de virtualizaci√≥n de software y hardware del proveedor ruso SharxDC üë©üèΩ‚Äç‚öïÔ∏è üë¥üèº üöß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hoy hablar√© sobre la plataforma hiperconvergente SharxBase. No hubo revisi√≥n de este complejo en Habr√©, y se decidi√≥ poner fin a esta injusticia. Nues...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Estamos probando SharxBase, una plataforma de virtualizaci√≥n de software y hardware del proveedor ruso SharxDC</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/jetinfosystems/blog/429042/">  Hoy hablar√© sobre la plataforma hiperconvergente SharxBase.  No hubo revisi√≥n de este complejo en Habr√©, y se decidi√≥ poner fin a esta injusticia.  Nuestro equipo logr√≥ probar la soluci√≥n "en batalla", los resultados est√°n a continuaci√≥n. <br><br><img src="https://habrastorage.org/webt/my/e0/-f/mye0-flo3itmlmf2o1zry6tu-eg.png" alt="imagen"><br><br>  PD: Hay muchas tablas, n√∫meros reales y otras "carnes" debajo del corte.  Para aquellos que est√°n inmersos en la esencia, ¬°bienvenidos! <br><a name="habracut"></a><br><h2>  Sobre el producto </h2><br>  La plataforma SharxBase se basa en servidores hechos por Intel y software de c√≥digo abierto OpenNebula y StorPool.  Viene en forma de una soluci√≥n en caja, que incluye hardware de servidor con virtualizaci√≥n preinstalada y software de almacenamiento distribuido. <br><br>  Una de las cuatro configuraciones est√°ndar b√°sicas (peque√±a, mediana, grande, almacenamiento) est√° disponible por pedido, que difieren en la cantidad de recursos inform√°ticos disponibles (procesadores, RAM) y espacio en disco.  Los servidores est√°n dise√±ados como m√≥dulos: un chasis t√≠pico de 2RU, que puede acomodar hasta cuatro servidores, para la instalaci√≥n en un rack de servidores est√°ndar de 19 ". La plataforma admite tanto el escalamiento horizontal al aumentar el n√∫mero de nodos, como el vertical, al aumentar la cantidad de RAM en los nodos , instalaci√≥n de unidades adicionales y tarjetas de expansi√≥n. Actualmente admitimos la instalaci√≥n de adaptadores de red, m√≥dulos de control de arranque, unidades NVMe. <br><br><h2>  Arquitectura de almacenamiento </h2><br>  Para la organizaci√≥n de almacenamiento distribuido con tolerancia a fallas, se utilizan unidades flash (SSD y / o NVMe).  El medio utilizado es Ethernet.  Para transferir el almacenamiento de almacenamiento, es necesario usar interfaces de red dedicadas, al menos dos interfaces de 25 GbE.  Los servicios que proporcionan almacenamiento distribuido funcionan en cada servidor del cl√∫ster y utilizan parte de sus recursos inform√°ticos.  La cantidad de recursos depende de la cantidad y el volumen de las unidades instaladas, en promedio, la sobrecarga es de 34 GB de RAM por host.  La conexi√≥n al almacenamiento distribuido se realiza a trav√©s del protocolo de acceso de bloque iSCSI.  Para garantizar la tolerancia a fallos, se admite la copia de seguridad de datos dos o tres veces.  Para instalaciones productivas, el fabricante recomienda el uso de triple redundancia.  Actualmente, a partir de las tecnolog√≠as de optimizaci√≥n de almacenamiento, solo se admite el aprovisionamiento delgado.  La deduplicaci√≥n y la compresi√≥n de datos mediante almacenamiento distribuido no son compatibles.  Las versiones futuras admitir√°n la codificaci√≥n de borrado. <br><br><h2>  Virtualizaci√≥n </h2><br>  Para iniciar una m√°quina virtual (VM), se utiliza el hipervisor KVM.  Toda la funcionalidad b√°sica para su creaci√≥n y gesti√≥n es compatible: <br><br><ul><li>  creaci√≥n de una VM desde cero con la indicaci√≥n de la configuraci√≥n de hardware requerida (n√∫cleos de procesador, tama√±o de RAM, n√∫mero y tama√±o de discos virtuales, n√∫mero de adaptadores de red, etc.); </li><li>  VM clonaci√≥n de una plantilla existente o; </li><li>  crear una instant√°nea (instant√°nea), eliminar una instant√°nea, revertir los cambios realizados en la m√°quina virtual desde el momento en que se tom√≥ la instant√°nea; </li><li>  Cambiar la configuraci√≥n de hardware de una m√°quina virtual creada previamente, incluida la conexi√≥n o desconexi√≥n de un disco virtual o un adaptador de red para una m√°quina virtual incluida (conexi√≥n / desconexi√≥n en caliente); </li><li>  Migraci√≥n de VM entre servidores de virtualizaci√≥n </li><li>  supervisar el estado de la VM, incluida la supervisi√≥n de la carga de recursos inform√°ticos y discos virtuales (tama√±o actual, volumen de E / S en MB / so IOPS); </li><li>  programar operaciones con m√°quinas virtuales de acuerdo con un cronograma (encendido, apagado, creaci√≥n de una instant√°nea, etc.); </li><li>  conectar y administrar m√°quinas virtuales a trav√©s de protocolos VNC o SPICE desde una consola web. </li></ul><br><img src="https://habrastorage.org/webt/s3/3z/cn/s33zcnamce-04s6gj2jf5oc31iy.png" alt="imagen"><br>  <i>Diagrama de bloques t√≠pico (4 nodos)</i> <br><br>  La gesti√≥n de la plataforma se realiza desde la interfaz gr√°fica o la l√≠nea de comando (local o remotamente cuando se conecta a trav√©s de SSH), as√≠ como a trav√©s de la API p√∫blica. <br><br>  Entre las limitaciones de la plataforma de virtualizaci√≥n, se puede observar la ausencia de mecanismos para equilibrar autom√°ticamente las m√°quinas virtuales entre los hosts del cl√∫ster. <br><br>  Adem√°s de admitir la virtualizaci√≥n del servidor, SharxBase tiene la capacidad de crear centros de datos configurados por software e infraestructuras de nube privada.  Como ejemplo de tales funciones, uno puede notar: <br><br><ul><li>  gesti√≥n de derechos de acceso en funci√≥n de la membres√≠a de usuarios en grupos y listas de control de acceso (ACL): los derechos pueden asignarse a diferentes grupos de usuarios que restringen el acceso a componentes de infraestructura virtual; </li><li>  contabilidad del consumo de recursos (contabilidad): procesadores, RAM, recursos de disco; </li><li>  Estimaci√≥n del costo de consumo de recursos inform√°ticos (showback) en unidades arbitrarias en funci√≥n de los recursos consumidos y sus precios; </li><li>  caracter√≠sticas b√°sicas de IPAM (gesti√≥n de direcciones IP): asignaci√≥n autom√°tica de direcciones IP para interfaces de red VM desde un rango predeterminado; </li><li>  capacidades b√°sicas de SDN: crear un enrutador virtual para transferir tr√°fico entre redes virtuales. </li></ul><br>  Utilizando el m√≥dulo de protecci√≥n de la informaci√≥n desarrollado, SharxBase implementa medidas adicionales para garantizar la seguridad de la informaci√≥n del sistema de gesti√≥n de la plataforma: requisitos personalizables para las contrase√±as de las cuentas de usuario (complejidad, duraci√≥n, duraci√≥n del uso, repetibilidad, etc.), bloqueo de usuarios, gesti√≥n de las sesiones de acceso actuales a la consola de gesti√≥n, registro eventos y otros. El software se inscribe en el Registro de software ruso (n√∫mero 4445).  Se recibi√≥ una conclusi√≥n positiva del laboratorio de pruebas sobre las pruebas de certificaci√≥n completadas con √©xito del software SharxBase en el sistema de certificaci√≥n FSTEC RF para el nivel 4 de monitoreo de la ausencia de NDV, y tambi√©n para el cumplimiento de las especificaciones t√©cnicas (cumplimiento de los requisitos para proteger los entornos de virtualizaci√≥n) hasta el nivel de seguridad GIS clase 1 / ISPD inclusive.  La obtenci√≥n de un certificado de cumplimiento de los requisitos en el sistema de certificaci√≥n para la seguridad de la informaci√≥n significa que se espera el n√∫mero RUSS RU.0001.01BI00 (FSTEC de la Federaci√≥n de Rusia) en diciembre de 2018. <br><br>  Una descripci√≥n detallada de la funcionalidad se da en la tabla a continuaci√≥n. <br><br><h2>  Monitoreo </h2><br>  SharxBase Monitoring proporciona acceso a informaci√≥n avanzada sobre el estado de la plataforma, configuraciones de alertas y an√°lisis de estado de la plataforma. <br>  El subsistema de monitoreo es un sistema distribuido instalado en cada uno de los nodos del cl√∫ster y que proporciona datos sobre el estado de la plataforma al sistema de gesti√≥n de virtualizaci√≥n. <br><br>  El subsistema de monitoreo en tiempo real recopila informaci√≥n sobre los recursos de la plataforma, como: <br><br><table><tbody><tr><th>  Nodos de servidor </th><th>  Fuentes de alimentaci√≥n </th><th>  Interruptores </th><th>  M√°quinas virtuales </th><th>  Almac√©n de datos distribuidos </th></tr><tr><td>  - N√∫mero de serie de la unidad <br>  - N√∫mero de serie del nodo y la placa base <br>  - Unidad y temperatura de la unidad <br>  - Modelo de CPU y carga <br>  - N√∫meros de ranura, frecuencia, tama√±o y disponibilidad de RAM <br>  - Direcci√≥n de nodo y almacenamiento <br>  - La velocidad de rotaci√≥n de los ventiladores de refrigeraci√≥n. <br>  - Estado del adaptador de red <br>  - N√∫mero de serie del adaptador de red <br>  - El estado del disco y la informaci√≥n del sistema. <br></td><td>  - N√∫mero de serie de la fuente de alimentaci√≥n <br>  - El estado de la fuente de alimentaci√≥n y su carga. <br></td><td>  - Modelo de interruptor <br>  - Estado del conmutador y sus puertos. <br>  - La velocidad de rotaci√≥n de los ventiladores de refrigeraci√≥n. <br>  - Estado de los ventiladores de refrigeraci√≥n. <br>  - Mostrar la lista de VLAN <br></td><td>  - carga de la CPU <br>  - carga de RAM <br>  - carga de red <br>  - Estado de la m√°quina virtual <br>  - Velocidad de escritura / lectura de disco <br>  - Velocidad de conexi√≥n entrante / saliente <br></td><td>  - Visualizaci√≥n de espacio libre / ocupado <br>  - Estado del disco <br>  - Espacio en disco usado <br>  - Errores de manejo <br></td></tr></tbody></table><br><h2>  Subtotales </h2><br>  Las ventajas de la soluci√≥n incluyen: <br><br><ul><li>  la posibilidad de entrega a organizaciones en listas de sanciones; </li><li>  La soluci√≥n se basa en el proyecto OpenNebula, que se ha desarrollado activamente durante mucho tiempo; </li><li>  soporte para todas las funciones necesarias con respecto a la virtualizaci√≥n del servidor, suficiente para instalaciones peque√±as y medianas (hasta 128 hosts); </li><li>  La presencia de un m√≥dulo de seguridad de la informaci√≥n que garantiza la implementaci√≥n de los requisitos reglamentarios en el campo de la seguridad de la informaci√≥n. </li></ul><br>  Las desventajas de la soluci√≥n incluyen: <br><br><ul><li>  menor funcionalidad en comparaci√≥n con otras soluciones de HCI en el mercado (por ejemplo, Dell VxRail, Nutanix); </li><li>  soporte limitado de los sistemas de respaldo (actualmente se ha anunciado el soporte de Veritas NetBackup); </li><li>  Algunas de las tareas administrativas se realizan desde la consola y no son accesibles a trav√©s de la web. </li></ul><br><h2>  Funcionalidad </h2><br><img src="https://habrastorage.org/webt/o6/lt/el/o6ltelcxnsjd_c96lx8knqgqxna.png" alt="imagen"><br><img src="https://habrastorage.org/webt/r9/qa/r-/r9qar-ldggojsce4ual_tty2lku.png" alt="imagen"><br><img src="https://habrastorage.org/webt/k4/bg/_y/k4bg_ybgnbidmtqxvxyby2hgubu.png" alt="imagen"><br><img src="https://habrastorage.org/webt/gc/km/28/gckm28xyjnpyrti65uxzlzos4cu.png" alt="imagen"><br><br>  Al expandir la cartera de soluciones hiperconvergentes, realizamos pruebas de rendimiento y tolerancia a fallas junto con el proveedor. <br><br><h2>  Pruebas de rendimiento </h2><br>  El banco de pruebas fue un cl√∫ster de 4 nodos de servidores Intel HNS2600TP.  La configuraci√≥n de todos los servidores era id√©ntica.  Los servidores ten√≠an las siguientes caracter√≠sticas de hardware: <br><br><ul><li>  modelo de servidor: Intel HNS2600TP; </li><li>  dos procesadores Intel Xeon E5-2650 v4 (12 n√∫cleos con una frecuencia de reloj de 2.2 GHz y soporte para Hyper Threading); </li><li>  256 GB de RAM (224 GB de memoria est√°n disponibles para ejecutar la VM); </li><li>  adaptador de red con 2 puertos QSFP + con una velocidad de transferencia de datos de 40 Gb / s; </li><li>  un controlador RAID LSI SAS3008; </li><li>  6 unidades SSD SATA Intel DC S3700 con una capacidad de 800 GB cada una; </li><li>  Dos fuentes de alimentaci√≥n con una potencia nominal de 1600 W cada una. </li><li>  El software de virtualizaci√≥n SharxBase v1.5 est√° instalado en los servidores. </li></ul><br>  Todos los servidores conectados al conmutador de red Mellanox.  El diagrama de conexi√≥n se muestra en la figura. <br><br><img src="https://habrastorage.org/webt/mn/-c/ve/mn-cveotdolbvmhfd0ugghmpaos.png" alt="imagen"><br>  <i>Diagrama de conexi√≥n de servidores en un banco de pruebas</i> <br><br>  Todas las funcionalidades descritas anteriormente se confirmaron como resultado de las pruebas funcionales. <br><br>  Las pruebas del subsistema de disco se llevaron a cabo utilizando el software Vdbench versi√≥n 5.04.06.  En cada servidor f√≠sico, se cre√≥ una VM con el sistema operativo Linux con 8 vCPU, 16 GB de RAM.  Para probar en cada VM, se crearon 8 discos virtuales de 100 GB cada uno. <br><br>  Durante las pruebas, se verificaron los siguientes tipos de cargas: <br><br><ul><li>  (Copia de seguridad) 0% aleatorio, 100% de lectura, tama√±o de bloque de 64 KB, 1 IO excepcional; </li><li>  (Restaurar) 0% aleatorio, 100% de escritura, tama√±o de bloque de 64 KB, 1 IO excepcional; </li><li>  (T√≠pico) 100% aleatorio, 70% de lectura, tama√±o de bloque de 4 KB, 4 IO sobresalientes; </li><li>  (VDI) 100% aleatorio, 20% de lectura, tama√±o de bloque de 4 KB, 8 E / S sobresalientes; </li><li>  (OLTP) 100% aleatorio, 70% de lectura, tama√±o de bloque de 8 KB, 4 E / S sobresalientes. </li></ul><br>  Los resultados de la prueba de estos tipos se presentan en la tabla: <br><br><img src="https://habrastorage.org/webt/_w/pf/it/_wpfitc9wrh6hgycysn3ywpne2g.png" alt="imagen"><br><img src="https://habrastorage.org/webt/1j/0q/sy/1j0qsyi-8tbdj8usufyacft_1fg.png" alt="imagen"><br><img src="https://habrastorage.org/webt/98/y6/uq/98y6uqj2sre-qyh_ocldf8sagrs.png" alt="imagen"><br>  El almacenamiento proporciona indicadores de rendimiento particularmente altos en operaciones de lectura y escritura secuenciales de 8295.71 MB y 2966.16 MB, respectivamente.  El rendimiento de almacenamiento a una carga t√≠pica (E / S aleatoria con bloques de 4KB con un 70% de lectura) alcanza 133977.94 IOPS con un retraso promedio de E / S de 1.91 ms, y disminuye con un aumento en la relaci√≥n de operaciones de escritura a operaciones de lectura. <br><br><h2>  Prueba de tolerancia a fallas </h2><br>  Estas pruebas permitieron verificar que una falla de uno de los componentes del sistema no conduzca a un apagado de todo el sistema. <br><table><tbody><tr><th>  Prueba </th><th>  Detalles de prueba </th><th>  Comentarios </th></tr><tr><td>  Error de disco en el grupo de almacenamiento </td><td>  14:00 - el sistema est√° funcionando normalmente; <br>  14:11 - deshabilitando el primer SSD en el Servidor 1; <br>  14:12 - La falla de SSD se muestra en la consola de administraci√≥n de la plataforma; <br>  14:21 - deshabilita el primer SSD en el Servidor 2; <br>  14:35: la falla de dos SSD se muestra en la consola de administraci√≥n de la plataforma; <br>  14:38 - devuelva las unidades a los servidores 1 y 2. Los indicadores LED en el SSD no se muestran; <br>  14:40 - el ingeniero a trav√©s de la CLI realiz√≥ la adici√≥n de SSD al repositorio; <br>  14:50 - en la consola de administraci√≥n de la plataforma se muestran como funcionando; <br>  15:00 - Se completa la sincronizaci√≥n de los componentes de VM; <br></td><td>  El sistema funcion√≥ normalmente.  El indicador de tolerancia a fallos es el indicado. </td></tr><tr><td>  Fallo de red </td><td>  15:02 - el sistema est√° funcionando normalmente; <br>  15:17 - deshabilite uno de los dos puertos del Servidor 1; <br>  15:17 - p√©rdida de una solicitud Echo en la direcci√≥n IP de la consola web (el servidor aislado sirvi√≥ como l√≠der), la VM que se ejecuta en el servidor es accesible a trav√©s de la red; <br>  15:18 - deshabilitando el segundo puerto en el Servidor 1, la VM y la consola de administraci√≥n del servidor dejaron de estar disponibles; <br>  15:20 - La VM se reinici√≥ en el nodo del Servidor 3; <br>  15:26 - Las interfaces de red del servidor 1 est√°n conectadas, el servidor regres√≥ al cl√∫ster; <br>  15:35 - se completa la sincronizaci√≥n de los componentes de los discos VM; <br></td><td>  El sistema funcion√≥ normalmente. </td></tr><tr><td>  Fallo de un servidor f√≠sico </td><td>  15:35 - el sistema est√° funcionando normalmente; <br>  15:36 - apagando el Servidor 3 a trav√©s del comando de apagado en la interfaz IPMI; <br>  15:38 - la VM de prueba reiniciada en el Servidor 1; <br>  15:40 - inclusi√≥n del servidor 3; <br>  15:43 - operaci√≥n del servidor restaurada; <br>  15:47 - se completa la sincronizaci√≥n. <br></td><td>  El sistema funcion√≥ normalmente. </td></tr></tbody></table><br><h2>  Resultados de prueba </h2><br>  La plataforma SharxBase proporciona un alto nivel de disponibilidad y tolerancia a fallas en caso de falla de cualquier componente de hardware principal.  Debido a la triple redundancia para el subsistema de disco, la plataforma garantiza la disponibilidad y seguridad de los datos en caso de una doble falla. <br><br>  Las desventajas de la plataforma incluyen altos requisitos de espacio en disco causados ‚Äã‚Äãpor la necesidad de almacenar y sincronizar tres copias completas de datos, y la falta de mecanismos para una utilizaci√≥n m√°s eficiente del espacio en disco, como la deduplicaci√≥n, la compresi√≥n o la codificaci√≥n de borrado. <br><br>  Con base en los resultados de todas las pruebas realizadas, podemos concluir que la plataforma hiperconvergente SharxBase puede proporcionar un alto nivel de disponibilidad y rendimiento para diversos tipos de cargas, incluidos los sistemas OLTP, VDI y servicios de infraestructura. <br><br>  <i>Ilya Kuykin</i> <i><br></i>  <i>Ingeniero de dise√±o l√≠der de sistemas inform√°ticos,</i> <i><br></i>  <i>Jet Infosystems</i> <i><br></i> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/es429042/">https://habr.com/ru/post/es429042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../es429032/index.html">Splunk Essentials para la aplicaci√≥n de la industria de servicios financieros, o c√≥mo Splunk ingresa al mercado de an√°lisis financiero</a></li>
<li><a href="../es429034/index.html">Algunas historias sobre programadores underground</a></li>
<li><a href="../es429036/index.html">Conf√≠a en m√≠, s√© lo que estoy haciendo: autoadaptaci√≥n de un robot modular al entorno de ejecuci√≥n de tareas</a></li>
<li><a href="../es429038/index.html">Rust News # 2 (octubre de 2018)</a></li>
<li><a href="../es429040/index.html">Parchear c√≥digo Java en producci√≥n sin anestesia</a></li>
<li><a href="../es429046/index.html">Cuatro a√±os de desarrollo de SObjectizer-5.5. ¬øC√≥mo ha cambiado SObjectizer durante este tiempo?</a></li>
<li><a href="../es429048/index.html">Consejos para un hoster novato</a></li>
<li><a href="../es429050/index.html">Ataque de intercambio de criptomonedas Gate.io registrado</a></li>
<li><a href="../es429052/index.html">¬øPor qu√© en las laptops t√°ctiles algunos SPA dejaron de admitir eventos t√°ctiles?</a></li>
<li><a href="../es429054/index.html">Encuentra N diferencias. Experiencia de prueba de dise√±o Tinkoff.ru</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>