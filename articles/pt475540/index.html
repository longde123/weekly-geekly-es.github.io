<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üõåüèΩ üë®üèø‚Äçü§ù‚Äçüë®üèΩ üë©üèø‚Äçüé§ Cuidado com as vulnerabilidades que trazem solu√ß√µes alternativas. Parte 1: FragmentSmack / SegmentSmack üÜò ü§í üë©üèΩ‚Äçüîß</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ol√° pessoal! Meu nome √© Dmitry Samsonov, trabalho como administrador de sistemas l√≠der em Odnoklassniki. Temos mais de 7 mil servidores f√≠sicos, 11 mi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cuidado com as vulnerabilidades que trazem solu√ß√µes alternativas. Parte 1: FragmentSmack / SegmentSmack</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/475540/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/qy/tg/kcqytg4znoagvdqffwtwxhwzcjs.jpeg"></div><br><br>  Ol√° pessoal!  Meu nome √© Dmitry Samsonov, trabalho como administrador de sistemas l√≠der em Odnoklassniki.  Temos mais de 7 mil servidores f√≠sicos, 11 mil cont√™ineres em nossa nuvem e 200 aplicativos, que em diferentes configura√ß√µes formam 700 clusters diferentes.  A grande maioria dos servidores est√° executando o CentOS 7. <br>  Informa√ß√µes sobre a vulnerabilidade do FragmentSmack lan√ßada em 14 de agosto de 2018 <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVE-2018-5391</a> ) e SegmentSmack ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVE-2018-5390</a> ).  Essas s√£o vulnerabilidades com um vetor de ataque √† rede e uma classifica√ß√£o bastante alta (7.5), que amea√ßa com nega√ß√£o de servi√ßo (DoS) devido √† exaust√£o de recursos (CPU).  Uma corre√ß√£o no kernel do FragmentSmack n√£o foi proposta naquele momento; al√©m disso, saiu muito depois da publica√ß√£o de informa√ß√µes sobre a vulnerabilidade.  Para eliminar o SegmentSmack, foi proposto atualizar o kernel.  O pacote de atualiza√ß√£o em si foi lan√ßado no mesmo dia, tudo o que restava era instal√°-lo. <br>  N√£o, n√£o somos contra a atualiza√ß√£o do kernel!  No entanto, existem nuances ... <br><a name="habracut"></a><br><h4>  Como atualizamos o n√∫cleo do produto </h4><br>  Em geral, nada complicado: <br><ol><li>  Download de pacotes </li><li>  Instale-os em v√°rios servidores (incluindo servidores que hospedam nossa nuvem); </li><li>  Certifique-se de que nada esteja quebrado; </li><li>  Verifique se todas as configura√ß√µes padr√£o do kernel se aplicam sem erros; </li><li>  Aguarde alguns dias; </li><li>  Verifique o desempenho do servidor; </li><li>  Alterne a implanta√ß√£o de novos servidores para um novo kernel; </li><li>  Atualize todos os servidores por data centers (um data center por vez para minimizar o efeito para os usu√°rios em caso de problemas); </li><li>  Reinicie todos os servidores. </li></ol><br>  Repita o procedimento para todos os ramos dos n√∫cleos que temos.  No momento, isso √©: <br><br><ul><li>  Stock CentOS 7 3.10 - para a maioria dos servidores comuns; </li><li>  Vanilla 4.19 √© para a nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nuvem √∫nica,</a> porque precisamos de BFQ, BBR, etc; </li><li>  O Elrepo kernel-ml 5.2 √© para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distribuidores altamente carregados</a> , porque o 4.19 costumava se comportar de maneira inst√°vel, e os recursos precisam dos mesmos. </li></ul><br>  Como voc√™ deve ter adivinhado, reiniciar milhares de servidores leva mais tempo.  Como nem todas as vulnerabilidades s√£o cr√≠ticas para todos os servidores, apenas reiniciamos aquelas diretamente acess√≠veis pela Internet.  Na nuvem, para n√£o limitar a flexibilidade, n√£o vinculamos cont√™ineres acess√≠veis externamente a servidores individuais com um novo n√∫cleo, mas reinicializamos todos os hosts sem exce√ß√£o.  Felizmente, o procedimento √© mais f√°cil l√° do que nos servidores regulares.  Por exemplo, cont√™ineres sem estado podem simplesmente mudar para outro servidor durante a reinicializa√ß√£o. <br><br>  No entanto, ainda h√° muito trabalho, e pode levar v√°rias semanas e, em caso de problemas com a nova vers√£o - at√© v√°rios meses.  Os atacantes est√£o bem cientes disso, ent√£o o plano B √© necess√°rio. <br><br><h4>  FragmentSmack / SegmentSmack.  Solu√ß√£o alternativa </h4><br>  Felizmente, para algumas vulnerabilidades, esse plano "B" existe e √© chamado de Solu√ß√£o alternativa.  Na maioria das vezes, essa √© uma altera√ß√£o nas configura√ß√µes do kernel / aplicativo, que podem minimizar o poss√≠vel efeito ou eliminar completamente a explora√ß√£o de vulnerabilidades. <br><br>  No caso do FragmentSmack / SegmentSmack <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, a</a> seguinte solu√ß√£o alternativa foi proposta: <br><br><blockquote>  ‚Äú <i>Voc√™ pode alterar os valores padr√£o de 4 MB e 3 MB em net.ipv4.ipfrag_high_thresh e net.ipv4.ipfrag_low_thresh (e seus an√°logos para ipv6 net.ipv6.ipfrag_high_thresh e net.ipv6.ipfrag_low_thresh) por 256 kB e 192 kB, respectivamente.</i>  <i>Os testes mostram uma queda ligeira a significativa no uso da CPU durante um ataque, dependendo do equipamento, configura√ß√µes e condi√ß√µes.</i>  <i>No entanto, pode haver algum impacto no desempenho devido a ipfrag_high_thresh = 262144 bytes, pois apenas dois fragmentos de 64K podem caber na fila de reconstru√ß√£o por vez.</i>  <i>Por exemplo, existe o risco de que aplicativos que trabalham com pacotes UDP grandes sejam interrompidos</i> . ‚Äù </blockquote><br>  Os pr√≥prios par√¢metros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na documenta√ß√£o do kernel s√£o</a> descritos a seguir: <br><br><blockquote><code>ipfrag_high_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments.</code> </blockquote> <br><p></p><blockquote> <code>ipfrag_low_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments before the kernel <br> begins to remove incomplete fragment queues to free up resources. <br> The kernel still accepts new fragments for defragmentation.</code> </blockquote> <br>  N√£o temos UDP grande em servi√ßos de produ√ß√£o.  N√£o h√° tr√°fego fragmentado na LAN; h√°, mas n√£o significativo, tr√°fego na WAN.  Nada √© um mau press√°gio - voc√™ pode rolar a solu√ß√£o alternativa! <br><br><h4>  FragmentSmack / SegmentSmack.  Primeiro sangue </h4><br>  O primeiro problema que encontramos foi que os cont√™ineres na nuvem √†s vezes aplicavam apenas parcialmente as novas configura√ß√µes (apenas ipfrag_low_thresh) e √†s vezes eles n√£o usavam de jeito nenhum - eles apenas travavam no in√≠cio.  N√£o foi poss√≠vel reproduzir o problema de forma est√°vel (manualmente, todas as configura√ß√µes foram aplicadas sem dificuldades).  Entender por que o cont√™iner cai no in√≠cio tamb√©m n√£o √© t√£o simples: nenhum erro foi encontrado.  Uma coisa era certa: reverter as configura√ß√µes resolve o problema de derrubar cont√™ineres. <br><br>  Por que n√£o √© suficiente usar o Sysctl no host?  O cont√™iner vive em seu Namespace de rede dedicado, portanto, pelo menos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">parte dos par√¢metros Sysctl da rede</a> no cont√™iner podem diferir do host. <br><br>  Como exatamente as configura√ß√µes de Sysctl no cont√™iner se aplicam?  Como temos cont√™ineres sem privil√©gios, a altera√ß√£o de qualquer configura√ß√£o do Sysctl entrando no pr√≥prio cont√™iner falhar√° - simplesmente n√£o haver√° direitos suficientes.  Na √©poca, nossa nuvem usava o Docker (agora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Podman</a> ) para lan√ßar cont√™ineres.  A janela de encaixe via API passou os par√¢metros do novo cont√™iner, incluindo as configura√ß√µes necess√°rias do Sysctl. <br>  No decorrer da enumera√ß√£o das vers√µes, descobriu-se que a API do Docker n√£o gerou todos os erros (pelo menos na vers√£o 1.10).  Ao tentar iniciar o cont√™iner atrav√©s do "docker run", finalmente vimos pelo menos algo: <br><br> <code>write /proc/sys/net/ipv4/ipfrag_high_thresh: invalid argument docker: Error response from daemon: Cannot start container &lt;...&gt;: [9] System error: could not synchronise with container process.</code> <br> <br>  O valor do par√¢metro n√£o √© v√°lido.  Mas porque?  E por que n√£o √© v√°lido apenas √†s vezes?  Aconteceu que o Docker n√£o garantiu a ordem na qual os par√¢metros Sysctl foram usados ‚Äã‚Äã(a vers√£o mais recente testada foi a 1.13.1); portanto, √†s vezes o ipfrag_high_thresh tentava definir-se para 256K quando o ipfrag_low_thresh ainda era de 3M, ou seja, o limite superior era mais baixo que o inferior, o que causava um erro. <br><br>  Naquela √©poca, j√° usamos nosso pr√≥prio mecanismo para reconfigurar o cont√™iner ap√≥s iniciar (congelar o cont√™iner atrav√©s do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cgroup freezer</a> e executar comandos no espa√ßo de nomes do cont√™iner via <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ip netns</a> ) e tamb√©m adicionamos par√¢metros Sysctl a esta parte.  O problema foi resolvido. <br><br><h4>  FragmentSmack / SegmentSmack.  Primeiro sangue 2 </h4><br>  Antes de sabermos como usar a solu√ß√£o alternativa na nuvem, come√ßaram a chegar as primeiras queixas raras dos usu√°rios.  Nesse momento, v√°rias semanas se passaram desde o in√≠cio da solu√ß√£o alternativa nos primeiros servidores.  A investiga√ß√£o inicial mostrou que foram recebidas reclama√ß√µes sobre servi√ßos individuais, e nem todos os servidores desses servi√ßos.  O problema recuperou um car√°ter extremamente vago. <br><br>  Antes de tudo, √© claro, tentamos reverter as configura√ß√µes do Sysctl, mas isso n√£o deu nenhum efeito.  V√°rias manipula√ß√µes com as configura√ß√µes do servidor e do aplicativo tamb√©m n√£o ajudaram.  Reinicializa√ß√£o ajudou.  A reinicializa√ß√£o para Linux √© t√£o antinatural quanto era uma condi√ß√£o normal para trabalhar com o Windows nos velhos tempos.  No entanto, isso ajudou e escrevemos tudo para uma ‚Äúfalha no kernel‚Äù ao aplicar as novas configura√ß√µes no Sysctl.  Qu√£o fr√≠volo era ... <br><br>  Tr√™s semanas depois, o problema voltou a ocorrer.  A configura√ß√£o desses servidores era bastante simples: Nginx no modo proxy / balancer.  O tr√°fego √© um pouco.  Novo introdut√≥rio: o n√∫mero de erros 504 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gateway Timeout</a> ) est√° aumentando todos os dias nos clientes.  O gr√°fico mostra o n√∫mero de 504 erros por dia para este servi√ßo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xk/hk/rj/xkhkrjedsakcdrx6m_z8hgcgjsw.png"></div><br><br>  Todos os erros s√£o sobre o mesmo back-end - sobre o que est√° na nuvem.  O gr√°fico do consumo de mem√≥ria para fragmentos de pacotes nesse back-end foi o seguinte: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/fb/gt/jyfbgtoqrbqwv6cird2zawvatza.png"></div><br><br>  Essa √© uma das manifesta√ß√µes mais marcantes do problema nos gr√°ficos do sistema operacional.  Na nuvem, ao mesmo tempo, outro problema de rede foi corrigido com as configura√ß√µes de QoS (Controle de Tr√°fego).  No gr√°fico do consumo de mem√≥ria para fragmentos de pacotes, parecia exatamente o mesmo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/09/nf/4r/09nf4rxogxv9xppeumwujvmcr3m.png"></div><br><br>  A suposi√ß√£o era simples: se eles parecem iguais nos gr√°ficos, t√™m o mesmo motivo.  Al√©m disso, quaisquer problemas com esse tipo de mem√≥ria s√£o extremamente raros. <br><br>  A ess√™ncia do problema corrigido foi que usamos o sheduler fq packet com configura√ß√µes padr√£o em QoS.  Por padr√£o, para uma conex√£o, ele permite adicionar 100 pacotes √† fila e algumas conex√µes em uma situa√ß√£o de falta de canal come√ßaram a entupir a fila com falha.  Nesse caso, os pacotes caem.  Nas estat√≠sticas tc (tc -s qdisc), isso pode ser visto da seguinte maneira: <br><br> <code>qdisc fq 2c6c: parent 1:2c6c limit 10000p flow_limit 100p buckets 1024 orphan_mask 1023 quantum 3028 initial_quantum 15140 refill_delay 40.0ms <br> Sent 454701676345 bytes 491683359 pkt (dropped 464545, overlimits 0 requeues 0) <br> backlog 0b 0p requeues 0 <br> 1024 flows (1021 inactive, 0 throttled) <br> 0 gc, 0 highprio, 0 throttled, 464545 flows_plimit</code> <br> <br>  "464545 flows_plimit" s√£o os pacotes descartados devido a exceder o limite da fila de uma conex√£o e "464545 descartado" √© a soma de todos os pacotes descartados deste sheduler.  Depois de aumentar o comprimento da fila para 1 mil e reiniciar os cont√™ineres, o problema deixou de aparecer.  Voc√™ pode sentar em uma cadeira e tomar um smoothie. <br><br><h4>  FragmentSmack / SegmentSmack.  √öltimo sangue </h4><br>  Primeiro, alguns meses ap√≥s o an√∫ncio de vulnerabilidades no kernel, finalmente uma corre√ß√£o para o FragmentSmack apareceu (lembro que, com o an√∫ncio em agosto, uma corre√ß√£o foi lan√ßada apenas para o SegmentSmack), que nos deu a chance de abandonar a solu√ß√£o alternativa, o que causou muitos problemas.  Alguns dos servidores durante esse per√≠odo j√° conseguimos transferir para um novo kernel e agora tivemos que come√ßar do in√≠cio.  Por que atualizamos o kernel sem esperar pela corre√ß√£o do FragmentSmack?  O fato √© que o processo de prote√ß√£o contra essas vulnerabilidades coincidiu (e se fundiu) com o processo de atualiza√ß√£o do pr√≥prio CentOS (que leva ainda mais tempo do que atualizar apenas o kernel).  Al√©m disso, o SegmentSmack √© uma vulnerabilidade mais perigosa, e uma corre√ß√£o para ela apareceu imediatamente, de modo que o argumento era o mesmo.  No entanto, n√£o pudemos simplesmente atualizar o kernel no CentOS, porque a vulnerabilidade FragmentSmack, que apareceu durante o CentOS 7.5, foi corrigida apenas na vers√£o 7.6, por isso tivemos que parar a atualiza√ß√£o para 7.5 e come√ßar tudo de novo com a atualiza√ß√£o para 7.6.  E assim √©. <br><br>  Em segundo lugar, as reclama√ß√µes raras dos usu√°rios sobre problemas retornaram para n√≥s.  Agora j√° sabemos com certeza que todos eles est√£o conectados ao download de arquivos de clientes para alguns de nossos servidores.  E atrav√©s desses servidores, houve um n√∫mero muito pequeno de uploads da massa total. <br><br>  Como lembramos da hist√≥ria acima, a revers√£o do Sysctl n√£o ajudou.  A reinicializa√ß√£o ajudou, mas temporariamente. <br>  As suspeitas com o Sysctl n√£o foram levantadas, mas desta vez foi necess√°rio coletar o m√°ximo de informa√ß√µes poss√≠vel.  Al√©m disso, havia uma extrema falta de capacidade de reproduzir o problema com o upload do cliente para examinar com mais precis√£o o que estava acontecendo. <br><br>  A an√°lise de todas as estat√≠sticas e logs dispon√≠veis n√£o nos aproximou da compreens√£o do que estava acontecendo.  Havia uma falta aguda da capacidade de reproduzir o problema para "sentir" uma conex√£o espec√≠fica.  Por fim, os desenvolvedores da vers√£o especial do aplicativo conseguiram obter uma reprodu√ß√£o est√°vel de problemas no dispositivo de teste quando conectado via Wi-Fi.  Este foi um avan√ßo na investiga√ß√£o.  O cliente conectado ao Nginx, procurou proxy para o back-end, que era nosso aplicativo Java. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/g8/4a/xwg84aziibg7aaglsv6hdc2vyn8.png"></div><br><br>  A caixa de di√°logo com problemas foi a seguinte (corrigida no lado do proxy Nginx): <br><br><ol><li>  Cliente: solicita√ß√£o de informa√ß√µes sobre o download de um arquivo. </li><li>  Servidor Java: resposta. </li><li>  Cliente: POST com arquivo. </li><li>  Servidor Java: erro. </li></ol><br>  Ao mesmo tempo, o servidor Java grava no log que 0 bytes de dados foram recebidos do cliente e no proxy Nginx que a solicita√ß√£o levou mais de 30 segundos (30 segundos √© o tempo limite do aplicativo cliente).  Por que tempo limite e por que 0 bytes?  Do ponto de vista do HTTP, tudo funciona como deveria, mas o POST com o arquivo parece desaparecer da rede.  E desaparece entre o cliente e o Nginx.  √â hora de se armar com o Tcpdump!  Mas primeiro voc√™ precisa entender a configura√ß√£o de rede.  O proxy nginx est√° por tr√°s do balanceador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">N3ware</a> L3.  O encapsulamento √© usado para entregar pacotes do balanceador L3 para o servidor, que adiciona seus cabe√ßalhos aos pacotes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hs/ge/mf/hsgemfebrlkpjzvtz-cdrghudxu.png"></div><br><br>  Ao mesmo tempo, a rede chega a esse servidor na forma de tr√°fego marcado por Vlan, que tamb√©m adiciona seus campos aos pacotes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-a/ns/ap/-ansap0a5oyjsnqrf0onvsxg0tw.png"></div><br><br>  E esse tr√°fego pode ser fragmentado (a porcentagem muito pequena de tr√°fego fragmentado de entrada de que falamos ao avaliar os riscos da solu√ß√£o alternativa), o que tamb√©m altera o conte√∫do dos cabe√ßalhos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gd/yx/48/gdyx48_bc15aj2xmhn5kb0_omci.png"></div><br><br>  Mais uma vez: os pacotes s√£o encapsulados por uma tag Vlan, encapsulados por um t√∫nel, fragmentados.  Para entender melhor como isso acontece, vamos rastrear a rota do pacote do cliente para o proxy Nginx. <br><br><ol><li>  O pacote chega ao balanceador L3.  Para o roteamento correto dentro do data center, o pacote √© encapsulado no t√∫nel e enviado para a placa de rede. </li><li>  Como os cabe√ßalhos de pacotes + t√∫neis n√£o se encaixam na MTU, o pacote √© cortado em fragmentos e enviado √† rede. </li><li>  O switch ap√≥s o balanceador L3 ao receber o pacote adiciona uma tag Vlan e a envia ainda mais. </li><li>  O switch antes do proxy Nginx v√™ (de acordo com as configura√ß√µes da porta) que o servidor est√° esperando um pacote encapsulado em Vlan, para que ele o envie como est√°, sem remover a tag Vlan. </li><li>  O Linux recebe fragmentos de pacotes individuais e os cola em um pacote grande. </li><li>  Em seguida, o pacote chega √† interface Vlan, onde a primeira camada √© removida - o encapsulamento Vlan. </li><li>  O Linux ent√£o envia para a interface Tunnel, onde outra camada √© removida - encapsulamento do Tunnel. </li></ol><br>  A dificuldade √© passar tudo isso como par√¢metros para o tcpdump. <br>  Vamos come√ßar do final: existem pacotes IP limpos (sem cabe√ßalhos extras) de clientes com o encapsulamento de vlan e t√∫nel removido? <br><br> <code>tcpdump host &lt;ip &gt;</code> <br> <br>  N√£o, n√£o havia esses pacotes no servidor.  Portanto, o problema deve ser anterior.  Existem pacotes com apenas o encapsulamento Vlan removido? <br><br> <code>tcpdump ip[32:4]=0xx390x2xx</code> <br> <br>  0xx390x2xx √© o endere√ßo IP do cliente em formato hexadecimal. <br>  32: 4 - endere√ßo e comprimento do campo em que o IP do SCR √© gravado no pacote de t√∫nel. <br><br>  O endere√ßo do campo teve que ser selecionado por for√ßa bruta, j√° que a Internet grava cerca de 40, 44, 50, 54, mas n√£o havia endere√ßo IP.  Voc√™ tamb√©m pode observar um dos pacotes em hexadecimal (o par√¢metro -xx ou -XX em tcpdump) e calcular em qual endere√ßo o IP √© conhecido. <br><br>  Existe algum fragmento de pacote sem o encapsulamento Vlan e Tunnel removido? <br><br> <code>tcpdump ((ip[6:2] &gt; 0) and (not ip[6] = 64)) <br></code> <br>  Essa m√°gica nos mostrar√° todos os fragmentos, incluindo o √∫ltimo.  Provavelmente, o mesmo pode ser filtrado por IP, mas n√£o tentei, porque n√£o existem muitos desses pacotes, e os que eu precisava foram facilmente encontrados no fluxo geral.  Aqui est√£o elas: <br><br> <code>14:02:58.471063 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 1516: (tos 0x0, ttl 63, <b>id 53652, offset 0</b> , flags [+], proto IPIP (4), length 1500) <br> 11.11.11.11 &gt; 22.22.22.22: truncated-ip - 20 bytes missing! (tos 0x0, ttl 50, id 57750, offset 0, flags [DF], proto TCP (6), length 1500) <br> 33.33.33.33.33333 &gt; 44.44.44.44.80: Flags [.], seq 0:1448, ack 1, win 343, options [nop,nop,TS val 11660691 ecr 2998165860], length 1448 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 05dc d194 2000 3f09 d5fb 0a66 387d E.......?....f8} <br> 0x0020: 1x67 7899 4500 06xx e198 4000 3206 6xx4 .faEE.....@.2.m. <br> 0x0030: b291 x9xx x345 2541 83b9 0050 9740 0x04 .......A...P.@.. <br> 0x0040: 6444 4939 8010 0257 8c3c 0000 0101 080x dDI9...W.\...... <br> 0x0050: 00b1 ed93 b2b4 6964 xxd8 ffe1 006a 4578 ......ad.....j <b>Ex</b> <br> 0x0060: 6966 0000 4x4d 002a 0500 0008 0004 0100 <b>if</b> ..MM.*........ <br> <br> 14:02:58.471103 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 62: (tos 0x0, ttl 63, <b>id 53652, offset 1480</b> , flags [none], proto IPIP (4), length 40) <br> 11.11.11.11 &gt; 22.22.22.22: ip-proto-4 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 0028 d194 00b9 3f04 faf6 2x76 385x E..(....?....f8} <br> 0x0020: 1x76 6545 xxxx 1x11 2d2c 0c21 8016 8e43 .faE...D-,.!...C <br> 0x0030: x978 e91d x9b0 d608 0000 0000 0000 7c31 .x............|Q <br> 0x0040: 881d c4b6 0000 0000 0000 0000 0000 ..............</code> <br> <br>  Estes s√£o dois fragmentos de um pacote (o mesmo ID 53652) com uma fotografia (a palavra Exif √© vis√≠vel no primeiro pacote).  Devido ao fato de haver pacotes nesse n√≠vel, mas n√£o colados em lix√µes, o problema est√° claramente na montagem.  Finalmente, h√° evid√™ncias documentais disso! <br><br>  O decodificador de pacotes n√£o revelou nenhum problema que impediu a montagem.  Tentei aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hpd.gasmi.net</a> .  A princ√≠pio, ao tentar empinar algo l√°, o decodificador n√£o gosta do formato de pacote.  Aconteceu que havia dois octetos extras entre Srcmac e Ethertype (n√£o relacionados a informa√ß√µes de fragmentos).  Depois de remov√™-los, o decodificador funcionou.  No entanto, ele n√£o mostrou problemas. <br>  Diga o que quiser, exceto para aqueles muito Sysctl, mais nada foi encontrado.  Faltava encontrar uma maneira de identificar servidores problem√°ticos para entender a escala e decidir sobre outras a√ß√µes.  Rapidamente, encontrei o contador certo: <br><br> <code>netstat -s | grep "packet reassembles failed‚Äù</code> <br> <br>  Est√° no snmpd sob OID = 1.3.6.1.2.1.4.31.1.1.16.1 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ipSystemStatsReasmFails</a> ). <br><br><blockquote>  <i>"O n√∫mero de falhas detectadas pelo algoritmo de remontagem de IP (por qualquer motivo: tempo limite excedido, erros, etc.)."</i> </blockquote><br>  Entre o grupo de servidores em que o problema foi estudado, em dois esse contador aumentou mais rapidamente, em dois - mais lento e em dois n√£o aumentou.  Uma compara√ß√£o da din√¢mica desse contador com a din√¢mica dos erros HTTP no servidor Java revelou uma correla√ß√£o.  Ou seja, o contador pode ser configurado para monitoramento. <br><br>  Ter um indicador confi√°vel de problemas √© muito importante para que voc√™ possa determinar com precis√£o se a revers√£o do Sysctl ajuda, pois sabemos pela hist√≥ria anterior que isso n√£o est√° imediatamente claro no aplicativo.  Este indicador permitiria identificar todas as √°reas problem√°ticas da produ√ß√£o antes que os usu√°rios a descobrissem. <br>  Ap√≥s a revers√£o do Sysctl, os erros de monitoramento foram interrompidos, assim a causa dos problemas foi comprovada e o fato de a revers√£o ajudar. <br><br>  Revertemos as configura√ß√µes de fragmenta√ß√£o em outros servidores, onde um novo monitoramento pegou fogo e, em algum lugar, alocamos ainda mais mem√≥ria para os fragmentos do que antes, por padr√£o (isso era udp-statistics, cuja perda parcial n√£o era percept√≠vel no cen√°rio geral). <br><br><h4>  As perguntas mais importantes </h4><br>  Por que os pacotes se fragmentam em nosso balanceador L3?  A maioria dos pacotes que chegam dos usu√°rios aos balanceadores s√£o SYN e ACK.  Os tamanhos dessas sacolas s√£o pequenos.  Por√©m, como o compartilhamento de tais pacotes √© muito grande, n√£o observamos a presen√ßa de pacotes grandes que come√ßaram a se fragmentar. <br><br>  O motivo foi o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">script de</a> configura√ß√£o de advmss corrompidos em servidores com interfaces Vlan (havia muito poucos servidores com tr√°fego marcado na produ√ß√£o naquele momento).  O Advmss permite transmitir ao cliente informa√ß√µes de que os pacotes em nossa dire√ß√£o devem ser menores para que, ap√≥s colar os cabe√ßalhos do t√∫nel neles, eles n√£o precisem ser fragmentados. <br><br>  Por que a revers√£o do Sysctl n√£o ajudou, mas a reinicializa√ß√£o?  A revers√£o do Sysctl alterou a quantidade de mem√≥ria dispon√≠vel para colar pacotes.  Ao mesmo tempo, aparentemente, o pr√≥prio fato de excesso de mem√≥ria para fragmentos levou √† inibi√ß√£o das conex√µes, o que levou ao fato de que os fragmentos estavam atrasados ‚Äã‚Äãna fila por um longo tempo.  Ou seja, o processo est√° em loop. <br>  A refuta√ß√£o anulou a mem√≥ria e tudo estava em ordem. <br><br>  Voc√™ poderia fazer sem solu√ß√£o alternativa?  Sim, mas h√° um grande risco de deixar os usu√°rios sem vigil√¢ncia em caso de ataque.  Obviamente, o uso da solu√ß√£o alternativa, como resultado, levou a v√°rios problemas, incluindo a inibi√ß√£o de um dos servi√ßos pelos usu√°rios, mas, no entanto, acreditamos que as a√ß√µes foram justificadas. <br><br>  Muito obrigado a Andrei Timofeev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">atimofeyev</a> ) por ajudar na investiga√ß√£o e a Alexei Krenev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">devicex</a> ) pelo trabalho tit√¢nico de atualizar Centos e kernels nos servidores.  O processo, que neste caso teve que ser iniciado v√°rias vezes desde o in√≠cio, por causa do qual se arrastou por muitos meses. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt475540/">https://habr.com/ru/post/pt475540/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt475518/index.html">Empresa canadense desenvolveu material que o torna invis√≠vel</a></li>
<li><a href="../pt475520/index.html">Transi√ß√£o CSS da propriedade height de 0px para auto</a></li>
<li><a href="../pt475522/index.html">HP: sua unidade original n√£o √© totalmente original. Quem √© o culpado e o que fazer?</a></li>
<li><a href="../pt475536/index.html">Curr√≠culo para um tradutor freelancer</a></li>
<li><a href="../pt475538/index.html">Onde come√ßa a cria√ß√£o de um mercado. Parte um</a></li>
<li><a href="../pt475542/index.html">Como o marketing por email mudou desde 2013: 4 principais tend√™ncias e estat√≠sticas atuais</a></li>
<li><a href="../pt475544/index.html">Cat√°logos de produtos, servi√ßos e muito mais</a></li>
<li><a href="../pt475546/index.html">S√≠ndromes viciantes de TI</a></li>
<li><a href="../pt475548/index.html">Matchmaking chato, sem desequil√≠brio e filas: um guia pr√°tico</a></li>
<li><a href="../pt475550/index.html">Sistemas ac√∫sticos para salas abertas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>