<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>🛌🏽 👨🏿‍🤝‍👨🏽 👩🏿‍🎤 Cuidado com as vulnerabilidades que trazem soluções alternativas. Parte 1: FragmentSmack / SegmentSmack 🆘 🤒 👩🏽‍🔧</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Olá pessoal! Meu nome é Dmitry Samsonov, trabalho como administrador de sistemas líder em Odnoklassniki. Temos mais de 7 mil servidores físicos, 11 mi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Cuidado com as vulnerabilidades que trazem soluções alternativas. Parte 1: FragmentSmack / SegmentSmack</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/odnoklassniki/blog/475540/"><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/qy/tg/kcqytg4znoagvdqffwtwxhwzcjs.jpeg"></div><br><br>  Olá pessoal!  Meu nome é Dmitry Samsonov, trabalho como administrador de sistemas líder em Odnoklassniki.  Temos mais de 7 mil servidores físicos, 11 mil contêineres em nossa nuvem e 200 aplicativos, que em diferentes configurações formam 700 clusters diferentes.  A grande maioria dos servidores está executando o CentOS 7. <br>  Informações sobre a vulnerabilidade do FragmentSmack lançada em 14 de agosto de 2018 <br>  ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVE-2018-5391</a> ) e SegmentSmack ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">CVE-2018-5390</a> ).  Essas são vulnerabilidades com um vetor de ataque à rede e uma classificação bastante alta (7.5), que ameaça com negação de serviço (DoS) devido à exaustão de recursos (CPU).  Uma correção no kernel do FragmentSmack não foi proposta naquele momento; além disso, saiu muito depois da publicação de informações sobre a vulnerabilidade.  Para eliminar o SegmentSmack, foi proposto atualizar o kernel.  O pacote de atualização em si foi lançado no mesmo dia, tudo o que restava era instalá-lo. <br>  Não, não somos contra a atualização do kernel!  No entanto, existem nuances ... <br><a name="habracut"></a><br><h4>  Como atualizamos o núcleo do produto </h4><br>  Em geral, nada complicado: <br><ol><li>  Download de pacotes </li><li>  Instale-os em vários servidores (incluindo servidores que hospedam nossa nuvem); </li><li>  Certifique-se de que nada esteja quebrado; </li><li>  Verifique se todas as configurações padrão do kernel se aplicam sem erros; </li><li>  Aguarde alguns dias; </li><li>  Verifique o desempenho do servidor; </li><li>  Alterne a implantação de novos servidores para um novo kernel; </li><li>  Atualize todos os servidores por data centers (um data center por vez para minimizar o efeito para os usuários em caso de problemas); </li><li>  Reinicie todos os servidores. </li></ol><br>  Repita o procedimento para todos os ramos dos núcleos que temos.  No momento, isso é: <br><br><ul><li>  Stock CentOS 7 3.10 - para a maioria dos servidores comuns; </li><li>  Vanilla 4.19 é para a nossa <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">nuvem única,</a> porque precisamos de BFQ, BBR, etc; </li><li>  O Elrepo kernel-ml 5.2 é para <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">distribuidores altamente carregados</a> , porque o 4.19 costumava se comportar de maneira instável, e os recursos precisam dos mesmos. </li></ul><br>  Como você deve ter adivinhado, reiniciar milhares de servidores leva mais tempo.  Como nem todas as vulnerabilidades são críticas para todos os servidores, apenas reiniciamos aquelas diretamente acessíveis pela Internet.  Na nuvem, para não limitar a flexibilidade, não vinculamos contêineres acessíveis externamente a servidores individuais com um novo núcleo, mas reinicializamos todos os hosts sem exceção.  Felizmente, o procedimento é mais fácil lá do que nos servidores regulares.  Por exemplo, contêineres sem estado podem simplesmente mudar para outro servidor durante a reinicialização. <br><br>  No entanto, ainda há muito trabalho, e pode levar várias semanas e, em caso de problemas com a nova versão - até vários meses.  Os atacantes estão bem cientes disso, então o plano B é necessário. <br><br><h4>  FragmentSmack / SegmentSmack.  Solução alternativa </h4><br>  Felizmente, para algumas vulnerabilidades, esse plano "B" existe e é chamado de Solução alternativa.  Na maioria das vezes, essa é uma alteração nas configurações do kernel / aplicativo, que podem minimizar o possível efeito ou eliminar completamente a exploração de vulnerabilidades. <br><br>  No caso do FragmentSmack / SegmentSmack <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">, a</a> seguinte solução alternativa foi proposta: <br><br><blockquote>  “ <i>Você pode alterar os valores padrão de 4 MB e 3 MB em net.ipv4.ipfrag_high_thresh e net.ipv4.ipfrag_low_thresh (e seus análogos para ipv6 net.ipv6.ipfrag_high_thresh e net.ipv6.ipfrag_low_thresh) por 256 kB e 192 kB, respectivamente.</i>  <i>Os testes mostram uma queda ligeira a significativa no uso da CPU durante um ataque, dependendo do equipamento, configurações e condições.</i>  <i>No entanto, pode haver algum impacto no desempenho devido a ipfrag_high_thresh = 262144 bytes, pois apenas dois fragmentos de 64K podem caber na fila de reconstrução por vez.</i>  <i>Por exemplo, existe o risco de que aplicativos que trabalham com pacotes UDP grandes sejam interrompidos</i> . ” </blockquote><br>  Os próprios parâmetros <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">na documentação do kernel são</a> descritos a seguir: <br><br><blockquote><code>ipfrag_high_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments.</code> </blockquote> <br><p></p><blockquote> <code>ipfrag_low_thresh - LONG INTEGER <br> Maximum memory used to reassemble IP fragments before the kernel <br> begins to remove incomplete fragment queues to free up resources. <br> The kernel still accepts new fragments for defragmentation.</code> </blockquote> <br>  Não temos UDP grande em serviços de produção.  Não há tráfego fragmentado na LAN; há, mas não significativo, tráfego na WAN.  Nada é um mau presságio - você pode rolar a solução alternativa! <br><br><h4>  FragmentSmack / SegmentSmack.  Primeiro sangue </h4><br>  O primeiro problema que encontramos foi que os contêineres na nuvem às vezes aplicavam apenas parcialmente as novas configurações (apenas ipfrag_low_thresh) e às vezes eles não usavam de jeito nenhum - eles apenas travavam no início.  Não foi possível reproduzir o problema de forma estável (manualmente, todas as configurações foram aplicadas sem dificuldades).  Entender por que o contêiner cai no início também não é tão simples: nenhum erro foi encontrado.  Uma coisa era certa: reverter as configurações resolve o problema de derrubar contêineres. <br><br>  Por que não é suficiente usar o Sysctl no host?  O contêiner vive em seu Namespace de rede dedicado, portanto, pelo menos <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">parte dos parâmetros Sysctl da rede</a> no contêiner podem diferir do host. <br><br>  Como exatamente as configurações de Sysctl no contêiner se aplicam?  Como temos contêineres sem privilégios, a alteração de qualquer configuração do Sysctl entrando no próprio contêiner falhará - simplesmente não haverá direitos suficientes.  Na época, nossa nuvem usava o Docker (agora <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Podman</a> ) para lançar contêineres.  A janela de encaixe via API passou os parâmetros do novo contêiner, incluindo as configurações necessárias do Sysctl. <br>  No decorrer da enumeração das versões, descobriu-se que a API do Docker não gerou todos os erros (pelo menos na versão 1.10).  Ao tentar iniciar o contêiner através do "docker run", finalmente vimos pelo menos algo: <br><br> <code>write /proc/sys/net/ipv4/ipfrag_high_thresh: invalid argument docker: Error response from daemon: Cannot start container &lt;...&gt;: [9] System error: could not synchronise with container process.</code> <br> <br>  O valor do parâmetro não é válido.  Mas porque?  E por que não é válido apenas às vezes?  Aconteceu que o Docker não garantiu a ordem na qual os parâmetros Sysctl foram usados ​​(a versão mais recente testada foi a 1.13.1); portanto, às vezes o ipfrag_high_thresh tentava definir-se para 256K quando o ipfrag_low_thresh ainda era de 3M, ou seja, o limite superior era mais baixo que o inferior, o que causava um erro. <br><br>  Naquela época, já usamos nosso próprio mecanismo para reconfigurar o contêiner após iniciar (congelar o contêiner através do <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">cgroup freezer</a> e executar comandos no espaço de nomes do contêiner via <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ip netns</a> ) e também adicionamos parâmetros Sysctl a esta parte.  O problema foi resolvido. <br><br><h4>  FragmentSmack / SegmentSmack.  Primeiro sangue 2 </h4><br>  Antes de sabermos como usar a solução alternativa na nuvem, começaram a chegar as primeiras queixas raras dos usuários.  Nesse momento, várias semanas se passaram desde o início da solução alternativa nos primeiros servidores.  A investigação inicial mostrou que foram recebidas reclamações sobre serviços individuais, e nem todos os servidores desses serviços.  O problema recuperou um caráter extremamente vago. <br><br>  Antes de tudo, é claro, tentamos reverter as configurações do Sysctl, mas isso não deu nenhum efeito.  Várias manipulações com as configurações do servidor e do aplicativo também não ajudaram.  Reinicialização ajudou.  A reinicialização para Linux é tão antinatural quanto era uma condição normal para trabalhar com o Windows nos velhos tempos.  No entanto, isso ajudou e escrevemos tudo para uma “falha no kernel” ao aplicar as novas configurações no Sysctl.  Quão frívolo era ... <br><br>  Três semanas depois, o problema voltou a ocorrer.  A configuração desses servidores era bastante simples: Nginx no modo proxy / balancer.  O tráfego é um pouco.  Novo introdutório: o número de erros 504 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">Gateway Timeout</a> ) está aumentando todos os dias nos clientes.  O gráfico mostra o número de 504 erros por dia para este serviço: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xk/hk/rj/xkhkrjedsakcdrx6m_z8hgcgjsw.png"></div><br><br>  Todos os erros são sobre o mesmo back-end - sobre o que está na nuvem.  O gráfico do consumo de memória para fragmentos de pacotes nesse back-end foi o seguinte: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/jy/fb/gt/jyfbgtoqrbqwv6cird2zawvatza.png"></div><br><br>  Essa é uma das manifestações mais marcantes do problema nos gráficos do sistema operacional.  Na nuvem, ao mesmo tempo, outro problema de rede foi corrigido com as configurações de QoS (Controle de Tráfego).  No gráfico do consumo de memória para fragmentos de pacotes, parecia exatamente o mesmo: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/09/nf/4r/09nf4rxogxv9xppeumwujvmcr3m.png"></div><br><br>  A suposição era simples: se eles parecem iguais nos gráficos, têm o mesmo motivo.  Além disso, quaisquer problemas com esse tipo de memória são extremamente raros. <br><br>  A essência do problema corrigido foi que usamos o sheduler fq packet com configurações padrão em QoS.  Por padrão, para uma conexão, ele permite adicionar 100 pacotes à fila e algumas conexões em uma situação de falta de canal começaram a entupir a fila com falha.  Nesse caso, os pacotes caem.  Nas estatísticas tc (tc -s qdisc), isso pode ser visto da seguinte maneira: <br><br> <code>qdisc fq 2c6c: parent 1:2c6c limit 10000p flow_limit 100p buckets 1024 orphan_mask 1023 quantum 3028 initial_quantum 15140 refill_delay 40.0ms <br> Sent 454701676345 bytes 491683359 pkt (dropped 464545, overlimits 0 requeues 0) <br> backlog 0b 0p requeues 0 <br> 1024 flows (1021 inactive, 0 throttled) <br> 0 gc, 0 highprio, 0 throttled, 464545 flows_plimit</code> <br> <br>  "464545 flows_plimit" são os pacotes descartados devido a exceder o limite da fila de uma conexão e "464545 descartado" é a soma de todos os pacotes descartados deste sheduler.  Depois de aumentar o comprimento da fila para 1 mil e reiniciar os contêineres, o problema deixou de aparecer.  Você pode sentar em uma cadeira e tomar um smoothie. <br><br><h4>  FragmentSmack / SegmentSmack.  Último sangue </h4><br>  Primeiro, alguns meses após o anúncio de vulnerabilidades no kernel, finalmente uma correção para o FragmentSmack apareceu (lembro que, com o anúncio em agosto, uma correção foi lançada apenas para o SegmentSmack), que nos deu a chance de abandonar a solução alternativa, o que causou muitos problemas.  Alguns dos servidores durante esse período já conseguimos transferir para um novo kernel e agora tivemos que começar do início.  Por que atualizamos o kernel sem esperar pela correção do FragmentSmack?  O fato é que o processo de proteção contra essas vulnerabilidades coincidiu (e se fundiu) com o processo de atualização do próprio CentOS (que leva ainda mais tempo do que atualizar apenas o kernel).  Além disso, o SegmentSmack é uma vulnerabilidade mais perigosa, e uma correção para ela apareceu imediatamente, de modo que o argumento era o mesmo.  No entanto, não pudemos simplesmente atualizar o kernel no CentOS, porque a vulnerabilidade FragmentSmack, que apareceu durante o CentOS 7.5, foi corrigida apenas na versão 7.6, por isso tivemos que parar a atualização para 7.5 e começar tudo de novo com a atualização para 7.6.  E assim é. <br><br>  Em segundo lugar, as reclamações raras dos usuários sobre problemas retornaram para nós.  Agora já sabemos com certeza que todos eles estão conectados ao download de arquivos de clientes para alguns de nossos servidores.  E através desses servidores, houve um número muito pequeno de uploads da massa total. <br><br>  Como lembramos da história acima, a reversão do Sysctl não ajudou.  A reinicialização ajudou, mas temporariamente. <br>  As suspeitas com o Sysctl não foram levantadas, mas desta vez foi necessário coletar o máximo de informações possível.  Além disso, havia uma extrema falta de capacidade de reproduzir o problema com o upload do cliente para examinar com mais precisão o que estava acontecendo. <br><br>  A análise de todas as estatísticas e logs disponíveis não nos aproximou da compreensão do que estava acontecendo.  Havia uma falta aguda da capacidade de reproduzir o problema para "sentir" uma conexão específica.  Por fim, os desenvolvedores da versão especial do aplicativo conseguiram obter uma reprodução estável de problemas no dispositivo de teste quando conectado via Wi-Fi.  Este foi um avanço na investigação.  O cliente conectado ao Nginx, procurou proxy para o back-end, que era nosso aplicativo Java. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xw/g8/4a/xwg84aziibg7aaglsv6hdc2vyn8.png"></div><br><br>  A caixa de diálogo com problemas foi a seguinte (corrigida no lado do proxy Nginx): <br><br><ol><li>  Cliente: solicitação de informações sobre o download de um arquivo. </li><li>  Servidor Java: resposta. </li><li>  Cliente: POST com arquivo. </li><li>  Servidor Java: erro. </li></ol><br>  Ao mesmo tempo, o servidor Java grava no log que 0 bytes de dados foram recebidos do cliente e no proxy Nginx que a solicitação levou mais de 30 segundos (30 segundos é o tempo limite do aplicativo cliente).  Por que tempo limite e por que 0 bytes?  Do ponto de vista do HTTP, tudo funciona como deveria, mas o POST com o arquivo parece desaparecer da rede.  E desaparece entre o cliente e o Nginx.  É hora de se armar com o Tcpdump!  Mas primeiro você precisa entender a configuração de rede.  O proxy nginx está por trás do balanceador <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">N3ware</a> L3.  O encapsulamento é usado para entregar pacotes do balanceador L3 para o servidor, que adiciona seus cabeçalhos aos pacotes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hs/ge/mf/hsgemfebrlkpjzvtz-cdrghudxu.png"></div><br><br>  Ao mesmo tempo, a rede chega a esse servidor na forma de tráfego marcado por Vlan, que também adiciona seus campos aos pacotes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-a/ns/ap/-ansap0a5oyjsnqrf0onvsxg0tw.png"></div><br><br>  E esse tráfego pode ser fragmentado (a porcentagem muito pequena de tráfego fragmentado de entrada de que falamos ao avaliar os riscos da solução alternativa), o que também altera o conteúdo dos cabeçalhos: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gd/yx/48/gdyx48_bc15aj2xmhn5kb0_omci.png"></div><br><br>  Mais uma vez: os pacotes são encapsulados por uma tag Vlan, encapsulados por um túnel, fragmentados.  Para entender melhor como isso acontece, vamos rastrear a rota do pacote do cliente para o proxy Nginx. <br><br><ol><li>  O pacote chega ao balanceador L3.  Para o roteamento correto dentro do data center, o pacote é encapsulado no túnel e enviado para a placa de rede. </li><li>  Como os cabeçalhos de pacotes + túneis não se encaixam na MTU, o pacote é cortado em fragmentos e enviado à rede. </li><li>  O switch após o balanceador L3 ao receber o pacote adiciona uma tag Vlan e a envia ainda mais. </li><li>  O switch antes do proxy Nginx vê (de acordo com as configurações da porta) que o servidor está esperando um pacote encapsulado em Vlan, para que ele o envie como está, sem remover a tag Vlan. </li><li>  O Linux recebe fragmentos de pacotes individuais e os cola em um pacote grande. </li><li>  Em seguida, o pacote chega à interface Vlan, onde a primeira camada é removida - o encapsulamento Vlan. </li><li>  O Linux então envia para a interface Tunnel, onde outra camada é removida - encapsulamento do Tunnel. </li></ol><br>  A dificuldade é passar tudo isso como parâmetros para o tcpdump. <br>  Vamos começar do final: existem pacotes IP limpos (sem cabeçalhos extras) de clientes com o encapsulamento de vlan e túnel removido? <br><br> <code>tcpdump host &lt;ip &gt;</code> <br> <br>  Não, não havia esses pacotes no servidor.  Portanto, o problema deve ser anterior.  Existem pacotes com apenas o encapsulamento Vlan removido? <br><br> <code>tcpdump ip[32:4]=0xx390x2xx</code> <br> <br>  0xx390x2xx é o endereço IP do cliente em formato hexadecimal. <br>  32: 4 - endereço e comprimento do campo em que o IP do SCR é gravado no pacote de túnel. <br><br>  O endereço do campo teve que ser selecionado por força bruta, já que a Internet grava cerca de 40, 44, 50, 54, mas não havia endereço IP.  Você também pode observar um dos pacotes em hexadecimal (o parâmetro -xx ou -XX em tcpdump) e calcular em qual endereço o IP é conhecido. <br><br>  Existe algum fragmento de pacote sem o encapsulamento Vlan e Tunnel removido? <br><br> <code>tcpdump ((ip[6:2] &gt; 0) and (not ip[6] = 64)) <br></code> <br>  Essa mágica nos mostrará todos os fragmentos, incluindo o último.  Provavelmente, o mesmo pode ser filtrado por IP, mas não tentei, porque não existem muitos desses pacotes, e os que eu precisava foram facilmente encontrados no fluxo geral.  Aqui estão elas: <br><br> <code>14:02:58.471063 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 1516: (tos 0x0, ttl 63, <b>id 53652, offset 0</b> , flags [+], proto IPIP (4), length 1500) <br> 11.11.11.11 &gt; 22.22.22.22: truncated-ip - 20 bytes missing! (tos 0x0, ttl 50, id 57750, offset 0, flags [DF], proto TCP (6), length 1500) <br> 33.33.33.33.33333 &gt; 44.44.44.44.80: Flags [.], seq 0:1448, ack 1, win 343, options [nop,nop,TS val 11660691 ecr 2998165860], length 1448 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 05dc d194 2000 3f09 d5fb 0a66 387d E.......?....f8} <br> 0x0020: 1x67 7899 4500 06xx e198 4000 3206 6xx4 .faEE.....@.2.m. <br> 0x0030: b291 x9xx x345 2541 83b9 0050 9740 0x04 .......A...P.@.. <br> 0x0040: 6444 4939 8010 0257 8c3c 0000 0101 080x dDI9...W.\...... <br> 0x0050: 00b1 ed93 b2b4 6964 xxd8 ffe1 006a 4578 ......ad.....j <b>Ex</b> <br> 0x0060: 6966 0000 4x4d 002a 0500 0008 0004 0100 <b>if</b> ..MM.*........ <br> <br> 14:02:58.471103 In 00:de:ff:1a:94:11 ethertype IPv4 (0x0800), length 62: (tos 0x0, ttl 63, <b>id 53652, offset 1480</b> , flags [none], proto IPIP (4), length 40) <br> 11.11.11.11 &gt; 22.22.22.22: ip-proto-4 <br> 0x0000: 0000 0001 0006 00de fb1a 9441 0000 0800 ...........A.... <br> 0x0010: 4500 0028 d194 00b9 3f04 faf6 2x76 385x E..(....?....f8} <br> 0x0020: 1x76 6545 xxxx 1x11 2d2c 0c21 8016 8e43 .faE...D-,.!...C <br> 0x0030: x978 e91d x9b0 d608 0000 0000 0000 7c31 .x............|Q <br> 0x0040: 881d c4b6 0000 0000 0000 0000 0000 ..............</code> <br> <br>  Estes são dois fragmentos de um pacote (o mesmo ID 53652) com uma fotografia (a palavra Exif é visível no primeiro pacote).  Devido ao fato de haver pacotes nesse nível, mas não colados em lixões, o problema está claramente na montagem.  Finalmente, há evidências documentais disso! <br><br>  O decodificador de pacotes não revelou nenhum problema que impediu a montagem.  Tentei aqui: <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">hpd.gasmi.net</a> .  A princípio, ao tentar empinar algo lá, o decodificador não gosta do formato de pacote.  Aconteceu que havia dois octetos extras entre Srcmac e Ethertype (não relacionados a informações de fragmentos).  Depois de removê-los, o decodificador funcionou.  No entanto, ele não mostrou problemas. <br>  Diga o que quiser, exceto para aqueles muito Sysctl, mais nada foi encontrado.  Faltava encontrar uma maneira de identificar servidores problemáticos para entender a escala e decidir sobre outras ações.  Rapidamente, encontrei o contador certo: <br><br> <code>netstat -s | grep "packet reassembles failed”</code> <br> <br>  Está no snmpd sob OID = 1.3.6.1.2.1.4.31.1.1.16.1 ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">ipSystemStatsReasmFails</a> ). <br><br><blockquote>  <i>"O número de falhas detectadas pelo algoritmo de remontagem de IP (por qualquer motivo: tempo limite excedido, erros, etc.)."</i> </blockquote><br>  Entre o grupo de servidores em que o problema foi estudado, em dois esse contador aumentou mais rapidamente, em dois - mais lento e em dois não aumentou.  Uma comparação da dinâmica desse contador com a dinâmica dos erros HTTP no servidor Java revelou uma correlação.  Ou seja, o contador pode ser configurado para monitoramento. <br><br>  Ter um indicador confiável de problemas é muito importante para que você possa determinar com precisão se a reversão do Sysctl ajuda, pois sabemos pela história anterior que isso não está imediatamente claro no aplicativo.  Este indicador permitiria identificar todas as áreas problemáticas da produção antes que os usuários a descobrissem. <br>  Após a reversão do Sysctl, os erros de monitoramento foram interrompidos, assim a causa dos problemas foi comprovada e o fato de a reversão ajudar. <br><br>  Revertemos as configurações de fragmentação em outros servidores, onde um novo monitoramento pegou fogo e, em algum lugar, alocamos ainda mais memória para os fragmentos do que antes, por padrão (isso era udp-statistics, cuja perda parcial não era perceptível no cenário geral). <br><br><h4>  As perguntas mais importantes </h4><br>  Por que os pacotes se fragmentam em nosso balanceador L3?  A maioria dos pacotes que chegam dos usuários aos balanceadores são SYN e ACK.  Os tamanhos dessas sacolas são pequenos.  Porém, como o compartilhamento de tais pacotes é muito grande, não observamos a presença de pacotes grandes que começaram a se fragmentar. <br><br>  O motivo foi o <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=">script de</a> configuração de advmss corrompidos em servidores com interfaces Vlan (havia muito poucos servidores com tráfego marcado na produção naquele momento).  O Advmss permite transmitir ao cliente informações de que os pacotes em nossa direção devem ser menores para que, após colar os cabeçalhos do túnel neles, eles não precisem ser fragmentados. <br><br>  Por que a reversão do Sysctl não ajudou, mas a reinicialização?  A reversão do Sysctl alterou a quantidade de memória disponível para colar pacotes.  Ao mesmo tempo, aparentemente, o próprio fato de excesso de memória para fragmentos levou à inibição das conexões, o que levou ao fato de que os fragmentos estavam atrasados ​​na fila por um longo tempo.  Ou seja, o processo está em loop. <br>  A refutação anulou a memória e tudo estava em ordem. <br><br>  Você poderia fazer sem solução alternativa?  Sim, mas há um grande risco de deixar os usuários sem vigilância em caso de ataque.  Obviamente, o uso da solução alternativa, como resultado, levou a vários problemas, incluindo a inibição de um dos serviços pelos usuários, mas, no entanto, acreditamos que as ações foram justificadas. <br><br>  Muito obrigado a Andrei Timofeev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">atimofeyev</a> ) por ajudar na investigação e a Alexei Krenev ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=pt&amp;u=" class="user_link">devicex</a> ) pelo trabalho titânico de atualizar Centos e kernels nos servidores.  O processo, que neste caso teve que ser iniciado várias vezes desde o início, por causa do qual se arrastou por muitos meses. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/pt475540/">https://habr.com/ru/post/pt475540/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../pt475518/index.html">Empresa canadense desenvolveu material que o torna invisível</a></li>
<li><a href="../pt475520/index.html">Transição CSS da propriedade height de 0px para auto</a></li>
<li><a href="../pt475522/index.html">HP: sua unidade original não é totalmente original. Quem é o culpado e o que fazer?</a></li>
<li><a href="../pt475536/index.html">Currículo para um tradutor freelancer</a></li>
<li><a href="../pt475538/index.html">Onde começa a criação de um mercado. Parte um</a></li>
<li><a href="../pt475542/index.html">Como o marketing por email mudou desde 2013: 4 principais tendências e estatísticas atuais</a></li>
<li><a href="../pt475544/index.html">Catálogos de produtos, serviços e muito mais</a></li>
<li><a href="../pt475546/index.html">Síndromes viciantes de TI</a></li>
<li><a href="../pt475548/index.html">Matchmaking chato, sem desequilíbrio e filas: um guia prático</a></li>
<li><a href="../pt475550/index.html">Sistemas acústicos para salas abertas</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>