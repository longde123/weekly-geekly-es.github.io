<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë®üèº‚Äçüéì üßëüèæ‚Äçü§ù‚Äçüßëüèª ‚ù£Ô∏è Intelligence artificielle √† usage g√©n√©ral. ST, √©tat actuel, perspectives ‚öõÔ∏è üññüèΩ üòÜ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="De nos jours, les mots ¬´intelligence artificielle¬ª signifient beaucoup de syst√®mes diff√©rents - d'un r√©seau neuronal pour la reconnaissance d'image √† ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Intelligence artificielle √† usage g√©n√©ral. ST, √©tat actuel, perspectives</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/post/468379/">  De nos jours, les mots ¬´intelligence artificielle¬ª signifient beaucoup de syst√®mes diff√©rents - d'un r√©seau neuronal pour la reconnaissance d'image √† un bot pour jouer √† Quake.  Wikipedia donne une merveilleuse d√©finition de l'IA - c'est "la propri√©t√© des syst√®mes intelligents pour effectuer des fonctions cr√©atives qui sont traditionnellement consid√©r√©es comme la pr√©rogative de l'homme."  Autrement dit, cela ressort clairement de la d√©finition - si une certaine fonction a √©t√© automatis√©e avec succ√®s, elle cesse d'√™tre consid√©r√©e comme une intelligence artificielle. <br><br>  Cependant, lorsque la t√¢che de ¬´cr√©er l'intelligence artificielle¬ª a √©t√© d√©finie pour la premi√®re fois, l'IA signifiait quelque chose de diff√©rent.  Cet objectif s'appelle d√©sormais Strong AI ou AI √† usage g√©n√©ral. <br><a name="habracut"></a><br><h2>  √ânonc√© du probl√®me </h2><br>  Il existe maintenant deux formulations bien connues du probl√®me.  Le premier est Strong AI.  La seconde est une IA √† usage g√©n√©ral (alias Artifical General Intelligence, en abr√©g√© AGI). <br>  Upd.  Dans les commentaires, ils me disent que cette diff√©rence est plus probable au niveau de la langue.  En russe, le mot ¬´intelligence¬ª ne signifie pas exactement ce que le mot ¬´intelligence¬ª en anglais <br><br>  <b>Une IA forte</b> est une IA hypoth√©tique qui pourrait faire tout ce qu'une personne pourrait faire.  Il est g√©n√©ralement mentionn√© qu'il doit r√©ussir le test de Turing dans le cadre initial (hmm, les gens le r√©ussissent-ils?), √ätre conscient de lui-m√™me en tant que personne distincte et √™tre capable d'atteindre ses objectifs. <br><br>  Autrement dit, c'est quelque chose comme une personne artificielle.  √Ä mon avis, l'utilit√© d'une telle IA est principalement la recherche, car les d√©finitions d'une IA forte ne disent nulle part quels seront ses objectifs. <br><br>  <b>L'AGI ou l'IA polyvalente</b> est une ¬´machine √† r√©sultats¬ª.  Elle re√ßoit un certain objectif fix√© √† l'entr√©e - et donne quelques actions de contr√¥le sur les moteurs / lasers / carte r√©seau / moniteurs.  Et l'objectif est atteint.  Dans le m√™me temps, AGI ne poss√®de initialement aucune connaissance de l'environnement, mais uniquement des capteurs, des actionneurs et du canal par lequel elle d√©finit ses objectifs.  Le syst√®me de gestion sera consid√©r√© comme une AGI s'il peut atteindre des objectifs dans n'importe quel environnement.  Nous lui avons demand√© de conduire une voiture et d'√©viter les accidents - elle s'en chargera.  Nous lui avons mis le contr√¥le d'un r√©acteur nucl√©aire pour qu'il y ait plus d'√©nergie, mais n'explose pas - elle peut le g√©rer.  Nous donnerons une bo√Æte aux lettres et demanderons de vendre des aspirateurs - nous nous en occuperons √©galement.  AGI est un r√©solveur de ¬´probl√®mes inverses¬ª.  V√©rifier combien d'aspirateurs sont vendus est une question simple.  Mais comprendre comment convaincre une personne d'acheter cet aspirateur est d√©j√† une t√¢che pour l'intellect. <br><br>  Dans cet article, je vais parler d'AGI.  Pas de tests de Turing, pas de conscience de soi, pas de personnalit√©s artificielles - IA exceptionnellement pragmatique et op√©rateurs non moins pragmatiques. <br><br><h2>  Situation actuelle </h2><br>  Il existe maintenant une classe de syst√®mes tels que l'apprentissage par renforcement ou l'apprentissage renforc√©.  C'est quelque chose comme AGI, mais sans polyvalence.  Ils sont capables d'apprendre et, par cons√©quent, d'atteindre des objectifs dans une vari√©t√© d'environnements.  Mais ils sont encore tr√®s loin d'atteindre des objectifs dans n'importe quel environnement. <br><br>  En g√©n√©ral, comment sont organis√©s les syst√®mes d'apprentissage par renforcement et quels sont leurs probl√®mes? <br><br><img src="https://habrastorage.org/webt/oj/3-/dl/oj3-dl6vkgtbhaxeilj7rkl0jxe.png"><br><br>  Tout RL est organis√© comme ceci.  Il existe un syst√®me de contr√¥le, certains signaux sur la r√©alit√© environnante y p√©n√®trent √† travers les capteurs (√©tat) et √† travers les organes directeurs (actions) il agit sur la r√©alit√© environnante.  La r√©compense est un signal de renforcement.  Dans les syst√®mes RL, le renforcement est form√© de l'ext√©rieur de l'unit√© de contr√¥le et indique dans quelle mesure l'IA parvient √† atteindre l'objectif.  Combien d'aspirateurs vendus √† la derni√®re minute, par exemple. <br>  Ensuite, une table est form√©e de quelque chose comme √ßa (je l'appellerai la table SAR): <br><br><img src="https://habrastorage.org/webt/lp/oo/ek/lpooekbdpwktkkmvsilzsytsnbo.png"><br><br>  L'axe du temps est dirig√© vers le bas.  Le tableau montre tout ce que l'IA a fait, tout ce qu'il a vu et tous les signaux de renforcement.  Habituellement, pour que RL fasse quelque chose de significatif, il doit d'abord faire des mouvements al√©atoires pendant un certain temps, ou regarder les mouvements de quelqu'un d'autre.  En g√©n√©ral, RL d√©marre lorsqu'il y a d√©j√† au moins quelques lignes dans la table SAR. <br>  Que se passe-t-il ensuite? <br><br><h3>  Sarsa </h3><br>  La forme la plus simple d'apprentissage par renforcement. <br><br>  Nous prenons une sorte de mod√®le d'apprentissage automatique et, en utilisant une combinaison de S et A (√©tat et action), nous pr√©disons le R total pour les prochains cycles d'horloge.  Par exemple, nous verrons que (sur la base du tableau ci-dessus) si vous dites √† une femme "soyez un homme, achetez un aspirateur!", Alors la r√©compense sera faible, et si vous dites la m√™me chose √† un homme, alors √©lev√©e. <br><br>  Quels mod√®les sp√©cifiques peuvent √™tre utilis√©s - je d√©crirai plus tard, pour l'instant je dirai seulement qu'il ne s'agit pas uniquement de r√©seaux de neurones.  Vous pouvez utiliser des arbres de d√©cision ou m√™me d√©finir une fonction sous forme de tableau. <br><br>  Et puis ce qui suit se produit.  AI re√ßoit un autre message ou un lien vers un autre client.  Toutes les donn√©es client sont entr√©es dans l'IA de l'ext√©rieur - nous consid√©rerons la base de clients et le compteur de messages comme faisant partie du syst√®me de capteurs.  Autrement dit, il reste √† affecter un A (action) et √† attendre des renforts.  L'IA prend toutes les actions possibles et √† son tour pr√©dit (en utilisant le m√™me mod√®le d'apprentissage automatique) - que se passera-t-il si je fais cela?  Et si c'est le cas?  Et quel renforcement y aura-t-il?  Et puis RL effectue l'action pour laquelle la r√©compense maximale est attendue. <br><br>  J'ai introduit un syst√®me si simple et maladroit dans l'un de mes jeux.  SARSA embauche des unit√©s dans le jeu et s'adapte en cas de changement des r√®gles du jeu. <br><br>  De plus, dans tous les types de formation renforc√©e, il y a une remise de r√©compenses et un dilemme d'exploration / exploitation. <br><br>  L'actualisation des r√©compenses est une telle approche lorsque RL essaie de maximiser non pas le montant de la r√©compense pour les N mouvements suivants, mais le montant pond√©r√© selon le principe "100 roubles est maintenant meilleur que 110 en un an".  Par exemple, si le facteur d'actualisation est de 0,9 et l'horizon de planification est de 3, alors nous formerons le mod√®le non pas sur le total R pour les 3 prochains cycles d'horloge, mais sur R1 * 0,9 + R2 * 0,81 + R3 * 0,729.  Pourquoi est-ce n√©cessaire?  Ensuite, cette IA, cr√©ant un profit quelque part l√†-bas √† l'infini, nous n'en avons pas besoin.  Nous avons besoin d'une IA qui g√©n√®re un profit ici et maintenant. <br>  Explorez / exploitez le dilemme.  Si RL fait ce que son mod√®le consid√®re comme optimal, il ne saura jamais s'il y avait de meilleures strat√©gies.  L'exploit est une strat√©gie dans laquelle RL fait ce qui promet un maximum de r√©compenses.  Explore est une strat√©gie dans laquelle RL fait quelque chose pour explorer l'environnement √† la recherche de meilleures strat√©gies.  Comment mettre en ≈ìuvre une intelligence efficace?  Par exemple, vous pouvez effectuer une action al√©atoire toutes les quelques mesures.  Ou vous pouvez cr√©er non pas un mod√®le pr√©dictif, mais plusieurs avec des param√®tres l√©g√®rement diff√©rents.  Ils produiront des r√©sultats diff√©rents.  Plus la diff√©rence est grande, plus le degr√© d'incertitude de cette option est √©lev√©.  Vous pouvez effectuer l'action de sorte qu'elle ait la valeur maximale: M + k * std, o√π M est la pr√©vision moyenne de tous les mod√®les, std est l'√©cart-type des pr√©visions et k est le coefficient de curiosit√©. <br><br>  <b>Quels sont les inconv√©nients?</b> <br><br>  Disons que nous avons des options.  Allez au but (qui est √† 10 km de nous, et la route est bonne) en voiture ou √† pied.  Et puis, apr√®s ce choix, nous avons des options - bougez prudemment ou essayez de vous √©craser dans chaque pilier. <br><br>  La personne dira imm√©diatement qu'il est g√©n√©ralement pr√©f√©rable de conduire une voiture et de se comporter avec prudence. <br><br>  Mais SARSA ... Il se penchera sur ce √† quoi la d√©cision d'aller en voiture a men√© auparavant.  Mais cela a conduit √† cela.  Au stade de la premi√®re s√©rie de statistiques, l'IA a conduit imprudemment et s'est √©cras√© quelque part dans la moiti√© des cas.  Oui, il sait bien conduire.  Mais quand il choisit d'aller en voiture, il ne sait pas ce qu'il choisira au prochain coup.  Il a des statistiques - puis dans la moiti√© des cas, il a choisi l'option appropri√©e, et dans la moiti√© - suicidaire.  Par cons√©quent, en moyenne, il vaut mieux marcher. <br><br>  SARSA estime que l'agent suivra la m√™me strat√©gie qui a √©t√© utilis√©e pour remplir la table.  Et agit sur cette base.  Mais que se passe-t-il si nous supposons le contraire - que l'agent suivra la meilleure strat√©gie dans les prochains mouvements? <br><br><h3>  Q-learning </h3><br>  Ce mod√®le calcule pour chaque √©tat la r√©compense totale maximale pouvant en √™tre obtenue.  Et il l'√©crit dans une colonne sp√©ciale Q. Autrement dit, si √† partir de l'√©tat S, vous pouvez obtenir 2 points ou 1, selon le mouvement, alors Q (S) sera √©gal √† 2 (avec une profondeur de pr√©diction de 1).  Quelle r√©compense peut √™tre obtenue de l'√©tat S, nous apprenons du mod√®le pr√©dictif Y (S, A).  (√âtat S, action A). <br><br>  Ensuite, nous cr√©ons un mod√®le pr√©dictif Q (S, A) - c'est-√†-dire √† quel √©tat Q ira si nous effectuons l'action A √† partir de S. Et cr√©ons la colonne suivante dans le tableau - Q2.  Autrement dit, le Q maximum qui peut √™tre obtenu √† partir de l'√©tat S (nous trions tous les A possibles). <br><br>  Ensuite, nous cr√©ons un mod√®le de r√©gression Q3 (S, A) - c'est-√†-dire √† l'√©tat avec lequel Q2 nous irons si nous effectuons l'action A √† partir de S. <br><br>  Et ainsi de suite.  Ainsi, nous pouvons atteindre une profondeur de pr√©vision illimit√©e. <br><br><img src="https://habrastorage.org/webt/ie/e_/gn/iee_gnm9ldz5uiv0dmyj4jni470.jpeg"><br><br>  Dans l'image, R est le renfort. <br><br>  Et puis √† chaque mouvement, nous s√©lectionnons l'action qui promet le plus grand Qn.  Si nous appliquions cet algorithme aux √©checs, nous obtiendrions quelque chose comme un minimax id√©al.  Quelque chose de presque √©quivalent √† des erreurs de calcul vers de grandes profondeurs. <br><br>  Un exemple courant de comportement d'apprentissage q.  Le chasseur a une lance, et il l'accompagne √† l'ours, de sa propre initiative.  Il sait que la grande majorit√© de ses futurs mouvements ont une tr√®s grande r√©compense n√©gative (il y a beaucoup plus de fa√ßons de perdre que de gagner), il sait qu'il y a des mouvements avec une r√©compense positive.  Le chasseur pense qu'√† l'avenir il fera les meilleurs mouvements (et on ne sait pas lesquels comme dans SARSA), et s'il fait les meilleurs mouvements, il vaincra l'ours.  Autrement dit, pour aller √† l'ours, il lui suffit de pouvoir fabriquer tous les √©l√©ments n√©cessaires √† la chasse, mais il n'est pas n√©cessaire d'avoir une exp√©rience de r√©ussite imm√©diate. <br><br>  Si le chasseur agissait dans le style SARSA, il supposerait que ses actions √† l'avenir seraient √† peu pr√®s les m√™mes qu'avant (malgr√© le fait qu'il a maintenant un bagage de connaissances diff√©rent), et il n'irait √† l'ours que s'il √©tait d√©j√† all√© √† et il a gagn√©, par exemple, dans&gt; 50% des cas (enfin, ou si d'autres chasseurs ont gagn√© dans plus de la moiti√© des cas, s'il apprend de leur exp√©rience). <br><br>  <b>Quels sont les inconv√©nients?</b> <br><br><ol><li>  Le mod√®le ne fait pas face √† la r√©alit√© changeante.  Si toute notre vie nous avons √©t√© r√©compens√©s pour avoir appuy√© sur le bouton rouge, et maintenant ils nous punissent, et aucun changement visible ne s'est produit ... QL ma√Ætrisera ce mod√®le pendant tr√®s longtemps. </li><li>  Qn peut √™tre une fonction tr√®s complexe.  Par exemple, pour le calculer, vous devez faire d√©filer un cycle de N it√©rations - et cela ne fonctionnera pas plus rapidement.  Un mod√®le pr√©dictif a g√©n√©ralement une complexit√© limit√©e - m√™me un grand r√©seau de neurones a une limite de complexit√©, et presque aucun mod√®le d'apprentissage automatique ne peut faire tourner les cycles. </li><li>  La r√©alit√© a g√©n√©ralement des variables cach√©es.  Par exemple, quelle heure est-il maintenant?  Il est facile de savoir si nous regardons la montre, mais d√®s que nous regardons ailleurs, il s'agit d√©j√† d'une variable cach√©e.  Pour prendre en compte ces valeurs non observables, il est n√©cessaire que le mod√®le tienne compte non seulement de l'√©tat actuel, mais aussi d'une sorte d'histoire.  Dans QL, vous pouvez le faire - par exemple, pour alimenter non seulement le S actuel, mais aussi plusieurs pr√©c√©dents dans le neurone ou ce que nous avons l√†.  Cela se fait dans RL, qui joue aux jeux Atari.  De plus, vous pouvez utiliser un r√©seau de neurones r√©current pour la pr√©vision - laissez-le s'ex√©cuter s√©quentiellement sur plusieurs trames d'historique et calculez Qn. </li></ol><br><h3>  Syst√®mes bas√©s sur des mod√®les </h3><br>  Mais que se passe-t-il si nous pr√©disons non seulement R ou Q, mais g√©n√©ralement toutes les donn√©es sensorielles?  Nous aurons constamment une copie de poche de la r√©alit√© et nous pourrons v√©rifier nos plans √† ce sujet.  Dans ce cas, nous sommes beaucoup moins pr√©occup√©s par la difficult√© de calculer la fonction Q.  Oui, il faut beaucoup d'horloges pour calculer - enfin, de toute fa√ßon, pour chaque plan, nous ex√©cuterons √† plusieurs reprises le mod√®le de pr√©vision.  La planification de 10 va de l'avant?  Nous lan√ßons le mod√®le 10 fois, et chaque fois que nous alimentons ses sorties √† son entr√©e. <br><br>  <b>Quels sont les inconv√©nients?</b> <br><br><ol><li>  Intensit√© des ressources.  Supposons que nous devons faire un choix de deux alternatives √† chaque mesure.  Ensuite, pour 10 cycles d'horloge, nous aurons 2 ^ 10 = 1024 plans possibles.  Chaque plan comprend 10 lancements de mod√®les.  Si nous contr√¥lons un avion avec des dizaines d'organes directeurs?  Et simulons-nous la r√©alit√© avec une p√©riode de 0,1 seconde?  Voulez-vous avoir un horizon de planification d'au moins quelques minutes?  Nous devrons ex√©cuter le mod√®le plusieurs fois, il y a beaucoup de cycles d'horloge du processeur pour une solution.  M√™me si vous optimisez d'une mani√®re ou d'une autre l'√©num√©ration des plans, il y a des ordres de grandeur plus de calculs qu'en QL. </li><li>  Le probl√®me du chaos.  Certains syst√®mes sont con√ßus de sorte que m√™me une petite impr√©cision de la simulation d'entr√©e entra√Æne une √©norme erreur de sortie.  Pour contrer cela, vous pouvez ex√©cuter plusieurs simulations de la r√©alit√© - un peu diff√©rentes.  Ils produiront des r√©sultats tr√®s diff√©rents, et √† partir de cela, il sera possible de comprendre que nous sommes dans la zone d'une telle instabilit√©. </li></ol><br><h2>  M√©thode d'√©num√©ration de la strat√©gie </h2><br>  Si nous avons acc√®s √† l'environnement de test pour l'IA, si nous l'ex√©cutons non pas en r√©alit√©, mais dans une simulation, alors nous pouvons √©crire sous une forme ou une autre la strat√©gie du comportement de notre agent.  Et puis choisissez - avec √©volution ou autre chose - une telle strat√©gie qui m√®ne √† un profit maximum. <br>  ¬´Choisir une strat√©gie¬ª signifie que nous devons d'abord apprendre √† √©crire une strat√©gie de telle mani√®re qu'elle puisse √™tre pouss√©e dans l'algorithme d'√©volution.  Autrement dit, nous pouvons √©crire la strat√©gie avec le code du programme, mais √† certains endroits, laissez les coefficients et laissez l'√©volution les ramasser.  Ou nous pouvons √©crire une strat√©gie avec un r√©seau de neurones - et laisser l'√©volution prendre le poids de ses connexions. <br><br>  Autrement dit, il n'y a aucune pr√©vision ici.  Pas de table SAR.  Nous s√©lectionnons simplement une strat√©gie, et elle donne imm√©diatement des actions. <br><br>  C'est une m√©thode puissante et efficace, si vous voulez essayer RL et ne savez pas par o√π commencer, je le recommande.  C'est une fa√ßon tr√®s bon march√© de ¬´voir un miracle¬ª. <br><br>  <b>Quels sont les inconv√©nients?</b> <br><br><ol><li>  La capacit√© d'ex√©cuter plusieurs fois les m√™mes exp√©riences est requise.  Autrement dit, nous devrions √™tre en mesure de rembobiner la r√©alit√© au point de d√©part - des dizaines de milliers de fois.  Pour essayer une nouvelle strat√©gie. <br><br>  La vie offre rarement de telles opportunit√©s.  Habituellement, si nous avons un mod√®le du processus qui nous int√©resse, nous ne pouvons pas cr√©er une strat√©gie astucieuse - nous pouvons simplement √©laborer un plan, comme dans une approche bas√©e sur un mod√®le, m√™me avec une force brute brutale. </li><li>  Intol√©rance √† l'exp√©rience.  Avons-nous une table SAR pour des ann√©es d'exp√©rience?  On peut l'oublier, √ßa ne rentre pas dans le concept. </li></ol><br><h3>  Une m√©thode d'√©num√©ration des strat√©gies, mais ¬´en direct¬ª </h3><br>  La m√™me √©num√©ration des strat√©gies, mais sur la r√©alit√© vivante.  Nous essayons 10 mesures d'une strat√©gie.  Ensuite, 10 mesures un autre.  Puis 10 mesures du troisi√®me.  Ensuite, nous s√©lectionnons celui o√π il y avait le plus de renfort. <br>  Les meilleurs r√©sultats pour la marche des humano√Ødes ont √©t√© obtenus par cette m√©thode. <br><br><img src="https://habrastorage.org/webt/zh/v_/sn/zhv_snutr8ma1aeenqr3itjojvc.png"><br><br>  Pour moi, cela semble quelque peu inattendu - il semblerait que l'approche bas√©e sur le mod√®le QL + soit math√©matiquement id√©ale.  Mais rien de tel.  Les avantages de l'approche sont √† peu pr√®s les m√™mes que les pr√©c√©dents - mais ils sont moins prononc√©s, car les strat√©gies ne sont pas test√©es tr√®s longtemps (enfin, nous n'avons pas des mill√©naires d'√©volution), ce qui signifie que les r√©sultats sont instables.  De plus, le nombre de tests ne peut pas non plus √™tre port√© √† l'infini - ce qui signifie que la strat√©gie devra √™tre recherch√©e dans un espace d'options peu compliqu√©.  Non seulement elle aura des ¬´stylos¬ª qui peuvent √™tre ¬´tordus¬ª.  Eh bien, l'intol√©rance v√©cue n'a pas √©t√© annul√©e.  Et, par rapport √† QL ou Model-Based, ces mod√®les utilisent l'exp√©rience de mani√®re inefficace.  Ils ont besoin de beaucoup plus d'interactions avec la r√©alit√© que les approches qui utilisent l'apprentissage automatique. <br><br>  Comme vous pouvez le voir, toute tentative de cr√©ation d'un AGI en th√©orie devrait contenir soit un apprentissage automatique pour pr√©voir les r√©compenses, soit une forme de notation param√©trique d'une strat√©gie - afin que vous puissiez reprendre cette strat√©gie avec quelque chose comme l'√©volution. <br><br>  Il s'agit d'une forte attaque contre les personnes qui proposent de cr√©er une IA bas√©e sur des bases de donn√©es, une logique et des graphiques conceptuels.  Si vous, les partisans de l'approche symbolique, lisez ceci - bienvenue dans les commentaires, je serai heureux de savoir ce que AGI peut faire sans la m√©canique d√©crite ci-dessus. <br><br><h2>  Mod√®les d'apprentissage automatique pour RL </h2><br>  Presque tous les mod√®les ML peuvent √™tre utilis√©s pour un apprentissage renforc√©.  Les r√©seaux de neurones sont bien s√ªr bons.  Mais il y a, par exemple, KNN.  Pour chaque paire S et A, nous recherchons les plus similaires, mais dans le pass√©.  Et nous cherchons ce que sera R. Stupide apr√®s √ßa?  Oui, mais √ßa marche.  Il y a des arbres d√©cisifs - ici, il vaut mieux se promener sur les mots-cl√©s "renforcement du gradient" et "for√™t d√©cisive".  Les arbres sont-ils pauvres pour capturer des d√©pendances complexes?  Utilisez l'ing√©nierie des fonctionnalit√©s.  Vous voulez que votre IA soit plus proche du g√©n√©ral?  Utilisez FE automatique!  Parcourez un tas de formules diff√©rentes, soumettez-les en tant que fonctionnalit√©s pour votre boost, jetez les formules qui augmentent l'erreur et laissez les formules qui am√©liorent la pr√©cision.  Soumettez ensuite les meilleures formules comme arguments pour les nouvelles formules, et ainsi de suite, √©voluez. <br><br>  Vous pouvez utiliser des r√©gressions symboliques pour la pr√©vision - c'est-√†-dire simplement trier des formules pour essayer d'obtenir quelque chose qui se rapproche de Q ou R. Il est possible d'essayer de trier des algorithmes - puis vous obtenez une chose appel√©e induction de Solomonov, qui est th√©oriquement optimale, mais presque tr√®s difficile √† former approximations des fonctions. <br><br>  Mais les r√©seaux de neurones sont g√©n√©ralement un compromis entre l'expressivit√© et la complexit√© d'apprentissage.  La r√©gression algorithmique d√©tecte id√©alement toute d√©pendance - pendant des centaines d'ann√©es.  L'arbre de d√©cision fonctionnera tr√®s rapidement - mais il ne pourra pas extrapoler y = a + b.  Un r√©seau de neurones est quelque chose entre les deux. <br><br><h2>  Perspectives de d√©veloppement </h2><br>  Quelles sont les fa√ßons de faire exactement AGI maintenant?  Du moins th√©oriquement. <br><br><h3>  √âvolution </h3><br>  Nous pouvons cr√©er de nombreux environnements de test diff√©rents et d√©marrer l'√©volution de certains r√©seaux de neurones.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Les configurations qui marquent plus de points au total pour tous les essais se multiplieront. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le r√©seau neuronal doit avoir de la m√©moire et il serait souhaitable d'avoir au moins une partie de la m√©moire sous forme de bande, comme une machine de Turing ou similaire sur un disque dur. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Le probl√®me est qu'avec l'aide de l'√©volution, vous pouvez bien s√ªr d√©velopper quelque chose comme RL. </font><font style="vertical-align: inherit;">Mais √† quoi devrait ressembler le langage dans lequel RL a l'air compact - pour que l'√©volution le trouve - et en m√™me temps, l'√©volution ne trouve pas de solutions comme "mais je vais cr√©er un neurone pour cent cinquante couches afin que vous deveniez tous fous pendant que je l'enseigne!" . </font><font style="vertical-align: inherit;">L'√©volution est comme une foule d'utilisateurs analphab√®tes - elle trouvera tous les d√©fauts dans le code et abandonnera tout le syst√®me.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Aixi </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez cr√©er un syst√®me bas√© sur un mod√®le bas√© sur un ensemble de nombreuses r√©gressions algorithmiques. </font><font style="vertical-align: inherit;">L'algorithme est garanti comme √©tant Turing complet - ce qui signifie qu'il n'y aura aucun motif qui ne pourra pas √™tre d√©tect√©. </font><font style="vertical-align: inherit;">L'algorithme est √©crit en code - ce qui signifie que sa complexit√© peut √™tre facilement calcul√©e. </font><font style="vertical-align: inherit;">Cela signifie qu'il est possible d'affiner math√©matiquement correctement vos hypoth√®ses sur la complexit√© du dispositif mondial. </font><font style="vertical-align: inherit;">Avec les r√©seaux de neurones, par exemple, cette astuce ne fonctionnera pas - l√†, la p√©nalit√© pour la complexit√© est tr√®s indirecte et heuristique. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Il ne reste plus qu'√† apprendre √† former rapidement des r√©gressions algorithmiques. </font><font style="vertical-align: inherit;">Jusqu'√† pr√©sent, le meilleur pour cette √©volution est l'√©volution et elle est impardonnablement longue.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Seed AI </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ce serait cool de cr√©er une IA qui s'am√©liorera. Am√©liorez votre capacit√© √† r√©soudre des probl√®mes. Cela peut sembler une id√©e √©trange, mais ce probl√®me </font></font><a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a d√©j√† √©t√© r√©solu pour les syst√®mes d'optimisation statique, tels que l'√©volution</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . Si vous parvenez √† le r√©aliser ... Tout sur l'exposant est-il au courant? Nous aurons une IA tr√®s puissante en tr√®s peu de temps.</font></font><br><br>  Comment faire <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Vous pouvez essayer d'arranger cela dans RL certaines des actions affectent les param√®tres de RL lui-m√™me. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ou donnez au syst√®me RL un outil pour cr√©er de nouveaux processeurs de pr√© et post-donn√©es pour vous-m√™me. Laissez RL √™tre stupide, mais il pourra cr√©er des calculatrices, des cahiers et des ordinateurs pour lui-m√™me. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Une autre option consiste √† cr√©er une sorte d'IA en utilisant l'√©volution, dans laquelle une partie des actions affectera son appareil au niveau du code. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Mais pour le moment, je n'ai pas vu d'options r√©alisables pour Seed AI - bien que tr√®s limit√©es. Les d√©veloppeurs se cachent-ils? Ou ces options sont-elles si faibles qu'elles ne m√©ritaient pas l'attention g√©n√©rale et m'ont d√©pass√©?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cependant, Google et DeepMind fonctionnent d√©sormais principalement avec des architectures de r√©seaux de neurones. </font><font style="vertical-align: inherit;">Apparemment, ils ne veulent pas s'impliquer dans l'√©num√©ration combinatoire et essayer de rendre leurs id√©es adapt√©es √† la m√©thode de propagation de l'erreur. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">J'esp√®re que cet article a √©t√© utile =) Les commentaires sont les bienvenus, en particulier les commentaires comme ¬´Je sais comment am√©liorer AGI¬ª!</font></font></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr468379/">https://habr.com/ru/post/fr468379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr468363/index.html">Mon magnum opus du monde du jeu mobile</a></li>
<li><a href="../fr468367/index.html">Amazon annonce un plan de r√©chauffement climatique</a></li>
<li><a href="../fr468369/index.html">Comment j'ai cr√©√© ¬´WildMAN¬ª - une parodie de nombreux jeux 8 bits et que j'ai r√©cemment port√© sur Android</a></li>
<li><a href="../fr468371/index.html">La conception du jeu √† la vie. T√©l√©chargement transparent ou immersion compl√®te dans God of War 4</a></li>
<li><a href="../fr468377/index.html">8 histoires sur la Chine int√©rieure. Ce qui n'est pas montr√© aux √©trangers</a></li>
<li><a href="../fr468381/index.html">Retour vers le futur? Eraser Quantum en attente</a></li>
<li><a href="../fr468383/index.html">G√©n√©rateur de m√®mes Ruby pour attirer l'int√©r√™t pour la langue</a></li>
<li><a href="../fr468385/index.html">Le bureau est mort, vive le bureau! Je collectionne habrastatistiki</a></li>
<li><a href="../fr468387/index.html">Le condens√© de mat√©riaux int√©ressants pour le d√©veloppeur mobile # 316 (du 16 au 22 septembre)</a></li>
<li><a href="../fr468389/index.html">Artyom Galonsky, Bureau de la STO du Bureau: ¬´Je suis contre une chose telle qu'un ing√©nieur DevOps¬ª</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>