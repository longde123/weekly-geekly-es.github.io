<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ğŸŒ³ ğŸ¦Œ ğŸ‘¨ğŸ¾â€ğŸ­ Ã‰quilibrage prÃ©cis de la charge ğŸ‘ŠğŸ¿ ğŸ§˜ğŸ» ğŸ‘¨ğŸ¼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cet article se concentrera sur l'Ã©quilibrage de charge dans les projets Web. Beaucoup croient que la solution Ã  ce problÃ¨me dans la rÃ©partition de la ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Ã‰quilibrage prÃ©cis de la charge</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/oleg-bunin/blog/423085/">  Cet article se concentrera sur l'Ã©quilibrage de charge dans les projets Web.  Beaucoup croient que la solution Ã  ce problÃ¨me dans la rÃ©partition de la charge entre les serveurs - le plus prÃ©cis, le mieux.  Mais nous savons que ce n'est pas entiÃ¨rement vrai.  <strong>La stabilitÃ© du systÃ¨me est beaucoup plus importante d'un point de vue commercial</strong> . <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  Le petit pic minute Ã  84 RPS de Â«cinq centsÂ» est cinq mille erreurs que les vrais utilisateurs ont reÃ§ues.  C'est beaucoup et c'est trÃ¨s important.  Il faut rechercher les raisons, travailler sur les erreurs et essayer de continuer Ã  Ã©viter de telles situations. <br><br>  <strong>Nikolay Sivko</strong> ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">NikolaySivko</a> ) dans son rapport sur RootConf 2018 a parlÃ© des aspects subtils et pas encore trÃ¨s populaires de l'Ã©quilibrage de charge: <br><br><ul><li>  quand rÃ©pÃ©ter la demande (nouvelles tentatives); </li><li>  comment sÃ©lectionner des valeurs pour les dÃ©lais d'expiration; </li><li>  comment ne pas tuer les serveurs sous-jacents au moment de l'accident / de la congestion; </li><li>  si des contrÃ´les de santÃ© sont nÃ©cessaires; </li><li>  comment gÃ©rer les problÃ¨mes de scintillement. </li></ul><br>  Sous dÃ©codage chat de ce rapport. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/2-j2ADWFkkE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  <strong>Ã€ propos de l'orateur:</strong> Nikolay Sivko co-fondateur de okmeter.io.  Il a travaillÃ© en tant qu'administrateur systÃ¨me et chef d'un groupe d'administrateurs.  OpÃ©ration supervisÃ©e Ã  hh.ru.  Il a fondÃ© le service de surveillance okmeter.io.  Dans le cadre de ce rapport, le suivi de l'expÃ©rience en dÃ©veloppement est la principale source de cas. <br><br><h2>  De quoi allons-nous parler? <br></h2><br>  Cet article parlera de projets Web.  Voici un exemple de production en direct: le graphique montre les demandes par seconde pour un certain service Web. <br><br><img src="https://habrastorage.org/webt/oy/5c/qt/oy5cqtlz-halhw7y5ayuz6xl9lm.png"><br><br>  Quand je parle d'Ã©quilibrage, beaucoup le perÃ§oivent comme "nous devons rÃ©partir la charge entre les serveurs - le plus prÃ©cis, le mieux." <br><br><img src="https://habrastorage.org/webt/pm/g2/sp/pmg2spartsnxrxcyhzi_4-ui64g.png"><br><br>  En fait, ce n'est pas entiÃ¨rement vrai.  Ce problÃ¨me concerne un trÃ¨s petit nombre d'entreprises.  Le plus souvent, les entreprises s'inquiÃ¨tent des erreurs et de la stabilitÃ© du systÃ¨me. <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  Le petit pic sur le graphique est Â«cinq centsÂ», que le serveur a renvoyÃ© en une minute, puis arrÃªtÃ©.  Du point de vue d'une entreprise, comme une boutique en ligne, ce petit pic Ã  84 RPS de Â«cinq centsÂ» reprÃ©sente 5040 erreurs pour les utilisateurs rÃ©els.  Certains n'ont pas trouvÃ© quelque chose dans votre catalogue, d'autres n'ont pas pu mettre la marchandise dans le panier.  Et c'est trÃ¨s important.  Bien que ce pic ne semble pas trÃ¨s grand sur le graphique, <strong>il l'est beaucoup chez les vrais utilisateurs</strong> . <br><br>  En rÃ¨gle gÃ©nÃ©rale, tout le monde a de tels pics, et les administrateurs n'y rÃ©pondent pas toujours.  TrÃ¨s souvent, lorsqu'une entreprise demande ce que c'Ã©tait, ils lui rÃ©pondent: <br><br><ul><li>  "Ceci est une courte rafale!" </li><li>  "C'est juste une version qui sort." </li><li>  "Le serveur est mort, mais tout est dÃ©jÃ  en ordre." </li><li>  "Vasya a changÃ© le rÃ©seau de l'un des backends." </li></ul><br>  Souvent, les gens <strong>n'essaient mÃªme pas de comprendre les raisons</strong> pour lesquelles cela s'est produit et ne font aucun post-travail pour que cela ne se reproduise plus. <br><br><h2>  Affiner <br></h2><br>  J'ai appelÃ© le rapport "Fine tuning" (Eng. Fine tuning), parce que je pensais que tout le monde ne se mettait pas Ã  cette tÃ¢che, mais cela en valait la peine.  Pourquoi n'y arrivent-ils pas? <br><br><ul><li>  <strong>Tout le monde ne parvient pas Ã  cette tÃ¢che,</strong> car lorsque tout fonctionne, ce n'est pas visible.  Ceci est trÃ¨s important pour les problÃ¨mes.  Le Fakapa n'arrive pas tous les jours, et un si petit problÃ¨me nÃ©cessite des efforts trÃ¨s sÃ©rieux pour le rÃ©soudre. </li><li>  <strong>Vous devez rÃ©flÃ©chir beaucoup.</strong>  TrÃ¨s souvent, l'administrateur - la personne qui ajuste la balance - n'est pas en mesure de rÃ©soudre ce problÃ¨me de maniÃ¨re indÃ©pendante.  Ensuite, nous verrons pourquoi. </li><li>  <strong>Il capture les niveaux sous-jacents.</strong>  Cette tÃ¢che est trÃ¨s Ã©troitement liÃ©e au dÃ©veloppement, Ã  l'adoption de dÃ©cisions qui affectent votre produit et vos utilisateurs. </li></ul><br>  <strong>J'affirme qu'il est temps de faire cette tÃ¢che pour plusieurs raisons:</strong> <br><br><ul><li>  Le monde change, devient plus dynamique, il existe de nombreuses versions.  Ils disent que maintenant il est correct de sortir 100 fois par jour, et la sortie est le futur fakap avec une probabilitÃ© de 50 Ã  50 (tout comme la probabilitÃ© de rencontrer un dinosaure) </li><li>  Du point de vue de la technologie, tout est Ã©galement trÃ¨s dynamique.  Kubernetes et d'autres orchestrateurs sont apparus.  Il n'y a pas de bon vieux dÃ©ploiement, lorsqu'un backend sur une IP est dÃ©sactivÃ©, une mise Ã  jour est lancÃ©e et le service augmente.  Maintenant, dans le processus de dÃ©ploiement dans k8s, la liste des IP en amont change complÃ¨tement. </li><li>  Microservices: maintenant tout le monde communique via le rÃ©seau, ce qui signifie que vous devez le faire de maniÃ¨re fiable.  L'Ã©quilibrage joue un rÃ´le important. </li></ul><br><h2>  Banc d'essai <br></h2><br>  CommenÃ§ons par des cas simples et Ã©vidents.  Pour plus de clartÃ©, je vais utiliser un banc d'essai.  Il s'agit d'une application Golang qui donne http-200, ou vous pouvez la passer en mode "donner http-503". <br><br>  Nous commenÃ§ons 3 instances: <br><br><ul><li>  127.0.0.1:20001 </li><li>  127.0.0.1:20002 </li><li>  127.0.0.1:20003 </li></ul><br>  Nous servons 100rps via yandex.tank via nginx. <br><br>  Nginx hors de la boÃ®te: <br><br><pre><code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; location / { proxy_pass http://backends; } }</code> </pre> <br><h3>  ScÃ©nario primitif </h3><br>  Ã€ un moment donnÃ©, activez l'un des backends dans le mode Give 503, et nous obtenons exactement un tiers des erreurs. <br><br><img src="https://habrastorage.org/webt/qp/m1/ro/qpm1rolcydmcpwpvule4pw97b-o.png"><br><br>  Il est clair que rien ne sort de la boÃ®te: nginx ne rÃ©essaye pas de la boÃ®te s'il a reÃ§u <strong>une rÃ©ponse</strong> du serveur. <br><br><pre> <code class="plaintext hljs">Nginx default: proxy_next_upstream error timeout;</code> </pre><br>  En fait, c'est assez logique du cÃ´tÃ© des dÃ©veloppeurs de nginx: nginx n'a pas le droit de dÃ©cider pour vous ce que vous voulez retravailler et ce qui ne l'est pas. <br><br>  Par consÃ©quent, nous avons besoin de nouvelles tentatives - de nouvelles tentatives, et nous commenÃ§ons Ã  en parler. <br><br><h2>  RÃ©essais <br></h2><br>  Il faut trouver un compromis entre: <br><br><ul><li>  La demande de l'utilisateur est sainte, se blesse, mais rÃ©pondez.  Nous voulons rÃ©pondre Ã  tout prix Ã  l'utilisateur, l'utilisateur est le plus important. </li><li>  Mieux vaut rÃ©pondre avec une erreur que de surcharger les serveurs. </li><li>  IntÃ©gritÃ© des donnÃ©es (pour les demandes non idempotentes), c'est-Ã -dire qu'il est impossible de rÃ©pÃ©ter certains types de demandes. </li></ul><br>  <strong>La vÃ©ritÃ©, comme d'habitude, se situe quelque part entre -</strong> nous sommes obligÃ©s d'Ã©quilibrer entre ces trois points.  Essayons de comprendre quoi et comment. <br><br>  J'ai divisÃ© les tentatives infructueuses en 3 catÃ©gories: <br><br>  1. <strong>Erreur de transport</strong> <br>  Pour le transport HTTP, il s'agit de TCP et, en rÃ¨gle gÃ©nÃ©rale, nous parlons ici d'erreurs de configuration de connexion et de dÃ©lais d'expiration de configuration de connexion.  Dans mon rapport, je mentionnerai 3 Ã©quilibreurs courants (nous parlerons un peu d'Envoy): <br><br><ul><li>  <strong>nginx</strong> : erreurs + timeout (proxy_connect_timeout); </li><li>  <strong>HAProxy</strong> : connexion timeout; </li><li>  <strong>EnvoyÃ©</strong> : Ã©chec de connexion + flux refusÃ©. </li></ul><br>  Nginx a la possibilitÃ© de dire qu'une tentative ayant Ã©chouÃ© est une erreur de connexion et un dÃ©lai d'expiration de connexion;  HAProxy a un dÃ©lai de connexion, Envoy a Ã©galement tout ce qui est standard et normal. <br><br>  2. <strong>DÃ©lai de demande:</strong> <br>  Supposons que nous ayons envoyÃ© une requÃªte au serveur, connectÃ© avec succÃ¨s, mais que la rÃ©ponse ne nous vienne pas, nous l'avons attendue et nous comprenons qu'il ne sert Ã  rien d'attendre plus longtemps.  C'est ce qu'on appelle le dÃ©lai d'expiration de la demande: <br><br><ul><li>  <strong>Nginx</strong> a: timeout (prox_send_timeout * + proxy_read_timeout *); </li><li>  <strong>HAProxy a</strong> <strong>OOPS :(</strong> - il n'existe pas en principe. Beaucoup de gens ne savent pas que HAProxy, s'il a rÃ©ussi Ã  Ã©tablir une connexion, n'essaiera jamais de renvoyer la demande. </li><li>  <strong>L'envoyÃ©</strong> peut tout faire: dÃ©lai d'expiration ||  per_try_timeout. </li></ul><br>  3. <strong>Statut HTTP</strong> <br>  Tous les Ã©quilibreurs, Ã  l'exception de HAProxy, sont capables de traiter, si nÃ©anmoins le backend vous a rÃ©pondu, mais avec une sorte de code erronÃ©. <br><br><ul><li>  <strong>nginx</strong> : http_ * </li><li>  <strong>HAProxy</strong> : <strong>OOPS :(</strong> </li><li>  <strong>Envoy</strong> : 5xx, erreur de passerelle (502, 503, 504), rÃ©cupÃ©rable-4xx (409) </li></ul><br><h3>  DÃ©lais <br></h3><br>  Parlons maintenant en dÃ©tail des dÃ©lais d'attente, il me semble qu'il vaut la peine d'y prÃªter attention.  Il n'y aura plus de fusÃ©e scientifique - il s'agit simplement d'informations structurÃ©es sur ce qui se passe gÃ©nÃ©ralement et sur la maniÃ¨re dont elles s'y rapportent. <br><br><h4>  DÃ©lai d'expiration de connexion <br></h4><br>  Le dÃ©lai de connexion est le temps pour Ã©tablir une connexion.  Ceci est une caractÃ©ristique de votre rÃ©seau et de votre serveur spÃ©cifique et ne dÃ©pend pas de la demande.  Habituellement, la valeur par dÃ©faut pour le dÃ©lai de connexion est dÃ©finie sur petite.  Dans tous les proxys, la valeur par dÃ©faut est suffisamment grande, et c'est faux - ce devrait Ãªtre des <strong>unitÃ©s, parfois des dizaines de millisecondes</strong> (si nous parlons d'un rÃ©seau Ã  l'intÃ©rieur d'un DC). <br><br>  Si vous souhaitez identifier les serveurs problÃ©matiques un peu plus rapidement que ces unitÃ©s-dizaines de millisecondes, vous pouvez ajuster la charge sur le backend en dÃ©finissant un petit backlog pour recevoir les connexions TCP.  Dans ce cas, vous pouvez, lorsque le backlog de l'application est plein, demander Ã  Linux de le rÃ©initialiser pour dÃ©border le backlog.  Ensuite, vous pourrez filmer le "mauvais" backend surchargÃ© un peu plus tÃ´t que le dÃ©lai de connexion: <br><br><pre> <code class="plaintext hljs">fail fast: listen backlog + net.ipv4.tcp_abort_on_overflow</code> </pre> <br><h4>  DÃ©lai d'expiration de la demande <br></h4><br>  Le dÃ©lai d'expiration des demandes n'est pas une caractÃ©ristique du rÃ©seau, mais une <strong>caractÃ©ristique d'un groupe de demandes</strong> (gestionnaire).  Il existe diffÃ©rentes demandes - leur gravitÃ© est diffÃ©rente, leur logique interne est complÃ¨tement diffÃ©rente, elles doivent accÃ©der Ã  des rÃ©fÃ©rentiels complÃ¨tement diffÃ©rents. <br><br>  Nginx lui <strong>-</strong> mÃªme <strong>n'a pas de dÃ©lai d'expiration pour la demande entiÃ¨re.</strong>  Il a: <br><br><ul><li>  proxy_send_timeout: temps entre deux opÃ©rations d'Ã©criture rÃ©ussies write (); </li><li>  proxy_read_timeout: temps entre deux lectures rÃ©ussies (). </li></ul><br>  Autrement dit, si vous avez un backend lentement, un octet de fois, donne quelque chose dans un dÃ©lai d'attente, alors tout va bien.  En tant que tel, nginx n'a pas request_timeout.  Mais nous parlons en amont.  Dans notre centre de donnÃ©es, ils sont contrÃ´lÃ©s par nous, par consÃ©quent, en supposant que le rÃ©seau n'a pas de loris lent, alors, en principe, read_timeout peut Ãªtre utilisÃ© comme request_timeout. <br><br>  Envoy a tout pour lui: timeout ||  per_try_timeout. <br><br><h4>  SÃ©lectionnez le dÃ©lai d'expiration de la demande <br></h4><br>  Maintenant, la chose la plus importante, Ã  mon avis, est de savoir quel request_timeout mettre.  Nous partons de combien il est permis Ã  l'utilisateur d'attendre - c'est un certain maximum.  Il est clair que l'utilisateur n'attendra pas plus de 10 s, vous devez donc lui rÃ©pondre plus rapidement. <br><br><ul><li>  Si nous voulons gÃ©rer la dÃ©faillance d'un seul serveur, le dÃ©lai d'expiration doit Ãªtre infÃ©rieur au dÃ©lai maximal autorisÃ©: <strong>request_timeout &lt;max.</strong> </li><li>  Si vous voulez avoir <strong>2 tentatives garanties d'</strong> envoyer une demande Ã  deux backends diffÃ©rents, le dÃ©lai d'expiration pour une tentative est Ã©gal Ã  la moitiÃ© de cet intervalle autorisÃ©: <strong>per_try_timeout = 0,5 * max.</strong> </li><li>  Il existe Ã©galement une option intermÃ©diaire - <strong>2 tentatives optimistes</strong> dans le cas oÃ¹ le premier backend s'est "Ã©moussÃ©", mais le second rÃ©pondra rapidement: <strong>per_try_timeout = k * max (oÃ¹ k&gt; 0,5).</strong> </li></ul><br>  Il existe diffÃ©rentes approches, mais en gÃ©nÃ©ral, le <strong>choix d'un dÃ©lai d'attente est difficile</strong> .  Il y aura toujours des cas limites, par exemple, le mÃªme gestionnaire dans 99% des cas est traitÃ© en 10 ms, mais il y a 1% des cas quand on attend 500 ms, et c'est normal.  Cela devra Ãªtre rÃ©solu. <br><br>  Avec ce 1%, quelque chose doit Ãªtre fait, car l'ensemble du groupe de demandes doit, par exemple, se conformer au SLA et tenir en 100 ms.  TrÃ¨s souvent, Ã  ces moments, la demande est traitÃ©e: <br><br><ul><li>  La pagination apparaÃ®t aux endroits oÃ¹ il est impossible de renvoyer toutes les donnÃ©es dans un dÃ©lai d'attente. </li><li>  Les administrateurs / rapports sont sÃ©parÃ©s dans un groupe distinct d'URL afin d'augmenter le dÃ©lai d'attente pour eux, et oui pour rÃ©duire les demandes des utilisateurs. </li><li>  Nous rÃ©parons / optimisons les demandes qui ne correspondent pas Ã  notre dÃ©lai d'expiration. </li></ul><br>  Ici, nous devons prendre une dÃ©cision, ce qui n'est pas trÃ¨s simple d'un point de vue psychologique, que si nous n'avons pas le temps de rÃ©pondre Ã  l'utilisateur dans le temps imparti, nous donnerons une erreur (c'est comme dans un ancien dicton chinois: "Si la jument est morte, descendez!") <strong>.</strong> <br><br>  AprÃ¨s cela, le processus de surveillance de votre service du point de vue de l'utilisateur est simplifiÃ©: <br><br><ul><li>  S'il y a des erreurs, tout est mauvais, il faut le rÃ©parer. </li><li>  S'il n'y a pas d'erreurs, nous nous adaptons au bon temps de rÃ©ponse, alors tout va bien. </li></ul><br><h3>  Tentatives spÃ©culatives # nifig <br></h3><br>  Nous nous sommes assurÃ©s que le choix d'une valeur de timeout est assez difficile.  Comme vous le savez, pour simplifier quelque chose, vous devez compliquer quelque chose :) <br><br>  <strong>Nouvelle spÃ©culation</strong> - une demande rÃ©pÃ©tÃ©e Ã  un autre serveur, qui est lancÃ©e par une condition, mais la premiÃ¨re demande n'est pas interrompue.  Nous prenons la rÃ©ponse du serveur qui a rÃ©pondu plus rapidement. <br><br>  Je n'ai pas vu cette fonctionnalitÃ© dans les Ã©quilibreurs que je connais, mais il y a un excellent exemple avec Cassandra (protection en lecture rapide): <br><br>  spÃ©culative_retry = N ms |  <strong>M <sup>e</sup> centile</strong> <br><br>  De cette faÃ§on, vous <strong>n'avez pas Ã  expirer</strong> .  Vous pouvez le laisser Ã  un niveau acceptable et en tout cas avoir une deuxiÃ¨me tentative pour obtenir une rÃ©ponse Ã  la demande. <br><br>  Cassandra a une opportunitÃ© intÃ©ressante de dÃ©finir une dynamique spÃ©culative_retry ou dynamique, puis la deuxiÃ¨me tentative sera effectuÃ©e Ã  travers le centile du temps de rÃ©ponse.  Cassandra accumule des statistiques sur les temps de rÃ©ponse des demandes prÃ©cÃ©dentes et adapte une valeur de timeout spÃ©cifique.  Cela fonctionne plutÃ´t bien. <br><br>  Dans cette approche, tout repose sur l'Ã©quilibre entre la fiabilitÃ© et la charge parasite. Pas les serveurs. Vous fournissez la fiabilitÃ©, mais parfois vous obtenez des demandes supplÃ©mentaires au serveur.  Si vous Ã©tiez pressÃ© quelque part et avez envoyÃ© une deuxiÃ¨me demande, mais que la premiÃ¨re a quand mÃªme rÃ©pondu, le serveur a reÃ§u un peu plus de charge.  Dans un seul cas, c'est un petit problÃ¨me. <br><br><img src="https://habrastorage.org/webt/uv/7c/bs/uv7cbswancegyh5vc8t7mwvr8uy.png"><br><br>  La cohÃ©rence du timeout est un autre aspect important.  Nous parlerons davantage de l'annulation de la demande, mais en gÃ©nÃ©ral, si le dÃ©lai d'expiration de la demande utilisateur entiÃ¨re est de 100 ms, il est inutile de dÃ©finir le dÃ©lai d'expiration de la demande dans la base de donnÃ©es pendant 1 s.  Il existe des systÃ¨mes qui vous permettent de le faire de maniÃ¨re dynamique: service Ã  service transfÃ¨re le reste du temps que vous attendez une rÃ©ponse Ã  cette demande.  C'est compliquÃ©, mais si vous en avez soudainement besoin, vous pouvez facilement dÃ©couvrir comment le faire dans le mÃªme Envoy. <br><br>  Que devez-vous savoir d'autre sur la nouvelle tentative? <br><br><h3>  Point de non retour (V1) <br></h3><br>  Ici, V1 n'est pas la version 1. Dans l'aviation, il existe un tel concept - la vitesse V1.  Il s'agit de la vitesse aprÃ¨s laquelle il est impossible de ralentir lors de l'accÃ©lÃ©ration sur la piste.  Il est nÃ©cessaire de dÃ©coller, puis de dÃ©cider quoi faire ensuite. <br><br>  Le mÃªme point de non-retour se trouve dans les Ã©quilibreurs de charge: <strong>lorsque vous avez transmis 1 octet de la rÃ©ponse Ã  votre client, aucune erreur ne peut Ãªtre corrigÃ©e</strong> .  Si le backend meurt Ã  ce stade, aucune nouvelle tentative ne sera utile.  Vous ne pouvez que rÃ©duire la probabilitÃ© qu'un tel scÃ©nario se dÃ©clenche, effectuer un arrÃªt progressif, c'est-Ã -dire dire Ã  votre application: Â«Vous n'acceptez pas de nouvelles demandes maintenant, mais modifiez les anciennes!Â» Et alors seulement, Ã©teignez-la. <br><br>  Si vous contrÃ´lez le client, il s'agit d'une application Ajax ou mobile dÃ©licate, il peut essayer de rÃ©pÃ©ter la demande, puis vous pouvez sortir de cette situation. <br><br><h3>  Point de non-retour [EnvoyÃ©] <br></h3><br>  L'envoyÃ© a eu un truc si Ã©trange.  Il y a per_try_timeout - il limite le temps que chaque tentative pour obtenir une rÃ©ponse Ã  une demande peut prendre.  Si ce dÃ©lai a fonctionnÃ©, mais que le backend a dÃ©jÃ  commencÃ© Ã  rÃ©pondre au client, alors tout a Ã©tÃ© interrompu, le client a reÃ§u une erreur. <br><br>  Mon collÃ¨gue Pavel Trukhanov ( <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=" class="user_link">tru_pablo</a> ) a fait un <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">patch</a> , qui est dÃ©jÃ  dans master Envoy et sera en 1.7.  Maintenant, cela fonctionne comme il se doit: si la rÃ©ponse a commencÃ© Ã  Ãªtre transmise, seul le dÃ©lai global fonctionnera. <br><br><h3>  Nouvelles tentatives: nÃ©cessitÃ© de limiter <br></h3><br>  Les nouvelles tentatives sont bonnes, mais il existe des demandes dites tueuses: les requÃªtes lourdes qui exÃ©cutent une logique trÃ¨s complexe accÃ¨dent beaucoup Ã  la base de donnÃ©es et ne correspondent souvent pas Ã  per_try_timeout.  Si nous envoyons encore et encore des tentatives, nous tuons notre base.  Parce que <strong>dans la plupart des services de base de donnÃ©es (99,9%), il n'y a pas d'annulation de demande</strong> . <br><br>  La demande d'annulation signifie que le client a dÃ©crochÃ©, vous devez arrÃªter tout travail maintenant.  Golang promeut activement cette approche, mais malheureusement, elle se termine par un backend, et de nombreux rÃ©fÃ©rentiels de base de donnÃ©es ne le prennent pas en charge. <br><br>  En consÃ©quence, les tentatives doivent Ãªtre limitÃ©es, ce qui permet Ã  presque tous les Ã©quilibreurs (nous cessons dÃ©sormais de considÃ©rer HAProxy). <br><br>  <strong>Nginx:</strong> <br><br><ul><li>  proxy_next_upstream_timeout (global) </li><li>  proxt_read_timeout ** comme per_try_timeout </li><li>  proxy_next_upstream_tries </li></ul><br>  <strong>EnvoyÃ©:</strong> <br><br><ul><li>  dÃ©lai d'attente (global) </li><li>  per_try_timeout </li><li>  num_retries </li></ul><br>  Dans Nginx, nous pouvons dire que nous essayons de faire des tentatives Ã  travers la fenÃªtre X, c'est-Ã -dire qu'Ã  un intervalle de temps donnÃ©, par exemple 500 ms, nous faisons autant de tentatives que bon.  Ou il existe un paramÃ¨tre qui limite le nombre d'Ã©chantillons rÃ©pÃ©tÃ©s.  Dans <strong>Envoy</strong> , la mÃªme chose est la quantitÃ© ou le dÃ©lai d'expiration (global). <br><br><h4>  Nouvelle tentative: appliquer [nginx] <br></h4><br>  Prenons un exemple: nous dÃ©finissons des tentatives de relance dans nginx 2 - en consÃ©quence, aprÃ¨s avoir reÃ§u HTTP 503, nous essayons d'envoyer Ã  nouveau une demande au serveur.  DÃ©sactivez ensuite les <strong>deux</strong> backends. <br><br><pre> <code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; proxy_next_upstream error timeout http_503; proxy_next_upstream_tries 2; location / { proxy_pass http://backends; } }</code> </pre><br>  Voici les graphiques de notre banc d'essai.  Il n'y a aucune erreur sur le graphique supÃ©rieur, car il y en a trÃ¨s peu.  Si vous ne laissez que des erreurs, il est clair qu'elles le sont. <br><br><img src="https://habrastorage.org/webt/3h/sx/aq/3hsxaq8qyifcoyq3mvcyzmxm_cc.png"><br><br><img src="https://habrastorage.org/webt/sc/f_/2w/scf_2wz9tctmouvrs9hpqtpmau0.png"><br><br>  <strong>Qu'est-il arrivÃ©?</strong> <br><br><ul><li>  proxy_next_upstream_tries = <strong>2.</strong> </li><li>  Dans le cas oÃ¹ vous effectuez la premiÃ¨re tentative sur le serveur "mort" et la seconde - sur l'autre "mort", vous obtenez HTTP-503 en cas de <strong>deux</strong> tentatives sur les "mauvais" serveurs. </li><li>  Il y a peu d'erreurs, car nginx "interdit" un mauvais serveur.  Autrement dit, si dans nginx certaines erreurs sont retournÃ©es par le backend, il cesse de faire les tentatives suivantes pour lui envoyer une demande.  Ceci est rÃ©gi par la variable <strong>fail_timeout.</strong> </li></ul><br>  Mais il y a des erreurs, et cela ne nous convient pas. <br><br>  <strong>Que faire Ã  ce sujet?</strong> <br><br>  Nous pouvons soit augmenter le nombre de nouvelles tentatives (mais revenir ensuite au problÃ¨me des Â«demandes de tueurÂ»), soit rÃ©duire la probabilitÃ© qu'une demande atteigne des backends Â«mortsÂ».  Cela peut Ãªtre fait avec <strong>des bilans de santÃ©.</strong> <br><br><h2>  ContrÃ´les de santÃ© <br></h2><br>  Je suggÃ¨re de considÃ©rer les bilans de santÃ© comme une optimisation du processus de choix d'un serveur Â«liveÂ».  <strong>Cela ne donne aucune garantie.</strong>  Par consÃ©quent, lors de l'exÃ©cution d'une demande d'utilisateur, nous sommes plus susceptibles d'accÃ©der uniquement aux serveurs Â«en directÂ».  L'Ã©quilibreur accÃ¨de rÃ©guliÃ¨rement Ã  une URL spÃ©cifique, le serveur lui rÃ©pond: "Je suis vivant et prÃªt". <br><br><h4>  ContrÃ´les d'intÃ©gritÃ©: en termes de backend <br></h4><br>  Du point de vue du backend, vous pouvez faire des choses intÃ©ressantes: <br><br><ul><li>  VÃ©rifiez l'Ã©tat de prÃ©paration pour le fonctionnement de tous les sous-systÃ¨mes sous-jacents dont dÃ©pend le backend: le nombre nÃ©cessaire de connexions Ã  la base de donnÃ©es est Ã©tabli, le pool a des connexions libres, etc., etc. </li><li>  Vous pouvez accrocher votre propre logique sur l'URL des contrÃ´les d'intÃ©gritÃ© si l'Ã©quilibreur utilisÃ© n'est pas trÃ¨s intelligent (par exemple, vous prenez l'Ã©quilibreur de charge de l'hÃ´te).  Le serveur peut se rappeler que "dans la derniÃ¨re minute, j'ai donnÃ© tant d'erreurs - je suis probablement une sorte de" mauvais "serveur, et pendant les 2 prochaines minutes, je rÃ©pondrai avec" cinq cents "aux contrÃ´les de santÃ©.  Je vais donc m'interdire! "  Cela aide parfois beaucoup lorsque vous avez un Ã©quilibreur de charge non contrÃ´lÃ©. </li><li>  En rÃ¨gle gÃ©nÃ©rale, l'intervalle de vÃ©rification est d'environ une seconde et vous avez besoin du gestionnaire de vÃ©rification d'intÃ©gritÃ© pour ne pas tuer votre serveur.  Ã‡a devrait Ãªtre lÃ©ger. </li></ul><br><h4>  ContrÃ´les d'intÃ©gritÃ©: implÃ©mentations <br></h4><br>  En rÃ¨gle gÃ©nÃ©rale, tout ici est le mÃªme pour tout le monde: <br><br><ul><li>  Demande; </li><li>  Timeout dessus; </li><li>  Intervalle pendant lequel nous effectuons les vÃ©rifications.  Les proxys trompÃ©s ont une <strong>gigue</strong> , c'est-Ã -dire une certaine randomisation afin que tous les contrÃ´les de santÃ© ne parviennent pas au backend en mÃªme temps et ne le tuent pas. </li><li>  <strong>Seuil malsain</strong> - seuil du nombre de contrÃ´les d'intÃ©gritÃ© ayant Ã©chouÃ© pour que le service le marque comme malsain. </li><li>  <strong>Seuil sain</strong> - au contraire, combien de tentatives rÃ©ussies doivent passer pour que le serveur puisse retourner en fonctionnement. </li><li>  Logique supplÃ©mentaire.  Vous pouvez analyser l'Ã©tat de vÃ©rification + le corps, etc. </li></ul><br>  Nginx implÃ©mente les fonctions de vÃ©rification d'intÃ©gritÃ© uniquement dans la version payante de nginx +. <br><br>  Je note une fonctionnalitÃ© d' <strong>Envoy</strong> , il a un <strong>mode panique de</strong> contrÃ´le de santÃ© <strong>.</strong>  Lorsque nous avons interdit, comme "malsain", plus de N% des hÃ´tes (disons 70%), il pense que tous nos bilans de santÃ© mentent, et tous les hÃ´tes sont en fait vivants.  Dans un trÃ¨s mauvais cas, cela vous aidera Ã  ne pas vous retrouver dans une situation oÃ¹ vous vous Ãªtes tirÃ© une balle dans la jambe et avez banni tous les serveurs.  C'est une faÃ§on d'Ãªtre Ã  nouveau en sÃ©curitÃ©. <br><br><h2>  Tout mettre ensemble <br></h2><br>  GÃ©nÃ©ralement pour les contrÃ´les de santÃ© dÃ©finis: <br><br><ul><li>  Ou nginx +; </li><li>  Ou nginx + autre chose :) </li></ul><br>  Dans notre pays, il y a une tendance Ã  dÃ©finir nginx + HAProxy, car la version gratuite de nginx n'a pas de contrÃ´le de santÃ© et jusqu'au 1.11.5 il n'y avait pas de limite sur le nombre de connexions au backend.  Mais cette option est mauvaise car HAProxy ne sait pas comment se retirer aprÃ¨s avoir Ã©tabli une connexion.  Beaucoup de gens pensent que si HAProxy renvoie une erreur sur les tentatives de nginx et nginx, alors tout ira bien.  Pas vraiment.  Vous pouvez accÃ©der Ã  un autre HAProxy et au mÃªme backend, car les pools de backend sont les mÃªmes.  Vous introduisez donc un niveau d'abstraction supplÃ©mentaire pour vous-mÃªme, ce qui rÃ©duit la prÃ©cision de votre Ã©quilibrage et, par consÃ©quent, la disponibilitÃ© du service. <br><br>  Nous avons nginx + Envoy, mais si vous Ãªtes confus, vous pouvez vous limiter Ã  Envoy uniquement. <br><br><h2>  Quel genre d'envoyÃ©? <br></h2><br>  Envoy est un Ã©quilibreur de charge pour les jeunes Ã  la mode, dÃ©veloppÃ© Ã  l'origine en Lyft, Ã©crit en C ++.  <strong>Hors de la boÃ®te, il peut faire un tas de petits pains sur notre sujet aujourd'hui.</strong>  Vous l'avez probablement vu comme un maillage de service pour Kubernetes.  En rÃ¨gle gÃ©nÃ©rale, Envoy agit comme un plan de donnÃ©es, c'est-Ã -dire qu'il Ã©quilibre directement le trafic, et il existe Ã©galement un plan de contrÃ´le qui fournit des informations sur ce dont vous avez besoin pour rÃ©partir la charge entre (dÃ©couverte de service, etc.). <br><br>  Je vais vous dire quelques mots sur ses petits pains. <br><br>  Pour augmenter les chances de rÃ©ussite d'une nouvelle tentative de rÃ©ponse la prochaine fois que vous essayez, vous pouvez dormir un peu et attendre que les backends reprennent leurs esprits.  De cette faÃ§on, nous traiterons les problÃ¨mes de base de donnÃ©es courts.  Envoy a un <strong>dÃ©lai d'attente pour les relances</strong> - fait une pause entre les relances.  De plus, l'intervalle de retard entre les tentatives augmente de faÃ§on exponentielle.  La premiÃ¨re nouvelle tentative se produit aprÃ¨s 0-24 ms, la seconde aprÃ¨s 0-74 ms, puis pour chaque tentative suivante, l'intervalle augmente et le retard spÃ©cifique est sÃ©lectionnÃ© de maniÃ¨re alÃ©atoire dans cet intervalle. <br><br>  La seconde approche n'est pas spÃ©cifique Ã  l'Envoy, mais un modÃ¨le appelÃ© <strong>Circuit breaking</strong> (disjoncteur ou fusible).  Lorsque notre backend s'Ã©mousse, en fait, nous essayons de le terminer Ã  chaque fois.  En effet, les utilisateurs dans une situation incomprÃ©hensible cliquent sur la page d'actualisation, vous envoyant de plus en plus de nouvelles demandes.  Vos Ã©quilibreurs deviennent nerveux, envoient de nouvelles tentatives, le nombre de demandes augmente - la charge augmente, et dans cette situation, ce serait bien de ne pas envoyer de demandes. <br><br>  Le disjoncteur vous permet simplement de dÃ©terminer que nous sommes dans cet Ã©tat, de tirer rapidement l'erreur et de donner aux backends Â«le souffle coupÃ©Â». <br><br><img src="https://habrastorage.org/webt/xb/mm/i8/xbmmi88cqacoqvkzdmujynq6da0.gif"><br>  <em>Disjoncteur (hystrix like libs),</em> <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="><em>original</em></a> <em>sur le blog d'ebay.</em> <br><br>  Ci-dessus, le circuit du disjoncteur Hystrix.  Hystrix est la bibliothÃ¨que Java de Netflix conÃ§ue pour implÃ©menter des modÃ¨les de tolÃ©rance aux pannes. <br><br><ul><li>  Le Â«fusibleÂ» peut Ãªtre Ã  l'Ã©tat Â«fermÃ©Â» lorsque toutes les demandes sont envoyÃ©es au backend et qu'il n'y a aucune erreur. </li><li>  Lorsqu'un certain seuil de dÃ©faillance est dÃ©clenchÃ©, c'est-Ã -dire que certaines erreurs se sont produites, le disjoncteur passe Ã  l'Ã©tat Â«OuvertÂ».  Il renvoie rapidement une erreur au client et les demandes ne parviennent pas au backend. </li><li>  Une fois dans un certain laps de temps, une petite partie des demandes est toujours envoyÃ©e au backend.  Si une erreur est dÃ©clenchÃ©e, l'Ã©tat reste "Ouvert".  Si tout commence Ã  bien fonctionner et Ã  rÃ©pondre, le Â«fusibleÂ» se ferme et le travail continue. </li></ul><br>  Dans Envoy, en tant que tel, ce n'est pas tout.  Il existe des limites de niveau supÃ©rieur sur le fait qu'il ne peut y avoir plus de N demandes pour un groupe en amont spÃ©cifique.  Si plus, quelque chose ne va pas ici - nous retournons une erreur.  Il ne peut plus y avoir N de nouvelles tentatives actives (c'est-Ã -dire des tentatives qui se produisent actuellement). <br><br>  Vous n'avez pas eu de nouvelles tentatives, quelque chose a explosÃ© - envoyez des nouvelles tentatives.  Envoy comprend que plus de N est anormal et toutes les demandes doivent Ãªtre traitÃ©es avec une erreur. <br><br>  <strong>Disjoncteur [EnvoyÃ©]</strong> <br><br><ul><li>  Nombre maximal de connexions de cluster (groupe en amont) </li><li>  Nombre maximal de demandes en attente du cluster </li><li>  Nombre maximal de demandes de cluster </li><li>  Nouvelles tentatives actives du cluster </li></ul><br>  Cette chose simple fonctionne bien, elle est configurable, vous n'avez pas Ã  proposer de paramÃ¨tres spÃ©ciaux et les paramÃ¨tres par dÃ©faut sont assez bons. <br><br><h4>  Disjoncteur: notre expÃ©rience <br></h4><br>  Auparavant, nous avions un collecteur de mÃ©triques HTTP, c'est-Ã -dire que les agents installÃ©s sur les serveurs de nos clients envoyaient des mÃ©triques Ã  notre cloud via HTTP.  Si nous rencontrons des problÃ¨mes dans l'infrastructure, l'agent Ã©crit les mÃ©triques sur son disque puis essaie de nous les envoyer. <br><br>  Et les agents tentent constamment de nous envoyer des donnÃ©es, ils ne sont pas fÃ¢chÃ©s que nous rÃ©pondions de maniÃ¨re incorrecte et ne partons pas. <br><br>         (       ,      )  ,     ,           . <br><br>            nginx limit req.    ,    , , 200 RPS.       ,   ,          ,   limit req. <br><br>           TCP     HTTP (  nginx limit req).            .      limit req . <br><br>     ,      ,  .   <strong> </strong>  Circuit breaker,  ,     N  ,   ,   - ,   ,  .   ,    ,      spool  . <br><br>  <strong></strong>   Circuit breaker       + request cancellation ( ).  ,    N   Cassandra, N   Elastic,  ,    â€”   ,         .      â€” ,   . <br><br><img src="https://habrastorage.org/webt/jo/l2/jk/jol2jk44vlcmz3twgvr0jgtpbii.png"><br><br><img src="https://habrastorage.org/webt/1a/7b/x4/1a7bx4yq20uehqoagd4tutqxgmw.png"><br><br>    ,         (:  â€”  Â«Â»,  â€” Â«Â»). ,      800 RPS    20-30.     Â«Â», ,    . <br><br><h2>    <br></h2><br>    â€”  ,  . <br><br>         ,   ,        â€”      .    . <br><br>  ,       , ,      ,   Health checks â€” HTTP 200. <br><br>    . <br><br><img src="https://habrastorage.org/webt/yu/cy/9s/yucy9sofdr-z7brvjedmnxt4_gc.png"><br><br>     Load Balancer, 3 ,         Cassandra.      Cassandra,   Cassandra   ,    Cassandra     data noda. <br><br>   â€”    : <strong>kernel: NETDEV WATCHDOG: eth0 (ixgbe): transmit queue 3 timed out.</strong> <br><br>   :     (    ),    64     . , 1/64   .     reboot,    . <br><br> ,  ,    ,      .  , ,        ,            .   ,    ,   .     ,   . <br><br> <strong>Cassandra: coordinator -&gt; nodes</strong> <br><br>  Cassandra,      (speculative retries),      .    latency  99 ,         . <br><br> <strong>App -&gt; cassandra coordinator</strong> <br><br>     .     Cassandra      Â«Â» ,    ,  ,  latency  .. <br><br>      gocql â€”   cassandra client.       .   HostSelectionPolicy,       <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">bitly/go-hostpool</a> .    Epsilon greedy  ,       . <br><br>     ,    <strong>Epsilon-greedy</strong> . <br><br>      (multi-armed bandit):       ,     ,     N     . <br><br>    : <br><br><ol><li>  Â« <strong>exploreÂ»</strong> â€”   : 10    ,  ,   . <br></li><li>  Â« <strong>exploitÂ»</strong> â€”      . <br></li></ol><br> ,    (10 â€” 30%)   <strong>round</strong> - <strong>robin</strong>    ,  ,  ,  .  70 â€” 90%        . <br><br> Host-pool          .         .        (    â€” ,    ,  ).      .      ,     , ,       . <br><br><h2>   </h2><br>   Â«Â» ()   â€”Cassandra  Cassandra coordinator-data.     (nginx, Envoy â€”  )    Â«Â» Application,     Cassandra  ,       ,      . <br><br>  Envoy    <strong>Outlier detection</strong> : <br><br><ul><li> Consecutive http-5xx. </li><li> Consecutive gateway errors (502,503,504). </li><li> Success rate. </li></ul><br>   Â«Â»  ,    -  ,   .   ,    .        â€”    ,   ,     .  ,    ,          . <br><br>   ,       Â«Â»,   max_ejection_percent.    ,      outlier,     .  ,    70%  â€”  ,   â€” , ! <br><br>      ,       â€” ! <br><br><h2>  <br></h2><br> ,     ,      .  ,       latency    , : <br><br><ul><li>  ,        .. </li><li>    ,      -,       . </li></ul><br> ,  <strong>    </strong> ,   .  ,      ,     ,         â€”  ,    . <br><br> <strong>      </strong> .  99%     nginx/ <s>HAProxy</s> /Envoy.   proxy ,           Â«Â». <br><br> <strong>    proxy</strong> (   HAProxy:)), <strong>  ,    .</strong> <br><br><blockquote>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">DevOpsConf Russia</a>      Kubernetes         .          <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a> . <br><br>    ,       â€” <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u="></a>      DevOps. <br><br>    ,   ,   <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">YouTube-</a> â€”              . <br></blockquote></div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr423085/">https://habr.com/ru/post/fr423085/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr423073/index.html">SystÃ¨me de fichiers interplanÃ©taire - hachage trivial (identitÃ©), bloc DAG et tampons de protocole</a></li>
<li><a href="../fr423075/index.html">Pourquoi les directeurs financiers sont-ils si dÃ©sireux de traduire les dÃ©penses d'investissement en informatique en opÃ©rations</a></li>
<li><a href="../fr423077/index.html">Guide de l'assembleur X86 pour les dÃ©butants</a></li>
<li><a href="../fr423079/index.html">Points clÃ©s d'une entrevue avec Elon Musk chez Joe Rogan</a></li>
<li><a href="../fr423083/index.html">Comment je suis devenu dÃ©veloppeur chez ABBYY</a></li>
<li><a href="../fr423087/index.html">Ne me pousse pas dans les yeux</a></li>
<li><a href="../fr423089/index.html">Programmeurs Ã  MBLT DEV 2018</a></li>
<li><a href="../fr423091/index.html">Flutter pour les dÃ©veloppeurs Android. Comment crÃ©er une interface utilisateur pour une activitÃ© Ã  l'aide de Flutter</a></li>
<li><a href="../fr423093/index.html">Nous augmentons le caractÃ¨re alÃ©atoire du fait que [probablement] [presque] par accident</a></li>
<li><a href="../fr423095/index.html">NouveautÃ©s d'Apple Presentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>