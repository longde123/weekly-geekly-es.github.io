<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>💽 🅰️ 🙌🏼 Teori dan praktik menggunakan HBase 🤳🏼 🐐 🧙🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Selamat siang Nama saya Danil Lipova, tim kami di Sbertech mulai menggunakan HBase sebagai repositori data operasional. Selama studinya, pengalaman di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Teori dan praktik menggunakan HBase</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/sberbank/blog/420425/">  Selamat siang  Nama saya Danil Lipova, tim kami di Sbertech mulai menggunakan HBase sebagai repositori data operasional.  Selama studinya, pengalaman diperoleh bahwa saya ingin mensistematisasikan dan menggambarkan (kami berharap itu akan bermanfaat bagi banyak orang).  Semua percobaan di bawah ini dilakukan dengan versi HBase 1.2.0-cdh5.14.2 dan 2.0.0-cdh6.0.0-beta1. <br><br><ol><li>  Arsitektur umum </li><li>  Menulis data ke HBASE </li><li>  Membaca data dari HBASE </li><li>  Caching data </li><li>  Pemrosesan Batch MultiGet / MultiPut </li><li>  Strategi memecah meja menjadi daerah (tumpah) </li><li>  Toleransi kesalahan, pemadatan dan lokalitas data </li><li>  Pengaturan dan kinerja </li><li>  Uji beban </li><li>  Kesimpulan </li></ol><a name="habracut"></a><br><h2>  1. Arsitektur umum </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/y9/wa/vl/y9wavltynzhs9r_7v5sn1d8ff68.png"></div><br>  Master siaga mendengarkan detak jantung aktif pada simpul ZooKeeper dan, jika menghilang, mengambil alih fungsi master. <br><br><h2>  2. Menulis data ke HBASE </h2><br>  Pertama, pertimbangkan case paling sederhana - menulis objek nilai kunci ke tabel tertentu menggunakan put (rowkey).  Klien pertama-tama harus mencari tahu di mana root region server (RRS) yang menyimpan tabel hbase: meta berada.  Dia menerima informasi ini dari ZooKeeper.  Kemudian dia beralih ke RRS dan membaca tabel hbase: meta, dari mana dia mengambil informasi yang RegionServer (RS) bertanggung jawab untuk menyimpan data untuk kunci baris yang diberikan dalam tabel yang menarik baginya.  Untuk penggunaan di masa mendatang, tabel meta di-cache oleh klien dan oleh karena itu panggilan selanjutnya menjadi lebih cepat, langsung ke RS. <br><br>  Kemudian RS, setelah menerima permintaan tersebut, pertama-tama menulisnya ke WriteAheadLog (WAL), yang diperlukan untuk pemulihan jika terjadi kerusakan.  Kemudian menyimpan data di MemStore.  Ini adalah buffer dalam memori yang berisi sekumpulan tombol yang diurutkan untuk wilayah tertentu.  Tabel dapat dibagi menjadi daerah (partisi), yang masing-masing berisi sekumpulan kunci yang terpisah.  Ini memungkinkan penempatan wilayah pada server yang berbeda untuk mendapatkan kinerja yang lebih tinggi.  Namun, meskipun pernyataan ini jelas, kita akan melihat nanti bahwa ini tidak berfungsi dalam semua kasus. <br><br>  Setelah menempatkan catatan di MemStore, klien menerima respons bahwa catatan berhasil disimpan.  Pada saat yang sama, itu benar-benar disimpan hanya dalam buffer dan sampai ke disk hanya setelah periode waktu tertentu berlalu atau ketika diisi dengan data baru. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xt/xi/p7/xtxip7moylyjdgqggsqiu8j_cm4.png"></div><br>  Saat melakukan operasi "Hapus", penghapusan data fisik tidak terjadi.  Mereka hanya ditandai sebagai dihapus, dan kehancuran itu sendiri terjadi ketika fungsi kompak utama disebut, yang dijelaskan secara lebih rinci dalam Bagian 7. <br><br>  File dalam format HFile diakumulasikan dalam HDFS dan dari waktu ke waktu proses compact minor dimulai, yang hanya menempelkan file kecil menjadi yang lebih besar tanpa menghapus apa pun.  Seiring waktu, ini berubah menjadi masalah yang memanifestasikan dirinya hanya ketika membaca data (kami akan kembali ke sini nanti). <br><br>  Selain proses boot yang dijelaskan di atas, ada prosedur yang jauh lebih efisien, yang mungkin merupakan sisi paling kuat dari database ini - BulkLoad.  Terdiri dari fakta bahwa kami secara mandiri membuat HFiles dan meletakkannya di disk, yang memungkinkan kami untuk menskala dengan sempurna dan mencapai kecepatan yang sangat baik.  Faktanya, batasan di sini bukanlah HBase, tetapi kemungkinan zat besi.  Di bawah ini adalah hasil memuat pada sebuah cluster yang terdiri dari 16 RegionServers dan 16 NodeManager YARN (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 utas), versi HBase 1.2.0-cdh5.14.2. <br><br><img src="https://habrastorage.org/webt/ro/bu/hf/robuhfegpwqyed6gmwg7he2bmfk.png"><br><br>  Dapat dilihat bahwa dengan meningkatkan jumlah partisi (wilayah) dalam tabel, serta Spark yang dapat dieksekusi, kami memperoleh peningkatan kecepatan pengunduhan.  Selain itu, kecepatan tergantung pada jumlah rekaman.  Blok besar memberikan peningkatan dalam pengukuran MB / detik, yang kecil dalam jumlah catatan yang dimasukkan per satuan waktu, semua hal lain dianggap sama. <br><br>  Anda juga dapat mulai memuat ke dalam dua tabel sekaligus dan mendapatkan kecepatan dua kali lipat.  Dapat dilihat di bawah ini bahwa 10 blok KB ditulis ke dua tabel sekaligus dengan kecepatan masing-masing sekitar 600 Mb / s (total 1275 Mb / s), yang bertepatan dengan kecepatan tulis 623 MB / s ke satu tabel (lihat No. 11 di atas) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gr/05/vp/gr05vpmhauzclwmn310erbgj22u.png"></div><br>  Tetapi peluncuran kedua dengan catatan 50 KB menunjukkan bahwa kecepatan unduh sudah sedikit tumbuh, yang menunjukkan perkiraan terhadap nilai batas.  Harus diingat bahwa praktis tidak ada beban pada HBASE itu sendiri, yang diperlukan hanyalah memberikan data dari hbase: meta, dan setelah melapisi HFiles, siram data BlockCache dan simpan buffer MemStore ke disk jika tidak kosong <br><br><h2>  3. Membaca data dari HBASE </h2><br>  Jika kami mengasumsikan bahwa semua informasi dari hbase: meta sudah memiliki klien (lihat bagian 2), maka permintaan segera dikirim ke RS tempat kunci yang diinginkan disimpan.  Pertama pencarian dilakukan di MemCache.  Terlepas dari apakah ada data di sana atau tidak, pencarian juga dilakukan dalam buffer BlockCache dan, jika perlu, di HFiles.  Jika data ditemukan dalam file, maka itu ditempatkan di BlockCache dan akan dikembalikan lebih cepat pada permintaan berikutnya.  Pencarian HFile relatif cepat karena penggunaan filter Bloom, mis.  Setelah membaca sejumlah kecil data, ia segera menentukan apakah file ini berisi kunci yang diinginkan dan, jika tidak, kemudian melanjutkan ke yang berikutnya. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/8-/zz/wz/8-zzwzcoed7obxmlgzxl3ihrzzw.png"></div><br>  Setelah menerima data dari ketiga sumber ini, RS membentuk tanggapan.  Secara khusus, itu dapat mentransfer beberapa versi objek yang ditemukan sekaligus jika klien meminta versi. <br><br><h2>  4. Caching data </h2><br>  Buffer MemStore dan BlockCache menempati hingga 80% dari memori RS on-heap yang dialokasikan (sisanya dicadangkan untuk tugas layanan RS).  Jika mode penggunaan khas sedemikian sehingga proses menulis dan segera membaca data yang sama, maka masuk akal untuk mengurangi BlockCache dan meningkatkan MemStore, karena  ketika menulis data ke cache baca tidak jatuh, maka penggunaan BlockCache akan terjadi lebih jarang.  Buffer BlockCache terdiri dari dua bagian: LruBlockCache (selalu on-heap) dan BucketCache (biasanya off-heap atau pada SSD).  BucketCache harus digunakan ketika ada banyak permintaan baca dan mereka tidak cocok dengan LruBlockCache, yang mengarah ke pekerjaan aktif Pengumpul Sampah.  Pada saat yang sama, Anda seharusnya tidak mengharapkan peningkatan radikal dalam kinerja dari menggunakan cache baca, tetapi kami akan kembali ke ini di Bagian 8 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rq/oe/nw/rqoenwgqtngb-a37gsof7sqbgn0.png"></div><br>  BlockCache adalah satu untuk seluruh RS, dan MemStore memiliki sendiri untuk setiap tabel (satu untuk setiap Keluarga Kolom). <br><br>  Seperti yang <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">dijelaskan</a> dalam teori, ketika menulis data tidak masuk ke cache, dan memang, parameter tersebut CACHE_DATA_ON_WRITE untuk tabel dan "Cache DATA on Write" untuk RS disetel ke false.  Namun, dalam praktiknya, jika Anda menulis data ke MemStore, lalu menggelontorkannya ke disk (membersihkannya dengan cara ini), lalu hapus file yang dihasilkan, kemudian dengan menjalankan permintaan get, kami akan berhasil menerima data.  Dan bahkan jika Anda sepenuhnya menonaktifkan BlockCache dan mengisi tabel dengan data baru, kemudian mendapatkan MemStore ke disk, hapus mereka dan minta dari sesi lain, mereka masih akan diambil dari suatu tempat.  Jadi HBase tidak hanya menyimpan data, tetapi juga teka-teki misterius. <br><br><pre><code class="bash hljs">hbase(main):001:0&gt; create <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf'</span></span> Created table ns:magic Took 1.1533 seconds hbase(main):002:0&gt; put <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span>, <span class="hljs-string"><span class="hljs-string">'cf:c'</span></span>, <span class="hljs-string"><span class="hljs-string">'try_to_delete_me'</span></span> Took 0.2610 seconds hbase(main):003:0&gt; flush <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span> Took 0.6161 seconds hdfs dfs -mv /data/hbase/data/ns/magic/* /tmp/trash hbase(main):002:0&gt; get <span class="hljs-string"><span class="hljs-string">'ns:magic'</span></span>, <span class="hljs-string"><span class="hljs-string">'key1'</span></span> cf:c timestamp=1534440690218, value=try_to_delete_me</code> </pre> <br>  Cache DATA on Read diatur ke false.  Jika Anda punya ide, selamat datang untuk membahas ini di komentar. <br><br><h2>  5. Pemrosesan Batch Data MultiGet / MultiPut </h2><br>  Memproses permintaan tunggal (Get / Put / Delete) adalah operasi yang agak mahal, jadi Anda harus menggabungkannya sebanyak mungkin dalam Daftar atau Daftar, yang memungkinkan Anda untuk mendapatkan peningkatan kinerja yang signifikan.  Ini terutama berlaku untuk operasi penulisan, tetapi ketika membaca ada jebakan berikut.  Grafik di bawah ini menunjukkan waktu pembacaan 50.000 catatan dari MemStore.  Membaca dibuat dalam satu aliran dan sumbu horizontal menunjukkan jumlah tombol dalam permintaan.  Dapat dilihat bahwa saat Anda menambah hingga seribu kunci dalam satu permintaan, waktu eksekusi turun, mis.  kecepatan meningkat.  Namun, ketika mode MSLAB dihidupkan secara default, setelah ambang ini, penurunan dramatis dalam kinerja dimulai, dan semakin besar jumlah data dalam catatan, semakin lama waktunya. <br><br><img src="https://habrastorage.org/webt/1k/ic/hj/1kichjm1xdpxskbx7avzppmlqty.png"><br><br>  Pengujian dilakukan pada mesin virtual, 8 core, HBase versi 2.0.0-cdh6.0.0-beta1. <br><br>  Mode MSLAB dirancang untuk mengurangi fragmentasi tumpukan, yang terjadi karena pencampuran data generasi baru dan lama.  Sebagai solusi untuk masalah ketika MSLAB diaktifkan, data ditempatkan dalam sel yang relatif kecil (chunk) dan diproses dalam batch.  Akibatnya, ketika volume dalam paket data yang diminta melebihi ukuran yang dialokasikan, kinerja turun tajam.  Di sisi lain, mematikan mode ini juga tidak disarankan, karena akan menyebabkan berhenti karena GC selama saat-saat kerja intensif dengan data.  Jalan keluar yang baik adalah meningkatkan volume sel, dalam hal penulisan aktif via put bersamaan dengan membaca.  Perlu dicatat bahwa masalah tidak terjadi jika, setelah merekam, jalankan perintah flush yang mem-flush MemStore ke disk atau jika pemuatan dilakukan menggunakan BulkLoad.  Tabel di bawah ini menunjukkan bahwa permintaan dari data MemStore dengan volume yang lebih besar (dan jumlah yang sama) menyebabkan perlambatan.  Namun, meningkatkan chunksize mengembalikan waktu pemrosesan menjadi normal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zb/jq/s3/zbjqs3tou2ywnnzc16p93fttbva.png"></div><br>  Selain meningkatkan chunksize, fragmentasi data menurut wilayah membantu, yaitu  membelah meja.  Ini mengarah pada fakta bahwa lebih sedikit permintaan datang ke masing-masing daerah dan jika mereka ditempatkan di sel, responsnya tetap baik. <br><br><h2>  6. Strategi membagi meja menjadi daerah (pemotongan) </h2><br>  Karena HBase adalah penyimpanan nilai-kunci dan partisi dilakukan oleh kunci, sangat penting untuk berbagi data secara merata di semua wilayah.  Misalnya, mempartisi tabel seperti itu menjadi tiga bagian akan menghasilkan data yang dibagi menjadi tiga wilayah: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/4c/9d/rg4c9dm-gbtodx0dqr3quc9h6we.png"></div><br>  Kebetulan ini mengarah ke pelambatan tajam jika data yang dimuat di masa depan akan terlihat seperti, misalnya, nilai-nilai panjang, yang sebagian besar dimulai dengan angka yang sama, misalnya: <br><br>  1000001 <br>  1000002 <br>  ... <br>  1100003 <br><br>  Karena kunci disimpan sebagai array byte, semuanya akan memulai dengan cara yang sama dan termasuk ke wilayah yang sama # 1 yang menyimpan rentang kunci ini.  Ada beberapa strategi split: <br><br>  HexStringSplit - Mengubah kunci menjadi string dengan pengkodean heksadesimal dalam kisaran "00000000" =&gt; "FFFFFFFF" dan mengisinya dengan nol di sebelah kiri. <br><br>  UniformSplit - Mengubah kunci menjadi array byte dengan pengkodean heksadesimal dalam rentang "00" =&gt; "FF" dan mengisinya dengan nol di sebelah kanan. <br><br>  Selain itu, Anda dapat menentukan rentang atau set kunci apa saja untuk dipisah dan mengatur pemisahan otomatis.  Namun, salah satu pendekatan paling sederhana dan paling efektif adalah UniformSplit dan penggunaan gabungan hash, misalnya, sepasang byte tinggi dari menjalankan kunci melalui fungsi CRC32 (rowkey) dan rowkey itu sendiri: <br><br>  hash + rowkey <br><br>  Kemudian semua data akan didistribusikan secara merata di seluruh wilayah.  Saat membaca, dua byte pertama dibuang begitu saja dan kunci aslinya tetap ada.  RS juga mengontrol jumlah data dan kunci di wilayah tersebut dan ketika batas terlampaui, secara otomatis memecahnya menjadi beberapa bagian. <br><br><h2>  7. Toleransi kesalahan dan lokalitas data </h2><br>  Karena hanya satu wilayah yang bertanggung jawab untuk setiap rangkaian kunci, solusi untuk masalah yang terkait dengan crash atau decommissioning RS adalah dengan menyimpan semua data yang diperlukan dalam HDFS.  Ketika RS crash, master mendeteksi ini melalui tidak adanya detak jantung pada node ZooKeeper.  Kemudian ia menugaskan wilayah yang dilayani ke RS lain dan karena HFiles disimpan dalam sistem file terdistribusi, host baru membacanya dan terus melayani data.  Namun, karena beberapa data mungkin ada di MemStore dan tidak punya waktu untuk masuk ke HFiles, WAL, yang juga disimpan dalam HDFS, digunakan untuk memulihkan sejarah operasi.  Setelah roll-over dari perubahan, RS mampu menanggapi permintaan, namun, langkah itu mengarah pada fakta bahwa bagian dari data dan proses mereka berada pada node yang berbeda, yaitu.  penurunan lokalitas. <br><br>  Solusi untuk masalah ini adalah pemadatan besar - prosedur ini memindahkan file ke node yang bertanggung jawab untuk mereka (di mana wilayah mereka berada), sebagai akibatnya beban pada jaringan dan disk meningkat tajam selama prosedur ini.  Namun, di masa depan, akses ke data terasa dipercepat.  Selain itu, major_compaction menggabungkan semua HFiles ke dalam satu file di kawasan, dan juga membersihkan data tergantung pada pengaturan tabel.  Misalnya, Anda dapat menentukan jumlah versi dari objek yang ingin Anda simpan atau masa pakainya, setelah itu objek dihapus secara fisik. <br><br>  Prosedur ini dapat memiliki efek yang sangat positif pada HBase.  Gambar di bawah ini menunjukkan bagaimana kinerja menurun akibat perekaman data aktif.  Di sini Anda dapat melihat bagaimana 40 stream ditulis ke satu tabel dan 40 stream membaca data pada saat yang sama.  Menulis stream membentuk semakin banyak HFiles, yang dibaca oleh stream lain.  Akibatnya, semakin banyak data yang perlu dihapus dari memori dan pada akhirnya GC mulai bekerja, yang secara praktis melumpuhkan semua pekerjaan.  Peluncuran pemadatan besar menyebabkan pembersihan penyumbatan yang dihasilkan dan pemulihan kinerja. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/x2/ha/ga/x2haga1cohdfilxz5ffu_vatfzy.png"></div><br>  Pengujian dilakukan pada 3 DataNode dan 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 thread).  HBase Versi 1.2.0-cdh5.14.2 <br><br>  Perlu dicatat bahwa peluncuran pemadatan besar dilakukan pada tabel “langsung”, di mana data ditulis dan dibaca secara aktif.  Ada pernyataan di jaringan bahwa ini dapat menyebabkan jawaban yang salah saat membaca data.  Untuk memeriksa, sebuah proses diluncurkan yang menghasilkan data baru dan menulisnya di atas meja.  Setelah itu saya langsung membaca dan memverifikasi apakah nilai yang didapat sesuai dengan yang dicatat.  Selama proses ini, pemadatan besar diluncurkan sekitar 200 kali dan tidak ada satu kegagalan pun yang tercatat.  Mungkin masalah jarang muncul dan hanya selama beban tinggi, jadi lebih aman untuk tetap menghentikan proses penulisan dan baca secara terjadwal dan melakukan pembersihan tanpa mengizinkan penarikan GC seperti itu. <br><br>  Selain itu, pemadatan besar tidak memengaruhi status MemStore, untuk mem-flush-nya ke disk dan memadat, Anda perlu menggunakan flush (connection.getAdmin (). Flush (TableName.valueOf (tblName)))). <br><br><h2>  8. Pengaturan dan kinerja </h2><br>  Seperti yang telah disebutkan, HBase menunjukkan keberhasilan terbesar di mana ia tidak perlu melakukan apa pun ketika menjalankan BulkLoad.  Namun, ini berlaku untuk sebagian besar sistem dan orang.  Namun, alat ini lebih cocok untuk penumpukan massal data dalam blok besar, sedangkan jika prosesnya membutuhkan banyak permintaan baca dan tulis yang bersaing, perintah Get and Put yang dijelaskan di atas digunakan.  Untuk menentukan parameter optimal, peluncuran dilakukan dengan berbagai kombinasi parameter tabel dan pengaturan: <br><br><ul><li>  10 utas dimulai pada waktu yang sama 3 kali berturut-turut (sebut saja blok utas). </li><li>  Waktu operasi semua aliran di blok adalah rata-rata dan merupakan hasil akhir dari operasi blok. </li><li>  Semua utas bekerja dengan tabel yang sama. </li><li>  Sebelum setiap dimulainya blok utas, pemadatan besar dilakukan. </li><li>  Setiap blok hanya melakukan satu dari operasi berikut: </li></ul><br>  - Taruh <br>  - dapatkan <br>  - Dapatkan + Pasang <br><br><ul><li>  Setiap blok melakukan 50.000 pengulangan operasinya. </li><li>  Ukuran rekaman dalam blok adalah 100 byte, 1000 byte atau 10.000 byte (acak). </li><li>  Blok diluncurkan dengan jumlah berbeda dari kunci yang diminta (baik satu kunci atau 10). </li><li>  Blok diluncurkan pada berbagai pengaturan tabel.  Parameter berubah: </li></ul><br>  - BlockCache = dihidupkan atau dimatikan <br>  - BlockSize = 65 Kb atau 16 Kb <br>  - Partisi = 1, 5 atau 30 <br>  - MSLAB = hidup atau mati <br><br>  Jadi, bloknya terlihat seperti ini: <br><br>  a.  Mode MSLAB dihidupkan / dimatikan. <br>  b.  Tabel dibuat dengan parameter yang ditetapkan: BlockCache = true / none, BlockSize = 65/16 Kb, Partitions = 1/5/30. <br>  c.  Atur kompresi GZ. <br>  d.  10 utas diluncurkan secara bersamaan melakukan 1/10 operasi put / get / get + put di tabel ini dengan catatan 100/1000/10000 byte, mengeksekusi 50.000 kueri berturut-turut (kunci acak). <br>  e.  Poin d diulang tiga kali. <br>  f.  Waktu operasi semua utas adalah rata-rata. <br><br>  Semua kemungkinan kombinasi diperiksa.  Dapat diprediksi bahwa seiring dengan meningkatnya ukuran perekaman, kecepatan akan turun atau penonaktifan caching akan melambat.  Namun, tujuannya adalah untuk memahami tingkat dan signifikansi pengaruh masing-masing parameter, oleh karena itu, data yang dikumpulkan diumpankan ke input fungsi regresi linier, yang memungkinkan untuk mengevaluasi keandalan menggunakan t-statistik.  Di bawah ini adalah hasil dari blok yang melakukan operasi Put.  Seperangkat kombinasi lengkap 2 * 2 * 3 * 2 * 3 = 144 opsi + 72 sejak  beberapa dilakukan dua kali.  Oleh karena itu, total 216 peluncuran: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ai/5u/wj/ai5uwj0fvmyo9hyqkjg4-cigceq.png"></div><br>  Pengujian dilakukan pada mini-cluster yang terdiri dari 3 DataNode dan 4 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 stream).  HBase versi 1.2.0-cdh5.14.2. <br><br>  Kecepatan penyisipan tertinggi 3,7 detik diperoleh ketika mode MSLAB dimatikan, di atas meja dengan satu partisi, dengan BlockCache diaktifkan, BlockSize = 16, catatan 100 byte dari 10 buah per paket. <br>  Kecepatan penyisipan terendah 82,8 detik diperoleh ketika mode MSLAB diaktifkan, di atas meja dengan satu partisi, dengan BlockCache diaktifkan, BlockSize = 16, catatan masing-masing 10.000 byte. <br><br>  Sekarang mari kita lihat modelnya.  Kami melihat model kualitas yang baik untuk R2, tetapi jelas bahwa ekstrapolasi dikontraindikasikan di sini.  Perilaku aktual sistem ketika mengubah parameter tidak akan linier, model ini tidak diperlukan untuk perkiraan, tetapi untuk memahami apa yang terjadi dalam parameter yang diberikan.  Sebagai contoh, di sini kita melihat dengan kriteria Siswa bahwa untuk operasi Put, parameter BlockSize dan BlockCache tidak masalah (yang umumnya dapat diprediksi): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/aq/vg/xt/aqvgxt9uyfs_l4m4crnm3adhliy.png"></div><br>  Tetapi fakta bahwa peningkatan jumlah partisi menyebabkan penurunan kinerja agak tak terduga (kita telah melihat efek positif dari peningkatan jumlah partisi dengan BulkLoad), meskipun hal itu dapat dimengerti.  Pertama, untuk pemrosesan, perlu untuk membentuk kueri ke 30 wilayah, bukan satu, dan jumlah data tidak sedemikian sehingga memberikan keuntungan.  Kedua, total waktu operasi ditentukan oleh RS paling lambat, dan karena jumlah DataNode kurang dari jumlah RS, beberapa daerah memiliki nol lokalitas.  Baiklah, mari kita lihat lima besar: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bx/md/iu/bxmdiuzfdzqlfe1l8_s_2ktecb0.png"></div><br>  Sekarang mari kita mengevaluasi hasil dari pelaksanaan blok Dapatkan: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/zl/2b/no/zl2bnojdyx-byfpty6yr9poebzg.png"></div><br>  Jumlah partisi telah kehilangan signifikansi, yang mungkin disebabkan oleh fakta bahwa data di-cache dengan baik dan cache baca adalah parameter yang paling signifikan (secara statistik).  Secara alami, meningkatkan jumlah pesan dalam permintaan juga sangat berguna untuk kinerja.  Hasil terbaik: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f0/6d/pu/f06dpurnzlck4po4jw1xpyrphl8.png"></div><br>  Nah, akhirnya, lihat model blok yang dieksekusi get first, lalu taruh: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/fc/23/aefc23q9mcfbtbumdc5qjmw_qrg.png"></div><br>  Di sini semua parameter signifikan.  Dan hasil dari para pemimpin: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q0/vo/th/q0vothoybhc8k6m1ysefviz3tco.png"></div><br><h2>  9. Memuat pengujian </h2><br>  Yah, akhirnya, kami akan meluncurkan muatan yang lebih atau kurang layak, tetapi selalu lebih menarik ketika ada sesuatu untuk dibandingkan.  Situs DataStax, pengembang kunci Cassandra, memiliki <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=id&amp;u=">hasil</a> NT dari sejumlah repositori NoSQL, termasuk HBase versi 0.98.6-1.  Pemuatan dilakukan oleh 40 stream, ukuran data 100 byte, disk SSD.  Hasil pengujian operasi Baca-Ubah-Tulis menunjukkan hasil ini. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ha/7b/bw/ha7bbwydc612f04jtwyqvdvg_ae.png"></div><br>  Seperti yang saya pahami, pembacaan dilakukan dalam blok 100 catatan dan selama 16 node HBase, tes DataStax menunjukkan kinerja 10 ribu operasi per detik. <br><br>  Sangat beruntung bahwa cluster kami juga memiliki 16 node, tetapi tidak terlalu "beruntung" yang masing-masing memiliki 64 core (utas), sedangkan uji DataStax hanya memiliki 4. Di sisi lain, mereka memiliki disk SSD, dan kami memiliki HDD dan lebih banyak lagi versi baru pemanfaatan HBase dan CPU selama pemuatan praktis tidak meningkat secara signifikan (secara visual sebesar 5-10 persen).  Namun demikian, kami akan mencoba memulai konfigurasi ini.  Pengaturan tabel secara default, pembacaan dilakukan dalam rentang tombol dari 0 hingga 50 juta secara acak (mis., Pada kenyataannya, setiap kali yang baru).  Dalam tabel, 50 juta entri dibagi menjadi 64 partisi.  Kunci hash crc32.  Pengaturan tabel default, MSLAB diaktifkan.  Mulai 40 utas, setiap utas membaca satu set 100 kunci acak dan segera menulis 100 byte yang dihasilkan pada kunci ini kembali. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/po/sd/el/posdel66zx7quvvo3kvrjb6uif8.png"></div><br>  Stand: 16 DataNode dan 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 stream).  HBase versi 1.2.0-cdh5.14.2. <br><br>  Hasil rata-rata lebih dekat dengan 40 ribu operasi per detik, yang secara signifikan lebih baik daripada dalam tes DataStax.  Namun, untuk keperluan percobaan, kondisinya dapat sedikit berubah.  Sangat tidak mungkin bahwa semua pekerjaan akan dilakukan secara eksklusif dengan satu meja, serta hanya dengan kunci yang unik.  Misalkan ada set kunci "panas" tertentu yang menghasilkan beban utama.  Oleh karena itu, kami akan mencoba membuat beban dengan catatan lebih besar (10 KB), juga dalam paket masing-masing 100, dalam 4 tabel yang berbeda dan membatasi rentang kunci yang diminta hingga 50 ribu. Grafik di bawah ini menunjukkan awal dari 40 utas, setiap aliran membaca satu set 100 kunci dan segera menulis secara acak 10 KB pada tombol ini kembali. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/ec/mr/f7ecmrebgulvlcyru05c7jzytyi.png"></div><br>  Stand: 16 DataNode dan 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 stream).  HBase versi 1.2.0-cdh5.14.2. <br><br>  Selama pemuatan, pemadatan besar diluncurkan beberapa kali, seperti yang ditunjukkan di atas tanpa prosedur ini, kinerja akan berangsur-angsur menurun, namun, beban tambahan juga terjadi selama eksekusi.  Penarikan disebabkan oleh berbagai alasan.  Kadang-kadang utas dihentikan dan sementara mereka memulai kembali ada jeda, kadang-kadang aplikasi pihak ketiga membuat beban pada kluster. <br><br>  Membaca dan menulis segera adalah salah satu skenario pekerjaan yang paling sulit untuk HBase.  Jika Anda hanya menaruh permintaan berukuran kecil, misalnya, masing-masing 100 byte, menggabungkannya menjadi 10 hingga 10 ribu lembar, Anda bisa mendapatkan ratusan ribu operasi per detik dan situasinya mirip dengan permintaan hanya baca.  Perlu dicatat bahwa hasilnya secara radikal lebih baik daripada yang diperoleh di DataStax terutama karena permintaan dalam blok 50 ribu. <br><br><img src="https://habrastorage.org/webt/kv/_j/bv/kv_jbvizskwbod1nxapokckg9s8.png"><br>  Stand: 16 DataNode dan 16 RS (CPU Xeon E5-2680 v4 @ 2.40GHz * 64 stream).  HBase versi 1.2.0-cdh5.14.2. <br><br><h2>  10. Kesimpulan </h2><br>  Sistem ini cukup fleksibel untuk dikonfigurasi, tetapi efek dari sejumlah besar parameter masih belum diketahui.  Beberapa dari mereka diuji, tetapi tidak termasuk dalam suite tes yang dihasilkan.  Misalnya, percobaan awal menunjukkan tidak signifikannya parameter seperti DATA_BLOCK_ENCODING, yang menyandikan informasi menggunakan nilai dari sel tetangga, yang cukup dapat dimengerti untuk data yang dihasilkan secara acak.  Dalam hal menggunakan sejumlah besar objek berulang, gain bisa signifikan.  Secara umum, kita dapat mengatakan bahwa HBase memberi kesan database yang agak serius dan dipikirkan dengan matang, yang bisa sangat produktif ketika berhadapan dengan blok data besar.  Terutama jika memungkinkan untuk menyebarkan proses membaca dan menulis dalam waktu. <br><br>  Jika sesuatu menurut Anda tidak cukup diungkapkan, saya siap untuk memberi tahu secara lebih rinci.  Kami sarankan untuk berbagi pengalaman atau berdebat jika Anda tidak setuju dengan sesuatu. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/id420425/">https://habr.com/ru/post/id420425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../id420409/index.html">Pelajari OpenGL. Pelajaran 5.7 - HDR</a></li>
<li><a href="../id420413/index.html">SQLite dan NW.js - petunjuk langkah demi langkah untuk menciptakan persahabatan yang kuat</a></li>
<li><a href="../id420415/index.html">Segala sesuatu yang ingin Anda ketahui tentang pengujian adaptor Wi-Fi, tetapi takut untuk bertanya</a></li>
<li><a href="../id420419/index.html">Pelari untuk mereka yang suka penghinaan atau bagaimana kami mengubah dan memodifikasi PixJam</a></li>
<li><a href="../id420423/index.html">Masalah antarmuka crossing darat</a></li>
<li><a href="../id420429/index.html">GUNAKAN, MERAH, PgBouncer, pengaturan dan pemantauannya</a></li>
<li><a href="../id420431/index.html">Mars Panduan Praktis untuk Terraforming untuk Ibu Rumah Tangga</a></li>
<li><a href="../id420433/index.html">"Format Jumat": jalan musikal - apa itu dan mengapa mereka tidak ada di Rusia</a></li>
<li><a href="../id420435/index.html">Mulai cepat dengan ARM Mbed: pengembangan mikrokontroler modern untuk pemula</a></li>
<li><a href="../id420437/index.html">Pengantar praktis untuk manajer paket untuk Kubernetes - Helm</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>