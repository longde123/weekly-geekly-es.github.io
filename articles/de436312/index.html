<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üó°Ô∏è üç¨ ‚ÜôÔ∏è Sprachsynthese f√ºr neuronale Netze unter Verwendung der Tacotron 2-Architektur oder ‚ÄûGet Alignment or Die Tryin '‚Äú üåï üêã „ÄΩÔ∏è</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Unser Team erhielt die Aufgabe, die Ergebnisse der Arbeit des k√ºnstlichen neuronalen Netzwerks f√ºr die Sprachsynthese zu wiederholen. Tacotron2- Autor...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Sprachsynthese f√ºr neuronale Netze unter Verwendung der Tacotron 2-Architektur oder ‚ÄûGet Alignment or Die Tryin '‚Äú</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/company/nix/blog/436312/"><img src="https://habrastorage.org/webt/yu/y0/2f/yuy02fd0i4fodpxxadfkf0k2ori.jpeg"><br><br>  Unser Team erhielt die Aufgabe, die Ergebnisse der Arbeit des k√ºnstlichen neuronalen Netzwerks f√ºr die Sprachsynthese zu wiederholen. <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Tacotron2-</a> Autorschaft DeepMind.  Dies ist eine Geschichte √ºber den dornigen Weg, den wir w√§hrend der Umsetzung des Projekts gegangen sind. <br><a name="habracut"></a><br>  Die Aufgabe der Computer-Sprachsynthese ist seit langem f√ºr Wissenschaftler und technische Experten von Interesse.  Klassische Methoden erlauben jedoch keine Synthese von Sprache, die nicht vom Menschen zu unterscheiden ist.  Und hier, wie in vielen anderen Bereichen, kam tiefes Lernen zur Rettung. <br><br>  Schauen wir uns klassische Synthesemethoden an. <br><br><h2>  Verkettete Sprachsynthese </h2><br>  Diese Methode basiert auf der Voraufzeichnung kurzer Audiofragmente, die dann kombiniert werden, um eine koh√§rente Sprache zu erzeugen.  Es stellt sich als sehr sauber und klar heraus, aber v√∂llig frei von emotionalen und intonationalen Komponenten, das hei√üt, es klingt unnat√ºrlich.  Und das alles, weil es unm√∂glich ist, eine Audioaufnahme aller m√∂glichen W√∂rter zu erhalten, die in allen m√∂glichen Kombinationen von Emotionen und Prosodie ausgesprochen werden.  Verkettungssysteme erfordern riesige Datenbanken und hartcodierte Kombinationen, um W√∂rter zu bilden.  Die Entwicklung eines zuverl√§ssigen Systems nimmt viel Zeit in Anspruch. <br><br><h2>  Parametrische Sprachsynthese </h2><br>  Verkettungs-TTS-Anwendungen sind aufgrund des hohen Datenbedarfs und der Entwicklungszeit begrenzt.  Daher wurde eine statistische Methode entwickelt, die die Art der Daten untersucht.  Es erzeugt Sprache durch Kombinieren von Parametern wie Frequenz, Amplitudenspektrum usw. <br><br>  Die parametrische Synthese besteht aus zwei Stufen. <br><br><ol><li>  Zun√§chst werden sprachliche Merkmale wie Phoneme, Dauer usw. aus dem Text extrahiert. </li><li>  Dann werden f√ºr den Vocoder (das System, das die Wellenform erzeugt) Zeichen extrahiert, die das entsprechende Sprachsignal darstellen: Cepstrum, Frequenz, lineares Spektrogramm, Kreidespektrogramm. </li><li>  Diese manuell konfigurierten Parameter werden zusammen mit sprachlichen Merkmalen in das Vocoder-Modell √ºbertragen und es werden viele komplexe Transformationen durchgef√ºhrt, um eine Schallwelle zu erzeugen.  Gleichzeitig wertet der Vocoder Sprachparameter wie Phase, Prosodie, Intonation und andere aus. </li></ol><br>  Wenn wir die Parameter approximieren k√∂nnen, die die Sprache f√ºr jede ihrer Einheiten definieren, k√∂nnen wir ein parametrisches Modell erstellen.  Die parametrische Synthese erfordert erheblich weniger Daten und harte Arbeit als verkettete Systeme. <br><br>  Theoretisch ist alles einfach, aber in der Praxis gibt es viele Artefakte, die zu ged√§mpfter Sprache mit einem ‚Äûsummenden‚Äú Klang f√ºhren, der √ºberhaupt nicht wie ein nat√ºrlicher Klang ist. <br><br>  Tatsache ist, dass wir in jeder Phase der Synthese einige Funktionen hart codieren und hoffen, eine realistisch klingende Sprache zu erhalten.  Die ausgew√§hlten Daten basieren jedoch auf unserem Sprachverst√§ndnis, und menschliches Wissen ist nicht absolut. Daher sind die verwendeten Zeichen nicht unbedingt die bestm√∂gliche L√∂sung. <br><br>  Und hier betritt Deep Learning die Szene in ihrer ganzen Pracht. <br><br>  Tiefe neuronale Netze sind ein leistungsf√§higes Werkzeug, das theoretisch eine beliebig komplexe Funktion approximieren kann, dh einen Eingabedatenraum X in den Ausgabedatenraum Y bringen kann. Im Kontext unserer Aufgabe sind dies Text und Audio mit Sprache. <br><br><h2>  Datenvorverarbeitung </h2><br>  Zun√§chst bestimmen wir, was wir als Eingabe haben und was wir an der Ausgabe erhalten m√∂chten. <br><br>  Die Eingabe ist Text und die Ausgabe ist ein Kreidespektrogramm.  Dies ist eine Darstellung auf niedriger Ebene, die durch Anwenden der schnellen Fourier-Transformation auf ein diskretes Audiosignal erhalten wird.  Es ist sofort zu beachten, dass die auf diese Weise erhaltenen Spektrogramme noch durch Komprimieren des Dynamikbereichs <b>normalisiert</b> werden m√ºssen.  Auf diese Weise k√∂nnen Sie die nat√ºrliche Beziehung zwischen dem lautesten und dem leisesten Ton in der Aufnahme verringern.  In unseren Experimenten erwies sich die Verwendung von auf den <b>Bereich [-4; 4]</b> reduzierten Spektrogrammen als die beste. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/849/8fe/eb1/8498feeb1bd525506739c85bc5230c65.png"><br>  <i>Abbildung 1: Kreide-Spektrogramm des auf den Bereich reduzierten Sprach-Audiosignals [-4; 4].</i> <br><br>  Als Trainingsdatensatz haben wir den <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">LJSpeech-Datensatz ausgew√§hlt</a> , der <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">13.100</a> Audiospuren f√ºr 2-10 Sekunden enth√§lt.  und eine Datei mit Text, der der auf Audio aufgezeichneten englischen Sprache entspricht. <br><br>  Ton, der die obigen Transformationen verwendet, wird in Kreidespektrogramme codiert.  Der Text wird tokenisiert und transformiert. <br><br>  in eine Folge von ganzen Zahlen.  Ich muss sofort betonen, dass die Texte normalisiert sind: Alle Zahlen sind m√ºndlich geschrieben und m√∂gliche Abk√ºrzungen werden entschl√ºsselt, zum Beispiel: ‚ÄûFrau  Robinson ‚Äú-‚Äû Missis Robinson ‚Äú. <br><br>  So erhalten wir nach der Vorverarbeitung S√§tze von Numpy-Arrays numerischer Sequenzen und Kreidespektrogrammen, die in npy-Dateien auf der Festplatte aufgezeichnet sind. <br><br>  Damit in der Trainingsphase alle Dimensionen in den Patch-Tensoren zusammenfallen, werden wir kurze Sequenzen mit Auff√ºllungen versehen.  F√ºr Sequenzen in Form von Texten sind diese f√ºr das Auff√ºllen von 0 und f√ºr Spektrogramme f√ºr Frames reserviert, deren Werte geringf√ºgig unter den von uns festgelegten Mindestspektrogrammen liegen.  Dies wird empfohlen, um diese Polster zu isolieren und sie von L√§rm und Stille zu trennen. <br><br>  Jetzt haben wir Daten, die Text und Audio darstellen und f√ºr die Verarbeitung durch ein k√ºnstliches neuronales Netzwerk geeignet sind.  Schauen wir uns die Architektur des Feature Prediction Net an, das unter dem Namen des zentralen Elements des gesamten Synthesesystems Tacotron2 hei√üt. <br><br><h2>  Architektur </h2><br>  Tacotron 2 ist nicht ein Netzwerk, sondern zwei: Feature Prediction Net und NN-Vocoder <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">WaveNet</a> .  Der Originalartikel sowie unsere eigene Vision der geleisteten Arbeit erm√∂glichen es uns, Feature Prediction Net als erste Geige zu betrachten, w√§hrend der WaveNet-Vocoder die Rolle eines Peripheriesystems spielt. <br><br>  Tacotron2 ist eine <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sequenz-zu-Sequenz-</a> Architektur.  Es besteht aus einem <b>Encoder</b> (Encoder), der eine interne Vorstellung vom Eingangssignal (Symbol-Token) erzeugt, und einem <b>Decoder</b> (Decoder), der diese Darstellung in ein Kreidespektrogramm verwandelt.  Ein √§u√üerst wichtiges Element des Netzwerks ist auch das sogenannte <b>PostNet</b> , mit dem das vom Decoder erzeugte Spektrogramm verbessert werden soll. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/744/8c3/587/7448c35878c1640d1e156e745fa2bd96.png"><br>  <i>Abbildung 2: Tacotron 2-Netzwerkarchitektur</i> <br><br>  Lassen Sie uns die Netzwerkbl√∂cke und ihre Module genauer betrachten. <br><br>  Die erste <b>Encoderschicht</b> ist die Einbettungsschicht.  Basierend auf einer Folge von nat√ºrlichen Zahlen, die Zeichen darstellen, werden mehrdimensionale (512-dimensionale) Vektoren erstellt. <br><br>  Als n√§chstes werden Einbettungsvektoren in einen Block von drei eindimensionalen Faltungsschichten eingespeist.  Jede Ebene enth√§lt 512 Filter der L√§nge 5. Dieser Wert ist in diesem Zusammenhang eine gute Filtergr√∂√üe, da er ein bestimmtes Zeichen sowie seine zwei vorherigen und zwei nachfolgenden Nachbarn erfasst.  Auf jede Faltungsschicht folgen eine Mini-Batch-Normalisierung und eine ReLU-Aktivierung. <br><br>  Die nach dem Faltungsblock erhaltenen Tensoren werden auf bidirektionale LSTM-Schichten mit jeweils 256 Neuronen angewendet.  Vorw√§rts- und R√ºcklaufergebnisse werden verkettet. <br><br>  <b>Der Decoder</b> hat eine wiederkehrende Architektur, dh bei jedem nachfolgenden Schritt wird die Ausgabe des vorherigen Schritts verwendet.  Hier werden sie ein Bild des Spektrogramms sein.  Ein weiteres wichtiges, wenn nicht sogar Schl√ºsselelement dieses Systems ist der Mechanismus der weichen (trainierten) <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Aufmerksamkeit</a> - eine relativ neue Technik, die immer beliebter wird.  Bei jedem Schritt des Decoders wird die Aufmerksamkeit zum Bilden eines Kontextvektors und zum Aktualisieren des Aufmerksamkeitsgewichts verwendet: <br><br><ul><li>  die Projektion des vorherigen verborgenen Zustands des RNN-Netzwerks des Decoders auf eine vollst√§ndig verbundene Schicht, </li><li>  Projektion des Encoderausgangs auf eine vollst√§ndig verbundene Schicht, </li><li>  sowie additive (bei jedem Zeitschritt des Decoders akkumulierte) Aufmerksamkeitsgewichte. </li></ul><br>  Die Idee der Aufmerksamkeit sollte wie folgt verstanden werden: "Welcher Teil der Codiererdaten sollte beim aktuellen Decodierschritt verwendet werden". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e47/f4d/410/e47f4d41040e5926c719ef909d21cc99.png"><br>  <i>Abbildung 3: Schema des Aufmerksamkeitsmechanismus.</i> <br><br>  Bei jedem Schritt des Decoders wird der Kontextvektor <i>C <sub>i</sub></i> berechnet (in der obigen Abbildung als "besuchte Codiererausg√§nge" angegeben), der das Produkt aus dem Codiererausgang ( <i>h</i> ) und den Aufmerksamkeitsgewichten ( <i>Œ±</i> ) ist: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b27/8dc/00e/b278dc00e38919c05351ff95c03dc309.png"><br><br>  wobei <i>Œ± <sub>ij</sub></i> die nach der Formel berechneten Aufmerksamkeitsgewichte sind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3cc/344/b9f/3cc344b9f86ba736045a83a7cae8b77e.png"><br><br>  Dabei ist <i>e <sub>ij</sub></i> die sogenannte ‚ÄûEnergie‚Äú, deren Berechnungsformel von der Art des von Ihnen verwendeten Aufmerksamkeitsmechanismus abh√§ngt (in unserem Fall handelt es sich um einen Hybridtyp, der sowohl ortsbezogene als auch inhaltsbasierte Aufmerksamkeit verwendet).  Die Energie wird nach folgender Formel berechnet: <br><br>  <i>e <sub>ij</sub> = v <sub>aT</sub> tanh (Ws <sub>i-1</sub> + Vh <sub>j</sub> + Uf <sub>i, j</sub> + b)</i> <br><br>  wo: <br><ul><li>  <i>s <sub>i-1</sub></i> - vorheriger versteckter Zustand des LSTM-Netzwerks des Decoders, </li><li>  <i>Œ± <sub>i-1</sub></i> - vorherige Aufmerksamkeitsgewichte, </li><li>  <i>h <sub>j</sub></i> ist der j-te verborgene Zustand des Encoders, </li><li>  <i>W</i> , <i>V</i> , <i>U</i> , <i>v <sub>a</sub></i> und <i>b</i> sind Trainingsparameter, </li><li>  <i>f <sub>i, j</sub></i> - Ortszeichen berechnet nach der Formel: <br><br>  <i>f <sub>i</sub> = F * Œ± <sub>i-1</sub></i> <br><br>  wobei <i>F</i> die Faltungsoperation ist. </li></ul><br><br>  F√ºr ein klares Verst√§ndnis der Vorg√§nge f√ºgen wir hinzu, dass einige der unten beschriebenen Module die Verwendung von Informationen aus dem vorherigen Schritt des Decoders voraussetzen.  Wenn dies jedoch der erste Schritt ist, sind die Informationen Tensoren mit Nullwerten, was bei der Erstellung von Wiederholungsstrukturen √ºblich ist. <br><br>  Betrachten Sie nun <b>den Operationsalgorithmus</b> . <br><br>  Zun√§chst wird der Decoderausgang aus dem vorherigen Zeitschritt in ein kleines PreNet-Modul eingespeist, das aus zwei vollst√§ndig verbundenen Schichten von 256 Neuronen besteht, die sich mit Dropout-Schichten mit einer Rate von 0,5 abwechseln.  Eine Besonderheit dieses Moduls ist, dass Dropout nicht nur in der Modellschulungsphase, sondern auch in der Ausgangsphase verwendet wird. <br><br>  Die PreNet-Ausgabe in Verkettung mit dem Kontextvektor, der als Ergebnis des Aufmerksamkeitsmechanismus erhalten wird, wird der Eingabe in ein unidirektionales zweischichtiges LSTM-Netzwerk mit 1024 Neuronen in jeder Schicht zugef√ºhrt. <br><br>  Dann wird die Verkettung der Ausgabe von LSTM-Schichten mit demselben (und m√∂glicherweise unterschiedlichen) Kontextvektor in eine vollst√§ndig verbundene Schicht mit 80 Neuronen eingespeist, was der Anzahl der Kan√§le des Spektrogramms entspricht.  Diese letzte Schicht des Decoders bildet das vorhergesagte Spektrogramm Bild f√ºr Bild.  Und bereits wird sein Ausgang als Eingang f√ºr den n√§chsten Zeitschritt des Decoders in PreNet geliefert. <br><br>  Warum haben wir im vorherigen Absatz erw√§hnt, dass der Kontextvektor m√∂glicherweise bereits anders ist?  Einer der m√∂glichen Ans√§tze besteht darin, den Kontextvektor neu zu berechnen, nachdem in diesem Schritt der latente Zustand des LSTM-Netzwerks erhalten wurde.  In unseren Experimenten hat sich dieser Ansatz jedoch nicht gerechtfertigt. <br><br>  Zus√§tzlich zur Projektion auf eine vollst√§ndig verbundene Schicht mit 80 Neuronen wird die Verkettung der Ausgabe von LSTM-Schichten mit einem Kontextvektor in eine vollst√§ndig verbundene Schicht mit einem Neuron eingespeist, gefolgt von einer Sigmoidaktivierung - dies ist eine Schicht mit "Stop-Token-Vorhersage".  Er sagt die Wahrscheinlichkeit voraus, dass der in diesem Schritt des Decoders erzeugte Rahmen endg√ºltig ist.  Diese Schicht soll ein Spektrogramm von nicht fester, sondern beliebiger L√§nge in der Modellausgangsstufe erzeugen.  Das hei√üt, in der Ausgangsstufe bestimmt dieses Element die Anzahl der Schritte des Decoders.  Es kann als bin√§rer Klassifikator betrachtet werden. <br><br>  Die Ausgabe des Decoders aus allen seinen Schritten ist das vorhergesagte Spektrogramm.  Dies ist jedoch nicht alles.  Um die Qualit√§t des Spektrogramms zu verbessern, wird es durch das PostNet-Modul geleitet, bei dem es sich um einen Stapel von f√ºnf eindimensionalen Faltungsschichten mit jeweils 512 Filtern und einer Filtergr√∂√üe von 5 handelt. Auf jede Schicht folgen Chargennormalisierung und Tangentenaktivierung (mit Ausnahme der letzten).  Um zur Dimension des Spektrogramms zur√ºckzukehren, leiten wir die Post-Net-Ausgangsdaten durch eine vollst√§ndig verbundene Schicht mit 80 Neuronen und addieren die empfangenen Daten mit dem Anfangsergebnis des Decoders.  Wir erhalten das aus dem Text erzeugte Kreidespektrogramm.  Gewinn <br><br>  Alle Faltungsmodule werden mit Dropout-Schichten mit einer Rate von 0,5 und Wiederholungsschichten mit der neueren <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Zoneout-</a> Methode mit einer Rate von 0,1 <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">reguliert</a> .  Es ist ganz einfach: Anstatt den im aktuellen Schritt erhaltenen latenten Zustand und Zellenzustand auf den n√§chsten Zeitschritt des LSTM-Netzwerks anzuwenden, ersetzen wir einen Teil der Daten durch die Werte aus dem vorherigen Schritt.  Dies erfolgt sowohl in der Trainingsphase als auch in der Entzugsphase.  In diesem Fall wird bei jedem Schritt nur der verborgene Zustand (der an den n√§chsten LSTM-Schritt √ºbergeben wird) der Zoneout-Methode ausgesetzt, w√§hrend die Ausgabe der LSTM-Zelle im aktuellen Schritt unver√§ndert bleibt. <br><br>  Wir haben PyTorch als Deep-Learning-Framework ausgew√§hlt.  Zum Zeitpunkt der Implementierung des Netzwerks befand es sich zwar in einem Vorabversionszustand, aber es war bereits ein sehr leistungsf√§higes Werkzeug zum Aufbau und Training k√ºnstlicher neuronaler Netze.  In unserer Arbeit verwenden wir andere Frameworks wie TensorFlow und Keras.  Letzteres wurde jedoch verworfen, da nicht standardm√§√üige benutzerdefinierte Strukturen implementiert werden m√ºssen. Wenn wir TensorFlow und PyTorch vergleichen, besteht bei Verwendung der zweiten kein Gef√ºhl, dass das Modell aus der Python-Sprache herausgerissen wurde.  Wir verpflichten uns jedoch nicht zu behaupten, dass einer von ihnen besser und der andere schlechter ist.  Die Verwendung eines bestimmten Frameworks kann von verschiedenen Faktoren abh√§ngen. <br><br>  Das Netzwerk wird durch die Backpropagation-Methode trainiert.  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">ADAM wird</a> als Optimierer verwendet. Der mittlere quadratische Fehler vor und nach PostNet sowie die bin√§re Kreuzentropie √ºber den tats√§chlichen und vorhergesagten Werten der Ebene "Stop Token Prediction" werden als Fehlerfunktionen verwendet.  Der resultierende Fehler ist eine einfache Summe dieser drei. <br><br>  Das Modell wurde auf einer einzelnen GeForce 1080Ti-GPU mit 11 GB Speicher trainiert. <br><br><h2>  Visualisierung </h2><br>  Bei der Arbeit mit einem so gro√üen Modell ist es wichtig zu sehen, wie der Lernprozess verl√§uft.  Und hier wurde TensorBoard zu einem praktischen Werkzeug.  Wir haben den Wert des Fehlers sowohl in Trainings- als auch in Validierungsiterationen verfolgt.  Zus√§tzlich zeigten wir Zielspektrogramme, vorhergesagte Spektrogramme in der Trainingsphase, vorhergesagte Spektrogramme in der Validierungsphase und Ausrichtung an, was ein additiv akkumuliertes Aufmerksamkeitsgewicht aus allen Trainingsschritten ist. <br><br>  Es ist m√∂glich, dass Ihre Aufmerksamkeit zun√§chst nicht zu informativ ist: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4ce/600/7be/4ce6007bef5b7fe17aa8df56472904bc.png"><br>  <i>Abbildung 4: Ein Beispiel f√ºr schlecht trainierte Aufmerksamkeitsskalen.</i> <br><br>  Aber nachdem alle Ihre Module wie eine Schweizer Uhr funktionieren, erhalten Sie endlich so etwas: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8e7/5d9/ff0/8e75d9ff0e0da78915c1a16f623a99e8.png"><br>  <i>Abbildung 5: Beispiel f√ºr erfolgreich trainierte Aufmerksamkeitsskalen.</i> <br><br>  Was bedeutet diese Tabelle?  Bei jedem Schritt des Decoders versuchen wir, einen Frame des Spektrogramms zu decodieren.  Es ist jedoch nicht klar, welche Informationen der Codierer bei jedem Schritt des Decodierers verwenden muss.  Es ist davon auszugehen, dass diese Korrespondenz direkt erfolgt.  Wenn wir beispielsweise eine Eingabetextsequenz von 200 Zeichen und ein entsprechendes Spektrogramm von 800 Bildern haben, gibt es 4 Bilder f√ºr jedes Zeichen.  Sie m√ºssen jedoch zugeben, dass die auf der Grundlage eines solchen Spektrogramms erzeugte Sprache v√∂llig frei von Nat√ºrlichkeit w√§re.  Wir sprechen einige W√∂rter schneller aus, andere langsamer, irgendwo, wo wir innehalten, aber irgendwo nicht.  Und bedenken Sie, dass alle m√∂glichen Kontexte nicht m√∂glich sind.  Deshalb ist Aufmerksamkeit ein Schl√ºsselelement des gesamten Systems: Sie legt die Entsprechung zwischen dem Decoderschritt und den Informationen vom Codierer fest, um die Informationen zu erhalten, die zum Erzeugen eines bestimmten Rahmens erforderlich sind.  Und je gr√∂√üer die Aufmerksamkeitsgewichte sind, desto mehr "Aufmerksamkeit" sollte dem entsprechenden Teil der Codiererdaten beim Erzeugen des Spektrogrammrahmens geschenkt werden. <br><br>  In der Trainingsphase ist es auch n√ºtzlich, Audio zu generieren und nicht nur die Qualit√§t der Spektrogramme und die Aufmerksamkeit visuell zu bewerten.  Diejenigen, die mit WaveNet gearbeitet haben, werden jedoch zustimmen, dass die Verwendung als Vocoder in der Trainingsphase ein inakzeptabler zeitlicher Luxus w√§re.  Daher wird empfohlen, <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">den Griffin-Lim-Algorithmus zu verwenden</a> , der eine teilweise Rekonstruktion des Signals nach schnellen Fourier-Transformationen erm√∂glicht.  Warum teilweise?  Tatsache ist, dass wir bei der Umwandlung des Signals in Spektrogramme Phaseninformationen verlieren.  Die Qualit√§t des so erhaltenen Audios reicht jedoch v√∂llig aus, um zu verstehen, in welche Richtung Sie sich bewegen. <br><br><h2>  Lektionen gelernt </h2><br>  Hier werden wir einige Gedanken zum Aufbau des Entwicklungsprozesses teilen und sie im Format von Tipps einreichen.  Einige von ihnen sind ziemlich allgemein, andere sind spezifischer. <br><br>  <b>Informationen zur Organisation des Workflows</b> : <br><br><ul><li>  Verwenden Sie das Versionskontrollsystem und beschreiben Sie alle √Ñnderungen klar und deutlich.  Dies mag wie eine offensichtliche Empfehlung erscheinen, aber immer noch.  Bei der Suche nach der optimalen Architektur treten st√§ndig √Ñnderungen auf.  Wenn Sie ein zufriedenstellendes Zwischenergebnis erhalten haben, sollten Sie sich selbst zu einem Kontrollpunkt machen, damit Sie die nachfolgenden √Ñnderungen sicher vornehmen k√∂nnen. <br></li><li>  Aus unserer Sicht sollte man in solchen Architekturen die Prinzipien der Kapselung einhalten: eine Klasse - ein Python-Modul.  Dieser Ansatz ist bei ML-Aufgaben nicht √ºblich, hilft Ihnen jedoch dabei, Ihren Code zu strukturieren und das Debuggen und Entwickeln zu beschleunigen.  Teilen Sie den Code und Ihre Architekturvision in Bl√∂cke, Bl√∂cke in Module und Module in Ebenen ein.  Wenn das Modul √ºber Code verf√ºgt, der eine bestimmte Rolle ausf√ºhrt, kombinieren Sie ihn zu einer Modulklassenmethode.  Dies sind g√§ngige Wahrheiten, aber wir waren nicht zu faul, um noch einmal dar√ºber zu sprechen. <br></li><li>  Stellen <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Sie</a> Klassen im <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Numpy-Stil</a> mit <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">Dokumentation zur Verf√ºgung</a> .  Dies vereinfacht die Arbeit f√ºr Sie und Ihre Kollegen, die Ihren Code lesen, erheblich. <br></li><li>  Zeichnen Sie immer die Architektur Ihres Modells.  Erstens hilft es Ihnen, einen Sinn daraus zu ziehen, und zweitens k√∂nnen Sie anhand einer Seitenansicht der Architektur und der Hyperparameter des Modells Ungenauigkeiten in Ihrem Ansatz schnell erkennen. <br></li><li>  Besser als Team arbeiten.  Wenn Sie alleine arbeiten, sammeln Sie immer noch Kollegen und besprechen Sie Ihre Arbeit.  Zumindest k√∂nnen sie Ihnen eine Frage stellen, die Sie zu einigen Gedanken f√ºhrt, aber h√∂chstens weisen sie auf eine bestimmte Ungenauigkeit hin, die es Ihnen nicht erm√∂glicht, das Modell erfolgreich zu trainieren. <br></li><li>  Ein weiterer n√ºtzlicher Trick ist bereits mit der Datenvorverarbeitung verbunden.  Angenommen, Sie m√∂chten eine Hypothese testen und die entsprechenden √Ñnderungen am Modell vornehmen.  Ein Neustart des Trainings, insbesondere vor dem Wochenende, ist jedoch riskant.  Der Ansatz kann anfangs falsch sein und Sie werden Zeit verschwenden.  Was ist dann zu tun?  Erh√∂hen Sie die Gr√∂√üe des Fensters "Schnelle Fourier-Transformation".  Der Standardparameter ist 1024;  Erh√∂hen Sie es um das 4-fache oder sogar das 8-fache.  Dadurch werden die Spektrogramme in der richtigen Anzahl ‚Äûgequetscht‚Äú und das Lernen erheblich beschleunigt.  Von ihnen wiederhergestelltes Audio hat eine geringere Qualit√§t, aber das ist jetzt nicht Ihre Aufgabe?  In 2-3 Stunden k√∂nnen Sie bereits eine Ausrichtung erhalten (‚ÄûAusrichtung‚Äú der Aufmerksamkeitsskalen, wie in der obigen Abbildung gezeigt). Dies best√§tigt die architektonische Korrektheit des Ansatzes und kann bereits mit Big Data getestet werden. <br></li></ul><br>  <b>Bau- und Trainingsmodelle</b> : <br><br><ul><li>  Wir stellten die Hypothese auf, dass Chargen, die nicht zuf√§llig, sondern basierend auf ihrer L√§nge gebildet w√ºrden, den Prozess des Modelltrainings beschleunigen und die generierten Spektrogramme verbessern w√ºrden.  Die logische Annahme, die auf der Hypothese basiert, dass je mehr ein n√ºtzliches Signal (und kein Padding) in das Trainingsnetzwerk eingespeist wird, desto besser.  Dieser Ansatz hat sich jedoch nicht gerechtfertigt, da wir in unseren Experimenten das Netzwerk nicht auf diese Weise trainieren konnten.  Dies ist wahrscheinlich auf den Verlust der Zuf√§lligkeit bei der Auswahl der Instanzen f√ºr das Training zur√ºckzuf√ºhren. <br></li><li>  Verwenden Sie moderne Algorithmen zur Initialisierung von Netzwerkparametern mit einigen optimierten Anfangszust√§nden.  In unseren Experimenten haben wir beispielsweise die Xavier Uniform Weight Initialization verwendet.  Wenn Sie in Ihrem Modul die Normalisierung durch Mini-Batch und einige Aktivierungsfunktionen verwenden m√ºssen, sollten sie sich in dieser Reihenfolge abwechseln.  Wenn wir beispielsweise die ReLU-Aktivierung anwenden, verlieren wir sofort das gesamte negative Signal, das an der Normalisierung der Daten einer bestimmten Charge beteiligt sein sollte. <br></li><li>  Verwenden Sie ab einem bestimmten Lernschritt eine dynamische Lernrate.  Dies hilft wirklich, den Fehlerwert zu reduzieren und die Qualit√§t der erzeugten Spektrogramme zu erh√∂hen. <br></li><li>  Nach dem Erstellen des Modells und erfolglosen Versuchen, es auf Stapeln aus dem gesamten Datensatz zu trainieren, ist es hilfreich, zu versuchen, es auf einem Stapel neu zu trainieren.    ,   alignment,           (    ).  ,      ,      . <br><br>    .        .  ,        ‚Äì      .    ,            .       ,       . </li><li>    RNN-              .      . ,           .        ?             LSTM-     -. <br></li><li>       ,   LSTM-,      ¬´ ¬ª: ¬´ <i>       ,         LSTM-.      ¬´¬ª  bf.   ,        ,    ,   LSTM-     ft  1/2.   ,        :    ,        ¬´¬ª  1/2,         .    bf    ,  1   2:     ft                 </i> ¬ª. <br></li><li>   seq2seq-         .       ‚Äî       ,         .           ?           ,        ( ). <br></li><li> Nun eine spezielle Empfehlung f√ºr das PyTorch-Framework.  Obwohl die LSTM-Schicht im Decoder im Wesentlichen eine eigene LSTM-Zelle ist, die bei jedem Schritt des Decoders nur Informationen f√ºr ein Element der Sequenz empf√§ngt, wird empfohlen, die Klasse <code>torch.nn.LSTM</code> anstelle von <code>torch.nn.LSTMCell</code> .  Der Grund daf√ºr ist, dass das LSTM-Backend in der CUDNN-Bibliothek in C und in der LSTMCell in Python implementiert ist.  Mit diesem Trick k√∂nnen Sie die Geschwindigkeit des Systems erheblich erh√∂hen. </li></ul><br>  Am Ende des Artikels <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=de&amp;u=">werden Beispiele f√ºr die Sprachgenerierung aus Texten vorgestellt, die nicht im Trainingssatz enthalten waren.</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/de436312/">https://habr.com/ru/post/de436312/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../de436302/index.html">Erfahrung mit der realen Importsubstitution mit dem russischen AERODISK-Speichersystem</a></li>
<li><a href="../de436304/index.html">Zimbra Collaboration Suite und der Kampf gegen Phishing</a></li>
<li><a href="../de436306/index.html">Maschinelles Lernen f√ºr Vertica</a></li>
<li><a href="../de436308/index.html">Rostelecom k√∂nnte ein Monopolist auf dem Markt f√ºr Rechenzentren werden</a></li>
<li><a href="../de436310/index.html">Wie Ivan DevOps. Gegenstand des Einflusses</a></li>
<li><a href="../de436314/index.html">Das japanische Robo-Hotel "feuerte" die H√§lfte seiner Roboter aufgrund der von ihnen verursachten Probleme</a></li>
<li><a href="../de436316/index.html">Wie Smartcards IT-Projekte vorantreiben</a></li>
<li><a href="../de436318/index.html">Neue Funktionen zur Netzwerkautomatisierung in Red Hat Ansible</a></li>
<li><a href="../de436320/index.html">Viele Eigenschaften oder Eigenschaftsobjekt: Auswahlkriterien</a></li>
<li><a href="../de436322/index.html">@ Pythonetc Dezember 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>