<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>üë¶üèø üë≤üèº üë®üèº Service de cache intelligent bas√© sur ZeroMQ et Tarantool ü§û üè£ üë¨</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ruslan Aromatov, d√©veloppeur en chef, ICD 



 Bonjour, Habr! Je travaille en tant que d√©veloppeur backend √† la Banque de cr√©dit de Moscou, et au cour...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>Service de cache intelligent bas√© sur ZeroMQ et Tarantool</h1><div class="post__body post__body_full"><div class="post__text post__text-html" id="post-content-body" data-io-article-url="https://habr.com/ru/company/mkb/blog/471740/">  <b>Ruslan Aromatov, d√©veloppeur en chef, ICD</b> <br><br><img src="https://habrastorage.org/webt/ld/1c/ck/ld1cckil16z47pv5vgyjgi7kwa8.png"><br><br>  Bonjour, Habr!  Je travaille en tant que d√©veloppeur backend √† la Banque de cr√©dit de Moscou, et au cours de mon travail, j'ai acquis une exp√©rience que je voudrais partager avec la communaut√©.  Aujourd'hui, je vais vous dire comment nous avons √©crit notre propre service de cache pour les serveurs frontaux de nos clients √† l'aide de l'application mobile MKB Online.  Cet article peut √™tre utile √† ceux qui sont impliqu√©s dans la conception de services et qui connaissent l'architecture des microservices, la base de donn√©es en m√©moire Tarantool et la biblioth√®que ZeroMQ.  Dans l'article, il n'y aura pratiquement aucun exemple de code et d'explication des bases, mais seulement une description de la logique des services et de leur interaction avec un exemple sp√©cifique qui travaille sur notre bataille depuis plus de deux ans. <br><a name="habracut"></a><br><h4>  Comment tout a commenc√© </h4><br>  Il y a environ 6 ans, le sch√©ma √©tait simple.  En tant qu'h√©ritage de la soci√©t√© d'externalisation, nous avons obtenu deux clients de services bancaires mobiles pour iOS et Android, ainsi qu'un serveur frontal qui les dessert.  Le serveur lui-m√™me a √©t√© √©crit en java, s'est rendu sur son backend de diff√©rentes mani√®res (principalement soap) et a communiqu√© avec les clients en transmettant du xml via https. <br><br>  Les applications clientes ont pu s'authentifier d'une mani√®re ou d'une autre, afficher une liste de produits et ... elles semblaient √™tre en mesure d'effectuer certains transferts et paiements, mais en fait, elles ne l'ont pas tr√®s bien fait et pas toujours.  Par cons√©quent, le serveur frontal n'a connu ni un grand nombre d'utilisateurs ni aucune charge s√©rieuse (ce qui, cependant, ne l'a pas emp√™ch√© de tomber environ une fois tous les deux jours). <br><br>  Il est clair que nous (et √† l'√©poque notre √©quipe √©tait compos√©e de quatre personnes), en tant que responsables de la banque mobile, ne convenions pas √† cette situation, et pour commencer, nous avons mis en ordre les applications actuelles, mais le serveur frontal s'est av√©r√© √™tre vraiment mauvais, il fallait donc r√©√©crivez rapidement l'ensemble, en rempla√ßant simultan√©ment xml par json et en passant au serveur d'applications <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">WildFly</a> .  R√©parti sur quelques ann√©es, le refactoring ne s'appuie pas sur un poste s√©par√©, car tout a √©t√© fait principalement pour s'assurer que le syst√®me fonctionne de mani√®re stable. <br><br>  Peu √† peu, les applications et le serveur se sont d√©velopp√©s, ont commenc√© √† fonctionner de mani√®re plus stable, et leurs fonctionnalit√©s √©taient en constante expansion, ce qui a pay√© - il y avait de plus en plus d'utilisateurs. <br><br>  Dans le m√™me temps, des probl√®mes tels que la tol√©rance aux pannes, la redondance, la r√©plication et - effrayant de penser - la surcharge ont commenc√© √† appara√Ætre. <br><br>  Une solution rapide au probl√®me a √©t√© d'ajouter un deuxi√®me serveur WildFly et les applications ont appris √† basculer entre eux.  Le probl√®me du travail simultan√© avec des sessions client a √©t√© r√©solu par le module <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Infinispan</a> int√©gr√© √† WildFly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/li/xt/wx/lixtwxkd09us1bfivvdxjv-dxrk.png" alt="Comme avant"></div><br>  Il semblait que la vie s'am√©liorait ... <br><br><h4>  Tu ne peux pas vivre comme √ßa </h4><br>  Cependant, cette option de travailler avec des sessions n'√©tait pas sans inconv√©nients.  Je citerai ceux qui ne nous convenaient pas. <br><br><ol><li>  Perte de sessions.  Le moins le plus important.  Par exemple, une application envoie deux demandes au serveur-1: la premi√®re demande est l'authentification et la seconde est une demande de liste de comptes.  L'authentification a r√©ussi, une session est cr√©√©e sur le serveur-1.  √Ä ce moment, la deuxi√®me demande du client s'arr√™te soudainement en raison d'une mauvaise communication et l'application bascule sur le serveur-2, renvoyant le transfert de la deuxi√®me demande.  Mais √† une certaine charge de travail, Infinispan peut ne pas avoir le temps de synchroniser les donn√©es entre les n≈ìuds.  En cons√©quence, le serveur-2 ne peut pas v√©rifier la session client, envoie une r√©ponse en col√®re au client, le client est triste et termine sa session.  L'utilisateur doit se reconnecter.  Triste </li><li>  Le red√©marrage du serveur peut √©galement entra√Æner la perte de sessions.  Par exemple, apr√®s une mise √† jour (et cela arrive assez souvent).  Lorsque le serveur-2 d√©marre, il ne peut pas fonctionner tant que les donn√©es ne sont pas synchronis√©es avec le serveur-1.  Il semble que le serveur ait d√©marr√©, mais ne devrait en fait pas accepter les demandes.  C'est g√™nant. </li><li>  Il s'agit d'un module WildFly int√©gr√© qui nous emp√™che de nous √©loigner de ce serveur d'applications vers des microservices. </li></ol><br>  De l√†, une liste de ce que nous aimerions a √©t√© en quelque sorte form√©e par elle-m√™me. <br><br><ol><li>  Nous voulons stocker les sessions client afin que tout serveur (quel que soit leur nombre) imm√©diatement apr√®s le lancement y ait acc√®s. </li><li>  Nous voulons stocker toutes les donn√©es client entre les demandes (par exemple, les param√®tres de paiement et tout cela). </li><li>  Nous voulons enregistrer toutes les donn√©es arbitraires sur une cl√© arbitraire en g√©n√©ral. </li><li>  Et nous voulons √©galement recevoir les donn√©es des clients avant que l'authentification ne passe.  Par exemple, l'utilisateur est authentifi√© et tous ses produits sont l√†, frais et chaleureux. </li><li>  Et nous voulons √©voluer en fonction de la charge. </li><li>  Et ex√©cutez dans le menu fixe, et √©crivez les journaux sur une seule pile, et comptez les m√©triques, etc. </li><li>  Oh oui, et pour que tout fonctionne rapidement. </li></ol><br><h4>  Farine de choix </h4><br>  Auparavant, nous n'avions pas impl√©ment√© l'architecture de microservices, donc pour commencer nous nous sommes assis pour lire, regarder et essayer diff√©rentes options.  Il √©tait clair tout de suite que nous avions besoin d'un r√©f√©rentiel rapide et d'une sorte de module compl√©mentaire qui traite de la logique m√©tier et constitue l'interface d'acc√®s au r√©f√©rentiel.  De plus, il serait int√©ressant de fixer un transport rapide entre les services. <br><br>  Ils ont choisi pendant longtemps, se sont beaucoup disput√©s et ont exp√©riment√©.  Je ne d√©crirai pas maintenant les avantages et les inconv√©nients de tous les candidats, cela ne s'applique pas au sujet de cet article, je dis simplement que le stockage sera <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">tarantool</a> , nous √©crirons notre service en java et <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">ZeroMQ</a> fonctionnera comme un transport.  Je ne dirai m√™me pas que le choix est tr√®s ambigu, mais il a √©t√© largement influenc√© par le fait que nous n'aimons pas les diff√©rents cadres gros et lourds (pour leur poids et leur lenteur), les solutions en bo√Æte (pour leur polyvalence et leur manque de personnalisation), mais en m√™me temps Nous aimons contr√¥ler autant que possible toutes les parties de notre syst√®me.  Et pour contr√¥ler le travail des services, nous avons choisi le serveur de collecte de m√©triques <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=fr&amp;u=">Prometheus</a> avec ses agents pratiques qui peuvent √™tre int√©gr√©s dans presque n'importe quel code.  Les journaux de tout cela iront dans la pile ELK. <br><br>  Eh bien, il me semble qu'il y avait d√©j√† trop de th√©orie. <br><br><h4>  D√©but et fin </h4><br>  Le r√©sultat de conception √©tait approximativement un tel sch√©ma. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/he/kw/oh/hekwohpipjcqh4ufeoykog6f8cc.png" alt="Comment voulons-nous"></div><br>  <b>Stockage</b> <br><br>  Il devrait √™tre aussi stupide que possible, uniquement pour stocker des donn√©es et leurs √©tats actuels, mais toujours fonctionner sans red√©marrage.  Con√ßu pour servir diff√©rentes versions de serveurs frontaux.  Nous conservons toutes les donn√©es en m√©moire, r√©cup√©ration en cas de red√©marrage via les fichiers .snap et .xlog. <br><br>  Table (espace) pour les sessions client: <br><br><ul><li>  ID de session </li><li>  ID client; </li><li>  version (service) </li><li>  heure de mise √† jour (horodatage); </li><li>  dur√©e de vie (ttl); </li><li>  donn√©es de session s√©rialis√©es. </li></ul><br>  Ici, tout est simple: le client est authentifi√©, le serveur frontal cr√©e une session et l'enregistre dans le stockage, en se souvenant de l'heure.  √Ä chaque demande de donn√©es, l'heure est mise √† jour, de sorte que la session est maintenue en vie.  Si, sur demande, les donn√©es s'av√®rent obsol√®tes (ou qu'il n'y en aura pas du tout), nous retournerons un code de retour sp√©cial, apr√®s quoi le client mettra fin √† sa session. <br><br>  Table de cache simple (pour toutes les donn√©es de session): <br><br><ul><li>  cl√©; </li><li>  ID de session </li><li>  type de donn√©es stock√©es (nombre arbitraire); </li><li>  heure de mise √† jour (horodatage); </li><li>  dur√©e de vie (ttl); </li><li>  donn√©es s√©rialis√©es. </li></ul><br>  Tableau des donn√©es client √† r√©chauffer avant la connexion: <br><ul><li>  ID client; </li><li>  ID de session </li><li>  version (service) </li><li>  type de donn√©es stock√©es (nombre arbitraire); </li><li>  heure de mise √† jour (horodatage); </li><li>  √©tat; </li><li>  donn√©es s√©rialis√©es. </li></ul><br>  Un domaine important ici est la condition.  En fait, il n'y en a que deux - inactif et √† jour.  Ils sont plac√©s par un service sous-jacent qui va au backend pour les donn√©es client afin qu'une autre instance de ce service ne fasse pas le m√™me travail (d√©j√† inutile) et ne charge pas le backend. <br><br>  Tableau des p√©riph√©riques: <br><br><ul><li>  ID client; </li><li>  ID de l'appareil </li><li>  heure de mise √† jour (horodatage); </li></ul><br>  La table des appareils est n√©cessaire pour que, avant m√™me que le client s'authentifie dans le syst√®me, connaisse son ID et commence √† recevoir ses produits (r√©chauffement du cache).  La logique est la suivante: la premi√®re entr√©e est toujours froide, car avant l'authentification, nous ne savons pas quel type de client provient d'un appareil inconnu (les clients mobiles transmettent toujours des ID d'appareil dans toutes les demandes).  Toutes les entr√©es suivantes de cet appareil seront accompagn√©es d'un cache de pr√©chauffage pour le client qui lui est associ√©. <br><br>  Le travail avec les donn√©es est isol√© du service java par les proc√©dures du serveur.  Oui, j'ai d√ª apprendre le lua, mais cela n'a pas pris beaucoup de temps.  En plus de la gestion des donn√©es elle-m√™me, les lua-procedures sont √©galement responsables du retour des √©tats actuels, des s√©lections d'index, du nettoyage des enregistrements obsol√®tes dans les processus d'arri√®re-plan (fibres) et du fonctionnement du serveur Web int√©gr√© via lequel un acc√®s direct aux donn√©es est effectu√©.  Le voici - le charme d'√©crire tout avec vos mains - la possibilit√© d'un contr√¥le illimit√©.  Mais le moins est le m√™me - vous devez tout √©crire vous-m√™me. <br><br>  Tarantool lui-m√™me fonctionne dans un conteneur docker, tous les fichiers lua n√©cessaires y sont plac√©s au stade de l'assemblage de l'image.  L'ensemble de l'assemblage √† travers des scripts gradle. <br><br>  R√©plication ma√Ætre-esclave.  Sur l'autre h√¥te, le m√™me conteneur s'ex√©cute exactement comme la r√©plique du stockage principal.  Il est n√©cessaire en cas de panne d'urgence du ma√Ætre - puis les services java passent en esclave, et il devient le ma√Ætre.  Il y a un troisi√®me esclave au cas o√π.  Cependant, m√™me une perte de donn√©es compl√®te dans notre cas est triste, mais pas fatale.  Selon le pire des cas, les utilisateurs devront se connecter et r√©cup√©rer toutes les donn√©es qui vont √† nouveau dans le cache. <br><br>  <b>Service Java</b> <br><br>  Con√ßu comme un microservice sans √©tat typique.  Il n'a pas de configuration, tous les param√®tres n√©cessaires (et il y en a 6) sont pass√©s par les variables d'environnement lors de la cr√©ation du conteneur Docker.  Il fonctionne avec le serveur frontal via le transport ZeroMQ (org.zeromq.jzmq - l'interface java du libzmq.so.5.1.1 natif, que nous avons nous-m√™mes construit) en utilisant notre propre protocole.  Il fonctionne avec une tarentule via un connecteur java (org.tarantool.connector). <br><br>  L'initialisation du service est assez simple: <br><br><ul><li>  Nous d√©marrons un enregistreur (log4j2); </li><li>  A partir des variables d'environnement (nous sommes dans le docker) nous lisons les param√®tres n√©cessaires au travail; </li><li>  Nous d√©marrons le serveur de m√©triques (jet√©e); </li><li>  Connectez-vous √† la tarentule (de mani√®re asynchrone); </li><li>  Nous d√©marrons le nombre requis de gestionnaires de threads (travailleurs); </li><li>  Nous commen√ßons un courtier (zmq) - un cycle de traitement de messages sans fin. </li></ul><br>  De tout ce qui pr√©c√®de, seul le moteur de traitement des messages est int√©ressant.  Voici un sch√©ma du microservice. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/fd/ef/myfdef3ggy0oerhyfvec3iozvwc.png" alt="Logique du courtier de messages"></div><br>  Commen√ßons par le d√©but du courtier.  Notre courtier est un ensemble de sockets zmq de type ROUTER, qui accepte les connexions de divers clients et est responsable de la planification des messages provenant d'eux. <br><br>  Dans notre cas, nous avons une socket d'√©coute sur l'interface externe qui re√ßoit des messages des clients utilisant le protocole tcp et l'autre re√ßoit des messages des threads de travail utilisant le protocole inproc (c'est beaucoup plus rapide que tcp). <br><br><pre><code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** //   (   ,   ) ZContext zctx = new ZContext(); //    ZMQ.Socket clientServicePoint = zctx.createSocket(ZMQ.ROUTER); //    ZMQ.Socket workerServicePoint= zctx.createSocket(ZMQ.ROUTER); //     clientServicePoint.bind("tcp://*:" + Config.ZMQ_LISTEN_PORT); //     workerServicePoint.bind("inproc://worker-proc");</span></span></code> </pre> <br>  Apr√®s avoir initialis√© les sockets, nous commen√ßons une boucle d'√©v√©nements sans fin. <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">/** *      */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> status;  <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> {   ZMQ.Poller poller = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ZMQ.Poller(<span class="hljs-number"><span class="hljs-number">2</span></span>);    poller.register(workerServicePoint, ZMQ.Poller.POLLIN);    poller.register(clientServicePoint, ZMQ.Poller.POLLIN);    <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> rc;    <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) {      <span class="hljs-comment"><span class="hljs-comment">//        rc = poller.poll(POLL_INTERVAL);      if (rc == -1) {        status = -1;        logger.errorInternal("Broker run error rc = -1");        break; //  -     }    //     ()    if (poller.pollin(0)) {       processBackendMessage(ZMsg.recvMsg(workerServicePoint));    }    //        if (poller.pollin(1)) {       processFrontendMessage(ZMsg.recvMsg(clientServicePoint));    }    processQueueForBackend(); }  } catch (Exception e) {    status = -1;  } finally {    clientServicePoint.close();    workerServicePoint.close();  }  return status; }</span></span></code> </pre><br>  La logique du travail est tr√®s simple: nous recevons des messages de diff√©rents endroits et faisons quelque chose avec eux.  En cas de rupture critique avec nous, nous quittons la boucle, ce qui provoque le crash du processus, qui sera automatiquement red√©marr√© par le d√©mon docker. <br><br>  L'id√©e principale est que le courtier ne g√®re aucune logique m√©tier, il analyse uniquement l'en-t√™te du message et distribue les t√¢ches aux threads de travail qui ont √©t√© lanc√©s plus t√¥t au d√©marrage du service.  En cela, une seule file d'attente de messages avec hi√©rarchisation d'une longueur fixe l'aide. <br><br>  Analysons l'algorithme en utilisant l'exemple du sch√©ma et du code ci-dessus. <br><br>  Apr√®s le d√©marrage, les threads qui ont d√©marr√© apr√®s le courtier sont initialis√©s et envoient un message de pr√©paration au courtier.  Le courtier les accepte, les analyse et ajoute chaque travailleur √† la liste. <br><br>  Un √©v√©nement se produit sur le socket client - nous avons re√ßu le message 1.  Le courtier appelle le gestionnaire de messages entrants, dont la t√¢che est: <br><br><ul><li>  analyse de l'en-t√™te du message; </li><li>  placer un message dans un objet titulaire avec une priorit√© donn√©e (bas√©e sur l'analyse d'en-t√™te) et une dur√©e de vie; </li><li>  placer le titulaire dans la file d'attente de messages; </li><li>  si la file d'attente n'est pas pleine, la t√¢che du gestionnaire est termin√©e; </li><li>  si la file d'attente est pleine, nous appelons la m√©thode pour envoyer un message d'erreur au client. </li></ul><br>  Dans la m√™me it√©ration de la boucle, nous appelons le gestionnaire de file d'attente de messages: <br><br><ul><li>  nous demandons le message le plus r√©cent √† la file d'attente (la file d'attente d√©cide de lui-m√™me en fonction de la priorit√© et de l'ordre d'ajout du message); </li><li>  v√©rifier la dur√©e de vie du message (s'il a expir√©, appelez la m√©thode pour envoyer un message d'erreur au client); </li><li>  si le message √† traiter est pertinent, essayez de pr√©parer le premier travailleur libre √† travailler; </li><li>  s'il n'y en a pas, remettez le message dans la file d'attente (plus pr√©cis√©ment, ne le supprimez pas de l√†, il y restera jusqu'√† l'expiration de sa dur√©e de vie); </li><li>  si nous avons un travailleur pr√™t √† travailler, nous le marquons comme occup√© et lui envoyons un message pour traitement; </li><li>  supprimez le message de la file d'attente. </li></ul><br>  Nous faisons de m√™me avec tous les messages suivants.  Le thread thread lui-m√™me est con√ßu de la m√™me mani√®re qu'un courtier - il a le m√™me cycle de traitement de messages sans fin.  Mais en elle, nous n'avons plus besoin d'un traitement instantan√©, il est con√ßu pour effectuer de longues t√¢ches. <br><br>  Une fois que le travailleur a termin√© sa t√¢che (par exemple, est all√© au backend pour les produits du client ou dans la tarentule pour la session), il envoie un message au courtier, que le courtier renvoie au client.  L'adresse du client √† qui la r√©ponse doit √™tre envoy√©e est m√©moris√©e d√®s l'instant o√π le message arrive du client dans l'objet titulaire, qui est envoy√© au travailleur sous forme de message dans un format l√©g√®rement diff√©rent, puis revient. <br><br>  Le format des messages que je mentionne constamment est notre propre production.  Hors de la bo√Æte, ZeroMQ nous fournit les classes ZMsg - le message lui-m√™me et ZFrame - une partie int√©grante de ce message, essentiellement juste un tableau d'octets, que je suis libre d'utiliser si je le souhaite.  Notre message se compose de deux parties (deux ZFrames), dont la premi√®re est un en-t√™te binaire et la seconde est des donn√©es (le corps de la demande, par exemple, sous la forme d'une cha√Æne json repr√©sent√©e par un tableau d'octets).  L'en-t√™te du message est universel et se d√©place √† la fois du client au serveur et du serveur au client. <br><br>  En fait, nous n'avons pas le concept de ¬´demande¬ª ou de ¬´r√©ponse¬ª, seulement des messages.  L'en-t√™te contient: version du protocole, type de syst√®me (auquel syst√®me est adress√©), type de message, code d'erreur au niveau du transport (si ce n'est pas 0, quelque chose s'est produit dans le moteur de transfert de message), ID de la demande (identifiant de transfert provenant du client - n√©cessaire pour le tra√ßage), l'ID de session client (facultatif), ainsi qu'un signe d'erreur de niveau de donn√©es (par exemple, si la r√©ponse du backend n'a pas pu √™tre analys√©e, nous d√©finissons ce drapeau pour que l'analyseur c√¥t√© client ne d√©s√©rialise pas la r√©ponse, mais re√ßoive des donn√©es d'erreur d'une autre mani√®re). <br><br>  Gr√¢ce √† un protocole unique entre tous les microservices et un tel en-t√™te, nous pouvons tout simplement manipuler les composants de nos services.  Par exemple, vous pouvez int√©grer le courtier dans un processus distinct et en faire un courtier de messages unique au niveau de l'ensemble du syst√®me de microservices.  Ou, par exemple, ex√©cutez les travailleurs non pas sous la forme de threads √† l'int√©rieur du processus, mais en tant que processus ind√©pendants distincts.  Et tandis que le code √† l'int√©rieur d'eux ne change pas.  En g√©n√©ral, il y a place pour la cr√©ativit√©. <br><br><h4>  Un peu sur les performances et les ressources </h4><br>  Le courtier lui-m√™me est rapide et la bande passante totale du service est limit√©e par la vitesse du backend et le nombre de travailleurs.  De mani√®re pratique, toute la quantit√© de m√©moire n√©cessaire est allou√©e imm√©diatement au d√©marrage du service et tous les threads sont imm√©diatement d√©marr√©s.  La taille de la file d'attente est √©galement fixe.  Lors de l'ex√©cution, seuls les messages sont en cours de traitement. <br><br>  √Ä titre d'exemple: en plus du thread principal, notre service de combat de cache actuel lance 100 autres threads de travail et la taille de la file d'attente est limit√©e √† trois mille messages.  En fonctionnement normal, chaque instance traite jusqu'√† 200 messages par seconde et consomme environ 250 Mo de m√©moire et environ 2 √† 3% du processeur.  Parfois, aux charges de pointe, il passe √† 7-8%.  Tout cela fonctionne sur une sorte de xeon virtuel dual-core. <br><br>  Le travail r√©gulier du service implique l'emploi simultan√© de 3 √† 5 travailleurs (sur 100) avec le nombre de messages dans la file d'attente 0 (c'est-√†-dire qu'ils sont imm√©diatement trait√©s).  Si le backend commence √† ralentir, le nombre de travailleurs occup√©s augmente proportionnellement au temps de sa r√©ponse.  Dans les cas o√π un accident se produit et que le backend monte, tous les travailleurs se terminent d'abord, apr√®s quoi la file d'attente de messages commence √† se boucher.  Lorsqu'il se bouche compl√®tement, nous commen√ßons √† r√©pondre aux clients qui refusent de traiter.  Dans le m√™me temps, nous ne commen√ßons pas √† consommer de la m√©moire ou des ressources CPU, en fournissant de mani√®re stable des mesures et en r√©pondant clairement aux clients ce qui se passe. <br><br>  La premi√®re capture d'√©cran montre le fonctionnement r√©gulier du service. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ji/ek/ap/jiekapcj1vylguqgijeheydw7nu.png" alt="Le travail r√©gulier du service"></div><br>  Et sur le second, un accident s'est produit - le backend pour une raison quelconque n'a pas r√©pondu en 30 secondes.  On voit qu'au d√©but, tous les travailleurs √©taient √©puis√©s, apr√®s quoi la file d'attente de messages a commenc√© √† se boucher. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4v/he/h8/4vheh8hrqsinriaelyow6bgp9wu.png" alt="Accident"></div><br><h4>  Tests de performance </h4><br>  Les tests synth√©tiques sur ma machine de travail (CentOS 7, Core i5, 16 Go de RAM) ont montr√© ce qui suit. <br><br>  Travailler avec le r√©f√©rentiel (√©crire sur la tarentule et lire imm√©diatement cet enregistrement de 100 octets de taille - simuler le travail avec la session) - 12000 rps. <br><br>  La m√™me chose, seule la vitesse a √©t√© mesur√©e non pas entre le service - les points de tarentule, mais entre le client et le service.  Bien s√ªr, j'ai d√ª √©crire moi-m√™me un client pour faire un test de stress.  Dans une seule machine, il √©tait possible d'obtenir 7000 rps.  Sur un r√©seau local (et nous avons de nombreuses machines virtuelles diff√©rentes dont la connexion physique n'est pas claire), les r√©sultats varient, mais jusqu'√† 5000 rps pour une instance est tout √† fait possible.  Dieu sait quel genre de performance, mais il couvre plus de dix fois nos charges de pointe.  Et ce n'est que si une instance du service est en cours d'ex√©cution, mais nous en avons plusieurs, et √† tout moment vous pouvez en ex√©cuter autant que vous le souhaitez.  Lorsque les services bloquent la vitesse de stockage, il sera possible de mettre √† l'√©chelle la tarentule horizontalement (fragment bas√© sur l'ID client, par exemple). <br><br><h4>  Service Intelligence </h4><br>  Le lecteur attentif pose probablement d√©j√† la question - quelle est ¬´l'intelligence¬ª de ce service, qui est mentionn√©e dans le titre.  Je l'ai d√©j√† mentionn√© en passant, mais maintenant je vais vous en dire plus. <br><br>  L'une des principales t√¢ches du service √©tait de r√©duire le temps n√©cessaire pour √©mettre leurs produits aux utilisateurs (listes de comptes, cartes, d√©p√¥ts, pr√™ts, packages de services, etc.) tout en r√©duisant la charge sur le backend (en r√©duisant le nombre de demandes dans les grands et lourds Oracle) en raison de la mise en cache dans la tarentule. <br><br>  Et il l'a assez bien fait.  La logique de r√©chauffement du cache client est la suivante: <br><br><ul><li>  l'utilisateur lance l'application mobile; </li><li>  Une demande AppStart contenant l'ID de p√©riph√©rique est envoy√©e au serveur frontal; </li><li>  le serveur frontal envoie un message avec cet ID au service de cache; </li><li>  le service recherche dans la table des appareils l'ID client de cet appareil; </li><li>  s'il n'est pas l√†, rien ne se passe (la r√©ponse n'est m√™me pas envoy√©e, le serveur ne l'attend pas); </li><li>  si l'ID client est localis√©, le travailleur cr√©e un ensemble de messages pour recevoir des listes de produits utilisateur qui entrent imm√©diatement en traitement par le courtier et sont distribu√©s aux travailleurs en mode normal; </li><li>  chaque travailleur envoie une demande pour un certain type de donn√©es √† l'utilisateur, mettant le statut de ¬´mise √† jour¬ª dans la base de donn√©es (ce statut prot√®ge le backend de la r√©p√©tition des m√™mes requ√™tes si elles proviennent d'autres instances du service); </li><li>  apr√®s r√©ception des donn√©es, elles sont enregistr√©es dans la tarentule; </li><li>  l'utilisateur se connecte au syst√®me, et l'application envoie des demandes de r√©ception de ses produits, et le serveur envoie ces demandes sous forme de messages au service de cache; </li><li>  si les donn√©es utilisateur ont d√©j√† √©t√© re√ßues, nous les envoyons simplement depuis le cache; </li><li>  si les donn√©es sont en cours de r√©ception (√©tat ¬´mise √† jour¬ª), alors un cycle d'attente de donn√©es est d√©marr√© √† l'int√©rieur du travailleur (il est √©gal au d√©lai de requ√™te au backend); </li><li>  d√®s que les donn√©es sont re√ßues (c'est-√†-dire que le statut de cet enregistrement (tuple) dans la table passe √† "inactif", le service les remet au client; </li><li>  si les donn√©es ne sont pas re√ßues dans un certain intervalle de temps, une erreur sera renvoy√©e au client. </li></ul><br>  Ainsi, dans la pratique, nous avons pu r√©duire le temps moyen de r√©ception des produits pour le serveur frontal de 200 ms √† 20 ms, soit environ 10 fois, et le nombre de demandes au backend d'environ 4 fois. <br><br><h4>  Les probl√®mes </h4><br>  Le service de cache fonctionne au combat depuis environ deux ans et r√©pond actuellement √† nos besoins. <br><br>  Bien s√ªr, il y a encore des probl√®mes non r√©solus, parfois des probl√®mes surviennent.  Les services Java dans la bataille ne sont pas encore tomb√©s.  La tarentule est tomb√©e plusieurs fois sur SIGSEGV, mais c'√©tait une ancienne version, et apr√®s la mise √† jour, cela ne s'est plus produit.  Pendant les tests de r√©sistance, la r√©plication tombe, un tuyau cass√© s'est produit sur le ma√Ætre, apr√®s quoi l'esclave est tomb√©, bien que le ma√Ætre ait continu√© √† travailler.  Il a √©t√© d√©cid√© en red√©marrant l'esclave. <br><br>  Une fois qu'il y a eu une sorte d'accident dans le centre de donn√©es, il s'est av√©r√© que le syst√®me d'exploitation (CentOS 7) avait cess√© de voir les disques durs.  Le syst√®me de fichiers est pass√© en mode lecture seule.  Le plus surprenant, c'est que les services ont continu√© de fonctionner, car nous conservons toutes les donn√©es en m√©moire.  La tarentule n'a pas pu √©crire de fichiers .xlog, personne n'a rien enregistr√©, mais d'une mani√®re ou d'une autre, tout a fonctionn√©.  Mais la tentative de red√©marrage a √©chou√© - personne n'a pu d√©marrer. <br><br>  Il y a un gros probl√®me non r√©solu, et j'aimerais √©couter l'opinion de la communaut√© √† ce sujet.  Lorsque la tarentule ma√Ætre se bloque, les services java peuvent passer en esclave, qui continue de fonctionner en tant que ma√Ætre.  Cependant, cela ne se produit que si le ma√Ætre se bloque et ne peut pas fonctionner. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e6/pu/wh/e6puwh26kmnngnrxkvyelgvvite.png" alt="Probl√®me non r√©solu"></div><br>  Supposons que nous ayons 3 instances d'un service qui fonctionnent avec des donn√©es sur une tarentule principale.  Les services eux-m√™mes ne tombent pas, la r√©plication de la base de donn√©es est en cours, tout va bien.  Mais soudain, nous avons un r√©seau qui se d√©sagr√®ge entre le n≈ìud-1 et le n≈ìud-4, o√π l'assistant fonctionne.  Service-1 apr√®s un certain nombre de tentatives infructueuses d√©cide de basculer vers la base de donn√©es de sauvegarde et commence √† y envoyer des demandes. <br><br>  Imm√©diatement apr√®s cela, l'esclave tarentule commence √† accepter les demandes de modification des donn√©es, √† la suite de quoi la r√©plication du ma√Ætre s'effondre et nous obtenons des donn√©es incoh√©rentes.  Dans le m√™me temps, les services-2 et 3 fonctionnent parfaitement avec le ma√Ætre, et le service-1 communique bien avec l'ancien esclave.  Il est clair que dans ce cas, nous commen√ßons √† perdre les sessions client et toutes les autres donn√©es, bien que tout fonctionne du c√¥t√© technique.  Nous n'avons pas encore r√©solu un tel probl√®me potentiel.  Heureusement, cela ne s'est pas produit depuis 2 ans, mais la situation est bien r√©elle.  D√©sormais, chaque service conna√Æt le num√©ro du magasin auquel il va et nous avons une alerte pour cette m√©trique, qui fonctionnera lors du passage du ma√Ætre √† l'esclave.  Et vous devez tout r√©parer avec vos mains.  Comment r√©solvez-vous de tels probl√®mes? <br><br><h4>  Plans </h4><br>  Nous pr√©voyons de travailler sur le probl√®me d√©crit ci-dessus, en limitant le nombre de travailleurs simultan√©ment occup√©s par un type de demande, en toute s√©curit√© (sans perdre les demandes en cours) arr√™tant le service et en poursuivant le polissage. <br><br><h4>  Conclusion </h4><br>  C'est peut-√™tre tout, m√™me si j'ai abord√© le sujet plut√¥t superficiellement, mais la logique g√©n√©rale du travail doit √™tre claire.  Par cons√©quent, si possible, je suis pr√™t √† r√©pondre dans les commentaires.  J'ai bri√®vement d√©crit comment un petit sous-syst√®me auxiliaire des serveurs frontaux de la banque fonctionne pour servir les clients mobiles. <br><br>  Si le sujet int√©resse la communaut√©, je peux vous parler de plusieurs de nos solutions qui contribuent √† am√©liorer la qualit√© du service client de la banque. </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/fr471740/">https://habr.com/ru/post/fr471740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../fr471724/index.html">Besoins agiles et c√©r√©braux: gestion du stress</a></li>
<li><a href="../fr471726/index.html">M√©thode moderne pour mesurer la r√©ponse impulsionnelle et la distorsion non lin√©aire</a></li>
<li><a href="../fr471728/index.html">Avalonia mes avantages et inconv√©nients</a></li>
<li><a href="../fr471736/index.html">Capteur Ethernet sans contact</a></li>
<li><a href="../fr471738/index.html">Une courte histoire sur la fa√ßon dont la commodit√© tire parfois dans le genou</a></li>
<li><a href="../fr471742/index.html">Sberbank AI Journey. Comment nous avons appris √† un r√©seau de neurones √† passer un examen</a></li>
<li><a href="../fr471744/index.html">Tarantool Data Grid: architecture et fonctionnalit√©s</a></li>
<li><a href="../fr471746/index.html">Guide complet de configuration des en-t√™tes HTTP pour la s√©curit√©</a></li>
<li><a href="../fr471748/index.html">Optimisation en pharmacie: ce que nous avons fait avec les math√©matiques</a></li>
<li><a href="../fr471750/index.html">La gestion des acc√®s privil√©gi√©s en tant que t√¢che prioritaire dans la s√©curit√© des informations (par exemple, Fudo PAM)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>