<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134228602-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134228602-6');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>📸 🌃 🏒 通过PyTorch中的可视化了解卷积神经网络 🚍 🍑 🧚🏼</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="在我们这个时代，机器在理解和定义图像中的特征和对象方面已经成功实现了99％的精度。 例如，我们每天都面临着这样的情况：智能手机的摄像头中的人脸识别功能，能够在Google上搜索照片，以很高的速度从条形码或书籍中扫描文本等功能。得益于一种称为卷积神经的特殊类型的神经网络，这种机器效率得以实现网络。 如...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script>document.write('<script src="//pagea' + 'd2.googles' + 'yndication.com/pagea' + 'd/js/a' + 'dsby' + 'google.js"><\/script>')</script>
  <script>
        var superSpecialObject = {};
        superSpecialObject['google_a' + 'd_client'] = 'ca-p' + 'ub-6974184241884155';
        superSpecialObject['enable_page_level_a' + 'ds'] = true;
       (window['a' + 'dsbygoogle'] = window['a' + 'dsbygoogle'] || []).push(superSpecialObject);
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly-es.github.io/index.html"></a>
    <div class="page-header-text">Geekly articles weekly</div>
  </header>
  <section class="page js-page"><h1>通过PyTorch中的可视化了解卷积神经网络</h1><div class="post__body post__body_full"><div class="post__text post__text-html post__text_v1" id="post-content-body" data-io-article-url="https://habr.com/ru/post/436838/"> 在我们这个时代，机器在理解和定义图像中的特征和对象方面已经成功实现了99％的精度。 例如，我们每天都面临着这样的情况：智能手机的摄像头中的人脸识别功能，能够在Google上搜索照片，以很高的速度从条形码或书籍中扫描文本等功能。得益于一种称为卷积神经的特殊类型的神经网络，这种机器效率得以实现网络。 如果您是一名深度学习爱好者，您可能听说过它，并且可以开发几个图像分类器。  Tensorflow和PyTorch等现代深度学习框架可简化图像机器学习。 但是，问题仍然存在：数据如何通过神经网络的各个层以及计算机如何从它们中学习？ 为了从头开始获得清晰的视图，我们进行了卷积，以可视化每一层的图像。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/2c6/958/8592c6958985979587858374abd08f98.png" alt="图片"><br><a name="habracut"></a><br><h2> 卷积神经网络 </h2><br> 在开始研究卷积神经网络（SNA）之前，您需要学习如何使用神经网络。 神经网络模仿人的大脑来解决复杂的问题并搜索数据模式。 在过去的几年中，它们已经取代了许多机器学习和计算机视觉算法。 神经网络的基本模型由分层组织的神经元组成。 每个神经网络都有一个输入和输出层，并根据问题的复杂性添加了几个隐藏层。 通过层传输数据时，神经元受到训练并识别体征。 神经网络的这种表示称为模型。 训练模型后，我们要求网络根据测试数据进行预测。 <br><br>  SNS是一种特殊的神经网络，可以很好地处理图像。 伊恩·勒昆（Ian Lekun）于1998年提出了它们，他们在输入图像中识别出存在的数字。  SNA还用于语音识别，图像分割和文本处理。 在创建卷积神经网络之前，多层感知器用于构造图像分类器。 图像分类是指从多通道（彩色，黑白）光栅图像中提取类的任务。 多层感知器需要很长时间来搜索图像中的信息，因为每个输入必须与下一层中的每个神经元相关联。  SNA使用称为本地连接的概念绕过了它们。 这意味着我们将仅将每个神经元连接到本地输入区域。 这样可以最大程度地减少参数的数量，从而允许网络的各个部分专门处理高级属性，例如纹理或重复图案。 感到困惑？ 让我们比较一下如何通过多层感知器（MP）和卷积神经网络传输图像。 <br><br><h2>  MP和SNA的比较 </h2><br> 多层感知器在输入层中的条目总数为784，因为输入图像的大小为28x28 = 784（考虑了MNIST数据集）。 网络必须能够预测输入图像上的数字，这意味着输出可以属于0到9范围内的以下任何类别。在输出层中，我们返回类别估计值，例如该输入是否为数字“ 3”的图像，那么在输出层中，与其他神经元相比，相应的神经元“ 3”具有更高的值。 再次出现一个问题：“我们需要多少个隐藏层，每个层中应该有多少个神经元？” 例如，采用以下MP代码： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f8/efc/e14/3f8efce14418a7df0be2e813399def5d.png" alt="图片"><br><br> 上面的代码是使用称为Keras的框架实现的。 第一隐藏层有512个神经元，它们与784个神经元的输入层相连。 下一个隐藏层：排除层，它解决了重新训练的问题。  0.2表示有20％的机会不考虑先前隐藏层的神经元。 我们再次添加了具有与第一个隐藏层（512）中相同数量的神经元的第二个隐藏层，然后添加了另一个排他层。 最后，以包含10个类的输出层结束该层集合。 最重要的类别将是模型预测的数量。 这是识别所有层后多层网络的外观。 多级感知器的缺点之一是它是完全连接的，这需要大量的时间和空间。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db3/df6/605/db3df6605d0ddb868eb14b347227b963.png" alt="图片"><br><br>  Convolts不使用完全粘合的层。 他们使用稀疏层（将矩阵作为输入），这比MP具有优势。 在MP中，每个节点负责了解整个图片。 在SNA中，我们将图像分成多个区域（像素的小局部区域）。 输出层组合从每个隐藏节点接收到的数据以查找模式。 下图是各层之间如何连接的图像。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae6/f7f/d61/ae6f7fd618d5296a0deecabdd2e06e77.png" alt="图片"><br><br> 现在，让我们看看SNA如何在照片中找到信息。 在此之前，我们需要了解如何提取符号。 在SNA中，我们使用不同的层，每个层都保留图像的标志，例如，它考虑了狗的图像，当网络需要对狗进行分类时，它必须识别所有标志，例如眼睛，耳朵，舌头，腿等。 这些标志已被破坏，并使用过滤器和内核在本地网络级别被识别。 <br><br><h2> 计算机如何查看图像？ </h2><br> 一个人看着图像并理解其含义听起来很合理。 假设您走路时注意到附近的许多风景。 在这种情况下，我们如何理解自然？ 我们使用主要的感知器官-眼睛为环境拍照，然后将其发送到视网膜。 一切看起来都很有趣，对不对？ 现在，让我们想象一台计算机也可以这样做。 在计算机中，使用一组介于0到255之间的像素值来解释图像。计算机查看并理解了这些像素值。 乍一看，他不知道物体和颜色。 它仅识别像素值，并且图像等效于计算机的一组像素值。 后来，通过分析像素值，他逐渐了解了图像是灰色还是彩色。 灰度图像只有一个通道，因为每个像素代表一种颜色的强度。  0表示黑色，255表示白色，黑色和白色的其他变体（即灰色）介于它们之间。 <br><br> 彩色图像具有三个通道，红色，绿色和蓝色。 它们代表3种颜色（三维矩阵）的强度，并且当值同时更改时，这将提供大量颜色，实际上是调色板！ 之后，计算机将识别图像中对象的曲线和轮廓。 所有这些都可以在卷积神经网络中进行研究。 为此，我们将使用PyTorch加载数据集并将滤镜应用于图像。 以下是一段代码。 <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Load the libraries import torch import numpy as np from torchvision import datasets import torchvision.transforms as transforms # Set the parameters num_workers = 0 batch_size = 20 # Converting the Images to tensors using Transforms transform = transforms.ToTensor() train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform) # Loading the Data train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers) import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy() # Peeking into dataset fig = plt.figure(figsize=(25, 4)) for image in np.arange(20): ax = fig.add_subplot(2, 20/2, image+1, xticks=[], yticks=[]) ax.imshow(np.squeeze(images[image]), cmap='gray') ax.set_title(str(labels[image].item()))</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/304/163/1ad/3041631ad58d7300a35af90b39b94584.png" alt="图片"><br><br> 现在，让我们看看如何将单个图像馈入神经网络。 <br><br><pre> <code class="python hljs">img = np.squeeze(images[<span class="hljs-number"><span class="hljs-number">7</span></span>]) fig = plt.figure(figsize = (<span class="hljs-number"><span class="hljs-number">12</span></span>,<span class="hljs-number"><span class="hljs-number">12</span></span>)) ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">111</span></span>) ax.imshow(img, cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) width, height = img.shape thresh = img.max()/<span class="hljs-number"><span class="hljs-number">2.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(width): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(height): val = round(img[x][y],<span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y] !=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> ax.annotate(str(val), xy=(y,x), color=<span class="hljs-string"><span class="hljs-string">'white'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> img[x][y]&lt;thresh <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">'black'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/264/f15/bff/264f15bffe653ae237f3e2fa1fc5c868.png" alt="图片"><br><br> 这就是数字“ 3”被分解成像素的方式。 从一组手写数字中，随机选择“ 3”，其中显示像素值。 在这里，ToTensor（）归一化实际像素值（0–255）并将其限制在0到1的范围内。为什么？ 因为它有助于后续部分中的计算，所以无论是解释图像还是找到其中存在的常见模式都可以。 <br><br><h2> 创建自己的过滤器 </h2><br> 顾名思义，过滤器过滤信息。 对于卷积神经网络，在处理图像时，将过滤有关像素的信息。 我们为什么要过滤呢？ 请记住，计算机必须经历学习过程才能理解图像，这与孩子的行为非常相似。 但是，在这种情况下，我们将不需要很多年！ 简而言之，他从头开始学习，然后全面发展。 <br><br> 因此，网络首先应该知道图像的所有粗糙部分，即边缘，轮廓和其他低级元素。 一旦发现它们，就为复杂症状铺平了道路。 要获得它们，我们必须首先提取低级属性，然后提取中级属性，然后再提取高级属性。 筛选器是一种提取用户所需信息的方法，而不仅仅是盲目的数据传输，因此计算机无法理解图像的结构。 首先，可以基于特定的过滤器提取低级功能。 这里的过滤器也是一组像素值，类似于图像。 可以理解为连接卷积神经网络中各层的权重。 将这些权重或过滤器乘以输入值以生成表示计算机对图像的理解的中间图像。 然后将它们乘以更多的过滤器以扩展视图。 然后，它检测到人的可见器官（假设图像中存在人）。 后来，由于包含更多的过滤器和几层，计算机惊呼：“哦，是的！ 这是一个男人。” <br><br> 如果我们谈论过滤器，那么我们有很多选择。 您可能需要对图像进行模糊处理，然后应用模糊滤镜，如果您需要增加清晰度，则可以使用清晰度滤镜，以此类推。 <br><br> 让我们看一些代码片段，以了解过滤器的功能。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/679/6a4/bb4/6796a4bb4830bda29c6d14212274a286.png" alt="图片"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/752/a89/805/752a89805fba54b5d0f9e90073ca9fde.png" alt="图片"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/84f/8a1/f9c/84f8a1f9c92b1996b0e4eed4a2a7dd5b.png" alt="图片"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/142/635/038/142635038ecef3606d53d5d9c85b26f8.png" alt="图片"><br><br> 这就是应用滤镜后图像的外观，在这种情况下，我们使用了Sobel滤镜。 <br><br><h2> 卷积神经网络 </h2><br> 到目前为止，我们已经看到了如何使用过滤器从图像中提取特征。 现在，要完成整个卷积神经网络，我们需要了解用于设计它的所有层。  SNA中使用的图层 <br><br><ol><li> 卷积层 </li><li> 汇聚层 </li><li> 全粘结层 </li></ol><br> 在所有三层中，卷积图像分类器如下所示： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b4/927/c31/8b4927c31b5f951d7026b30d68695bea.png" alt="图片"><br><br> 现在，让我们看看每个图层的作用。 <br><br>  <b>卷积层（CONV）</b>使用通过扫描输入图像执行卷积操作的滤镜。 它的超参数包括一个可以为2x2、3x3、4x4、5x5（但不限于此）的滤镜大小和步骤S。结果O称为要素图或激活图，其中所有要素均使用输入层和滤镜进行计算。 下图是应用卷积时生成特征图的图像， <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/14d/aab/a2a14daab68c91f8d92ba0c54509493b.png" alt="图片"><br><br>  <b>合并层（POOL）</b>用于压缩卷积层之后通常使用的功能。 联合操作有两种类型-这是最大联合和平均联合，分别获取特征的最大值和平均值。 以下是合并操作的操作， <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c2d/03f/2f8/c2d03f2f8734efade8cbc80d44d3767e.png" alt="图片"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/e5c/558/01ae5c558fa6647bfb9c19b9edabbb37.png" alt="图片"><br><br>  <b>完全连接层（FC）</b>使用平面输入操作，其中每个输入都连接到所有神经元。 它们通常用于网络的末端，以将隐藏层连接到输出层，这有助于优化班级成绩。 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d28/558/188/d285581882fa97824cdc0ad6ecb31873.png" alt="图片"><br><br><h3>  PyTorch中的SNA可视化 </h3><br> 现在我们已经拥有构建SNA的完整思想，让我们使用Facebook的PyTorch框架来实现SNA。 <br><br>  <b>步骤1</b> ：下载要通过网络发送的输入图像。  （这里我们用Numpy和OpenCV做到） <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline img_path = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> bgr_img = cv2.imread(img_path) gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY) <span class="hljs-comment"><span class="hljs-comment"># Normalise gray_img = gray_img.astype("float32")/255 plt.imshow(gray_img, cmap='gray') plt.show()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d0f/3d2/291d0f3d28f3091716c3aba41dc35c59.png" alt="图片"><br><br>  <b>第2步</b> ：渲染滤镜 <br><br> 让我们可视化过滤器，以更好地了解我们将使用哪些过滤器， <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np filter_vals = np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>] ]) print(<span class="hljs-string"><span class="hljs-string">'Filter shape: '</span></span>, filter_vals.shape) <span class="hljs-comment"><span class="hljs-comment"># Defining the Filters filter_1 = filter_vals filter_2 = -filter_1 filter_3 = filter_1.T filter_4 = -filter_3 filters = np.array([filter_1, filter_2, filter_3, filter_4]) # Check the Filters fig = plt.figure(figsize=(10, 5)) for i in range(4): ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[]) ax.imshow(filters[i], cmap='gray') ax.set_title('Filter %s' % str(i+1)) width, height = filters[i].shape for x in range(width): for y in range(height): ax.annotate(str(filters[i][x][y]), xy=(y,x), color='white' if filters[i][x][y]&lt;0 else 'black')</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/4c7/75f/1fc/4c775f1fc19bd461679d5a45831f1e2e.png" alt="图片"><br><br>  <b>步骤3</b> ：确定SNA <br><br> 该SNA具有卷积层和具有最大功能的池化层，并且权重使用上面显示的过滤器进行初始化， <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn.functional <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> F <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Net</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(nn.Module)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, weight)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() <span class="hljs-comment"><span class="hljs-comment"># initializes the weights of the convolutional layer to be the weights of the 4 defined filters k_height, k_width = weight.shape[2:] # assumes there are 4 grayscale filters self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False) # initializes the weights of the convolutional layer self.conv.weight = torch.nn.Parameter(weight) # define a pooling layer self.pool = nn.MaxPool2d(2, 2) def forward(self, x): # calculates the output of a convolutional layer # pre- and post-activation conv_x = self.conv(x) activated_x = F.relu(conv_x) # applies pooling layer pooled_x = self.pool(activated_x) # returns all layers return conv_x, activated_x, pooled_x # instantiate the model and set the weights weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor) model = Net(weight) # print out the layer in the network print(model)</span></span></code> </pre><br><blockquote><pre> <code class="plaintext hljs">Net( (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False) (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) )</code> </pre> </blockquote>  <b>步骤4</b> ：渲染滤镜 <br> 快速浏览使用的过滤器， <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">viz_layer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(layer, n_filters= </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n_filters): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, n_filters, i+<span class="hljs-number"><span class="hljs-number">1</span></span>) ax.imshow(np.squeeze(layer[<span class="hljs-number"><span class="hljs-number">0</span></span>,i].data.numpy()), cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Output %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">12</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) fig.subplots_adjust(left=<span class="hljs-number"><span class="hljs-number">0</span></span>, right=<span class="hljs-number"><span class="hljs-number">1.5</span></span>, bottom=<span class="hljs-number"><span class="hljs-number">0.8</span></span>, top=<span class="hljs-number"><span class="hljs-number">1</span></span>, hspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>, wspace=<span class="hljs-number"><span class="hljs-number">0.05</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): ax = fig.add_subplot(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, i+<span class="hljs-number"><span class="hljs-number">1</span></span>, xticks=[], yticks=[]) ax.imshow(filters[i], cmap=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>) ax.set_title(<span class="hljs-string"><span class="hljs-string">'Filter %s'</span></span> % str(i+<span class="hljs-number"><span class="hljs-number">1</span></span>)) gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>).unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br> 筛选条件： <br><br><img src="https://habrastorage.org/getpro/habr/post_images/885/5c9/cac/8855c9cace448bed1d831c3dc4731828.png" alt="图片"><br><br>  <b>步骤5</b> ：按层筛选结果 <br><br> 出现在CONV和POOL层中的图像如下所示。 <br><br><pre> <code class="python hljs">viz_layer(activated_layer) viz_layer(pooled_layer)</code> </pre><br> 卷积层 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/4c8/1bc/6bb4c81bc6ef16044dfc22e9e36bbaa6.png" alt="图片"><br><br> 池层 <br><br><img src="https://habrastorage.org/getpro/habr/post_images/789/278/823/78927882302ae10f6403ba3a498669fd.png" alt="图片"><br>  <a href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=zh-CN&amp;u=">来源</a> </div></div><p>Source: <a rel="nofollow" href="https://habr.com/ru/post/zh-CN436838/">https://habr.com/ru/post/zh-CN436838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../zh-CN436790/index.html">256行裸机C ++：几小时内从头开始编写光线跟踪器</a></li>
<li><a href="../zh-CN436792/index.html">DEFCON会议19.匿名，我们。 第一部分</a></li>
<li><a href="../zh-CN436828/index.html">向Boost-1.65.1的过渡和出现的错误</a></li>
<li><a href="../zh-CN436830/index.html">直到2019年的Android机器人技术：真实的故事； 分5部分； 第5部分</a></li>
<li><a href="../zh-CN436836/index.html">分析防火墙中的7级应用程序的好处。 第2部分。安全性</a></li>
<li><a href="../zh-CN436840/index.html">从光泽到神经科学的道路：关于媒体和内容营销职业的专题播客</a></li>
<li><a href="../zh-CN436842/index.html">Veeam解决方案，用于在Nutanix AHV平台上备份和恢复虚拟机。 第二部分</a></li>
<li><a href="../zh-CN436846/index.html">前端世界第348周（2019年1月14日至20日）的前端世界摘要</a></li>
<li><a href="../zh-CN436848/index.html">NSA宣布发布用于逆向工程的内部工具</a></li>
<li><a href="../zh-CN436850/index.html">编写单元测试时的常见错误。 Yandex讲座</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter57283870 = new Ya.Metrika({
                  id:57283870,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/57283870" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134228602-6', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

<footer class="page-footer">
  <div class="page-footer-legal-info-container page-footer-element">
    <p>
      Weekly-Geekly ES | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
    </p>
  </div>
  <div class="page-footer-counters-container page-footer-element">
    <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=9uU9J9pq8z7k8xEBHYSfs6DenIBAHs3vLIHcPIJW9d0&co=3a3a3a&ct=ffffff'/></a>
  </div>
</footer>
  
</body>

</html>